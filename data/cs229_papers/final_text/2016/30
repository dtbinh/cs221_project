answering
ye tian


nicholas huang



answering qa challenging
artificial intelligence
understand
answer
encoding
decoding complicated
encouraged qa
lstm

outperforms lstm
comfortable






answering qa enjoyed academic flexibility
flexibility comes great
focused finite
rely
reasoning answer
family
lstm qa
lstm successfully weston
incorporates external attention mechanism
attention mechanism nns attend

attention mechanism lstm
details
combine lstm

li


kind call lstm lstmmemn





weston designed babi
evaluating qa skills babi
consisting
aspect intelligence
details
weston learns combining inference followed
memn variant differentiable weston designed
automatically memn babi weston recorded performances surpassing lstm weakly
wide
attention mechanism mitigates traditional lstm enabling attend internal





babi weston
paragraphs answer designed insights aspects qa
vocabulary
prevents
realistic


https fb babi
























att lstm
en





















att lstm
en





















memn
en





















memn
en





















memn pe
en





















lstmmemn
en






















en





















accuracies





qa
answer tuples
concerned
solely tuple supply
answer
encode

vocabulary encode contexts
integer cij
encode
indices answer
summarize



introduce



memn

memn store contexts embedded embedding denote
embedding ca combine
fc embed embedding fc

embed contexts yet
embedding candidate
answer embeddings
attention
fs ca default scoring cosine
candidate
attention
hops hop
fed stacked


encoding
encoding encoding
experimented
lstm call variant lstm
lstmmemn
replacing cosine
fs multilayer perceptron
architectures name variant mlp


lstm family

rnns lstms tools
lstms
encode attention
powerful mechanism
contexts
lstm embed

fimean

















accuracies babi en
legend lstm attention pyramid lstm memn lstmmemn
embedding
feed lstms sequentially denote lstm fu
denote
fr concatenation

variants lstm
encoding contexts fr
traditional
lstm lstm tends forget
receives ago
doesnt
encodes
mitigate
pyramid feed reading
lstm stable longterm
attention mechanism address attention mechanism comes flavors attention mechanism
token attention computes attention multilayer perceptron

attention mechanism combine
pyramid





theano
babi en babi en
lstm
exception reasoning relations encoding lstmmemn

memn jointly
reaching






memn
memn
computationally easy
mitigate hops
memn





https github

nonlinearities
secondly memn statistically flexible
qa helps memn encodes
fashion lose
orders encoding encoding boost
lstmmemn lstm
flexible encoder
beats plain memn
lstmmemn
overfit memn
say flexible
encoder
variants lstm dont
memn gaps
accuracies accuracies
computationally
harder generalize
attention lstm conduct
hop reasoning hops unfortunately worsens

matter
think contribute difficulty qa
supporting facts involved
relation
supporting facts formulate answer supporting facts decrease

reasons behind straightforward

hyperparameters
supporting facts becomes fully
encode hyperparameter
relation supporting facts

produces
needs encode describing positions
asks
precisely going

needs connect
intermediate sophisticated
reasoning predominantly relations


encoding
sophisticated encoding lstmmemn achieves
beating
contributing difficulty

supporting facts unusually
whereas commonly

turns
lstms translates dependency undesired phenomena vanishing causes
lstm forget facts hochreiter
memn
candidate lstmmemn somehow
overcome partially outperforms





areas remains explored

babi generalize
complicated
memn achieves nearly perfect
accuracies
showing sign
proper generalize
shrink gap
accuracies
children book hill
cnn daily mail qa
hermann whereas vocabulary
babi vocabulary


babi

https github
rc

vocabulary cnn daily
mail
thousands
memn
comes forms hops
vary tremendously
memn regulate hops moreover encoder

facing lstm
dependency explicit memn
mitigate
tackle dependency
turing graves






experimented memn lstms
qa solutions
memn outperform lstm
lstmmemn
achieves accuracies sequential
think lstm
explored

acknowledgement
professor chris potts professor bill helped defining
li helped debug
wouldnt converge
lisa lab tutorial
great head lstm


cho bengio translation jointly align
translate corr abs
graves wayne turing corr abs

url arxiv abs

hermann
kay

https github lisa lab


som teaching read comprehend corr abs url
arxiv abs
hill bordes chopra weston
principle reading childrens
books explicit representations
corr abs url
arxiv abs
hochreiter schmidhuber shortterm

hochreiter bengio
schmidhuber flow recurrent nets
difficulty dependencies
kumar su english
socher ask anything dynamic
corr abs url
arxiv abs

hinton sequential thought processes

weston fergus weakly
corr abs url
arxiv abs
weston chopra bordes
corr abs url
arxiv abs


weston bordes chopra
mikolov
answering prerequisite toy
corr abs url
arxiv abs


