parallelizing autoencoder
tom jiang


autoencoder
archive handouts
parallelizing possibly distributing
designed
parallel goals

compiler mature optimized primary
parallel
findings respects promising nearly faster
depending circumstances parallelizing proved
parallel efficiently
fewer dependencies speedups


engineers google designed
aims easy parallel
call parallel statement

channels primary synchronization
constructs easy synchronize progress waiting
updating condition unwieldy prone statement
waits something arrive channel thread calls
powerful supported elected

combining
easy sort parallel

challenges
autoencoder parallel forwards
backwards propagation prevents overlapping
processed

updated
synchronization
updates activation
synchronize
activation
synchronize
updated


parallelize computes aspect
activation deltas indices
array operations operates
inclusive updates care ensure things
updated simultaneously stored copies
call
delta updates activation
calculations sends
channel
recommended
call
wait opted caller thread
piece
calling everything
something thread pool worker
loop read channel blocks
something read thread sends messages threads begin
piece worker charge
depending receives channel
thing parallelization portion easy read
confusing condition implementations share
divided executed


breaking
chunks calling traditional thread pool
thread channel thread
shared channel slower surprising

chunks lots
keeping processors busy synchronization
cpu bound synchronization carefully divide
ensure processor finishes

compiler
compilers recommended compiler
gccgo gcc runtime
manages lightweight gccgo creates thread
advantage gcc optimizer compilers gccgo
faster despite inefficiencies


compiler optimized eigen
default
intermediate surprising beat
runs slower threaded
intermediate
perhaps surprisingly gccgo compiled runs faster
threaded mode
care sure accessed never
advantage cache locality
implementations compilers beat
thread speedup fairly threads pure
gccgo fastest creates
threads pure speedup perspective gccgo
thread pool greatest speedup threads
speedup

conclusions
promise easy
parallel thing explored truly
parallelize
channels master worker

effort developing reduces dependencies
greatly speedup massive amounts parallelism

fiappendix
intermediate

intermediate

intermediate


