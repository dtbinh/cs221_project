gpu
michael harris jae hyun park jeffrey wang




goals





ages
compile link sdn
imple trix
graphics gpus cudamatrix
tedious
scalar arithmetic
cpu
tional art
mnist hours
wise fortunately computations hyperbolic tangent inverse
parallelized

gpus perfect candidate
wise arithmetic operations
gpus
wise wise
immensely overhead
gpus ac static zeros
operations
scalar propagation
dom
efficiently unified device
block submatrix abstraction operating
architecture cuda supports parallel
rectangular blocks gpu processors cublas blocks
written cuda implements
operations computations
port except wise wise hide complexities

gpu providing
clean interface seamlessly
initialize cpu gpu
separated operator
benchmarked
gpu manager saw speedup
tion recently integrated
tation
sdn
lazy transpose lazy transfer cpu
gpu

wrapper cuda
cublas clean interface integrates sdn
cudamatrix
sdn eigen algebra pack

padding multiples
improves
custom
parallelized




arithmetic operators
cublas cuda cuda
cublas
interface
interface getting massive gains
great
sdn literally
convenient continue
writing cudamatrix
style freedom
eigen desired
discuss detail essential



management cuda
somewhat gpu equivalent pointers drawn primarily
pool kb allocate discretely allocate
kb arrays

kb kb
allocation big primarily
allocation
gpu resource
resolve wrote
manager gpu allocations
pass manager implements pieces functionality firstly
caches allocated arrays generates temporary essentially operation arrays allocated
expensive arrays
caching
secondly manager treats
allocations specially
allocate kb pointers
arrays
kb hold sixteen kb arrays
manager greatly



block operations

eigen implements abstraction operating
blocks
extensively sdn
cuda
gpu aspect block operations
blocks virtual
never explicitly constructed
integers specifying block
operations
copying block blocks
bypass construction calling




gpu manager

lazy transfer

gpu accessible
cpu maintain copies
gpu cpu
keeping copies synchronized
huge overhead transfers
gpu cpu expensive
lazy transfers
copy dirty
computations
gpu copy
cpu never updated
read cudamatrix reconstructions
synchronization occur latter occurs rarely eigen reconstructions bottom

intermediate




correctness

reconstructions identical
reconstructions autoencoder mnist
cudamatrix
eigen
verified
cudamatrix eigen


cudamatrix
eigen bottom






sdn correctness

reconstructions

cudamatrix eigen
settings

autoencoder mnist

autoencoder mnist

cudamatrix


middle speedup
autoencoder bottom






benchmarks

benchmarks
desktop quad core ghz intel xeon
cpu nvidia gtx gpu gpu
moving lab computers dual
nvidia gtx gpus
benchmark cudamatrix
nice cuda
mostly interface
cudamatrix cudamatrix outperforms multiplies
adds

graphs
graphs
trends
noticed benchmarking cudamatrix operations faster
padded multiples floats
overhead computations
transfer
pad

noticed improvements
complicated
cudamatrix
worth complications
interface

autoencoders mnist

speedup cudamatrix
eigen autoencoders mnist speedup improves dramatically
reaching
speedup

showing speedups mnist
speedup cudamatrix arithmetic
operations eigen arithmetic operations
speedup improves speedup

autoencoders

huge win
hours




acknowledgements

thanks ian quoc advising
paul providing great
profiler cuda allowed
quickly issues
thanks professor teaching
opportunity
awesome


speedup cudamatrix eigen

ly yen
gpus restricted boltzmann electrical engineering toronto




raina graphics processors



conclusions

cublas nvidia corporation
nvidia cuda

integrated cudamatrix sdn
moved computations cudamatrix
tanh element
turn tight loop
slow parallelized efficiently cuda moved cudamatrix
wise tanh helped
certainly kind
speedup eigen
speedup
sdn codebase
eigen optimized cudamatrix spent
speedup

indicated moving computations
cpu gpu greatly speeds
cudamatrix
gets eigen room
improvements noted cudamatrix purpose

elsewhere
heavy computations
clean abstraction benefit
cudamatrix

cuda programming guide nvidia corporation
nvidia cuda




