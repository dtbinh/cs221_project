gpus speedup coding
self taught
anand

coding designed exploit
gpu self taught iterative
solving convex originally suggested
parallelized coordinate
generalized regularized squares
demonstrate speedups sign
projected
portion self taught
unlabelled handwritten digits handwritten letters demonstrated





coding alternating convex subsets
solving self taught
illustration
characterize hours bases
shelf improving
designing exploit commodity
gpus comparisons
implementations demonstrate handwritten digits
briefly coding
rk
coefficient coefficient rn


bs si







bi


sparsity penalty si
sparsity convex holding convex
holding convex simultaneously solved
solving bases squares quadratic constraints lagrange
dual regularized squares sign
conducive massive parallelism
presence inverse closed solutions generic libraries solving
inverse gpu exist cublas
sign exploits sparsity generic regularized
squares wider community





regularized squares parallelized coordinate

solving regularized squares
keeping bases alternative sign coordinate
particularly parallel involves proceeding
coordinate
alternatively directions coordinates
descend resultant suggested
lends parallelism allowing simultaneous computations
allowing simultaneous
coordinate thread
simplified





ax

coefficient rn
rk optimize keeping






ya



yt


rk jth xx
reduces
repeated
specified tolerance
easier parallelize thread coordinate wise
optimum turn parallelized
substitute sign
sign analytical inverse




gpu parallelized coordinate


cuda enabled gpus come streaming
sms scalar cores threads
cores sm communicate shared unit thread
execution sp executed concert threads
block blocks scheduled gpu block sm
threads blocks rely communications blocks scheduled
hardware sms resources handle
exploit parallelism sms creating blocks
dont require communications
coefficient computations exploit sm scheduling threads
block coordinates coordinates shared computations
norms temporary mostly stored


providing proof



simplified schematic illustrating blocks threads portions

shared sm shared cores sm
stored device shared sms
executed thread repeated
illustrates
benchmark
gpu equivalent cpu
macbook pro intel core duo gt gpu fair
speedup graphs speedups upto modest
macbook pro sm cores say cores tesla gtx

speedup parallelized coordinate gpu sign cpu
speedup cpu gpu



bases projected

coding keeping
squares quadratic constraints
bs










bi


solved efficiently lagrange dual involving
lagrange dual inverse parallelizable spirit exploring
parallelizable projected
closed rule
projected
bs
constrained scaling


bi

bs bs

cublas libraries speedups
cpu typically
lagrange dual closed faster
fraction decrease
accordingly iterative
dominated coefficient
lagrange dual seconds cublas seconds
incidentally projected seconds
benefits rudimentary reliable






proceed gpu implementations sections
learnt breaking patches
learnt hours gt profile


learnt patches gpu
alternating cublas middle joint
speedups versions
patch





self taught handwritten letters

self taught handwritten letters
interested handwritten letters tile
albeit scenario say labelled handwritten
letters seek automatically characterize coming
handful letters feasible handwritten
digits digits
initially overfitted
looked handwritten digits
digits strokes
digits


leftmost handwritten digits
overfitted
learnt handwritten digits letters interested
self taught
labelled letters feed
labelled multiclass purpose require
letter fashion
worth noting parallelized coordinate
tabulated






letters








suggested

conclusions
parallelized coordinate conducive
parallelism gpu valuable instances
sign



replacement coefficient coding
self taught
demonstrated
generic regularized applicable
areas
speedups sign modest gpu
macbook pro powerful gpus scales
benefits
self taught speedups
definitely
saw speedups
compelling reasons gpus
probabilistic


definitely involve exploiting parallelism
textual classifications
speeds commodity gpus think
onboard robots devices

acknowledgements
immense thanks rajat raina providing invaluable guidance insights
optimizing
coordinate thanks honglak lee providing gpu
benchmarking thanks dr andrew encouraging


rajat raina alexis battle honglak lee benjamin packer andrew self taught transfer unlabeled icml
honglak lee alexis battle rajat raina andrew coding
nips nips
cublas nvidia inc
cuda programming guide nvidia inc
multiclass cornell tj




