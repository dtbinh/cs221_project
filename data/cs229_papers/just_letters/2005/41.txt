chest pain in the emergency department  use of asymmetric penalties in sequential
minimal optimization with feature selection to improve clinical decision making accuracy
acknowledgements  i would like to extend my gratitude to dr  judd hollander for providing
the data set compiled at the hospital of the university of pennsylvania 
abstract
background  disposition and assessment of patients presenting with acute chest pain is
one of the most difficult decisions made in the emergency department  computational
tools have been employed with limited and variable success in this decision space 
objective  apply an asymmetric cost support vector machine to a large data set of
patients seen in an urban academic emergency department to assess the separability of
the data and develop an accurate classifier over the data set 
methods  an asymmetric cost support vector machine and principal components analysis
were implemented in matlab with appropriate variations on john platts sequential minimal
optimization algorithm  non compound linear  radial basis  and polynomial kernels were
examined  using hold out cross validation  optimal parameters for a radial basis and
polynomial kernels  the optimal non compound kernel  the optimal penalty values for false
positives negatives  and optimal dimensional reduction factor were assessed 
results  the data is highly non separable  but by selecting optimal parameters  we
achieved a sensitivity of       and a specificity of        the sensitivity and specificity
that would have resulted from selecting based upon best error alone were       and
       test set and training set error rates were comparable and compare favorably to
previous classification methods applied in the same decision space 
conclusions  support vector machines show promise in providing an accurate classifier
with good performance for patients presenting with chest pain to an urban academic
emergency department  even robust methods have difficulty accurately classifying the
data  asymmetric penalties mitigate the problems encountered with imperfect separation 
further optimizations are possible both to the asymmetric cost support vector machine
itself and to accurately assess the impact of feature selection 
introduction
background  chest pain represents one of the most common presenting complaints of
patients to emergency departments  eds  nationally     the range of diseases that map to
the complaint of chest pain vary widely in both severity and organ system involved  from
psychosomatic  to heart burn on up to potentially lethal cardiovascular pathology 
complicating the clinical assessment      national data suggests that only     of patients
who present to emergency departments with the complaint of chest pain are subsequently
found to have acute cardiac ischemia or another diagnosis requiring admission  
furthermore  of admitted patients  only         are thought to have chest pain related to
acute ischemic cardiovascular pathology        this incurs tremendous costs to the system
without identifiable benefit    in       over three million patients were admitted to us
hospitals with chest pain and the costs to the system for those not found to have an
ischemic etiology for the pain is well over    billion dollars by the most conservative
estimates    however  injudicious discharge of these patients home can result in major
patient morbidity and mortality       in fact  untreated myocardial infarctions have at least a
      month mortality    several studies estimate the rate of discharge of chest pain with
 

fiischemic heart disease to be roughly   to             different computational techniques
have been used to differentiate patients as far back as         an algorithms attributable
to goldman et al  suggest that no one  not even the healthy male in his mid   s  is safe
for discharge home based upon an ad hoc threshold for posterior probability of disease
 usually         the    rubric  in and of itself  has not been established with any sort of
methodological rigor beyond expert opnion      
despite tremendous clinical advances  the rate of missed myocardial ischemia has
remained around    since              neural networks  bayesian methods  and computer
designed decision rules have had variable efficacy in improving on the sensitivity of
experienced clinicians       generally these studies establish improvements in specificity to
levels anywhere from      on up to       but often at considerable cost in sensitivity
down to            there is also the challenge of practitioner acceptance  hollander et al
found that out of     patients enrolled in a study  feedback to physicians with a neural
network affected decisions in only two cases   
expert clinical opinion has major limitations as well  ting et al found that each year
of post graduate clinical experience resulted in a     increased odds of admitting a patient
with suspected ischemic chest pain without an increase in the frequency of detecting
legitimate cases resulting in a marked increase in the rate of unnecessary admission   
dreiseitl et al analyzed four standard statistical computing techniques to identify which
features of a data set were most predictive of ischemic causes for chest pain where  x   
    there was significant inter method variability in which features were selected  but
overall eleven features were selected as important by most algorithms  interestingly  a
consulting cardiologist identified only three of the eleven selected features  surprisingly 
five of the nine features identified by the cardiologist as important were not selected by
any of the learning algorithms   
objective  application of a support vector machine  svm  to the classification of patients
presenting to the ed with a complaint potentially referable to ischemic heart disease 
svms provide a simple and intuitive method by which to differentially handle false positive
and false negative classifications as the consequences of each are not symmetric  the
costs of a missed myocardial infarction are clearly not comparable to the costs of an
unnecessary admission both in monetary and health measures quality adjusted life
years  qalys         a review of the published literature suggests that there is no
published work on the use of svms in this clinical space    feature selection with pca was
examined 
dimensional reduction has several potential advantages  i  reduced
computational complexity  ii  lower dimensional data can be more effectively compiled in a
clinical scenario  iii  mapping data that is not separable in a higher feature space to a lower
feature space where a chosen kernel can more accurately separate the data  the goal is
to draw a distinction between those that require admission and those that do not even if the
final diagnosis is not attributable to a cardiac etiology  clinical evidence suggests that in an
expansive enough data set  perfect classification is not possible  success should be
measured by a reduction in false positives without untoward effects on sensitivity thereby
providing a positive economic and system benefit over existing clinical methods 
methods
a data set from the hospital of the university of pennsylvania in the department of
emergency medicine was modified for this study 
data labeling  data was labeled based upon a final who diagnosis  the number of
possible diagnoses considered was simply collapsed into a binary classification problem of 
 

fii  patients with pathology that required admission  ii  patients who did not require
admission  patients who required admission were ones who either had a final diagnosis
of acute mi  usa  aortic dissection     pulmonary embolism      or experienced a
major complication within    days of being seen in the ed 
pre processing and incomplete data  incomplete data was replaced with maximum
likelihood estimates of data among examples that share a final label  data categories such
as blood pressure and heart rate demonstrate that demonstrate non monotonic behavior
were divided into n variables with binary labels  specifically  a very high low blood
pressure or heart rate denote abnormalities of different type and even degree 
scaling of data  to avoid domination of other parameters by one parameter in the
computation of the inner products  some feature values had to be scaled  several
authors have commented on this  but this is not a formal requirement of svm application   
in the chest pain data set  this is potentially an issue with a generally very sparse matrix
with mostly binary indicator variables and a several data points with routine values in
excess of       consequently  all data values were linearly scaled on the interval        by
dividing by the maximum value for each parameter 
asymmetric cost regularization  c  is the penalty incurred when a positive class  a data
example justifying admission  is labeled for discharge  a false negative  while c  is the
penalty incurred when a negative class  a data example justifying discharge  is labeled for
admission  a false positive   the form of the dual is then 
m

max w         i 


i   

m

s t   


i   

i

   

  m m  i     j  
 y y  i j x  i  
  i    j   

t

x   j 

y  i      

    i  c  for y   i       
    i  c   for y  i      

choosing a kernel  the data is not completely separable with a linear kernel for any value
of c  or c   non linear feature spaces were considered including radial basis and
polynomial kernels  two methods were used to select and construct kernels  kernels
were selected to minimize alignment to each other while maximizing alignment of the
component kernels to the data  the definition of alignment used is attributable to cristianini
et al   

 

 

a k i   yy t  
a k i   k j    

k i   yy t
k i   k i yy t   yy t
ki   ki
ki   ki ki   ki

where k i and k i are kernel matrices

linear kernel   xx t
radial basis kernel   e

 

   xi  x j   xi  x j  

t

 

 

 

p

polynomial kernel   k   x     xx t     xx t

in performing this task  three kernels were considered  with the understanding that the
linear kernel is just a degenerate case of the radial basis kernel   the second method of
kernel selection is detailed below  the alignment method was well suited to selecting the
best value for p of the polynomial kernel 
setting parameter values  the values of c   the linear function c    f c    and the value
gamma for the radial basis kernel are not known a priori  furthermore  of the  
fundamental kernel forms considered  nothing clearly identified one kernel as better than
 

fianother  the following maximization problems were therefore posed and solved with holdout cross validation  hocv  using      of the data set 

m
m m

 arg max    sensitivity    specificity   max w         i     y  i   y   j   i j x  i  
  i    j   
c    f  c        k   x 

i   


arg max
m




  m m  i     j  
 i   t   j   
x 
 arg max     error  max w         i   y y  i j x
  i    j   


c    f  c        k   x 
i   

   

t

x

   

m

s t   

  i y  i      
i   

    i  c  for y  i       
    i  c   for y  i      

                

the variable  above formalizes the notion that the model is intended to emphasize
sensitivity over specificity in the non separable case  no mixed kernels were considered in
the course of this optimization  binary search over the value space was used  while not
ideal  in the interests of time  if the algorithm in the course of optimization did not converge
within      runs of the main smo train loop  then the decision function was assessed with
the parameter estimates at forced termination 
principal components analysis  the data was analyzed with and without dimensional
reduction with principal components analysis  pca   due to time limitations  parameters
were not optimized in the reduced feature spaces 
results
descriptive statistics of data set       patients are compiled in the data set from an urban
academic emergency department from july       through december                 
were diagnosed with ischemic heart disease or a related disease process benefiting from
admission  of these      less than    had an acute process such as aortic dissection or
pulmonary embolism as the putative non ischemic cause for the patients discomfort 
demographics are presented in table    after pre processing of the data set  we identified
   total parameters in the data set  table    
optimal penalties  c  and c    c         from the parameter optimization  the functional
relationship between c   and c   was  c     f  c         c    as a validation method for the
value of c   selected  a receiver operator curve  roc  curve was constructed over
different training sample sizes for a set value of c  and c    f c    the largest area under
the curve  auc  was achieved for values approximately in the range of c       c       c  
the match was not exact  but the proximity of the estimates validated the optimization
results 
performance  the best test set performance achieved a sensitivity and specificity of      
and       respectively after training with     of the data set  training set sensitivity and
specificity were       and       respectively 
with the asymmetric cost svm  we
achieved a higher sensitivity along with a lower specificity and overall error rate of       
there was no systematic pattern to the errors  however  the algorithm was designed to
 





  j  










fibias towards a lower false negative rate  this is predicated on the tremendous costs of an
untreated cardiac ischemic event in terms of qalys lost and costs incurred to the system
from complications due to delays in therapy and the cost of litigation resulting from the error
in question  appendix a  the argument for a biased classifier  
feature selection  the true impact of feature selection could not be accurately assessed
as parameters were not optimized prior to applying pca to the data  as it stands  feature
selection performed best when the top ten or eleven features were selected 
conclusions
insights  first and foremost  the data is highly non separable which is consistent with the
initial hypothesis and comports with the observations of experienced clinicians and the
published literature on the topic  this characteristic of the sample space more than any
other may be what defines it as one of the most difficult decision making problems in
medicine  despite this  considerable insight is obtained 
comparison to the status quo  the asymmetric cost smo was able to classify patients
into a category for discharge at a rate higher than current clinical practice  the associated
drop in sensitivity may not  depending on the prior  meet the rubric of a posterior probability
of disease that is less than     the sensitivity achieved out performs many machine
learning applications applied to this decision space  but does so with a drop in specificity 
limitations and future directions  there are several aspects of the methodology that were
not fully optimized prior to analysis  as stipulated above  in the interests of time  when
optimizing parameters the smo train loop was terminated if there was no convergence
within      runs  the algorithm should be allowed to converge over time thereby avoiding
an incomplete inaccurate optimization  one way to facilitate this is to incorporate an
optimization to platts algorithm identified by keerthi et al that is particularly helpful for large
values of c    furthermore  more analysis on the impact of different types of kernels 
including compound kernels  on the performance of the algorithm needs to be assessed 
generalizability of results  its entirely possible that the feature space over which data was
collected was still inadequate  a consensus approach among experienced clinicians
should be used to establish the data collection feature space for any future work  the
demographics of this data set are not representative of most emergency departments in
this country  the parameter estimates derived in this research are likely not applicable to a
suburban emergency department in the bay area without risking high generalization error 
this hypothesis is  of course  amenable to exploration as such data sets become available 

references  reference  tables  and appendix available upon request  restricted posting format

too long for

 

fi