automated extraction of event details from text
snippets
kavi goel  pei chin wang
december         

 

introduction

we receive emails about events all the time  a message will typically include the title of the
event  the date and time  the location  and sometimes a description  unless the information
is received as part of an integrated email calendar system  this data must be entered into
calendar software manually  similarly  finding text on a web page about an event must be
entered manually into a calendar  the extra time required to do this data entry discourages
users from recording the event at all which reduces the usefulness of both the email and their
calendar software 
for our cs     project  we would like to partially automate this data entry process  in
our proposed tool  the user could select a block of text and choose add to calendar to
automatically extract the event information from the text snippet  this information could
then be piped into a calendar program  for example  given the block
computer systems laboratory colloquium
    pm  wednesday  october         
nec auditorium  gates computer science building b  
http   ee    stanford edu
topic  the reiser   filesystem
the algorithm should be able to return the title  date  time  and location of the event 
to address this issue  we design a modified hmm that incorporates context features 

 

previous work

in our literature search  we found three previous works on extraction of meeting information
from text documents  dalli    applied a named entity recognizer to first pre process the
data and then used an e mail summarizer to extract attributes  however  we were not able
to find the results of this strategy  black and ranjan    used rapier and a hand coded
pattern matcher  the most similar approach to ours was done by almgren and berglund    
in their approach  a hand coded pattern and keyword matcher was used to identify obvious
attributes  when no matching pattern was found  a hidden markov model was used  the
system produced an accuracy of     for date and time extraction  while title and location
accuracy was approximately     
 

fi 

data source

the scarcity of annotated data was found to be a difficulty in our project  we labeled
personal emails to produce a training corpus of    snippets  the corpus included formal
examples  semi structured snippets in which formatting provided significant clues regarding
the underlying states  like the example shown in the introduction   as well as informal
natural language snippets in which there were generally fewer formatting clues about state
transitions 

 

algorithm description

   

hidden markov model

we use a modified hidden markov model  see figure    to identify the meeting attributes
 title  date  time  and location  of the text snippet 
the hidden states of the hmm correspond to the attributes listed above with the addition
of an other state for extraneous data  an observation for a given time slice consists of a
set of features related to a specific word from the snippet  for each word  we consider the
previous  current  and next word to get more contextual information  features are assumed
to be conditionally independent given an underlying state  so the overall emission probability
is simply the product of emission probabilities for each feature  the full list of features is as
follows 
 the current word token  as well as the previous and following words in the snippet
 the part of speech tags of each of these words
we use the maximum entropy part of speech tagger by adwait ratnaparkhi 
 the named entity tag of each of these words
we use lingpipe  a suite of linguistic tools  as our named entity tagger  the namedentity tagger tags words recognized as organization  person  location 
or pronoun 
 capitalization of the current word  capitalized or lowercase 
typically  in snippets containing event information  a new line or a colon is indicative
of a transition between states  in order to capture these state transition indicators  the
probability of a transition is conditioned on both the previous state and on the observed
formatting or punctuation symbol  blanks permitted  

   

constraining the hidden state sequence

for the event extraction task  hidden states should not be repeated after they have already
been identified once  for example  it does not make sense to have a state sequence  location date time location title  a traditional hmm cannot capture this constraint
since the next state depends only on the current state and not upon the entire sequence of
previous states 
 

fifigure    modified hmm
in order to prevent the hmm from making predictions of this sort  we keep track of the
states that have already been visited  when calculating the transition probabilities at a given
time slice  any previously visited state is prohibited  the probabilities of the transitions to
permitted states are not renormalized to sum to   because this would incorrectly inflate the
probabilities of paths where a hidden state had been incorrectly chosen in the past 
in addition  we constrain the hmm to end in a special end state which allows the
algorithm to incorporate knowledge that in a typical snippet some states are much more
likely to be the last state seen than others 

   

feature selection

in our hmm  we use    features to represent the observation in each time slice  in the hope
that the pos tags and ne tags would be useful indicators to the hmm  to test this  we
performed feature selection on the feature set by varying the add n smoothing constants for
each feature  intuitively  note that increasing a smoothing constant causes all emission probabilities from that feature to become closer together  given that the overall likelihood of an
observation is the product of the output of each feature  a feature which outputs relatively
uniform probabilities will have less impact on the overall emission probability than one that
outputs widely varying probabilities  to do optimization  we used a greedy hill climbing
algorithm that iterates through all the smoothing constants and updates the smoothing constant if the performance based on our metric improved  ultimately  no smoothing constants
grew to a large value  optimal values ranged from     to       suggesting that all of the
features contained useful information 

 

fi 

results

   

metrics used

we use three different metrics to measure the performance 
   number of correctly annotated words 
   number of perfect attributes   i e  a completely correct title for a particular snippet 
   jaccard similarity  we use the formula
avgjaccard  

  x  a  b 
n n  a  b 

to measure the overlapping range of a  the set of words that comprise an attribute 
and b  the set of words that comprise the prediction for all examples n 
the number of correctly annotated words is an optimistic measurement of how well the
hmm is doing  on the other hand  the number of perfect attributes gives no credit if the
algorithm misses a title by a single word  the jaccard metric provides a balance between
the two other metrics presented 

   

performance of hmm

the hmm was trained and tested using   fold cross validation on the data set  performance
is as shown in figures   and    overall  none of the meeting attributes were extracted with
sufficient reliability to be end user ready  we noted that correctly extracting a title from
natural language snippets is a particularly difficult task that might require more sophisticated
semantic understanding techniques than are available with an hmm  however  there are
some reasons to believe that our hmm strategy could still be effective for extraction of
times  dates  and locations  in some examples  the algorithm correctly extracted essential
information about the time and date even though it did not match the labeling of the
correct answer  for example  saturday  november     the output of the hmm  and this
saturday  november     test example labeling  are both correct answers from a practical
standpoint even though a mismatch is marked as incorrect  a complete system would not
only extract the time and date but convert them to canonical forms so that the resulting
content could be checked for accuracy  eliminating the need to check the exact word selection 
accuracy num of correct states num of perfect attributes jaccard
title
     
     
     
date
     
     
     
time
     
     
     
location
     
     
     
in some cases  there was evidence that insufficient training material had been provided 
for example  the algorithm in one case marked  onedigit pm  numbers are replaced with
tokens during preprocessing  as a location even though  given a larger set of training data 
this should have been easily identified as a time  more training data would also allow us
to compare performance on formal  semi structured  versus informal  natural language  test
examples and to do more reliable feature selection 
 

fifigure    performance of hmm

 

conclusion

this paper has described a system that can perform event information extraction based on
a flexible hmm based learning algorithm  performance was reasonable but not end user
ready  although it is difficult to compare performance to that of other systems      and
     since each algorithm was trained and tested on different data sets  it appears as if our
algorithm extracts attributes with accuracy comparable to a hand tuned pattern matching
system  however  an hmm based solution provides a framework that may prove to be more
robust with respect to unexpected test examples  i e  misspellings or unusual conventions 
than a pattern matching based system  providing more training data and embedding the
hmm in an end to end system would provide more realistic conditions in which to test the
full potential of the algorithm 

 

reference

  dalli  angelo automated email integration with personal information management applications      
  black   julie and ranjan  nisheeth  automated event extraction from email      
  almgren  magnus and berglund  jenny  information extraction of seminar information
     

 

fi