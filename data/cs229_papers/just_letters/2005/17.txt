grouptime  probabilistic scheduling
kendra carattini and mike brzozowski
introduction
perhaps
one of
computer supported
cooperative work  cscw s greatest
successes of the past decade has been group
scheduling  but virtually all major
groupware systems available today present a
binary view of group calendaring  users are
either free or they are busy  this presents
the false appearance that all of a users free
time is equally free  in reality  not all
available times are equally freepeople
often prefer to keep large blocks of free time
open for work  or try to avoid scheduling
nonessential meetings the night before a
large assignment is due  for instance 
user group
a prime example of a user segment whose
calendars are difficult to predict is college
students  without a clearly defined monday
to friday  nine to five work week  many
students find they need to be available to
work at almost any timeday or night 
weekday or weekend  at the same time a
plethora of commitments compete for
students time  from classes and studying to
jobs  interviews  meetings  and rehearsals to
parties and dates  many students must
coordinate their schedules with others on a
regular basis to arrange team meetings 
whether for class or for outside activities 
and yet each user has their own set of
priorities when it comes to resolving the
inevitable scheduling conflicts that arise 
related work
probabilistic models have been successfully
employed before to predict user availability 
horvitz  koch  kadie  and jacobs used a
bayesian network to forecast a users
presence and availability  based on calendar

information  sensors at his her desk  and the
location of various mobile devices      this
operated more on the short term 
specializing in predicting  for instance  how
soon a colleague who just stepped out of the
office would return  and joe tullio used
bayesian networks to estimate the likelihood
of a user attending a given meeting based on
empirical evidence about which meetings
he she attended in the past  hoping to
improve group calendar accuracy     
however  both projects were specific to
relatively
more
constrained
office
environments  as far as we know no one has
yet applied probabilistic methods to more
erratic student schedules 
in our earlier study  brzozowski and
carattini       a system was built to model
what times would most likely be convenient
for students to schedule their commitments 
the model was then used to predict the
optimum meeting times for a user based on
implicit user preferences for how he she
likes to schedule free time  over time  such
a system would adapt to its users unique
scheduling priorities and preferences 
model ontology and training data
following the model ontology in our earlier
study  see selection from brzozowski and
carattini  probabilistic group scheduling
for chaotic people in appendix a for full
details of the model ontology   we asked a
group of students to provide us with the
details of their commitments over a period
of one week  we then proposed various
meeting times and types over the course of
the week  and asked the users to classify
their availability for the given appointment
into one of four categories  cant make it 

firather not  is ok  and works great  we
extracted features from this data that
corresponded to things like the category of
the meeting to be scheduled  e g  was it a
lecture or a dance rehearsal    the day of
week and time of day of the meeting  the
events that the meeting conflicted with  and
the amount of time before and after the
meeting to the next scheduled event  we
then trained our model use softmax
regression for the   possible labels 
methodology and discussion
the goal of the early study was to model
and build a system for a very small number
of people that supplied large data sets so that
the system could be fine tuned for an
individual 
we found  however  that
although we obtained mild success using
cross validation on a single users data set 
when we learned and tested on different
users  the error rate was very high  the goal
of our present study is to now use a much
larger set of users     users in total with    
data points each  and determine the best
method of transfer learning for determining
a prediction on a user with a sparse data set 
the utility of this is obvious  when a user
begins using the system  there will be little
or no data to reflect the individuals
preferences  we can  however  still make
useful predictions for a person for which we
have no training data  as some principles
hold true across many users  for example 
most of the users in our sample did not wish
to schedule anything between  am to  am
on any day  simply training on all user data
sets may not be the best way to represent
these trends  as an initial       cross
validation over all    users gave an error
rate of       
the first method of transfer learning
attempted was to first learn the weights for
each individual user  and then cluster the
users according to their weights  the idea

behind this method is that people who like to
schedule things in a similar way will have
similar weights  and therefore cluster
together  the cross validation technique
was then repeated on each of the individual
clusters 
the initial clustering was
hierarchical  see fig      to obtain an overall
view of the data and determine a good value
for the number of clusters to use 

fig    initial hierarchical clustering of    user study  each
data set is a row and each feature weight is a column  red
corresponds to negative weights and green to positive
weights 

we determined the best number of clusters
for the training set to be six  and proceeded
to cluster the data into six clusters using kmeans  the clusters can be seen in fig    

finumber of data
sets

cluster  
 
 
 
 
 
 

 
 
  
 
  
 

cross validation error
rate
       
       
       
       
       
       

off by  
       
       
       
       
       
       

off by  
        
       
        
        
       
       

off by  
        
       
        
        
        
        

all data
sets
  
       
       
                
table    error rates for clusters given by k means algorithm  off by   indicates the predicted class was one class
away from the correct class  e g  rather not is one class away from cant make it  

we then repeated the       cross validation
on each of the individual clusters  and
obtained the error rates show in table   

fig    k means clustering of    user study into   clusters 
each data set is a row and each feature weight is a column 
red corresponds to negative weights and green to positive
weights 

the results are mixed  three of the clusters
showed an increase in error rate over the
entire user set trained system  whilst the
other three showed a decrease  one cluster
in particular  cluster    showed a very large
decrease in error rate  although these error
rates are high  we can see that most of the
errors are off by one  i e  the classification
predicted by our algorithm was one class
away from the actual class  e g  works
great is one class away from is ok   the
error rate from cluster    which only had
one users data set  was extremely high 
this result shows the need for a good
transfer learning algorithm  as learning over
a single users data provides the algorithm
with insufficient data to accurately predict a
scheduling preference 
the results from this initial transfer learning
method were disappointing  in that there was
on average no decrease in the cross
validation error rate over the baseline using
all data points  the next step was to
investigate alternative methods of transfer
learning in the hopes of attaining decreased
cross validation error rates 
an initial attempt at simulating online
learning using the perceptron algorithm
yielded the results shown in table    the
class labels were modified so that is ok

fiand works great were combined into one
class  and rather not and cant make it
were combined into another 

data
set
ktph
w  p
all
all

  training points
used in online
portion of
learning
   
   
    
   

error
rate
     
     
       
     

table    error rates for data sets using the
perceptron algorithm for online learning 

only one result showed a minor
improvement over the baseline softmax
cross validation error  the trial first trained
on      data points using the perceptron
algorithm  and then proceeded to use the
online learning update for the next     data
points  which were all from the same user  
the rest of the results showed little to know
improvement over random guessing     
error rate   and so this method of learning
was not pursued further 
the third method of transfer learning
investigated was weighted regression  four
different kinds of weighting schemes were
investigated on the six clusters 
   the weights were calculated according
to the euclidean distance between them
using the formula 

 x j
w   i     exp 

 i  

j

   

 x j   
 

   

   the weights were calculated using the
time of day and day of week features
only  times of day and days of week
closer to the point in question were
weighted more heavily 
since the
feature vectors contained mostly
boolean values and had very high
dimension  the euclidean distance
between many of the vectors was nearly
identical  for this reason  we decided to
use a distance metric based on only a
few key features that were not boolean
values 
   the third weighting scheme gave a
training point a weight of     if the
query point was targeted for the same
user as the training point came from  if
the user was different  the training point
was given a weight of     
   the fourth weighting scheme was a
combination of the first and third
schemes 
the weighted regression was run on each of
the smaller clusters using both       and
      cross validation  the results are
shown in table   

ficluster

weighting
scheme

     
cv

 

 

x

 

 

 

     
cv

error rate off by  

off by  

off by  

       

       

        

        

x

       

       

       

        

 

x

       

       

        

        

 

 

x

       

       

      

       

 

 

x

       

       

       

        

 

 

x

       

       

       

        

 

 

x

       

       

       

        

 

 

x

       

      

        

        

 

 

x

       

       

        

      

 

 

x

       

       

        

        

 

 

x

       

       

       

        

 

 

x

       

       

      

        

 

 

x

       

       

       

        

 

 

       

       

       

        

x

table    error rates for clusters using   different weighted regression schemes  scheme   weighted according to euclidean
distance    according to distance in the time of day and day of week dimensions only    according to the user  and   was a
combination of   and   

we can see that  with one exception  the
error rate decreased from the baseline for
each cluster using the un weighted
regression  the combination weight scheme
     consistently gave the lowest error rates
of all the weighting schemes  but there does
not appear to be a straightforwardly
discernable hierarchy of weighted regression
schemes  it is not surprising that the
combined weight scheme gave the lowest
error rate  since this scheme takes into
account both how closely related two data
points are  distance  and how their sources
are linked  same or different users  
these results indicate that weighted
regression within a cluster would be a useful
way to learn weights for an individual that

would make use of other users training data
as well  and thereby require fewer training
points for accurate predictions from the
individual in question  the online learning
method did not prove very promising for
this task  nor did the clustering on its own 
future work in this area should therefore
concentrate on finding the best weighted
regression scheme using a minimal number
of data points from an individual user  as
this will be the actual task faced in a
groupware scheduling application  other
areas of interest that will arise in such a
setting include learning with an incomplete
set of features  e g  if the user does not
provide a full calendar   and taking into
account the person who initiates the meeting
invitation  e g  a boss vs  a friend  

fireferences
   horvitz  e   koch  p   kadie  c  m   and jacobs  a 
coordinate  probabilistic forecasting of presence and
availability  proceedings of the eighteenth
conference on uncertainty and artificial intelligence 
edmonton  alberta  morgan kaufman                
   tullio  j  intelligent groupware to support
communication and persona management  acm
symposium on user interface software and
technology  doctoral consortium         
   brzozowski  m   and carattini  k 
probabilistic group scheduling for chaotic people 
cs      stanford university  fall        

fiappendix a  selection from 

probabilistic group scheduling for chaotic people
mike brzozowski

kendra carattini

 the time of week t  encapsulating both a day and time 

model ontology

 the presence of other events eo overlapping with t and
their corresponding categories 

we believe that the probability that a user will
want to schedule meeting m at time t  where t
is a span of time on one or two specific days  is
dependent on 

 the length of time dt  between the end of preceding
event e  and the start of t 

 the category c of m  that is  what type of meeting it is 
table   shows the categories we chose for this domain 
which capture a range of pressures  social  academic 
professional  economic  etc   to attend various types of
events 

 the length of time dt  between the end of t and the
start of the following event e  

category

description

example from user data

interest

an optional event the user might like to attend psychology professors talk on
out of interest  not because friends will be present politics of fear and iraq prison abuses

interview

a job interview 

lecture

a lecture for a class the user is taking  attendance ling     lecture
at which is not strictly required 

practice

practice for a sports team or group 

tae kwon do practice

project

a meeting to work on a team class project

cs     class project

rehearsal

a rehearsal for a performance group or show 

viennese
rehearsal

section

an optional discussion section for a class 

cs     section

seminar

a small seminar with required attendance 

cs     seminar

sleep

time the user plans to be asleep  

six
hours
rehearsal

socialprivate

a social event with a small group of friends  peer thanksgiving dinner at a friends
pressure encourages the user to attend 
apartment

socialpublic

a social event with a large group of people  most ragtime ball
of whom will not notice if the user is absent 

study

time the user plans to be studying 

phone interview with yahoo 

ball

opening

preceding

waltz

morning

study for cs     midterm

 

we discovered that not all users view sleep as an inflexible time commitment but believe our model should reflect that some
users dont mind scheduling meetings far into their sleep hours 
 

fifigure   a sample of data collectd from a test subject  white boxes are prior scheduled commitments  red boxes are times labeled can 
t
make it  yellow ones are rather not  green ones are labeled is ok  blue ones are works great  the four large columns represent four days
of the week  the columns within each day correspond to different hypothetical commitment categories  notice the subject is more willing to
miss events marked
and has no flexibility to get out of
 

studygroup

a meeting with a group not obligated to work cs     problem set   study session
together  to study 

work

work as part of a paying job 

course advisor office hours

training data

we gathered training data by soliciting subjects
complete schedules for the following week and
encoding a series of events according to our
ontology  we then proceeded to ask subjects to
consider a series of hypothetical meetings
randomly selected from categories the subject is
likely to encounter  e g  users who do not
participate in sports or performing arts are not
asked about extra practices or rehearsals  users
are not invited to pick a time to meet to sleep  
subjects were given a graphical representation
of the schedule they supplied us with the
suggested times highlighted  they were asked to
assume that they wanted to schedule m at some
point over the week and to label each
prospective hour long slot t with one of four



rather not  the subject would rather not attend at t
but could if necessary 



is ok  the subject could meet at t but there are other
times that work better 



works great  t is one of the best times for the
subject to meet 

currently we have      data points obtained
from nine subjects  representing a variety of
preferences and some variations in priorities 
the subjects are undergraduate and graduate
students in both engineering and humanities
disciplines 
a sample of the data collected from one subject
is shown in figure   
feature calculations

after our early trials with logistic regression  it
became apparent that the time of day  day of

table   possible values for category c in data collected

options 


week  and time until the events before and after
the hypothetical meeting were the most highly

cant make it  the subject cannot attend m at t
under any circumstances 
 

fiweighted in our hypothesis function  however 
this achieved poor results with high error over
our training data  we hypothesized this may be
due to the fact that there is not a linear
relationship between some features and the
likely classifications  we therefore considered
alternate possibilities for these highly weighted
features that would more accurately represent
the users preferences 

actually preferred to schedule their events close
together  thereby leaving large blocks of free
time for other purposes  in order to take this into
account  we adopted a similar approach as the
time of day and day of week features  however 
since the dts are continuously valued  it is
implausible to directly calculate the expected
value over every possible dt  instead  we fit the
data to  nd through  th order polynomial
regressions  using a       cross validation
split   and averaged the mean squared error over
ten trials for each order polynomial 
we chose cubic regression as our feature
function because it had one of the lowest
generalization errors  and did not appear to
overfit the training data  as an additional
feature  we decided to experiment with a
combined regression over the joint distribution
of dt  and dt   this cubic regression proved to
have an even lower generalization error  and
was included in the final feature set  this seems
intuitive if we consider people trying to
schedule their meetings in the cracks in their
schedule  i e  places where both dt  and dt  are
low  but not where one of them is low and the
other high  thus  it makes sense that these
features are not necessarily independent  after

day and time

initially  the time of day feature was simply a
value between   and     representing the halfhour slot when the hypothetical meeting would
begin  but the most favorable times are not
necessarily early or late in the day  so we
calculated the expected value of the
convenience labeling over each time slot over
the entire week  similarly  we modified the day
of week feature  which had initially been a value
between zero and six to return the expected
value of convenience calculated for each day
over every time slot 
due to the sparseness of the training data  we
implemented a smoothing algorithm to avoid
zero probabilities and ensure that slots with few
examples were not biased  to do this  instead of
taking the exact expectation over each time slot
 and day of week   we calculated a locally
weighted average of the expectation over all
time slots  and days of week   these
adjustments increased the accuracy of our
learned hypothesis on the training data slightly 
a sample of the final results can be seen in
figure   
times between events

the initial features storing the times in between
events  dt  and dt   were simply the raw values
of dt  and dt  in minutes  this formulation
seemed reasonable  as it favors a positive
response when the hypothetical event occurs far
away from other events in the schedule  we
soon realized  however  that this scheduling
preference did not generalize from the training
data  but rather from our own pre suppositions
about how people schedule their time  after
observing more training data  we noticed that a
significant proportion of our test subjects

figure   distribution of e label   day  time  for a subject  see
figure   for color coding   for this subject  the best times to
meet tend to be between    am and   pm 

 

fialtering the dt feature functions to reflect the
user responses  we were able to obtain    
accuracy on our training data using logistic
regression 

categories  and the no conflict category
returned the smoothed expected value over all
non conflicting queries in the training data  the
features were smoothed by taking a fraction
 lambda  of the expected value over the conflict
category and summing with    
lambda  expected value over all conflict
categories  however  adding this feature
actually decreased the accuracy of our
hypothesis function  this was a rather surprising
result  since the similar formulations over the
other features had all increased accuracy 
one possible reason for this may be that the
weights being learned for each of the label
classifiers themselves dont bear a linear
relationship with  say  p rather not   thus  if
the classifier were to attach a positive weight to
the expectation it would attach an even greater
weight to an expectation closer to works great 
thus  our estimate of the expectation does not
necessarily contribute to the correct p y s wed
expect 

conflicts

the conflict category feature s  posed an
interesting dilemma  choosing a number to
reflect the category of a conflict would
incorrectly suppose a continuous relationship
between conflict categories  when actually the
categories are either independent or their
preferential ordering is indeterminable a priori 
to overcome this  we implemented a separate
feature for each potential conflict category  for
each query  a boolean array was built to
indicate which categories the query conflicted
with  note this number can be greater than one 
since we can conflict with multiple events for a
given query   the feature for each category was
then   if the query conflicted with an event in
this category  and   otherwise  this
formulation seemed unsatisfactory  since it blew
up the size of our feature set so that a different
weight had to be trained on each conflict
category  in order to improve this  we tried to
use the same expectation trick we described
earlier 
softmax regression

we still felt that this did not adequately capture
the midrange of possible labels  rather not is
ok   the chief area of interest  since we seek to
provide nonbinary classification  our next step
was to implement softmax regression  using
softmax  we were able to classify the responses
into each of the four classes  rather than
arbitrarily splitting the response space in half 
this yielded encouraging results  and errors
typically took the form of misclassifying into
neighboring labels  e g  is ok into works great
instead of cant make it  
conflicts revisited

we attempted to implement a conflict
category feature that would return the expected
value of the response  given training data with
that conflict  categories that did not conflict
with any of the training data returned a
smoothed expected response over all conflict
  

fi