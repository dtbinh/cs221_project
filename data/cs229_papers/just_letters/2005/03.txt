cs     final report

motamedi  percival

assessing opinion and usefulness of product reviews
brian percival  ali motamedi

abstract
in this project we examined the possibility of using supervised learning techniques to classify
the opinion of online product reviews that are not accompanied by a quantitative opinion
measure  the classifiers were trained using amazon com customer reviews  the test results
showed that a combination of a heuristic feature selection algorithm and a support vector
machine  svm  classifier trained with      samples can classify the tone of product review
written for the same product type with     confidence  our results also suggest that a classifier
trained on reviews of one product type can predict the tone of reviews of a random product with
    accuracy 
data collection
source of training data  the amazon com website is a rich source for customer written product
reviews  the on line store has thousands of products under many different product categories 
each product may have hundreds of reviews  as a result  the website offers thousands of reviews
for many products  each review is accompanied by the authors rating for the product being
reviewed  which provides raw data for determining the authors intended opinion in the review 
further  each review can be identified as helpful or non helpful to other consumers  yielding a
score for the helpfulness of the review 
we selected two product categories on which to train opinion classifiers  digital cameras and
photography books  both categories offered over     products with one product review or more 
using a perl script  we crawled through search results listing the products within the specified
category in order by average customer rating  this allowed us to be sure to extract reviews from
all products that had been reviewed at least once  to avoid brand or product names from biasing
the classifier  a maximum of only    reviews were extracted for each product  the first   
reviews listed by amazon com were the reviews extracted  which appeared to be the    most
recently written reviews  the html for each review was converted to an xml representation to
allow for multiple iterations of feature extraction  the text of the reviews was then stemmed
according to porters stemming algorithm      and each review converted to a feature row vector
in a training or test matrix 
class definition  initially  we intended to classify the reviews into three categories  positive 
neutral and negative  with the following raw rating to classification mapping  each possible pair
of classes were to be used to train an svm to vote for one of two classes  and the class that
received a majority vote would be the output of the overall classifier 
customer rating
       
   
       

class
negative
neutral
positive
total

camera examples
    
   
    
    

book examples
    
    
     
     

table    review classes and numbers of examples of each 

source of test data  we were able to obtain a large set of review data  enabling us to randomly
select up to     positive and     negative reviews  leaving the remaining reviews to be used for

fics     final report

motamedi  percival

verification  no training reviews were ever used for verification on the same classifier they were
used to train 
we also manually saved and scored reviews retrieved through online search of newsgroups
on google  we searched with queries such as digital camera opinion and photography book
opinion  and extracted all messages that appeared to be expressing an opinion  these were
roughly     of all results returned  we each used our personal impression of the nature of the
opinion being expressed in the review to be used as the expected output of the classifier  because
of the manual nature of this process  we had many fewer newsgroup reviews for verification    
book reviews     camera reviews and    random reviews  which were found by searching for
product opinion 
feature selection
features  we considered three types of features to classify the reviews  single word features 
punctuation  and numeric features
intuitively  the negative reviews would be expected to contain more features of negative
meaning such as poorly  disappointed  worst etc  and positive reviews would have more
inherently positive words such as  good  impressive  and superb  there were many words in all
caps formats  these words were considered as an emphasized version of the original word  thus
they are counted more than one when they have occurred in a review  for punctuations we
consider the count of      and   to be a separate feature of each review  the total number of
digits in a review can be used to assess its usefulness  the number of numeric characters is
another feature 
feature selection algorithm  we trained our classifier using the svm method  since the
complexity of svm algorithm is proportional to the feature size  we chose to limit the total
number of features to guarantee convergence in a reasonable time  to achieve this goal  the total
number of features were chosen to be at most       this value achieves a balanced tradeoff
between efficiency and accuracy of the classifier 
table   shows the total number of unique single word features in a set of training samples
with different sizes 
 of reviews
  of unique features

    
     

    
     

table    the total number of unique features before filtering 

to achieve goal of reducing the number of features below       we first used a stemming
algorithm     to condense words derived from the same stem word to the same feature  as
mentioned above  further  we used a heuristic method to rank the features according to their
utility and kept the     of the most useful features  these heuristics are based on the following
observations 
  the features that occur in only a tiny fraction of all reviews are not present in enough
examples to be useful for classification  in order to remove these less frequent features 
we required that each features of occurrence in all documents be at least       
  features with uniform distribution across all classes convey no information about the
opinion expressed in a review  many of these words fall into the following categories 
o common words related to english grammar structure such as  is  a  to  the
o general neutral factual terms  camera  book

fics     final report

motamedi  percival

value function  to remove the words identified in the second observation  we defined a heuristic
value function  this value function is a measure of the discriminability of each feature  or a
measure of how disproportionately present it is in one class of reviews over another  the value
function should return zero for a feature uniformly distributed across categories  it should also
return a relatively high value for features with high probability of presence in one category
versus the others  in order to calculate this function we need to find the distribution of each
feature in each category defined as follows 
pji  

the percentage of occurences of feature i in category j
the sum of percentages of occurence of feature i in all categories

   

the following heuristic function is then calculated to rank the features 
v i       pji   pki    

   

j k
j k

if a feature has a uniform distribution across categories this function return a value close to
zero  on the other hand  if a feature shows a strong presence in one of the categories compare to
the others  this function returns a value between   and    table   shows the most valuable
features according to the defined value function  and their corresponding frequencies 
word
crap
junk
lemon
thottam
evolt
jameson
honor
cancel
ds
gateway
void
trash
corrupt
refund
receive
pleasantly

  pos
 
  
 
  
 
  
 
 
  
 
 
 
 
 
  
  

 neut

 neg

 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 

  
   
  
 
 
 
  
  
 
  
  
  
  
  
  
 

value function
                
                
                
                
                
                
                
                
               
               
                
                
                
                
               
                

table     the most useful features ranked by the their value 

as the table shows  the feature selection method chooses the words that are mostly indicative
of certain tone in a review  for example  in the top of the list the words like crap and junk are
most useful in classifying a review as negative and words like pleasantly are useful in classifying
a review as positive 
training method
we used a matlab implementation of platts smo     algorithm to calculate the svm
parameters  w b  for a linear classifier  in the first experiment we trained three different
classifiers with      training samples for each  table   shows the misclassification rate for these
classifiers 

error

positive vs 
negative
      

positive vs  neutral

neural vs  negative

      

      

table    the error performance of each classifier after training with      samples

fics     final report

motamedi  percival

as the results indicate  the classifiers cannot differentiate between the negative and neutral
classes  on the other hand  it can successfully classify a positive review from the other two
types  to resolve this issue we decided to combine the neural and negative classes into a single
class 
after this modification  we trained a single svm to classify between positive and negative
reviews  we performed two experiments  in the first experiment we examined the
misclassification rate as a function of the training set size  two svm were separately trained by
                    and      samples from digital camera and book reviews  we then tested the
classifiers on a number of random reviews of the same type that were not included in the training
set  in order to have a symmetric error probability of positive and negative reviews we decided to
have equal number of training samples of each category  in the second experiment we planned to
test if a classifier trained with one product category can estimate the opinion of other product
reviews  in this experiment the classifiers were trained with digital camera reviews and tested on
book review and vice versa 
results
error vs  training set size on same review type  figure   shows the error during verification on
the test set for various training set sizes  in each case  all reviews in the corpus not used for
training were used in the verification step  the training set sizes used were                    
and       in each case      of the training set were positive reviews and     were negative
reviews  in both cases  the error rate appears to asymptotically approach a value near      with
little decrease in error from      to      in book reviews  and from     to      in digital
camera reviews 

 a 

 b 

figure    error vs  training set size for a  book reviews and b  digital camera reviews

error for reviews from different sources  table   shows the errors obtained when a classifier for
one product type is used to classify the reviews for the other product type  in both cases  the
entire review corpus for each product type was used in the test set  for testing the book classifier
with digital camera test data  there were      test reviews  for testing the digital camera
classifier with book review test data  there were        test reviews 
in table    the error rates for positive test reviews and negative test reviews are reported
separately  to reflect our desire to equalize the error between positive and negative reviews  the
overall error rate in each experiment is also presented 

fics     final report

motamedi  percival

training data
camera reviews

testing camera reviews

testing book reviews

positive
reviews
   

positive
reviews
   

overall

negative
reviews
   
   

book reviews

   

negative
reviews
   

   
   

   

   

overall
   
   
table    errors for each cross training experiment  first two rows present accuracy of the digital camera trained
classifier  second two rows present accuracy of the book trained classifier  errors are shown on the overall test set
as well as for the positive and negative sub sets 

table   presents similar data for the product reviews found in newsgroups  for these
experiments  fewer test reviews were available     book reviews     digital camera reviews  and
   random reviews  here again  errors are higher than the review classifiers testing reviews of
the same product type  but errors are near the errors obtained on the cross training experiments
of table    though the sample sizes are small  these preliminary results suggest that a single
general classifier is feasible that will obtain error rates less than     
book review from
newsgroups
training data
camera reviews
overall

camera reviews from
newsgroups

random product reviews
from newsgroups

detecting
positive
reviews

detecting
negative
reviews

detecting
positive
reviews

detecting
negative
reviews

detecting
positive
reviews

detecting
negative
reviews

   

   

   

   

   

   

   

   

   

table    errors for cross training experiment using newsgroup postings as reviews 

conclusions
with the feature set we selected  our results suggest that      training samples are sufficient
to achieve        error rate when trying to classify reviews on the same product type  the
results also suggest that roughly     error rate can be achieved with a single general classifier
using the features obtained through the above methods  some further work that might improve
the error rate would be to include bi  and tri grams of words as features  to capture meaning in
such phrases as not bad or never appropriate   which are lost in a unigram feature set as in
this work  in addition  some work might be done to investigate ways to detect reviews that refer
to more than one product  some reviews  for example  compare one product to another  which
might appear to the classifier to present conflicting opinions within the same review 
references
    http   tartarus org  martin porterstemmer 
    j  platt  fast training of support vector machines using sequential minimal optimization 
       also  http   research microsoft com users jplatt smo html 

fi