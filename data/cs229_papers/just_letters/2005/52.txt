combining monocular and stereo depth cues
fraser cameron
december         
abstract
a lot of work has been done extracting depth from image sequences  and relatively less has been done
using only single images  very little has been done merging these together  this paper describes the fusing
of depth estimation from two images  with monocular cues  the paper will provide an overview of the stereo
algorithm  and the details of fusing the stereo range data with monocular image features 

 

introduction

recent work has been done on depth estimation from single images    while this is exciting  it could benefit from
the existing stereo image depth algorithms  in a real control system there would afterall be streams of incoming
images  this paper will discuss first the stereo algorithm  then how to fuse stereo depths with the monocular
features  and finally the results obtained so far 
there has been a lot of work done calculating depth from two stereo images  here we assume only knowledge
of the camera calibration matrix  and no relative rotation  the rest is estimated from correspondencences  we
use feature based depth estimation techniques due to the availability of matlab     code  and a perceived
speed bonus  this is more fully explained in section   
recent work by sexena et al  has focused on depth estimation from monocular image cues  they have
supervised learning to train a markov random field  this work has also been applied to obstacles depth
estimation for use controlling a remote control car at speed     this second application specifically requires fast
computation  the intention of this work is to begin combining monocular and stereo image cues into a fast
computation  allowing improved depth estimation for dynamic control applications  we lay the groundwork for
this in section   
due to time constraints we were only able to generate and test the stereo system  these details are provided
in section   
finally the paper close with some conclusions  and notes on work in progress in section    and acknowledgements in sections   

 

stereo depth estimation

depth estimation in this paper using image sequences is broken up into   sections  feature detection and
correlation  estimating the fundamental matrix  additional guided matching  depth estimation  and error
estimation 

   

feature detection and correlation

for correlation we desire a relatively small and even smattering of feature points  this is achieved by corners  in
this paper we use harris corner detection  which uses a threshold on the top two singular values of small image
windows  the harris algorithm uses a spatial edge suppression technique to prevent detecting multiple edges
where only one exists 
  ashutosh

saxena  sung h  chung  and andrew y  ng  learning depth from single monocular images
d 
kovesi 
matlab
and
octave
functions
for
computer
vision
and
image
processing
http   www csse uwa edu au pk research matlabfns 
  a zisserman  matlab functions for multiple view geometry http   www robots ox ac uk  vgg hzbook code 
  jeff michels  ashutosh saxena  andrew y  ng  high speed obstacle avoidance using monocular vision and reinforcement
learning
  p 

 

fikovesi provides a correlation scripts for matching through monogenic phase or direct intensity correlation 
each searches for high window correlations in a range around each feature  only matches where each feature
selects the other are kept  kovesi notes a typically better performace for matching through monogenic phase 
and a potential comparability of speed  i have used monogenic phase matching here and regular matching later 
to get the benefits of both 

   

fundamental matrix

from the correlated matches we can generate the fundamental matrix  f   for the two images  f encodes the
location in pixels of the projection of the two cameras on the opposite image plane as well as the rotation between
the two images  the equation defining f is 
ptr  f  pl    

   

where pr   and pl refer to the pixel coordinates of the matches in the right and left image respectively 
here we use the random sample consunsus algorithm  ransac  which iteratively computes an f based
on a random sample of matches and evaluates how many matches agree  the eight point algorithm is used
to construct f  to evaluate whether or not a match fits the current estimate ransac simply evaluates    
and applies a pre determined threshold  ransac stores the details of the best candidate found  the algorithm
terminates when it is     sure that it has chosen a random set of matches containing no outliers  the probability
of an outlier being picked is set from the fraction of matches were deemed inliers for the best candidate 
since ransac classifies correlations into inliers and outliers  we simply discard the outlying matches 
the ransac distance function ignores the direction of the match  and large angular deviations from the
epipolar line for matches with very similar pixel coordinates  while these checks could be included in the distance
function  they are used afterwards  to avoid high computation costs in the iterative ransac algorithm 
this implementation assumes no relative rotation  since most control algorithms using image streams will
have small relative angular rotations  and will be concerned primarily with nearby objects  this leads to a faster
and more consistent ransac convergence 

   

additional guided matches

now that we have determined f we have a lot more information about where matching features should be  indeed
we can reduce it to a one dimensional line search  further  since we have only translation we can determine
whether a matching feature should be closer or farther from the epipole  we use this information to perform
another dual correlation search  we perform several rounds of matching  removing succesfully matched points
at the end of each round  to obtain as many correlations as possible 

   

depth estimation

with the camera calibration matrix  m   and f found above  one can obtain the essential matrix  e  which
encodes information on the relative rotation  and position of the two cameras  not knowing how far or what
direction we have moved  results in a scale factor ambiguity in e  we obtain the normalized e using 
e p

mfm

   

tr  m f m  t  m f m     

given that we have assumed pure translational motion  e takes the form 


 
tz ty
e    tz
 
tx 
ty tx
 
where t is the normalization of the unknown translation vector  it is worth noting that

   
h

t 
t 

t 
t 

 

i

is

the projection of the epipole on the image plane  using m and t one can estimate the depth of points by
triangulating their position using 
t   xr t 
zl  
   
xl  xr
here xr and xl refer to the  st coordinates of the projected matches in the right and left camera frame respectively 
this equation follows the convention that third component of the projected matches is    they are obtained
 

fifrom pr   m pr   since we have assumed r   i  the only ambiguity comes from the sign of t   which just causes
a sign switch in zl   in cases where xr  xl is too small we substitute yr and yl instead 

   

error estimation

to estimate the error of this stereo algorithm  a series of photographs and laser depth scans were taken around
campus with   foot separations  thus the comparison is between the depths calculated from the pictures and
the laser range depths 
there are a number of sources of error in this matching 
   the laser depths are taken slightly misaligned from the camera  this misalignment can very easily cause
massive errors due to the features being positioned often on the edge of large stepchanges in depth  an effort
has been made to select pictures with many features in consistent depth regions 
   the relatively fewer corners offered by structured rather than unstructured scenes increases the correlation
accuracy  since there are simply fewer wrong choices to confuse the matching algorithm  in extreme cases  there
were not enough matches to estimate f   leading to no depth data at all 
   since the pictures were taken at different times near sundown  about   minutes  some scenes have different lighting conditions  we have removed the scenes with very significant lighting changes  but some lighting
differences remain  also  some objects  people and bikes  have moved between pictures 
   the laser depths and stereo depths have different ranges  this leads to large errors when stereo depths
are well beyond the maximum laser depth 
   metal or windows can create spurious features from reflected light  they can also create laser depth
readings by not reflectingthe laser back 

 

fusing with monocular cues

saxena  et als learning depth from single monocular images uses a markov random field to model the
depth relations among image patches  they then maximize the distance probability over several parameters
given the dataset  while  they use both laplacian and gaussian distributions  we will only concern ourselves
with gaussian distributions here  further they use multiple scale leading to several layers of distance paramters 
only base distances are left independent  as the lower resolution copies are restricted to be averages  we extend
their gaussian model by adding a gaussian penalty for distance from the stereo depths  the model is 
 
 
m
x  di      dm  dsf m i    x
 d     xti r   
 

p  d x        exp 
 
 
z
 sf
  r
m
i  
isf m


m
  x
x  di  s   dj  s   
x

   
exp 
 
  rs
s   i  
jn   i 

di  s  refers to distance i at scale s  sf m is the set of point
p provided by the stereo algorithm  dsf m i is the
distance provided by the stereo algorithm for point i dm   isf m  sfdim    sf m is an empirically determined
quantity expressing the error in the measurements  all other parameters are as described in saxena  et als work 
we choose to restrict
e    d      the maximum liklihood estimate
if we put this into the standard gaussian form  exp  d   t 
for d would be   expanding by order of d we see 
x x
e            

 
   
 r
 rs i  sf m
sf m
jn   i 

s

   

where sf m    r   and rs are known empirically   can then be found by iteratively fitting it to minimize the
gap between the two term of order   in d  this  combined with the current monocular techniques gives us a
gaussian models for each di   and thus the maximum liklihood estimate 

 

results

figure   shows an image in various stages of processing  for unstructured environments there are an enormous
number of corners leading to many correlations  these are then whittled down by estimating f   and converted
into depths  shown below  stereo depth algorithms will never show the sky as it has no corners 
 

fi a 

 b 

 c 

figure    a  base left image  b  potential matches from correlation  c right camera image with inlying matches 
crosses indicate corners on right image 

  
  
  
  
  
  
  
  
  
   
   
  

 a 

   

   

   

   

 b 

figure    a  estimated depth from features  b  laser range scan data
figure   shows the estimated depth on the left and the true depth on the right 

 a 

 b 

 c 

figure    a  base left image  b  potential matches from correlation  c right camera image with inlying matches 
crosses indicate corners on right image 
figure   shows a structured scene  here we see the effect of glass  causing the algorithm to give the depth of
a reflection in the top right  further  we see the error inherent in extremely regular textures 
for a selected set of structured images  for which a large proportion of the features exist in surfaces as
opposed to on the edge  this stereo algorithm has a true log depth to calculated log depth correlation of        
and a standard deviation of         for reference  a constant guess of the mean produces a standard deviation
of         due to the scale ambiguity in our range estimates  they were shifted to have the same mean 

 

conclusion

image manipulation  and   d reconstruction are not trivial  indeed  although some code existed  i needed to
code up a substantial amount of the process  particularily  there were numerous spurious matches that needed
to be pruned 
the stereo algorithm that resulted works  but would benefit from some fine tuning  and speed enhancements 
it is more accurate  but less informative for structured scenes 

 

ficomparing the calculated range data to laser range scans requires significant processing to ensure that the
limited ranges  and misalignments do not destroy the results  i could easily get near zeros correlations through
the right choice of scene 
clearly more work needs to be done  the to do list includes using the camera calibration to undistort the
image  adaptive distance limits for the guided matches  automatically selecting features insensitive to laser scanner camera misalignment for error measures  algorithm speed improvements  and of course complete integration
with monocular features 

 

aknowledgments

i would like to thank prof  ng for the loan of his book introductory techniques for   d computer vision by
trucco   verri  also  ashutosh saxena was quite generous with his time  further  i used a large number of
online resources  mainly the sample chapter from andrew zissermans multiple view geometry in computer
vision entitled epipolar geometry and the fundamental matrix 

 

fi