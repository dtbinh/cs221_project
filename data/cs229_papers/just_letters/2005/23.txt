                                                                        

machine learning for auto dynamic difficulty in
a   d space shooter
nick cooper
nacooper stanford edu
http   www stanford edu  nacooper

abstract
this project explores the idea of using online reinforcement learning to allow a   d space
shooter to adapt its levels based on a users playing style  the level building agent uses
knowledge about the recent actions of the player as input  and attempts to learn
parameters associated with generating enemies in the level  in hopes of maximizing a
players enjoyment of the game  as the players skill increases  the maximum value of
this function changes  and the agent must continue modifying its parameters in order to
maximize this changing function 

background
a major challenge in video game design is accommodating players of many different
skill levels  some games today do not even attempt this  they are either directed at
casual gamers  or highly skilled hardcore gamers  the problem with this is that either
type of game will probably not be enjoyable to the other crowd  hardcore gamers will be
bored by a game that is too easy  and casual gamers will not be able to pick up a new
game that is overly difficult  many games try to accommodate a wider range of players
by including a difficulty setting  which the player can manually adjust  however  there
are also some problems with this approach  while playing through a game  players will
naturally learn from their experiences  and become more skilled at the game  while
possibly developing their own unique strategies and techniques  and  while this is
happening  the difficulty of the game is remaining constant  and a single general strategy
of the player can be used repeatedly with success  even more importantly  players
themselves are often bad at judging their own skill level  and can easily become bored or
frustrated with a game based on a bad initial difficulty selection 
some developers have tried to solve these problems with the idea of auto dynamic
difficulty  for example  in the game max payne  the players performance in a given
level determines how strong the enemies will be in the next level  variables such as
enemy health and weapon strength are adjusted  in order to make the game easier or more
difficult for the player  depending on his her skill level  the health of the player and the
number of times the player died in the level are used to determine how to adjust the
difficulty 

game description
the game being used for this project is a   d space shooter called office wing  in which
the player controls a paper plane and battles various office supplies  the players
viewpoint is from the side  and all control of the plane is in   d  the paper plane is
continuously moving through the level  with the viewpoint scrolling to the right  enemies
appear in waves  and come from the right side of the screen  the goal of the player is to
progress as far as possible before losing all of his her lives  this can be done with or
without defeating enemies  although defeating enemies makes this task easier by

fieliminating the current threats to the player  enemies can be defeated by using several
types of projectile weapons that are available to the player  the player dies when hit by
an enemy or some other hazardous object  such as an enemy projectile  if the player has
extra lives remaining  the paper plane immediately returns to the screen and the game
continues  otherwise  the game is over and the player has lost 

differences from other reinforcement learning problems
several aspects of this problem make it somewhat different than other reinforcement
learning problems  first  the function we are trying to maximize is player enjoyment 
this is not only a difficult function to define  but it is also changing over time as the
player gains experience and skill  also  since we are trying to learn online at a fast
enough pace to keep a player interested  it is more desirable to make fast progress
towards an optima than to make slow progress and eventually converge  in addition  if we
are able to converge to the maximum of this function temporarily  it may not be a good
idea to stay at the optimum  even if the player is being challenged  it will not be fun to
face the same exact challenge repeatedly 

rewards  inputs  and parameters
player enjoyment is modeled as a relation between the number of close calls and player
deaths  close calls occur when an enemy or other hazardous object passes within a
defined area around the player  a high number of deaths indicate that the game is too
difficult for the player  and as a result the player is probably not having fun  a low
number of deaths and close calls indicate that the player is not being challenged  and may
be losing interest in the game  what we seek is a high number of close calls and a low
number of deaths  which would indicate that the player is being given a good challenge
without being overwhelmed 
the level building agent receives positive awards for close calls  and negative awards for
player deaths  the agent also automatically receives a negative reward for each wave it
sends at the player  which acts as a penalty for when there are very few close calls and no
deaths in a wave 
the features of the state that are observable by the level building agent are very few 
consisting only of values indicating the players total movement distance and most used
weapon over a recent time period  larger sets of features were considered  but testing
revealed that these two features were of far greater importance than the rest  both
parameters are based on recent or current time because a players skill level and playing
style are continuously changing over time  as a result  the agent should not take into
account the players actions that occurred a long time ago  as the player may have
developed new skills and strategies since then 
the level building agent attempts to learn and continue to update parameters in order to
keep the player enjoyment function at a maximum  these parameters are related to the
types  number  formations  and movement patterns of enemies to be sent out in a given
enemy wave 

reinforcement learning methods
the problem of learning the optimal action to take at a given state was posed as an mdp 
with    possible states corresponding to different weapon preferences and movement
amounts  the rewards and state transition probabilities were learned as gameplay

fiprogressed  and a slightly modified version of value iteration was used to learn the
optimal value function and policy 
the basis for this was the bellman equations 

 p  s  v  s 
r s  a     p  s v s 

v s    maxa r s  a    
 s 

  argmaxa

s sa



s sa

after an action completed  the level building agent received information about the
resulting state s and the reward r s  a  for the previous state and completed action  this
information was used to update the reward estimates  the state transition probabilities 
and then the value and policy at each state  learning a model with state transition
probabilities proved to be important  because it allowed the agent to predict the players
state resulting from an action  for example  if the player is primarily using the inkblob
weapon  it may be very likely that they will switch weapons if they encounter whiteout
enemies  who are resistant to the slowdown effects of the inkblob 
one of the most important modifications that allowed learning to occur online at a
reasonable speed was splitting up an action into four different components  without this
split  there would have been approximately       possible actions at each state  an
action was represented as a formation  a path type  a number of enemies  and one to three
enemy types  with each component having     possible values  each of these components
was treated as an individual action and optimized independently  for example  if the
level building agent executed the action  formation   linear  path type   vertical wave 
number of enemies      enemy type     enemy type     enemy type     pencil   then a
portion of the reward received would be associated with each of the linear  vertical wave 
  enemies  and pencil component actions  a policy is associated with each component 
and the overall policy takes the action components given by each component policy 
state transition probabilities were updated after each wave  with each p sa s  being
computed as 
psa s     times took we action a in state s and got to s
 times we took action a in state s
the reward updates were more complicated  let a be the action taken at state s 
r s  a     rold s  a        r observed s  a          

        

where r old s  a  is the previous estimate of the reward for taking action a at state s  and
robserved s  a  is the reward just observed for taking action a at state s  this is essentially a
weighted average of the previous reward estimate and the newly observed reward  this
takes into account the previously observed rewards and the most recently observed
reward for a state action pair  generally favoring the most recent reward  the value used
for   in this equation was determined after a great deal of experimentation 
this estimated reward update alone did not yield very desirable results  for example  if
the player had been using the spitball weapon for the entire game and suddenly switched
to the inkblob  suddenly the games difficulty would drop significantly  because its

fireward estimates for these unseen states were still at their initial values  this problem
was handled by also updating the reward estimates for unobserved states 
for each si  s
r si  a     rold si  a        r observed s  a          

        

each estimated rewards for action a under states other than s were updated very slightly 
as a result  situations such as the weapon switch described above did not radically
change the games difficulty 
yet another problem that resulted even with the above modifications was getting stuck at
local optima  at local optima  the agent would repeatedly send out identical or nearidentical waves of enemies  and receive a small positive reward each time  although
mathematically it is a good thing to be consistently receiving positive rewards  this can
start to irritate the player  because it fails to provide gameplay variety  this problem was
solved by slightly increasing the estimated reward for actions that have not been used
recently  because it is possible and likely that the players abilities have changed over
time  as a result  if the same action is used repeatedly  other actions will eventually
appear more favorable  and the agent will select a different action 
the first implementation of this learning algorithm involved only one wave being sent at
the player at a time  with the next wave appearing only after the previous wave was
finished  after this was working well  the game still never got quite difficult enough for
more skilled player  to fix this problem  the algorithm was extended to handle multiple
waves at once  this modification allowed the agent to modify the rate at which it sent
waves at the player  with this extension  rewards were distributed equally among all
currently active waves  a queue was used to store the action associated with each active
wave  as well as the state at which the action was executed  as waves finished  their
entries were popped from the queue  and used to update the model and value function as
in the original single wave case 

experimental results
in order to judge how well the algorithm was performing in adapting to the player  the
number of close calls and deaths for each wave was recorded for several players  an
overall high number of close calls  with a drop in close calls after a death would indicate
that the algorithm is performing well  in general  a high number of close calls is
desirable  but after the player dies  the game should become less challenging  so close
calls should drop 
the following plots show the numbers of
close calls and deaths for different
players  the first plot shows a player
who only played the game for    waves 
the players deaths occurred during two
parts of the game  during each of which
the number of close calls was very high 
indicating that the player was being
challenged  also  after each death the
difficulty dropped  as shown by the
decrease in close calls following each
death 

fithis next plot shows a player who
played for a somewhat longer time
period of    waves  again  each death
was shortly followed in a decrease in
difficulty  overall  the number of close
calls per wave was high  only dropping
to   during a small number of waves 

the final plot shows a more skilled
player  playing the game for a much
longer time period of    waves  after
about    waves of gameplay  the number
of close calls remains above zero for all
except for two waves  in most later
waves  the number of close calls
fluctuates around    as before  the
difficulty tends to drop after each death 
it is important to note that graphs are generally not a great measure of player enjoyment 
in addition to collecting numerical data from several players experience with the game  i
had ten people play the final version of the game for as long as they could survive  and
give me feedback about how fun they thought the game was  all players reported that
they found the game to be both very enjoyable and intense  and a few told me later on
that they were addicted  however  some players found that the game got difficult too
quickly during their first game  but in their second or third game after knew how to play 
they thought that the difficulty adapted well  the overly quick increase in difficulty in the
first game was probably caused by the automatic negative reward for each wave being
too large in magnitude  causing the game to get difficult a bit too fast  future testing can
be used to find a better value for this reward  or perhaps another learning algorithm can
be used to find better rewards associated with close calls  deaths  and waves over the
course of numerous games 
overall  i believe that this project was a great success  yielding a fun game that is
innovative both in terms of gameplay and in its application of machine learning 
countless extensions of this project are possible  including the use of this idea in more
complex types of games  for example  a bot in a first person shooter could learn that
the player is a camper  player who prefers to remain stationary while letting the
opponents come to them  and adjust its strategy to account for this  or  an ai opponent in
a real time strategy game can learn what types of units and attack patterns a particular
player has the most difficulty defending against  the ideas i have developed while
working on this project could potentially be used to help make games of various genres
more enjoyable and adequately challenging for all types of players 

fi