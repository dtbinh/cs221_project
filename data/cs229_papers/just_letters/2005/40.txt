optimizing document scoring for query retrieval
brent ellwein
baellwe cs stanford edu

abstract
the goal of this project was to automate the process
of tuning a document query engine  specifically  i
used machine learning to find an optimal scoring
function given term frequency  document
frequency  and query matching  the accuracy of
the results was measured as the greatest average
precision of the top ten scored documents 

background
this project considered   statistics when scoring a
documents relevance to a given query  these
statistics are common throughout information
retrieval 
 term frequency  tf 
a measure of the number of times that a given
token i appears in a document
 inverse document frequency  idf 
a measure of the inverse of the number of
documents that the token i appears in
 overlap  coord 
a measure of the number of query terms in the
document
for a typical information retrieval system  one
would not use the raw number  but would instead
use a scaling function of the statistic  one might
use square roots or logs to perform the scaling  the
total score is reported as a combination of the
individual terms  however  it is not generally
known what the best scoring function or
combination of functions is  in fact  the best
scoring function may be different depending on the
domain of the corpus and any artifacts of the
content  

experimental conditions
for training  testing  and the collection of results  i
used the cranfield collection  this corpus consists
of approximately      abstracts and     queries all
related to aeronautical engineering  each document
is scored relevant or not relevant for each of the
queries 

scoring is calculated as the scaled product of tf  idf 
and coord  after computing the score for each
document  the top    most relevant documents are
compared against the known relevancy values  the
precision is the ratio of relevant documents to total
number of documents returned      
these
precision scores are then averaged and the result is
the overall precision of the engine on the queries 
the objective is to maximize this overall precision 

learning environment
in order to find the optimal combination of scaling
functions  i used the following parameterization
function weights  each of the parameters i are
optimized by the learning algorithm 
 tf  as a function of frequency f 

       f      log  f       f     e f
 idf  as a function of document frequency df and
number of documents n 

    n  df        log n  df        n  df       df
 coord  as a function of term matches m and
number of query terms q 

       m  q      log m   q       m  q     m
 total score  product of the parameters 

   tf      idf      coord
each parameter was optimized according the k fold
cross validation for various values of k  it seems
obvious that the parameters         and    are
redundant  that is to say their values could in effect
be combined with the parameters from the groups
they correspond to   but early experiments showed
that they improved the convergence rate  later  as
normalized coordinate ascent was implemented 
their purpose was decoupled from the parameters
inside the groups and they became scaling factors
for each group as a whole 
the default parameters for the query engine  which
are typical default parameters used throughout the
field of information retrieval  are 

tf    log n   df              m   q
 

for example  when trying to avoid spam documents on
the internet  one might want to severely dampen  or even
limit the tf score 

in other words                        
all others would be set to zero 

fibounded coordinate ascent
the first algorithm i attempted to optimize the
parameters with was a bounded coordinate ascent
model  it is important to note that each training
iteration started with random values for each of the
parameters  i discovered that if i initialized the
values to the default values  that the algorithm was
unable to learn from the local optimum that the
values represent  in this model  each parameter is
optimized individually according to the newtonraphson method  at any given optimization step 
the values each theta is allowed to take are strictly
bounded by     and     the model was initialized
with random values and optimization was repeated
until convergence  the results of this optimization
were highly unsatisfactory compared against the
default configuration 
the following charts
summarize the precision results for a couple trial of
k fold validation  for these charts  training refers
to the queries used as the training set and test
refers to the queries held out of training and used
for validation  default values refer to the default
configuration mentioned above  learned refers to
the optimized parameters 
average precision  k    
   
    
   
    

experiments show that even when using the entire
dataset  the optimized classifier will typically under
perform the default classifier  an explanation for
the negative results is that training yielded a high
level of variation in the parameters across training
datasets  this indicates the presence of many local
optima that the algorithm was unable to recover
from  below is a table which summarizes the
fluctuations in the values for each theta 
standard deviation for each theta
  

  

 

 

 

 

 

 

 

 

 

 

 

k  

 

 

 

 

k     

  

        

     

  

  

k     

as can clearly be shown in the chart  nearly each
value for theta is fluctuating violently across each
of k trials of k fold cross validation  the high
degree of variation led me to believe that the
parameters were not converging to optima close to
the global maximum  but were too highly influence
by their initial random values 

   
    

normalized coordinate ascent

   
    
   
    
 
 

 

 

training default

 

 

 

training learned

 

 

test default

 

  

test learned

average precision  k    
   
   
   
   
   
   
 
 

 

training default

 

 

 

training learned

  

  

  

test default

  

  

test learned

it is easy to see from these graphs that the
optimized classifier generally performs worse than
the default classifier for both training and test
queries  the optimized classifier more closely
matches  and even out performs the default
classifier when k is large which suggests that a
large number of training examples is necessary in
order to properly optimize the parameters  further

the next logical step in order to increase the
precision of the optimized query engine was to add
restrictions on the values each parameter could
hold  as before  the values had to be initialized at
random  first  i tried restricting the value to be
bounded by   and     but that did not provide
noticeable improvement 
then  i added the
restriction that  after each optimization  all the
values be re normalized across categories so that
they would sum to    naturally  all the values
would then be restricted to the range     after
optimization  but the newton raphson steps would
still be allowed to explore beyond the   boundary 
while this yielded slightly better results  the results
were still on average only equal to the default
classifier  going another step further  i restricted
the parameters to sum to one only within specific
groupings  the groupings are enumerated as
follows 
 tf group
 


i   

i

  

fi idf group

average precision  k        norm alized 

  



i

    

  

i   

    

 coord group

    

  



i

  

    

i    

   

 total score group
 

  



i

 

  

 

 

training default

i    

i ran the training and testing examples again and
recorded that the fluctuations in the parameters
theta had noticeably reduced  while it is true that
the forced scaling is cause for some scaled
reduction  another thing to notice is that several
parameters almost never change  the following
chart  as before  summarizes the results 

 

 

 

training learned

 

 

test default

 

  

test learned

average precision  k        norm alized 
    
    
    
    
    

standard deviation for theta  normalized 

    

   

 
 

   

 

 

training default

   
   
   
 
 

 

 

 

 

 

k  

 

 

 

 

                       

k     

k     

while there is some variation in the data  this can
be explained by looking at the raw values for the
parameters  which are too numerous to include in
this report   i will provide an in depth discussion
for each parameter which will provide some
reasoning for the behavior of its optimized value 
having succeeded in reducing the variance of the
optimization  we can now look more closely at the
trial results for k fold cross validation  as before 
the following precision charts summarize the
behavior for each of k           
average precision  k       norm alized 
    
    
    
    
    
 
 

training default

 

 

training learned

 

test default

 

test learned

 

 

  

training learned

  

  

test default

  

  

test learned

notice that the precision is consistently better for
the learned parameters over the default settings 
this generally applies both to the training data  as
you would expect  and to the testing data as well 
one interesting item to notice is that in comparing
k    to k     we find a higher percentage of the
cross validation precisions to be less than the
default for the case of k     this suggests that the
learning algorithm is overtraining on the training
data and that the algorithm has too much variance
and not enough bias  looking for trends in the
data  i will suggest why this might be the so 

parameter optimization and
trends
the learning algorithm partitions the parameters
into   groups by their normalization  in this
discussion  it is convenient to think of the
parameters in these natural groupings 
 total score group
parameters        behave in one of   distinct
ways  either    is dominate  i e  has a value
between     and       or the weight is roughly
evenly distributed among the   parameters  for
the case where    is dominate  the difference in
test precisions between default and optimized
parameters tend to be larger than when the
weight is more evenly distributed  even though
the parameters are multiplied  and thus can be
treated as a single entity  arranging them in this
fashion allowed the learning algorithm to

fi 

recall that n is the number of documents in the corpus
while df is the number of documents which contain the
term of interest 

benchmarking
during the discussion of the tests  i have shown
that the optimization process can produce gains in
average precision over the top ten results  now  i
will move the discussion to include some other tests
to see if the optimized query parameters are really
better  or if the gains in precision came at too great
a cost 
 precision   recall
one metric common in ir is to plot a precisionrecall curve  comparing both the default and
optimized parameters  the following precisionrecall curves are produced 
 

precision

   
   
   
   
 
 

   

   

   

   

 

recall
default

learned

it is easy to tell from the graph that there is not a
noticeable difference between the two classifiers
when averaged over all the queries  generally  the
learned classifier has a slightly higher precision rate
as evidenced by the plot below  for the plot 
positive values indicate that the learned classifier
performed better  i e  the differences as calculated
as learned   default 
     

difference in precision

retrain this parameter multiple times 
experiment tests show that the learning algorithm
performs less effectively if the parameters are
collapsed into a single entity 
 tf group
for each trial the optimization of both   and  
was essentially    effectively  the algorithm has
provided experimental evidence to suggest that
one would not want to use an exponential or
constant term frequency  the other parameters in
this group     were given approximately equal
weights and grouped with apparent randomness 
that is to say  for one trial    and   might each
have     while   is   while on another trial      
and   might each have      on other trials 
either      or   might be   while the other  
zero  because there is not a strong correlation
between any combination and a more precise
result  the data suggests that any of the scaling
functions  liner  logarithmic or root  will have an
equal benefit on the overall query precision 
 idf group
the parameter   dominated this group  as is
evidenced by the low deviations for each element
in this group  ignoring the noisy case of k   
which is not enough training samples   the values
for each theta were essentially the same for each
set of training data  while the other values    
        were each zero  or very near zero   
was consistently at or very near    so  the
experimental data suggests that a good classifier
will use log n    df       as a scoring function
for idf  
 coord group
as evidenced by the low deviations for these
parameters  they nearly always optimized to the
same value  in the case of             that value
was always zero  the values of     and    were
coupled  either    was one and    zero  or they
both were a half  there is a high correlation
 about      of the case where             and
the test results being lower for the optimized
parameters than the default parameters  so  it
would seem that it is more advantageous  higher
bias  to have    equal   and the others set to
zero  functionally  this corresponds to only
taking into consideration the number of matches
between a query and a document and ignoring the
size of the document altogether 

     
     
 
 

   

   

   

   

 

      
      

recall

clearly  by measuring only the rate of precision at a
given recall level  there is not a significant
difference between the learned and default
classifier  while this result may not be terribly
impressive  it does show that the learned query
parameters have a similar bias and variance as the
default query parameters 
now  having shown that the learned parameters are
on average no worse than the default parameters 
lets take a closer look at precision in the top    

fisince we were trying to optimize the precision in
the top ten documents  we would expect to see
some performance increases for the top ten results 
the queries can be grouped into three groupings 
those where the precision was the same  those
where the learned performed better and those where
the default performed better 
 default was better     queries 
typically  the difference between default and
learned was only one  in fact the of the    queries
in this group  only one query had a difference
less than     many of the losses were from
documents moving from the  th or   th rank to a
slightly lower rank  only two queries which had
relevant results in the top ten for the default had
none for the learned parameters 
 learned was better     queries 
as was the case when the default was better  the
difference in this group was typically only   
however  there were two cases where the
precision was improved by    and   case where
the precision was improved by    moreover 
there were two cases where the number of
relevant documents increased from   to   
 precision was the same      queries 
for most of the queries  the precision in the top
ten was the same  the only differences in this
group were that the ranking of individual
documents was somewhat re ordered  in rare
cases  a relevant document was replaced with one
whose original rank was greater than    

learning rate
as mentioned previously  the default values
represent a stubborn maximum such that it is hard
to explore beyond it  to get around this feature  i
was forced to initialize the parameters for each
theta randomly for each training trial  fortunately 
the learning algorithm typically converged after
only   or   passes through that data  below is a
chart which summarizes a learning curve from one
of the examples 
learning curve
   
   
   
   
   
 
 

 

 

 

 

 

it e ra t io ns
classifier

default

 

conclusions
using the optimizations found by the learning
algorithm  one can expect to see more precise
results in the top ten queries about     of the time 
while experiencing less precise results about    of
the time  this net gain indicates that the learned
parameters provide some gain for the top ten
results  but as evidenced by the precision recall
figure  these gains do not accumulate over time 
beyond simply the gain in precision  there are
several other results which are worth noting  first 
the algorithm was successful in demonstrating that
some of the proposed features are bad candidates
and should not be used in an information retrieval
system 
one surprising result was that the
algorithm was unable to consistently select
parameters for the tf group  the data suggests
that using linear  logarithmic or root scaling of the
term frequency will have basically the same effect 
i had expected that the linear parameter would be
optimized to a very low value  another surprising
experimental result was that the default values
represented a stubborn local maximum  and if i
initialized using these values  the learning
algorithm could not recover from this maximum 

acknowledgements 
 dr  christopher manning  stanford  was helpful
by providing the cranfield data files as well as
suggesting candidate scaling functions to
parameterize 
 lucene  http   lucene apache org   was the
backbone of the project 
the open source
libraries allowed me to build custom indices and
run queries against them while training 

fi