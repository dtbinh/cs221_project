cs    project final report
december         

handwritten digit recognition  investigation and improvement
of the inferred motor program algorithm 
john a  conleya   my phuong leb

 

introduction

handwritten digit recognition is a classic problem in machine learning  it has been studied
intensely by many authors for decades  the existence of standardized  publicly available
mnist training and test data     has led to extensive benchmarking of varied approaches to
the problem  the methods of approach and their error rates are compiled at      the best
performance thus far      acheived using a convolutional neural net with cross entropy and
elastic distortions of the training set  is      error on the test set 
the wealth of research that has gone into the digit recognition problem makes it difficult
to improve on the best accuracy so far acheived  it is interesting  however  to understand the
workings of one existing approach to the problem and to apply different ideas from machine
learning to improve on this approach 
we chose to investigate the inferred motor program method of hinton and nair      this
approach uses a generative model consisting of two sets of opposing springs whose stiffnesses
are controlled by a motor program  neural networks are then trained to infer the motor
programs required to reconstruct the handwritten digits 
our aim is to understand the approach in     and to identify improvements to it  to do
the latter  we will implement a version of the inferred motor program algorithm in matlab
and implement modified versions  we will compare the performance of both on the mnist
database 

 

the inferred motor program algorithm

the generative model in     controls a pen with two pairs of opposing springs at right
angles  the stiffness of each spring is changed at    discrete time steps according to a motor
program  with appropriate choices of these    stiffnesses  the pen can follow the trajectory of
a handwritten digit  given a pen trajectory  a realistic image can be created by interpolating
ink between the trajectory points and convolving with a kernel which depends on two ink
parameters  the    stiffnesses and two ink parameters make up the    parameters of the
generative model 


e mails  a conley stanford edu and b myphle stanford edu

fiwith the generative model specified  the next step is to train a neural network  the reconstruction nn  to infer the motor program parameters from the image of a digit  there
will be a separate reconstruction nn for each digit class  there are in fact    reconstruction nns  since performance is improved by using additional networks for dashed ones and
sevens   each reconstruction nn takes in the pixel intensities of an image as its input  and
uses     hidden units and    logistic output units to return the inferred motor program
parameters  because the motor programs used to generate the mnist training images are
not known  the reconstruction nn must generate its own training set of images with known
motor programs  to do this  each reconstruction nn is initialized with a set of biases and
very small weights such that when given an mnist image  it is guaranteed to output a motor program that will generate a prototypical digit image of the class  this motor program
is fluctuated and from it  an image is generated  this image motor program pair is used
as input and output to train the reconstruction nn using backpropagation  this process is
repeated  typically in batches  with every mnist training image in that class 
once trained  the reconstruction networks can be used for recognition as follows  given
a mnist test set image  the reconstruction nn from each class can be used to infer a motor
program  each motor program can be scored using     the sum of squared pixel error of
the reconstructed image with the test image      the squared distance between the pixel
error and its projection into the pca hyperplane of that class      a similar pca distance 
but for the pen trajectory  not the pixel error  the scores are input to a classifier nn
with    softmax output units and a cross entropy error  the authors also implement an
additional local search step that improves the test set error  this step uses a neural network
to emulate the generative model  so that pixel error can be backpropagated to update the
motor program 
the authors also identify a simpler and more efficient way to make use of the information
provided by the reconstruction nns  by applying the appropriate reconstruction nn a
motor program can be associated with each image in the training set  then a single neural
network  the recognition nn  can be trained to output both the class label and the motor
program for an image  the idea is that the additional output  the motor programs  acts as
an informative regularizer which improves recognition performance  because this method 
of the ones the authors implemented  gave the best results  we chose to investigate it rather
than the other classificatiers 

 

building the prototypes

to create a prototype motor program for each digit class  we began with a typical image
from the class  we selected points roughly evenly along the skeleton of the digit  in the time
order they would have been written  the location of these trajectory points is not enough to
specify the four stiffnesses at each time step  we put in an arbitrary constraintthat at each
time step  one of the vertical stiffnesses and one of the horizontal stiffnesses equals      the
value     was chosen because it lies at the point of steepest slope of the sigmoid function 
which is the most efficient point for backpropagation training 

 

fifigure    training of a recognition network 

 

training the reconstruction nns

to generate an image from a motor program  we attempted to flesh out the pen trajectory
by spacing gaussian beads of ink along it  this did not reproduce the mnist images as
accurately as the ink kernel in      so we used his  and added a step to recenter the image
by its center of mass  since the mnist images are centered in this way  
our reconstruction nns have two layers with     hidden units  we trained them as
described above  using batches of     mnist training images and running backprop for   
epochs before updating the weights and using the next batch  we used momentum of     
and a search then converge learning rate annealing schedule  for each digit  we ran through
the whole training set     times  the training is detailed in fig    
some mnist images and their reconstructions are shown in fig     the majority of
the images are well reconstructed  the dashed one and seven that are shown in the figure
are poorly reconstructed  however  the solution to this is to train a separate network for
dashed ones and sevens  as discussed in      getting extremely accurate reconstructions 
however  is not crucial for training the recognition nn  this is because the reconstructed
motor programs are used for training  but not the reconstructed images  thus we chose not
to implement separate reconstruction nns for dashed ones and sevens 

 

the recognition nn  modifications and results

hinton et  al  use a   layer recognition nn with     hidden units in the first layer and     
in the second  training this using the motor programs as additional output  as described
above  they acheive a       error rate on the test set  for time and memory considerations 
 

fifigure    reconstruction of mnist images  for each digit class  the lower row contains the
original mnist images  and the upper row contains the reconstructions of these images by
the reconstruction nns 

we chose a simpler architecture  a   layer nn with     hidden units  and trained it the
same way using the four spring stiffnesses motor programs as additional outputs  we will
refer to this as the four spring recognition nn  because of the relative simplicity of the
architecture  we cannot directly compare our results to those acheived in      instead we use
this architecture consistently in order to compare the effects of the modifications described
below 
the inferred motor program algorithm is motivated by the idea that a realistic generative
model should provide the most natural way to classify patterns  the four spring model is
physically plausible but there is degneracy in its parameters  there is a one to one mapping
between sets of four spring stiffnesses and pairs of x  y accelerations  thus we can replace
the    spring stiffnesses with    accelerations and two initial displacements  this replacement can be implemented either in the reconstruction nns  the recognition nn  or both 
due to time constraints we chose to only implement and evaluate this modification for the
recognition nn  we will refer to this as the two acceleration recognition nn  this reduction
of parameters has two main benefits  first  for a given number of hidden units  the number
of weights is reduced  and thus training should be faster  we found that the training time
for the two acceleration nn was     less than that of the four spring nn  second  since the
output space is smaller  for a given number of hidden units the network should be able to
model the variation better  this claim is supported by our results  below 
to evaluate the benefit of the additional outputs  either spring stiffnesses or accelerations 
we implemented  for comparison  a recognition nn of the same architecture that we trained
using only the class labels  the results of all three cases are shown in table   
it is surprising that the labels only nn outperforms the nns with additional outputs 
one possibility is that each reconstruction nn still does not model the mnist digits of
 

ficase
labels only
four spring
two acceleration

training set error test set error
  
     
      
     
      
     

table    results  the same learning rate and number of iterations were used in each case 
its class very well  thus the motor programs associated with the training images may be
confusing the recognition nn rather than informing it  the fact that we have not implemented separate reconstruction nns for dashed sevens and ones could be contributing to this
problem  another possibility is suggested by the following observation  the cross entropy
error during training was lowest for the labels only case  higher for the two acceleration case 
and still higher for the four spring case  this implies that the learning rate and number of
backprop iterations we used to train in all three cases was more optimal for the labels only
case  by using a validation set to optimize training in each case  a fairer comparison could
be obtained 

 

conclusions

we have demonstrated the feasibility of reducing the complexity of the generative model
used in      much more work  however  needs to be done to validate the claim that this
can lead to an improvement in classification  the main idea of hinton et  al   that a a
generative model can be used both to generate more training data and to associate useful
extra information with training examples to improve classification  can be applied to other
problems in pattern recognition 

acknowledgments
we would like to thank geoffrey hinton and vinod nair for useful discussions 

references
    y  lecunn 
c  cortes 
the
http   yann lecun com exdb mnist  

mnist

database

of

handwritten

digits 

    g  hinton  v  nair  inferring motor programs from images of handwritten digits  to be published in nips       available at http   www cs toronto edu  hinton absps vnips pdf 
    p  y  simard  d  steinkraus  j  platt  best practice for convolutional neural networks
applied to visual document analysis  international conference on document analysis
and recogntion  icdar   ieee computer society  los alamitos  pp                
 

fi