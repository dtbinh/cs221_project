cs    project report  ner adaptation
pi chuan chang
 

surabhi gupta

ner approaches  memm

maximum entropy markov model  memm   mccallum et al         is a kind of sequential model  which
models the likelihood by a sequence of states given
markov assumption  instead of using state transition
probability and observation probability given a specific
state in hmm  memm models the transition probability
of the current state given both previous state and observation 
p  s o   
 

n
x
i  
n
x

p  si  s     si    o 
p  si  si    o 

i  

for each transition probability  memm uses maximum entropy model  maximum entropy model is used
to model the data distribution  which gives us as much
information as possible  without any constraints  the
uniform distribution gives us maximum entropy  however  we have training data which gives some facts about
the true distribution  what maximum entropy model
does is to maximize the entropy of the data give such
constraints coming from the training data  the constraints are that the expected value of each feature in the
distribution is the same as its actual count in the training
data  i e 
n
x

fa  oti   sti    

i  

n x
x

p  s si    oti  fa  oti   s 

i   ss

 where t     tn are the time index such that sti   si   
also  the features are binary indicator function of both
current observation and a possible new current state  it
can be shown that the entropy model given those constraints has exponential form 
p  si  si    o   

x
 
exp 
a fa  o  si   
z o  si   
a

which is the same as the distribution which maximizes
likelihood 

yun hsuan sung

       there are three named entity classes we are interested in  location  organization  and person    other than these  there is also a background symbol used to mark non named entity words  the basic
training set  conll train is part of the reuters corpus 
and there are          words  the conll      shared
task defined a development set  conll testa  and a final
test set conll testb   for now  the experiments weve
done are using memm classifier 
another data set we use is the email datasets  email
datasets includes   datasets  enronmeetings  enronrandom and newsgroups  in our experiments  the performance on enronrandom sets is the highest  the reason
that results on enronrandom are better than enronmeetings is because the enronrandom training data        
words  is more than enronmeetings training data       
words   however  even though newsgroups training
data has        words  the result on the newsgroups
data is the worst in   sets  it is because the data in newsgroups is very diverse  there are even a lot of codes inside 
times of india data  we wanted some data that had
completely different names from american news in order to test for cotraining  section     we went to the
times of india website and downloaded the documents 
we hand annotated    documents with information of
person  location and organization  this data was then
converted to the conll format for use in our experiments 

  how to do adaptation on memm
adaptation is to train a model in one domain which
has enough training data  then  instead of retraining a
new model in the new domain  we adapt the existing
model to fit the new domain by using a small amount
of training data   chelba and acero        used memm
for the task of capitalizing text by using map adaptation
with a prior distribution on the model parameters chen
and rosenfeld         we use the same technique to do
adaptation in named entity recognition 
apply map adaptation with a prior distribution to
memm can be formulated as 
x
l    
p x  y  log p  y x  
x y

 

data sets

we used the datasets and evaluation scripts developed
for the conll ner english task  sang and meulder 



f
x
 i      
i  

 

 i 

i

  c  

in conll      shared task  there was a fourth class  misc 

fiwhere  i   i       f are the parameters trained in old
domain     i  i       f are the parameters adapted by
new domain data 
in this maximum log likelihood formula  we add the
second term in right hand side to compensate the problem of insufficient data in new domain  instead of retraining the new parameters  we consider both the likelihood for new parameters and the distance between new
parameters and old parameters  then we optimize their
sum  the second term can also be explained as adding
some penality if the new parameters are too far from old
parameters  to estimate the parameters in our memm 
we used limited memory quasi newton bfgs based on
the algorithms in jorge norcedals numerical optimization book nocedal        
the variance of the prior terms can also be tuned 
when the variance becomes large  the regularization
power of the prior becomes weaker  a more fine grained
adaptation is to adjust the covariance matrix as a diagonal matrix  where the diagonal terms are the variance of
each corresponding feature  but in memm  the features
space is sparse so it is not easy to compute the variance
of each of the features 

 

cotraining with the internal and external
views

cotraining can be used when there exist two separate
views of the data  like two disjoint sets  blum and
mitchell         the goal of cotraining is to use these
two different views to leverage the results on unlabeled
data  in the case of our data  we have two views   the
internal view and the external view  internal view can be
thought of as the local features  the specific names used  
and the external as the global features  such as the structure of the content   when we compare the say the times
of india and american news content  the external view
is similar  but the internal views are different  similarly
when comparing say email and news data  the internal
views are the same but the external views are different 
one strategy that has been applied by blum and
mitchell  is to train two separate learning algorithms separately on each view  and then each of the algorithms
predictions on the new unlabeled examples are used to
enlarge the training set of the other  one motivation for
this approach is that it is easier to get unlabeled data than
labeled data  in another strategy  sarkar sarkar       
mentions that training a statistical parser on combined
labeled and unlabeled data strongly outperforms training on only the labeled data  nigam  ghani  nigam
and ghani        demonstrated that when learning from
labeled and unlabeled data  algorithms can leverage an
natural independent split of features if one exists or cre 

ate a split  and this outperforms algorithms that do not 
here we present a split of the features  table  
presents a split of the features that we have used  the notation used is as follows  s    shape of current word  s 
  shape of previous word  s    shape of next word  w 
  shape of current word  w    shape of previous word 
w    shape of next word 
algorithm used as adapted from  nigam and ghani 
      is described in figure   

  random forest
random forest  breiman and cutler    is a set of decision trees  to classify a new piece of data  each decision
tree make its own classification  the final decision is
made by a vote between the decision trees and the forest chooses the label which has the most votes  it was
shown in the classification that the error rate depends on
the following 
   the correlation between any two trees in the forest 
increasing the correlation increases the error rate 
   the strength of each individual tree  increasing the
strength of each tree reduces the error rate 
instead of doing adaptation to get a better classifier
in new domain  we increase the robustness of classifier
by building a random forest  in our setting  each tree
in the random forest is represented by a memm classifier  we separate our all features into    different classes
by their properties  each memm classifier is randomly
assigned some of classes of features  unlike voting between classifiers  we add the probability of each classifier and choose the class which gets the highest probability score 
class   argmax

x

c

pi  c o 

i

the unbalanced number between the background
class and named entity class causes the classifier to label
the data as background class  this unbias effect became
more obvious when we combined the probability score
between classifiers  in order to overcome this unbias effect  we introduced a penalty threshold to background
class  we label the background class until the score of
background is larger than that of named entity 

  experimental results
    baseline experiments  matched cases  training
and testing data from same domain 
we have done experiments of matched environments for
training and testing  there are three data sets in the
email data  see table      

fiinternal
w 
prefix and suffix of w 
s 

external
w 
w 
s    s    w  s    w  s 

co occurrence in a window
useprevsequences and usesequences   pseq  
usetypeysequence   shape tps   
uselongsequence   ppseq  

uselastrealword
usenextrealword
usedisjunctive
usetypeysequences   tns   
uselongsequence   ppseq  

ambiguous
w     w 
s  s    s  s    s  s  s 
usetypeysequence
  tps in cpc   ttps incpcp c   

table    features separated as internal and external
trainingdataa  trainingdatab   training data
while there exist documents without class labels
 
build classifier a using the first split of features and the training
data
build classifier b using the second split of features and the training
data
go through all the unlabelled documents
 
document d   one which a is most confident about
add d to trainingdataa
remove d from unlabelled data
 
repeat for classifier b
 
testing 
use predictions of classifier a and b by multiplying together their
score on the test data 

figure    co training algorithm
train 
test 
conlleval
per

conll train
conll testa
p
r
           

    map adaptation
f
     

table    baseline results on the conll data
train 
test 
datasets
enronrandom
enronmeetings
newsgroups

per
per
per

email train
email test
p
r
           
           
           

f
     
     
     

table    results on the email data
    mismatched cases
to show how the ner performance degrades when moving to a different testing environment  see table   
train 
test 
datasets
enronrandom
enronmeetings
newsgroups

conll train   class
email test
p
r
f
per                  
per                  
per                  

table    mismatched  train on conll  test on email
datasets

applying the adaptation approach is described in section    with all the training data of each email data set
as adaptation data  we get the result in table    one
question is that if we have so many adaptation data 
why cant we just combine it with the conll training
data and train a classifier  we also tried that  table    
and the result shows that doing map adaptation gives
better performance  the map adaptation performance
is also better than that in table   
train 
test 
adapt 
datasets
enronrandom
enronmeetings
newsgroups

conll train    class 
email test
email train
p
r
f
per                  
per                  
per                  

table    using all the email training data as adaptation
data
instead of using all the enronmeetings train data as
adaptation data  we also tried to increase the amount of
adaptation data from   documents to all of the     documents  we plot the f score curve as the number of documents increases   figure    in this figure  when we put
in    or    documents as adaptation data  the performance dropped  since we have features that can decide
  to   labels at the same time  too few adaptation data

ficonll train   class    email train
email test
p
r
f
per            
     
per            
     
per            
     

precision

recall

fb 

fb  baseline

     
     
     

table    combining conll train and email training set as
training data

accuracy

train 
test 
datasets
enronrandom
enronmeetings
newsgroups

     
     
     
     

f scores

conlleval f score

     

     
     
     
     
     
     
     
     
     
    

   

   

   

   

   

   

   

   

   

   

    

threshold

figure    enronmeetings with    classifiers and   
classes of features
conll

enronmeetings

ernonrandom

newsgroups

     

 

   

   

   

   
     

  of adaptation documents

     

f score

     

figure    adaptation f score to the number of adaptation documents

     
     
     

might just hurt the performance  but the performance
improves after    documents  and when the number of
adaptation data reaches      about        words   the
performance is reasonably good  when using all the    
documents to adapt  the map adaptation outperforms
throwing in everything as training data or training on
only enronmeetings train  in fact  using map adaptation as in table   gives us the best performance we have
on this dataset 
    cotraining
we used conll as our training data and did two sets of
experiments  in the first case  we tested on times of india data and in the second case we tested on the enronmeetings data  see table    in each case the unlabeled
data we used is from the same domain as the test data 
according to these results it seems that cotraining did
worse than the baseline of the memm classifier without
the use of any unlabeled documents  we have some intuition about why this could be the case which we have
described in section   

toi baseline
toi cotraining
enron baseline
enron cotraining

overall
     
     
     
     

person
     
     
     
     

location
     
     

organization
     
     

table    f scores  train on conll train  test on enron 
toi data

     
     
     
  

  

  

  

number of features  classes 

figure    best result on enronmeetings with    classifiers
    random forest
in these sets of experiments  we used conll training set as our training data and tested on conll test set 
enronmeetings  enronrandom  and newsgroups  we
trained our random forest with         and    memm
classifiers  we also adjusted each memm classifiers
with randomly chosen             and    classes of features  in the final decision part  we tried different penalty
threshold  from         to the background class 
in figure    when change the penalty threshold from
    to      f  score increases  achieves maximum around
     then decrease  and the maximum f  score is
        which improves around     in figure    using more classes of features in random forest decreases
the f  score in all the test set  besides  we have maximum f  score                 and        by using
   classes of features in conll  enronmeetings  and
enronrandom  respectively  for newsgroups  we get
maximum f  score        by using    classes of features

fienron baseline
enron cotraining

overall
     
     

person
     
     

table    f scores  train on conll train  test on enron data

 

analysis

    map adaptation
map adaptation on the mean of the gaussian gives us
better performance on each of the   email data sets compared to only training on conll data 
    co training
co training does not seem to work well for our task  we
manually created a split of features into internal and external  it seems that does not work with the current split
of features  when we trained our memm classifier on
only one set of features  then co training outperformed
the experiments without cotraining as in table    this
is something that we would like to further explore in the
future 
    random forest
using random forest in ner increases the robustness
a little bit in some new domain  we get around      
gain in both enronmeetings and enronrandom  however  we get worse result in newsgroups and also compensate the performance in original domain 
in figure    the precision doesnt monotonically decrease and the recall doesnt monotonically increase
when we increase the penalty to background class  the
reason is that in the standard conll evaluation  it adds
positions  b and i to the label  which means beginning
and intermediate  the result is correct only when both
the label and position are correct  although increasing
the threshold  we tends to label more aggressively to
non background class  it is still possible to get wrong
position then get wrong result 

 

acknowledgments

we would like to thank prof  chris manning for his help
with this project  we consulted him for most of our issues with our implementation on named entity recognition and he gave us valuable feedback  we would also
like to thank prof andrew ng whom we consulted for
specific machine learning algorithms  we learnt a lot
from the lecture on debugging learning algorithms 

 

future work

we will try out the cotraining algorithm with a different
split of features  because our results showed the current
features split actually hurt the performance too much 

we will also try to use crf classifier instead of memm
classifier to see if we can get more improvements  also 
we can include features from parser as a knowledge of
recognizing the named entities  and were also interested in using unlabelled data to get useful features of
ner adaptation 

references
avrim blum and tom mitchell        combining labeled
and unlabeled data with co training  in colt  proceedings of the workshop on computational learning theory 
morgan kaufmann publishers  pages       
leo breiman and adele cutler 
random forest 
www stat berkeley edu users breiman randomforests 
ciprian  chelba and alex acero        adaptation of maximum entropy capitalizer  little data can help a lot  in
proccedings of emnlp 
stanley f  chen and ronald rosenfeld        a survey of
smoothing techniques for me models  in ieee transactions on speech and audio processing 
andrew mccallum  dayne freitag  and fernando pereira 
      maximum entropy markov models for information
extraction and segmentation  in proc    th international
conf  on machine learning  pages         morgan kaufmann  san francisco  ca 
kamal nigam and rayid ghani        analyzing the effectiveness and applicability of co training  in cikm  pages
     
jorge nocedal        numerical optimization  springerverlag new york 
eric f  tjong kim sang and fien de meulder        introduction to the conll      shared task  languageindependent named entity recognition  in proceedings of
the      conference on computational natural language
learning 
anoop sarkar        applying co training methods to statistical parsing  in proceedings of the  nd meeting of the
north american association for computational linguistics  naacl       pages        

fi