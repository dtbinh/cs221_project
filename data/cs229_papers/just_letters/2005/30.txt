applying dual tree kde to gene interaction network integration
melroy saldanha
chester shiu
cs   
abstract
in high dimensional spaces a nave approach to calculating joint
probability densities is computationally intractable  o mn  where m     training
points  n     query points   we have implemented a variant of a nonparametric
machine learning algorithm  tree based kernel density estimation  to solve this
problem  we will refer to this implementation as dual tree  we have found our
implementation to be very efficient over large datasets  o sqrt mn    but it does
not scale as well in high dimensions  while dual tree would be quite useful for
computational statisticians and data miners  our particular motivation comes from
a problem in bioinformatics  merging protein interaction networks 
introduction
a number of experimental techniques have generated evidence about
gene gene and gene protein interactions  this data has been used to create
hypothetical interaction networks that are of great biological interest  however 
the networks tend to be derived from a single type of data source and do not
uniformly cover the same sets of genes and or proteins  it would be highly useful
if there was a robust algorithm to integrate the various networks into one
coherent network  complete with the probability of each potential interaction 
serafim batzoglou s lab has developed such an algorithm     but it is limited by its
ability to calculate joint probability densities over large datasets with large
dimensions  thus  the application of dual tree will address the algorithm s
weakest point  the resulting upgraded algorithm will represent a major
advancement in the field of protein network integration  particularly in accuracy
and scalability  
the most nave approach to kernel density estimation is to sum up the
prior probability of each query point with every data point in the training set  for
m queries and n training examples this yields o mn   which is too slow for most
interesting problems  one commonly used approximation is to divide the
hypothesis space into bins  treat the center of each bin as a point  and multiply
the resulting answer by the number of points in that bin  unfortunately this is a
poor approximation for large n 

approach
a much better implementation is dual tree     we can speed things up
considerably without loosing too much accuracy if we take into account the
structure of the data  first  we assume that each point is fitted with its own
gaussian with its own parameters  hence the non parametric aspect of the

fialgorithm  the final distribution across the entire set could look like anything  
next  we observe that gaussians decay exponentially  so only points close to the
query point will make non infinitesimal contributions to the prior probability sums 
thus  with some  calculable and bounded  error  we can surround each query
point with a bounding box and consider only those training points in the bounding
box  to efficiently implement this bounding box  we can keep track of the lower
extreme and upper extreme vertex of the bounding box for each node in the
mrkd tree as one of its sufficient statistics       if we know all of the query points
before runtime  we can achieve even greater speedup by fitting the query points
to another mrkd tree  the comparison would then be between the nodes on each
tree instead of a query point to a node  in essence  dual tree is simply building
these two trees and doing a iterative deepening traversal of each tree  this takes
o sqrt mn  time 
building a kd tree  suppose we are given n points in rk  to organize them we
will build a kd tree  there are many ways to do this  this is the procedure we
used 
 a  sort all n points along each of the k dimensions to build an auxiliary n
by k matrix of sorted indices  this takes ko n log n   using quicksort 
 b  for each of the k dimensions  calculate the range ri   xi max  xi min 
where i ranges over each dimension from   to k 
 c  partition along the dimension with the greatest range  partition this
dimension at the median  keeping half in one  n   by k  submatrix and half
in another submatrix of equal size 
 d  repeat this for each submatrix until we have at most nmin points in
each submatrix  where nmin    and is the minimum particle count for each
node of the kd tree 
caching sufficient statistics  we will cacherthe mean   the covariance matrix  
r
and the coordinates of the bounding box  l   u   of the points contained in that
r
r
node leaf  where l is the point in the box closest to            and u is the
point in the box closest to             after building the initial kd tree we can do
post traversal on it and calculate each parents statistics in terms of its children 
 a  to calculate  for a parent with two children that contain n  and n  of
their own children 
n    n    
     
n    n  
 b   can be calculated similarly 

fi c  the new bounding box can be found by
r
r
r r
r
r
 l   u      min  l child     l child     max  u child     u child     
where min and max are along each dimension 
bounding box comparison  to determine whether a training node makes a
significant contribution to a query node  we must determine the shortest distance
between their respective bounding boxes  the keyr observation
here is that each
r r r
dimension can be minimized interpedently  so for l    l    u    u   to find the shortest
r r r
r r
r
r
r
distance in dimension x  we find min  l x  l   x   l x  u   x   u x  l   x   u x  u   x    hence 
to find the minimum distance between two bounding boxes can be done in o k 
time  where k is the dimensionality of the data 
estimate maximum contribution  once we know the minimum distance between
the bounding boxes of two nodes  we can calculate the maximum contribution by
applying the gaussian kernel to that minimum distance and multiplying by the
number of training points in the box  in other words  the maximum possible
contribution of those training points to the density occurs if all the training points
were crowded in one area as close as possible to the query points 
dual tree algorithm  the algorithm runs an iterative deepening approach in
which for every level  i  in the query tree and for every level  j  in the training tree 
we compare every query node found at that level  i  with every node found at
level  j  in the training tree  when comparing two individual nodes  we estimate
the maximum contribution as described above  and if that contribution is less
than some tolerance   we prune away the training node  and implicitly its
children  and subtract the maximum contribution from   once      we can no
longer prune nodes without increasing error so we explicitly calculate all
probabilities from then on  note that  is inherited from the parent  but the two
children can ultimately use up different amounts of  depending on what training
nodes they prune  for the other case  where we have found the two nodes to be
close enough  we just repeat the comparison for the query node with the children
of that training node  once we have reached the leaves  we directly calculate the
densities between each point in the query node with each point in the training
node  using a gaussian kernel  since we have managed to prune away distant
training points that contribute little to the density  we have considerably reduced
our calculations and thereby greatly improved the overall running time  note that
even with a small  tolerance          on average many nodes are pruned 
results
as can be seen on the graph that follows  dual tree does not do well in
high dimensional spaces  the problem is that for really high dimensional spaces
most of the complexity comes from the number of dimensions rather than the
number of points  so pruning away points does not result in as much speedup 

fihowever  when dimensionality and the number of training points is held constant 
dual tree is considerably faster than a nave o mn  approach and retains that
edge even for large m 

the most curious result is that as n increases  dual tree performs worse
asymptotically than a nave approach  there is still a speedup for n        or so 
but after that dual tree definitely performs more slowly  this was re verified on
another dataset with      training points  this suggests an inefficiency in the
way that we are traversing and pruning the training tree  there might be issues
with memory garbage collection at the interface between our c   code and the r

fiinterface that invokes it  we are continuing to revise our code to improve
efficiency and are confident that observed asymptotic performance can be
improved 

future work will consist primarily of optimizing the performance of the
code and experimenting with other pruning methods 
acknowledgements
we collaborated closely with balaji subbaraman srinivasan in serafim
batzoglou s lab on this project  it should be noted that implementations of dualtree do exist  but their performance leaves something to be desired  especially
for large datasets  hence our efforts to rewrite the algorithm in c   and expose
an r interface so that it can be used as a general purpose tool  and serafims lab
in particular in this case  
 

srinivasan bs  novak af  flannick ja  batzoglou s  and mcadams hh  integrated
protein interaction networks for    microbes  in submission 
 

a gray and a moore  rapid evaluation of multiple density models  aistats       

 

j h  friedman  j l  bentley  and r a  finkel  an algorithm for finding best matches in
logarithmic expected time  acm transactions on mathematical software               
september      
 

k  deng and a w  moore  multiresolution instance based learning  in proceedings of
the twelfth international joint conference on artificial intelligence  pages           
san francisco      

fi