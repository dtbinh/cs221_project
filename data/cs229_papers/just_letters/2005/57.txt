multicore learning algorithm
cheng tao chu  yi an lin  yuanyuan yu

   summary
the focus of our term project is to apply the map reduce principle to a variety of machine learning
algorithms that are computationally expensive  instead of using expensive computer clusters  we focus
on implementing the framework on multi core computer environment  on top of that  in order to apply
the framework to a variety of modern machine learning algorithms  we focus on parallelize the summation
operation in the algorithms by developing customized mappers and reducers  in the following sections  we
will show the simplified architecture in section    the implemented algorithms and the formula where the
framework is applied in section    and finally the experiments and its result disccision in section   
   architecture
there is a list of well known machine learning algorithms which require a huge amount of independent
training examples  to expedite the training process  we will implement a framework which distributes the
training operation  majority of which is actually summation  to different threads processes  the original
map reduce paper     from google has outlined a full blown framework that is specialized in this type of divide
and conquer works  using the same set of principle from      our goal is to implement a relatively lightweight
architecture that focuses on multi core environment and provides a flexible interface  the interface should
be easily adapted to different learning algorithms  while achieving decent boost in efficiency  i e  we hope to
witness a n fold boost in a n core environment  
figure   shows a high level view of the architecture and how it processes the data  in step    the mapreduce engine is responsible for splitting the design matrix by samples  rows   the engine then caches the
splitted data for the following map reduce invokes  every algorithm has its own engine instance  and every
map reduce task will be delegated to the engine  step     similar to the original map reduce architecture 
the engine will run a master  step      which coordinates the mappers and the reducers  the master is
responsible for assigning the splitted data to different mappers  and then collects the processed intermediate
data from the mappers  step       and         right after the intermediate data is collected  the master
will in turn invoke the reducer to process them  step         please notice that some mapper and reducer
       

algorithm
       

data

   run

 
   data input

         query info

engine
     run

   

       reduce

master
       intermediate data

mapper

reducer

       map  split data 

mapper

mapper

mapper

         query info

figure    adopted map reduce framework
 

       result

fi 

cheng tao chu  yi an lin  yuanyuan yu

operations requires additional information from the algorithm  in order to support these operations  the
algorithm can pass these information through the query info interface  step         and          
   adopted algorithms
as mentioned in section    the framework is applied to parallelize operations in the algorithms  currently 
locally weighted linear regression  naive bayes  gaussian discriminative analysis  k means  logistic
regression  neural network  em  independent component analysis  principal component analysis  are
implemented in the system and the detailed description of where the operations parallelized are shown
below 
 locally weighted linear regression
pm
the lwlr
    problem can be solved by using normal equation a   b where a   i   wi  xi xti  
pm
and b  
i   wi  xi yi    in summation form  we divide the process among different mappers and
customize two sets of mappers reducers for computing a and b respectively  since we know that a
is a feature by feature matrix and b is a feature by one matrix  as long as the amount of samples is
significantly larger than the number of features  the framework would enhance the performance by
the parallelization 
 naive bayes
in the naive bayes algorithm      we have to calculate the parameters xj  k y     xj  k y   and y
from the training data  in order to find the s  we need to sum over xj   k and y from training
data to calculate p  x y   as in the summation form  we can divide the labor using our map reduce
framework among multi processors to expedite the summation process  the reducer then sums up all
the intermediate results and get final maximum likelihood for prediction 
 gaussian discriminative analysis classic gda     algorithm needs to learn the following four
parameters         and   for all the summation form involved in these computation  we may
leverage the map reduce framework to parallerize the process  each mapper will handle the summation
 i e    yi        yi        yi      xi   etc  for part of the training samples  that is  suppose we split
the training set into n pieces  the framework will typically  though not necessarily  launch n mappers 
each of which handles one piece of the data  finally  the reducer will aggregate the intermediate sums
and calculate the final result for the parameters 
 k means k means is a very common unsupervised learning algorithm  the goal is to cluster the
training samples into k virtual categories  the algorithm iteratively computes the centroids of every
cluster and reassigns each sample to its closest cluster  this process continues until it converges or
reaches a preset maximum number of training rounds  it is clear that the operation of computing the
euclidean distance between the sample vector and the centroids can be parallerized by splitting the
data into individual blocks and clustering samples in each block separately  by the mapper   then 
the reducer will collect the mappers work and recompute the centroids for each cluster 
 logistric regression approaching a classification problem using logistic regression      we choose
the form of hypothesis as h  x    g t x           exp t x   the learning is done by training 
to classify the data  and the likelihood function can be optimized by using
gradient ascent rule  we
p
choose a batch gradient ascent rule so the summation form          y i   h  x i  xj  i   can be
easily parallelized using our map reduce framework 
 neural network neural network learning methods provide a robust approach to approximating realvalued  discrete valued  and vector valued target function  our work focus on the back propagation
algorithm  which uses gradient ascent to tune network parameters to best fit a training set of inputoutput pairs  by defining a network structure  we use a   layer network with   output neurons
classifying the data into   categories   each mapper propagates its set of data through the network 
for each data  the error is back propagated to calculate the gradient for each of the weights in the
network  the reducer then sums the partial gradient from each mapper and does a batch gradient
ascent to update the weights of the network  in spite that back propagation algorithm is usually done
by stochastic gradient ascent  the batch gradient ascent also converges and performs fairly well 

fimulticore learning algorithm

 

 principal component analysis principal components analysis     bascically tries to identify the
subspace in which the data set lies  the principal components are essentially the most dorminant
vectors that span this subspace  mathematically  we can prove that the principal eigenvectors of the
empirical covariance matrix of the data are exactly what we want  looking at the empirical covariance
pm  i 
t
 
matrix  m
 x i    it is clear that we can parallelize the computation of this matrix  that
i   x
p
t
is  by dividing the training set into smaller groups  we can compute subgroup  x i   x i    for each
group using a separate mapper  and then the reducer will sum up the partial results to produce the
final empirical covariance matrix 
 independent component analysis unlike pca  which find the most dominant vectors  ica
tries to identify the independent sources vectors based on the assumption that the observed data are
linearly transformed from the source data  refer to     for the cocktail party problem as a classic
motivating example  and one of the most important practical application of ica   the main goal is
to compute the unmixing matrix w   in      the stochastic gradient descent was adopted to maximize
the likelihood function  however  that will pose additional challenge as we will discussed that in the
end of the section  we implement batch gradient
descent instead
 
  to optimize the likelihood  in this
    g w t x i   
t
scheme  we can independently calculate the
x i  in the mappers and sum them
  
 
up in the reducer 
 expectation maximization em     is typically a two phase algorithm  in our implementation  we
use mixture of gaussian as the underlying model  for parallelization  in e phase  every mapper just
 i 
process its subset of the training data and compute the corresponding wj  expected psuedo count   in
m phase  three sets of parameters needs to be updated      and   for   every mapper will compute
p
 i 
subgroup  wj    and the reducer will sum up the partial result and divide in by m  for   each mapper
p
p
 i 
 i 
will compute subgroup  wj x i    and subgroup  wj    and the reducer will sum up the partial result
p
 i 
and divide them  for   every mapper will compute subgroup  wj   x i   j     x i   j  t   and
p
 i 
subgroup  wj    and the reducer will again sum up the partial result and divded them 
it turns out that many machine learning algorithms  e g  ica  that use stochastic gradient descent all
pose an interesting challenge for parallelization of their algorithms  the problem is that in every step of
the gradient descent  the algorithm updates a common target  e g  the unmixing w matrix in ica  this
becomes a prime example of the race condition scenario in many multithreading applications  that is 
when one gradient descent step  involving one training sample  is updating w   it has to lock down this
matrix  read it  compute the gradient  update w   and finally release the lock  this lock release block is
essential and inevitable in order to prevent corrupted or outdated w from being used  on the other hand 
however  this clearly creates a fatal bottleneck for the parallelized algorithm 
we believe that parallelizing this type of machine learning algorithm that involves procedures like stochastic gradient descent will be an interesting topics to investigate  this is certainly one of our main concerns
for future work 
   experiments
to compare the performance improvement  we implement each algorithms with two versions  one equipped
with the map reduce framework and the other run without the framework  to ensure the faireness  both
versions of an algorithm use the same way to load the data and the same algorithm to train the classifier  in
the experiment  the machine  cs       we used has two intel pentium iii cpu     ghz and  gb memory
and is installed with linux redhat   kernel        smp 
table   lists the performance difference of each algorithm run on cs       in the experiments  we used a
       x   desgin matrix for the lwlr  gda  nb  pca  for k means and logistic regression  we used a
      x    for nn  we used a      x    and for em  we used a      x    design matrix in order to constrain
the running time of the experiment  for ica  we used a      x  data matrix  the tables show the average

fi 

cheng tao chu  yi an lin  yuanyuan yu

lwlr
gda
nb
k means     iters 
original
               
               
               
               
map reduce
               
               
                
                
improvement                                                                           
logistic
nn
pca
ica
em
               
                
               
               
               
               
                 
                
                
               
                                                                                              
table    performance comparison run on cs     

time taken to run and the average cpu usage for each algorithm with and without the map reduce framework 
for each algorithm  we run it ten times and get the average statistics  in particular  for the cpu usage  it is
actually the real running time divided by the summation of the time spent in both the system and the user
space 
as shown in table    we can see the running time of all the algorithms improve by factors from        
 pca  to          nn   as only the summation in each algorithm is parallelized  in terms of the level
of parallelization  the improvement varies as we expected  e g  in pca  to solve svd decreases the level
of parallelization and in lwlr  to solve the normal equationn also limits the improvement  on the other
hand  nn almost doubles its performance by adopting the framework  in addition  if we examine the cpu
utilization rate  it is obvious that further parallelization is possible  in particular  pca only benefits from the
framework by         cpu utilization  although it is still a significant improvement  if we further parallelize
svd  we could expect a even better performance 
the benefit of parallelizing machine learning algorithms is significant as shown in the experiments  nn 
in particular  has a nearly double performance boost up  by adopting the map reduce framework  the
algorithms can be easily extended by parallelized operations  although in our implementaion  we only
parallelize the summation for the sake of simplicity  in addition  applying the framework on a multi core
machine further eliminates tremendous communication overhead as in      it takes around one minute to
startup the framework  moreover  theoretically  as the number of cores gets larger  we can expect even more
performance enhancement and we would like to treat that as a future work  as the multi core computers
are getting more and more popular  we believe a better architecture which exploits this parallel computation
environment would attract more attention and play an important role in the machine learning field 
   future works
in only one quarter  what we can implement is pretty constrained  therefore  we leave the followingas
the future works 
 svm and pegasus parallelization 
 experiments on computers with more than two processing units 
   acknowledgement
the project is supervised under professor andrew ng  many thanks to him for his enthusiasm and devotion
in supporting us through the process  we would also like to thank gary bradski for the knowledge in
parallelization implementation and the experimental environments  finally  we would like to thank skip
macy for sharing his valuable profiling experience in vtune 
references
    jeffrey dean and sanjay ghemawat  mapreduce  simplifed data processing on large clusters  operating systems design
and implementation  pages              
    andrew y  ng  generative learning algorithms  http   cs    stanford edu  
    andrew y  ng  independent components analysis  http   cs    stanford edu  

fimulticore learning algorithm

    andrew y  ng  mixtures of gaussians and the em algorithm  http   cs    stanford edu  
    andrew y  ng  principal components analysis  http   cs    stanford edu  
    andrew y  ng  supervised learning  http   cs    stanford edu  
e mail address  chengtao stanford edu  ianl stanford edu  yuanyuan stanford edu

 

fi