stair subcomponent 
learning to manipulate objects from simulated images

justin driemeyer
cs    term project
december         

fioverview
for my project  i am working with
ashutosh saxena on a subcomponent of
the stanford ai robot  i e  stair 
one of the goals for stair is to be able
to pick up and manipulate objects in its
environment using a robotic arm using a
camera mounted on the end  which we
are approaching with machine learning 
however  in order to learn hundreds of
images will be needed  all with
information on camera orientation and
object grasping point  so instead of
taking a ton of pictures with a digital
camera  the labeling of which would be
tedious to say the least  we decided to
train using simulated ray traced images 
where the coordinate system is well
known  then test on real images 
although the final goal is to
implement this work on a robotic arm
with a variety of objects  that seemed
beyond a reasonable scope for what
could be accomplished in the time frame
of a class project  so i focused on the
classification problem at the heart of the
project  can we train using only
simulated images with the grasping point
identified and get good results on real
images 
related work
there are several factors at play in
this approach  for one  we are using just
a single camera on the end of the arm
and attempting to control a robotic arm
in  d space using a monocular image 
although we could use two successive
frames in sequence and try to derive
depth information from that  the images
are not just taken from the top of a

moving robot  but even worse from the
end of an arm on top of it  so  the
odometry information on the separation
of the two images is error prone  not to
mention computation time and simple
mathematical limits to bifocal depth
resolution  so instead once we start
working on the robot we will be using
work done earlier by ashutosh on
learning depth from monocular images 
it also builds on work described in
the high speed obstacle avoidance
using monocular vision and
reinforcement learning paper  where
they trained the algorithm first using
simulated images to get basic driving
skills  then trained further in the real
world  their results using just simulated
images showed that you can learn from
simulated images  and the simulated
images they used were pretty coarse
open gl  worlds away from the most
realistic scenes which can be constructed
using a ray tracer 
simulated images
assuming that the more realistic an
image the more applicable learning on
that image will be to the real world  we
decided to work with ray traced images
instead of open gl  all work in this
area was done using pov ray and open
source extensions in megapov  this
also allows us to build objects using
constructive solid geometry instead of
having to model everything using
polygons  but of course a ray tracer can
use polygons too if we want  but for
many objects csg should work just fine 
furthermore cgs allows simple
representation of things which in a
polgonalized space would be very
complex  such as a mug  thus we have

fia great degree of flexibility in object
creation 
initial work was done just in
generating some sample images and
balancing the degree of accuracy vs 
render time  the current implantation
has mainly used a simulated image of a
mug  but after observing performance on
that i tested the approach on a book
object as well  our current graphics take
       seconds to render per image 
allowing a thousand images to be
rendered overnight  however  based on
experimental results  i believe in many
cases         images will be sufficient 
the method we are using to identify
points of interest is a smattering of
feature detectors  and relies on the object
having some bit of texture  so a bit of
texture was added to the sample objects
through a high frequency bozo texture 
which has the effect of giving the object
a little bit of noise  this approach can
be applied to any object  mitigating the
need to define specific texture mappings 

object  so  for the book the title is a
random    character string  the height 
width  and depth are also all randomized
between certain parameters  i e   the
depth ranges from     to      in
addition to the random light position 
camera location  etc  some example
book images 

for training each image has a
corresponding grasp image  which is
black except at the regions where the
object should be held  for the mug this
is defined as the handle  for the book it is
along the center of the spine  to define
grasp points spheres were used  although
other primitives could be used as well 
here are some sample training images
along with their corresponding target
images 

each image is in a random position 
with the main light casting a shadow at a
random point along a general ceiling
area  the mug is always in the same
location on a flat white tabletop  but is of
a random color  some sample training
images follow 

when designing the book object i
decided to investigate the effect of
randomizing the dimensions of the

note in the third image no handle
was visible  thus no part of the image is
labeled as the target 
production of the labeled target
images it entirely handled by the ray

fitracer  so there is never any need for
handling labeling the target region of the
images after they are produced 
learning
all learning was done using matlab 
although far too slow for actual
deployment on a robot  it is much easier
to experiment with different methods
and parameters there  and once those
have been nailed down the program can
be implemented in something faster 
the approach for the learning was to
break each    x    down into   x  
  x   patches then apply a smattering of
   feature detectors  which i received
from ashutosh  each patch is then
labeled as   if any part of it contains a
grasp region  even so  since many
images dont even contain the grasping
region due to camera angle or occlusion 
nearly      of regions ended up labeled
  when training on the mug  this
significantly weights the negative
examples  so to even out the dataset  and
to speed convergence by reducing the
extreme number of training examples
    images yield                     
examples  negative examples are
randomly trimmed until they are about
equal  i also experimented with
different ratios  up to   negative
examples to   positive  but found that
somewhere between     and     seemed
to yield the best results  i also
experimented with simply replicating the
positive examples with some
perturbation  but trimming the negative
examples yielded better results and was
also easier to parameterize 
for the actual learning algorithm i
tried both logistic regression  which i
wrote  and kernalized svm from

http   asi insarouen fr  arakotom toolbox index html 
however  after spending a whole day on
svm attempting different kernels and
different parameters  even when i got
something that would converge if i
changed the number of training
examples it no longer converged 
besides  the svm results were mediocre
at best  so i decided to focus on logistic
regression instead 
to fine tune the parameters  i
experimented on the mug images  i
tested varying the number of training
images from    up to      and varying
the ratio of negative to positive images
from     up to     
results
i tested two objects in each category 
one which should be easier to identify 
and one which should be more difficult 
for mugs i tested one dark blue mug
which would stand out well against the
white background and which had a
handle somewhat similar to the handle in
the training example  large enough to
fit the whole hand  the second mug was
white so it would be harder to resolve
against the white background  and it had
a handle which only fits one finger  very
different from the handle in the training
example  for the book object  the first
book is a c   reference manual  very
rectangular and with corners which
should be easy to resolve  the second
book is a hardcover yearbook with a
rounded spine and very dark to the
corners are hard to resolve 
before trimming the number of
negative examples  training and testing
on the training images led to      
accuracy in classifying regions as handle

fior non handle overall  but only       
accuracy correctly classifying the handle
regions  this of course translated to
even worse performance on the real test
images  after trimming  there was a
significant improvement in grasp point
detection  although the false positive rate
also increased  however  this is not an
issue as long as the grasp point is
detected consistently and false positives
are spurious  i found the best
performance training on    examples
and a negative to positive example ratio
of     or        regions identified as
grasp point regions are highlighted by
having their red component inverted 
 which shows as a red or blue highlight 
note the consistently excellent detection
of the easier target  while detection of
the second challenge target fails in a
few images  represented by image   in
both sets  but is detected in most images 

   training images        ratio  book

   training images        ratio  mug
these images are a representative
cross section  in the bottom right book
image  note the spine is on the right side 
this was the worst performance in terms
of false positives  most were more
similar to the third image  full results
for these parameters are on the next page 
future work
continuing the project will involve
producing more simulated objects  as
well as experimenting with using
multiple simulated objects for one class
of objects  i e   having several models of
a mug instead of just one   also 
orientation of the robotic pincher needs
to be incorporated as well  and then it
needs to be implemented in a real time
language instead of matlab 

fifull mug results    images       ratio

full book results    images       ratio

fi