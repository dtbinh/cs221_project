using embedding algorithms to find a low dimensional representation of
neural activity during motor planning
cs    final project  fall     
afsheen the plumber afshar and john the whale cunningham
introduction
patterns of neural activity in certain brain areas are understood to drive motor behavior  in the time
immediately preceding a movement  there is a period of preparatory neural activity  called the  plan period   this
activity can be measured as cell firing patterns  where we record as data  for each neuron  the number of observed
action potentials in small time bins  thus  the recorded data is equivalent to a high dimensional firing rate vector as
a function of time  with one dimension for each neuron 
prior work has supported the hypothesis that neurons in the dorsal premotor cortex  pmd  region of the
brain modulate their activity depending on the direction  distance  and speed of an upcoming movement  churchland
and shenoy   numerous theories have attempted to explain what these neurons are representing by their firing
patterns  but no single encoding scheme has proven universally successful in explaining the observed data 
recently  various researchers have begun to propose models of plan period activity in pmd that do not
restrict neurons to solely representing a small subset of the dimensions of the upcoming movement  e g  parameters
in a kinematic model of the arm   yu et al   specifically  they have proposed that  while planning  the populations
firing rate vector is in a low dimensional manifold of the high dimensional firing rate space  further  it posits that
when the operation of planning involves altering firing rates so that they move to a subspace of that manifold  this
is called the optimal subspace hypothesis  various experimental results  such as the finding that firing rate variance
across similar trials decreases as a function of time  agree with this hypothesis  churchland et al  
if firing rates during planning occupy a low dimensional manifold  there should be a way to represent these
data using many fewer dimensions than the number of neurons  this low dimensional representation could reveal
the fundamental neural signatures of motor planning as well as correlate with behavioral features of the impending
movement  such a representation could be very useful for neural prosthetics in addition to basic neuroscience 
current work involves using expectation maximization to do exactly this  yu et al   this work investigates
how well the simpler algorithms of principle component analysis  pca   isomap  local linear embedding  lle  
and sensible pca  spca  perform on this problem 

background
linear methods  pca and spca  roweis 
pca is a linear projection method that works on the simple principle of ordering the axes in terms of their
variability  an eigenvector decomposition  and selecting the dimensions of most variability  highest eigenvalues  
this well known analysis has the benefit of simple implementation  quick runtime  and optimal mean squared error
over the class of linear methods  while it is very useful in assessing important dimensions in the data  it lacks a
proper probabilistic model under which one can evaluate test data  that is  given a n dimensional  n number of

neural units  training set  x   i          m   pca does not learn a generative model for x  hence  one can not
calculate the likelihood of any training or test data 
spca addresses this problem by adding a proper probabilistic framework to pca via a factor analysis
approach  calling the data x and the latent variables  iid  y  we assume the model 
 i  

v   n     i 

 x   n     cc t   i 
we can calculate the likelihood of any data under this model using p   x        to learn this model  we will employ

x   cy   v

y   n     i 

the em algorithm  using bayess rule and the rules for conditioning on gaussian random vectors  we can write 
e step 

qi   y   i       p  y   i     x   i     c        n   x   i     i  c     y   i   where    c t  cc t   i   

letting x be the n x m  number of units by size of training set  matrix of training data  further analysis then yields 

  xx        tr   xx  cxx     mn  where    mi  mc   xx 
m step  c
thus  spca builds a factor analysis model around the data set  the drawback of this approach is that it requires
the iterative em algorithm  which can be computationally costly and can find only local optima  however  in our
application  we found that this em algorithm ran quickly and produced consistently the same optima  which are
equal to the principle components found in pca  in  roweis   the author confirms that spca has never been found
to have local optima  though no proof has been found  with slightly more computational and algorithmic
complexity  spca produces the same projection results as pca and yields a probabilistic model  which will be
useful in comparing this method to other inference techniques 
new

t

t

 

new

nonlinear methods  general approach

t

t

t

t

fipca will fail to find any lower dimensional space that is embedded non linearly in a higher dimension  e g 
a manifold twisted up into higher dimension   for euclidean manifolds  isomap and lle avoid this shortcoming of
linear projection by arguing that  for a given point in a well sampled space  the points nearest neighbors will lie
only in that low dimensional space and will not exhibit the higher dimensional convolutions  then  if we preserve
the local geometry and dimension of each neighborhood  we should be able reconstruct the manifold using only the
dimension of those neighborhoods  however  these methods suffer when neighborhoods do not well represent the
manifold on which the data lies  e g  due to sampling sparsity   while lle and isomap assess local geometry and
reconstruct differently  their high level approach is quite similar 

lle roweis et al  
lle examines the neighborhood of k points around each data point to approximate the high dimensional
data xi by a collection of linear subspaces  these linear subspaces are then used to embed the high dimensional xi
into a lower dimensional data set yi of predefined dimension 
specifically  this is broken into two operations  the first  called reconstruction  involves following the
weights needed to reconstruct each xi by a linear combination of its k nearest neighbors 


e  w       x i  wij x j
 i
j







 

w     arg min e  w  
s t wij     i
j

note that wij     for all points xj that are not within the k neighbors of point xi  these weights w are then
used to embed xi in the lower dimensional space in the second  embedding step to produce y  this maintains the
relative positions of the original points when performing the embedding  note that the constraints that each row of
w sum to   makes lle invariant to rotation of the original data set x 


e  y       yi  wij y j
 i
j

isomap tenenbaum et al  

 






y     arg min e  y  

isomap also involves two steps  and it also tries to maintain the relative positions of data points x when
performing the embedding to points y  it goes about doing this by first creating a neighborhood graph in which all
points are connected to their k neighbors  k is again a chosen parameter   distances between all pairs of points are
calculated by traversing this graph instead of using cartesian distance  the concept here is that  with enough
sampling  distance in the manifold between any two points can be well approximated by a series of hops along the
shortest path of a neighborhood graph  this approach makes isomap preserve the relative positions of points when
performing the embedding operation 
the algorithm embeds the xis in a lower dimensional space of given dimension by trying to maintain the
distances between all pairs of points as best as possible 

e     dg      dy   l 

y   arg min e
where dg is the matrix of pairwise distances in the original space  dy is the matrix of pairwise distances in
the embedded space  and the tau operator converts these distances to inner products for improving optimization 

methods
an electrode array  cyberkinetics  inc   was used to record from the pmd of a rhesus macaque monkey
during the delay period prior to forty reaches to a single target  the targets were presented on a fronto parallel
screen about    cm from the monkey  spike sorting  the process by which broadband neural data is processed to a
set of discrete firing events  was done by hand using time amplitude hoops  only those units that were deemed  by
an expert spike sorter  to be  i  of high quality  and  ii dedicated to motor planning were kept for analysis  a total of
   single and multi units were used for the following study  where a unit represents a distinct pattern of neural
activity that can be attributed to one or possibly multiple neurons  each neural unit then has a pattern of discrete
firings indicative of a continuously changing firing rate  which we view as a data set with dimensionality equal to
the number of units  the tempo behavioral acquisition system  measurement computing  was used for all trial
timing and behavioral control 
code from the authors of lle and isomap was downloaded and tailored to the data structures used  code
for pca and spcas em algorithm was written by afshar and cunningham 

notes on implementation comparison vs  milestone

fiit is pertinent to note differences in approach from those reported in the project milestone  after many
different approaches to pca  lle  and isomap yielded no useful results  see results shown on milestone   we
changed the representation of our data structure  we first used binned spike counts  bin width of   ms across     
neural units   owing to the sparsity of spikes  bins almost always contain zero spikes  and in no case more than six 
thus  each data point  spike counts for one time bin  in      dimensions was effectively confined to lie on the nonnegative orthant  integer lattice  from zero to six  even in high dimensional space  it seems doubtful that there
would exist structure here 
to correct for this problem  we returned to the recorded spike times for each neural unit and convolved this
event signal with a unit gaussian of standard deviation   ms  on the very reasonable assumption that neural firing
rates are modulated continuously  this convolution produces continuously changing firing rates across the time
course of a trial  this operation preserves the information in the data but enforces continuity and smoothness  with
this alteration  we produce appealing results 
also  pca and spca have been included in the analysis  pca was initially added as a cross comparison
with isomap and lle  we found that it performed as well as isomap and better than lle  see discussion   having
been frustrated by the lack of probabilistic framework in these techniques  we also sought a more theoretical
learning model  this search produced spca  using spca also has the benefit of learning a hidden linear system
that generates this data  this allows us to compare model quality vs other  more complex models being developed
in our research that include non linearities and dynamics 

results
the following figures show plan period firing rate trajectories that were projected onto a   d manifold
using the four dimensionality reduction techniques  red circles indicate the beginning of the trial     ms before the
target appears  i e   a time of undirected neural activity where we know no plan has formed   black circles indicate
when the go cue was given to move  e g   the end of the plan period   one blue trajectory connects each pair of red
and black circles 
figure     a  firing rate trajectories to one reach target projected onto   d using pca  note consistency of trajectories through
firing rate space to a optimal subspace of planning   b  residual error curve of pca showing effective dimensionality of around   

figure     a  firing rate trajectories to one reach target projected onto   d using spca   b  em learning algorithm of spca 

fifigure     a firing rate trajectories to one reach target projected onto   d using isomap  k       b  residual
reconstruction error curve showing intrinsic dimensionality of roughly   

figure       dimensional noisy s curve projected onto   d
using lle  k      used for control to ensure correct
algorithm implementation  

figure    firing rate trajectories to two reach targets projected
onto   d using spca  trajectories to each target begin at
 red yellow  circles  follow  black  green  paths  and end at
 blue magenta  circles in their respective optimal subspaces
figure    speed of trajectories through latent space found using spca 

fidiscussion and conclusions
for this data to be consistent with our hypothesis  we expected to see distinguishable trajectories over time
through the low dimensional manifold  settling into the optimal subspace  indeed  in all of pca  spca  and
isomap  we see these trajectories quite nicely  figs          and     to go further with this hypothesis  we expect that
different reach targets would settle to different optimal subspaces  representing different reach plans  we see this
clearly in figure    where we group all trials to two different reach targets  from the red and yellow circles  we see
that each trial begins in the same large  noisy null subspace  and then  when the target is presented  the brain
quickly traverses to one of two optimal subspaces  determined by which target was presented   ending in the blue
and magenta circles  it has also been hypothesized that  once a target is presented  the brain will form a motor plan
quickly  reaching the optimal subspace and then staying roughly confined to that space  by plotting the velocity of
the trajectories in figure    we see exactly this effect  before the target onset  the brain state is slowly moving
around a noisy null state  when the target is presented  after a small latency attributable to the visual system   the
brain rushes predictably to the optimal subspace  where shortly after it slows and remains 
the above discussion focuses on the neuroscientific results of this analysis  there were also notable
differences in the performance of the dimensionality reduction techniques  linear methods  pca and spca  have
done as well visualizing the structure as the nonlinear methods  isomap  with an appropriate choice of neighborhood
size  can readily recover the structure  but its computational complexity does not justify its use given the similar
outcome  interestingly  we have not been able to get meaningful results from lle for any choice of parameters 
we show in figure   a control  the noisy   d s curve embedded into    dimensions  to illustrate that our algorithm
is working correctly  one might suspect that the embedding via neighborhood mappings may be a source of this
error  indeed  for nave choices of neighborhood sets and data sampling rates  this fact can cause failure in both
lle and isomap  while we were able to overcome these roadblocks for isomap  we have not been able to do so
with lle  our working hypothesis for this failure is that  due to the sparsity in high dimension of this data set 
modeling each point as a linear combination of its nearest neighbors is very inaccurate  thereby destroying the
structure in the data  unfortunately  the lack of a proper model framework for lle and isomap make it difficult to
further analyze their successes and failures 
for this reason  we conclude with the opinion that  of all the dimensionality reduction techniques we have
investigated  spca stands out as the most suitable for this application  both in terms of having a well defined
machine learning framework and in terms of efficiently producing quality results  being able to visualize
trajectories through the brains state space suggests many new avenues of research  including the determination of
laws of motion and behavioral correlates in this state space  having a sound probabilistic framework and an
efficient dimensionality reduction algorithm  well tested against its peers  is critical in approaching any of these
larger issues  and thus we consider this work a successful and crucial step towards that goal 

acknowledgments
we thank mark churchland and byron yu for valuable discussions and data collection  and missy howard and
mackenzie risch for expert veterinary care 

references
churchland mm and shenoy kv  movement speed alters distance tuning of plan activity in monkey pre motor
cortrex  soc  for neurosci  abstractss        
churchland mm  yu bm  ryu s  santhanam g  shenoy k  reaction time and the time course of cortical pre motor
processing  soc for neurosci  abstracts        
roweis s   saul l  nonlinear dimensionality reduction by locally linear embedding  science v     no      
dec           pp           
roweis s  em algorithms for pca and spca  neural information processing systems     nips    
tenenbaum jb  de silva v  langford jc  a global geometric framework for nonlinear dimensionality reduction
science                           december     
yu bm  afshar a  santhanam g  ryu si  shenoy kv  sahani m  talk and poster   extracting dynamical structure
embedded in neural activity  neural information processing society  nips    

fi