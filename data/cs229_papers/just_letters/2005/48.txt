cs    final project  clustering news feeds with flock
ari steinberg   ari steinberg stanford edu
december         

 

introduction

   

the problem   information overload

the rise of blogging and rss  really simple syndication  have created a more personalized way of reading the
news  with a much richer diversity of information and perspectives  unfortunately  though  the rapid growth of these
technologies has created a new problem   how to handle the vast amount of information now available  unfortunately 
while rss aggregators have helped to bring all of the information to one place  they do not solve the problem of
filtering through this huge amount of information  this problem tends to get worse as the reader begins to follow
more and more blogs  and it is now common for blog readers to spend several hours every day going through all these
stories  many bloggers have discussed this problem  googles blog search tool shows        results for information
overload    

   

previous approaches

while most aggregation sites have thus far ignored this problem  there have been some attempts to address it  for
example  searchfox  attempts to assign a score to each new article based on the words it contains and how they
compare to the readers past reading habits  theoretically allowing the reader to ignore low scoring articles that the
filter predicts will not be interesting  while this works in some situations  accurately modeling the users interest
level would be extremely difficult  especially given the many outside influences that cause readers to change their
habits over time  expecting such a system to work perfectly would be naive  and if it does not work perfectly the
user risks missing stories he may be interested in that have been assigned a low score  this approach also has the
risk of losing serendipity  a user will only see articles similar to the past articles he liked and will never see any
new topics that might be interesting to him  it is also undesirable in that the user needs to train the filter 
another  more promising approach to this problem is to display articles clustered according to topic  while this
will not reduce the number of articles displayed  it allows the user to quickly filter entire sets of articles on topics
that are not interesting to him without having to make any impossible predictions about what topics the user likes 
this approach has been successfully applied on many news sites  as well as a popular new blog reading site called
memeorandum    however  these implementations all have a fixed set of sources from which they display articles and
do not allow the user to specify the sources he is interested in monitoring  thus losing the benefit of personalization
rss has brought us 
  http   blogsearch google com blogsearch q information overload
  http   rss searchfox com 
  e g 

http   news google com 

  http   www memeorandum com 

 

fifigure    flocks aggregator view

   

our approach

we approach this problem with a combination of clustering and aggregation  unlike existing news clustering sites 
we allow the user to select which feeds are to be clustered  the clustering will enable the user to quickly filter out
topics he is not interested in  and by restricting our clustered view to the set of feeds the user has specified  the user
is guaranteed to be shown all of the articles from the sources he is interested in  however  restricting our clustering
to only a small set of sources presents some additional challenges that we will have to address 

 

implementation

   

flock

rather than create a new rss aggregator from scratch  we implement our feed clusterer on top of an existing  open
source aggregator   the one built into the web browser flock    flock is a desirable base platform as it is built upon
the mozilla platform and thus has been designed to enable a great deal of extensibility  while unlike firefox it ships
by default with an aggregation view  figure   shows flock with its current aggregation view  because flock is
capable of displaying any collection of feeds in one aggregation view  we would ideally like to do the clustering on
the fly  since we do not know what view the user will look at ahead of time   thus  speed is important  so we will
often comment on how various algorithmic decisions impact this in our analysis below 

   

clustering algorithm

we implement several clustering algorithms including k means  expectation maximization  em   and several variants of hierarchical agglomerative clustering  hac   our implementation of em uses a mixture of gaussians model 
hac  described in      starts with each article being placed into its own cluster and greedily chooses the closest
pair of clusters to merge one at a time  to select the closest pair of clusters  there are numerous options  including the
base similarity metric  for which we implement matching  dice  jaccard  overlap  cosine  information radius  and
l  norm  all as described in      as well as how to update the similarities when merging the clusters  we implement
the single link and complete link variations  
ignoring for the time being the differences in results  which will be discussed in section     hac has some
advantages that make it more desirable for our task than the more traditional em and k means approaches  most
  http   www flock com 

 

fifigure    the distribution of cluster sizes for a human labeled set of     blog posts and news articles spanning
various topics and   days  there is one cluster with    posts representing a particularly hot topic at the time
 yahoos acquisition of del icio us   while the rest of the clusters all contain under   posts  there were a total of    
clusters 
importantly  hac does not require a preset number of clusters   since it is hierarchical  it will continue to merge
articles until they have all been connected into one giant cluster  of course  putting everything in one cluster is
not at all useful  but the agglomeration can be stopped at any point along the way when either the desired number
of clusters is reached or when all of the remaining pairs of clusters have low similarity values  in order to make
comparisons between the many similarity metrics as well as to em and k means easier  we cheat by hard coding
the correct number of clusters beforehand  but with hac it would be easy to tune the similarity cut off appropriately 
this should if anything help em and k means far more than hac  since selecting the appropriate number of clusters
is a much harder process for em and k means 
it is also important to note that unlike typical clustering problems where the goal is to make a relatively small
number of large clusters with lots of documents in each cluster  in our problem we expect to see a large number of
clusters  since in general there tend to be a huge number of completely different topics with articles written about
them  with the majority of clusters having only one document each  only the top stories of the day should have
more than a couple documents in their clusters  a human labeled clustering of     blog posts and news articles
resulted in     different clusters  as shown in more detail in figure   
as a result of these properties of our problem  hac  traditionally assumed to be the slower algorithm  tends to
perform significantly faster than em and k means  while it is possible to implement hac with runtime quadratic
in the number of documents  even our simplistic o n  f    n  k n     where n is the number of documents  k is the
number of clusters and f is the number of features per document  implementation is significantly faster than our
em and k means implementations  which are o knf   per iteration  this is because in most applications k is small
enough to be ignored whereas for us k and f are both comparable to n  and because the constant factors tend to
heavily favor hac  the other algorithms must do many random restarts to select the appropriate number of clusters 
we also implement a variant of k means in which we precompute the similarities of the documents and avoid ever
explicitly computing the centroids  using a trick described in       this helps to speed up k means  but hac is still
noticeably faster 

   

feature selection

feature selection is another important area to explore for this problem  the most basic approach to this problem is
to use all of the text in the feed  after stripping out html and splitting words on punctuation and spaces  and to
use a standard binomial or multinomial bag of words model  however  we improve upon this using stop words and
 

fialso experiment with stemming  using bigrams  and different ways of weighting certain features  these weighting
factors can include idf as well as boosting  the weight of words depending on where they occur  such as if they are
in the title of a post instead of the body   finally  while we generally strip out html  it can be useful to include
any urls linked to in the feed  since often blog posts will include links to the document they are commenting on 
which can be a dead give away that they are addressing the same story 
one problem is that web sites have a great amount of variety in the amount of information presented in the feed
itself  while some sites present the entire contents of their articles in their rss feed  others provide only a short
snippet  some provide feeds containing full html including links and images  while others provide a feed with only
plain text and all of the links removed  and worse some contain embedded text ads  cnns feed often uses read
full story for latest details as the entire body text of an article  thus  just using the content provided in the feed
may miss out on some of the details that we would like to capture  so it could be useful to crawl the contents of
the site linked to in the post  we choose not to do this  however  since it would significantly slow things down and
presents additional problems in terms of the variety of content on the linked pages  it is hard enough to cluster small
well formed xml snippets let alone full documents full of navigational content and often with mal formed html 

 

results and discussion

as usual with clustering  evaluation is a tricky problem  even given a hand labeled clustering treated as correct 
evaluating how close a given clustering is to the correct clustering is nontrivial  we implemented the rand index
and the jaccard index  both of which are based upon the percentage of o n    pairwise comparisons of the clustered
articles that are correct  as described in       because we have so many clusters with only one article  the rand
index gives highly inflated numbers across the board  almost always above     in our tests   so we show results for
the more interesting jaccard index  which ignores pairs that are correctly separated      also finds jaccard to be
superior 
we hand clustered two separate sets of feeds   one a collection of     posts from approximately    blogs and
news feeds on a huge array of subjects merged into     clusters  the one displayed in figure     the other a set of   
posts from   mainstream news sites    the first set was used as a development set while the second was held out and
used purely to test the final version of the clusterer  to validate that our labeling makes sense  we had two judges
label the second set of articles  the jaccard index between the two judges was fairly high          indicating that 
as we expected  there really is a correct clustering that a human can identify 
table   shows the results of running hac and k means on the set of     blog posts and news articles described
earlier  the dice and jaccard metrics perform identically   not too surprising given that they are both based
primarily on intersection  notice that in general the cosine and information radius similarity metrics tend to do
best  and in particular they do best using bigrams and boosting  also  in general  bigrams tend to do better with
boosting  perhaps because the boosted title words help to reduce the effect of some noisy bigrams  we expect that
significant further improvements can be made by tweaking the boosting scaling factors   we chose not to do this to
avoid overfitting our data 
the table also makes it clear that k means  in addition to running slower  gives far worse results on this task
than hac  this is not entirely surprising  since k means is more typically used in a setting where the number of
clusters is nowhere near as close to the number of documents  em ran even slower than k means and thus we were
not able to obtain complete results  however  preliminary em results indicate that it too performed far worse than
hac  both in terms of speed and accuracy  one test on the smaller second set of news articles  for example  had a
rand index of       and jaccard index of        compared to       and       for hac using the same features  
unfortunately  one drawback of using the jaccard index for evaluation is that it is hard to intuitively understand
how good or bad a performance of       actually is  anecdotally  the results look reasonable enough to suggest that
  note  when using the word boost in this paper i am referring to upweighting and not the machine learning strategy used in e g 
adaboost 
  the news sites used were the front page rss feeds from the new york times  cnn  yahoo news  and the bbc 

 

fimatching
dice
jaccard
overlap
cosine
l 
infrad
kmeans
fastkm

binomial
multilink
     
     
     
     
     
     
     
     
     

single
     
     
     
     
     
     
     
 

multinomial no boosting
complete link
normal stem
idf
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           

single
bigrams
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
 

multinomial boosting
complete link
normal stem
idf bigrams
     
           
     
     
           
     
     
           
     
     
           
     
     
           
     
     
           
     
     
           
     
     
           
     
     
           
     

table    performance of hac and k means using a variety of different settings  single refers to single link
clustering  only relevant to hac   boosting refers to upweighting urls by a factor of   and title words by a factor
of    these numbers were chosen intuitively and without testing  further optimization could be done here   k means
is repeated   times with    iterations each and with the best scores for each of the   runs averaged  fastkm is a
tweaked k means implementation  em ran too slowly on this data set to get results 

      should at least be somewhat helpful to a reader hoping to cluster his feeds  but more thorough evaluation
should be done here 
to test the generalizability of our algorithm  we evaluated the best set of algorithms and features from the earlier
results on our testing set of    news articles  the jaccard score here when using complete link hac with information
radius as a similarity metric  bigrams  and boosted weights for titles and urls was        however  it turns out
that   of the cnn articles contain the body text read full story for latest details  even though they are completely
unrelated  we believe this problem can be solved in the future by storing a per feed document frequency count 
and using it to downweight terms  for now  though  we try eliminating these   stories from the test set  resulting
in a jaccard index of         very close to the result observed on our development set and showing that our work
generalizes reasonably well 

references
    l  denoeud  h  garreta  and a  guenoche  comparison of distance indices between partitions  in applied
stochastic models and data analysis       
    christopher d  manning and hinrich schutze  foundations of statistical natural language processing  chapter  
  lexical acquisition and      clustering  pages                     the mit press  cambridge  massachusetts 
     
    m  steinbach  g  karypis  and v  kumar  a comparison of document clustering techniques  in kdd workshop
on text mining       

 

fi