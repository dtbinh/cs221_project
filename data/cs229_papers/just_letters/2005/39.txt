hierarchical learning from natural images
simon berring

jeff sun

cs     machine learning  autumn     
stanford university
stanford  ca      
sberring stanford edu  jsun cs stanford edu

abstract
in this paper  we apply unsupervised learning methods to construct response functions for v 
simple cells  v  complex cells  and v  simple cells from a set of natural images  to support this  we reimplement existing sparse coding methods with the use of commercial optimization software 

introduction
the human visual cortex contains a small number of self contained functional units that fit
together in reasonably well understood pathways  the ventral pathway  which is concerned with object
recognition  has four stages  v   v   v   and it  the inferior temporal lobule   v  consists of simple
cells that resemble localized  oriented gabor filters and complex cells that respond to identical stimuli
independent of phase  v  outputs information to v   which contains cells thought to respond to broader
image contours 
since the landmark paper of field   olshausen         it has been known that linear filters
learned as sparse codings on datasets of natural images correspond almost exactly to the receptive fields
of v  simple cells  hoyer   hyvrinen         among others  realized that learning an additional layer of
nonlinear energies replicated the behavior of v  complex cells  v  has received the great majority of
research focus in this area because its behavior is well understood and  perhaps  of suspicion that simple
information theoretic elements should not apply to higher levels  however  hoyer   hyvrinen       
demonstrates that similar sparse coding techniques may yield the contour activation patterns one would
expect in v  
in this paper  we expand on these results in two ways  first  we introduce learned features at every
level  feeding forward from the image to v  and from v  to v   this is in contrast to the hand coded v 

 

filayers of previous results  second  we improve existing implementations by treating sparse coding as a
convex optimization problem  achieving notable speedup 

problem formulation
the problem we pose is that of learning in a three level network  which we perform in two layers
in a feed forward manner  in the diagram below  the feed forward direction is bottom up from pixels to
v  filters 

figure    the markov network representation of our three tiers of feature learning  adapted from h h 

we begin with a set s of    large images  we normalize each image by subtracting the average
pixel intensity of the image from every pixel and then normalizing the variance of the images pixels 
more precisely  we perform the following 

 s   s   s    s   s  
 
s  
 s   s s   
 
s   
 
we then randomly sample m    patches of size        from each image  resulting in m total
patches  we represent these patches by a training matrix x     x         x  m      
 
finally  we use pca to whiten x  reducing its dimensionality from     to      more precisely 
we set each x i  to its projection onto the     top eigenvectors of cov x   this serves to denoise the data
 
and also gives a speed boost to subsequent optimization steps 

 

fibecause learning v  complex cells is a nonlinear problem  we employ an off the shelf
implementation of independent subspace analysis to perform the learning   we then forward feed our
v  complex cell responses as training input x     x         x  m     to learn sparse v  filters  this brings
us to our second layer of learning  which turns out to be a rather typical convex optimization problem 
we may formulate our convex optimization problem  penalized in favor of sparseness  as follows 
 
m

 

min     x  i    bc  i      c  i 
b  c

i  

 

we optimize this problem in two stages  which basically results in a flavor of coordinate descent 
sparse coding stage  given a matrix of basis vectors b  we learn a sparse representation c i 
 
for each input response x  i  with the fast lssol optimizer package   this step theoretically admits for
tricks like q r factorization  
basis pursuit
stage  given a matrix of sparse representations c    c         c  m       we learn a
 
matrix of basis vectors b with a fast lagrange dual algorithm  
 

results

figure    canonical v  features from field and olshausen        left  our generated vi features right 
note the similarity to localized  oriented gabor filters 

figure    v  features from non learned v 
note the elongated contours 
 

thanks to honglak lees recommendation 
thanks to professor michael saunders 
 
we tried this but could not yield great results  so we dropped the effort 
 
developed by honglak lee and andrew ng 
 

 

fifigure    our closest approximation to good end to end learning  we ran isa on its own tweaked outputs 
we found some features that seem characteristic of v   but many poor ones as well 
note that the problem this methodology solves is not quite equivalent to the goal we specified 

conclusions
convex optimization  as always  is a useful paradigm for porting assorted problems into an
extremely well studied domain  by leveraging lssol  we were able to improve   fold on the speed of
a research system  without applying any special domain knowledge to the algorithm  of course  it is not
surprising that we recently learned that honglak lee et al have just been able to write a more specialized
algorithm that beats us by another factor of       but we are still pleased with the results 
as for our goal of implementing end to end learning from data  well we wish that had
happened  want to give us an extension 

acknowledgements
we owe immense debts to several contributors  honglak lee provided matlab code for basis
optimization  and invaluable direction 

professor michael saunders showed remarkable patience

working with us through the details of various optimization packages  authors chen  donoho  saunders 
field  olshausen  and hoyer  hyvrinen deserve great credit for making the code used in their papers
publicly available 

 

fisources
t  serre  l  wolf and t  poggio 
proceedings of cvpr          

object recognition with features inspired by visual cortex 

p  hoyer and a  hyvrinen  a multi layer sparse coding network learns contour coding from natural
images  vision research                         
a  hyvrinen and p  hoyer  emergence of phase and shift invariant features by decomposition of
natural images into independent feature subspaces  neural computation                        
s  chen  d  donoho and m  saunder  atomic decomposition by basis pursuit  siam journal on
scientific computing                 
b  olshausen and d  field  sparse coding with an overcomplete basis set  a strategy employed by v  
vision research                     
b  olshausen and d  field  emergence of simple cell receptive field properties by learning a sparse
code for natural images  nature                    

 

fi