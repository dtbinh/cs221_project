sean ting
        

removing ionospheric corruption from low
frequency radio arrays
thanks to shep doeleman  colin lonsdale  and roger cappallo of haystack observatory for their help in
guiding this project in its formative stages over the summer

i 

introduction

the eor is believed to be the time during which the intergalactic medium  igm  
which consisted entirely of neutral hydrogen  was reionized by the first stellar and quasistellar objects  as such  it represents an important era in the history of the universe 
however  direct observations of the eor are currently near impossible  because of the
gunn peterson effect the eor is difficult to study at wavelengths of less than about one
micron  carilli         as a result  it is best observed by radio through near ir
wavelengths  one particular candidate for observation  due to the high concentrations of
neutral hydrogen prior to the eor  is the    cm emission line of hydrogen  after
accounting for the red shifting of the universe during the eor  estimated to have
occurred at some point in     z        this corresponds to an observing frequency on the
order of tens to hundreds of mhz  i e  low frequency radio waves  carilli        
this poses a problem because the ionosphere causes large phase delays in low
frequency radio waves  in particular  the phase delay of the signal caused by the
ionosphere is characterized by the equation 
    
ne   s  ds      t hom pson  et al      
    
    frequency
ne  l     electron density at length l along path from antenna to top of the ionosphere
delay  

where the integral is taken with respect to the signals path through the ionosphere 
correspondingly  low frequency radio signals have large phase delays due to the
ionosphere and the differential phase delays between two antennae can cause an apparent
offset of sources which corrupts the eor signal 
thus  in order to map the sky at low frequencies it is necessary to remove the
effects of the ionosphere from the data  for radio telescope arrays with small diameters
compared to structures in the ionosphere  this can be done by identifying bright
calibration sources within the field of view  if we know their actual positions from other
methods  then by observing their apparent position we can determine the offset induced
by the ionosphere for these sources  if the positional offsets for many such sources are
known then we can construct a model of how the ionosphere affects areas between
calibration sources  in particular the offsets for points between calibration sources can be
predicted and so the ionospheric effects can be removed from the data  note that it is
important that the array be smaller than structures in the ionosphere so that the
approximation that light traveling from the source goes through the same section of the
ionosphere in reaching all antennae holds 

fiat the present there are no radio telescopes that can remove ionospheric effects to
a low enough level for accurate measurements of the eor because they lack the
sensitivity required to observe a large numbers of calibration sources  however  mit 
the cfa  and various australian groups are constructing the mwa in western australia
that will have the requisite sensitivity  in order to generate and refine an algorithm for
removing ionospheric effects  a simulation of the measurements to be taken was created 
ii 

generating a calibration algorithm

currently  the algorithm used to remove ionospheric corruption from low
frequency radio observations fits low order zernike polynomials to the observed offsets
 cotton and condon         because the mwa should have significantly greater
sensitivity than the current generation of low frequency radio arrays  it should be able to
locate a greater number of bright calibrator sources over the coherence time of the
ionosphere  this allows for greater possibilities in the functional fit performed on the
offsets  in particular  it should be possible to locate a better space of functions to fit to
the data than second degree zernike polynomials 
one way to explore these possibilities is by generating a basis of orthogonal
functions for a chosen space of functions over the positions of known calibrator sources
in the sky plane  then using the properties of orthogonal functions  least squares fits to
the data can be quickly calculated for various subspaces of the original function space 
this idea suggests applying model selection to find the optimal functional subspace  to
produce orthogonal functions over a general set of m points 
   choose a linearly independent ordered list of functions   f    f     k   f n    
f i            f i   c       i   n of length less than or equal to the number of

calibration sources    x    y       x     y      k     x m y m     
   define an inner product space over the set of continuous functions f          
m

by f   g     f   x j   y j   g   x j   y j     using this inner product then we use the
j   

traditional definition of orthogonality  two functions are orthogonal if f   g    
   perform gram schmidt orthogonalization on the functions over the inner product
space to get an orthonormal basis  e    e    k   en   for span  f    f     k   f n  
at this stage it is important to note that in step   we have not actually defined an innerproduct space but rather something that closely resembles an inner product space 
given a vector space v over a field f we generate an inner product space by defining a
real valued mapping over v  v satisfying the following properties 

fii   v  v     for v   v
ii   v  v       v    
iii   u   v  w   u   w   v  w for all u   v  w   v
iv  av  w   a v  w for all a   f and v  w   v
v  w  v   v  w for all v  w   v
however the mapping we have defined does not satisfy condition ii   any function f
that has zeros at all of the calibration points will satisfy f   f     although we do not
necessarily have f       this can pose a problem during gram schmidt
orthogonalization which will be addressed later  but for the moment assume that none of
the new functions ei       i   n produced during gram schmidt orthogonalization satisfy
   ei        where        is the norm derived from the inner product     f      f   f

    

 

then it is a basic theorem in linear algebra  one whose proof does not depend on
condition ii  of an inner product holding  that given an inner product space v   a vector
v   v   and a subspace u   v the value    v   u    where u   u is minimized by taking
u equal to the projection of v on to u   in our case we have v   c and so assume that
the phase screen can be represented by some continuous function g that takes values on
   x    y       x     y      k     x m y m    corresponding to the measured offsets at those points 
then this theorem yields the result that

   g   e      g   e  g   e

    

m

 

   g   x

j

  y j     e  x j   y j     where

j   

e   span e    e    k   en     span  f    f     k   f n   is achieved by letting e equal the
projection of g onto span e    e    k   en     in other words we can find best least squares
fit to the offsets within any function space by finding an orthonormal basis for that
space under this pseudo inner product space and projecting the values of the known
offsets onto this space  the nice property of an orthonormal basis is that finding the
projection of a vector onto the span of the basis is very efficient  more concretely 
having found an orthonormal basis the best fit function is defined by 
n

g     ej  g ej
j   

as is mentioned above we have not quite produced an inner product space because we
can have    f        even if we do not have f       this can cause problems in the gramschmidt orthogonalization process because at one step functions are divided by their
norms  which results in an undefined function if the norm is equal to    although we
have not been able to fully characterize for what distributions of points this occurs  it
seems that this is only a problem in points with an underlying symmetry  for instance
this regularly occurs if the points all lie exactly on grid points  however  when random

fidistributions of points were used no problem was found  thus  since sources in the sky are
distributed randomly this algorithm should not run into problems 
   applying model selection
using the above algorithm  it is quick and efficient to generate a functional space
of degree approximately fifty  or on the order of the expected number of calibrator
sources for the mwa over an   x   degree field of view  this project focused on finding
an optimal polynomial subspace with which to model the ionospheric corruption 
although other functions may improve the fit  polynomials have traditionally been used
for calibrating radio arrays and have performed well at low orders  however  because the
only empirical results on polynomial fits to ionospheric offsets have been performed with
far fewer calibration sources than the mwa should see  likely on the order of three to
five times less   it is an open question as to how the mwa should be calibrated  k fold
cross validation provides a logical means for addressing this question for several reasons 
first  it can be run separately on the x offsets and the y offsets so that if the
characteristics between those two sets are different  k fold cross validation can recognize
this and provide a better fit than would occur if a single order were forced on both
polynomials  second  although the number of calibration sources will be increased from
in past situations  training examples are still relatively scarce and so k fold cross
validation makes more efficient use of the data than other hold out cross validation 
in order to analyze the effectiveness of adding automatic model selection to radio
array calibration the observation simulation developed at haystack observatory in
westford  ma was used  it allows the user to specify an input sky  ionosphere  and radio
array and then generates the image the radio array observes after ionospheric corruption 
then  using the original sky map  the positional offsets caused by the ionosphere can be
determined  by passing the calibration algorithm the position and offset of the n
brightest  sources  a model of how well the algorithm performs as a function of the
number of calibrator sources can be generated  here  the performance of the algorithm
can be measured in absolute terms by examining the root mean square of the positional
offsets with no calibration and then after the fit  it is also important to measure the
performance relative to polynomial fits of fixed order 
  

results and future directions

adding in an automatic model selection mechanism to the algorithm yielded
disappointing results  the algorithm was evaluated by looping over twenty randomly
generated skies with     sources each over an   x   degree field of view as well as five
ionospheres simulating various levels of turbulence and total electron content that the
array might face  initially  because of the scarcity of training examples  leave one out
cross validation was used  however  this produced results significantly worse than did
manually fixing the degree of the fit to be a polynomial of third  fourth  or fifth degree
 see fig    for summary of important results   examining the order of polynomials
averaged into the final fit using loocv indicated that this was because when only one
point was used to test each fit  there were a large number of zero or one polynomials

fibeing averaged  which have little predictive power  and a large number of polynomials of
order greater than    which over fit the data 
this suggested two additional refinements that could be made  removing the
highest degree polynomials from the functional space should remove the tendency of the
model selection to over fit and performing k fold cross validation for large k  but with k
less than n which will give the algorithm more points to test each fit on  again reducing
the tendency to over or under fit  implementing these refinements did improve the
algorithm  however it still performed worse than a fixed order fit  several additional
tweaks failed to improve the algorithm significantly 
while  the attempts to employ model selection to improve calibration for low
frequency radio arrays was unsuccessful  it is likely that other aspects of machine
learning can be applied  after the failure of the bulk of my work on applying model
selection i considered the possibility that with the additional points locally weighted
linear regression could effectively model the corruption  and initial tests on this have
indicated that it performs significantly better than do polynomial fits  although this
method suffers from its inability to produce a single analytic function  this seems like a
very fruitful future direction and i will present the results to my advisors at haystack
observatory this holiday break  there is a further possibility that if supervised learning
can be applied to determine the best order fit under different ionospheric conditions  i e 
solar minimum vs  solar maximum  presence of traveling ionospheric disturbances  night
vs  day  and for different radio frequencies  thus  while the initial attempts at applying
machine learning to radio array calibration was unsuccessful  it has suggested further
avenues of study 

fig   

fireferences
carilli  c l   radio astronomical probes of cosmic reionization and the first luminous sources  probing the
twilight zone  asp conference series       
cotton  w  d  and condon  j  j   calibration and imaging of    mhz data from the very large array in
proceedings of ursi general assembly        aug        maastricht  the netherlands  paper
      pp            
thompson  r   moran  j   and swenson  g  interferometry and synthesis in radio astronomy       
new york  cambridge university press 

fi