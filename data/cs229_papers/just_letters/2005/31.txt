wael salloum
brad moore
rajat raina

 

final project
  th december     
page   of  

introduction

nearly     years ago  psychologist charles spearman  hypothesized incorrectly all dimensions of
mental ability can be explained solely by one underlying factor  which he dubbed as g  factor
analysis seeks to uncover if there are indeed underlying independent  and unmeasurable  factors
that affect the observed dependent variables  a typical factor analysis attempts to answer four
major questions 
 how many factors are needed to explain relationship within data 
 do these factors have real life meanings  if so  what 
 do hypothesized factors explain the data well  and how much can the dimensionality be
reduced while still maintaining inter data relationship 
 can factors measure the amount of pure random or unique variance within each observed
variable 
factor analysis is often trained using the expectation maximization algorithms to try and identify
the mean  variance  and dimensional mapping that corresponds to the lower dimensional factors 
professor ng hypothesized that this algorithm could both be sped up and improved if ideas from
probabilistic principal component analysis are used  the rest of the   page paper will present
the new algorithm  discuss the data set and experiential methodology  present conclusions drawn
from the results  and then present selected results  additional material such as derivation of the
algorithm  the basic factor analysis model    selected scripts from the    code files  and supporting
graphs are left for the appendix  we will start by presenting the algorithms 

 

algorithms

   

factor ppca

the established algorithms  em  gda  and pca  can be seen in appendix a see page    here
we present the algorithm for factor analysis using probabilistic principal component analysis  the
derivation of this algorithm is fully described in appendix b  see page    
given k and x 
a   calculate sample mean  and sample variance  
b   while not converged
i   calculate the k largest eigenvalues d             k   with corresponding  column  eigen 
 
vectors v    v         vk   of       sorted in descending order 

ii   let d denote the number of elements of d      set t   min k  d 

 
 dii       i   j  t
iii   set i  j  k  l
 ij  
  
o w
iv   set u  i   vi  

fi 

factor analysis using ppca


v   set i  j  ij  

   t  ii i   j
  
i 
  j

 

vi   set        u  l

   

hypothesis

in approaching this analysis  we expected that factor ppca would be faster then factor em  but
not necessarily more accurate  as it may impose the restrictive assumptions of ppca  nor did we
expect it to maximize likelihood at every iteration  thus no longer guaranteeing convergence  our
test were generated to test this hypothesis 

 

experimental methodology

   

optimizing factor em

in order to ensure that the ppca factor algorithm was properly tested  a lot of effort was dedicated
to optimizing factor em and gda for matlab  by leveraging compiled existing code and reducing
the number of for loops necessary for the code to execute  everything was rewritten so that sums
can be replaced with matrix multiplications while maintaining accuracy of result  this provides a
stiffer test of accuracy and time efficiency of the ppca factor algorithm 

   

tests

we designed several layers of tests in order to ascertain the various properties of these algorithms 
we developed two main tests for time as that was our main criteria 
a   every time we executed the algorithm  we tested for how long it ran in order to measure
its time  we then plotted for each data set time versus dimensions  this gives us a direct
comparison between factorem and factorppca  in each test  we ensured that we initialized
the algorithm with identical starting points and identical starting criteria 
b   in order to ascertain why one algorithm took longer then the other  we had each algorithm
return the number of iterations it took to converge  similarly  we plotted the number of
iterations versus dimensions  allowing us to determine if one algorithm took longer to converge
because of iteration or computational cost 
we also developed three main tests for accuracy 
a   we trained the algorithm on the entire data set  and then plotted training error as a function
of dimension per algorithm  this allows us a direct comparison of error per dimension across
algorithms 
b   we also trained the algorithm on     of the data set  and then tested the predicative ability
on the remaining      this gives us an idea of the predicative ability of the algorithms 
c   we also plotted the total log likelihood per dimension per algorithm  in order to ascertain
which algorithm claimed to have a greater likelihood of explaining the data set 
we also developed several tests to ensure the algorithms were correct 

fifactor analysis using ppca

 

a   we tested if the algorithm were indeed correct by measuring if the log likelihood was increasing
per iteration  as it should be in any em algorithm  if so  we can conclude if its correct  this
was the problem our em algorithm does not monotonically increase 
b   wanting to ensure if initialization had any effect on the algorithm  we initialized the algorithms at   different points  one where  is the identity matrix i  one where  is       i  and
the last where  is      i  if the results were unaffected  we can conclude that the matrix is
invariant to scaling   was always chosen to be random as its initialization only affects em 
finally  in order to ensure that results were indeed at maxima points  we ran each of the above tests
on the algorithm initialized with the converged result of the other algorithm  it was suspected that
it wont take many iterations before the algorithm converges  the last part of any experimental
methodology is of course  the data set selection 

   

data set

all of our data came from the uci machine repository  see      the data downloaded comprises
  distinct sets in machine  we attempt to predict if the computer is faster then the average
processor speed based on publicly released machine data  in balance  we attempt to analyze
psychological data  given the relative weighting and distance  here there exists   classes  left 
right  and balanced  in votes  we attempt to predict the party of the politician given his votes
in    key issues  finally  we generated random data that remained consistent with both gda and
factor analysis to see how well the algorithms performed  as we present the results  we will go into
more detail as to the implementation of the data sets  for complete detail  please see appendix c 
 see page     

 

conclusion

as the results are best interpreted in light of conclusions  we present the conclusions first  it seems
that factorppca is both faster per iteration and more accurate  it seems to fare better with a
convergence test based on likelihood rather then on global maxima on the matrices  as those may
be scaled or altered but have similar predictive capabilities  initialization appears to be important 
as indicated by machine  in that  should be initialized to have similar magnitude to the data sets 
otherwise  subtracting   tends to get swamped  and we get  s in the diagonal  the algorithm
as a result no longer guarantees convergence  and likelihood  as predicted  does not increase with
every iteration  nevertheless  this looks to be a promising algorithm and there seems to be several
mechanisms that can be used to fix these problems  first  the data set can be normalized   this
wasnt tried  but this may resolve problem of having the data sets of varying magnitudes   second 
there could be a further refinement of the convergence criteria made  as there is far more plots
and data then can fit in   pages  the secondary results and graphs are reproduced in the appendix 

 

results

the following graphs are   d band graphs  em or ppca designates the standard algorithm  if
the name is followed by a    the algorithm is run with psi initialized to       i  with     it means
it was multiplied by     with init  this means it was initialized with the results of the other
algorithm  the following graphs were generated with a convergence test on  and a cap of number
of iterations of      

fi 

factor analysis using ppca

 full set     iter vs dim per alg
 full set     iter vs dim per alg

 full set     iter vs dim per alg
em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

    
   

   

   

   

   
number of iterations

number of iterations

   

   
   
   
   

   
   
   

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

  

  

number of iterations

    

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

  

  

  

   

   

   

   

   

 

 

 

  

 
 

 
   

   
 

 
   

   
 

 

 

 

k

algorithms

   

 
   

   
 

 
   

   

 
   

 

 
   

   

k

 
   

   

 
   

   

 

 

   
 

 
   

   
 

 

 a 

k

algorithms

 

 

 b 

algorithms

 c 

figure    iteration figures  random  machine  balance
only three data sets are presented  the results of votes are included in appendix e  see page     
here  we clearly note that machine failed to converge within      iterations  while it converged in
all other cases but one 

 full set   time vs dim per alg
 full set   time vs dim per alg

 full set   time vs dim per alg
em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

   

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

 
 

 

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

   
   

 

   

 

   

 

   

 

time

time

time

   

   

 
   

 
   

   

 

   

 

 

 

 

 

 

   

   
 

 
   

   
 

   

 

algorithms

 a 

k

 
   
   

 
   

   
 

 

   
 

 
   

   

 
   

   
 

 

   

   
 

 
   

k

 

 

 

   
 

 

   

algorithms

k

 b 

 

 

algorithms

 c 

figure    time figures  random  machine  balance
it is clear that factorppca outerperfomed factorem except when it failed to converge  moreover 
it is interesting to note that in many cases  factorppca required more iterations to converge  and
yet  still managed to outperform factorem  thus  it was clear we needed to analyze our convergence
criteria 

fifactor analysis using ppca

k       full set   sum of loglik per iterationclass    

 

k       full set   sum of loglik per iterationclass    

   

k       full set   sum of loglik per iterationclass    

    

    

factorppca
em

factorppca
em

   

factorppca
em

    

   

    

    

   

   

likelihood of training data

likelihood of training data

likelihood of training data

    

   

    

    

    

    

    

    

   

    

   

    

   

 

 

 

 

 

 
iteration

 

 

 

  

    

  

    

 

 

 

 

 

  

  

    

  

 

   

 

   

 
iteration

iteration

 a 

 b 

   

 

   

 

 c 

figure    likelihood figures random  machine  balance
both em  ppca stagnates well before we terminate the algorithm and becomes cleared by looking
at the graphs in appendix e  as a result  its possible  and likely  that the convergence on psi is
overrestricting and does not increase predictive ability by any significant amount 

 full set   error vs dim per alg

 full set   error vs dim per alg
em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

  
  

 full set   error vs dim per alg
em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

  

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

  
  

  
  

  
  
  
  
  

  

percent classification error

percent classification error

percent classification error

  

  

  

  

  

  
  
  
  
  
  

 
  

  

  

 

 

 
   

 
 
   

 

 
   

   
 

   

 

 a 

k

 
   
   

 
   

algorithms

 

   
 

   
 

   

 
   

 
   

 

   

   
 

   
 

k

 

 

 
   

 

   
 

 

   
algorithms

k

 b 

 

 

algorithms

 c 

figure    error figures  random  machine  balance
while speed is important  it is important to analyze classification error  test error is presented in
the appendix  as well as comparison against gda  these graphs are of training error on the full
set and indicate that factorppca is clearly the winner here  thus  factorppca is a promising
algorithm  but is susceptible to initialization  magnitude of the data  machine   and may require a
more refined convergence test based on likelihood rather then on maxima of psi  the first figures
in the appendix test this convergence criteria  this concludes our project 

fi 

factor analysis using ppca

 

appendix a  basic models

   

basic factor analysis model

factor analysis assumes x   n is an observed random variable according to the following model 
z  n     i 
x z  n     z   
where z   k is the latent random variable  
the parameters are     n      nk   and    diag          n      nn  
the joint distribution for  z  x  is 
 
 


z
 
i
t
 n
 
x

 t   
an important note is that t    must be symmetric positive semidefinite  as it is the sum of
the square of a matrix and a covariance matrix 
given a training set  x i   m
i     the log likelihood of the parameters is 
m

  x  i 
 x   t  t       x i    
          c  m log       
 
t

  c  m log  t     

i  
 t

m

       x  i 
 x    x i    t
 
m
i  

where c is constant 
the ml estimator for  is simply given by the mean of the data and can be calculated once 
m

 

  x  i 
x
m
i  

thus  factor analysis seeks to solve 

 log  t      tr  t     

argmax 
 

   

 z
l

pm

 i     x i    t
i    x
 
m

factor em model

as this is our primary model to test against  its important to restate the results 
the log likelihood of the model is as follows 
q
 
 i 
t
t
   i 
 
l         log m
e     x         x   
n
i  
t
        

the e step is as follows 
 

note k   n yields the trivial optimal model by setting    inn   n kn   

 
   
 

fifactor analysis using ppca

z  i   x i 

  t  t       x i     

z  i   x i 

  i  t  t      

the m step is as follows 
m
  m
  
x
x
 i 
t
t
 x   z  i   x i 
z  i   x i  z  i   x i    z  i   x i 
  
  

i  
m
x

 
m

i  
t

t

x i  x i   x i  tz i   x i  t  tz i   x i  x i    z  i   x i  z  i   x i t   z  i   x i  t  

i  

ii   ii  
ij

   

     if i    j

gaussian discriminant analysis model

this model has parameters j   p  y   j   j   and   the mle estimates are 
m

j

 

j

 

  

  x
i y  i    j  
m
i  
pm
 i    jx i 
i   iy
p
 
m
 i    j
i   iy
m
  x  i 
 x  y i    x i   y i   t
m
i  

 

fi 

factor analysis using ppca

 

appendix b  derivation of factor ppca

   

maximizing w r t   with  fixed

for the sake of brevity  let   

pm

i    x

 i    x i   t

m

 

therefore 
 l
    t  t    t    t  t    t  t    t
setting the gradient to zero and taking transposes  the extreme values of  must satisfy 
    t      

   

lets only the consider when       and t        as those cases trivially satisfy the above
constraint  the two other cases are considered at the end 
assuming  is non singular    its singular value decomposition exists and is given by
 
      u lv t   where u    u    u            uk     nk is an column orthonormal matrix 
l    l    l            lk   i   kk is a diagonal matrix of singular values  and v   kk is an orthogonal
 
matrix  thus  we have the decomposition       u lv t  
substituting this into eqn    for  yields 
 

 

    u l  u t   i   u l      u l

   

if li     for some i              k   then the solution for u can have an arbitrary column ui   thus 
reduce the system to only the constrained dimensions and invert l   
 

 

    u l  u t   i   u l      u l
 

 

 

    u l  u t   i   u      u

 

   u  i  q       u

 

      u  i  q    u

 

      u   u  i   l   

 

 

 

 

 

 

where q   diag q            qk     kk with each qi   li        li    
step   to   uses the matrix inversion lemma to yield   u l  u t   i     i  u qu t   the last step
follows from the observation that qi   li        li    the ith diagonal entry of i  q must be        li    
for any li       this equation can be written in terms of the column ui as 
 

 

      ui        li   ui
 

 


      
thus  ui must be an eigenvector of 
denoting the corresponding eigenvalue by i   this implies li   i    which constrains i     
 

the assumption that  is non singular amounts to  within the factor analysis model  that the diagonal entries
are       that is  the normally distributed error has non zero variance along every dimension of x 
 
note that reducing l did not adversely affect the solution as we can permute the rows and columns back by a
linear operator  then use block matrix notation to invert and consider only the segments we are interested in 

fifactor analysis using ppca

 

therefore 
 v can be any k  k orthogonal matrix  so let v   i 
 u and l are chosen together  column by column  each column ui and diagonal entry li should
be picked using one of the following choices 
n
choice    li     
 arbitrary ui   
choice    li   i     ui corresponding eigenvector to eigenvalue i  

   

the global maximum

without loss of generality  let l    l            lt represent the nonzero values picked using choice  
 
 
above  and let a    a            at     represent the corresponding eigenvalues of         furthermore  let u  t   nt contain the t columns of u picked using choice   above and let
l  t    l            lt   i   tt   using choice    lt             lk     with undetermined corresponding eigenvalues 
q
 
the eigenvalues of    are simply i  for i                 n  
t are exactly the diagonal entries of l     
the eigenvalues of u  t l   t u  t
  t
then 

  
      u  t l  t   v t  block matrix multiplication 
 

 

 

t
t          u  t l   t u  t
  i   
 n
 
t
y
y
t
 
log         log
i 
ai

 

i  

i  

since the eigenvalues of  u  t l   t u  t   i  are the t values  a    a            at   and  n  t   s 
tr  t      s    tr  u  t l   t u  t   i   s 
t
n
x
x
 
  
ai
i  

i t  
t

  l
    log        tr t      

 
t
n
x
x
ai
   n log i   
log ai   t  
i  

i t  

we therefore need to minimize
l   n log i   

t
n
x
x
     log ai    
ai
i  

i t  

since the following holds 
 ai    

i                 t 

     log x    x

x    

 f  x    x       log x  is an increasing function of x for x     
 

since this is explicitly in the form of an eigenvector decomposition 

   

fi  

factor analysis using ppca

eqn    is minimized over  ai   and t by picking the largest t eigenvalues as a            at and also
picking t as high as possible 
t   min k  number of eigenvalues of s      

   

maximizing w r t   with  fixed

compute the gradient of l
 w r t   
 l
    t         t       t     
setting the gradient to zero  the extreme values of  must satisfy i  j  i    j 
  t       ii    
 

ii      t  ii
ij

   

   

cases

case          this is a minimum of l
 
t
t
case            this means       which has a solution when up to k eigenvalues of
s   are nonnegative  and the rest are zero  i e   s   must be positive semidefinite  meaning all
eigenvalues are nonnegative  but further conditions are required to guarantee a rectangular square
root  the solution is given in its svd form as    u lv t where u    u            uk     nk has
eigenvectors of s   in its columns  l   kk is a diagonal matrix with the square roots of the
corresponding eigenvalues on the diagonal  and v   kk is an arbitrary orthogonal matrix   this
case seems unlikely in real data because it requires  for example  the sample covariance s to be of
the form s    rank k matrix   diagonal matrix  

 

for an explanation  refer to the definition of the square root of a symmetric positive definite matrix  the major
difference in our case is that  is rectangular  whereas the standard definition refers to a square matrix as the squareroot  the bridge follows from the following two facts  first  the regular square matrix square root can be obtained by
appending  n  k  zero columns to the rectangular   second  we can have a rectangular square root  only  because
of the condition on zero eigenvalues of s    namely  that this latter matrix has rank at most k for a rectangular
decomposition of this form 

fifactor analysis using ppca

 
   

  

appendix c  data sets
random

random data was generated using a natural dimensions of    various higher dimensions and
number of training samples were used  with results being consistent across any such variation  so
the results that were presented were with a total training sample of     and n      this allowed
us to have at least one data set that terminated relatively quickly  allowing us to generate quick
results 

   

voting

below is the voting data set information as taken from the names section of the repository 
relevant information 
this data set includes votes for each of the u s  house of
representatives congressmen on the    key votes identified by the
cqa  the cqa lists nine different types of votes  voted for  paired
for  and announced for  these three simplified to yea   voted
against  paired against  and announced against  these three
simplified to nay   voted present  voted present to avoid conflict
of interest  and did not vote or otherwise make a position known
 these three simplified to an unknown disposition  
   number of instances           democrats      republicans 
nays were treated as     yeas as    abstaining as    and the goal was to predict  based on voting
record  was someone a democrat or a republican 

   

machine

below is the machine data set information as taken from the names section of the repository 
   attribute information 
   vendor name    
 adviser  amdahl apollo  basf  bti  burroughs  c r d  cambex  cdc  dec 
dg  formation  four phase  gould  honeywell  hp  ibm  ipl  magnuson 
microdata  nas  ncr  nixdorf  perkin elmer  prime  siemens  sperry 
sratus  wang 
   model name  many unique symbols
   myct  machine cycle time in nanoseconds  integer 
   mmin  minimum main memory in kilobytes  integer 
   mmax  maximum main memory in kilobytes  integer 
   cach  cache memory in kilobytes  integer 
   chmin  minimum channels in units  integer 
   chmax  maximum channels in units  integer 
   prp  published relative performance  integer 
    erp  estimated relative performance from the original article  integer 

fi  

factor analysis using ppca

as there were   different classes formed by binning  and we needed results per class  we decided
to combine these into   classes   above the mean of the global data set  and   below  we then
tested if it was above the mean or below 

   

balance

below is the balance data set information as taken from the names section of the repository 
   relevant information 
this data set was generated to model psychological
experimental results  each example is classified as having the
balance scale tip to the right  tip to the left  or be
balanced  the attributes are the left weight  the left
distance  the right weight  and the right distance  the
correct way to find the class is the greater of
 left distance   left weight  and  right distance  
right weight   if they are equal  it is balanced 
   number of instances          balanced      left      right 
the classes l  b  and r  were converted directly into ascii representation  as the classes are
not used in anyway in the calculation except as denotation  it doesnt matter  as long as they are
distinct  the goal was to predict the correct class 

fifactor analysis using ppca

 

  

appendix d  code

this section contains only the four relevant scripts used to ascertain our results  nearly    scripts
were written that execute   different variations of the algorithms discussed  they will be provided
if requested 

   

basic factor em

below is the code for the basic  unoptimized  factor em algorithm 
                              
  cs     factor analysis   em
                              
  note  this problem asssumes each column is a training sample 
function  mu  lambda  psi  lik  numiter    factorem x  k  psi 
lambda 
                                                   
 defining parameters we need for e m factor analysis
                                                    
 parameters for e m algorithm
 n m    size x  
  dimensions of x
mu   zeros k m  
  for each dimension  for each sample
sigma   eye k  
  a single n x n covariance matrix
i   eye k  
  k x k identity
error        
  error theshold
                                                   
 defining our predicitions for e m factor analysis
                                                    
 what we are trying to predict
mu   mean x     

  n x  

                                                   
 e m factor analysis
                                                    
numiter      psiold   psi       error  while   max abs diag psi psiold      error      numiter         
                                     
  storing old value of psi
                                     
psiold   psi 
numiter   numiter     
                   

fi  

factor analysis using ppca

  e step
                   
for i     m
mu   i    lambda   inv lambda   lambda   psi    x   i    mu  
end
sigma   i   lambda   inv lambda   lambda   psi    lambda 
                   
  m step
                   
term       term      
for i     m
term    term     x   i    mu    mu   i  
term    term    mu   i    mu   i    sigma 
end
lambdanew   term    inv term   

phi     
for i     m
phi   phi   x   i    x   i    x   i    mu   i    lambda   lambda   mu   i    x   
end
psi   diag diag phi m   
lambda   lambdanew 
 l  lik numiter     factorlik x  mu  lambda   lambda   psi  
end

   

factorppca with likelihood convergence test

                              
  cs     factor analysis   ppca
                              
  note  this problem asssumes each column is a training sample 
function  mu  lambda  psi  lik  numiter    factorppca x  k  psi 
                                                   
 defining parameters we need for ppca factor analysis
                                                    
 parameters for ppca
 n m    size x  
i   eye n  
  n x n identity
error       
  error theshold
                                                   
 defining predictions for ppca factor analysis
                                                    

fifactor analysis using ppca

mu   mean x     
lambda   zeros n k  

  

  n x  
 just so we know we are  predicting it 

 calculate sample variance and  old psi 
s      xminusmu   x   mu ones   m   for i     m
s   s   xminusmu   i    xminusmu   i  
end
s   s   m 
 sample variance
psiold   psi       error 
 to store old psi value
                                                   
 ppca factor analysis
                                                    
numiter      liknew      likold     
 setting initial paramters
while  max abs liknew   likold     error      numiter         
 to remove unnecesary repeated matrix computation
numiter   numiter     
psisqrt   psi      psinegsqrt   inv psisqrt  
 get the largest number of valid eigenvectors values
 v  d    eig psinegsqrt   s   psinegsqrt    get eig vectors values
diagd   diag d  
 convert d into a vector of lambdas
t   min k  sum diagd       
 find the valid t
 sorted  indices    sort diagd  descend    find indices with largest eig  st
validlambda   indices   t  
 get indices of t largest lambdas
l   diag diagd validlambda           
 create l for valid lambdas
u   v    validlambda  
 create u for valid lambdas
 storing old value of psi  and calculating lambda psi
psiold   psi 
psi   diag diag s   lambda   lambda   
lambda   psisqrt   cat    u   l  zeros n  k   t   
 l  lik numiter     factorlik x  mu  lambda   lambda   psi  
 convergence
if numiter  
likold  
liknew  
end

test
 
lik numiter      
lik numiter  

end

   

test code

                                    
  cs     classification test
                                     

fi  

factor analysis using ppca

  note  this problem places assumes each column is a training sample 
  this also  knows  that factorem and factorppca have a cap of     
function      test x  y  t 
                                                                            
  as i didnt want to restrict this to a particular prespecified dimension 
  i go for each possible classifictaion  caluclate the values  and then
  also calculate the likihood of that value  and classify it accordingly 
                                                                            
                                                      
  comparing factorem to factorppca on real data sets
                                                      
 general paramaters
class   unique y  
 getting classifications
numclass   length class  
 number of classes
 n m    size x  
 numdimensions  numsamples
k       n 
 define dimensions to consider
 iteration paramters
ppcaiter   zeros    length k   
ppca  iter   zeros    length k   
ppca iter   zeros    length k   
ppcainititer   zeros    length k   
emiter   zeros   length k   
em  iter   zeros   length k   
em iter   zeros   length k   
eminititer   zeros   length k   

   zero average iterations to date
   zero average iterations to date
   zero average iterations to date
   zero average iterations to date
   zero average iterations to date
   zero average iterations to date
   zero average iterations to date
   zero average iterations to date

 time paramters
ppcatime   zeros    length k   
ppca  time   zeros    length k   
ppca time   zeros    length k   
ppcainittime   zeros    length k   
emtime   zeros    length k   
em  time   zeros    length k   
em time   zeros    length k   
eminittime   zeros    length k   

   average time used so far
   average time used so far
   average time used so far
   average time used so far
   average time used so far
   average time used so far
   average time used so far
   average time used so far

 likelihood parameters
ppcalik      
ppca  lik      
ppca lik      
ppcainitlik      
ppcatestlik      
emlik      

 initializing to empty
 initializing to empty
 initializing to empty
 initializing to empty
 initializing to empty
 initializing to empty

fifactor analysis using ppca

em  lik      
em lik      
eminitlik      
emtestlik      

 initializing to empty
 initializing to empty
 initializing to empty
 initializing to empty

 likelihood array parameters
plikiter   ones length k         numclass  
elikiter   ones length k         numclass  
 parameters for test error
indices   randperm m  
numtrain   round m        
xtrain   x    indices   numtrain   
ytrain   y indices   numtrain   
xtest   x    indices  numtrain    m   
ytest   y indices  numtrain    m   

  

 likelihood array
 lkelihood array

 random permutation of indices
 number in training sample
 pick  st     of random index
 pick last     of random index

 executing test for each dimension  classification  test training  etc 
for i     length k 
 for each dimension
for j     numclass
 for each potential classification

                                                                                    
  training error
                                                                                    

 factorppca
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorppca x    y    class j   k i   eye n   
ppcatime i    ppcatime i     cputime   t  numclass   getting time difference
ppcaiter i    ppcaiter i    iter numclass 
 summing number of iterations
 ppcalik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    loglik of
plikiter i    iter  j    lik 
 storing likelihood

 factor em with factoppca initialization
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorem x    y    class j   k i   psi  lambda  
eminittime i    eminittime i     cputime   t  
 getting time differenc
eminititer i    eminititer i    iter numclass 
 summing number of iter
 eminitlik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    prob of

 factorppca with psi     
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorppca x    y    class j   k i   eye n        
ppca  time i    ppca  time i     cputime   t  numclass   getting time difference
ppca  iter i    ppca  iter i    iter numclass 
 summing number of iteratio
 ppca  lik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    loglik
 factorppca with psi      

fi  

factor analysis using ppca

t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorppca x    y    class j   k i   eye n         
ppca time i    ppca time i     cputime   t  numclass   getting time difference
ppca iter i    ppca iter i    iter numclass 
 summing number of iterations
 ppca lik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    loglik o

 factorem
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorem x    y    class j   k i   eye n   rand n k i
emtime i    emtime i     cputime   t  
 getting time difference
emiter i    emiter i    iter numclass 
 summing number of iterations
 emlik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    prob of ith
elikiter i    iter  j    lik 
 storing likelihood

 factorppca with em initilization
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorppca x    y    class j   k i   psi  
ppcainittime i    ppcainittime i     cputime   t  numclass   getting time differenc
ppcainititer i    ppcainititer i    iter numclass 
 summing number of iter
 ppcainitlik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    logli

 factorem with psi     
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorem x    y    class j   k i   eye n        rand 
em  time i    em  time i     cputime   t  
 getting time difference
em  iter i    em  iter i    iter numclass 
 summing number of iteratio
 em  lik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    prob of it

 factorem with psi      
t   cputime 
 get current time and execute
 mu  lambda  psi  lik  iter    factorem x    y    class j   k i   eye n         rand
em time i    em time i     cputime   t  
 getting time difference
em iter i    em iter i    iter numclass 
 summing number of iterations
 em lik   j   l    factorlik x  mu  lambda   lambda   psi    lik i j    prob of ith

                                                                                    
  training versus test
                                                                                    
 mu  lambda  psi  lik  iter    factorem xtrain    ytrain    class j   k i   eye n  
 emtestlik   j   l    factorlik xtest  mu  lambda   lambda   psi    lik i j    prob
 mu  lambda  psi  lik  iter    factorppca xtrain    ytrain    class j   k i   eye n 
 ppcatestlik   j   l    factorlik xtest  mu  lambda   lambda   psi    lik i j    pr
end
 classification error on training samples
ppcaerror i    classifyerror ppcalik  y  
ppca error i    classifyerror ppca lik  y  
ppca  error i    classifyerror ppca  lik  y  

fifactor analysis using ppca

  

ppcainiterror i    classifyerror ppcainitlik  y  
ppcatesterror i    classifyerror ppcatestlik  ytest  
emerror i    classifyerror emlik  y  
em error i    classifyerror em lik  y  
em  error i    classifyerror em  lik  y  
eminiterror i    classifyerror eminitlik  y  
emtesterror i    classifyerror emtestlik  ytest  
end
 computing gda
 gdaerror  mu  sigma    gda xtrain  ytrain  xtest  ytest   gdaerror
  gdaerror   ones    length k   
 plots
x   ones   length k     defining x axess of plots

                                                                                            
  unnormalized
                                                                                            
 plotting time it takes
hold off figure    plot  x  k  emtime    bo  x  k  ppcatime 
 ro  x      k  em time    kd  x      k  ppca time   gd   hold
on plot  x      k  em  time    bx  x      k  ppca  time   rx  x
     k  eminittime    ks  x      k  ppcainittime   gs  
xlabel algorithms   ylabel k   zlabel time   title  full
set   time vs dim per alg   legend em  ppca  em   ppca  
em    ppca    eminit  ppcainit   view    
 plotting number of iterations
hold off figure    plot  x  k  emiter    bo  x  k  ppcaiter 
 ro  x      k  em iter    kd  x      k  ppca iter   gd   hold
on plot  x      k  em  iter    bx  x      k  ppca  iter   rx  x
     k  eminititer    ks  x      k  ppcainititer   gs  
xlabel algorithms   ylabel k   zlabel number of iterations  
title  full set     iter vs dim per alg   legend em  ppca 
em   ppca   em    ppca    eminit  ppcainit   view    
 plotting error per dimension
hold off figure    plot  x  k  emerror    bo  x  k  ppcaerror 
 ro  x      k  em error    kd  x      k  ppca error   gd  
hold on plot  x      k  em  error    bx  x      k  ppca  error 
 rx  x      k  eminiterror    ks  x      k  ppcainiterror 
 gs   xlabel algorithms   ylabel k   zlabel percent
classification error   title  full set   error vs dim per alg  
legend em  ppca  em   ppca   em    ppca    eminit 
ppcainit   view    

fi  

factor analysis using ppca

 plotting test error
hold off figure    plot k  gdaerror    kx  k  emtesterror    bo 
k  ppcatesterror   rd   xlabel k   ylabel percent
classification error  
title test set       
legend gda  em  ppca  
 plotting likelihood per dimension per class
for i     length k 
for j     numclass
figure  i      numclass   j     
 finding data points for ppca em
pmax   find plikiter i     j       
emax   find elikiter i     j       
x      min pmax     emax         
py   plikiter i  x  j  
ey   elikiter i  x  j  

 figure n    
 find when it ends
 find when it ends
 find the minimum
 pull out values
 pull out values

 plotting them
plot x  py   ro  x  ey   bd  
xlabel iteration  
ylabel likelihood of training data  
title  k     num str k i       full set   sum of loglik per iteration  class  
legend factorppca  em  
end
end

   

data format

                                    
  cs     real data sets
                                     
  note  this problem places in each column a training sample 
                            
 turning off warnings
                           
s   warning off  all  
                                                      
  preprocess data for badges
                                                      
clear
max      
x      
fid   fopen badges data  rt  

 clear variables
 maximum dimension to consider
 defining x
 open file

fifactor analysis using ppca

fgetl fid  
 skip first whie space
i     
 currently at  th element
maxdim     
 currently max dimension is  
while feof fid      
 until end of file
i   i     
 go to next element
tline   fgetl fid  
 read   line
y i        tline          
 set y     if      if dim   min length tline       max    whats this dimension  if   max  
maxdim   max maxdim  dim  
 whats largest size dimension 
x    x    double tline    dim         zeros max   dim      
 concatenate x with new input
end
x   x   maxdim     
 shorten x to valid dimensions
fclose fid  
 close file
test x  y  badges  
 testing it on read data
                                                      
  preprocess data for flags
                                                      
clear
 clear variables
x      
 defining x
i     
 currently at  th element
fid   fopen flag data  rt  
 open file
while feof fid      
 until end of file
i   i     
 go to next element
tline   fgetl fid  
 read   line
mat   regexp tline        s   w   match  
y i       mat    
 storing classification
x   i    str double mat                      dropping class  text    numeric
end
fclose fid  
 close file
test x  str double y   flags  
 testing it on read data
                                                      
  preprocess data for balance scale
                                                      
clear
 clear variables
x      
 defining x
i     
 currently at  th element
fid   fopen balance scale data  rt  
 open file
while feof fid      
 until end of file
i   i     
 go to next element
tline   fgetl fid  
 read   line
mat   regexp tline        s   w   match  
y i       double tline     
 storing classification
x   i    str double mat  
 text    numeric
end
fclose fid  
 close file

  

fi  

test x  y  balance scale  

factor analysis using ppca

 testing it on read data

                                                      
  preprocess data for machine
                                                      
clear
 clear variables
x      
 defining x
i     
 currently at  th element
fid   fopen machine data  rt  
 open file
while feof fid      
 until end of file
i   i     
 go to next element
tline   fgetl fid  
 read   line
mat   regexp tline        s   w   match  
y i       mat    
 storing classification
x   i    str double mat       
 text    numeric
end
fclose fid  
 close file
y   str double y  
 converting to numeric
test x  y   mean y   machine  
 testing it on read data
                                                      
  preprocess data for house votes
                                                      
clear
 clear variables
x      
 defining x
j     
 currently at  th element
fid   fopen house votes    data  rt  
 open file
while feof fid      
 until end of file
j   j     
 go to next element
tline   fgetl fid  
 read   line
mat   regexp tline        s   s  match  
for i     length mat 
 for each dimension
if strcmp mat i  n 
 if voted no
x i j       
 store   
else
 otherwise either       or y    
x i j    strcmp mat i  y  
 store   if yes or   o w
end
end
y j      strcmp tline    r  
   for republican    democrat
end
fclose fid  
 close file
test x  y  votes  
 testing it on read data
                                                                 
  preprocess data for random test sample
                                                                 
clear
 clear variables
n     
 number of dimensions

fifactor analysis using ppca

m      
 number of samples
k     
 smaller dimensions
psi   diag rand n     
 random psi
lambda   rand n k  
 random lambda
mu   rand n    
 mean for positive class
x   mu   ones   m    lambda   randn k  m    psi      randn n  m  
mu        rand n    
 mean for negative class
x    x  mu   ones   m    lambda   randn k  m    psi      randn n 
m   
y    ones m     zeros m     
 inputting classification
test x  y  random  
 testing it on read data

  

fi  

  

factor analysis using ppca

appendix e   more results

these results are the plots  similar to above of likelihoods 
 full set   time vs dim per alg

 full set     iter vs dim per alg

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

 

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

    
   

   
   
   
number of iterations

time

 

   

 

   
   
   
   
   

   

   

 

 

 

  

   
 

 
   

   
 

   
 

   
 

   

 

 
   

k

 
  

 
   

 

   
 

 

   

algorithms

 a 

k

 

 

algorithms

 b 

figure    time convergence  random  machine
the rest of the plots associated with this test are not replicated below for sake of alleviating
redundancy as they are similar to the plots below  they are available upon request however as
they reflect the benefits of the new convergence test 

fifactor analysis using ppca

  

the plots below are the results of the other tests we executed on these algorithms  they are
reproduced here for readers critique 
test set    
  

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

   

gda
em
ppca

   
factorppca
em

  

factorppca
em

   

   

   

   

  

likelihood of training data

likelihood of training data

percent classification error

  
   

   

   

   

   

   

  

   

   

   

   

  

  

 

   

 

   

 
k

   

 

   

   

 

 

 

 

 a 

 
iteration

 

 

   

 

 

   

 

   

 b 

 

   
iteration

 

   

 

   

 

 c 

figure    tested on random data set

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

   

   

factorppca
em

factorppca
em

   

   

   

   

   

   

   

   

   

   

likelihood of training data

likelihood of training data

likelihood of training data

k       full set   sum of loglik per iterationclass    

   
factorppca
em

   

   

   

   

   

   

   

   

   

   

   

   

 

 

 

 

 
  
iteration

 a 

  

  

  

  

   

 

 

 

 
iteration

 

 

 

   

 

 b 

figure    tested on random data set

 

 

 

 
iteration

 c 

  

  

  

fi  

factor analysis using ppca

test set    

 

  

  

k       full set   sum of loglik per iterationclass    

x   

 

k       full set   sum of loglik per iterationclass    

gda
em
ppca

    

factorppca
em

factorppca
em
    

 

  
    

 

  

  

likelihood of training data

    

  

likelihood of training data

percent classification error

  

    

    

 

    

  

 

    

 

  
    

 

 

 

   

 

   

 
k

   

 

   

    

 

 

  

  

  

 a 

  
  
iteration

  

  

  

  

  

 

  

  

  

 b 

  
iteration

  

  

  

  

 c 

figure    tested on machine data sample

 

 

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

x   

 

 

    

k       full set   sum of loglik per iterationclass    

x   

factorppca
em

factorppca
em

factorppca
em

    

   

 
    

 

    

    

    

likelihood of training data

 
likelihood of training data

likelihood of training data

    

 

   

 

   

 
    

 
    

  
   
    

    

 

 

 

 

 
iteration

 a 

  

  

  

  

  

 

  

  

  

  
iteration

  

  

  

  

 

 

 b 

figure    tested on machine data sample

 

 

 

 
iteration

 c 

  

  

  

  

fifactor analysis using ppca

  

test set    
   
gda
em
ppca

 full set   error vs dim per alg
 full set     iter vs dim per alg
em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

   

  
 

percent classification error

   

number of iterations

   
   
   
   
   

 

   

percent classification error

    

em
ppca
em 
ppca 
em  
ppca  
eminit
ppcainit

 
 
 
 

 

   

 

   

 

 

   

 

 

  

  

  

   

   
 

 
   

 

   

 
 

 
 

 

 

   

   
k

   

 

 
  

k

algorithms

 

 

 a 

 

 

  

  

k

algorithms

 b 

 c 

figure     tested on votes data sample

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

    

    
factorppca
em

factorppca
em

    

    

    

    

    

    

    

    

    

    

likelihood of training data

    

likelihood of training data

likelihood of training data

k       full set   sum of loglik per iterationclass    

    
factorppca
em

    

    

    

    

    

    

    

    

    

    

    

    

    

    

 

 

 

 
iteration

 a 

 

 

 

    

 

 

 

 
iteration

 

 

 

    

 b 

figure     tested on votes data sample

 

 

 

 
iteration

 c 

 

 

 

fi  

factor analysis using ppca

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

    

k       full set   sum of loglik per iterationclass    

    

    

factorppca
em

factorppca
em

    

    

    

    

    

    

factorppca
em

    

    

    
likelihood of training data

likelihood of training data

likelihood of training data

    

    

    

    

    

    

    

    

    

    

    

    

    

 

 

 

 

 
iteration

 

 

 

    

 

 

 

 

 

 a 

 
iteration

 

 

 

    

 

 

 

 

 

 b 

 
 
iteration

 

 

 

  

 c 

figure     tested on votes data sample

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

    

factorppca
em

factorppca
em

    

    

    

    
likelihood of training data

likelihood of training data

likelihood of training data

factorppca
em

    

    

    

    

    

    

    

    
 

k       full set   sum of loglik per iterationclass    

    

    

 

 

 

 
 
iteration

 

 

 

    

    

    

    
 

  

    

 

 

 

 

  

  

    
 

  

 

 

 

 
 
iteration

iteration

 a 

 b 

 

 

 

  

 c 

figure     tested on votes data sample

k        full set   sum of loglik per iterationclass    

k        full set   sum of loglik per iterationclass    

    

    
factorppca
em

factorppca
em

    

    

    

    

    
likelihood of training data

    
likelihood of training data

likelihood of training data

    

    
 

k        full set   sum of loglik per iterationclass    

    

factorppca
em

    

    

    

    

 

 

 

 
iteration

 a 

  

  

  

    
 

    

    

    

    

 

 

 

 

 
iteration

 

 

 

  

  

    
 

 b 

figure     tested on votes data sample

 

 

 

 

 
iteration

 c 

 

 

 

  

  

fifactor analysis using ppca

k        full set   sum of loglik per iterationclass    

k        full set   sum of loglik per iterationclass    

    

k        full set   sum of loglik per iterationclass    

    
factorppca
em

    

factorppca
em

    

    

    

    

    

factorppca
em
    
likelihood of training data

likelihood of training data

    
likelihood of training data

  

    

    

    

    

 

 

 

 

  

  

    
 

  

    

    

    

    

    
 

    

 

 

 
iteration

iteration

 a 

 

 

    
 

 

 

 

 b 

 
iteration

 

  

  

 c 

figure     tested on votes data sample
k       full set   sum of loglik per iterationclass    
    
factorppca
em
    

    

likelihood of training data

    

    

    

    

    

    

    

 

 

 

 
iteration

 

  

  

 a 

figure     tested on votes data sample
test set    
  

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

   

gda
em
ppca

    

factorppca
em

factorppca
em

  
    

   

  

  

  

likelihood of training data

likelihood of training data

percent classification error

    

   

  

   

    

    

   
    

  

   
    

  

 

 

   

   

   

   

 
k

 a 

   

   

   

   

 

   

 

   

 

   

 
iteration

   

 

   

 

    

 b 

figure     tested on balance data sample

 

   

 

   

 
iteration

 c 

   

 

   

 

fi  

factor analysis using ppca

k       full set   sum of loglik per iterationclass    

k       full set   sum of loglik per iterationclass    

   

k       full set   sum of loglik per iterationclass    

    

    

factorppca
em

factorppca
em

   

factorppca
em

    

    

    

    

   

   

likelihood of training data

likelihood of training data

likelihood of training data

   

    

    

    

    

    

    

   
    

    

    

    

   

   

 

   

 

   

 
iteration

   

 

   

 

    

 a 

 

   

 

   

 
iteration

   

 

   

 

    

 b 

 

   

 

   

 
iteration

   

 

   

 

 c 

figure     tested on balance data sample

references
    uci machine learning repository 
    gene h  golub and charles f  van loan  matrix computations  the johns hopkins university
press  baltimore  md  usa       
    roger a  horn and charles r  johnson  matrix analysis  cambridge university press  new
york  ny  usa       
    andrew y  ng  factor analysis cs    lecture notes 
    m  tipping and c  bishop  probabilistic principal component analysis  journal of the royal
statistical society  series b     part                 

fi