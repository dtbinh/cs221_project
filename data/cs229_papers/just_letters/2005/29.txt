daniel a  woods

cs    final project writeup
december         

original project description
title 
a discriminative learning model for rna secondary structure prediction
non cs    collaborators 
chuong  tom  do   andrew ng lab
empirical discovery of rna secondary structure is expensive and time consuming  but is a necessary part of
exploring function  software tools exist for performing these predictions  the best of which either heuristic
physics modeling or generative learning models  currently  the best of each are approximately equal in
performance 
while perfect predictions would require physics modeling well beyond our current computational capabilities  current levels of performance are much lower than perfect  i believe it may be possible to create a
program better than the current machine learning methods by making two improvements 
    current methods model rna sequence and secondary structure as stochastic context free grammars 
and then use a generative learning model to find the most likely parse  and  therefore  the most likely structure   as we learned in class  discriminative models generally enjoy higher performance than generative
learning models  this implies that performance may increase if discriminative learning were implied on top
of the same stochastic context free grammar model of rna sequence and secondary structure 
    current software tools return the most likely structure of a given rna molecule  however  it may
be possible that a particular substructure is most likely among all possible structures  but it simply does
not occur in the most likely overall structure  in order to increase overall predictive accuracy  i would prefer
to return the most likely structure on a part by part basis rather than to return the most likely overall
structure  i believe this would be more useful biologically because software predictions are never assumed
accurate  but rather are the first step leading to manual refinement 

background
the first thing we did was to find the most recent work done on the subject of using machine learning
applied to rna secondary structure prediction  which turns out to be a recent paper by robin d dowell
and sean r eddy      it models rna secondary structure as a stochastic context free grammar  scfg  
and learns using a model very similar to an hmm  substituting the scfg in place of the state machine 
this is a generative model  as alluded to in the original proposal 
an hmm can be converted to a discriminative model using using conditional random fields  crfs      
our plan is to improve on dowell and eddys algorithm by doing what crfs do improve on to hmms  the
problem is very analogous and crfs translate over nicely without any significant mathematical obstacles 

fiscfgs
here is a sample grammar from eddys paper  which is referred to as g  

g    s  asa   as   sa   ss   

here  the asa production refers to a pairing of two produced bases a and a  this rule is actually shorthand
for all the paired productions that are possible  a a  a c  a g        although some of these pairings are
chemically impossible  we let the algorithm learn this fact rather than enforce it manually   this scfg
is a straightforward and simple representation of the structures possible with rna  however  it performs
extremely poorly as will be shown after introduction of another scfg  g  

g    s
l
f

 ls   l
 af a   a
 af a   ls

the performance of these  compared to mfold  is as follows 
generative g          
generative g          
mfold v              
note  these scores are given in the form sensitivity   specificity    where sensitivity refers to the percentage
of correct pairing that were predicted and specificity refers to the percentage of predicted pairings that are
correct 
eddy does evaluate several other grammars  but g  is relatively simple and performs nearly as well as
the best  so it was selected as the first candidate to create a discriminative model for 

predictions
given a set of weights  the probability of a parse y given a sequence x can be calculated as follows 
t

ew f  x y 
p  y   y x   x    x t
 
ew f  x y  
y   y

here  f  x  y  refers to a vector of feature counts for x and y  and w is the set of weights which must be
learned  see training section   y refers to the set of all possible parses  as used in the denominator to
form the partition function 
more generally  what we need is to find the probability that y is part of some set of parses a  for example  as will be important for posterior decoding   we can use this generalization to find the probability
 

fithat the bases at locations i and j are paired by calculating the probability that y is in the set of all parses
in which i and j are paired  this more general form is as follows 
x t
ew f  x y 
y   a

p  y  a x   x    x

ew

t

f  x y    

y   y

training
training refers to optimization of w according to a training set  and subject to the array of regularization parameters c  w has a gaussian prior   optimizing w requires taking a gradient of the likelihood of
the correct parse  or parses  in the case of ambiguous grammars  with respect to w 

 w   w 

 

x

x
ew f  x y 
ew f  x y 
f  x  y  x t

f  x  y  x t
  c  w
 
 
ew f  x y   yy
ew f  x y  
ya
t

t

y   y

y   y

  eyp  y  x x y a   f  x  y    eyp  y  x x   f  x  y     c  w
although a simple gradient decent could be used here  we opted for l bfgs  which performs the same
function but converges more quickly 

posterior decoding
as mentioned in the original statement of the problem  we would like to maximize the percent accuracy
of our predictions rather than simply returning the single parse which is most likely  the latter could be
calculated by finding the parse with the highest probability  but our approach requires an additional step 
using techniques alluded to in the predictions section  we can find for any i and j the probability pi j that
the bases at those locations will be paired  similarly  we can find for any location i the probability pi that
its corresponding base is unpaired 
it is a straightforward dynamic programming implementation to maximize the overall score given these
probabilities according to the following recurrence 

 




p
 i     score i      j 
pj   score i  j    
score i  j    max


 m pi   j   pj i       score i      j    


score i  k    score k  j 

if
if
if
if
if

i j
i j
i j
i   j
i k j

here  m is a parameters that can be used to adjust the overall propensity to create pairings  as will be seen
in the results section  we adjusted m until our discriminative model had the same sensitivity as eddys
generative model  thus allowing a straightforward comparison on the basis of specificity 

 

firesults
our results showed a significant improvement on eddys generative model for the g  grammar  we performed both of these tests ourselves  using separate data sets for training and testing 
generative g              
discriminative g              
again  we adjusted m until the sensitivity levels were very close  so that an even comparison could be
made of specificity levels 

conclusions
part of what originally caught our attention about this project was the observation that eddys performance
was comparable to the best physics based algorithms using something extremely simple and lightweight 
now that we have demonstrated that the change to a discriminative model really does offer significant improvement in this simple case  we will attempt to create a much richer model in hopes of achieving results
similar to  or even better than  those provided by mfold  which has been the standard for over two decades  
rna secondary structure prediction is critical to medical research  and an improved algorithm would be
extremely helpful in aiding advancement of biology and medicine 

references
   robin d dowell  sean r eddy  evaluation of several lightweight stochastic context free grammars for rna secondary structure prediction  bmc bioinformatics           
   john lafferty  andrew mccallum  fernando pereira  conditional random fields  probabilistic
models for segmenting and labeling sequence data  proc    th international conf  on machine
learning     
   michael zucker  mfold web server for nucleic acid folding and hybridization prediction  nucleic
acids research                   

 

fi