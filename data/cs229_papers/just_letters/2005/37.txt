named entity recognition
vijay krishnan and vignesh ganapathy
 vijayk   vignesh  cs stanford edu
december         
abstract
named entity recognition  ner  is a subtask of information extraction that seeks
to locate and classify atomic elements in text into predefined categories such as the
names of persons  organizations  locations  expressions of times  quantities  monetary values  percentages  etc  we use the javanlp repository http   nlp stanford edu javanlp   
for its implementation of a conditional random field crf  and a conditional markov
model cmm   also called a maximum entropy markov model  we have obtained results
on majority voting with different labeling schemes  with backward and forward parsing
of the cmm  and also some results when we trained a decision tree to take a decision
based on the outputs of the different labeling schemes  we have also tried to solve the
problem of label inconsistency issue by attempting the naive approach of enforcing hard
label consistency by choosing the majority entity for a sequence of tokens  in the specific
test document  as well as the whole test corpus  and managed to get reasonable gains 
we also attempted soft label consistency in the following way  we use a portion of the
training data to train a crf to make predictions on the rest of the train data and on the
test data  we then train a second crf with the majority label predictions as additional
input features 

 

introduction

named entity recognition  ner  is a subtask of information extraction that seeks to locate
and classify atomic elements in text into predefined categories such as the names of persons 
organizations  locations  expressions of times  quantities  monetary values  percentages  etc 
existing approaches to ner have explored exploiting 
 word features of the token and the words in its neighborhood 
 the parts of speech of the word in question and its neighbors 
 features corresponding to certain prefixes and suffixes of the word and its neighbors 
 features corresponding to the labels of its neighbors 
existing approaches have used maximum entropy taggers  svms and crfs for ner  we
used the conference on computational natural language learning conll  dataset in order
i

fito train and test our ner tagger  the conll dataset is a standard benchmark used in the
literature  our training data was ner annotated text with about          tokens  while the
test data contained around        tokens 

 

combining results of different labeling schemes

there are two broad representations for named entity chunks i e  inside outside representation
and representations encoding start end information  the latter is more useful if we have
instance of named entities of the same kind immediately following each other since it enables
us to locate the boundaries 
we have collected the existing training data and modified it to represent them in the forms
iob   iob   ioe   ioe   iobes and io formats  these are described below 
 iob   here  i is a token inside a chunk  o is a token outside a chunk and b is the
beginning of chunk immediately following another chunk of the same named entity 
 iob   it is same as iob   except that a b tag is given for every token  which exists at
the beginning of the chunk 
 ioe   an e tag used to mark the last token of a chunk immediately preceding another
chunk of the same named entity 
 ioe   it is same as ioe   except that an e tag is given for every token  which exists at
the end of the chunk 
 start end  this consists of the tags b  e  i  s or o where s is used to represent a
chunk containing a single token  chunks of length greater than or equal to two always
start with the b tag and end with the e tag 
 io  here  only the i and o labels are used  this therefore cannot distinguish between
adjacent chunks of the same named entity 
there are existing machine learning techniques such as hidden markov models  hmm  
maximum entropy me  and support vector machines  svms  used for ner  currently we
are working with an existing implementation of the maximum entropy model from the javanlp
repository  the features considered here for each word w i  are the two words on either side
of the word i   their parts of speech pos   and their labels  in addition a few morphological
features like focus word prefix  focus word suffix  previous word prefixes and suffixes 
there are parsing methods considered for the data one is forward parsing which parses from
left to right and the other is backward parsing which parses from the right to left  so  with
six different chunks representation schemes and two parsing techniques  we have    different
outputs 
we converted back the outputs of the    different models to the iob  scheme and then
implemented a simple majority vote of the decisions of the different taggers  we managed
to get small gains on the raw token tagging accuracy but got mixed results with regard to
performance on the f  score  we also trained a decision tree on the output of the different
taggers  in order to make a decision jointly based on the different outputs  with the objective
of combining the outputs in a better fashion than giving each model equal weight 
ii

fifigure    f  scores obtained with different labeling schemes with the conditional markov
model cmm  run backward and reverse  also shown are the f  scores corresponding to a
majority voting of the predictions of these different labelings and the f  scores  for the case
where a decision tree gave the output label  taking as input  the output label of the    different
taggers 

   

results

it can be seen from figure   that among the six different labeling schemes  ioe  and iobes tagging gave marginally better results as compared to others using the forward parsing technique 
the majority vote gave mixed results for different datasets  but typically tended to perform
close to the best of the labeling schemes we also calculated the f  score for an oracle tagger
which tags the entity correctly if any one of the twelve outputs tagged the entity correctly  this
result is a kind of theoretical upper bound estimate for the best f  score that we can possibly
get using this combination of multiple labeling schemes 

 

label consistency

we performed manual analysis of the error to find out other ways to improve the f  score of
the results of different labeling schemes  a good number of errors could be fixed by proper
application of label consistencies  an example of this is melbourne cricket ground which
occurred often in the text and was tagged correctly for some instances and incorrectly for other
instances 
it is very likely that multiple occurrences of sequences of tokens correspond to the same
entity especially if they are close to each other  since these are non local dependencies  we need
approximate methods to capture them  finkel et al  capture label consistency at the document
level  by fixing a penalty for disagreement among labelings for the same sequence of tokens  at
its different occurrences in a document  and used gibbs sampling for tractable inferencing 
iii

fifigure    graph showing the f  scores with the default crf  the f  scores corresponding to
enforcement of hard label consistency at the corpus and document levels respectively  and the
f  score with soft label consistency with   rounds of crf training   the last two f  scores
correspond to the oracle scores at the corpus and document level respectively  the oracle
scores are obtained by awarding a point if the entity is labeled correctly in any part of the
corpus document 
we find that there is scope for improvement even with label consistency at the corpus level 
we would intuitively expect that there should be smaller penalty for label disagreement of
a token at the corpus level  when compared to disagreement with labelings within a single
document  however  there are relatively small number of cases wherein a named entity token
appears multiple times in a single document  when compared to its count across the corpus 
thus  label consistency over the whole corpus is quite a rich source of information  which could
be exploited to get good gains on the f  measure 
we obtained improvements with even naive hard label consistency enforced at the document
and corpus levels  we enforced this hard consistency by doing post processing on the output of
the crf  for multiple occurrences of a sequence of tokens in a named entity  we changed the
labeling of all occurrences to the majority labeling for that sequence of tokens over the whole
corpus  we also attempted a similar experiment  wherein we flipped the labels to the majority
labeling within a document 
we also attempted soft label consistency in the following way  we use a portion of the
training data to train a crf to make predictions on the rest of the train data and on the test
data  we then train a second crf with the majority label predictions as additional input
features 

   

results

it can be seen from figure   that the improvement in the f  score is significant in case of the
corpus level majority vote as compared to the document level vote  the result of using these two
iv

fimajority vote results as additional input features for the crf did not give any improvement 
the results with the oracle taggers  depict a loose upper bound on the extent to which we
could hope to improve performance with label consistency 

 

conclusions and future work

majority voting over different labeling schemes on the cmm gave us mixed results with different
data sets  in most cases it did reasonably well while the decision tree method performed poorly
in comparison 
capturing of label consistency is a rich source of information  which we also observed during
error analysis  we could get improvements with even naive enforcement of hard consistency
via majority voting  we could not get improvements using label consistency majority vote
information obtained from a crf trained on different data  as additional features  we could
look at better ways to encode the soft label consistency which is a non local dependency  one
option is to attempt using markov random fields  another option is to train a crf without
label consistency incorporated and independently learn a model for label consistency alone 
both
in addition  we are currently not able to do significantly better with crfs than with cmms 
this is because  while the cmm can be given a previous context of say five tokens  we cannot
give a previous context of more than one or two tokens to the crf since that exponentially
blows up training and testing time  this could be possibly handled by pruning down the search
space of the crf possibly by getting the best three labels for each token without considering
a larger context and then finally searching over these labelings alone 

acknowledgments
we would like to thank prof  chris manning his help and guidance  we would also like to
thank jenny finkel and trond grenager for the highly fruitful and enlightening discussions we
shared 

references
    kudo t and matsumoto y        chunking with support vector machines  in naacl      pages         
    erik tjong and kim sang        memory based named entity recognition  in conll      pages         
    john lafferty  andrew mccallum and fernando pererira        conditional random fields 
probabilistic models for segmenting and labeling sequence data  in icml      
    jenny finkel  trond grenager  and christopher d  manning        incorporating non local
information into information extraction systems by gibbs sampling  in acl      
    javanlp code documentation  online at  http   nlp stanford edu nlp javadoc javanlp 

v

fi