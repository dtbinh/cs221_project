cs     project report
younggon kim  younggon stanford edu 

smarter text input system for mobile phone
   abstract
machine learning algorithm was applied to text input system called t   support vector machine
 svm  is used to predict the word  the experiment shows the algorithm can reduces error by
       

   background
     t  system
   alphabets are mapped to   numeric keys  from   to     a user presses one key for each
alphabetic letter  for example   ace  is entered by pressing        because mapping is manyto one  converting sequence of number to alphabetic word can be ambiguous  a conventional
phone picks most common word among the valid words in a dictionary 
if the word displayed is not the user want  the user press   to display other word until the right
word appears  if word is right  the user presses   to enter a space and moves on 

     room for improvement
conventional algorithm is context insensitive  in other words  it does not look at other words to
predict  if context is used for decision  the accuracy of prediction might be improved 

   framework
     notation
here are several terminology used throughout the paper 
token   a word   sequence of alphabet letters  token does not contain white space 
encode    a function which maps token to code  as described in     
code  a sequence of numbers mapped from a token 
candidates    a function which maps code to possible tokens 
i e  candidates code     token   encode token    code 

fi     problem formulation
using these definitions  the problem can be formalized as following 
input  sequence of tokens followed by code
tokenk  tokenk     token   token   code
problem  choose right token among candidates code 
note that there is only one code in the input  in t  system  a user gives feedback for each word
entered  therefore  the system knows the right tokens for all previously entered codes 

     input text
it is good to have an enough input text to train and verify  i decided to use free text files
available in http   www gutenberg org   the site has a number of books which does not have
copyright problems  for example  it has the adventure of sherlock holmes by conan doyle and
the war of the worlds by h  g  wells 
the novels do not represent well texts used in messaging  they are more formal and tend to
have long sentence  for the sake of getting the data  this project confined the scope to written
language 

   algorithms
     simple word counting
this algorithm is used to emulate the behavior of conventional phone  additionally  this
algorithm serves as a reference to evaluate proposed algorithm 
word counting algorithm is simple  it keeps counts for each word appearing in the training
examples  prediction is made by comparing the counts among candidates code   a token which
has highest counts is chosen 

     svm
     binary classification
lets begin discussion with a binary classification  the algorithm will be extended to multiclass
case in the later section 
in a binary case  number of candidates is two  for example  code    has two candidates   of
and me  every sentence includes of and me is a training example for code    

filets say that the sentence is give me a box of chocolate  this sentence has two training
examples for code    
input sentence  give me a box of chocolate
example   for code     give
example   for code     give me a box
example   is classified as me and labeled as    while example   is classified as of and
labeled as    
     feature vector
each example need to be converted to feature vector for processing  for the sake of simplicity 
assume total number of tokens is six 
token   a
token   box
token   chocolate
token   give
token   me
token   of
feature vector indicates whether example has the token or not  using above examples 
vector for example                
vector for example                
     encoding a distance
above scheme does not take the distance into account  in example    give is right before the
code while give in example   is four tokens away from the code  intuitively  the correlation
will decrease as the distance gets longer  to encode this knowledge  exponential function  distance

is used instead of    following is a result from this encoding 

vector for example                
vector for example                      
     evaluation
once svm algorithm learns from examples  each code has its own w and b parameters  to
predict token  calculate w    feature vector of test case    b 

fi     extending to multiclass
binary classification algorithm can be run several times to support multiclass  for example 
code      has three candidates  mind  mine and nine  svm algorithms are run three times with
different classification labels  in the first run  examples for token mind are labeled as    and
others are labeled as     similar steps are taken for second and third run 
after algorithm learns  each candidate has w and b  evaluation requires several calculations  i e 
calculate w    feature vector of test case    b for each candidate  choose the token which has
the largest value 

   implementation
script language ruby is chosen because of its flexibility and string  hash adt  initial
implementation is too slow to run on a large dataset  a few optimizations are made  some is a
trade off between accuracy and speed 
     sparse vector
feature vector is sparse  instead of using basic array  customized adt is built  dot product
between feature vector became much faster  it sped up the algorithm considerably  because each
learn requires m m   inner products  where m is the number of training examples 
     limiting distance
if token is far away from the code  it does not contribute much to predict  max distance is
limited to max distance which is adjustable parameter  any token has larger distance than
max distance is ignored when building feature vector 
several experiment had been conducted by varying max distance    was chosen because
going above   marginally increased accuracy 
     limiting number of training examples
number of training example has direct impact on running time  some common word such as
of has a lot of training examples  to set upper bound of running time  maximum number of
training example is limited to max num examples  each candidate can not have more
than max num examples examples  if any candidate has more than this  extra examples
are randomly dropped 
for example  if max num examples        maximum training examples for code    is
    which is composed of     for token of and     for token me
max num examples had a direct impact on accuracy as well  more on this will be

fidiscussed in results section 

   training and evaluation
    of sentence is used for training and     is used for evaluation  two different modes of
experiment were conducted  in the first experiment  only one book  adventures of sherlock
holmes  is used  in the second experiment  eight books are used  the input to the second
experiment should have more variety on the selection of words and style of writing 
in the evaluation phase  each word in the sentence is predicted by two algorithms  there were
several cases 
trivial  only one candidate exists  no prediction is necessary
out of dictionary  the algorithm has never seen the token in the training set  both algorithms
can not predict correctly 
no context  test case does not have enough contexts to use svm  there are two cases  one case
is when the word is at the beginning of sentence  the other case is when test vector has no
overlap with any training examples  in other words  w    feature vector of test case  becomes
zero for all candidates 
non trivial  all other cases in which both algorithms competes 

   results
     experiment    one book with        words
word distribution

 
    
    
   

fiff

 
    
         

ff

fiprediction result

     fi 
   fi 
   fi 
   fi 
  fi 
 fi 

     fi   

     fi   

  fi        

  fi        

   fi      fi          

     

     experiment    eight books with         words
word distribution

acb d e d f g
hfii j
w o t n b d e d f g
  v j

kml noqp
w o s  o r td ns x n yzd o n t fqb u
  j v j
 
prediction result

     fi 
   fi 
   fi 
   fi 
  fi 
 fi 

     fi   

     fi   

  fi        

  fi        

   fi      fi          

     

     comparison
in experiment    svms number of error is     of word countings  in experiment    svms

finumber of error is     of word countings  svm outperformed word counting by factor of  
when one book is used as input  this advantage is reduced when eight books are used as input 
this is related to the fact that same max num examples of     is used for both
experiments  second experiment has input data set more than    times larger than the data of
first experiment  to achieve similar performance  max num examples should have been
increased as well  due to the large running time  however  max num examples was kept
same 

   conclusion
svm is implemented to improve accuracy of t  input system  the error was reduced by     in
the best case  however  long running time of the algorithm prevented it to maintain same
performance in a large dataset 

fi