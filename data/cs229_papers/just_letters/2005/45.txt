cs     course project  fall       dec          
l  regularized logistic regression
tejas rakshe s  and ashish kumar
abstract
we implemented subgradient descent   like method for l  regularized logistic regression
and nonlinear conjugate gradient method for huber loss function regularized logistic
regression  we investigated various aspects of these algorithms and used them on various
datasets  obtained from the university of california at irvine repository and cs   
class  
purpose
to implement logistic regression with regularization using l  and l  like norms with
different implementation strategies and investigate their properties 
notation
the function to be minimized is of the form f          n
f   is the negative of log likelihood function of logistic regression 
    n is the regularization term  n   for l  norm          is the weight of regularization
term  the noise  if added  is such that p fraction of the training labels were flipped 
methods
 a  subgradient descent  like algorithm
motivation
l  norm is not differentiable at zero and usual gradient descent can oscillate when one of
the parameters is close to zero  to overcome the problem  we modify the update rule for
the weights  
update rule for gradient descent 
for a small positive           
if  i       perform usual update  class notes  cs    
if  i     and
   f     i       then dont change i 
if  i     and
if  f     i      then i    i        f     i       
if  f     i       then i    i        f     i       
step size    
we vary it as  iteration        sqrt iteration   boyd        advantage   the step size
slowly decreases  making the algorithm stable  ie  it converges and doesnt oscillate  
disadvantage   if the initial guess is too far from the actual solution  we may not be able
to reach the actual solution  since the step size diminishes 

fi b  nonlinear conjugate gradient  with newton raphson and fletcher reeves 
method for huber loss regularization
motivation for huber loss regularization  why is it better than l  or l  
the huber loss function approximates l  norm  and also has a desirable property of
being differentiable everywhere  unlike l  norm  its a quadratic function near the origin
and linear otherwise  such that the function is continuous and differentiable  it behaves
like an l  norm regularization for smaller residuals and like an l  norm regularization
for larger residues  its considered robust  due to the fact that it does not heavily penalize
higher residuals at the cost of lower residuals  as l  norm does   at the same time it
doesnt insist on driving residuals to zero  at the cost of leaving some residues very high
 as l  norm does   boyd        notation for the huber loss function is same as that used
in  boyd       
what is nonlinear conjugate gradient  with newton raphson and fletcherreeves       shewchuk        please refer to the paper for the details of the algorithm 
however we modified the algorithm slightly to make it more robust 
psuedocode in its original form 
modified form 
ab
 update  we use  
instead
b    
a
of
for the  update step  for a
b
small   to eliminate divide by zero
error  in case initial guess is too far
away from the actual solution   this
makes the algorithm stable  the
problem of divide by zero error
 shewchuk        is common for
conjugate gradient method 
initial point  we use a special
scheme for choosing the initial point 
we solve the problem without
regularization by gradient descent
and use the solution as the initial
guess  unregularized gradient descent
problem is computationally efficient
to solve  and gives a solution
reasonably close to the actual
solution 

experiments
we used validation scheme with     training and     testing data  with p   and p    
or      ie  with label flipping noise  

fidatasets we used the algorithm on   different datasets      homework   data for cs    
    voting patterns dataset  uc irvine      breast cancer diagnosis data  uc irvine 
please see the web links for details of the datasets 
      http   www ics uci edu  mlearn databases voting records 
      http   www ics uci edu  mlearn databases breast cancer wisconsin 
results
figure   shows each one of   datasets for subgradient like algorithm  graphs  a    b  and
 c  are for one of three datasets each  in each graph there are plots for training and
misclassification errors  with and without label flipping noise  figure   shows the same
with non linear cg algorithm with huber loss 
discussion
we implemented l  and l  like regularized logistic regression successfully with two
different methods  and demonstrated its application on three datasets  each of the
datasets comes from a different source and has different characteristics  we can see a
general trend in all graphs that regularization produces better test results with noisy
training data  in some datasets  there is a clear optimum for the regularization parameter 
where the test data error is lowest  also we note huber loss is more robust than l  norm
to outliers as can be seen by the improvement of test error in all three cases with its
respective training error for noisy data 
work in progress
  
implementation of smo like algorithm
  
use of exponential priors  joshua goodmans approach 
  
convergence analysis for all the algorithms
  
computational efficiency analysis and comparison
references
   ng  andrew y   feature selection  l  vs  l  regularization and rotational invariance 
proceedings of the   st international conference on machine learning      
   shewchuk  jonathan r   an introduction to the conjugate gradient method without
the agonizing pain  school of computer science  carnegie mellon university 
pittsburgh  pa            
   stephan boyd  lin xiao and almir mutapcic  subgradient methods  notes for
ee   o  stanford university      
   stephan boyd and lieven vandenberghe  convex optimization  cambridge
university press       
acknowledgements
we thank prof  andrew ng  rajat raina  ashutosh saxena and rion snow of cs
department  stanford university  for the invaluable help we received from them
regarding various aspects of the project  we are also grateful to people at the university
of california at irvine for making several datasets available for public use in their
repository 

fieffect of regularization on misclassification using l  norm criteria
   
   

error fraction

training error

dataset  homework  
algorithm  l  norm w  sg

test error

   
   

training error
on noisy
training data 
p    

   
   
 
 

   

 

   

 

   

 

test error on
noisy data 
p    

regularization   

fig    a 
    

training error

dataset  breastcancer
algorithm  l  norm w   sg

e rr or fra c tion

   
test error

    

   

training error on
noisy training
data  p    

    

 
 

   

 

   

 

   

 

test error on
noisy data 
p    

regularization   

fig    b 
    

dataset  voting records
algorithm  l  norm w  sg

erro r f ractio n

   

training error

test error
    

training error on
noisy training data 
p    

   

    
test error on noisy
data  p    
 
 

   

 

   
regularization   

fig   c 

 

   

 

fieffect of regularization and noise on misclassification using huber
loss criteria
   

dataset  homework   
algorithm  huber loss w  cg

training error

    

error fraction

   

test error

    
training error on
noisy training
data  p    

   

    

test error on
noisy data  p    

 
 

   

 

   

 

   

 

regularization   

fig    a 
    

dataset  breastcancer
algorithm  huber loss w   cg

training error

   

test error

error fraction

    

    
training error on
noisy training data 
p    

    
test error on noisy
data  p    

    

 
 

   

 

   

 

   

 

regularization   

fig    b 
    

dataset  voting records
algorithm  huber loss w   cg

training error

   

error fraction

test error
    

   

training error on
noisy training
data  p    

    

test error on noisy
data  p    
 
 

   

 

   

regularization   

fig    c 

 

   

 

fi