sentence unit detection without an audio signal

william morgan

  introduction and motivation
sentence unit  su  detection is the task of dividing a sequence of words into individual sentences 
su detection is a close relative of sentence boundary detection  which has been a topic of study in
the computational linguistics community for over a
decade   palmer and hearst        reynar and ratnaparkhi       
su detection is specific to the context of automatic speech recognition  asr  systems  which typically produce an unstructured sequence of words
from an audio signal  and must then recover latent
structural features in the signal such as word case
 true casing  and sentence boundaries  su detection  in order for the output to be ready for human
consumption  recent efforts by the darpa ears
program  office        to improve asr quality has
renewed interest in this problem 
work on su detection in modern asr systems
typically takes in to account the full set of features available from the audio signal  features like
prosidy  voice quality  and even  in the case of multimodal systems  visual cues like gestures have all
been shown to be informative in deciding on sentence boundaries   liu et al         stolcke et al  
     
in this study  we apply conditional random fields
 crfs  to examine the feasibility of detecting sentence boundaries directly from text  i e  without the
corresponding audio signal  these results act as a
baseline  suggesting the true usefulness of features
extracted from the audio and video for su detection 


no collaborators or advisors 

  crfs
much work on sentence unit detection has used
hidden markov models  hmms  as the underlying model   shriberg et al         renals and gotoh        christensen et al         kim and woodland        recent experiments with crfs  however  have shown they can exhibit better performance on the su detection task than hmms or maximum entropy approaches   liu et al        
a crf is an undirected graphical model of representing an event sequence e globally conditioned on
an observation sequence o  crfs are naturally applicable to many problems to which hmms have traditionally been applied applied  but unlike hmms 
which maximize the joint distribution p  e  o  
crfs directly maximize the posterior event probabilities p  e o  
the most likely sequence of events in a crf is
given by
e   arg max


e

exp  

p

k gk  e  o  
z  o 

k

where gk  e  o  are potential functions over the
events and observations  and z a normalization
term  in general the gk can be any functions  but
in many cases  including ours  it is computationally
beneficial to restrict oneself to a first order crfs  as
exemplified by figure   
in this paper we use the stanford nlp crf implementation 

  data
our data was drawn from the nist rt    evaluation  ldc publication ldc    t     nist       

fie i 

oi 

ei

oi

e i  

  metrics

oi  

in evaluating a system for su detection  it is likely
that one is concerned both with type i and type ii
errors  we measured system performance using precision and recall  which capture both these types of
errors  in our case  they are defined as

figure    a first order crf  e represents the events
 sentence boundary or not  and o represent the observations  the words of asr output  
this corpus consists of     human edited transcriptions      drawn from broadcast news and    
drawn from conversational speech  altogether comprising of    k words  of this  approximately    
was set aside as test data  the remainder was used
as training data  

  feature extraction and modeling
the crf event for su detection was encoded as
a boolean value for each word in the training data 
specifying whether that word was at the beginning
of a sentence or not 
the transcript data contained  for each word of
speech  the lexeme  written representation   start
time  duration  and speaker identification  from
these attributes we extracted the following features 
the lexeme  lower cased  to prevent confounding
from case information   the duration  the delay since
the prior word  if any   whether a speaker change
had occurred 
two additional sources of information were
added  part of speech tags for each word were determined by the stanford nlp part of speech tagger 
a bi directional cmm tagger 
additionally  a bigram language model was built
from a portion of the the ldc english gigaword
corpus  ldc publication ldc    t     the language model was based on     million words of
english text  and simply estimated  for each word
wi   the probability p  wi is the first word in a
sentence wi     
 

broadcast news and conversational speech have different
characteristics  we mainly ignore this  but do try to control for
it by drawning samples from the training corpus proportionately
from both types 

p 

  correctly marked as su
  marked as su

and

  correctly marked as su
 
  sus in the corpus
one pleasing feature of these two metrics is that
while either may be gamed individually  it is impossible to game both simultaneously  in our case  precision may be gamed by tagging only the first word
of the test set as a su  and not tagging anything else 
and recall may be gamed by tagging every word as
an su  but having high scores in both requires a tagger of both high accuracy and large coverage 
it is often convenient to combine these two scores
into a single number  typically this is done by the
harmonic mean  or f measure 
r 

f  

 p r
 
p  r

intuitively  as p approaches r  f approaches
and as they diverge  f approaches   

p  r
   

  results
table   gives the result of the experiments  somewhat surprisingly  neither the part of speech tags
  pos  nor the language model   lm  significantly affected the performance of the system as
compared to the baseline  for comparison  three
systems which randomly assign su or not su tags
to each word are given  it is clear that while the
trained systems far exceed these in precision  the difference in recall scores is much lower 
figure   shows the f measure of the baseline system on both training and test sets as a function of
training set size  near maximal scores are obtained
with fairly small training set sizes      documents  
suggesting that more work on feature extraction is
needed to improve scores 
figures   and   show the precision  recall and fmeasure of the baseline system on the test and training sets respectively  again  we see that by the time

fi  

precision
fmeasure
recall

  

metric

  

  

  
  
  
  

fmeasure

training set
test set
rand      
rand      

 

   

   

   

   

 

training set size  docs 

  
  
  

metric

  
  

precision
fmeasure
recall

   

   

   

   

   

   

training set size  docs 

figure    training vs test set performance  fmeasure  of the baseline system 

 

   

   

training set size  docs 

figure    precision  recall and f measure of the
baseline system on the test set 
the training corpus contains    documents  we have
achieved the vast majority of our final performance 
these graphs also highlight the fact that the systems
errors are primarily those of omission  not of incorrect labelling 
for comparison purposes  liu et al         report a boundary classification error rate of      
when training a crf without using only textual features  i e   the system correctly labelled        of
the words as either su or non su  our error rate by
the same metric is        which is roughly comparable  they trained on the same corpus   they used
several other textual features  such as automaticallyderived word classes  that may account for the difference  when they added prosodic features  their
error rate went down to       

  conclusion and future work
baseline system performance was a respectable fmeasure of     specifically  approximately     of
the labels the system made were correct  and it de 

figure    precision  recall and f measure of the
baseline system on the training set 
system
baseline
 pos
 lm
just   doc
random    
random    
random   

precision
     
     
     
     
     
     
     

recall
     
     
     
     
     
    
    

f measure
     
     
     
     
     
     
    

table    results of training the crf to perform su
detection  the baseline system has neither part ofspeech tags nor language model probabilities  just
  doc is identical to the baseline system but is only
trained on one document  the random systems
simply assign a su tag to each word by flipping a
coin weighted with the respective probability 
tected about     of the possible sus  however 
the fact that test set performance reached most of
its maximal value with only    documents suggests
a need for better modeling and feature extraction  it
is likely that features from the audio stream itself
would improve these scores significantly 
it is somewhat surprising that neither the language
model nor the part of speech tagger had any effect
on the system performance  more work is needed to
understand this  one approach would be to examine individual errors and compare the features and
feature weights involved to see if the source of the
errors can be pinpointed 
one interesting question still unaswered is the
performance of the system on actual asr output 
the data used in this study was exclusively humanannotated and thus did not have the characteristic er 

firors of asr output  which will likely have a negative
effect on system performance 

references
heidi christensen  steve renals  and yoshihiko gotoh 
      punctuation annotation using statistical prosody
models  in isca workshop on prosody in speech
recognition and understanding  september    
ji hwan kim and p  c  woodland        the use of
prosody in a combined system for punctuation generation and speech recognition  in proceedings of eurospeech       pages           november    
yang liu  andreas stolcke  elizabeth shriberg  and mary
harper        using conditional random fields for
sentence boundary detection in speech  in proceedings of the   rd annula meeting of the acl  pages
        june    
nist 
     
rt   f
workshop
agenda
and
presentations 
http   www nist gov speech tests rt rt     fall presentations  
november 
darpa information processing office        effective  affordable  reusable speech to text  ears  
http   www darpa mil ipto programs ears  
d  d  palmer and m  a  hearst        adaptive sentence boundary disambiguation  in proceedings of the
fourth applied conference on nlp  pages      
steve renals and yoshihiko gotoh        sentence
boundary detection in broadcast speech transcripts 
in proceedings of isca workshop  asr  challenges
for the new millenium asr       pages        
july    
j  reynar and a  ratnaparkhi        a maximum entropy approach to identifying sentence boundaries  in
proceedings of the fifth applied conference on nlp 
pages      
elizabeth shriberg  andreas stolcke  dilek z  hakkanitur  and gokan tur        prosody based automatic segmentation of speech into sentences and topics  speech communications     pages        
andreas stolcke  elizabeth shriberg  mary harper  and
yang liu        comparing and combining generative and posterior probability models  some advances
in sentence boundary detection in speech  in proceedings of the conference on empirical methods in nlp 
june    

fi