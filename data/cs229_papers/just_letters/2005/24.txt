machine learning of expressive microtiming in brazilian and reggae drumming
matt wright  music  and edgar berdahl  ee   cs        december     
abstract
we have used supervised machine learning to apply
microtiming to music specified only in terms of
exact  quantized  theoretical note times  specifically 
the input consists of a sort of musical score  note data
specifying the quantized attack times for a variety of
percussion instruments  the output of the regression
schemes we tried is simply the microtiming deviation
to apply to each note  in particular  we trained
locally weighted linear regression   k nearestneighbors  lwlr knn   kernel ridge regression
 krr   and gaussian process regression  gpr  on
data from skilled human performance of a variety of
reggae drum parts and brazilian rhythms  evaluating
our results with cross validation  we found that the
three methods are quite comparable  and in all cases
the mean squared error is substantially less than the
mean squared microtiming of the original data 
subjectively  our results are satisfactory  the applied
microtiming captures some element of musical style
and sounds much more expressive than the quantized
input 
introduction
most styles of music are based on a theoretical model
of rhythm in which the timing of notes is specified in
terms of exact integer divisions of the beat  in real
performance by skilled human musicians  a large part
of the expression comes from microtiming  small but
meaningful variations in the exact timing of notes to
produce feel  groove  or swing  computergenerated realizations using perfectly timed notes
lack microtiming and are generally characterized as
mechanical  dry  inhuman  etc 
we applied supervised machine learning to the
problem of generating stylistically appropriate
microtiming for notes specified only in terms of
theoretical exact times  i e   notes whose onset times
are exactly submultiples of the beat and that therefore
have zero microtiming  in particular  our algorithms
input is the midi score of a series of drum notes  and
the output is the microtiming to apply to each note 
midi is a symbolic representation that  for our
purposes  represents music by specifying timing 
loudness  and timbre  i e   which drum  for each note 
a musician can manipulate other parameters of a
performance in order to influence its expressiveness 
including tempo  dynamics  and articulation 
currently we address only microtiming  tempo is
constant for our training data  because it is almost
constant in these musical styles   dynamics and
articulation we leave for future work 

related work
most of the related work pertains to synthesizing
expressive concert piano music  the oldest line of
research of which we are aware began in      at the
royal institute of technology  kth  in stockholm 
where johan sundberg and others determined rules
that should govern the expressive parameters of
music     they used an iterative process by inventing
rules  asking professional musicians to comment on
the musical output of the algorithms governed by the
rules  then manually fine tuning the rules  the rules
were therefore limited by the imaginations of the
researchers  and the parameters for the rules were
approximated using a limited number of listening
tests  nevertheless  many of the rules were able to
predict performance parameters in tests on actual
performance data from mozart sonatas provided by
gerhard widmer     
however  some rules
systematically failed on certain musical phrases 
leading to the conjecture that such rules should
incorporate knowledge of the phrase structure of a
piece  at least for the domain of concert piano 
in contast  gerhard widmer sought to use
machine learning to automatically discover the
rules governing expressive concert piano music
performance  and to have these rules expressible in
simple english  in his first attempt  he applied
nearest neighbor methods  bayesian classifiers 
decision tables  decision trees  and classification rules
to the melodies of thirteen sonatas by mozart 
concluding that decision trees and classification rules
performed best      he considered only the
discretized and therefore drastically simplified
classification problem  i e   predicting whether a
given note would be played early or late  with
resulting accuracies in the neighborhood of      he
further determined that different models were needed
for different meters and tempi and that rhythmic
structure was more closely tied to tempo deviations 
while melodic structure had more influence on the
dynamics 
widmer later became interested in the idea of
partial models  which he desired to explain some
characteristics of expressive performance without
causing overfitting due to the learning algorithms
attempting to explain every training sample  as a
result  widmer developed the plcg  partition 
learn  cluster  and generalize  algorithm  based on a
sequential set covering algorithm     and some
generalization heuristics  many of the resulting

fiexpressive music performance rules coincided with
musicological theory     
next widmer added some higher level features to
further aid his learning algorithms  however  the
features were so high level that they required a
manual and somewhat subjective musical analysis of
the training data by music experts  the phrase
information was deemed the most important aspect of
these features  in particular  each note was considered
to belong to phrases of similarly behaving notes on
both smaller and larger timescales  in essence  the
learning algorithms were using human input to help
cluster perceptually related notes together     
the hierarchical nature of the new features
allowed widmer to experiment with some more
exotic inductive learning algorithms such as the
relational case based learner distall  it used a
distance measure which when applied to two phrases 
not only measured the dissimilarity between the two
phrases a and b  but also measured the dissimilarity
between the phrases related to a and b  where the
relatedness between phrases was determined using
the high level phrase analysis features     
while this method indeed shows promise  it has
some weaknesses because of which we have decided
not to implement it  in particular  widmer states that
he has had better results with the dynamics than the
timing and tempo  whereas we are most interested in
the timing  in addition  widmer makes some
assumptions about how the local dynamics and tempo
of each phrase contribute to the overall dynamics and
tempo  that is  he assumes that they are additive and
makes no musicological argument for why this
should be the case  he further chooses to
parameterize the local dynamics and tempo curves
using second order polynomials  and this decision
seems motivated more computationally than
musicologically  finally  widmer even admits that
the largest source of error in the model was the fitting
of quadratic functions to the dynamics and tempo
curves     
training data
we have purchased collections of midi sequences of
drum parts for reggae and brazilian music genres
from keyfax newmedia  expert percussionists in
each genre recorded these sequences in real time via
midi equipped acoustic drum sets and similar input
devices  in general each consists of about    bars of
a basic rhythm with minor variations  played on the
full battery of instruments  subjectively  these
sequences sound authentic  their microtiming
variations are enough to give a sense of each musical
style  we manually selected sequences that were long
enough  at least   bars   sounded authentic to us 

and had some degree of interesting microtiming 
resulting in the following collection of training data 
rhythm
  notes   insts
mean mt  std mt 
escola
    
  
      
      
olodum
    
  
      
      
sambareg    
  
      
      
rokbahia
   
 
      
      
maracana     
 
      
      
partalto
   
  
      
      
sambafnk    
  
      
      
afoxe
   
 
      
      
baiao
    
 
      
      
reggae
   
  
      
      
lvr rock
   
  
      
      
ska
   
 
      
      
table    input data by rhythm  showing number of
notes and instruments  and mean absolute value and
variance of microtiming  mt  measured in beats 
since the input to our machine learning algorithm
is the unexpressive  perfectly timed version of a
rhythm  we had to quantize these expressively timed
sequences  in some respects this is analogous to
transcribing a rhythm in written notation  because it
involves making subjective categorical judgments
about the rhythmic meaning of each note according
to the presumed underlying metrical structure  that
is to say  though the quantization process itself is
purely mechanical and deterministic  the choice of
which level of subdivision to quantize to involves
human subjectivity 
quantization actually turned out to be much more
difficult than we anticipated  because many of the
parts had some notes at the   nd note level  yet many
of the notes that we considered to be   th notes have
such large microtiming  i e   they were played so
early or so late  that they were assigned to the wrong
beat when quantized to a   nd note grid  we solved
this problem with a heuristic hybrid quantization
algorithm that quantizes each note to both the   thnote and   nd note grids  preferring the   th note
result unless multiple notes from the same timbre
collide by being quantized to the same   th note  in
which case the algorithm then uses the   nd note
quantization  in other words  this heuristic makes the
strong modeling assumption that all notes at the   ndnote level are part of an adjacent pair of   nd notes 
not part of a syncopation 
distance metric
all of the learning algorithms we tried  with the
exception of gpr  are based directly on some notion
of distance between a given input note and each note
in the training data  our goal in devising the distance
metric was to account for the factors that we believed
determine how a given note will be microtimed 

fitimbre  position in the rhythmic cycle  and the
presence of other notes nearby in time  our distance
metric has three components 
 timbral distance is zero between two notes
on the same timbre  e g   snare drum   one
between notes of totally different timbres
 e g   crash cymbal vs  bass drum  and a
subjective ad hoc number between zero and
one for instruments that sound similar
and or have similar rhythmic functions 
 metric position distance is zero between
two notes at the same point in a bar  e g  
both notes on the   thnote after beat two  
small for notes in positions that perform
similar rhythmic functions  e g   the  thnote
after beat one vs  the  th note after beat
three   and one for notes at rhythmically
dissimilar positions 
 rhythmic context difference captures the
effect on microtiming of notes played
shortly before or after the given note on the
same or different timbres  our current
implementation looks at the timbres and
relative temporal position of all notes
within one beat of the input notes  although
we could certainly extend this window if it
proves helpful 
our current distance metric is nonnegative and
symmetric  but the triangle equality does not
necessarily hold 
knearestneighbors   locally weighted linear
regression
the simplest algorithm we tried outputs a linear
combination of the microtimings for the k training
notes nearest to the given test note  weighted
weighted  inversely  by distance  for example  we
compute the weight w i j  corresponding to the
distance between the notes i and j as follows 
when k equals the size of the training set  k nearestneighbors  knn  is equivalent to locally weighted
linear regression  lwlr   this was never the
optimal value of k 
we selected k and  to minimize the mean
squared error  mse  over all our data with   fold
cross validation  resulting in k     and         
selecting k and  to minimize the mse for each
rhythm results in only a very small decrease in test
error  so we believe the k and  parameters to be
global 

kernel ridge regression
we realized that knn was not using knowledge of
the actual microtimings in deciding which training set
elements to use for prediction  so we applied kernel
ridge regression  krr  to try to improve the
performance  this formulation of a general
regression problem using kernels was very useful for
us because it allowed us to directly apply our distance
metric  which was motivated by our intuition into this
particular problem 
while our distance measure is nonnegative and
symmetric  the triangle equality does not necessarily
hold  furthermore  our distance measure is not a
mercer kernel  and so the matrix k of pair wise
distances w i j  is not positive semidefinite in general 
therefore  to ensure that we were able to backsolve
the necessary set of linear equations  we needed to
pick the magnitude of the ridge term  to be at least
slightly larger than the smallest eigenvalue of k  we
used cross validation to determine the optimal values
for  and   which suggested that  should be chosen
as small as possible  luckily  the smallest possible 
was always a few orders of magnitude smaller than
the large eigenvalues of k 
figure   shows part of the results from crossvalidation for the rhythm escola  for a given note
index  a positive stem means that a note was  blue  or
is estimated to be  red  played late  whereas a
negative stem corresponds to an early note relative to
the score  despite the difficult nature of the
regression problem  krr is able to make reasonable
estimates of many of the microtimings  a stem plot
of the results from knn also looks similar 

figure    comparison of the true microtimings
versus those predicted by krr

fikrr sometimes performed slightly better than knn
and sometimes slightly worse  see fig      perhaps
the suboptimal performance was related to the fact
that our distance metric was not strictly speaking a
kernel  however  we could certainly improve the
performance of both knn and krr by refining our
distance metric as a few of the weights currently used
in the metric were chosen by making educated
guesses  however  searching this space would be
very time consuming because we would have to use
cross validation  and we might not gain much insight
into the problem through this approach 
gaussian processes regression
the typical formulation of gaussian process
regression  gpr  is similar to krr but it allows
automatic fine tuning of the weights involved in
estimating distances      we found this aspect of
gpr attractive  and so we maximized the loglikelihood with respect to these weights  often called
hyperparameters  using a conjugate gradient descent
method  
the main challenge lay in reformulating our
distance metric in terms of a euclidean distance
between feature vectors  this was particularly
difficult because the easiest way of defining a
distance metric  as we did above  is as a function of
two notes and their respective contexts in the score 
but any feature vector representation needs each note
to be translated into an element in a vector  we thus
required feature vectors containing several hundred
elements in order to incorporate all of the aspects of
our distance metric  we formulated our feature
vectors for the ith note as follows 
 the first five elements described the ith notes
onset time within the measure  we chose
these descriptions such that the distances
between rhythmically similar positions in the
measure would be small compared with
distances between other positions in the
measure 
 the sixth element describes the timbral
height of the ith note  that is  timbres with
more energy at lower frequencies are
assigned smaller timbral heights than timbres
with more energy at higher frequencies 
 the seventh element describes the amplitude
with which the ith note should be played  in
midi terminology this is the velocity of the
note 

 

matlab code released by carl rasmussen        
http   www kyb tuebingen mpg de bs people carl cod
e 

 the remaining     elements consist of a
locally windowed representation of an
exploded representation of the score  the
explosion results from forming the cross
product of the    possible timbres versus the
possible note locations in a window
surrounding the ith note  currently we use a
window consisting of a beat before and a beat
after the ith note  since we are using a   ndnote level quantization  there are    such
positions 
however  we found that given the amount of data we
had quantized  see table     we could estimate on the
order of only    to    hyperparameters robustly 
thus  we needed a way to reduce the dimensionality
of the score component of the feature vectors  this
step was further required due to the computational
intensiveness of the conjugate gradient descent
method 
dimensionality reduction
principal components analysis  pca  made for a
good starting point for this step because of the sparse
nature of the feature vectors  in fact  some of the
dimensions of the exploded score were completely
unnecessary for particular rhythms  for example 
rhythms not containing a particular timbre needed to
have the corresponding columns in the feature
vectors removed  and the preprocessing inherent in
implementing pca could perform this function
automatically 
some work had already been carried out in this
area  a paper by ellis and arroyo outlines a method
for analyzing popular music using what they term
eigenrhythms and indirhythms     they make
many genre specific assumptions that we cannot
make  and so we have had to carry out some
additional steps to achieve a reasonable singular
value spread  these steps require intuition into the
important patterns in the data  for example  we found
that we needed to apply a rotation of the feature
vectors that re aligns every two beats in a bar with
the edges of feature vector  this leads to significantly
larger correlations in the data due to the periodic
characteristics of rhythms that are not genre specific 
thus  we used pca to project the     exploded
score dimensions down to six  even though this
projection only preserved only about    of the
variance  it allowed us to apply gpr successfully
while using some information from the score 
including the other seven features to which we did
not apply pca  we thus used gpr to estimate
thirteen hyperparameters  judging by the resulting
mse  the gpr method performed approximately as
well as knn and krr 

fihowever  we believe that gpr has much more
room for improvement  we would like to experiment
with a number of additional pre pca rotations of the
exploded score data  and we also have not yet used
cross validation to find the optimal number of pca
dimensions  in addition  there may be better ways of
reducing the dimension of the exploded score that
result in eigenvectors more relevant to the regression
task  along these lines  we have carried out some
preliminary experiments with independent
components analysis  ica   in particular  we applied
ica to the five components output by pca  ica
tended to result in more differentiated components
than those generated by pca  this differentiation is
important due to the assumption that the covariance
matrix is diagonal 
results and future applications
the performance of all four methods  measured
by the mse on   fold cross validation  is strikingly
similar  see fig      some rhythms appear more
difficult to microtime using our algorithms  but this
variation in the mse is correlated with the variation
in the actual microtiming variance of the rhythms
themselves  that is to say  some rhythms simply have
more microtiming than others  all of our algorithms
perform significantly better than applying no
microtiming at all  in an mse sense  in addition  they
are also better in a perceptual sense  according to our
preliminary and informal listening tests  a machinemicrotimed score sounds much more human and less
mechanical than a completely regular  quantized
score  we of course also made comparisons with
randomly microtimed scores where the standard
deviation of the random noise applied was the same
as that of the actual microtimings applied by a
musician  these random examples did not sound
realistic at all because they were not microtimed
consistently 

in the future  besides refining the working details of
the gpr implementation to improve the mse  we
plan to work on applying this machine microtiming
method  one intriguing application is cross synthesis 
where the microtiming deviations from one rhythm
are applied to another quantized score 
references
   

   

   

   

   

   

   

   

   

d  p  w  ellis and j  arroyo  eigenrhythms 
drum pattern basis sets for classification and
generation  int  symp  on music info  retr 
ismir     barcelona       
j  frnkranz  separate and conquer rule
learning  artificial intelligence review    
        pp      
c  e  rasmussen  evaluation of gaussian
processes and other methods for non linear
regression  phd thesis  dept  computer
science  university of toronto       
j  sundberg  a  askenfelt and l  frydn  musical
performance  a synthesis by rule approach 
computer music journal            pp       
j  sundberg  a  friberg and r  bresin  attempts
to reproduce a pianist s expressive timing with
director musices performance rules  journal of
new music research             pp          
a  tobudic and g  widmer  case based
relational learning of expressive phrasing in
classical music  advances in case based
reasoning   th european conference  eccbr
      madrid  spain        pp         
g  widmer  large scale induction of expressive
performance rules  first quantitative results 
international computer music conference 
berlin  germany       
g  widmer  machine discoveries  a few simple 
robust local expression principles  j  new music
res              pp       
g  widmer and w  goebl  computational
models of expressive music performance  the
state of the art  journal of new music research 
           pp         

fi