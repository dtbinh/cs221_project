learning traffic light control policies
avram robinson mosher
christopher egner

fi  

introduction
with the growth of modern cities and the reliance of many of their populations on personal
automobiles for the primary mode of transport  finding improved means to control the flux of
vehicles has grown increasingly important  there are substantial benefits to be derived from
improved traffic flow  for many commuters  reclaiming part of their day would enhance their
quality of life and less congestion would engender fewer accidents  saving lives  furthermore  time
spent traveling to and from work is not time spent doing work  in fact  most people are essentially
constrained to perform only the task of driving as they commute  goods must be transported and
service providers must travel to their clients  clearly  traffic delays impinge on productivity and
economic efficiency  there is also the concern of pollution as cars are generally less efficient in
stop and go than in smooth flowing traffic  longer commutes also mean longer running times and
entail more greenhouse gases 
by improving the policies that control traffic lights  traffic flow can be improved to mitigate
these problems and it can be done for considerably less cost than other infrastructural
improvements such as increasing the capacity and number of roadways or adding public transit
systems 
currently  there are efforts to create reasonable control policies  but most are ad hoc and
constitute manual tuning  the result is that drivers notice policies that are clearly suboptimal  we
will show that  by applying machine learning techniques  we hope to derive policies that are  at least 
locally optimal and are  in the general case  better than manual tuning  these policies would yield a
net improvement in the efficiency of traffic systems while maintaining fairness  since engineers are
no longer hand tuning policies  automated policy search could also yield a reduction in cost of
system design 
  

methods
we attempted to learn traffic light control policies for the same road map  see figure     
using three different approaches  the first approach is traffic balancing  essentially  it attempts to
balance the green time of a direction at the light with the relative amount of traffic arriving in that
direction at the intersection  the second approach uses simulated annealing and a cost heuristic to
derive the traffic light control policies in a reinforcement learning context  the final approach is to
recast the problem as a markov decision process  mdp  and use policy iteration to find policies 
all three methods rely on a simulator to generate the features of a function to determine the quality
of a solution 
    
simulation
since there is no readily apparent closed form function that predicts traffic flow on an
arbitrary map  we resort to simulation  by simulating traffic flow and extracting salient features  we
derive a basis on which to compare policies 
       modeled phenomena
the focus of our work is to apply and analyse the success of various machine learning
techniques for learning traffic light control polices  the simulation of traffic flow given a map 
speed limits  vehicle features  driver patterns  et cetera  is incidental to our work and hence deriving a
realistic and validated simulation is simply beyond our scope  to mitigate this disadvantage  we
designed our learning tools so that any simulation capable to producing the metrics we require can
be attached and thus we are not strictly dependent on one particular simulator 
we use a simulator largely developed by samantha chui  tj hsiang  and jennifer shen at
stanford  this simulation is correct to a first order approximation  it models roads and
intersections  controlled by traffic lights  roads are single lanes with speed limits  cars accelerate
to speed limits  never exceeding them  and decelerate to avoid collisions and to comply with traffic
lights  drivers choose the shortest distance between their starting location and their destination 
accidents  merging  multilane roads  turn lights  varying speed limits  and driver aggressiveness are
not modeled 
       metrics
we have two intuitive criteria for determining the quality of a policy  the first is that a

fipolicy for individual traffic lights should  in aggregate  maximise the number of cars that are able to
travel from their point of departure to their destination within the course of the simulation  this is
the efficiency criterion  the second is that the policy should be fair  clearly  optimising for flow
alone could cause starvation or near starvation as minor streets intersect major thoroughfares 
white it is entirely appropriate that major roadways should take precedence  extreme wait times
should be penalised  this is the fairness criterion 
these two criteria motivate five simulation metric to measure the quality of an ensemble of
individual light policies  given a road map 
 time required to travel key routes
 average speed of all cars
 throughput at each intersection
 disparity in wait time distributions for different directions are each intersection  and
 average time vehicles spent accelerating and decelerating while under the control of a light 
the first two metrics are global in the sense that their values are for a simulation run and not for
individual lights  they primarily address the efficiency criterion  the last three are local  meaning
that the policy for each light is evaluated individually against them  the first local metric again
addresses primarily the efficiency criterion while the second local metric addresses the fairness
criterion  the final metric specifically targets stop and go oscillation which increases pollution and
hinders the efficient  smooth flow of traffic  it also discourages policies which change the light very
frequently  which may be unsafe 
    
throughput balancing
throughput balancing is an algorithm for locally maximising the expectation that a vehicle
arriving at a light will not have to stop  it assumes fixed length periods for traffic lights and adjusts
the fractional amount of time for each direction to be the fractional amount of traffic arriving for
each direction  in other words  if     of the traffic flows north south through and intersection 
then the light should be green for north south     of the time  this algorithm greedily attempts to
increase traffic flow at each individual light in the hopes that this will maximise the flow for all
lights  if we ignore the effects of acceleration and yellow lights and let x be both the fraction of
traffic arriving at an intersection for either of the two roads and also the fractional amount of time
for which the light is green for this road  we have the expectation that any car arriving at the
intersection will be able to proceed g  x   as
 

g  x     x        x  
dg  x  
    x  

dx
 
 

argmin g  x       min g  x      
x
x
 
 
this implies that  if we ignore situations where neither direction at a light is green  then the
expectation that any 
car arriving in either direction will be able to continue through the light is at
least      since   in the real world  the period it takes for a light to completely cycle is large
compared to time spent
 i e  in the yellow red and red red phases   the above
 switching directions

bound reasonably approximates real world conditions 
    
simulated annealing
simulated annealing is a non deterministic search technique  parameters are altered and the
new solution is evaluated  if the alteration is an improvement  it is accepted with a probability given
by the sigmoid function
 
p  x    
   e tx
where x is the quality of the solution according to some fitness function and t is the
temperature of the annealing process  temperature decreases linearly during rounds  as the






fiprocess cools  changes leading to improved fitness values are more and more likely to be accepted 
for our fitness function  we used each metric individually  a probability that the change in that
metric should be accepted is derived  a random number is then generated from a uniform
distribution and a vote is cast by that metric for acceptance of the change  a simple majority vote
decides whether or not the change will be accepted 
under our model the parameters for each light are the length of time it spends green for
each direction  policies were randomly initialised and the annealing process continued for
approximately      rounds  annealing was then repeated several times in an effort to mitigate the
effects of random seeds and local optima 
    
markov decision process
the markov decision process  or mdp  formalism is attractive for traffic light control for
two reasons  first  the markov assumption  that the next state of traffic only depends on the current
state  is reasonable  vehicles having already left the intersection generally have little effect on local
conditions  second  there are algorithms for determining locally optimal policies once the problem
is recast in the formalism 
first intuition may lead one to model the entire traffic system globally with a state of the
system being the state of each traffic light  two variables  one for each direction  each taking on
three values  green  yellow  and red   however  since the number of states is exponential in the
number of variables  this quickly becomes intractable  for example  a simple grid pattern with four
roads in each direction has    traffic lights  this yields            states  more concretely  many
large east coast american cities have blocks between     and     of a mile long  a    by    grid 

representing about one square mile  would have               states  orders of magnitude more than
the number of atoms in the universe  more complex
traffic systems  such as those with turn lights 

explode to intractability even faster 
therefore  we reduce the problem from
 a global
 scope to a local scope  we treat each traffic
light as its own mdp  its state is defined
as
its
configuration
 assignment of green  yellow  red to

its two directions  and the configurations of the four adjacent lights  we also introduce a temporal
variable to the state to represent the time spent in the current configuration of the five lights  this
model is linear in the number in traffic lights and so the problem becomes tractable  the actions
available in any state are to transition the light to green in a certain direction if it is not already 
since the adjacent lights may change their configurations external to the decision of the central
lights policy  state transitions become probabilistic under this model 
there is an intuition to back this local model  a traffic light is concerned with traffic that
comes from four other sources  there is generally a correlation between that lights configuration
and the amount of traffic it sends  i e  that more traffic arrives when it is green in the direction
pointing toward the central light  this traffic takes a certain amount of time to propagate and hence
a model that observes the four adjacent lights and tracks the time for which they have been in their
current configuration is reasonable 
once we have the model in place  we then use the policy iteration algorithm determine a
locally optimal policy for each light  the policy for the system is the combined policy for all the
lights 
  

results and conclusions
applied the three different models of the problem to a traffic simulator to discover if we
could improve policies over time where improvement is measured by the metrics outlined in section
       each model has its flaws as does the simulator 
our algorithms competed on the same map  figure       this map was chosen because it
based a common grid pattern with varying distances between intersections  traffic loads were
increased for certain streets  if our algorithms were performed well  we expected to see policies for
lights where heavily traveled streets intersected lightly traveled ones to favor the heavily traveled
ones  we also expected to see improvements in our metrics since the system should ameliorate as

fieach light learns
and improves the
way it processes
its traffic 
throughput
balancing did not
produce useful
policies  there are
two primary
reasons for this 
first  the
assumption of
fixed time periods
for light cycles is
likely far too
strong  also  the
hypothesis class
for light policies
does not permit
offsets between
lights  i e  all lights
start their cycles
at the same time 
consequently 
figure    
these policies assume a fixed order  that is the algorithm will never find a policy where a light starts
inverts its cycle to be north south then east west if it started in east west followed by north south 
given the algorithms inability to achieve improvement and the significant limitations of the class of
policies  we believe that the modeling assumptions induce sizable modeling error 
tests resulting from simulated annealing also showed no improvement over time though
they did reveal significant obstacles produced by the simulator  we found that  if we allowed the
random number generator seed to vary between runs  we saw significantly different results  for
example  without changing the policies of any lights  one run moved about     combined vehicles
through the    intersections
of the map in ten minutes of
simulated time while another
moved     this instability is
shown in figure      the
baseline throughput for each
round of simulated annealing
is shown as are the vote cast
based the throughput metric
and the final decision to
accept the change in the
parameters of a light or not 
if a change was accepted at
an iteration  a point appears in
the upper row and if it was
rejected  in the lower row 
therefore  we would expect
to see the throughput metric
stay approximately the same
in the next round if the final
figure    
decision was the reject the

fichange  this is not what we observed 
the markov decision process also did not perform as well as hoped  it is possible that
modeling the decision of an adjacent light to change as a random variable allows too much
information to be lost  further  we found the establishing the reward for a state to be difficult given
the instability of simulation conditions over varied random seeds 
in the end  we have doubts that the search space for policies is amenable to machine learning
models performing optimisation  while local optima are a nontrivial problem  this issue is dwarfed
by the problem of instability in the simulator  since we do not have a simulator that was subjected
to a rigourous verification and validation  it is not possible for use to say whether the high variance
over random seeds was an effect of the simulator itself or rather that the nature of traffic is such that
the fine details are of primary importance  thorpe  thorpe     notes that small changes in traffic
light policies can lead to large changes in traffic congestion  while he sights this as a reason to
seek improvement through automated learning  we are leery that small changes in policy can lead to
large  often unpredictable changes in traffic congestion  making this search space very difficult for
machine learning techniques 
  
references
chui  s   t  hsiang  j  shen  traffic sim  stanford university       
puterman  m  markov decision processes  descete stochastic dynamic programming  john
wiley   sons      
thorpe  t  vehicle traffic light contro using sarsa  mathers thesis  department of computer
science  colorado state university       

fi