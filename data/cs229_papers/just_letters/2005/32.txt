applying synthetic images to learning grasping
orientation from single monocular images
  steve chuang and eric shan  

  introduction
determining object orientation in images is a well established topic in computer vision  techniques for
pose estimation use learned or pre designed object models in order to approximate human facial positions
or hand signals         object tracking algorithms generally leverage multiple  consecutive images with
slight variation as a means to extracting affine and other transformations         object detection and
description  particularly for the case of buildings  often employs global properties such as time of day  for
light direction  and explicit structural models         while these methods have been able to achieve high
levels of accuracy  our aim  in the context of robotic arm grasping of objects viewed through a mounted
camera  is to generalize a learning process across many object classes using single monocular images 
much of our project extends from ashutoshs work on monocular cues and feature filters  see ashutosh  
this project addresses a sub problem in the task of robotic arm grasping 


our work focuses on identifying the final angular position of the robotic
arm hand necessary to grasp an object at a given optimal point of
contact  such as a cup handle or the brim of a cap  we further constrain
the problem to learning two angles    about y axis  and   about zaxis   as opposed to the more standard roll  yaw  and pitch for simplicity
in our model and data collection  as a second objective  our project evaluates the application of synthetic
images to training samples in place of hand labeled data  our motivation for this is two fold     generating
accurately labeled training samples across several object classes potentially takes considerably less time
than hand labeled samples and    the number and quality of training samples can scale easily using
graphics tools  as a start  we focus on cups with handles as a well defined class with regular characteristics 
in section    we discuss the framework and tool we used for generating synthetic images  as well as the
steps for hand labeling    real cup images  in section    we briefly introduce the feature vectors used to
create our design matrices  section   details our results related to performing feature selection on our    
features  section   discusses a simple model for making predictions on  and   and our results are
discussed in section    we conclude with readily possible avenues for future work in section   

  framework for generating synthetic images
our database of objects comes from the princeton shape benchmark  and we considered   instances of
cups from the database  the meshes are loaded into blender d  the open source modeling tool we use for
generating our synthetic images  blender d supports python scripts for manipulating objects in a scene 
realism was added using blender d features  such as smooth shading  we chose blender d for its built in
functions for texture mapping  anti aliasing  ray tracing  etc    and a script was used to rotate the object and
record rotation angles  we were unable to properly access blender ds projection and viewport matrices  so
labeling the target grasp point  as input into our learning algorithm  was done ad hoc  we generated a set of
images with a red sphere located at the target location and ran a script to find the red spot and record the
pixel location grasp point for a particular rotation of the object  the optimal  x  y  pixel coordinates and
synthetic images are then passed in as an argument for feature extraction  each cup instance is represented
by roughly      images  distinct rotation pairs  each 

fifigure     synthetic cup images generated from blender d

figure     real cup images from a digital camera
images of a real world cup at    angles were manually labeled by visually matching the cup appearance to
a synthetic image with a known configuration  we expect a labeling error of       for these samples  the
real world images were adjusted in the following way  the background was removed and the image
brightness and contrast were manually normalized to be a slightly dark gray  as is the case with our
synthetic images  we find this normalization process acceptable because it can be automated for any
object class and lighting environment based on the training datas brightness and contrast distribution 

  feature vector
the color input images are converted to ycbcr color space  see ashutosh   only the y  luma  channel is
used  sixteen masks are convolved with the image multiple resolutions  features consist of summing over a
patch of pixels at a particular location relative to the grasping point for each of the various filter and
resolution scales 
images were generated at a resolution of     x      and non overlapping patches of    x    were defined 
as opposed to in  ashutosh   resolution scales were chosen to cover the width of a typical handle  the local
neighborhood of the handle center  and the entire cup 
in previous work  ashutosh      features were used  because we did not want to make assumptions about
color  unlike trees and sky   we removed the two color related features  additionally  we wanted to be
invariant to lighting conditions  so we removed the local average mask  the remaining masks include law
masks and oriented edge detectors  we added two additional features more suited to our task  both of which
further emphasize edges  the first is a harris corner detector  which attempts to capture depth and structure
information by detecting the amount of t junctions in the local neighborhood      the second added
feature is the take the max of all oriented edge detectors to also try to capture the complexity of the
structure around the local region  different angles will occlude different parts of the cup and show edges
due to occlusion  we want to capture as much correlation between edges and angle as we can 
the convolved response for each image is squared to get the energy of the signal  the summed response
over various patches  centered on the labeled grasping point  for a particular filter and a particular
resolution make up a single feature 

  feature selection

fibecause of the large number of features  there is a risk of overfitting the data  for expediency  we used
forward selection  instead of backward  and   fold cross validation on synthetic images and examined the
mean squared error as a function of the number of features  the benefits of additional features appear to
flatten out past    features  a set of features was selected for each prediction angle  comparison with
feature selection using   fold cross validation

performance vs    of features       samples 

    

  
y angle error
z angle error

y
z
y
z

    

train
train
test
test

  

    

  
average error  degrees 

mean squared error  degrees 

    

    

    

   

  

  

   

   

  
   

 

 

  

  

  

  
  
  
number of features   iteration count

  

  

  

   

 

 

  

  

  
  of features

  

   

   

testing on real images shows a similar flattening past    features  the real test image set has the problem
of being drawn from a different distribution than the synthetic training images  which causes the
discrepancy between training and test errors  it is therefore inconclusive whether feature selection using
cross validation on synthetic images is indicative of the best features for real images  certain features may
generalize better from synthetic to real images 

figure    a  top features for 

figure    a  best feature for 

b  top features for 

b  best feature for 

fi  prediction model
in order to improve the accuracy of our predictions  we create classifiers for several patches centered on the
grasping point  this helps mediate if the grasping point is not accurate and also helps capture neighboring
information  each of which have its own neighbor features  too   the variance of the estimates on the
training data for each classifier is used to weight our confidence in its prediction  our final prediction is a
weighted average of all of our classifiers  we used five patches centered on the grasping point  because
feature selection was slow  we used the same set of features for all patches  but feature selection could be
done for each patch separately  or in groups based on distance from the center 
our final prediction is 

pred   y    

 
   
yi

c i c  t   r  b l   i 

where i refers to the appropriate patches for the current prediction    is the variance particular to each
patch type in the set  c enter   t op neighbor   r ight   b ottom   l eft    and we have the standard 
 
r
 
yi   x it x i x it y o xi    i xi
for each i  we also tried regularized ridge regression  and later with the gaussian kernel   which did not
help reduce generalization error 

 

 

  results
our first plot shows the performance  training and test errors for both angles  of our features and learning
model as a function of the number of samples  within around the first     samples  the performance
flattens out for all   tests  we believe the model is suffering from high variance  but the cause is due to
performance vs  sample size      features 
  
y
z
y
z

  

train
train
test
test

  

average error  degrees 

  

  

  

  

  

  

 

 

 

   

    

    

    

    

  of samples

insufficiently diverse training data as opposed to an insufficient number of training samples  varying
lighting environments  textures  specularities  and physical structures all contribute to training diversity 
to assess whether synthetic images can serve as a substitute for real images  we ran three tests 
   train on synthetic data for one cup instance and test on synthetic data for a different cup instance 
   train on synthetic data for two cup instances and test on real images for a different cup instance 
   perform loocv on the real images in order to arrive at a mean error 
the results for the tests are shown in the table below for both angles  we observe that training on synthetic
images and testing on synthetic images of new cups  not just new images of known cups  produces closely
comparable error to the test error from testing on real images  as a result  we posit that the synthetic
training data can be a roughly suitable substitute for real training data in general  as long as appropriately
diverse data is used to avoid the variance from above   note  however  that no information or lessons

fishould be interpreted from the raw mean errors  magnitudes  since the labels on the real data are expected
to have a       error 
 
train synth
train real

test synth
     

test real
     
     

 
train synth
train real

test synth
     

test real
     
     

  conclusions
our project suggests that using synthetic images to train  d orientation for robotic is possibly effective 
our generated dataset had copious varieties of orientations  but only a few samples of cups and only a
single light model  the main source of error appears not to be from generalizing to real images  but rather
from generalizing to different types of cups  more variation of the cup shape in the training data may help 
one method for possibly accounting for the possible differences between synthetic and real images is to do
feature selection using real images while training on synthetic images  this would  however  offset some of
the benefits of synthetic images in that one would need to label at least a reasonable number of real images 
in our project  we used a simple parameterization that matched the procedural process we used to rotate the
object  the learning algorithm will likely perform better by reparameterizing the output target labels to one
that matches cup appearance more  rather than parameterizing by a sequence of rotations  one strategy may
be to project the handle angle onto each of the x y  y z  and x z planes and train a classifier to predict the
angle of the projection  furthermore  instead of taking a weighted average of multiple hypotheses 
maximizing a model of the distribution of y given the hypotheses is another area for future work  the
model should capture the highly correlated nature of the prediction  along with parameters that represent
our confidence in each predictor 
the use of synthetic images in lieu of large quantities of real labeled training data appears to be a promising
method for learning object orientation  which is essential for acquiring enough data when using many weak
features  there is  however  significant work still to be done  including analyzing the amount of error due to
cup shape variation and lighting differences between synthetic and real images  reparameterizing the target
orientation  and creating a better model for generating hypotheses 

acknowledgements
we would like to thank ashutosh saxena for his support during the project 

references
    n  kruger  m  potzsch  and c  v d  malsburg  determination of face position and pose with a learned representation based on
labelled graphs  technical report        ruhr universitat  january      
    s b  kang and k  ikeuchi  the complex egi  a new representation for   d pose determination  ieee trans  pami  vol      july
     
    j  shi and c  tomasi  good features to track  proc  ieee conf  computer vision and pattern recognition       
    r  manmath and j  olinesis  extracting affine deformations from image patches   i  finding scale and rotation  ieee cvpr       
    c  lin and r  nevatia  building detection and description from a single intensity image  computer vision and image
understanding  vol           
    c  lin  a  huertas  and r  nevatia  detection of buildings using perceptual grouping and shadows  ieee cvpr       
    s  karlsson  monocular depth from occluding edges  masters thesis      e    lund institute of technology 

fi