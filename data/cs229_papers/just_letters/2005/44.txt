query by humming  finding songs in a polyphonic database
john duchi
computer science department
stanford university
jduchi stanford edu

benjamin phipps
computer science department
stanford university
bphipps stanford edu

december         

abstract

take the pitches and the durations that have been calculated to find the actual recording represented  there
has been research in this area before  but most has been
using music stored in midi or some other symbolic
formats     or in monophonic  single voice  recordings
     in real polyphonic recordings  a number of factors
complicate queriesthese include high tempo variability  which depends on specific performances  and the
inconsistencies of the spectrum of sound due to factors
such as instrument timbre and vibrato 
to move beyond the above listed difficulties of making queries over polyphonic recordings  we basae our
algorithms on a generative probabilistic model developed by shalev shwartz et al       this builds on work
in dynamic bayesian networks and hmms     to create a joint probability model over both temporal and
spectral probabilistic components of our polyphonic
recordings to give us a retrieval procedure for our sung
queries 

query by humming is the problem of retrieving musical performances from hummed or sung melodies 
this task is complicated by a wealth of factors  including noisiness of input signals from a person humming or singing  variations in tempo between recordings of pieces and queries  and accompaniment noise in
the pieces we are seeking to match  previous studies
have most often focused on the problems of retrieving melodies represented symbolically  as in midi format   in monophonic  single voice or instrument  audio
recordings  or retrieving audio recordings from correct
midi or other symbolic input melodies  we take a step
toward developing a framework for query by humming
in which polyphonic audio recordings can be retrieved
by a user singing into a microphone 

 

introduction

 

suppose we hear a song on the radio but either do not
catch its title or simply cannot remember it  we find
ourselves with songs stuck in our heads and no way to
find the songs save visiting a music store and singing
to the music clerk  who can then  hopefully  direct us
to the pieces we want  automating this process seems
a reasonable goal 
the first task in such a system is to retrieve pitch
from a human humming or singing  there is a large literature on retrieving pitches from voice via a machine 
there are many algorithms to detect pitch  most rely
on a combination of different calculations  often  a
sliding window of   to    ms intervals is preprocessed
to gain initial estimates of pitch  then windowed autocorrelation functions     or a power spectrum analysis
is done  after these steps  there is often interpolation
and postprocessing on the sound data to remove errors such as octave off problems     to give a series of
frequencies and the times at which the frequencies are
estimated 
the second task in a query by humming system is to

problem representation

though there are two parts to our problempitch extraction and retrieving musical performances given a
melodythe latter necessitates the most detailed problem setting  formally  using notation essentially identical to that of       we are able to define the set of
possible pitches   in hz   in well tempered western
music tuning  as           s     s  z   thus  a
melody is a sequence of pitches p  k   where k is the
length of the melody  in notes  
for our purposes  the real performance of a melody
is a discrete time sampled audio signal  o   o            ot  
where ot is the spectrum of one of our performances
at the tth discrete sample  these performances are
those drawn from our database of pieces that we query 
because we assume short time invariance of our input
sounds  we set the samples to be of length     seconds 
to completely define a melody  we have a series of
k pitches pi and durations di   where the melody is to
play p  for d  seconds and so on  performances of
 

fipieces  however  rarely use the same tempo  and thus a
melody can have much more variability than the model
given  as such  we define a sequence of scaling factors
for the tempo of our queries  m   r   k   the set of
sequences of k positive real numbers  in our testing 
each mi is drawn from a set m of all the possible scaling
factors   thus  the actual duration of pi is di mi   which
we must take into account when matching queries to
audio signals 
now we have our problem defined  given a melody
hp  di  we would like to find the likelihood of some
performance  that is  we would like to maximize o in
our generative model p  o p  d  

is
    cos       

   

detecting pitch in speech

in our implementation  we followed saul et al s approach of running our sung query signals through a low
pass filter to remove high frequency noise  using halfwave rectification to remove negative energy and concentrate it at the fundamental  then separating our signals into a series of eight bands using bandpass  th order chebyshev filters  we can then use pronys method
for our sinusoid detection  which has proven accurate
in previous tests      saul et al  also define cost functions that allow us to determine whether sounds are
  extracting pitch
voiced or not and whether the least squares method
has provided an accurate enough fit to a sinusoid  see
having defined our problem  we see that the first step     for more details  
must be to extract pitches and durations from a sung
query  saul et  al   have described an algorithm that
does not rely on power spectrum analysis or long autocorrelations to find pitches in voice      their algorithm  which is called adaptive least squares   als 
uses least squares approximations to find the optimal frequency values of a signal  a method known
as pronys method     uses only one sample lagged
 a  waveform of scale
and zero sample lagged autocorrelation as well as least
squares  which reduces errors in resolution sometimes
found by ffts as a result of low sampling rates  and
we can extract pitches in time linear in the number of
samples we have 
 

   

 

    

  
 

    

    

    

    

     

     

   

   

   

   

   

finding the sinusoid in voiced
speech

  

 

any sinusoid that we sample at discrete time points n
has the following form and identity 
sn
sn

 

    

    

    

    

     

     

 b  frequencies of sung scale

  a sin n    


sn    sn  
 
 
cos 
 

figure    raw data and frequencies
  
  

this allows  as in      us to define an error function

 
x
xn    xn  
e    
xn  
 
 
n

  
  
  
  
  

 

   

    

    

    

    

    

if our signal is well described by a sinusoid  then when
figure    pitches of the sung scale
    cos      the error should be small  the solution
to our least squares is given by
p
  n xn  xn    xn    

    transforming to melody
   p
 
 
n  xn    xn    
using the above method  we retrieve a frequency at
thus  we minimize our signals error function and then every     second interval  which we downsample from
check that our signal is sinusoidal rather than expo        hz to     hz  which allows for quicker compunential and not zero  then our estimated frequency tations  given our set of frequencies  f            fn   over
 

fi   

our n samples  we assign each fi to its corresponding
midi pitch pi            then use mean smoothing to
achieve better pitch estimates for every pi   we group
consecutive identical pitches from the samples to give
us our melody hp  di   h p    d              pk   dk  i  lastly 
we compress this melody to be in a    note  one octave 
range  because it helps our computational complexity
in the alignment part of our algorithm to have fewer
possible pitches  and spectra alignments are not overly
sensitive to octave off errors  to see examples of frequencies and pitches extracted from singing  see figures
  and   

 

a generative model
melodies to signals

we let oi represent a sequence of samples  that we
suppose is generated by note and duration  pi   di   in
our query  from a piece in our database  that is 
oi   ot             ot   where pi ends at time sample t and
t    t  di   we use a harmonic model of p  oi   almost
identical to that in     
f  oi   is the observed energy of some block of samples oi over the entire spectra  we get this from the
fourier transform   also  we assume that we have a
soloist in all of our recordings  and that s   oi   is the
energy of the soloist at frequency  for our samples 
and our model assumes that s is simply bursts of energy centered at the harmonics of some pitch pi   this
is a reasonable assumption for our soloists energy  because often the harmonics of the accompaniment will
roughly follow the soloist  that is  we have a burst
at pi h for h                 h   and we set h to be   
to keep the number of harmonics reasonable  we can
define the noise of a signal at some frequency  to be
the energy that is not in the soloist or any of his or
her harmonics  frequencies that are multiples of    or
n    oi     f  oi    s   oi    this gives us that

from

as mentioned in section    we have a generative model
that we are trying to maximize  p  o p  d   more concretely  given a melody query hp  di  we would like to
find the acoustic performance o that hp  di is most
likely to have generated 

   

modeling spectral distribution

log p  oi  pi   di    log

probabilistic time scaling

  s   oi     
  n    oi     

as in      we treat the tempo sequence as independent
of the melody  which ought to hold for short pieces   to
give us problem of finding the o in our database that
maximizes the following 

where        is the l   norm  see     for this derivation  
we assume that di is implicit in conditional probabilities when given pi from here on  because they pitches
and durations in our queries come in pairs 
to actually get the energy of the soloist and the
x
noise 
assuming the soloist is performing at a frequency
p  o p  d   
p  m p  o p  d  m  
  we use a method called subharmonic summation
m
proposed by hermes      this method allows us to
in this  having the m parameter in the conditional sim  determine if a pitch is predominant in a spectrum by
ply means that we are scaling the sequence of durations adding all the amplitudes of its harmonics to the fundad by m  m is modeled as a first order markov process  mental frequency  the formula we apply is as follows 
so
h
x
k
y
s    
dh f  h 
p  m    p  m   
p  mi  mi    
h  

i  

where d is a contraction rate that usually is set to make
it so that lower frequencies are more important  we
set d     so we can simply remove all energy at the
frequencies we assume are the soloists   thus  when
we are performing a query of a piece and we would
like find the probability of a block of signals oi given
the current pitch in our query  we simply remove all
the peak frequencies at multiples of our querys pitchs
frequency  then find the remaining signals and treat
them as noise  see figure     this gives us p  oi  pi   

because the log normal distribution has the nice trait
that it somewhat accurately reflects a persons tendency to speed up  rather than slow down  when doing
a musical query or performance  we say that
p  mi  mi      

m
 
    log m i   
i 
e   
 
 

we also assume a log normal distribution of p  m     so
log   m     n        in these equations  the  parameter describes how sensitive our model is to local tempo
    matching algorithm
changeshigh          means that our model is not
very sensitive to tempo changes  low         give us with the background we have now put in place  we see
a model very sensitive to tempo changes 
we can develop a dynamic programming algorithm as in
 

fipath algorithms for hmms  that we see in figure    due
to     with some modifications 

  

  

   initialization

f   

  

t     t  t      t        

  

  

   inductive building of 
for i      to k  t      to t       min  to max 

  

 

 

  

   

   

   

   
  hz 

   

   

   

   

 i  t     
max
 i     t        p         p  ot               ot  pi  
 

   

 m

figure    solo vs  noise  stars are solo frequency  the
rest is noise

where t    t   di     
   termination
p  

    to retrieve our polyphonic piece given some k length
query of pitch duration pairs h p    d              pk   dk  i 
more specifically  in our implementation  for a given
polyphonic piece  we have a spectrum sample every    
seconds  and there are t samples for the entire length
of the piece  recall that p  oi  pi     p  ot               ot  pi  
for appropriate t  t    we also have the tempo scaling
factors to account for  that is  p  mi  mi    from above 
in the algorithm  we call these tempo scaling factors 
 to vary tempo for our algorithm  each scaling factor
is simply a different small multiple of     that is added
or subtracted from di to give us a different duration  
there is also a chance that there are rests in the pieces
we consider  and we must take into account rests in our
queries  as such  if we have that pi      we replace
p  oi  pi   with the spectrum probability of a rest

max

 tt m

 k  t   

figure    the alignment algorithm we use

   

complexity of matching a query

the complexity of this algorithm  which is relatively
easy to see from the for loop nesting  is o kt  m      
where k is the number of notes in our query  t is the
number of time samples in the polyphonic piece we
query  and m is the set of possible tempo scaling values  this holds as long as p  ot               ot  pi   can be
computed in constant time  which we guarantee in our
implementation 
to achieve constant time probability lookups  we
pre compute all the probability values of sample blocks
oi using fast fourier transforms with     points for
good resolution  we compute the probability p  o p 
for each pitch p that we can see in our queries for
all the possible lengths of samples in our audio signal
o  we compute probability for every block of samples ot         ot  of length     to     seconds  because
singers cannot change pitch in under    s  and in most
music  especially the music we use  pitches are rarely
held for longer than two seconds  effectively  this gives
us o t      probabilities for each pitch  of which we
have     this pre computation  while expensive  significantly helps running times  because we do not have
to do spectral analyses every time we wish to calculate
p  o p  in our algorithm 

p  rest pi       
 
    p  ot               ot  pi   
 
p  ot               ot  pi     
in our model  this says that if we have a rest in our
query  then the pitches before and after pitch pi in our
query ought not be very present in the spectrum 
putting all of this together  we define  i  t    to be
the joint likelihood of ot and mi   or as the maximum
 over the set m of all the scaling factors  probability
that the ith note of our query ends at sample index t 
and its duration is scaled by  
 i  t      max p  ot   mi  p  d 
mi m i

while this is the joint likelihood of our polyphonic
pieces first t samples and its first i scaling factors given  
experimental results
p and d  and ideally we would have just the likelihood of the samples o   o            ot   we still use  as we ran tests on five different beatles songshey jude 
the retrieval score given our query  all this gives us let it be  yesterday  its only love  and ticket to
the alignment algorithm  reminiscent of most probable ride  the system  given a melody represented by
 

fipitch duration pairs  retrieved the song whose alignment score was the highest  as a first test  the system
was given correct symbolic representations of parts of
all five songs  copied directly from scores  in this 
the retrieval was perfect  as expected for our small
database 

was     for queries sung in the correct key  this accuracy is fairly good  though it is orders of magnitude
worse than the accuracy achieved with correct symbolic queries  thus  as long as the system did not have
to do any transposition  the querying worked for our
small database 

is bothersome and will be a subject of future work  we
also would like to expand the system to handle incorrect accidental modulations in singing  to give it a distribution over incorrect pitches  in the inductive part
of the algorithm  instead of taking p  o p   we could
define a distribution over the probability that the user
meant to sing note p in his query  for example  we
might look at p     p  p     and take the maximum
of      p  o p      p  o p      p  o p        which would allow the singer to miss some pitches by a semitone but
would increase the time complexity of our algorithm by
a factor of the number of pitches over which we take a
distribution to allow incorrect singing 
the systems speed is also relatively low  to build a
large database the alignment procedure would need a
significant speedup  it may be useful to look into learning to automatically extract themes from polyphonic
music  then performing queries over those themes  in
spite of the difficulties inherent in this problem  we
have demonstrated that a query by humming system
searching polyphonic audio tracks is feasible 

   

references

   

sung queries in key

our systems retrieval rates on the five songs  given
queries that were sung in the songs keys  were again
perfect  the average ratio
highest retrieval score for query hp  di
second highest retrieval score for query hp  di

transposition

to test our systems resilience to transposition  shifting an entire melody but keeping its relative pitches
constant   we had the alignment procedure attempt to
align the melody we gave it  as well as the    other
melodies that were transpositions  the ith transposition
simply shifts all the pitches of p up i  of the original
melody  the piece retrieved was the one which had the
maximum alignment score on any one transposition of
the melody 
as before  when given correct symbolic representations  transposed scores   the retrieval procedure was
flawless  always returning correct results 
when the queries  however  were sung but not in the
key in which the beatles sang  for example  yesterday
sung in e instead of f  a half step down   results were
not as optimal  hey jude and its only love the system still identified correctly  but the other three songs
had significantly worse results  sometimes being given
lower alignment scores on a melody than as many as
three other songs  the reasons for this are not totally
clear  but we speculate that transpositions of a query
may put it into the key of a different song from our
database  which would make it easier to for a query to
match the spectra of an incorrect song 

 

    saul  l   lee  d   isbell  c   and lecun  e  real
time voice processing with audiovisual feedback  toward autonomous agents with perfect
pitch  advances in neural information processing systems     pp             mit press  cambridge  ma       
    shalev shwartz  s   dubnov  s   friedman  n  
and singer  y  robust temporal and spectral
modeling for query by melody  sigir    pp 
         acm press  new york  ny       
    proakis  j   rader  c   ling  f   nikias  c   moonen  m   and proudler  i  algorithms for statistical signal processing  prentice hall       
    rabiner  l  on the use of autocorrelation
analysis for pitch determination  ieee transactions on acoustics  speech  and signal processing      pp              
    meek  c  and birmingham  w  johnny cant
sing  a comprehensive error model for sung
music queries  in proc  ismir       
    durey  a  and clements  m  melody spotting
using hidden markov models  in proc  ismir 
     

conclusions and future

    hermes  d  measurement of pitch by subharmonic summation  journal of acoustical society of america         pp                

we have taken a step toward building a polyphonic
music database that can be queried by singing  while
we met with success as long as queries were in the correct key  the systems inability to handle transposition
 

fi