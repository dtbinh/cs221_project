title  clustering and segmentation of probablistic interaction networks
team members  marcin mejran  dina thomas

introduction and motivation
protein protein and protein dna interactions in the genome are modeled as interaction
networks  and there are more than a dozen methods to detect these interactions  as a
result at present there are a number of different interaction networks available for each
sequenced organism  however even though most of these interaction predictors have
been individually shown to predict experiment  the networks generated by different
methods are often not superposable in any obvious way  this seeming paradox has
simulated a burst of recent research in network integration  integrating these different
networks to arrive at a statistical summary of which proteins work together within a
single organism can help detect linkages that would have been missed if only single
predictor was used  it can also help strengthen the confidence of known linkages
by formulating the network integration problem as a binary classifier we can quantify the
extent to which integration improves prediction accuracy over a single source  if two
proteins have a shared functional category we say the link between them is labeled as
l   and if they are in different functional categories the link is labeled l    the
network integration problem is a binary classifier in high dimensional feature space as
shown in figure  a 

  a binary classifier paradigm

  b roc area for classifier

figure   b  shows the area roc curve for one data set and shows that classifier
performance increases monotonically as more data sets are combined  prof serafims
group has developed an integration algorithm recently  the next step now is to extract
information from these networks that have been integrated  potentially using this
information to then create better integration and network comparison methods  there has
been some success using regression methods on pairs of proteins  although other machine
learning methods havent been tested yet  the goal of this paper is to test clustering
methods on these networks  and to extract information and summary statistics from them 
the data consist of protein interactions networks  which are represented as
undirected graphs in which nodes represent proteins  edges represent interaction

fiprobabilities  the nature of the edge weights biases clustering towards generating
correct clusters  in our case  these desired clusters are those which group together
proteins with similar function  this can allow us to find the function of unknown proteins
based on what cluster they are in  and what other proteins are in that cluster  a potentially
more important goal however is to find certain proteins which have important or
otherwise interesting functions  from a biological point of view  these may include
proteins that form the centers of a hub or which connect multiple together clusters  such
proteins can be thought of as vital to an organisms survival  and from a clustering point
of view their removal is expected to greatly affect clustering results  we tested the
reasonability of our clustering results using annotation information for each protein  the
information used is a simple one line description which exists for roughly half the
proteins  one of our main goals  besides finding clusters for our data  was to create a
code base in r which can be used for future clustering of networks  we needed to take
care so that our implementation is flexible as network characteristics differ from genome
to genome  for example  some networks have more proteins with high probability
linkages than others 
algorithms implemented
we implemented a number of clustering algorithms  hierarchical clustering 
kmeans clustering and markov clustering  and some extensions for these algorithms  for
the actual clustering algorithms we mainly used pre existing code from the r library
cluster and the program mcl for markov clustering  the algorithms in the cluster
library have the advantage of natively working on dissimilarity matrixes which is the
form our data is in  the main goal as a result was to create a unified clustering
mechanism in r so that all clustering could be performed at once 
the first algorithm we implemented was the traditional hierarchical clustering
method  or rather a close cousin of it  we used the agnes and diana algorithms in the
cluster library for r  which perform divisive and agglomerative clustering
respectively  as the name implies the divisive algorithm  diana  begins with one large
cluster and recursively breaks it into smaller clusters based on the elements which are the
furthest apart  agglomerative on the other hand  use by agnes  starts with many small
clusters and then recursively combines them together into a tree  the second fundamental
algorithms we applied are k means and soft  fuzzy  k means  or rather derivatives of
them  as with hierarchical clustering we use method from the cluster library  pam
performs something very similar to k means except that it uses medoids for cluster
centers  which must be elements in the data set  and sums over dissimilarities instead
euclidian distances  clara is an extension of pam designed to work with large data
sets  which is the case for much of the data we use  by dividing the data into subsets of a
fixed size  in soft k means we do assign each data point to a cluster but instead we find
the probability of each data point being in each cluster  fanny  like pam  provides a
soft k means derivative for dissimilarity matrixes  we have also implemented the gap
method for selecting the number of k means clusters 

fithe last clustering algorithm we implemented was markov clustering  mcl 
which works by simulating flow on a graph  this algorithm is based on the property
that in a graph a random walk inside a dense cluster will visit many of the nodes before
leaving the cluster  the basic idea is to simulate flow in a graph  promoting flow where
the connections are strong and demoting it where they are weak  so that flow between
clusters dies out but not within clusters  there are two phases in the algorithm  inflation
and expansion  expansion can be considered the flow going outwards into other areas 
while inflation can be considered the strengthening weakening of the flow within the
structure  mathematically the expansion is characterized by converting into a markov
graph and computing the powers of the associated stochastic markov matrix  inflation is
performed by an entry wise hadamard schur product combined with diagonal scaling 
we opted to use the mcl package for markov clustering instead of coding in r  as the
package was optimized for large data sets  as some of ours are  which are known to cause
problems  ram usage  for un optimized implementations  to characterize individual
nodes in the network we implemented graph information algorithms like degree
distribution  clustering coefficient and between ness centrality  all these methods give an
indication of which nodes can be potential hubs in the network   see glossary for
definitions  
experiments
we performed clustering on a number of data sets  to extract only highly
probable clusters we first threshold the network by removing edges with very low
probability of interaction  since dissimilarity matrixes need values for each entry we set
the probabilities of interaction for these removed edges to   instead  the algorithms in
general grouped biologically related proteins with good consistency  although there was a
good amount of noise in the clustering  the threshold parameter is dependent on the dataset and was empirically determined  for the helicobacter pylori data set  the threshold
was found to be      which gave good distribution of clusters i e  not too many small
clusters or a single large cluster 
observations
   there are many weak linkages and very few strong ones  and the strong ones are
disjoint  as a result if they are removed through the threshold  algorithms will create
many clusters of single nodes 
   there are a non trivial number of nodes that are weakly connected to a large set of the
remaining nodes  more than     in some cases  as a result a low threshold resulted in
single large cluster  however as mentioned in point   a high threshold leads to many
small clusters  these nodes have high between ness centrality and low clustering
coefficient  meaning that they are hubs between different sections of the graph but are
not in any clique themselves  the effect of these nodes on clustering is seen in the plots
below we obtained for a data set  plot  a  gives the banner plot which displays the
hierarchy of clusters like a tree and it plots the distances at which members are merged 
the concentration of bars at the right side indicates a lot of nodes are merged initially at
the top of the hierarchy  plot  b  gives the principal components as the ellipses  we can
see concentration of points in one component 

fithe existence of such proteins causes problems as they have a very large influence on
clustering  see below   however are not very biologically significant for each cluster 
   the stability of clusters for a node can be stated as its resistance to fragmentation when
the node is deleted  a node that renders a cluster less stable is critical and gives the
protein that is critical for the functioning of the biological network  these are the proteins
that are essential for the survival of the organism  the detection of these proteins is
critically dependent on the clustering method used as well as the threshold parameters 
to characterize critical nodes of the clusters  we knocked out certain nodes and
performed clustering on the resulting network 
the scatter plots for experiments on helicobacter pylori are given below 

as seen from the plots above  nodes with high clustering coefficient do not affect the
stability of the cluster in most cases  however nodes with high between ness centrality 
are found to be critical  it is also seen that mcl characterizes the critical nodes better

fithan agnes or pam  in some runs high clustering coefficient nodes also fragmented
clusters 
   mcl provides good clustering  and unlike the other methods can naturally work with
removed edges  as a result it will place isolated proteins in their own clusters  the results
are not always consistent  as the algorithm is not deterministic however in general it is
quite sensitive to the removal of seemingly important proteins 
conclusions
we were able to find many useful properties of such networks and the clustering
of them  there are two sets of highly important nodes  those which are weakly connected
to everything and those that are in well connected clusters  the former has a
disproportionate influence on the clustering  meaning that many of the clusters may not
be significant  at the same time  the removal of such nodes can lead to finding clusters
which are separate and biologically significant  markov clustering was found to create
biologically significant clusters which and able to detect the removal of significant nodes 
we were able to create a significant r codebase which can be used to run further analysis
on these methods 
glossary
   between ness centrality  the between ness centrality of a node v in a graph is the
sum of the fraction of shortest paths between all pairs of nodes that pass through v 
   clustering coefficient  clustering coefficient of a node v having n neighbors is
the ratio n  n  n     where n is the number of edges between the n neighbors 
   pam  partitioning around mediods   this finds k representative medoids from the
data set 
   clara  clustering large applications   this clustering method is used on large
data sets  the data is divided into sub datasets of equal size and pam is applied to
each subset 
   agnes  agglomerative nesting   this is a agglomerative hierarchical clustering
algorithm 
   diana  divisive analysis clustering   this is a divisive hierarchical clustering
algorithm 

references
markov clustering  http   micans org mcl 
cluster package for r  http   cran r roject org src contrib descriptions cluster html
hierarchical cluster visualization  http   citeseer ist psu edu koren  twoway html
k mean gap statistic  http   citeseer ist psu edu tibshirani  estimating html
between ness centrality  http   www biomedcentral com                

fi