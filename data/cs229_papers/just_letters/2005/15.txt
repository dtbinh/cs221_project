learning to pick up a novel object

chioma osondu and justin kearns

   abstract
in order for an autonomous robot to successfully
function in a natural environment it needs to be able
to able to interact with objects it has never seen
before  specifically it needs to be able to manipulate
objects in the environment that it has no prior
knowledge of  the robot must not only be able to
cope with uncertainty in the shape of the object  but
also with uncertainty in the position and orientation
of the object  this problem is one that can be
approached with machine learning techniques  our
project is to teach a robot arm to pick up items it has
never seen before  we will attempt this problem
mainly with a supervised learning approach  we will
train the arm by showing it a variety of objects  it
will use monocular vision to extract features of the
object and we will show it how to pick up the object
up  given this training set  it will learn the best way
to pick up objects it has never seen before 
other work in this area includes using uncalibrated
stereovision to pick up an uncertain object at
cambridge university

   learning
this was done using linear regression 

    data collection
we started off with images of   objects  in this case 
markers of different colors and slightly different
shapes  for each marker  we had   different
orientations  for each orientation  we had   
pictures taken from different positions  as the robot
arm approached the marker  this process yielded a
total of     images for training purposes 
in
addition to collecting images  the robot joint angles 
operational coordinates and motor encoder values 
corresponding to each image  were saved to files
 one file per image   each image was     x    
pixels in size  i e  along the x axis and y axis  

    initial feature selection
our initial challenge in getting the robot arm to pick
up unknown objects was to determine what features

to use  since as mentioned earlier  we had images
and the corresponding robot position when the
image was taken as well as the final position of the
robot in the gripping position  we needed to
determine the relationship between these images and
the final orientation of the end effector when the
object was gripped 
the first attempt at feature extraction was done
through a feature vector program written by
ashutosh saxena  the program divides an image
into patches of equal sizes and returns    attributes
for each patch attained by using various filters  the
number of patches could be modified to suit
different training needs   after careful inspection 
we found that the optimal way of dividing up the
image was to have    divisions along the x axis and
   along the y axis  this guarantees that at least one
edge of the object would be captured by one of the  
patches  which we would be used in the next phase
of the computation 
we chose to use the   patches surrounding the center
of the object  as well as the center patch in a tic tactoe board configuration  this gave the total of  
patches mentioned above  determining the center of
each object was done in following way  first  the
image is converted to grayscale and then threshold
values are used to separate the background  which
was white  from the object  second  given the
points above the threshold  the largest of the clusters
found in the image is noted  finally  the midpoint in
the x and y direction are used to determine the image
center 

      training
for all of the learning described below  we used
linear regression  we also added a bias column of
all  s to the training matrix in each of the attempts
described below  for some of the tests  we
introduced a regularization parameter  lambda with
values ranging from     to       this did not
significantly improve the results  in fact  most of
the time  the results were identical to the results

fiobserved with the normal linear regression in closed
form  so we reverted back to linear regression
without regularization      out of the     images
were used for training and the remainder was used as
the test set  the chi squared error was used to
analyze effectiveness accuracy of our model  so all
the errors reported below are chi squared error
values with formula  y is actual orientation  

test error  we decreased the number of features  first
by taking various combinations of patches  such as
the   corners and center of our tic tac toe board as
well as other combinations of more and less patches 
as expected  this improved the test errors but had
the adverse effect of increasing the training error 

sum  y   predictedy       sum  y   mean y       

this approach  which seemed to be the winning
approach  uses a single highly informative patch 
the patch needed to have the most information
about the orientation of the object  to select this
patch we used different heuristics  for example  for
each patch  we found the average of its edge scores
and divided each edge score by the average  the
patch containing the edge with the highest value was
used as the best patch  the above heuristic used in
isolation did not select the most informative patch 

        training  take   

the    attributes corresponding to each of the  
patches were used as a row in the training matrix  in
the target matrix  each group of   rows 
corresponding to   patches of the same image 
would have identical values  in addition to these 
each of the    images that were captured while
moving to the final position was also associated with
  outcome  i e  the final orientation of the endeffector  this meant that    rows in the target
vector had identical values  this proved to be a very
bad approach to the problem as the both the training
and test errors were very high 

        training  take   

        training  take   

the training matrix was unchanged  but we decided
to look at our target vector differently  instead of
having each image group of    images taken en
route to the final position associated with the same
value in the target vector  we used the difference
between the gamma  the orientation of the wrist
joint  at the time the image was taken and the final
gamma  for testing  this gamma difference value
tells us how much to increase or decrease gamma to
match the object orientation  this approach was
minimally better than the previous approach 
we came to the conclusion that drastic changes had
to be made to our training model  many of the   
attributes were not informative with regard to our
problem  so we decided to concentrate only on the  
edge filters from the feature vector  ignoring the
other attributes that deal with texture  et cetera 
also  having   rows corresponding to the same
output value seemed inappropriate because it was
unintuitive 
        training  take   

the training matrix was set up in such way that the  
edges corresponding to each of the   patches
associated with one image were stored in   row 
therefore  there was   row per image and   
attributes per row 
this proved to be a better
model  with significantly better train than test error 
this also was a case of over fitting  to improve the

figure    the object the robot is supposed to grasp is seen
in the earlier picture  the estimated center of the object
and the   patches around the center that are used in the
heuristic computation below 

the heuristic that achieved the best results selected
the best patch by combining the process described
above with the following  the edge to average ratios
computed above were summed along the edges for
each of the   patches  i e  the sum of all edge   ratios
in the   patches and edge   ratios  etc  we
considered the two greatest scores as the two best

fiedges  then for both of the edges  whichever edge
was greatest in the most patches  we considered the
dominant edge 
the patch having the most
difference between the dominant edge and the
second best edge was then selected as the best patch 
this heuristic was further reinforced by visual
inspection 
using this patch  we tried all possible subsets of the
  edges and found that using   edges generated the
lowest test error of        with a training error of
        while the test error  was significantly better
than before  it was still not good enough to use in
practice  it was generally correct in regard to
following the shape of the curve  but the magnitude
was much smaller as can be seen in the graph below 

orientation value could be misleading  for example 
in the case of a white pen with a red cap  the hsv
would only pick up the red cap and the orientation
angle would be large even if the cap angle was not 
because of the small area 

figure    saturation channel of object with estimated end
points  the object used here is a remote control 

figure    the actual end effector orientation compared
with the predicted end effector position  using   edges as
described above  on the test data 

    alternate feature selection
we decided to start from scratch on our own
features  a lot of the collected images had bad
shadows  making it difficult to determine the ends of
the object from the threshold value  we used the
saturation channel of hsv  which generally
eliminated the background and returned only parts
 non white  of the object  we used the method
described earlier to estimate the ends of the object
based on the position of the largest cluster  with
these points  we needed to determine the coordinates
of the object ends  we knew an end of the object
was either in the top left or bottom left  so we
checked the region in the top left and if part of the
object was there  the object was oriented top left to
bottom right  if it was not  then it was oriented
bottom left to top right 
the object orientation was determined by finding the
angle between the vector pointing in the y direction
from the object center and the vector from the center
of the object to the end point  this angle alone did
not seem to be enough because the magnitude or the

to fix the orientation magnitude problem  we added
the area that we analyzed as a feature to enable it to
learn when to give more and less weight to the
magnitude of the value  it also seemed logical that
the position of the camera when the picture was
taken would affect the analysis  so the difference in
the camera and object position as well as the pitch of
the camera were included 
we tried all subsets of the   features and found the
pitch did not help  so we narrowed it down to the  
features  heuristic angle  camera to object distance 
and area of the object analyzed  these   features
generated the best test error  at that point in the
project  of       

figure    the actual end effector orientation compared
with the predicted end effector position using the features
described above on the test data 

fiedges with the area  angle of orientation and distance
between the camera and the object as features 
 

figure    the   edge filters that produced the best test and
training error 

randomly selected rows of the entire feature matrix
were used as training data and the remainder was
used for testing  of the    images for each object
orientation  one image was randomly selected for
test and the other   were used for training 
figure    the actual end effector orientation compared
with the predicted end effector position using the features
described above on the training data 

the test error was good compared to previous cases 
but looking at the graph of actual output vs 
predicted output  the magnitude and sign were better
matches compared to the previous graph  but it did
not fit the curve of the graph 
since the previous features in the other training
experiments fit the curve better  we combined the
two sets of features  so that the number of features in
the training matrix was    therefore  the total
number of columns in the training matrix was   
including the bias column  the test error here was
     
we tried training and testing on other parts of our
    images using these features and found that our
test error was between     and      which was worse
than previously observed  this can be attributed to
biasing our data toward the pens we trained on  so
we mixed up our data so that we trained on all types
of pens and tested on all types of pens  which
resulted in test errors of about       however  the
graph of predicted vs  actual was more promising  so
we tried out the predicted values on the robot  the
results of the experiments did not meet our
standards 

figure    the actual end effector orientation compared
with the predicted end effector position using the edges
from figure   combined with the other three features
mentioned above  this is on training data 

      training  final take 

since the best results described above were not
much better than random data selection  we decided
to collect more data  this time the selection of
objects included a variety of long thin objects  the
lighting was also better this time around as there was
a light source directly above the robot  eliminating
most of the shadow  the total number of images in
the collection grew to      we ran similar tests and
found this time  that it was more fitting to use  

figure    the actual end effector orientation compared
with the predicted end effector position using the edges
from figure   combined with the other three features
mentioned above  this is on the test data 

as can be seen from the figures above  getting more
data proved to be a worthwhile decision  the
observed test error was      and the train error was

fi      mostly from magnitude misses  while there are
some magnitude misses  the robustness of the
robots gripper allows it to grasp objects with
orientation values within range 

arm was oriented correctly  but estimated the center
poorly 

    testing
the robot takes a picture of an object and writes the
image to one file  while the corresponding joint
angles  operational coordinates  and motor encoder
values are written to a separate file  this image is
then analyzed and an end effector orientation  which
is equivalent to a single target value  is returned
using the processes described above  since all of the
analysis and training was done in matlab  to fully
automate the robot motion  we needed to integrate
the c   code and the matlab code  to this end  the
matlab code was compiled into a standalone
executable  a call was then made from the c  
code to the matlab executable after the files needed
for analysis were generated  the matlab executable
would output a file containing the predicted gamma
difference value  which would then be used to
generate the final end effector orientation  as well as
the object center  the object center was needed to
compute the values of some of the other robot
operational coordinates  we use the camera intrinsic
and fixed z plane of the table to compute the objects
location in the robots coordinate system and move
to that position with the learned gamma value  we
make   assessments of the object location from
different positions and choose the median  if the
robot grips the object  it puts it in a box  otherwise it
returns to the start position and tries again  since the
z plane was fixed  the most important coordinate to
facilitate the desired outcome was the orientation of
the gripper  the gripper had to be turned the right
way for the robot to successfully pick up the object
in its view 

    experimental results
we tested the robot with   objects in arbitrary
orientations that were not included in its training set 
  pens of various color and shape    eraser  and  
coiled usb cable  the robot correctly guessed the
sign of the orientation on all attempts where the
object was clearly oriented in   direction  when the
object was oriented straight up  the robot guessed a
small orientation in either direction  the match of
the magnitude of the robots orientation was not
exact  but it was always in a range close enough for
the robot to get the objects in its gripper on every
attempt it made  with   exception on the coil of
wires in a crescent orientation in which the robot

figure    the harmonic arm in the start configuration 
while analyzing the object  left   the harmonic arm in
the final position  gripping the object with its endeffector  using the correct orientation to ensure that the
object can be picked up 

   conclusions
initially  it seemed highly unlikely that a linear
classifier would be successful at this task  this was
because  it was hard to imagine a linear relationship
between the orientation of the end effector and the
features  which were used during the training  our
initial attempts were discouraging because even
though we were able to drive the training error
down  the test error only increased  after extensive
analysis of our initial features and target variables 
we were able to lower the test error  even though the
values would not be good enough  in practice 
this led to the creation of an unrelated set of
features as described earlier 
the major breakthrough here was combining the two
sets of features giving us values for the orientation
of the end effector  which could actually be used
with an amazing degree of success in practice 

acknowledgements
ashutosh saxena was an invaluable resource and
mentor throughout the project and we would like to
say a special thanks to him  also  many thanks to
jimmy zhang and anya petrovskaya for help with
the harmonic robot arm 

references
   saxena  ashutosh  chung  sung h     ng 
andrew         learning depth from single
monocular images 

fi