learning to automatically discover meronyms
austin shoemaker and varun ganapathi
december         

abstract

natural language processing tasks 

search would

benet from the ability to perform shallow semanwe present a system for automatically discovering

tic queries 

meronyms  noun pairs in a part whole relationship 

to search for all terms that bear semantic relation to

from text corpora  more precisely  our system begins

some other terms  one important semantic relation

for example one would like to be able

by parsing and extracting dependency paths similar

to extract is the part whole relation  otherwise known

to  but not the same as  those used by  snow et al  

as meronymy  one question that one would like to be

      

for each noun pair we calculate an empiri 

able to ask is does x contain y  or what does x

cal distribution over dependency relations  which are

contain   the latter of course being a harder ques 

then used as features of a support vector machine

tion  

classier 

noun pairs are labeled as meronyms if

there exists a path traversing only meronym and hypernym links between the nouns  since the method

the meronymy relation is surprisingly ambiguous

of labeling training examples treats sentences as bags

as the authors discovered when trying to manually

of words  our training examples are extremely noisy 

label sentences 

however  we are able to nd a classier that performs

relations often apply only to a specic instantiation

better than similar previous work 

of an entity rather than the general case 

unlike other relations  meronymy
for in 

stance  the dog had brown fur and oppy ears  in

 

this case  it is only the dog in this specic sentence

introduction

that has brown fur and oppy ears  should one label brown fur as a meronym of dog  or merely label

many natural language processing applications de 

fur and ear as a meronym of dog  since most dogs

pend on ontologies such as wordnet in order to ob 

have fur and ears  it s clear we re making decisions

tain prior knowledge about the semantic relationships

based on background knowledge rather than knowl 

between words  unfortunately  the domain of word 

edge conveyed by the sentence 

net is limited in scope  and is time consuming and expensive to maintain and extend  furthermore  wordnet has no concept of probability  for a given word 

our task is to detect when pairs of words could

wordnet stores a list of its relations to other words 

possibly be meronyms under normal circumstances 

but does not store the probability of the occurrence

in other words  even though in one particular set 

of that relationship in normal usage  recently  sub 

ting a man may have a tail  we would hope that our

stantial interest has been directed toward the idea

system would not strongly imply a meronymic rela 

of automatic detection of semantic relations between

tionship between man and tail  in general  a car has

words 

windows even though there are jeeps   so we would
want the algorithm to identify window as a meronym

automatic extraction of semantic relations between nouns from text corpora is important to many

of car 

 

fi   

related work

 a  collect noun pairs from corpora
 b  for each noun pair  collect sentences where

while part of whole relations have been studied in

the nouns occur together 

great depth in the liberal arts  we were unable to
nd many papers investigating automatic methods

 c  parse the sentences and extract dependency

for discovery of meronyms  hearst  hearst        in 

paths relating pairs 

vestigated the use of high precision lexico syntactic
patterns indicative of hypernymy relations 

 d  label each noun pair with its features using

how 

wordnet

ever  this approach was not successful when applied

 e  train a classier based on this data

to meronymy  because the common patterns were also
indicative of other semantic constraints 

   test 

berland and charniak identied ve lexical patterns that tend to indicate part whole relations by se 

 a  for any pair of nouns  extract features and

lecting the patterns that maximized a likelihood function

p w p 

and a variation of same  where

w

apply classier 

is the

outcome of the random variable generating wholes 
and

p

   

is that for parts  meronyms were selected by a

 surprise  metric  which is high when

p w p   p w  

we mostly follow snow et al  in our feature extrac 

or a signicant  dierence test  the proposed system

tion 

was about     accurate on the top    proposed parts 
girju et al 

generation of features

based on the assumption that local syntactic

structure can help predict certain semantic relation 

worked on improving the results of

ships  such as meronymy  we selected features that

hearst s approach  hearst s method of using lexico 

encapsulate the syntactic relationship between a pair

syntactic patterns lters the input corpus resulting in

of nouns as they occur in a given sentence 

noun pairs that are more likely to be in meronymic

patterns are called dependency paths  we apply a de 

relations  in that paper  they learned semantic con 

pendency parser  which produces a directed acyclic

straints using wordnet class labels as features in

graph of syntactic relations between words  the de 

order to remove false positives from the result of

pendency path  then  is the shortest path separating

applying lexico syntactic patterns 

the noun pair in this graph  a single semantic rela 

they achieved

these

    accuracy  but involved the use of much manual

tion is expressed as

relation word  pos   word  pos   

work  the corpus was rst ltered by hand picked

where

represents the word in a specic

lexico syntactic patterns  word sense disambiguated

syntactic class  and relation marks the manner in

by hand  and nally class labels were extracted from

which one word governs the other 

wordnet  as such  we do not see them as a fair com 

wordn pos 

we make one modication to the feature extraction

parison to the work we describe here 

algorithm as follows  even with moderately large cor 

snow et al         propose an automatic method

pora  sparsity can still be a problem  making it more

using dependency paths for automatically identifying

dicult to classify new examples  our system com 

noun pairs in a hypernymy relationship 

pensates for this by introducing

in spirit

anonymized

depen 

our approach to the problem is similar  although we

dency path features  which describe the grammatical

modify the feature set and use a dierent classier 

structure of the syntactic relationship while leaving
the identity of the specic words unspecied 

 

this

improved results signicantly 

methodology

   

our approach is as follows 

generation of labels

we labeled each of the noun pairs extracted from our
   training 

corpora automatically using wordnet  of course  we

 

filost examples if wordnet did not contain the pairs of

figure    wordnet compared to human labeling

nouns  which happened quite often  given a noun x
and y  we label y a meronym of x if in wordnet it is
possible to reach y from x using only hypernyms and
meronymy relations  consider the following example 
where

x y

x   y

means that

means that

x

x

is a meronym of

is a hypernym of

y 

and

y 

management  management 
   directorate board of directors 
   board 
this was derived from  peter cawdron  group strategy development director  and bill shardlow  group
personnel director  will become part of the board  s
management committee 

beginning from the word in the sentence management  the system looks up the sense  or senses  in
wordnet coresponding to that word and then search
for senses matching board  in this case we see that
management contains a board of directors which is
a type of board  hence  the management contains a
board  this doesn t always work out so well 

level    floor level storey 
   structure construction 
   foundation  base 

nouns actually were semantically related in the sentence 

of these  a small number actually were in

a meronymic relationship 

figure     displays the

this noun pair was derived from the following sen 

confusion matrix of wordnet s labels vs the human

tence  this has increased the risk of the government

labels  it s clear that wordnet is adding signicant

being forced to increase base rates to     from their

noise to the training data 

current     level to defend the pound  economists
and foreign exchange market analysts say 

   

as one can see  word sense ambiguity results in
spurious labelings of sentences 

such spurious sen 

training

we used svmlight to train several classiers using

tences create dependency paths between nouns that

dierent kernel and gamma values 

are not indicative of meronymy  despite such exam 

quency of meronyms in data is extremely low  on the

ples  it s the case that while this particular sentence

order of     we created balanced training and test

may not be informative of the relation bewteen level

sets by subsampling the negative examples 

since the fre 

and base  other sentences may contain more valuable

we began with a    k sentence corpus from the

relations  and our classier may be able to tease out

     new york times  after processing  this resulted

the relationship 

we provide this as an example of

in a total data set size with         pairs  of these 

the noisyiness of the data on which we are operating 

about      pairs were in a meronymy relationship 

in order to examine the value of the wordnet la 

we created a balanced training set of      positive

bels  we hand labeled      noun pairs as meronyms 

and      negative example  a balanced test set of    

we consider a pair of nouns to be meronyms if the

positive and     negative examples 

 

figaussian  rbf  



precision

recall

linear

      

      

       

      

      

 

future work

this research has illustrated the strengths of using syntactic structure to predict semantic structure 

figure    comparison of svm kernels

while underlining the reality that complex relationships such as meronymy can often only be inferred
with a sucient understanding of the local semantic

figure    precision recall for rbf kernel 

context 

        

while our classier was trained using wordnet 
we recognized that with our human labeled data as
ground truth  wordnet achieves only     precision
and      recall 

wordnet  aside from error result 

ing from annotation mistakes  by denition cannot
represent semantic relations in terms of context  and
thus limits the ability of our classier to grasp human intuition 

moving forward  it is clear that the

next step is to collect more human annotated data
and train on much larger text corpora 
the authors speculate that incorporating a probabilistic model for co reference resolution would likely
improve classication accuracy substantially  because
it would enable syntactic relations to be traced across
sentences  part of the challenge lies in the fact that
meronymy relations are often assumed to be background knowledge  and the words frequently occur
together with no explicit mention of the relationship 

 

results

analyzing context across sentences could potentially
capture these patterns of discourse 
our analysis using wordnet shows that the ratio
of the number of holonyms and meronyms stemming

in the table below you can see the precision recall

from a word sense was strongly indicative of its own

as reported by svmlight for various parameter set 

meronymy  e g 

tings  we experimented with various kernels  including linear  polynomial  and found that the rbf kerfor the rbf kernel to be      using

 parameter
loocv  figure  

displays the precision recall curve 

as you can see 

nel performed best 

versely  items that are part of many entities are more

we selected the

likely to be a meronym in any given situation  this
could perhaps be estimated by counting the number
of occurrences of  of x  or  x s  without considering

the classier can achieve quite high accuracy rates
when the recall is limited 

if an entity contains many things 

then in general it is more likely to be a holonym  con 

the other noun  the next step is to add such proper 

this is reasonable con 

ties to the feature lists of the noun pairs themselves 

sidering that wordnet is an extremely noisy sample
of the truth  therefore  many of the positive exam 

 

ples are actually random  in a sense  because none
of the dependency patterns are actually indicative of
meronymy 

conclusion

we have investigated the challenges of automat 

we plan to test the classier against human labeled

ically

data to see how it fares against wordnet 

extracting

holonym meronymy

relationships

from text corpora  and proposed an svm classier

 

fibased on an extension of syntactic dependency paths 
the best classier had        precision at       
recall  our eorts have illustrated the challenge and
promise of identifying meronymy relationships using
global syntactic structure  the authors believe that
signicant improvements in accuracy can be made
with a larger quantity of human labeled data  and by
leveraging the intrinsically context dependent nature
of meronymy in more sophisticated ways  this problem is emblematic of many of the challenges faced
in nlp research today  and its solution will enable
an order of magnitude improvement in information
access and discovery technologies 

references
berland and e  charniak        finding parts in very
large corpora  in proceedings of the   th annual
meeting of the association for computational linguistics  pages       college park  md 

r  girju  a  badulescu  and d  moldovan       
learning semantic constraints for the automatic
discovery of part whole relations  in proceedings of
the human language technology conference   north
american chapter of the association for computational linguistics conference  edmonton  canada 

marti a  hearst        automated discovery of wordnet relations  in christiane fellbaum  editor  wordnet 

an electronic lexical database  mit press 

cambridge  ma 
t 

joachims 

making

large scale

svm

learning

practical  advances in kernel methods   support vector learning 

b  schlkopf and c  burges and a 

smola  ed    mit press       
rion snow  daniel jurafsky  and andrew y  ng 
 learning syntactic patterns for automatic hypernym
discovery   nips      

 

fi