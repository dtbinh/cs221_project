learning depth in light field images
douglas v  johnston

 

introduction

plenoptic camera can be found in     
this paper will structure the data is a slightly
different manner than that of a plenoptic camera 
but one that can be mathematically manipulated
in a similar fashion  instead of one high resolution camera  with a sensor of m by n pixels  we
have a number of low resolution pinhole cameras 
arranged in a plannar grid  the dimensions of
this grid are labeled as u and v  each camera has
a resolution of s by t pixels  see figure    it is
easy to see that the overall number of sensor elements is u  v  s  t  which we will set equal to
the m  n resolution of the single high resolution
camera  our implementation assumes an original
resolution of    megapixels  or             with
u and v both equal to     giving an array of    
cameras  dividing the original resolution by     
we end up with individual camera resolutions of
         pixels  which is the resolution used in
this project 
in order to make use of the data  the images
are aligned and placed into a focal stack  the focal stack is created by giving different offsets to
each image in the u  v plane  and hence creating focus at different depths using pictures taken
at the same time  the rest of this paper will use
the data from a camera array as opposed to a
microlens camera  due to the abundance of camera array training data  however  the theory presented here can easily be transformed to operate
on a microlens camera 

photographic images reduce the three dimensional
world they are capturing into a  d plane  there
are a variety of applications in which it would
be useful to determine the original distance to
objects in the scene from the focal plane of the
image  such applications include robotic vision
systems   d scene reconstruction  and surveying 
recent efforts to provide automatic analysis of
scene depth using a single image have proved quite
successful     however  ambiguities still arise in
complex scenes  and non general assumptions of
the environment are made in ensure the best response of the algorithms  in contrast to traditional cameras  recent work has made use of a light
field camera  which captures the  d light field of
the scene  the details of how this is done are
described below  by using the extra information
in the light field  we hope to provide a more general implementation of a depth map learning algorithm  which requires fewer parameters to train 
and which will work in a variety of environments 

 

light field acquisition

a hand held  plenoptic camera    is capable of
capturing information such that the raypath of
light hitting a pixel can be determined  in practical terms  this has the advantage  among others 
of enabling a single photographic image to be refocused at varying focal planes across the scene 
the ability to refocus the elements of the scene
comes at the price of reducing the resolution of
the  d image  for every additional separate focal plane captured  the resolution of the image is
halved  the details of the implementation of a

   

image processing

the synthetic aperture data is processed in matlab to give independent focal plane data  by recombining different offset pixels from each image 
 

fifigure    to simulate a plenoptic camera  an array of camera is used to capture many low resolution
images of the same scene simultaneously  the images are aligned and  using a synthetic aperture  a
focal stack is created  with each slice having sharp focus at a different depth in the scene 
convolution filters  we independently calculate
the response of each of         spot detectors 
for relative depth  we compare the gradient for
each image patch and its surrounding   neighbors
 left  right  up  down  front  and behind   we define our depth measurement potential as

several results are shown in the figure   

 

determining depth

to determine a depth map for the entire image  we
break the image down into component subimages 
and assign a depth value to each subimage  because of the powerful depth estimation available to
us by using light field images  we place a high importance on the information we can extract from
the focal stack  the reasons for doing so are twofold  first  to improve run time performance  we
attempt to limit the number of features used to be
as small as possible     secondly  the ability to use
the focal stack is what sets this technique apart
from others  so we attempt to use the data contained within to the greatest extent  the larger
synthetic aperture used  the greater the circle of
confusion will be  and hence the more blurred objects will appear if they are not at the focal plane
of the individual focal stack image being evaluated  to account for image registration error  we
apply a two pixel wide gaussian blur function to
the resulting edge image 
features must be identified which can give information on the depth of a subsection of the image  we use the notion of both relative and absolute depth to help build our features  for absolute depth  we look for texture using a set of

 

m x
f
x
 dij  xtij  
 
i   j  

where i is the set of image patches  j is the set of
focal stack images  d is the depth  x is the absolute
feature vector  and  and  are parameters of the
model  secondly  we create a depth smoothness
prior    using our described method for calculating relative depth 
 

f
m x
x  xij  xkj   
x
 
i   j  
kn  i 

where the variables are the same as above  with
the additon of n  i  which is the set of six neighbors  final  we use the general markov random
field equation
p  x   x   

 
 
exp  u  x  
z
t

using the combination of  and  as our energy function  and z  our normalization constant 
putting the three together we have
 

fifigure    single camera image and two focal stack images  the focal stack contains a discrete set of
focal depths throughout the scene  sixteen images comprise the focal stack during this experiment 
shown here are two such images  left  image from one camera  center  only the foreground is in
focus  right  only the background in focus 

figure    left  actual depthmap  right  computed depthmap  areas without texture are not
computed well 

figure    depthmap created from real data using stanford camera array  left  original image from
one camera  right  computed depthmap  significant improvement over images with no texture is
made in images such as this with lots of texture variation 
 

fi 
p  d x       

limitations of the model

 
 
exp        
z
 

there are some trouble areas  and room for improvement in the model  because of the reduced
which we use to learn the depthmap 
spatial resolution of the images  fine textures are
not able to be resolved  many of the man made
objects present in the scenes  such as wood ta  results
bles  white walls  and granite floors exhibit texture only at small spatial scales  which is lost in
the model was trained on both real and synthetic
our images  in image segments without significant
data  from a variety of scene locations  synthetic
features  there is little difference in the segment in
data was modeled in pov ray  while real data
different focal stack images  therefore  our heavy
was acquired from the stanford camera array 
reliance on edge detection in the segment breaks
in all  over     images were used to train  bedown  and learning in the segment is poor 
cause one of the goals of the project is to make as
generic a learning model as possible  the test data
varied significantly from the training data  the
acknowledgments
model responded well to the new images  as the  
learning mainly relies on edges  this makes depth
recognition in previously untrained locations quite we thank vaibhav vaish and mark levoy for the
possible 
 d camera array image data used for training 

references
    james diebel and sebastian thrun  an application of markov random fields to range sensing 
nips          
    jeff michels  ashutosh saxena  and andrew y  ng  high speed obstacle avoidance using monocular
vision and reinforcement learning  proceedings of the twenty first international conference on
machine learning  icml            
    ren ng  light field photography with a hand held plenoptic camera  stanford tech report  ctsr
              
    ashutosh saxena  sung h  chung  and andrew y  ng  learning depth from single monocular
images  nips          

 

fi