applying machine learning to mlb prediction   analysis
gregory donaker
gdonaker cs stanford edu
december         
cs     stanford university
introduction
major league baseball  mlb  is a multi billion dollar statistics filled industry 
individual players are chosen based on their raw statistics such as batting average  onbase percentage  or slugging percentage  furthermore  there are communities of fans
who closely follow the statistics of the game to analyze the contributions of individual
players and including them in fantasy leagues  there is no definitive formula for what
factors will lead a team to victory  yet by analyzing many years of historical records
many trends may emerge  i am applying machine learning techniques to predict the
outcomes of individual major league baseball games and demonstrate the relative
importance of different elements of a team  i believe that applying these techniques to a
broad set of features will provide not only a basis for prediction but also reveal potential
strengths and weaknesses of individual teams by quantifying the significance of a specific
asset and its relevance to victory  such a tool could prove invaluable to mlb managers 
sports broadcasters and fantasy sports enthusiasts 
data sources
i have harvested my data from two sources for the two levels of granularity they
offer  the baseball archive  http   www baseball  com  offers downloadable
historical statistics for teams  batting and pitching on an annual basis  while this data is
very useful for incorporating in broad statistics  it does not provide the game by game
granularity that my research needs  unfortunately  i could not find a source of ready to
process boxscores so i have had to compile them via web scraping  baseball
boxscores  http   www baseball boxscores com    has thousands of complete boxscores
for a subset of mlb teams  including about two hundred complete team seasons  i used
perl scripts to harvest a total of     complete team seasons         individual games   i
selected the complete seasons for the boston red sox  baltimore orioles  chicago cubs
and atlanta braves and for all years between      and       my team selection was
based on the limited number of box scores available and the desire to have two teams
from each the national  atlanta and chicago  and american  boston and baltimore 
leagues  i compiled the individual game data in a form where it can be relatively easily
cross referenced to the broader dataset for feature extraction 
features
i have chosen a set of twenty five features which form a    dimensional vector
representing a single game  each feature must be extracted computed from the different
datasets and includes only information available before the game officially started  most
of my selected features are not readily available but must be computed based on the
boxscores of the game and the proceeding games  my features include 
 

my apologies to baseball boxscores com for putting an abnormally large load on their servers while
harvesting the data 

fi
























and


opposing team record of previous season               
win in previous game             for first game of season 
wining percentage over the previous    games               
wining percentage over the previous    games               
starting pitcher era  in previous game         divided by   
starting pitcher era in previous season         divided by   
opposing starting pitcher era in previous season         divided by   
head to head matchup between the two teams               
home game           for a home game 
hits by starters in previous game         divided by    
hits by starters in previous    games         divided by     
left handed starting pitcher       
left handed opponent starting pitcher       
percentage of batters opposite handed from opponent starting pitcher           
percentage of opponent batters opposite handed from starting pitcher           
average age of starters         divided by    
average age of opponent starters         divided by    
average height  inches  of starters         divided by    
average height  inches  of opponent starters         divided by    
average years since starters mlb debut         divided by    
average years since opponent starters mlb debut         divided by    
average batting average of starters in previous season               
average batting average of opponent starters in previous season               
avg num of at bats by starters in prev season         divided by     
avg num of at bats by opponent starters in prev season         divided by     
win         for supervised learning  testing 

in the case that no previous era for a pitcher can be found  i am assigning a value of    
 a relatively average era   winning percentages for the first games of the season do not
reference the previous season  rather i calculate them as if the team was             
before the start of the season  head to head matchups are smoothed by initializing with
one win and one loss  i acknowledge that some of these assumptions smoothings may
not be ideal  yet throwing out all games with an imprecise feature set would produce a
model that would not apply to roughly     of games played 
while the divisors on some of these statistics may seem arbitrary  they have been
chosen to scale all features to a range between roughly zero and one  as defined  no
features can be negative  yet many features have the ability to exceed one  this scaling
was done so upon examining the trained weights  one can get a good sense of the relative
importance of different features  many more features would be available if performing
real time gathering  yet many desirable statistics are extremely difficult to compute based
on the difficulty of obtaining complete historical data 
training and test sets
i designed my training and test sets to be as close a representation as possible to
applying a classifier on a future season  i built both generic and team specific classifiers 
from here on  i will define a horizontal dataset as one which includes data for many
teams for the same time period and a vertical dataset as one composed entirely of a
single teams games over a longer time period 

 

earned run average   number of earned runs allowed in   innings 

fimy primary horizontal dataset consists of the           seasons for the red
sox  braves  orioles and cubs     team seasons        games   the corresponding test
set contains the      season for the same four teams    team seasons      games   i
initially implemented and tuned my classifiers on red sox examples  so i chose to
evaluate my vertical classifiers on an arbitrary different team to avoid bias  my primary
vertical training set is composed of all atlanta braves games for the                      and           seasons     seasons        games   the corresponding test
contains all braves for the            and      seasons    seasons      games   while
some classifiers performed notably better on different datasets  i originally assigned these
for my training and test sets and will report the errors accordingly 
applied algorithms
i implemented a range of machine learning models for the two datasets defined
above including logistic regression  nave bayes  support vector machine  svm  and
an ensemble classifier over the other three  i initially implemented the perceptron
algorithm but decided to limit my focus to the others in order to maintain an odd number
of models for the ensemble classifiers  as i had been gathering and preprocessing my
features in perl  i chose to use perl for my logistic regression implementation 
i used multinomial nave bayes both with and without including the priors of the
training sets  while my datasets are close to     wins  i wanted to see the effect of any
imbalance on the resulting classifiers  both these implementations were performed in
matlab 
i attempted to train my svms using my matlab smo implementation  yet my
running time was excessively long so i switched to an optimized off the shelf product for
these models  svm light  provided an efficient svm implementation allowing me to
easily train multiple models without further trimming my datasets  i built models with
both linear and polynomial kernels  trying numerous parameterizations of the polynomial
kernel 
results and discussion
my result accuracies are summarized in the following two tables 
classifier  vertical 
logistic regression
nave bayes
nave bayes  no prior 
svm  linear kernel 
svm  polynomial kernel 
ensemble  majority
ensemble  unanimous

 

accuracy
      
      
      
      
 all win 
      
      

classifier  horizontal 
logistic regression
nave bayes
nave bayes  no prior 
svm  linear kernel 
svm  polynomial kernel 
ensemble   majority
ensemble   unanimous

accuracy
      
      
      
      
 all win 
      
      

t  joachims  making large scale svm learning practical  advances in kernel methods  support vector
learning  b  schoelkopf and c  burges and a  smola  ed    mit press       

fithe confusion matrices corresponding to the above results demonstrate that the
performance of these models is better than just selecting the outcome with the higher
prior  for all models  the number of true positives exceeded the number of false positives
and similarly the number of true negatives exceeded false negatives  this trend can be
easily observed in the logistic regression confusion matrix for the vertical test set 

predicted win
predicted loss

actual win

actual loss

   
   

  
   

logistic regression outperformed all non ensemble classifiers  one of the most
useful attributes of this model is that the converged weights indicate a relative importance
of different features  as expected  the weights for the vertical and horizontal datasets
differ in their ordering and in some cases even their sign  the weights on the horizontal
dataset could be used when assembling a baseball team to help chose one player over
another  the two most significant weights were the sum of starter at bats in the previous
season and the starting pitchers era in the previous season  both of these match
common intuitions of what makes a successful team  i am not implying that these
features cause the win  but rather noticing a strong correlation  the least significant
weight  on the other hand  was the number of hits by starters in the previous    games 
the value of this feature was likely subsumed by the teams record over the same period 
the interplay between these weights would be useful for a baseball owner  sports
gambler or fantasy sports enthusiast  it is worth noting that my accuracies of logistic
regression over the four possible vertical datasets ranged significantly  with accuracy
over     for the chicago cubs vertical 
while the accuracies for the different nave bayes models  with and without
priors  are identical  the models produce different confusion matrices  the results had
the same margins  but were shifted up or down based on the prior  as i am interested
in the most general and effective models possible  i used the model without the prior
probability for my ensemble classifier 
even though the accuracies were not as high as logistic regression  svms with a
linear kernel provided tangible results  for both datasets  svms with polynomial kernels
produced models that predicted that all elements of the test sets were of the positive class
 wins   i trained multiple models  varying parameterizations of the kernels  all giving
the same result  while i was unsuccessful in finding a useful parameterization  this does
not mean that svms with polynomial kernels cannot be used for baseball classification 
the ensemble classifiers improved accuracies over the datasets  the standard
classifier made a prediction based on the majority vote of the three sub classifiers  while
the unanimous classifier returned unknown for any game where the three classifiers did
not completely agree  the higher accuracy of the unanimous ensemble classifier can help
assign an effective confidence value for a given prediction 
when looking at the above results  it is important to note that the records of
baseball teams do not exhibit uniform distributions  in the      season  for example  the
team with the best record won       of its games while the worst won         even the
 

statistics as reported on mlb com

fibest teams lose with a certain amount of underlying randomness  hence one cannot
expect a baseball classifier to ever have near perfect accuracy  while my classifier
accuracies do not reach my initial hopes  they do demonstrate a practical application of
machine learning techniques and offer insight into the dynamics of the game 
conclusion
baseball prediction is a challenging and uncertain endeavor  while my classifiers
were not able to achieve the accuracies i had hoped for  they did reinforce that there is a
strong correlation between these quantifiable features and victory  with the underlying
randomness of baseball  i imagine that any classifier  human or machine  would be
unable to demonstrate the high accuracies achieved by some common classification
applications  regardless  my experiments demonstrate the feasibility of applying
machine learning to baseball and suggest potential improvement with a more extensive
feature set 

fi