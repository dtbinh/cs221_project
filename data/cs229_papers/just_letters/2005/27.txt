semantic extensions to syntactic analysis of queries
ben handy  rohini rajaraman
abstract
we intend to show that leveraging semantic features can improve precision and recall of query results in information
retrieval  ir  systems  nearly all existing ir systems are built around the  bag of words  model  one major
shortcoming of this model is that semantic information in documents and queries is completely ignored  it has been
shown in a limited setting that some syntactic information can be useful to ir      we intend to show that
leveraging semantic features such as synonyms can improve precision and recall of query results  especially when
used in conjunction with syntactic relationships  in this paper  we describe the various machine learning algorithms
and feature sets that we use to learn a binary relevance classification function  at this stage  we are not attempting to
build an actual ir system using our techniques  but we hope to show the benefits of such a system 
motivation
traditional ir systems that are based on the  bag of words  representation of documents often return irrelevant
documents simply because they contain words that appear in the query  such systems also ignore relevant
documents that may contain information related to the query albeit expressed in words that aren t exact matches but
are closely related to words in the query  as an example  consider the query that contains the phrase  reduce cost but
increases pollution   this query should return documents that discuss  cutting price although raising contamination 
as more relevant than documents than contain  increases cost and reduces pollution   another example is that if a
query contains  increase pollution   then it should avoid results that contain  does not increase pollution  
the data
we used the nyt  apw  and xie collections of the      aquaint corpus  these are three of the five collections
used for the trec      hard track  our query topics and relevance judgments are also
taken from the trec      hard track  these three collections contain a total of         news articles amounting
to      megabytes of text  the trec      hard track contains    topics  and we used a subset of the descr
tagged questions as our queries  we chose only questions for which there was a reasonable number of both relevant
and irrelevant examples  in our training and testing  we chose to ignore documents on which the judges could not
agree  as it is not clear how we would like our system to classify these documents  since we need the syntactic
information in the documents  we have pre processed the documents with the natural language parser minipar so
each document is transformed into a list of relations of the form  word  word type relation word type word  where
relation is one of twenty nine different relation types and wordtype  and wordtype  can take on twenty two
different part of speech values  in this initial investigation  we do not attempt to retrieve random documents out of
the document collection  instead  we will examine the same set of documents that have been rated by human judges 
in our experiments  we consider both the case where our testing sets consist of documents query pairs for the same
queries as our training set  and the case where we are testing on a different set of queries than we have trained on 
while the goal is to train a system to perform well on queries it has never seen  we can still learn much about the
shortcomings and potential of our system by testing and training with the same queries 
the features
to determine whether a candidate document is relevant to a query  we compare the minipar relations in the query
to the minipar relations in the document  however  rather than direct relation comparison  we first would like to
find other possible relations with similar meaning  for each query relation  we use wordnet to find other words that
are related to word  and word   we maintain the following different similarity classes for each word in the query 
synonym  hypernym  hyponym  holonym  and meronym  now we can compare each   tuple minipar relation in
the query with each   tuple in the document  and identify both perfect matches and partial matches that may serve as
reasonable substitutes 
for our initial investigations  we kept a count of exact matches  off by synonym matches  off by hypernym
matches  etc  for all matches and partial matches  we require that the relation type be the same in the query relation
and document relation  we also required that one word in the relation be an exact match  but this yielded extremely
sparse vector  in addition  we consider an off by synonym match to be a different kind of similarity for different

 

firelation types  this results in similarity vectors for our query document pair consisting of num syntactic relations
x num semtanic relations entries  each representing the count of matches of a different similarity type  using the
five semantic features from wordnet listed above plus exact matches  and all    semantic relations from minipar
resulted in vectors of size         
after testing this approach with several queries  we realized that our feature vectors were extremely sparse  in order
to get more information to base our classification on  we relaxed the constraint that one word in the relation must
form an exact match  now the off by hypernym partial match category encompassed relation matches where
neither the first words nor the second words formed a weaker match than hypernym  in addition  we added  
features to the end of the vector  one for each semantic similarity class  these values counted total partial matches
for each similarity class  regardless of relation type  while these new features had some dependence on existing
features  it was useful to have a few more non zero features on which the classifier could base its decision 
in a third investigation  we tried using a much smaller number of features that would hopefully capture a similar
amount of information  now we had only one feature for each relation type  and we used this to keep a weighted
count of complete and partial matches  we chose to give partial matches  the weight that exact matches received 
this approach allows us to better learn the appropriate weighting to give to each type of relationship  but we do not
learn the perhaps more subtle waiting between the different similarity classes 
we did not have a chance to experiment with other possible feature sets  but there were several that we considered 
one subtle variation to our approach would be to add an additional num relation types number of features to the
end of each vector counting the number of occurrences of each relation type in the query  this could help ensure
that a new classification is based primarily on training examples for syntactically similar queries  larger feature
vectors could be created by counting different kinds of relation mismatches  however  without some way of
grouping the relation mismatches  this would cause the feature vector to become prohibitively large and require an
unreasonable amount of training data 
wordnet also has a much richer set of features than the ones we are currently using  and our initial experiments can
help us determine which features we should drill down to a finer granularity  for example  we are currently using
the recursive search functionality to gather all of the words in a similarity class  but by counting the words at each
level differently  we could learn more subtle weightings for each partial match 
it is also possible to take completely different approaches to constructing a query vector  for instance  it would be
possible to implement a more elaborate sentence matching algorithm  based on the minipar relations in a given
sentence  possibly based on graph matching      this technique has been effective in question answering systems 
and may be useful in information retrieval as well  as a simpler modification  it may also be useful to generate a set
of features that captures the proximity of matches and partial matches in the document  for example  we could
reduce our vector to only hits that occur in the best few sentences  alternatively  keep count of how many times
multiple matches occur in a consecutive sequence of document relations  such as counting the times there are two
matches in four consecutive relations  or three matches in a sentence 
learning the relevance function
we now have a set of document query similarity vectors  but we do not know how this relates to the actual
relevance of the document to the query  it is clear that vectors with a higher sum of values indicates a better match 
but we still must learn exactly which weights it is appropriate to give to each match  as we are ignoring documents
in which judges disagreed or did not make a relevance judgement  we have binary relevant irrelevant labels for each
query document pair  the naive bayes algorithm and support vector machines are two machine learning
algorithms that are effective in binary classification problems that involve large feature spaces such as this 
we implemented nave bayes and experimented with boosting  the motivation to use boosting was to improve the
performance of our naive bayes classifier on test samples drawn from queries that our system hadn t seen during
training  as out test results reveal  the naive bayes learner performed better than the basic ir system on test samples
drawn from queries used during training  but performed poorly on queries not seen during training  we expected
boosting to help because in many cases the classifier did perform better than a random classifier  in terms of
accuracy  thereby leading us to believe that it was a weak classifier  hence amenable to boosting  but  boosting
didn t improve the performance of our classifier on the test set  we found that while the performance on the training

 

fiset improved significantly  as expected  with the number of weak classifiers used  the performance on the test
queries showed little or no improvement up to three weak classifiers and began to decrease thereafter  this could be
because in some cases  due to lack of sufficient relevance structure in the test queries  our naive bayes classifier did
worse than a random classifier on queries it hadn t seen during training  it could therefore not be considered a  weak
classifier  in the sense of the term used in the context of boosting 
we also experimented with support vector machines  svms  to learn a relevance function  we used the libsvm
package     which allowed rapid prototyping and easy experimentation  using a simple linear kernel  and some of
the optimizations available in the package  we were able to get better results than with our nave bayes attempts  in
addition  svms have a natural confidence measure  which corresponds to distance from the test point to the
separating hyper plane  we found this confidence measure to be a much more reasonable way to rank results than
the probability ratio we contrived for the nave bayes scenario  this confirms the result that computing confidence
is more reliable with support vector machines than nave bayes     
testing
after training one of our machine learning algorithms  we have a weight vector that can be used to make the
relevant irrelevant decisions with new query document pairs  to apply our function  we convert the new querydocument pairs into features as previously described  and use our hypothesis function with learned weights to
determine relevance  one way to compute the error of our relevance function is to divide the number of
misclassifications by the size of the test set  however  to get figures that are comparable to other information
retrieval systems  we must compute precision and recall  computing precision and recall requires us to rank the
results  rather than just classifying them as relevant  we will use the confidence measures described in the previous
section to rank our results for various learning methods 
in the results section  we compare the results of our system to that of a traditional ir implementation  we use
lemur     to get our traditional ir results with a normalized tf idf scoring scheme  as this system crawls all
documents to come up with its results  its task is significantly more difficult  recall that our system merely ranks
the results that human judges have rated consistently  to compute the precision of the lemur results  we first
removed all results that had not been consistently ranked by human judges from the ranked list  and then computed
how many of the top returned documents were relevant  computing recall is significantly more muddled  as we are
clearly not working with the whole set of documents that are relevant to the query 
now that we have a ranked list of results  we can compute the precision p when returning the top         or   
documents for a query  we found that these precision rates were the best way to evaluate our performance against a
traditional ir system 
at a high level  there are two ways to divide up the query document pairs into a training set and a testing set  the
first way is to include documents from all queries in both the training set and the test set  this gives our system a
huge advantage  because it can leverage similarities that may only apply to a single query  since there were training
examples for that query that were used to train the system  it is much more fair to only test using query document
pairs where the query is not a part of any query document pair that was used in training  this testing provides
stronger evidence that we have truly learned a general relevance function  we tested using both of these
methodologies  because there is something to learn from both  in addition  we tried varying the number of queries
that were used to train the system from as few as one query to as many as    queries 
results
using large feature vectors  our classifiers performed well on queries seen during training  table    
the svm classifier did a better job than the nave bayes classifier at predicting the relevance of documents for both
types of queries 
   familiar queries  queries that werent used to train the classifiers 
   unseen queries  queries that were used to train the classifiers 
however  even the svm classifiers performance wasnt satisfactory on unseen queries  we looked at this as a
problem of high variance  although the test sets contained a different set of documents even for the queries seen
during testing  the classifier had already been trained on the structure and semantic content of the query  thereby
making these test samples more like training data 

 

fitable  
query classifier
  
lemur
svm
nb
   
lemur
svm
nb
   
lemur
svm
nb
   
lemur
svm
nb
   
lemur
svm
nb
   
lemur
svm
nb
  
lemur
svm
nb

p   
 
 
 
   
    
   
   
 
    
 
 
 
 
 
    
 
 
    
 
 
 

p   
 
 
    
   
    
    
    
    
    
 
 
    
 
    
    
    
 
    
    
 
    

p   
 
 
    
    
    
    
   
    
    
 
 
    
    
    
    
    
    
    
   
 
    

table  
query classifier
  
lemur
svm  long 
svm  short 
  
lemur
svm  long 
svm  short 
  
lemur
svm  long 
svm  short 
   
lemur
svm  long 
svm  short 
   
lemur
svm  long 
svm  short 
   
lemur
svm  long 
svm  short 
   
lemur
svm  long 
svm  short 

p   
   
    
    
   
   
    
   
    
    
   
    
    
   
    
    
   
    
    
   
    
    

p   
    
    
    
    
    
    
    
   
    
    
    
    
   
    
    
    
    
    
   
    
    

p   
    
    
   
    
    
    
   
    
    
    
    
    
   
    
    
 
    
    
   
    
    

table    performance measured by precision in top     top    and top    documents retrieved by the two
classifiers on familiar queries  table    performance measured by precision in top     top    and top   
documents retrieved by the two classifiers on unseen queries

table    performance of basic ir and our classifiers averaged over   familiar queries
avg p 

   

   

   

lemur

   

    

    

svm
nb

    
    

    
    

    
    

 
   
   

lemur

   

svm

chart    performance of basic ir
and our classifiers averaged over  
familiar queries

nb
   
 
p    p    p   

reduced feature vectors 
we then experimented with a reduced set of features to reduce variance  these new features  while still capturing
the syntactic or structural similarity weighted by semantic matches  avoided over fitting and generalized better to
unseen queries  we tabulate below  the results obtained by running the svm classifier on these reduced feature
vectors as compared to its performance on the longer feature vectors  the table also gives the performance of the
basic ir  lemur  system on these queries  for comparison 

 

fitable    performance of basic ir and our classifiers averaged over   unseen queries
avg p 

   

   

   

lemur

    

    

    

svm  long feature vectors 
svm  short feature vectors 

    
    

    
    

    
    

   
   
   
   

lemur

   

svm long 

   

svm short 

chart   performance of basic ir
and the svm classifier using long
and short feature vectors 
averaged over   unseen queries

   
 
p    p    p   

conclusions
our classifier did very well in judging the relevance of new documents to queries it was trained on  queries whose
structure and semantic content the classifier was familiar with  it however did not do well on most unseen queries 
we suggest that the anomalies to the latter case are queries whose structure is similar to one of the queries that the
classifier was trained on  this further suggests that  the performance of the ir system can be improved  over a
restricted domain of queries by identifying the types of queries in the domain  based on their syntactic structure  and
training the classifier with representative queries from each class  so that with a high probability  all unseen queries
are familiar to the classifier 
while we have shown that better result ranking can be done by a syntactically and semantically aware system  this is
not the same as doing full information retrieval  it will take significantly more work to design a complete ir system
that can leverage some of the techniques we have demonstrated  several of these techniques will not scale well to a
large scale high performance query engine  another area for future research is examining what kinds of
compromises can allow us to minimize the computational cost while still reaping some of the benefits we have seen 
references
    saria  taylor        document retrieval using syntactic analysis of queries 
    haghighi  ng  manning        robust textual inference via graph matching 
    delany  cunningham  doyle        generating estimates of classification confidence for a case based spam
filter 
    kim  hahn  zhang        text filtering by boosting naive bayes classifiers 
    chih chung chang and chih jen lin  libsvm   a library for support vector machines        software available
at http   www csie ntu edu tw  cjlin libsvm 
    ogilvie  callan        experiments using the lemur toolkit 

 

fi