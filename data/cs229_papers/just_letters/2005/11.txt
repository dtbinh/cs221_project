application of clustering for unsupervised language learning
jeremy hoffman and omkar mate
abstract
we describe a method for automatically learning word similarity from a
corpus  we constructed feature vectors for words according to their
appearance in different dependency paths in parse trees of corpus
sentences  clustering the huge amount of raw data costs too much time
and memory  so we devised techniques to make the problem tractable  we
used pca to reduce the dimensionality of the feature space  and we
devised a partitioned hierarchical clustering approach where we split the
data set and gradually cluster and recombine the partitions  we succeeded
in clustering a huge amount of word data with very reasonable time and
memory cost 
motivation
fully automated learning of similar words and dependency paths is extremely
pertinent to many natural language processing  nlp  applications  similarity estimation
can help with problems of data sparseness in statistical nlp      and clustering could
automatically generate similarity estimates  word similarity estimates also can be used
in question answering and machine translation  major areas of current nlp research 
overview of process
our process is to build a large input matrix that describes words from their
appearances in various dependency paths  cluster the words in this feature space  then
estimate that words that ended up in the same cluster are semantically similar  since
language is so varied with a vast vocabulary  we must build a huge matrix to infer
anything useful from our clusters  however  clustering may not be tractable for huge
matrices due to time and computer memory constraints  so we took a more complex
approach  in the following sections  we discuss how principle component analysis and a
multi tiered form of hierarchical clustering solved this problem and allowed a large
matrix to be clustered 
input data
our input data was a corpus of six million newswire articles  parsed using
minipar into dependency path triplets  as in      the corpus contained about        
unique noun pairs and about        unique dependency paths  from these data  we

ficonstructed a matrix of training examples  our input matrix has m rows corresponding to
different nouns  and n columns corresponding to different dependency paths  an entry  i 
j  is the number of times that noun i appeared in dependency path j  any given word is
likely to only appear in a very small number of sentences  even in a corpus of six million
articles  so the input matrix is very sparse 
principle component analysis
to cluster nouns  we need to reduce the column dimension of the input matrix  in
order to do this  we use principal components analysis  pca   pca was implemented
using simple lanczos algorithm for singular value decomposition       see figure   

figure    pca using singular value decomposition

the input matrix a is decomposed as
a   u s vt 
s has nonzero entries only along the diagonal  representing singular values of a  or
eigenvalues of aat  since aat represents covariance of rows of a   i e  nouns   its
maximum eigenvalues represent eigenvectors directions in which covariance is
maximized  we choose the first k  which happen to be k largest  entries of s  and the first
k columns of u  to get u   the choice of k is based on the size of the eigenvalues and
the desired computational efficiency for clustering   we multiply u by s to get a 
which is the desired output matrix with reduced column dimensions 
following are the results we obtained by running pca on input matrices of
various sizes 
no  of rows  i p 
    
     
     
     
      

no  of columns  i p 
    
    
    
     
     

no  of columns  o p 
  
   
   
   
   

fipartitioned hierarchical clustering
hierarchical clustering  hc  is well suited to the task of grouping similar nouns 
in contrast to hard assignment clustering algorithms such as k means  hc builds a tree 
or dendrogram  of closest points deterministically  the basic hc procedure to cluster m
points into k clusters is as follows  first  start with m clusters of one point each  then 
find and merge the two closest clusters  and repeat m k times 
we computed the distance between two points  i e  nouns  as their cosine 
computed by dot product of their feature vectors  the distance between two clusters can
be defined as the minimum  maximum  or average distance between points in the clusters 
minimum distance clustering tends to produce long chains  while spherical clusters
more intuitively match word similarity  maximum distance clustering is susceptible to
outliers  which makes it unsuitable for this problem because the data is noisy  the corpus
could contain a few bizarre sentences   thus  average link clustering was the most
suitable approach  specifically  we maintain along with each cluster the mean of the
feature vectors of its points  and compute the similarity of two clusters as the cosine of
their mean vectors 
hc is computationally expensive  in particular  average link clustering on m
points in d dimensional space takes o dm logm  time and o dm m   memory      even
if cluster mean vector and cosine values are discretized to   byte integers  storing m
clusters and their pairwise cosines takes   dm m   bytes  for m         this is      bytes
     gb  more than the memory of most computers 
to make hc tractable  we devised an approach that we call partitioned
hierarchical clustering  in this approach  the m points are split into k partitions of size
m k such that hc on m k points can be executed on a single computer  for each partition 
hc is used to reduce the number of clusters by     by making m   k  mergers  so that
the m k points are combined into m   k  clusters  then pairs of partitions are
concatenated to create k   partitions each containing m k clusters  which contain a total of
 m k points   hc is again used to reduce the size of each partition by      and pairs of
partitions are again combined  until all partitions are eventually recombined in the
log kth step   see figure    
since partitioned hc requires running hc on at most m k clusters at a time  its
space requirement is o dm k  m k     an improvement by a factor of about k   thus
partitioned hc can be run on a computer that otherwise would not have enough memory
to cluster the data  partitioned hc is also fast  it requires log k steps  and the ith step
entails i instances of running hc on m k clusters  thus partitioned hc has a time cost of
o d logk   m k  log m k    an asymptotic improvement over normal hc of a factor

fi k log k    furthermore  the process can be parallelized on several computers  while this
was not possible with normal hc 
partition large
input matrix

 

cluster
cluster
combine
combine
partitions partitions partitions partitions
individually
individually

cluster
final
partition

 
 

 
   
 

 

 
 

   
 

   

 

        

 
 
   
 

 

 
 

figure    partitioned hierarchical clustering with k   initial partitions 

the problem introduced by partitioned hc is that the guarantee of the closest
points being merged is lost  if two very close points are in separate partitions that only
meet in the final step  they may not be merged at all if they belong to clusters whose
means are farther apart  this is unlikely to happen except in borderline cases  because
close points should end up in close clusters that will eventually be merged  but a further
investigation into partitioned hcs deviation from normal hc clusters would be
worthwhile  a possible solution to this problem is to perform partitioned hc multiple
times  randomly picking a different initial partitioning of the data each time  and then
averaging the word similarity results 
implementation and testing
to evaluate the feasibility of our approach  we implemented pca with svd
using the code from the svdpackc package     and implemented partitioned
hierarchical clustering from scratch  for our trial runs input matrix  we took        of
the most frequently occurring nouns and        of the most frequently occurring
dependency paths from the corpus  running pca on the data reduced the column

fidimension of the matrix from        to      in about four minutes on a windows xp pc
with    mb ram   dividing the        rows into four partitions of        and
performing partitioned hc  successively reducing the        clusters by half  we
generated       clusters  in about    minutes on a dual   ghz xeon shared linux
machine with   gb ram   when hierarchical clustering was run without partitioning 
an out of memory error occurred 
thus we conclude that our approach is fundamentally sound  and can allow
clustering of matrices with a very large number of both rows and columns 
further work
to continue this work  we would try running our program on even larger input
matrices  to evaluate the semantic significance of our cluster output  we would compare
the word similarity suggested by our cluster output to a gold standard such as wordnet
     latent semantic analysis      or human tagged data  we would compute the
pairwise similarity of all of the words in our input data  and compare the average
similarity of words that were clustered together in our output to the average similarity of
words that were not 
acknowledgements
we acknowledge the valuable guidance provided by rion snow  prof  andrew
ng  prof  dan jurafsky  and prof  gene golub 
references
   berry  svdpackc ftp   cs utk edu pub berry
    laham  d         latent semantic analysis   cu boulder  last updated oct       accessed nov
      http   lsa colorado edu 
    golub  g   and loan  c         matrix computations  baltimore  md  johns hopkins u press 
    pereira  f   tishby  n   and l  lee         distributional clustering of english words    th annual
meeting of the acl  pp        
    schtze  h  single link  complete link   average link clustering  no date given  accessed nov
      http   www csli stanford edu  schuetze completelink html
    snow  r   jurafsky  d   and ng  a         learning syntactic patterns for automatic hypernym
discovery  advances in neural information processing systems          

fi