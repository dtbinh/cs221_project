machine learning for patent classification
david black   peter ciccolo
problem description   background
our project is an application of machine learning technology to text
classification on united states patents to automatically differentiate between
patents relating to the biotech industry and those unrelated  additionally  we
attempted to further divide biotech patents into various groups  using k means and
chi squared to automatically identify potentially interesting subcategories in the
patents  our end goal  which has not yet been fully achieved  is a system that
automatically obtains recently filed patents from the uspto  classifies them  and
enters them into a patent database 
this project was done in collaboration with professor woody powell of the
sociology department in order to further his research  professor powell has been
analyzing trends in the modern biotechnology industry by looking at patenting
behavior  however  up until now he has been gathering said patent data by hand 
which has proven largely impractical  having an automated program extract
biotechnology patents will further his research by leaps and bounds  and allow him
to build a map of the of the entire country s biotech industry in order to analyze
subtle trends in innovation 
for our positive training examples  we had access to a collection of
approximately        patents that were collected via a project undertaken by
david black two years ago  for our negative training examples  we had access to
an effectively arbitrarily large number  about          of unclassified patents 
both sets had some degree of noise due to the collection method  which we
determined to be less than    on the positive training examples  and less than    
on the unclassified patents 
for each patent  we had access to the following data  patent number  title 
abstract  dates of application and acceptance  inventors and inventors towns of
residence  assignees and their locations  and various patent office assigned
classifications 

problem methodology   justification
of the data we had available on each patent  we chose to use only the title 
abstract  and assignee names for training  this is mainly because the other fields
were too inconsistently available for us to have confidence in their usefulness  in
addition  based on how the positive training examples were collected  several of

fithe fields would be artificially biased  for example  a disproportionate number of
the positive examples were collected from the boston area 
in order to facilitate training on the patents  some preprocessing was required 
this consisted of first removing the undesired fields  lowercasing all text data 
removing punctuation  and substituting a number token for all numerical terms 
to create feature vectors from the data  additional processing was performed 
first  the titles and abstracts were tokenized  then the tokens were run through a
porter stemmer in order to collapse tokens into their semantic categories and
reduce the feature count  the final token types used were assignees  title
unigrams  title bigrams  abstract unigrams  and abstract bigrams  all tokens were
tagged with their type  finally  the vectors were length normalized 
we then ran smo to train an svm  the smo we implemented was based on
platts      paper with a few minor suggested improvements  the kernel used
was gaussian to account for the infinite dimensional input space  in addition  the
smo implementation used an error cache and dot product cache in order to speed
up execution  since the svm was linear  the final calculated product was a weight
vector w and threshold b which could be saved to a file  eliminating the need to
store all training examples 
once classification results from the svm had been obtained  the next step
was to run a k means clustering algorithm on those results in order to automatically
discover potentially interesting subclasses within the biotech patents  once these
clusters were obtained  we ran a chi squared analysis on them in an attempt to
determine their semantic content 

results   conclusions
due to time constraints  we were unable to run the svm on the entire set of
       training examples  however  training on even an extremely small dataset
produced competitive results and there was consistent improvement in the
accuracy as training set size increased 

fi   

classification accuracy vs
training set size
 learning rate 

    

   

accuracy

    

   

    

   

    

   
 

   

    

    

    

    

training examples

in addition to the data points shown on the graph  the smo algorithm was run
with        training examples  this resulted in a model with       accuracy over
the entire dataset  svm training time was the major obstacle encountered in this
project  and future implementations would be well served by efforts to improve
smo efficiency 
k means divided the data into   groups  and chi squared was used to
discover the maximally discriminative features of these groups  unfortunately 
chi squared analysis revealed that the groups discovered had little to no semantic
content  this may have been partially due to the inclusion of common words in
the feature set  and stop word filtering might provide significant improvements 
both to the value of k means and the speed of svm training 
in addition to the improvements described above  future plans for this project
include training on the full dataset and setting up an automated harness to classify
new patents drawn from the uspto website as they are released 

fi