classification of news articles using named entities
with named entity recognition by neural network
nick latourette and hugh cunningham

   introduction
our paper investigates the use of named entities as features for the
classification of news articles by topic  prior work with the mcclatchy company  and
in particular the sacramento bee  informs us that categorization of news articles
poses a significant challenge to newspapers interested in determining the interests of
their users  given the large volume of news articles produced  past  present  and
future   an automated approach to this work is desirable 
we implement a system for article classification using the named entities
contained within the article as the basis for classification  many algorithms for text
classification  naive bayes classification  for instance  use the words within the text
as features for classification  for highly similar categories or topics  like current
coverage of civil strife in syria and egypt  the words used by articles in either category
may be very similar  e g   words like violence or military may be equally likely to
appear in an article about egypt as they are likely to appear in an article about syria  
we hypothesize that  for such closely related topics  using named entities for
classification will perform better than other approaches  intuitively  if the named entity
assad appears within an article then it is very likely that that article should be
classified as belonging to the category syria  we use a set of     articles created by
time as our data set      articles categorized as covering either syria or egypt  the
selection of time as a source is based only on the convenient access to a large
number of pre categorized articles there  based on the relative similarity of the topics
of civil strife in syria and egypt we select these categories as an appropriate sample
for our investigation 
before utilizing named entities as features for classification  we need to
address the non trivial problem of named entity recognition  ner   we chose to
implement our own neural network approach because of its proven success with
respect natural language classification problems  its easy parallelizability  and because
we were interested in implementing a deep learning algorithm  note that we borrowed
the implementation details of the neural network from pa  of cs   n  after identifying
named entities in the sample  we train a naive bayes classifier 

fi   prior work
y c  gui et al  provide some precedent for the use of named entities in the
classification of news articles      their research is on the use of named entities for
classifying news articles within categories hierarchically  of particular interest is their
focus on what they refer to as close categories  highly similar categories within the
same hierarchy  e g   presidential elections in different countries  gui et al  do not
describe the techniques used for ner or extraction of named entities  but found that
an svm trained on named entities outperformed one trained on terms  where terms
refers to words or phrases within the articles 
beyond this particular utilization of named entities for classification by gui et al  
there is extensive work on the general problem of named entity recognition as well as
on the problem of text classification       

   neural network
knowingthattheneuralnetworkwouldbethemostchallengingpartofourprojectin
termsofimplementation wedecidedtostartwithasinglehiddenlayerandmoveonfrom
there belowisanillustrationofourneuralnetwork notethatwanduarematrices
representinglineartransformationsthatareperformedontheinputstothehiddenlayerand
thefinalclassificationrespectively inadditiontothelineartransformations eachnodein
thehiddenlayerwillalsoperformanonlineartransformationtoitsinput notethatifwe
weretonotdothis thewholeneuralnetworkwouldjustbeperformingonebiglinear
transformationonthedata whichwouldbeconsiderablylesspowerfulintermsof
classificationsitcouldrepresent 

fito represent our words as vectors we will use a dictionary of approximately
        words  where each word is represented by a    dimensional vector  each
dimension is a weighted feature trained by an unsupervised learning method that tries
to capture a words syntactic and semantic information along with the the context in
which the word is normally used     lets call this dictionary l  to decide whether a word
is referring to a person or not we  use the words vector representation from dictionary
l  along with the vector representations of the c   of the words surrounding that word
as input to our neural network  to train our model we used stochastic gradient descent
to try and minimize our cost function j  we used the following equations 
j     m 

 

 

m

r
    y i    log hi       y i     log    hi        m
i  

 

nc h

h

j   k  

k  

 

 
  w k j
     u  k



a   tanh w x    h   sigmoid u t a 
dj
du 

  m 

 

 

 

m

 ai    hi  yi 

i  

     mr     u  

 

z    u         a     u         a     u         a          t 

 

 
m

 

m

 zi   xi    hi  yi        mr     m 

dj
dw 

 

dj

dxj i 

   hi  yi   u k       a k     w k j

i  

h

k  

training and deciding our parameters for the neural network
our data set to train and test the neural network consisted of blocks of text where
each word in the text is labeled with a   or a    a one indicating that the word pertains
to a person and a zero indicating that the word does not  in total our blocks of text had
        labeled words in them  in order to properly train our model and tune the
parameters  which consisted of a learning rate  context size  hidden layer size  the
number of iterations of the gradient descent algorithm  and our choice of the
regularization constant r  we decided to implement k fold cross validation  where we
chose k to be     from the models we tested we chose the best average f  score
whose runtime was below a certain threshold  our resulting choice was as follows 
context size    
learning rate        
iterations    
hidden network size     
regularization constant        
average precision of model         
average recall of model         
average    score of model         

fi   classification
we implement a naive bayes classifier  limiting the size of the vocabulary to
consist only of the named entities identified by the neural network  the performance of
this classifier is compared to a typical naive bayes classifier whose vocabulary
consists of all words encountered in the sample articles  common words like a  as 
etc  excluded   we partition our data randomly into five folds of forty articles each for
cross validation and use each fold as the validation set once for both the named entity
naive bayes classifier and the baseline naive bayes classifier 
the baseline naive bayes classifier correctly identified the category of the
articles in the validation subset with an average accuracy of      our own naive
bayes classifier using a vocabulary consisting of named entities performed worse in
correctly categorizing articles in the validation set with an average accuracy of only
    

   conclusion and future work
currently we are using named entity mentions rather naively  if many articles of a
topic x mention person y in our training set  then we are more likely to classify any
article that mentions y as belonging to topic x  however  if our training set does not
have any mentions of person y  then identifying that an article mentions y is useless
during our classification  several features of our data set may have also contributed to
poor performance of our classifier  given a u s  centric perspective in many articles 
the entities barack obama and john kerry appear in many articles from both
categories so that these entities hinder rather than aid in correct classification 
additionally  many articles contain named entities not mentioned in any other articles in
the data set  such as persons interviewed by the author  and the identification of these
entities does not aid in classification at all 
the performance of our classifier could be improved by disambiguating named
entities to associate them with their real world identities  this would allow us to discard
entities not directly associated with either category  our classification could also be
improved by a more robust means of named entity recognition that would identify
organizations of nations as named entities rather than only people 

fi   references
    gui  yaocheng  et al   hierarchical text classification for news articles based on
named entities   advanced data mining and applications  springer berlin heidelberg 
              
    mccallum  andrew  and kamal nigam   a comparison of event models for naive
bayes text classification   aaai    workshop on learning for text categorization  vol 
          
    nadeau  david  and satoshi sekine   a survey of named entity recognition and
classification   lingvisticae investigationes                   
    huang  eric h   et al   improving word representations via global context and multiple
word prototypes   proceedings of the   th annual meeting of the association for
computational linguistics  long papers volume    association for computational
linguistics       

fi