keyword extraction and semantic tag prediction
james hong
stanford university
stanford  ca        
jamesh   stanford edu

michael fang
stanford university
stanford  ca        
mjfang stanford edu
abstract

content on the web is often organized through user generated tags for intuitive search and retrieval 
such tags convey meta information about the subject matter of the texts they represent  for this
project  we applied machine learning  bayesian co occurrence  k nn  svm  nns  to predict tags
of stackexchange posts obtained from kaggle  facebook recruiting   keyword extraction iii 
using our non parametric  fuzzy nearest neighbor search algorithm  we achieved a f  score of
      on a testing set with unseen data and       on the kaggle test set  containing many duplicate
data points   furthermore  when predicting a single tag per post  our algorithm attained
approximately       accuracy  on unseen data   surpassing the      accuracy attained by stanley
  byrne         our keyword tag co occurrence model and fuzzy nns proved to be fast and
practical for large scale subject and tag prediction problems with tens of thousands of tags and
training documents 

  

introduction

tagging is one of the simplest ways to organize web content  allowing users to specify subjects  metadata
tags are vital for efficient search and indexing  many contemporary social media sites support some system
of content tagging  stanley   byrne         an automated tagging system to model user generation of tags
would therefore be useful for purposes of tag suggestion  document and topic clustering  and tag validation 
  

data analysis and preprocessing

from the kaggle competition  we obtained a training set with           stackexchange posts  consisting
of the title  body  and tags  as well as a testing set consisting of           title and bodies of posts  these
posts cover a broad range of topics from     stackexchange sites  e g  stackoverflow  mathoverflow 
askubuntu   each post has up to   tags  and the number of tags is roughly modeled by a normal distribution
with a mean of      and standard deviation of       in accordance with zipfs law  tag frequency drops off
dramatically  the top     tags constitute       and the top       tags constitute       of all tags 
through data analysis we observed a high degree of redundancy in the raw datasets  for instance  the raw
training set contained only           unique points  in addition            points in the raw testing set are
duplicated from the training set  in conducting our experiment  we elected to train and test our algorithms
exclusively on non duplicate data  since this would more closely resemble real world conditions and studies
by other researchers  stanley   byrne         the data was preprocessed as follows  with an emphasis on
speed   
   create a custom lexicon using an english dictionary with stop words removed and terms extracted
from the tag set  
   apply greedy multi word matching to capture multi word terms in the lexicon 
   filter out terms not in the lexicon  removing noise such as misspellings or arbitrary variable names
in embedded code segments  
 

we also tried stemming  but it proved ineffective for anything other than space reduction later in the co occurrence
model as most keywords could not be stemmed  pos tagging proved to be too slow on large datasets  our
preprocessing procedure also had the effect of reducing the training dataset from over   gb to under   gb 
 
an alternative method of generating a domain specific lexicon for stackoverflow posts would be to use computer
science textbook indexes as a corpus 

fi  

evaluation criteria

we measured performance in two ways  the mean f  score  and accuracy predicting a single tag  both on
non duplicated test data  using hold out cross validation for        labeled posts separated from the nonduplicated training set   f  score is formulated below  where  is precision and  is recall   is true positive
frequency   is false positive frequency  and  is false negative frequency 
   
  

 
 

   


 
 

 


 

model selection

we applied a variety of models to tag prediction  from our initial experiments with nave bayes and svm
classifiers for individual tags  e g  to predict whether c  is a tag   we concluded that such an approach is
intractable on large tag sets containing hundreds or thousands of tags  instead we focused on algorithms
that considered thousands of tags using heuristics  co occurrence  to reduce search space  to measure
learning rate  we trained each model on successively larger datasets 
   

 greedy  tag in text baseline

in our baseline algorithm  we compiled a list of common tags  for each test example  we simply predicted
whichever tags from the list were present in the text of the post  this returned an f  score of        
   

nearest neighbor search for top     tags 

compared to the greedy tag in text approach  nearest neighbor search  nns  on the top     tags
provided a more realistic baseline for a simple tagging system  the algorithm is as follows 
   for each tag y in the top      concatenate all training examples with tag y  generate a bag of words
 bow  vector with frequency counts  and apply the tf idf transform with document frequency
computed using the entire training set   these vectors correspond to centroids for each tag  
   for each test example  generate a bow vector and apply the tf idf transform  compute the
distance between the test example vector and every tag centroid vector using cosine similarity 
   rank the tags by similarity score to obtain predictions 
there are many flaws with this approach  first  the top     tags account for only       of all the tags in
all posts in the training set  imposing a strict upper bound on recall  also  more efficient space partitioning
algorithms for nns break down in high dimensional feature spaces  making nns computationally
expensive  linear search   consequently  we were unable to efficiently process any training set larger than
        samples using nns for     tags  however  as a baseline  we attained a f  score of approximately
     and an accuracy of       when predicting a single tag 
   

tag keyword co occurrence model

using our training examples  we constructed a co occurrence matrix to model the joint probability of word
and tag presence in a post  from this we can approximate upper bounds on the conditional probabilities for
each tag  given a set of words in a query  this allows us to quickly identify keywords that are strongly
correlated to individual tags  
   for each tag y  estimate

max



     using the co occurrence matrix 

   return the top k tags 

in the project milestone write up  this nns approach was referred to as nearest tag distributions 
to reduce space complexity  non keyword entries in the matrix can be removed  this serves as an extra filter against
stop words 
 
 

fiwhile simple  this algorithm captures approximately     of the true tags within the first       tags
predicted  thus  it serves as a potential intermediate filter for the k nn and svm is tag  binary classifiers 
     

svm and k nn extensions to co occurrence model

the co occurrence model suffered from high bias when predicting less salient tags  those lacking specific
keywords   we attempted to remedy this bias by using additional text features based upon the word tag
results from the co occurrence matrix  and feeding them to svm and k nn classifiers  this retains the
advantage of working in lower dimensions  instead of working in the space of the lexicon  then  we applied
forward search to determine the most relevant features on each algorithm  which were  the probability
bounds given in the above  tag presence in text  and the overall tag probability 
first  we used the k nearest neighbors algorithm with large k      to obtain probability estimates based on
the proportion of positive neighbors  next  we experimented on class weighted and regularized svm
classifiers with linear  polynomial  rbf  and sigmoid kernels  using the functional margin as a measure of
confidence for each candidate tag  for the svm  the best performance was observed with an rbf kernel and
positive negative weights of      both svm and k nn improved performance on small training sets  where
the co occurrence model was prone to over fit  however  they were less effective on larger training sets 
   

 high dimensional  fuzzy nearest neighbor search

the main issue that plagued our     tag nns algorithm was the high dimensionality of the feature space 
to address the curse of high dimensionality  we implemented a fuzzy approximate nns algorithm for
efficient lookup and computation of semantic similarity across all of the        possible tags observed in
the training set  using the keyword tag co occurrence model presented above  we narrowed the search
space of possible tags to       candidate tags  and applied linear nns in this local search space  our
algorithm 
   for a test example  use the keyword tag co occurrence model to predict       candidate tags 
   for each candidate tag  extract its corresponding row in the keyword tag co occurrence matrix as
a tag centroid  to increase the salience of the keywords  top non zero entries  corresponding to
each tag  we zeroed out all but the largest     components and renormalized the centroids 
   compute cosine distance  similarity measure  between the test example  a normalized bow
vector  and each candidate tag centroid  and re rank the tags by similarity 
   output the tags corresponding to the k nearest tag centroids 
in subsequent experiments  we adjusted our fuzzy nns similarity function to penalized candidate tags with
low semantic similarity  and reward tags with high similarity  whereas the keyword tag co occurrence
model generally performed well on common and specific tags such as php  css  and c   which correspond
strongly to specific keywords  fuzzy nns improved performance on more general tags with high semantic
similarity  e g  algorithms  parsing  image   furthermore  fuzzy nns improved precision when predicting
similar  hierarchical tags  e g  facebook  facebook api  and facebook graph api   predicting three tags per
query  we achieved       recall and       precision on        unseen test examples when considering all
       possible tags  
  

results

table    accuracy predicting a single tag
model     of training examples
keyword tag co occurrence
co occurrence   k nn
co occurrence   svm
fuzzy nns
 

   
      
      
      
      

    
      
      
      
      

  k
      
      
      
      

   k
      
      
      
      

    k
      
      
      
      

when only considering the top     tags  fuzzy nns achieves precision of        recall of        and f  of       

fiperformance on predicting a single tag
   

word tag co occurrence

accuracy

   
   

co occurrence   tf idf

   

co occurrence   k nn

   

co occurrence   svm

   

   

co occurrence   hd fuzzy nns

   

co occurrence   hd fuzzy nns  
multiword features

 
   

    

     
      
  of training examples

       

figure    on the single tag prediction benchmark  our high dimensional fuzzy nns algorithm performed      better
than the keyword tag co occurrence model alone when trained on           training examples  on smaller training
sets  the k nn classifier  on intermediate features  with keyword tag co occurrence improved test accuracy as the cooccurrence model  even with smoothing  is highly prone to over fitting on small training sets            

table    f  score performance  predicting   tags per query 
model     of training examples
keyword tag co occurrence
co occurrence   k nn
co occurrence   svm
fuzzy nns

   
      
      
      
      

    
      
      
      
      

  k
      
      
      
      

f  score

tagging performance on f  measure
   
    
   
    
   
    
   
    
   
    
 

   k
      
      
      
      

    k
      
      
      
      

tag in text 
nearest neighbors      tag baseline 

word tag co occurrence
co occurrence   tf idf

co occurrence   k nn
co occurrence   svm

co occurrence   hd fuzzy nns
   

    

     
      
  of training examples

       

co occurrence   hd fuzzy nns  
multiword features

figure    similar to the single tag metric  the fuzzy nns algorithm attained significantly better f  performance
          over our standalone co occurrence algorithm and           top     tag nns algorithm 

results in kaggle competition  using title string hashing to identify duplicates in the kaggle test set and
then fuzzy nns to tag the remaining non duplicates  we achieved a f  score of          on the kaggle
competition metric  and ranked    of     teams at the time of submission and        as of            
  

discussion

the main challenges of the stackexchange tag prediction problem include      the large number of possible
tags      the large number of word multi word features      the size of the training testing sets      the
difficulty of measuring semantic similarity between domain specific terms  and     high variability in user
tagging behavior 
 

for each test query  we returned exactly   tags  regardless of how many tags are in the solution  this is one area that
can be optimized for better performance on the f  metric used by kaggle 

fiour co occurrence model addresses     and      allowing us to estimate the probability of a tag 
furthermore  the co occurrence matrix generated from our           point training set can be easily stored
to fit into main memory  allowing for high performance keyword extraction  to reduce the dimensionality
of our features and estimate semantic similarity between documents  we originally implemented a lsi
model  however  lsi does not scale well to large training sets with over   million data points  instead  tag
centroids generated from the keyword tag co occurrence were more efficient to compute  and allowed us
to process the entire kaggle testing set in reasonable time and overcome the issues of high dimensionality
    with considerable improvement in precision and recall over the co occurrence model without fuzzy
nns      finally      users varied on the number and the specificity of tags  for this reason  accuracy
predicting only   tag and f  score on all tags are both critical metrics of a taggers performance  the former
testing accuracy on overall subject prediction  and the latter on the viability of the algorithm as a tag
recommendation system  using a regression model to predict the number of tags would probably improve
performance on f  and kaggle rankings  finally  compared with the stackexchange prediction study by
stanley and byrne         which used an act r inspired co occurrence to achieve     accuracy on the
single tag metric  fuzzy nns achieved better accuracy       with similar training set sizes  the difference
is most likely the product of re ranking candidate tags by tag centroid similarity 
ultimately  our results demonstrate that a fuzzy nns algorithm based on tag keyword co occurrence and
cosine distance can be an effective method for large scale tag and subject classification given enough
training data  this relates to similar problems such as hashtag prediction  tag recommendation  and search
and indexing  an additional application for tag prediction would be to filter spam irrelevant tags and posts
 stanley   byrne        
  

future directions

here are five possible areas of improvement  in order to further improve precision and recall of the tagging
fuzzy nns algorithm   i  a more sophisticated similarity function can be used to estimate semantic
similarity  for instance  wikipedia pages or google search results could be used to generate expanded text
representations for less common keywords  sahami   heilman         additionally  syntactic parsing
strategies could improve data preprocessing   ii  unsupervised clustering can be applied training examples
with common tags  so as to model multiple tag sub centroids for more precise similarity comparison   iii 
applying locality sensitive hashing  minhash  could allow for tractable nns against not only tag centroids
but the training examples themselves  das  datar  garg    rajaram          iv  for practical subject
identification systems  modeling tag specificity through tag tag hierarchies can better balance the trade off
between accuracy and specificity in tag prediction  deng  krause  berg    li         finally   v  using
distributed computing would allow more sophisticated preprocessing steps  feature extraction  and semantic
similarity measures 
  

bibliography

    das  a   datar  m   garg  a     rajaram  s          google news personalization  scalable online  www       pp           
banff  acm 
    deng  j   krause  j   berg  a  c     li  f  f          hedging your bets  optimizing accuracy specificity trade offs in large
scale visual recognition  cvpr   pp              providence 
    kaggle          facebook recruiting iii   keyword extraction  retrieved from kaggle  http   www kaggle com c facebookrecruiting iii keyword extraction
    kuo  d          on word prediction methods  berkeley  ucb eecs 
    lott  b          survey of keyword extraction techniques 
    manning  c  d     schutze  h          foundations of statistical natural language processing  cambridge  mit press 
    sahami  m     heilman  t  d          a web based kernel function for measuring the similarity  www       pp           
new york  acm 
    stanley  c     byrne  m  d          predicting tags for stackoverow posts  iccm  pp            ottawa  iccm 

 

using fuzzy nns  we were able to predict tags for the entire kaggle test set     million queries  on a      macbook
air with a     ghz dc ulv processor and  gb of ram in the span of   hours 

fi