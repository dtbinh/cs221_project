michela meister
detecting drivable surfaces in a vehicles path
cs    final report
introduction 
many autonomous driving programs rely on lidar sensors to detect drivable
surfaces  however  lidar sensors can be expensive and do not have a very large range 
physical factors  such as dirt  the sensors mounting on the car  and light can cause
additional problems  dahlkamp et al    computer vision methods can greatly extend this
range and can sometimes be cheaper than investing in a high powered sensor 
in this project i propose a method that uses the video feed of a car mounted
camera to classify surfaces ahead as either drivable surfaces or obstacles 
i would like to thank tao wang of sail for mentoring me on this project 
methods 
overview
we begin with the assumption that the ground directly in front of the vehicle  the
training area  is drivable  we can then compare an image of this area of drivable
ground to surfaces ahead of the vehicle in the current video frame  if these other surfaces
look sufficiently similar to the training area  we can decide that they are also probably
drivable  if they are sufficiently dissimilar  we classify these other surfaces as obstacles 
these other surfaces could actually be drivable  and may just have a different appearance
from the training area  but for the purposes of this project  we classify surfaces different
from the training area as obstacles and surfaces similar to the training area as drivable 
choosing surface patches to compare
how can we compare the training area to other surfaces in the video frame  it
would be inaccurate to simply crop a patch of size m x n from the training area and crop
another patch of size m x n of a surface ahead of the vehicle  because the second patch is
a farther distance from the camera than the training patch  the second patch represents a
much larger road area in  d space than the training patch 
instead  using the cameras transformation matrix and gps data on the vehicles
movements  we create a perspective transform to warp the training area to its pixel
representation i seconds ago when it was at a position d with respect to the vehicle  we
find the perspective transform using the following method 
let h be the quadrilateral that defines the training area in frame f at time t  the
perspective transform is defined by h and h  the pixel representation of h at time t i  to
find h we have to find the pixel representation of each corner of h at time t i  for each
corner c of h represented by pixel p x y  in video frame f  we use the cameras
transformation matrix to convert from pixel p x y  to the  d position p x y z  of c with
respect to the camera at time t  given gps data on the vehicles movement  we can find
the  d position p x y z  of c with respect to the camera at time t i  using the
cameras transformation matrix once again  we convert this  d position to the pixel
representation p x y  of c in video frame f  the four values of p x y  found through

fithis process make up the corners of quadrilateral h  which is used in conjunction with h
to create the perspective transform  szeliski  ch    
thus  since we can warp the contents of the training area to its representation at
position d with respect to the vehicle  we can crop an m x n patch of warped training area
and the m x n surface patch that is actually at position d with respect to the vehicle for
comparison 
patch collection
using the method described above  we collect a data set of      pairs of patches 
where each pair contained one patch of training area  the training patch  and one patch
of surface area further ahead of the car  the variable patch  when warping the training
area  we use time intervals of i             and     seconds  to get variable patches at
different distances from the car  figure   and figure   serve as examples of patch
collection with i        the trapezoid of green dots  h  at the bottom of figure   defines
the training area  which is warped to the rectangle of green dots  h  in figure    the
variable patch  figure    is harvested by cropping h from figure    and the training
patch  figure    is harvested by cropping h from figure   
figure    original image

fifigure    warped image

figure    variable patch  

figure    training patch  

we then manually label       pairs as same or different  while the patches in
figures   and   are labeled same  figures   and   are labeled different  as figure  
shows the bottom of a car 
figure    variable patch  

figure    training patch  

patch comparison
given a training patch and a variable patch  we need to determine whether they
are similar enough for the variable patch to be determined drivable  we usedthe sum of
the distance squared between corresponding pixels in a pair of patches as a metric for the
similarity of the patches  a lower distance value means the patches are more similar  so

fithe variable patch is likely drivable  while a high distance value means the patches are
less similar  so the variable patch is likely an obstacle 
for the      manually labeled pairs  we compare the distance values for each pair
to their labels  we tried each pairs distance value as a threshold for similarity and
calculated the precision and recall for each threshold  as plotted in figure    a variable
patch was considered a true positive if it was labeled an obstacle by both the threshold
and the manual labeling  a false positive if it was labeled an obstacle by the threshold but
not by the manual labeling  and a false negative if it was labeled drivable by the threshold
but not by the manual labeling 
results 
figure    precision vs  recall

when a threshold of     for the average distance squared between two corresponding
pixels is used  precision reaches a maximum of     and recall is      when a threshold of
    for the average distance squared between two corresponding pixels is used  precision
is     and recall reaches a maximum of     
analysis   sources of error 
precision is always very low  however precision and recall increase together  a
recall of     means that all obstacles are recognized  but false positives are a large
problem 
there are many possible sources of error to explain the prevalence of false
positives  first  there may be human error in the manual labeling of the patches  we label
pairs as same if both patches were clear road surface  disregarding lane markings and
shadows  and only label patches as different if the variable patch contained an obstacle 
such as the car in figure    however  while a white lane marking is not an obstacle  it is a

fidifferent color from plain road surface  and so the sum of the distance squared between
corresponding pixels for such a pair may suggest that the variable patch is an obstacle 
there also is error in warping the training area  which may result in error in the
data set  after much debugging  we found that  because of the difference in height
between the cars imu and camera  points far ahead of the vehicle could not be
reprojected perfectly to their locations at a prior timr  because of this error  some training
patches may not have been pure road surface 
conclusions 
we found a preliminary threshold for determining the similarity of two patches
and thereby classifying unknown patches  however errors still need to be corrected in the
warping of the training area and in the manual labeling of patches  the sum of the
distance squared between corresponding pixels in a pair of patches is a metric that can
successfully recognize obstacles  but with many false positives  we must address lane
markings and shadows  which alter the appearance of road surface but not the drivability 
in order to decrease these false positives 
future work 
i chose this project  because it is my dream to build driver assistance technology 
data collection for this project was a huge challenge that taught me a lot about computer
vision but which also required significant time  coming at the expense of applying more
extensive machine learning techniques  in the following months i plan to continue this
project  and with more time i plan to implement other techniques for classifying patches 
i might experiment with different metrics  such as the peak color or intensity of a patch in
order to decide the similarity of two patches  i am also considering using the histogram of
oriented gradients to define the attributes of a patch  i could then use an svm to classify
patches given these attributes  eventually i would like to extend the patch classification
to the entirety of the vehicles view 
references 
h  dahlkamp  a  kaehler  d  stavens  s  thrun and g  bradski  self supervised
monocular road detection in desert terrain 
szeliski  richard  computer vision  algorithms and applications    sept      draft 

fi