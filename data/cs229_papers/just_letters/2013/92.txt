clustering methods without given number of clusters
peng xu  fei liu

 

introduction

the degree matrix is dened as d   diag d   
diag d    d            dm    here  di is determined by i  or by
x i   theres no essential dierence  in the following
passage we may use d   d x i    to denote the mapping
x i   d x i    to simplify the illustration  similarly 
wi j has no dierence with w  x i    x j     and i can be
used to denote the point x i   it wont cause any ambiguity 

as we know  kmeans method is a very eective algorithm of clustering  its most powerful feature is the
scalability and simplicity  however  the most disadvantage is that we must know the number of clusters
in the rst place  which is usually a dicult problem
in practice  in this paper  we propose a new approach
peak searching clustering to realize clustering without now  consider the random walk on graph g with
given the number of clusters 
weighted adjacency matrix w   then  the random walk
has a stationary distribution  over all the points  its
our method is based on the similarity graph    of the reasonable to claim that  the closer to the cluster cendata points  through the relationships between points  ter a point i is  the higher  is  that is to say   is a
i
i
we capture those points which are near the cluster local maximum of the stationary distribution if i is
centers  by nding the peak area of the stationary the closest point to the center  from the theories in
distribution of the random walk on the correspond  stochastic process  we know that   is the solution of
ing similarity graph  we gure out a way to capture the linear equation 
the points near the cluster center areas  which we calm
l peak points in this paper  the advantage of our

p     subjected to
i    
method is that we dont need the number of clusters
i  
as an input  our algorithm estimate the number of
clusters in the dataset  which we believe can be a good
where p   d  w is the corresponding transition maindictor of the true value 
trix 
since g is symmetric  we can directly solve  as   d 
so we can use d to indicate the relative value of  

 

peak searching clustering
   

denote x     x i   i       m as the sample data set  m
is the sample size  n is the dimension  x i   rn   since
we do not know how many clusters there are in x  we
try to have a good estimation of the number of clusters or the possible cluster centers  assume the data set
is dividable  then the points in one cluster tend to be
near to each other  our simple strategy is to capture
those points which are near the cluster centers 

peak searching process

for a well dened clustering problem  there should
be several peaks in the set   x i    d x i       i  
              m   that is  if we consider the mapping d from
 x      x              x m    to r  then the mapping should
have several local maxima areas  which are the center areas of the clusters  given the degree of all the
points  we want to capture one point per such area 
to locate one cluster center  we call these points the
peak points of clusters  to nd such points  rst 
    degree and stationary distribution we get the point with the highest degree as the rst
peak point  note that  peak points should not be close
consider the similarity graph g    v  e   with v   to each other because each of them is near the center
 x      x              x m     and w  rmm is the corre  of dierent clusters  therefore  if we have an approsponding weighted adjacency matrix  the generalized
priate neighborhood of the rst peak point  then the
n
degree of a point x i  is dened as di  
j   wi j   point of highest degree outside the neighborhood will
 

fibe the second peak point  theoretically  we can capture all the rest peak points by cutting o appropriate neighborhoods of the existing peak points  but
it is dicult to determine the size of neighborhoods 
here we introduce the concept of persistency of points 
as we increase the size of neighborhood  the highest
degree outside the neighborhood will decrease  specif 

ically  we consider a k nearest neighborhood of the rst
peak point  and as we increase k from   to m  more
and more points are included in the neighborhood  the
highest degree outside the growing neighborhood is decreasing  but there exist some resistance against such
drop tendency  we illustrate this by an  d example 
figure   

   

   

   

   

 

 

   

   

   

   

   

   

   

   

d

 
  

 

x 

 

x 

 

x

 

  

 

k 

 

 

 x   x   

 

 

  

k

  

figure    in the left gure  there are two peaks and we need to capture the peak points x   x    we get x  as
the rst peak point  because it has the maximum value of d  then we continuously remove the neighborhood
of x    which means we remove  x  k  x   k  with k growing from   to   as k grows  the maximum value
outside the interval  x  k  x   k  drops  as shown in the right gure  but it drops to d x    at k   k  and keeps
the level until x  is included in the neighborhood of x  and then the value goes down again  in this example k
is continuous  but the basic idea is the same for discrete situations 
we call such resistance against the drop tendency as
the persistency of a point  in the example above  as
we cut o the neighborhood of x  by larger and larger
size  x  shows up to resist the drop tendency  the persistency of x  is dened as  x   x     k    which means
the length of period it holds the maximum value of
d outside the neighborhood of x    similarly we can
dene persistency to any other point  to simplify our
illustration  we call the maximum value of d outside
the k nearest neighborhoods of some points as s k   s
also depends on the some points  but for simplicity 
we just use s k  if it doesnt cause ambiguity 

cy  since short persistency does not reect the stable
structure  but the lower bound varies between dierent data sets and it is not easy to select  here we use
another method  we preprocess the data to nd out
those points that are likely to be near the centers of
clusters  recall that we have d as the indicator of the
likelihood of a point to be near the center  if di is higher than most of the dj where j is near i  then di is more
likely to be a center  to compare di with dj where j is
near i  we simply compare it with the average weighted
value of dj   where the weight is wi j   an indicator of
how j is near i  we call such average weighted value
as hi   then we can compute h by 

peak points tends to have high persistency than nonh   dp t   dw d 
peak points  in the example above  x  has the highest
persistency  so we pick it out as the second peak point  then  the termination condition is 
for a potential peak point x i    if di   hi   then i can
after we get n peak points  we search the  n      th
be considered as a real peak point  and the searching
peak point  if there is any  by the following rules  we
process can proceed  if its not  then i is not considered
cut o the k nearest neighborhoods of the n current
as a real peak point  and the process terminates 
peak points simultaneously and observe the point with
the highest d among the rest of the points  then we the idea of nding h comes from the theory related
pick out the most persistent point during the growth to heat equation  h is indeed a smoothness of d by
of k from   to m  then  the point we pick out is the convoluting d with some probability distributions  it
potential  n      th peak point 
drags down the peaks and raises up the valleys 
the behavior of heat is the same  as the time goes on 
now we consider the termination of the searching proheat ows from position with high temperature to pocess  one way is to set a lower bound for the persistensition with low temperature  and the solution to the
 

fi   

heat equation is a convolution of the initial data with
the poisson heat kernel  if we want to know where the
peaks are  we simply nd where the values are dragged
down  and the dragged down area should be the
peak area 

clustering

after getting all the peak points  we have the number
of clusters as the number of peak points  noting that
the peak points are quite close to the cluster centers 
we can directly regard the peak points as the cluster
centers and all the other points are assigned to dierent clusters based on the distance between the points
to the peak points  the point shares the same cluster
label with the nearest peak point 

algorithm   peak searching clustering
input 
  x i    i                 m   input data
 w   weighted adjacency matrix
output 

 

 peak points  cluster centers 

experiments

 cluster labels of all points

in this section  we will use both simulated data and real
data to demonstrate the utility of our approach  in the
pseudo code 
real data experiments  we compare our methods with
m
dpmeans method    and kmeans method  throughout
   compute di   j   wi j
the experiments  we use normalized mutual informam m
   compute hi   j     j   wi j dj   di
tion nmi  between the ground truth classes and algorithms outputs for evaluation  when using kmeans 
   add x i  which has the highest di into the set of we use our estimation of the number of clusters as the
peak points
input  as to dpmeans  we apply max min random selection method to estimate  based on our estimation
   find all peak points  repeat until stop 
of the number of clusters 
construction of graph  we can either use mutual
 set the persistency of all points to  
k nearest neighbor graph or gaussian weighted graph
 for k                 m
in our experiments  in this paper  we use gaussian
 find x i  which has the highest di a  weight  we choose the parameter    in gaussian simimong those points that are not in the larity function to be the mean variance of the original
k nearest neighborhoods of all the cur  data  we also have done experiments with mutual krent peak points
nearest neighbor graph and generally we can achieve
 let the persistency of x i  increase by   good performance when setting k     m  the result
is not shown in this paper 
end
 find x c  which has the highest persistency

first  we use   simulated sets data of gaussian distribution on the  d plane to show how our algorithm
works  figure           we also apply our method to
  uci data sets  for each set of real data  we rst apply pca to the original data keeping over     principle components   and then implement our clustering
methods  the results are shown in tabel   

 if dc   hc
 add x c  into the set of peak points
else
 stop nding peak points
end
end
   set the peak points as the cluster centers  let
each of the other points shares the same cluster
label as its nearest peak point 

 

fi 

 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

 

 
 

 

 

 

data with label

 

 

 

 

 

 

data without label

figure    the left gure is the original sample points generated by   gaussian distributions with covariance
matrix equal to i  and mean equal to        for the red colored set         for the green colored set  and       
for the blue colored set  the number of sample points in each colored set is      red       green       blue 

 

 

 

 

 
 

 

 

 
 

 

 
 

 

 
 

 

 

 

 

 

 

 

 

 
 

 

 
 

 

 
 

 

 

 

the degree d of each point

 
 

 

 

 

 

 

 

 

 

 

h of each point

the comparison of d and h
figure    in the left gure  d represents the stationary distribution of the random walk on the graph g  the
value of d successfully reects how much a point is near a cluster center  the peak areas in d are exactly the
center areas of clusters  the middle gure shows h  a smoothness of d  in the right gure  we mark the points
with d   h as red  and d   h as blue  we see that such standard is a good indicator of whether or not a point
is in the center area of a cluster 

s

s

   

s

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

k
persistency  with   peak point
 

 

   

    

    

    

    

    

   

k
persistency  with   peak points
 

 

   

    

    

    

    

    

k
persistency  with   peak points
 

 

   

    

    

    

figure    these three gures shows the value of s k  of the peak points  s k  drops as k grows  but as we can
see  there are points that resist such drops  in the gure  we marked out with red line the largest persistency
period  in the rst gure  we use   peak point  the point with highest value of d  and keep cutting o its k th
nearest neighborhood  and capture the second peak point by its persistent behavior  in the middle gure  we
use peak point   and   simultaneously to cut o their k th nearest neighborhoods and capture the third peak
point  in the right gure  as weve already found   peak points  theres no signicant persistency anymore 
what we get is a fake peak point  by the corresponding value of h and d  its not in the center area and the
searching process terminates 

 

fi 

 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

 

 
 

 

peak points

 

 

 

 

 

 

 

 

assigned labels

figure    as the searching for peak point terminates  we can assign the cluster labels to all the points  the left
gure shows the peak points found  which is marked as red  the right gure shows the assigned labels 
data set
iris   
wine   
seeds   
soybeans   
pima   

psc
         
         
         
         
         

dpmeans
         
         
         
         
         

a good clustering method can only provide reasonable
solutions instead of the right solutions  which is exactly what our approach has done  the last point is
the parameter selection problems  as in psc  the construction of similarity graph turns out to be a very
important step  either using k nn graph or guassian
weighted graph  we have parameters k or  to be determined  in some extreme cases  the clustering result
is quite sensitive to the parameter selection  in our experiments  we choose    as the mean of variances of the
data points in all attributes  probably this is not the
best choice  how to select  might be a good problem
to work on 

kmeans
         
         
         
         
         

table    uci data sets nmi number of clusters   in
the rst column are the names of data sets and the
numbers in the parentheses are the numbers of true
classes  in the other columns  the numbers in the
parentheses are the numbers of clusters algorithms output  in the case of kmeans  it is same with that of
peak searching clustering psc  

 
 

conclusions

discussion
in this paper  we start from a simple intuition  and
provide a new clustering method without knowing the
number of the clusters  peak searching clustering  we
explain our ideas from the perspective of random walk 
we also introduce the persistency concept in the peaksearching process  and in the experiments  psc does
a good job in clustering problems 

in the previous sections  we have exhibited a brand
new clustering method  peak searching clustering  it
has good performances in many cases  there are a few
points we should mention here 
first  our method is based on the similarity graph  and
essentially based on the euclid distances of data points 
in this case  our method can only deal with linearly
dividable problems  the same with kmeans approach 
for the structures like a ring within a ring  we cannot
seperate the two rings  second  psc is a very intuitive
and straightforward method  we start from the connection between points and try to distinguish points
lying in the centers with those lying on the margins  we give reasonable clusters based on those center
points  however  as to clustering problem itself  the
number of clusters is probably undeterminable in practice  since there is no absolute standard of clustering 

references
    u  von luxburg  a tutorial on spectral clustering 
tech  rep       max planck institute for biological cybernetics  august      
    kulis  b  and jordan m i  revisiting k means 
new algorithms via bayesian nonparametrics  in
icml      

 

fi