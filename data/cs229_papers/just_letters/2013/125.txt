strategies for better sleep spindle detection
wendy nie  chengcheng fan

   introduction
    sleep spindles
sleep spindles  hallmarks of stage   sleep  are bursts of brain activity that may be detected through electroencephalography  eeg 
measurements   measurements of voltage across a scalp  traditionally  they are scored visually  however  this approach is time consuming 
applying machine learning to sleep spindle detection would reduce identification efforts 
sleep spindles are comprised of a group of rhythmic waves which progressively increase and then gradually decrease in amplitude      
according to aasm standards  spindles are scored according to their frequency        hz   duration       seconds   and envelope  maximal
amplitude should occur at the temporal midpoint of a spindle      

figure    examples of spindles  shown in red  from raw data set

    data
eeg voltage measurements  sampled at     hz  were taken from the central scalp region of     sleeping subjects  experts of sleep
research were polled to identify sleep spindles  from these polls  a set of      gold standard  gs  spindle observations was established and
each time series sample was marked spindle positive or spindle negative 
researchers then applied a wavelet based detector to the raw data with the goal of estimating spindle placement and duration  estimated
spindles are true positives if their overlap with a gs is greater than or equal to     of their union with the same gs  the detector yielded     
true positive  tp       false positive  fp   and     false negative  fn  observations 
from each observation     features were extracted 

   objectives
the objective of this project is to improve the performance of the sleep spindle detection system  two strategies were used to achieve this
   cascade the results of the wavelet detector with a classifier that discriminates between true positive  tp  and false positive  fp 
observations  to improve classification  we must choose a classifier and select the most effective of the    extracted features 
   discretize the time series data and generate and test a hidden markov model  hmm   this strategy enables the incorporation of contextual
data  which is difficult to extract in the cascaded system paradigm  to improve detection  a good state model must be selected and accurate
estimations of state transition probabilities and observation probabilities must be made 

   methods
    wavelet detector and classifier cascade
a data flow chart of the cascaded wavelet detector and classifier system can be found in figure    the classifier classifies positive
observations of the wavelet detector  tp   fp   yielding a different set of overall false positive  fp   true positive  tp   and false negative  fn 
observations  referring to the notation in figure    the total number of cascaded fp  tp  and fn observations can be computed from fp  tp 
and fn observations and classifier sensitivity and specificity estimates using equations   through    the addition of the classifier increases overall
specificity 
tp     tp    sensitivity  
 eq    
fp     fp       specificity  
 eq    
fn   fn   tp      sensitivity  
 eq    

figure    structure of the cascaded system

fi      feature selection
to reduce redundancy and processing time  subsets of features were selected and tested  blind selection schemes include forward
selection and minimization of correlation between features within a set  schemes requiring more human involvement include pca for dimension
reduction and subgroup feature selection 
forward search terminates when the algorithm accuracy exceeds      after repeated trials  features are ranked by how often they were
selected 
we assume that the stronger the correlation between two features  the less information they provide as a group  in correlation
minimization  the squared cross correlation feature pairs are arranged in a matrix  the value of feature set   i j k   is the sum of all the squaredcross correlation values in rows and columns  i  j  and k  the set with the smallest value is assumed to be optimal 
in subgroup feature selection  features were grouped in terms of similarity and one feature from each group is added to the final set  for
example  rawfrequency and huupp frequency are likely to be highly correlated so either one or the other is added to the final set  to assess
features within a subgroup  the performance of a single feature from the subgroup with all features from every other subgroup is evaluated 
pca was predominantly used to visualize features within the subspace  however  assuming the principal component direction has a high
variance because fp and tp are nearly separable within that direction  then the principal component vector also hints at the importance of each
feature for classification  the features corresponding to the elements with the largest absolute values in the principal component vector might be
a part of the optimal set of features 

      classifier selection
two linear classifiers and svm were considered 
logistic regression and gaussian discriminant analysis are probability based  gaussian discriminant analysis is asymptotically efficient if the
assumption that features follow a gaussian distribution a feature space is true 
for svm classification  we followed the recommendations given by hsu et al      for tuning parameters of svm classifier  we started with
gaussian kernels and set the regularization parameter to    then we tried different parameters for training  and selected the best parameters via
cross validation  the important parameters recommended by hsu et al      are c for soft margin and scaling factor  sigma  in the radial basis
function kernel  we repeated this process for polynomial and multilayer perceptron kernels 

    time series windows   the hidden markov model
as an alternative  the time series data was divided into windows that are labeled either spindle positive or spindle negative based on gs
scoring 
first  a moving average of the time domain data  found through convolution of the data with a    sample gaussian blur filter  was
subtracted from the original data  windows of    samples that advanced at a hop rate of    samples were used  features were extracted from
each window  windows were spindle positive if over     of its samples were spindle positive 
it was assumed that the data would adhere to a hidden markov model  within this model  each window has a hidden state  z   and an
observation  x  
window state was chosen to be the number of past spindle positive or spindle negative windows adjacent to the current state  if a series
of spindle positive windows are noted  the state is positive  if a series of spindle negative windows are noted  the state is negative  if the window
switches from spindle negative to spindle positive or vice versa  its state becomes    states were capped within the range of     to    
representing   and   seconds of data  respectively  these values were chosen because of typical spindle duration and time between spindle
values 
n

n

      forward backward algorithm

figure    sliding windows   forward backward algorithm
taking advantage of conditional independence in the hmm model  a recursive means of computing the probabilities of z given all x
values exists      in the equations below  i and j represent state numbers  z represents the state of sample window n  and x represents the
observation at sample window n  x represents all the observations spanning windows n through m  n is the total number of windows 
n

n

n

n m

the probability that a zn is state i is the product of i n  and i n  
 eq    
 and  may be computed recursively 

 eq    
 eq    

fi eq    
 eq    
 and  are initialized as follows 
 eq    
 eq     
to perform the recursion described  the transition probabilities between states  the probability of a specific observation  and the probability of
the initial state must be approximated using training samples  to maintain tractable values   i n  and i n  were scaled to sum to   for each n 

   results
    cascaded system results
      initial classifier cascade test
an initial performance check was used to verify the effectiveness of logistic regression  gda  and svm  and to establish baseline sensitivity 
specificity  and accuracy values when all    features are used  the results in table   were amassed through    trials of    fold cross validation 
table    initial cascade test results
svm  gaussian kernel  
svm  polynomial kernel
svm  mlp kernel   c  
logistic
gda
c   
  c    
  
sensitivity
                                   
            
           
specificity
                                   
            
            
overall accuracy
                                   
            
            
     tp      fp  and     fn observations were previously made using the wavelet based detector  cascading the wavelet based
detector with a logistic regression for separating tp from fp results in     tp      fp  and      fn observations 

      feature and classifier selection results
        forward search results
   trials of forward search were performed using     accuracy as the threshold for terminating the run  figure   shows the frequencies
of the    features being selected by    forward search trials  the features that are most frequently selected are                       and    

features
               
sensitivity
specificity
overall accuracy

figure    frequency of features selected by forward search

           
           
           

features
                   
       
           
           
           

all    features

           
           
           

table    logistic regression using forward search results

        minimum correlation
because the minimum correlation set computes a value for every possible combination  it can only be reasonably used for smaller sets  the
set with the lowest value for each number of features are listed in table   along with its corresponding sensitivity  specificity  and overall accuracy 
performance was approximated through    trials of    fold cross validation with logistic regression  table    
table    minimum correlation sets for fp and logistic regression result
number of features
best set
sensitivity
 
      
           
 
         
           
 
            
           
 
             
           
 
                
          

specificity
            
           
           
           
           

accuracy
           
           
           
           
           

        best within each group
the best feature sets found through the minimum correlation technique contain features that would have been intuitively sorted in different
feature categories  following this logic  one last test involved selecting the best features from each category heuristically  the categories and
their feature numbers are listed in table   
for each category  feature sets were created using a single feature from the category and every feature from the other categories 
overall accuracy  assessed through    logistic regression trials with    fold cross validation  was used to rank the category features 

fitable    subgroup feature selection
group

frequency

amplitude

time toamplitude

sir

spr

contextbp

contextwa

duration

power

activity

sigma
index

sigma
power

features  ranked from
best to worst

    

    

         

           
        

           
        

              
     

              
     

 

 

     

  

  

the best features from each category were combined into the feature set                                      sensitivity  specificity  and
accuracy  estimated through    trials of    fold cross validation of logistic regression  are                           and             
respectively 

        pca
we performed pca on the data and obtained    principal components and scores  the scree plot below only shows the first    instead of the
total     components that explain almost      of the total variance  moreover  the first component by itself explains about     of the variance  so
that it might be a reasonable way to reduce the dimensions  however  when we plot the principal component scores of tp and fp observations 
we find that none of the first   principal components discriminates the two groups very well  figure    

figure    pca results

      svm classification
after some initial tests  we decided to use gaussian kernels for our svm classifier  because the other two kernels  polynomial and multilayer
perceptron  dont converge very well  we tuned parameters by trying a geometric sequence of the regularization parameter c from  e   to  e  by
a factor of     and a geometric sequence of the scaling factor from  e   to  e  by a factor     the    fold cross validation results of this tuning
process are shown in figure   below  we found that c       and sigma      give the best accuracy result  we then used svm and the optimal to
select the best   features for discriminating fp and tp observations  feature    and feature    are the best pairs for discriminating tp and fp
using svm 

figure    turning svm parameters

sensitivity
specificity
overall accuracy

svm  c    sigma             features
           
           
           

figure    svm classifier on best   feature pairs

svm  c     sigma          features
            
           
           

svm c     sigma       feature        
            
            
             

      forward backward recursion
the subset of features                                      was found to be relatively effective at separating tp from fp observations 
some of them  such feature     duration   cannot be computed within the hmm windows paradigm  the ones that could                            
         were extracted from each window and used to assess the performance of the forward backward recursion algorithm 
a single    fold cross validation trial of the forward backward algorithm resulted in tp  fp  and fn counts of             and     
respectively 

fi   discussion   future work
    classifier cascade
the addition of the classifier to the wavelet detector increases overall specificity and decreases overall sensitivity  if the wavelet detector is
configured to have a positive bias  overall performance could be within the desired range 

    classifier selection
logistic regression performed better than gaussian discriminant analysis  suggesting features dont follow a gaussian distribution  svm
with gaussian kernels perform better than linear  polynomial or mlp kernels  after tuning  svm and logistic regression have similar results 

    feature selection
in our forward selection scheme  the most frequently selected features found are likely consistently selected near the beginning  while the
ones that appear less often are likely selected later and inconsistently  by adding one feature at a time to the set  we might miss sets that are
strong together but weaker alone initially  which is why its results might be different from the minimum correlation set strategy 
the minimum correlation set strategy identifies sets that provide the most information  however  the information is not necessarily
useful  for example  if a feature value is always    its correlation with other features is always    but its useless in classification  since brute
force computation of correlation values for every combination is not much faster than directly testing the classifier using every set  in this case  a
direct classification test might be more useful  interestingly  the minimum correlation set returns features that would have been manually placed in
different subgroups  suggesting the subgroups chosen do  as desired  have low correlation with each other 
manually dividing the features into subgroups is faster than the minimum correlation set strategy  but assumptions were made about which
features are correlated enough to place into a group 
pca results showed classes arent separable in the dimensions with the largest variances 

    hmm
a series of hmm tests could be a project in itself  the fact that values didnt pan out could be due to the markov model assumptions
 observations are predictable by a state  observations might not be normally distributed  or state discretization schemes  window and hop sizes
selected  
the hidden state scheme selected might not be a good predictor of observations because the durations of spindles can vary by   fold    
to   seconds  and maximal amplitude always occurs at the center  a better scheme might involve assigning a ratio to each state  the number of
spindle positive windows in a row so far divided by the total number of spindle positive windows in a row  discretized 
the normal distribution assumption was likely also problematic  examining histograms of feature values  most features are positive and
positively skewed  if features adhere to the log normal distribution or the chi squared distribution with a single degree of freedom  the issue would
be simplified because respectively  their logs and square roots would be normally distributed  however  that was not the case 

inaccurate conditional observation probability estimates ruin hidden state probability estimates  one way to fix this is to find an established
distribution to which the observations can fit  another way would be to discretize the observations and estimate the conditional probability
heuristically  which would require either a lot of training data  or very coarse discretization 
a window size of     seconds was selected because spindles last at least    seconds  and it might be difficult to discriminate between
spindle positive and spindle negative states using features extracted from a window that is much larger than    seconds  however if window size is
too small  the data will be noisy  a hop size that is too large might miss spindles altogether  but if hop size is too small  the number of states
becomes too large 
the hmm model is flexible but configurations must be further explored 

   conclusions
the best features within each subgroup are                                       with logistic regression  the set yields sensitivity 
specificity  and accuracy values of                           and              respectively  with svm  this yields sensitivity  specificity  and
accuracy values of                         and              based on this  we recommend both classifiers 
for the wavelet detector and classifier cascade  this results in     tp      fp  and      fn observations  the total number of false
observations of the cascaded system is       the total number of false observations for the wavelet detector is       thus  the classifier
increases accuracy  but it should be noted that it decreases sensitivity and increases specificity 
the hmm model made      tp       fp  and     fn observations  its sensitivity is high  but its specificity is too low 
although there is an opportunity to capture more contextual information using the hmm model  important features  such as an observation
duration estimate  are also lost  the hmm model parameters must be tested more thoroughly 
because true negatives are so numerous  and because true positive data is required for research  in spindle detection  it would make sense
to maximize specificity given a sensitivity approaching       i e  yielding false positives is okay as long as there are very few false negatives  

references
    silber  michael h   et al   the visual scoring of sleep in adults   j clin sleep med                     
    http   www aasmnet org scoringmanual v      html index html gscoringstagen  html
    hsu  chih wei  chih chung chang  and chih jen lin  a practical guide to support vector classification 
    t  mailund  c  storm  hidden markov models slide show for pattern recognition in bioinformatics  department of computer engineering  aarhuss 
    

fi