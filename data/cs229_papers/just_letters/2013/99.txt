automated recommendation systems
collaborative filtering through reinforcement learning
mostafa afkhamizadeh

alexei avakov

reza takapoui

department of ms e 
stanford university
email  mafkhami stanford edu

department of electrical engineering 
stanford university
email  linrav stanford edu

department of electrical engineering 
stanford university
email  takapoui stanford edu

abstractwithin this work we explore the topic of large scale 
automated recommendation systems  we focus on collaborative
filtering approaches  wherein a system suggests new products
to users based on their viewing history as well as other known
demographics  there are several approaches to this in current
literature  the simplest of which treat it as a matrix completion
problem  we explore the setting from a reinforcement learning
perspective by applying traditional algorithms for reinforcement
learning to the problem 

i  p roblem f ormulation
numerous online services such as netflix  amazon  yelp 
pandora  online advertisings  etc  provide automated recommendations to help users to navigate through a large collection
of items  every time a user queries the system for a new item 
a suggestion is made on the basis of the users past history and
 when available  their demographic profile  two typical ways
of producing these recommendations are collaborative filtering
and content based filtering  there are two simultaneous goals
to be satisfied  helping the user to explore the available items
and probing the users preferences 
one of the models that captures this setting well is the multiarm bandit  an important model for decision making under
uncertainty  in this model a set of arms with unknown reward
profiles is given and  at each time slot  the decision maker
must choose an arm to maximize his expected reward  clearly 
the decision at each time slot should depend on previous
observations  thus  there is a trade off between exploration 
trying arms with more uncertain reward in order to gather more
information  and exploitation  pulling arms with relatively high
reward expectations 
for our purposes the arms have a very specific structure
and this setting has previously been referred to as the linearbandits model  see       here  it is assumed that the underlying matrix of preferences  which contains the rating user
i gives to item j at entry  i  j   has a low rank structure 
hence  ratings made by user i to item j can be approximated
by a scaler product of two feature vectors ai   bj  rp  
characterizing user and item respectively  in other words our
observations  rij can be viewed as
rij  

ati bj

  zij

where zij represents the unexplained factors 
in the general setting  both the user and item feature vectors
are treated as unknown  and our recommendation algorithms

must estimate them over time  however  some works  like
     make a simplifying assumption that the item feature
vectors are known  we explore both settings  but find more
meaningful results in the case where the item feature vectors
are known  in this case  the item feature vectors can be either
constructed explicitly  or derived from users feedback using
matrix factorization methods  with the item latent vectors in
hand  we can treat each user independently and throughout the
explore exploit trade off  we can try to estimate and exploit
the users latent vectors  these feature vectors can depend
on users demographic information and their past behavior in
rating items 
the goal of our system is to develop a recommendation
policy  which suggests items to users  this policy will  at
each time slot  output a recommendation based on the previous
observations  this policy must properly adjust for the exploreexploit trade off  and classically there are two types of policies 
which differ in the way they perform exploration  optimistic
policies  e g  upper confidence bound  ucb   and probabilistic
policies  e g  posterior sampling  ucb algorithms have been
applied to this problem in the past  but posterior sampling is
less common  posterior sampling  also known as thompson
sampling  was introduced in      and offers significant advantages over ucb methods  as shown in       however until
recently it has not been very popular or feasible 
our primary objective is to explore the feasibility of collaborative filtering through posterior sampling  we analyze
its performance on real world data  specifically the freely
available movielens datasets  and compare it to existing
methods such as ucb and the work done in    
ii  s ystem m odels and a lgorithms
in this section we will introduce some notation used
throughout the rest of this work  as well as the algorithms
that we seek to implement 
a  notation
we have a set of users  i                 m  with corresponding
feature vectors ai  rp   and items  j                 n  with
corresponding feature vectors bj  rp   we refer to these
feature vectors collectively as a  rpm and b  rpn  
thus the true ratings can be captured in the matrix at b 
at each time t  z    a user i t  will enter the system and

fibe recommended an item j  t    after which they will give it a
rating r t  according to
r t    ati t  bj  t    z  t 
where z  t  captures the unexplainable deviation of the observation from our model  we 
refer to the viewing
history at time
 t 
t as the sequence h t    i      j       r          i e  all the
viewings in the system before time t  thus on a high level  at
time t our program seeks to use its knowledge of user ai t  to
make the best possible recommendation 
the job of a recommendation system is to define a function
h     which given a user will output a recommendation for
that user  unknown to the system  there is some optimal policy
 t 
which at each time t would output recommendation j   to
measure the performance of our system  we will compare
the systems recommendations to the best recommendation 
specifically define the regret of the system  at time t  to be
r t   

t 
x

h
i
ati    bj      e r   


   

that is  at each time step we increase our regret by how far the
expected rating of our recommendation differs from the best
possible rating  ultimately we seek to derive a policy which
achieves minimal regret 
b  posterior sampling
algorithm   posterior sampling
start with prior distribution on  a  b   f  a  b 
for t               do
observe arrival of user i t 
sample a  b  f  a  b h t   
compute and output recommendation j  t  where
h
i
j  t    arg max e ati bj
j

 
observe the users rating r t 
end for
the idea behind posterior sampling algorithm is to force
optimism through probabilistic action  specifically at each
time step  t  we will make a recommendation j  t  based on
the probability that it is the best possible recommendation 
 t 
p j  t    j   however  this probability is inaccessible  so
instead the algorithm samples a model for the unknown feature
vectors based on the probability that they are the true feature
vectors  given the viewing history   and finds the optimal
recommendation should this be the true model  it can be
shown that this sampling technique is equivalent to sampling
a recommendation based on the probability it is optimal  and a
more detailed description of the algorithm and its motivations
can be seen in      thus the algorithm proceeds to keep track
of the distribution of model parameters at each time step  and
updates them accordingly 

to implement this algorithm all that remains is to choose
a prior on the model parameters  and compute their posterior
distribution given a viewing history  as in      and other prior
literature  we assume ai   bj  n     ip  p  i i d   furthermore
we assume that unexplained deviations of the observations are
gaussian  i e  z  t   n     z     now we are ready to compute
the posterior distribution 
using bayes rule observe  for compactness we use f    to
denote the distribution of the argument  
fi

fi


f h t  fi a  b f  a  f  b 
fi  t 

 
f a  b fih
f h t 

qt 
f  a f  b      f z    
  r

qt 
f  a f  b      f z     dadb
a b
in the above z       r     ati    bj       and the integral of the
denominator is over the entire space of rpm  rpn  
for the rest of this report  we consider the simpler case
where the vectors bj are given and we treat each user independently  this problem is extensively studied in literature  but
as far as we can tell has never been solved or analyzed through
posterior sampling  we explore it more concretely below 
for compactness we will consider only the feature vector
of a single user  a  rp   a priori we assume it comes from
n     ip  p  as above  and we now consider the viewing history
h  t  to be the history of the active user  as opposed to all
users   we can now compute the posterior distribution as
follows 
fi 
 fi

f h a h t  fi a fa  a 
fi  t 

fa h a fih
 
fh h t 

qt 
fa  a      fz z    
  r

qt 
f  a      fz z     da
rp a

qt 
fa  a      fz r     at bj    
  r

qt 
f  a      fz r     at bj     da
rp a
but observe in this simple case computing the posterior is
much simpler  the numerator is clearly a gaussian  and the
denominator is just a normalizing
term  thus we determine

fi
 t 
 t 
a fih t   n a   a   we can formulate a recursive
 t 

 t 

update rule for the parameters a   a by massaging the
numerator into an appropriate form  this is done in the
appendix   we find the following update equations for the
posterior 
  
t
b t   b t  
 t 
 t  
a
 
a
 
z 


r t    t  
 t 
 t 
 t      t  
a
   a a
a
 
b
z 
these recursive update equations are convenient for implementation  and can be used efficiently by storing     however
some intuition as to their operation can be seen by applying

fithe matrix inversion lemma  through it  we find 
 t  

 t 
a

 

 t  

a

a

 t  

bj  t   btj t   a
 t  

z    btj t   a
 t  

 t 
a

 

 t  
a

 

r t   a

bj  t  

 t  

 a

 t  

z    btj t   a

 t   t

bj  t   a
bj  t  

bj  t  

thus  essentially  at each step the posterior mean shifts towards
 or away  the feature vector of the recommended item  similarly the covariance  thins out to select for single direction 
the rest of our work revolves mostly around analyzing
the simplified problem setting  however this simplification is
extremely useful for the general case as well  observe 
fi

  fi

 fi

fi
fi
fi
  f a fib  h t  f b fih t 
f a  b fih t 

a ucb  in the general problem setting  it is unclear how to
implement ucb in any meaningful way  however it is rather
elegant in the simplified case of given item feature vectors 
in the simplified case   using the priors described in the
previous section  we observe that the posterior of a given
h t  is gaussian  thus the distribution on the reward of
recommending item j is also gaussian  we compute the mean
and variance as follows 
j 

  btj a bj   z 

j

  btj a

thus computing the p th percentile of the reward can be done 
simply by inverting the cdf of the normal distribution 
d  mixed approaches



in the above p is a polynomial function in the entries of b  
k is some scalar  and c is a vector in rnp   unfortunately 
even in this form  it is still unclear how to sample from this
distribution 

from evaluation we observe that u cb and posterior sampling each have unique advantages  thus we propose various
schemes that allow you to achieve the various performance
trade offs of both  first we propose an  greedy approach and
second we propose a two phase approach  these were both
studied in the simplified case  but could potentially be applied
to the general setting as well 
the  ucb algorithm will flip a weighted coin at each
timestep to decide weather to obtain a recommendation
through posterior sampling or through ucb  specifically the
algorithm will elect to perform ucb  percent of the time 
the two phased approach will begin by learning through
posterior sampling until some time t   after which it proceeds
to output recommendations through the ucb approach 
in the next section  we will thoroughly study the performance of all of the algorithms presented in this section 

c  a ucb approach

e  the case of no repeat recommendations

thus we can perform posterior sampling in the general
fi case

by first sampling item features b  according to f b fih t   
and then sampling a from a gaussian distribution with mean
and variance determined by the previously derived update
equations given the
fi selected features b  unfortunately the
distribution of b fih t  is quite complicated  after vectorizing


the matrix b into a vector b  rnp we find 
 

  
 fifi
    
 



exp k b t b   ct b
f b fih t   p b

algorithm   ucb
start with prior distribution on  a  b   f  a  b   and an
optimism parameter p        
for t               do
observe arrival of user i t 
compute the distribution on the reward of each item
for all items  compute uj   the p th percentile of
the reward of item j
compute and output recommendation j  t  where
j  t    arg max uj
j

observe the users rating r
end for

 t 

ucb is a completely different approach from posterior
sampling  at each timestep the algorithm computes an upper
confidence bound on the reward of each of the items  the
algorithm will then suggest the algorithm with the highest
ucb  for our purposes  we will use a specific percentile of
the reward as the ucb of each item  this is generally hard
to do and other literature uses various heuristics to determine

throughout this work we assumed that it is relevant to
recommend the same item several times  however  in some
settings this is not very natural  for instance  if the system
provides recommendations for viewing movies  all of the
above algorithms would eventually chose to show the same
movie over and over  clearly this is not very useful  and
this can be resolved in several ways  we could lower the
reward of successive viewings  but this adds a complicated
time dependence to our model  more simply we can prohibit
the algorithm from suggesting the same item multiple times 
in the case of suggesting movies this is natural since users
would rarely view the same production multiple times 
iii  i mplementation and e valuation
in this section  we present our implementation results for
the aforementioned algorithms  for the purpose of numerical
simulations  we used matlab  we have carried out algorithms both on synthetic data and freely available movielens
dataset 
for the purpose of synthetic data  we generate a random
          matrix  the same size as movielens data  with
rank    by generating random gaussian feature matrices and
multiplying them together  each of the entries of feature

fi   
posterior sampling
ucb with     percentile
ucb with      percentile
ucb with      percentile
ucb with       percentile

   
   
   
cumulative regret

matrices comes from a n       p   then  we will take item
feature vectors as granted and try to estimate user feature
matrix by considering a gaussian prior 
figure   shows the cumulative regret versus time  for
posterior sampling and ucb with four different parameters 
we can see that posterior sampling might work worse at first
by exploring too much  but it pays off later when the better
understanding of the arms comes to help later 

   
   
  
  
  

  
  

 

 

   

   

   

   
time

   

   

   

   

  

fig     cumulative regret of posterior sampling and ucb algorithms for synthetic data

  

with rank      

  
  
  
  
 
 

 

   

   

   

   
time

   

   

   

   

fig     cumulative regret of posterior sampling and ucb algorithms on synthetic data

notice that the regret observed by each of these algorithms
is very good compared to the total reward they obtain  the
cumulative rewards at t     is in the order of     while the
difference in rewards is in the order of     in order to show
this  we have plotted the cumulative reward versus time for
all of these algorithms in figure   and it can be seen that it
is close to the optimal reward 
   
posterior sampling
ucb with     percentile
ucb with      percentile
ucb with      percentile
ucb with       percentile
optimal reward

   

   

algorithms  here  the greedy algorithm chooses the arm that
maximized the expected instantaneous reward and can be
considered as ucb with percentile      at each time  the
 greedy algorithm makes greedy decision with probability  
and performs an iteration of posterior sampling with probability      by looking at figure    we can see that the
performance of the greedy algorithm improves dramatically
when its combined with posterior sampling     of the
time  this will result in even more computationally efficient
methods while the regret still remains acceptably low 
  
  
  
  
cumulative regret

cumulative regret

  

cumulative reward

  

posterior sampling
ucb with     percentile
ucb with      percentile
ucb with      percentile
ucb with       percentile

  
  
  
  
  

posterior sampling
eps    
eps    
greedy

 
 

   

   

   

   

   

   
time

   

   

   

   

fig     cumulative regret for hybrid approach on synthetic data

 

   

 

 

   

   

   

   
time

   

   

   

   

fig     cumulative reward of posterior sampling and ucb algorithms on synthetic data

we also tried similar simulations for different ranks of
the underlying matrix  figure   shows the performance of
these algorithms when the rank of the preference matrix is
     it can be seen that posterior sampling outperforms ucb 
one interesting observation is that unlike posterior sampling 
ucb methods are very sensitive to the parameters used in
algorithms  in our case the percentile parameter   and using
inappropriate parameter may result in non zero asymptotic
regret  we observe that the optimal tuning is highly sensitive
to the data  specifically to its rank 
as a variation of the introduced algorithms  we have
carried out a combination of posterior sampling and greedy

we also carried out the posterior sampling and ucb algorithms under the assumption that no item can be recommended
to a user more than once  notice that in this case  we expect
the regret to be decreasing at some point  because the expected
regret at time      is equal to zero  figure   shows the
performance of these algorithms in this case 
we have implemented all these methods for the movielens
dataset and got similar results  for example figure   shows
the cumulative regret for the posterior sampling and ucb with
different parameters on movielens dataset  as seen in figure
   ucb algorithm with parameter      works better than the
other instances of ucb  which further shows that there is no
rule for finding the best parameters for ucb algorithms 
iv  c onclusions
all of the algorithms we analyzed perform extremely well 
the difference in regret between them is negligible compared

fi 
posterior sampling
ucb    
ucb    
ucb    

 

cumulative regret

 

 

 

 

 

 

 

   

   

   
time

   

   

   

fig     cumulative regret for the case of no repetitions  on synthetic data
  
  
  

cumulative regret

  
  
  
  
  
posterior sampling
ucb with     percentile
ucb with      percentile
ucb with      percentile
ucb with       percentile

  
 
 

 

  

  

  

  

   

only the feature vector of a single user  a  rp   then 

qt 

 fi
fa  a      fz r     at bj    
fi  t 
  r
f a fih

qt 
f  a      fz r     at bj     da
rp a
t 

y 
  c  fa  a 
fz r     at bj    

  fi
    
fi
  c  fz r t    at bj  t   f a fih t  
 
r t    at bj  t  
  c  exp 

 z 
 
 
 a   t  
 
exp   a   t  
 t  t  
a
a
a
 
now it is clear that the distribution remains gaussian  at this
point simply compute the coefficients of the quadratic and
linear terms to solve for the new mean and covariance  this
yields
  
 t    t   t
b
b
 t 
 
a t    
a
z 


r t    t  
 t 
 t      t  
 t 
 


b

 
a
a
a
a
z 
b  woodury matrix identity   update rules
recall the woodury matrix identity 

time

fig     cumulative regret of posterior sampling and ucb on movielens dataset

 a   u cv      a   a  u c     v a  u

 

v a 
 t 

to the total reward collected  thus we advocate posterior
sampling as the best general purpose solution for several
reasons  first  it is extremely efficient compare to the ucb
style approach  second  it does not require any tuning  while
we observed that ucb can outperform posterior sampling it
is extremely reliant on proper tuning  which can be hard to
determine in practice   furthermore  posterior sampling can
clearly be extended to the general problem setting  whereas our
stated ucb method is not  lastly we note that using previously
mentioned hybrid approaches it is possible to achieve many
different efficiency regret trade offs 
v  f uture w ork
it would be interesting to more closely analyze the general case  posterior sampling can be implemented through
gibbs sampling  or the metropolis hastings algorithm  ucb
as described in this paper would be much more difficult to
implement  but we could try various heuristics and other ucb
style algorithms 
alternatively this work could be continued in the practical direction by building a real life recommendation system
utilizing these algorithms and studying its performance 
a ppendix
a  derivation of posterior update rules
again consider the simplified case where we know the latent
feature vectors of the items  for compactness we will consider

for ease of notation in this section we will refer to a as
 t 
t   a as t   r t  as r  and lastly we refer to b t  simply as
b  apply the lemma to the previously derived update rules 

 
bbt
t  
t     
z
t  bbt t 
  t    
z   bt t  b
now we can plug this into the derivation of t  


r
t   t t    t      b
z



t  bbt t 
r
 
 
t    
t  t      b
z   bt t  b
z
t  btt  b
  t    
 
   bt t  b
   z

r z t  b   bt t  bt  b  t  bbt t  b
z 
z    bt t  b


rt   t  btt 
  t   
b
z    bt t  b
r eferences
    yash deshpande  andrea montanari  linear bandits in high dimension
and recommendation systems  available online 
http   arxiv org abs          
    daniel russo  benjamin van roy  learning to optimize via posterior
sampling  available online  http   arxiv org abs          
    paat rusmevichientong  john n  tsitsiklis  linearly parameterized bandits  available online  http   arxiv org abs          

fi