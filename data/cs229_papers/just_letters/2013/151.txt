cs     project report
keyword extraction for stack exchange questions
jiaji hu  xuening liu  li yi

 

introduction

 

the stack exchange network is a group of questionand answer websites with each site covering a specific
topic  including stack overflow for computer programming questions  similar to many websites  stack
exchange sites use tags for fast item retrieval and
flexible grouping  ultimately providing better user experience and easier management 
in this project  we build an automated keyword extraction system for stack exchange questions  which
generates suggested tags for new questions based on
the title and content of that question  this system
has advantages over the status quo in terms of correctness and convenience 
our keyword extraction system takes a questions
title and text content as input  and outputs no less
than one and up to five tags deemed to be suitable
for the question  for each possible tag  a classifier is
trained to predict the presence of that tag  classification results from all the predictors are then aggregated to return the final output 
our project focuses on evaluating a number of different kinds of features extraction methods as well as
classification methods  in an attempt to gain insight
into their effectiveness in the field of keyword extraction  detailed test results are shown and error analysis given in the report  and cases regarding specific
tags are discussed to reach preliminary conclusions 

data collection and
processing

data for training and testing is collected from the
stack exchange sites  stack exchange provides a
dump of its data every three months  which is free
for us to use  we also collected data from a competition on kaggle com 
our input data format has title and content
fields representing the title and content of the question  and a tags field which indicates the correct
answer tags for that question 
after our initial collection of data  we had around
  gb of text data  which contained around   million
examples  in the process of developing our system 
due to constraints in memory and time  we used up
to         examples for training for testing  fig   
shows the word frequency distribution of the first
       examples in our dataset 

   

number of words

  

log log word frequencies

 

   

   

   

     
  

 

   

   
   
number of appearances

   

   

figure    word frequencies

prior work

from fig     we see that around      of all words
in the generated dictionary appear only once  another     of words appear only twice  this will result in very sparse unigram feature vectors  the issue
of sparse features is addressed when we describe our
feature extraction later in section     

keyword extraction is a topic that has been generating increasing interest both in industry and academia 
according to      well known keyword extraction algorithms rely on the fundamental concept of tf idf 
in      a naive bayes classifier was used along with
tf idf for keyword extraction 
 

fimodel

   
   

our keyword extraction model is as follows 
the model reads in two strings  title and content
as input and performs unigram feature extraction on
these strings to return a feature vector for that input 
then  the feature vector is input into a group of binary classifiers  one classifier for every possible tag 
that predicts whether an input should have the specific tag  finally  the classification results are aggregated so that all the tags where the predictor output
was positive will be our final output  if a question is
not assigned at least one tag in this process  we compare probabilities to give it one tag  if the system
gives a question more than five tags  we take the five
most common tags as the output 
through practice  we find that the two cases above
do not occur often  and throughout the project  we
have focused on improving the effectiveness of the binary classifiers as opposed to working on aggregating
their outputs 
we used the f  score as the evaluation method
for our system  the reason is that in our problem 
the accuracy of the binary classifier may not be a
good indication of the effectiveness  since for any tag 
the vast majority of examples should not be assigned
that tag  therefore  a trivial predictor that always
predicts no would in fact reach very high accuracy 
even though it is practically useless 
in cases like these  the f score is a good measure to
measure a tests accuracy  the f score is computed
by considering both the precision p and the recall r
of the test  the f  score is calculated as follows 
 pr
f   
p r

training and dev f  scores on naive bayes on label php
training f 
dev f 

   

   

dev scores on naive bayes on label php
f 
precision
recall

   
scores

   
scores

 

   

   
   
   

   
   
 

    

    
    
number of training examples

    

     

   
 

    

    
    
number of training examples

    

     

figure    learning curve and f score for naive bayes

classifier was extremely hesitant to classify examples
as positive  this resulted in a low f  score  where
precision was relatively high  but recall was very low 
note that even the training f  score could not go
higher than around     
to improve out baseline system  we worked on both
the classification algorithm and the feature extraction
step  for the classification algorithm  we switched to
a support vector machines  and moved on to develop
methods of enhancement such as boosting  for feature extraction  we tried four different methods with
varying results 

   

feature extraction

for the baseline system  we first used word frequency
to form unigram feature vectors  the feature vectors obtained through this process were very sparse 
to address this issue  we tried stemming  stop word
removal and l  based feature selection 
     

stemming

   

stemming is a practice often used in natural language processing which reduces words to their roots 
in our project  we try to maximize the f  score of this process condenses the unigram feature vector by
our classifier 
combining similar words  we used the porter stemming algorithm on our text inputs with the following
results 

 

methods

   

   

baseline system

dev scores on svm without stemming on label php

   

   

dev scores on svm with stemming on label php

   

   

   

scores

scores

for our baseline system  we used a naive bayes classifier using the multinomial event model and laplace
smoothing  we used unigram features extracted from
f 
f 
precision
precision
the text without additional processing  we trained
recall
recall
the naive bayes classifier on some tags to observe its
performance  an example result is shown in fig    
from the results  we observed that due to the large figure    classification results with and without
skew towards negative examples  the naive bayes stemming
   

   

   

 

   

   

   

    

    

    

    
    
number of training examples

    

     

    

    
    
number of training examples

    

     

fiwith stemming  we reduced the feature vector dimension by      however  results show that stemming causes the system to have lower precision 
higher recall  and similar f  score to a system without stemming  this means that stemming makes the
system more prone to classifying examples as positive  but adds a fair amount of false positives 
we believe that the technical nature of our input
caused trouble with the stemming algorithm  and by
stemming  we may be losing information useful for
classification  therefore  we did not continue to use
stemming for our feature extraction 

feature dimension  using this method  we obtain a
scheme for discarding and retaining features from the
original feature matrix  in the future  we only select
the features retained by the feature selection process 
the learning curves for a linear svm classifier with
features before and after feature selection is shown in
fig    
dev scores on svm without feature selection on label php
    

    

    

scores

    

scores

    
    

    

    

    

    

     

    

f 
precision
recall

    

stop word removal

     

for stop word removal  we studied methods of removing particular words from our dictionary without affecting the classification performance  we eliminated
stop words and rare words whose occurrences were
below a certain threshold  in particular  by removing
words that appeared only once in the whole training set  we were able to cut the feature dimension by
     if we moved the threshold to    we would cut
another     
through testing  we found that removing oneoccurrence words did not result in a noticeable difference from the classification results of our system 
however  removing two occurrence words slightly
lowered the f  score  therefore  we concluded that
it was only safe to exclude one occurrence words 
in addition  since different words may have different importances for classification  we tried using some
weighting method to filter out unimportant words
and highlight indicative words  we tried using tfidf to do this  but had little success 

dev scores on svm with feature selection on label php

    

     

     
     
number of training examples

     

      

f 
precision
recall

    
     

     

     
     
number of training examples

     

      

figure    classification results with and without feature selection
using l  based feature selection  we were able to
reduce our feature dimension from         to      
 a     reduction  moreover  the classification f 
scores did not suffer from the lower feature dimension 
and in fact even rose in most experiments 
     

separating title and content

after we found a good way to reduce our feature dimension  we were free to try ways to increase features
so that more useful information was input to the classification algorithm  we believed that the questions
title contained different information from its content 
and that we would benefit from treating them differently  since we were able to separate the question title and content in our inputs  we tried treating words
in the title and words in the content as different features  effectively doubling our feature dimension  we
      l  based feature selection
trained our classifier on the features before separation
though removing rare words cut our feature dimen  and after separation  with results shown in fig    
sion approximately by half  we were not satisfied with
dev scores on svm without title content on label php
dev scores on svm with title content on label php
this rough method  for our feature extraction  we
moved on to try out feature selection  the method
we chose was l  based feature selection 
after we acquire the feature matrix for our training
set  we train a linear svm using the l   penalty 
f 
f 
precision
precision
the training results in a learned weight vector  which
recall
recall
represents how important the classifier believes each
feature dimension to be  using the l   penalty  the
learned weight vector will be sparse  with many zeros figure    classification results with and without sepin the weight vector  for our feature selection  we do arating title and content
the following  for each feature dimension  if the same
from fig     we see that when the number of traindimension of the weight vector is zero  discard that
    

    

    

    

scores

    

scores

    

    

    

    

    

    

    

    

     

 

    

     

     
     
number of training examples

     

      

     

     

     
     
number of training examples

     

      

fiing examples is small  the performance of the classifier is significantly better with separating titles and
content  as the amount of training data increases 
separating title and content for features still performs
better  with an f  score approximately      higher
when there are         training examples  we believe
that separating title and content for features provides
extra useful information to the classifier  so it needs
fewer training examples to perform well  when the
amount of training data is high  the extra information
still helps raise performance 

number of examples and feature dimensions grew  the
time and space requirements for training and storing component classifiers became more than we could
handle  therefore  we were unable to conduct our experiments on the full dataset  and only show results
obtained on smaller training sets 

 

results

   

classification performance

for our final system  we used unigram features after separating title and content and l  based feature
    learning algorithms
selection for the feature extraction step  for the clas      support vector machines
sification step  we used a linear svm with parameter
c set by cross validation  fig    and table   shows
to improve on our baseline system  we turned to usthe precision  recall and f  score of the system on
ing svms as our classification algorithm  after trying
label php  the performance of the system is similar
different kernels  we found that the linear kernel was
on other tags 
easier to train and suffered less overfitting problems 
fig    shows the performance of the rbf kernel and
dev scores on svm with feature selection and title content on label php
linear kernel on different tags  we see that the linear
    
kernel is more consistent and does better on average 
    

boosting

    

scores

     

    

one major challenge in our problem is that the training examples for any tag are heavily biased towards
negative examples  therefore  the positive examples
are more likely to be misclassified by our svm  note
that we do very well in terms of true negatives   the
boosting mechanism forces component classifier to focus on the misclassified examples  which makes it very
suitable for our classification problem and evaluation
method  according to      adaboost with heterogeneous svm can work better compared with generally
used adaboost approaches with neural networks or
decision tree component classifiers  the problem is
how to generate such kind of diverse and moderately
accurate  weak  svm component classifiers      suggests using rbf svm  whose parameter  can be adjusted to balance between accuracy and diversity  and
to obtain a set of moderately accurate rbf svms for
adaboost  since large  corresponds to less accurate
classifiers and gives a chance that two classifiers can
disagree with each other more  we adopted and implemented the diverse adaboost svm approach proposed in     and achieved better generalization performance than a single svm  experimental results
and comparisons are made in fig    in section     
one caveat of our boosting algorithm was that it
was very memory and computation intensive  as the

    
    
    

f 
precision
recall

    
     

     

                           
number of training examples

      

figure    classification results for label php

pos
neg

pos
   
   

neg
   
    

precision
recall
f  score
accuracy

     
     
     
     

table    confusion matrix and classification statistics
for label php
from fig     we see that the performance of the system gets better as the number of training examples
increase  with         examples  we achieve an f 
score of        which is a good improvement on our
baseline system  to study the effect of the additional
improvements to feature extraction that we implemented  we trained an svm using features without
feature selection or title content separation  the resulting f  score was       therefore  it is confirmed
 

fito extract enough information from the words
to make a positive prediction  this is the most
popular error 

that our feature extraction methods result in an improvement of performance 
for boosting  due to the time and memory constraints  we could not run the diverse adaboost svm
algorithm on the whole dataset  instead  we used
     examples for training and testing  and compared
the performance of diverse adaboost svm with a
single svm with linear or rbf kernel  results in fig   
show that adaboost svm on average gets a     performance improvement compared with a single svm 

   our classifier was mislead  if a strong keyword
appeared in the text  the classifier is inclined
to predict positive  however  the keyword may
have been mentioned in passing  and was not a
key part of the question  for example  i know
this is possible in php  but how to do it in rubyon rails  would made our php classifier produce a false positive prediction 

 
linearsvm  average f          
rbfsvm  average f          
adaboostsvm  average f          

   

   the correct label was noisy  some of the labels
that our classifiers predicted made sense  unfortunately  the given correct answer did not
contain that label  in that case  we got false
positives 

   
   

f 

   
   
   
   
   

 

   
 

c 

java

php

javascript

android

conclusion

jquery

tag

in this project  we designed a system to predict tags
for questions on question and answer sites such as
stack overflow  we trained binary classifiers for
different tags  experimenting with feature extraction methods and classification algorithms  for feature extraction  we studied the effects of stemming 
stop word removal and l  based feature selection
on feature dimension and classification performance 
we also performed feature engineering by separating
question title and content to improve performance 
improved feature extraction provided a    improvement on f  score  in terms of classification algorithms  we tested and compared naive bayes  linear
and rbf support vector machines  and implemented
adaboost svm with good results  our final system
consistently achieves an f  score of over      for most
tags 

figure    comparison between adaboost svm  linear svm and rbf svm

   

feature analysis

using the weight vector learned by the linear svm
classifier  we can examine the weights to find out
which are the most predictive features of any particular tag  by doing this  we can get a view of what
our classifier learned from all the training data 
after analyzing the weight vectors  we found that
the weight vector was rather noisy  for example  in
the case of label java  some words that did not seem
to be related to java made it into the top    features  this shows that the classifier still suffers from
overfitting  however  there were still learned features
that were immediately recognizable as good features
 for example  java and eclipse were both top features for label java 

   

references
    eibe frank  gordon w paynter  ian h witten  carl gutwin  and craig g nevill manning 
domain specific keyphrase extraction       

error analysis

we also analyzed some test examples that our clas    xuchun li  lei wang  and eric sung  adsifiers got wrong  in our opinion  there were three
aboost with svm based component classifiers  enmain types of reasons for classification errors 
gineering applications of artificial intelligence 
                   
   there was not enough information in the input 
some of the false negative examples were very     brian lott  survey of keyword extraction techshort and did not contain the target label in
niques       
the title or text  our classifier was not able
 

fi