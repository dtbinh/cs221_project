a machine learning approach to webpage
content exraction
jiawei yao

xinhui zuo

department of cs
stanford university
email  jwyao stanford edu

department of ms e
stanford university
email  xzuo stanford edu

abstractapart from the main content  a webpage usually
also contains boilerplate elements such as navigation panels 
advertisements and comments  these additional elements are
typically not related to the actual content and can be treated
as noise that needs to be removed properly to improve the users
reading experience  this is a difficult problem as html is loose
in semantics and flexible in structure  in this paper  we model the
webpage content extraction problem as a classification problem
and employ machine learning method to solve it  for each text
block in the html document  we select a set of relevant features 
based on which an svm classifier is used to predict whether this
text block is content or non content  the results show that our
approach can achieve performance comparable to some popular
existing algorithms in this field 
keywordstext mining  content extraction  web semantic  algorithm

i 

fig     example webpage  original on the left  annotated main
content on the right

i ntroduction

over the years  the internet has become lots of peoples
primary source of information and people are spending more
and more time on web browsing  the news sites  blogs or other
information rich websites  for various reasons  displays many
boilerplate information such as advertisements and navigation
panels alongside the main content  see fig    for example  
it is desirable that these boilerplates be removed because 
   generally they are a distraction to the users and overload
of them greatly breaks reading experience     they hamper
information retrieval and the removal of them will benefit
search engines  and    not all websites are optimized for handheld devices which have relatively limited screen space and
extraction of main content can provide a hand held device
friendly view  the popularity of many read it later services like
instapaper  pocket and readability demonstrates the important
of content extraction 
however  the removal of these noisy elements is nontrivial 
because html is not a semantically strict language and
enforces no structural constraints  so content providers are free
to compose html in whatever way they like  although the
latest standard  namely html   promotes semantics of html
by introducing tags like  article    nav  and  aside  
the standard has not been widely adopted  to improve this
situation  webpage content extraction  ce  algorithms were
introduced and ce has been an active research area for the
past decade  many early efforts rely heavily on heuristics about
webpages with main content  such as text density  text to tag
ratio  etc  which are not universally applicable and might break
randomly because of the flexibility of html 
in this paper  we put webpage content extraction in the
machine learning setting and introduces a supervised learning

approach to content extraction  we break an html page into
text blocks which is structure independent and extract features
from the text blocks  an svm classifier is trained on the
text blocks and will be later used to predict the content of
test documents  we also do semantic analysis based on id
and class attributes associated with text blocks using naive
bayes algorithm  the result of semantic analysis can be incorporated into features of text blocks to improve classification
accuracy  experiments are conducted on two datasets from
     and       which represent characteristics of webpages
from several years ago and webpages with more recent trends
respectively 
the rest of this paper is organized as follows  section ii
gives an overview of works related to airplane model recognition problem  in section iii  our approach of airplane model
recognition is explained in detail  in section iv  the dataset 
design and evaluation metrics of the experiment are described 
results of the experiment and discussion are presented as well 
finally  section v gives conclusion of this paper and puts
forward some future work 
ii 

r elated w orks

some of the existing algorithms              are based on
the observation that compared to the content fragments  the
non content fragments of an html document are usually
highly formatted with more tags and also contain less text
and shorter sentences  as a result  non content fragments
have a higher tag density while the content fragments have
a higher text density  specifically  a number of features on the
block level are evaluated according to their information gain

fi kullback leibler divergence  in      among which the number
of words and link density are proved to be the most relevant
ones 
alternatively  when a web page is divided into a tree whose
nodes are visually grouped blocks  spatial and content features
may also be used to detect the non content nodes in this tree as
in      this approach is called vision based page segmentation
 vips  technique and it suffers from high computational cost
to render a page in order to analyze it  there are also some
other approaches like template detection algorithms which
classify the identical parts found in all webpages as noisy
components      apparently  the application of these algorithm
is limited to webpages from the same website  and thus it
would be cumbersome to build models and templates for
different websites or different versions of one website 
in this project  we adopted the definition of blocks in
    and model webpage context extraction as a classification
problem on the block level  for this classification problem  we
have selected three types for features  text features  relative
position and id   class token feature and used svm
as classifier  the blocks classified as content would then be
merged to construct clean webpages  which are compared
with the gold standard  the results of our machine learning
approach is comparable to the performance of the algorithm
in     
iii 

o ur a pproach

our approach is  like many other popular content extraction
methods  text block based  we blockify training documents
and train an svm classifier based on features extracted form
the blocks  for test documents  the same blockify algorithm
is applied and the blocks classified as content are extracted to
construct the main content 
a  blockifying
although many previous works are block based  they do
not document the blockifying process clearly  in our approach 
we use html parser to traverse the html dom tree and
accumulate relevant information such as text and links as
traversing  when the beginning or end of a block level element 
is reached  we output the accumulated text as a text block 



number of words in this block and the quotient to its
previous block



average sentence length in this block and the quotient
to its previous block



text density in this block and the quotient to its
previous block



link density in this block

the definition of features such as number of words and
average sentence length are intuitive and trivial  and the link
density is calculated as the ratio of number of words within all
 a  tags associated with the text block to the total number of
words in the block  as in      we assum that the there are   
characters in a line  and the text density of a block is defined as
the average number of words in one line  which is calculated
as the ratio of the total number of words to the total number
of lines 
the second type of feature is the relative position of a block
in the webpage  to be specific  for a document divided into n
blocks  we discretize their position into m relative positions 
and the relative position of the nth block is
relative pos n    b

the last type of feature is the id class token feature 
in modern html documents  the id and class attributes
capture the semantic information in the html left behind by
programmers     for example  tokens such as ad and nav
usually indicate that the associated elements are non content 
but of course we dont want to manually analyze and collect
such indicative tokens  which will be not only heuristic based
but also non efficient  thus we employed the event model
naive bayes algorithm to learn the top n  which we set to
    id and class tokens that are the surest indicators of
non content blocks    we compute the probability and score of
k th token as follows  assuming there are m blocks  
pm pni
 i 
  xj   k  y  i          
i  
pj  
k y    
m
 i      n    v  
i
i     y
pm pni
k y    

i  

the first type is called text features  which are extracted
based on text properties inside a text block  according to the
evaluation of block features based on their information gain
 kullback leibler divergence  in      for each block we have
selected seven most relevant features 
  block level elements are formatted with a line break before and after the
element  thereby creating a stand alone block of content   note that we also
treat  br  as block level element as it modifies text flow 

pj  
m

i  

b  feature extraction
as we are transforming the content extraction problem
into a classification problem  we wish to use features that are
strongly indicative whether a text block is content or not  in
our approach  we select three types of features  text features 
relative position  and id class token feature 

n
 mc
n

scorek   log

 i 

  xj   k  y  i          
  y  i      ni    v  

k y  
  blocks  with k 
k y  

the first two equations are multinomial event model with
laplace smoothing and the third equation is weighted score
of k th token  we select tokens with    largest scores  and
add    binary features indicating whether a block has the
corresponding token or not 
  with preliminary analysis  we found that indicative tokens for content
blocks vary but for non content blocks are relatively stable  so we use
indicators for non content blocks only 
  a sample list of such tokens from our are  menu  module  nav  widget 
sidebar  footer  right  dropdown  cat  nagivation 

fic  svm training and classification
after feature extraction  we label each block as content or
non content based on whether they appear in the gold standard 
then we employ svm to this binary classification problem
on the block level with eighteen features as decried in the
previous section  following the suggestions in      we first
scale all the attributes value to the same range so that the
features with a higher absolute value wouldnt dominate the
value of the kernel output  since we are using large datasets
and the dimension of our feature set is relatively small  radial
basis function  rbf  would be a good choice as the kernel 
we also tried parameter tuning as described in     but the
improvement is negligible so in our final evaluation we just
use default parameters of c and  
after the svm classifier is obtained  when a test document
is given  we first use the same blockify method to break test
document into blocks and then use the classifier to label the
blocks as content or non content  texts from all the content
blocks are then merged to form the extracted content 
iv 

e valuation

a  datasets
two datasets were used to evaluate our approach  the
first one is the l s dataset  which consists of     manually
classified google news articles from     different web sites
in      from      and the second one is the dragnet dataset 
which consists of      webpages from rss feeds  news
websites and blogs in      from      each dataset have either
manually marked content or manually extracted content  which
can be used as gold standard 
with some inspection  documents from the l s dataset are
generally simple structured and most text are main content 
while documents from the dragnet dataset are more complexstructured with heavy boilerplate elements  some even containing long comments  therefore  both datasets represent
webpage design trend of their respective times  as we aim
to build a generic content extraction tool  a well handling of
both datasets would be highly desirable 
b  performance metrics
the evaluation metrics are based on both the bag of
words and longest common subsequence of gold standards and
extracted contents  specifically  precision  recall and f  scores
are calculated  for evaluation with bag of words  we have the
precision and recall as
p   

 wp  wl  
 wp  

r   

 wp  wl  
 wl  

where wp is the set of words we have classified as content
and wl is the set of words in the gold standard 
for evaluation with longest common subsequence  where
each word in the document is distinct even if two worlds are
lexically the same  this metric is more meaningful as what
users finally see are paragraphs of text instead of discrete

words  let lcs p  l  denote the longest common subsequence between the predicted content and gold standard  we
have the precision and recall as
p   

lcs p  l  length
p length

r   

lcs p  l  length
l length

and the f  score is calculated as
 
f     
 
p   r
we define the metric as token level f  if p   p    r   r   
or lcs f  if p   p    r   r    when evaluation  we use both
token level f  and lcs f  as evaluation metric 
c  experiment design
in evaluation  we randomly chose     documents from
each datasets and run   fold cross validation on it  for each
run we collect evaluation metric described as before and use
the average as the final metric  we apply different feature set
combinations to train the classifier 


text features only



text features   relative position features



text features   relative position features   id class
token features

we use method from     as baseline  which uses combination of features from          and css features and is method
with the best performance we are able to find 
d  results and discussion
the experiment results on the l s dataset is in fig    and
results on the dragnet dataset is in fig     the blue bars are
token level f  and the green bars are lcs f   
firstly  we can see that for both datasets our approach is
comparable to the baseline method  and even outperforms it
a bit in the l s dataset   this is a good indicator that our
approach can accurately extract main content from documents
with diverse characteristics  it is noticeable that on the l s
dataset both f  measures are close to     while on the dragnet
dataset they are around      this aligns with our previous
observation that documents from dragnet datasets are more
recent and complex structured  which makes content extraction
much harder 
secondly  considering different feature sets  we can see that
on the l s dataset  the differences in f  measures using different feature combinations are very small  but on the dragnet
dataset  adding id class token features gives a non trivial
increase in f    this may result from the trend that html
documents contain more semantic id   class tokens in
recent years as semantic html has been promoted  looking
forward  we believe that with html  document become more
semantically rich  the id class token features  or other
equivalent features capturing semantic part of a document  will
play a more important role in content extraction 

filcs f    which is more expensive to compute 
v  c onclusion and f uture w ork

token lcs f  measure on l s dataset
   
   

webpage content extraction is a difficult and interesting
problem to tackle  in this paper  we put this problem into
a machine learning perspective and introduced a supervised
learning approach which can solve this problem to some
degree  with preliminary evaluation using two f  measures 
our method can achieve comparable performance to the stateof the art method 

   
   
   
   
   
   
   
 

text

text pos

text pos class

baseline

fig     results on l s dataset

however  our approach is far from perfect  actually  content
extraction problem is an ai complete problem so there might
never be a perfect solution   for example it may extract more
content than necessary or even miss part of main content  some
techniques shall be added to ensure the completeness of the
extracted content   many previous works assume the continuity
of main content  which may not be      true but is applicable
to most cases 
at this stage  we used a set of features that are reported to have the highest information gain  kullback leiblerdivergence  in      in the future  other feature selection
methods  such as forward backward feature selection  can be
employed to evaluate a larger set of relevant features 

token lcs f  measure on dragnet dataset
   
   
   

moreover  at this moment we only have the data set
from      which has been manually classified to set the gold
standard on the block level  when data set with gold standard
at a larger scale is available to us  instead of classification
problem defined on the block level as in      we would like to
explore the possibility of intra document analysis  we expect
the html dom tree structure and semantics to have potential
contribution to content and non content classification problem 

   
   
   
   
   
   
 

acknowledgement
text

text pos

text pos class

baseline

fig     results on dragnet dataset

we would like to thank the cs    staff and others for
reviewing our poster and offering constructive suggestions  we
greatly appreciate mr  matt peters for kindly offering us the
dragnet dataset for evaluation 
r eferences

thirdly  we also manually inspected extracted content of
some document as we believe what really matters is what
the user actually sees  to our disappointment  although the
metrics are good  some of the extracted content misses some
parts of actual main content  some in the beginning  some
in the end  and others in between  from a realistic setting 
extracting extraneous part is bearable  but missing some critical
content is never acceptable  maybe we can add an evaluation
measure which record whether all text in gold standard appears
in extracted content and apply some technique to ensure that
in the future 
last but not least  lcs f  is consistently lower than tokenlevel f  in each experiment  which is expected as lcs f  is a
more rigorous measure  however  this also reminds us that for
evaluations sake  token level f  can be a reasonable proxy of

   

   
   
   
   
   
   

c  kohlschutter p  fankhauser  and w  nejdl  boilerplate detection using
shallow text features  in proceedings of wsdm     pages         
acm       
t  weninger  w  h  hsu  and j  han  cetr  content extraction via tag
ratios  in proceedings of www     pages          acm       
f  sun  d  song  and l  liao  dom based content extraction via text
density  in sigir  volume     pages               
m  peters  and d  lecocq  content extraction using diverse feature
sets  in proceedings of www     pages             
r  song  h  liu  j  wen  and w  ma  learning block importance models
for web pages  in proceedings of www     pages               
l  chen  s  ye  and x  li  template detection for large scale search
engines  in proceedings of sac     pages                 
c  hsu  c  chang  and c  lin  a practical guide to support vector
classification  technical report  department of computer science and
information engineering  national taiwan university  taipei       

fi