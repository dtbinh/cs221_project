cs     fall        final  project  writeup  
prediction  of  total  daily  incoming  solar  energy  based  on  weather  
models  
team  members   jave  kane  and  julie  herberg    
december             

introduction  

  
our  project  responds  to  a  kaggle  challenge  posted  by  earthrisk  technologies        to  
discover  which  statistical  and  machine  learning  techniques  provide  the  best  short  
term   predictions   of   solar   energy   production    renewable   energy   sources   such   as  
solar   and   wind   offer   environmental   advantages   over   fossil   fuels   for   generating  
electricity    but   due   to   the   temporal   variability   of   these   sources    utilities   must   be  
prepared   to   make   up   deficits   with   power   from   traditional   fossil   fuel   plants   or  
purchases   of   power   from   neighboring   utilities    maintaining   the   correct   mix   requires  
detailed   accurate  short term  predictions  of  wind  patterns  and  solar  illumination   
  

figure      left   gefs  weather  forecast  grid  sites   blue  dots   and  oklahoma  mesonet  stations  where  solar  
radiation  measurements  are  made   red  dots    right   plot  of  training  set  output     black   at  one  mesonet  
station   and  input  feature       downward  long wave  radiative  flux  average  at  the  surface   red    at  
nearest  gef  site  at  one  time  of  day   full  time  domain  of  the  training  set  is      years        years  are  shown    

contestants   are   asked   to   predict   the   total   daily   incoming   solar   energy   at       
oklahoma   mesonet     mesoscale       network    environmental   monitoring   stations  
       which   serve   as   solar   farms   for   the   contest    every   five   minutes   these   stations  
record   mesoscale   weather   observations    including   incoming   solar   radiation      the  
output   for   the   kaggle   challenge    the   features   are   weather   predictions   from   the  
noaa esrl  global  ensemble  forecast  system   gefs           our  goal  for  this  project  
is   to   set   up   a   framework   for   this   type   of   research   and   identify   relevant   machine  
learning   tools    the   winning   kaggle   approach   was   made   public   nov                  we  
discovered  this  only  within  the  last  week    and  turned  out  to  be  similar  to  ours   

  

fidata  

  
the  training  features  come  from  an  ensemble  of      weather  forecasts   one  
unperturbed   and      perturbed  using  various  instrument  uncertainties   for  five    
hour  intervals  during  each  of  the        days  from  january            through  
december             inclusive       full  years    at      x     gef  sites   a  grid  of      
longitudes  and     latitudes   figure      left  panel    there  are  fifteen  forecast  features  
 see       for  a  full  list    including   for  example        downward  long wave  radiative  
flux  average  at  the  surface   w  m           air  pressure  at  mean  sea  level   pa         total  
cloud  cover  over  the  entire  depth  of  the  atmosphere   and       maximum  
temperature  over  the  past     hours  at     m  above  the  ground   k    training  output  is  
the  total  daily  incoming  solar  energy  in   j  m      integrated  over  each     hour  day  at  
the      mesonet  sites   the  right  panel  of  figure     plots  the  output  for  one  mesonet  
station  over       years   and  one  feature  from  the  nearest  gef  site   gef  test  features  
are  provided  for  each  of  the        days  from  jan            through  dec                  
years    test  output  is  held  privately  on  the  kaggle  website   model  predictions  of  the  
output  can  be  submitted  to  the  website   which  returns       a  total  error  on  the  entire  
output   and       the  ranking  of  the  model  output  on  the  leader  board        there  were  
     contestants  by  the  end  of  the  competition   which  closed  nov                 

models  

  
we  used       linear  regression  via  the  normal  equations        minimum  absolute  
error   mae   using  matlab  fminsearch   finds  minimum  of  unconstrained  
multivariable  function  using  derivative free  method         with  parameters  
initialized  from  linear  regression   and       gradient  boosted  regression   gbr    a  
predictive  model  typically  used  in  models  associated  with  decision  tree  learning  
 dtl    dtl  utilizes  a  stepwise  approach  to  evaluate  a  large  dataset  and  evaluates  
the  results  at  different  nodes     likewise   gbr  builds  a  prediction  model  with  the  
ensemble  of  weak  prediction  models   such  as  decision  trees         we  black  boxed  
gbr  in  the  sense  that  it  was  not  clear  to  us  how  to  characterize  an  overall  error  
function  that  gbr  is  minimizing    we  simply  evaluated  the  mae  of  the  gbr  outputs    
we  also  tested  k    linear  regression   but  consistently  encountered  poorly  
conditioned  feature  matrices   so  we  did  not  pursue  this   for  a  given  model  family  we  
generated  one  set  of  parameters   or  gbr  model   per  mesonet  station   i e   we  did  not  
consider  possible  correlations  between  stations   
  
for  preliminary  model  assessments   we  performed  holdout  cross  validation  and  
k    fold  cross  validation   we  also  considered  options  for  feature  aggregation  
including   using  only  the  nearest  gef  site  to  a  mesonet  station   the  mean  or  
distance weighted  mean  of  the  features  at  nearest  four  gef  sites   and  all  features  at  
those  four  sites   we  considered  aggregating  the  five  daily  gef  measurements   using  
either  the  middle  one   or  averaging  over  all  five   we  wound  up  mostly  using  all  five  
three hour  measurements  as  separate  features   

fieven  for  k    linear  regression   some  matrices  at  some  mesonet  stations  were  poorly  
conditioned   apparently  due  to   surprising   near dependence  of  two  features        
air  pressure  and       specific  humidity    for  simplicity  we  preempted  this  issue  by  
using  matrix  pseudo inverse  in  the  normal  equations   a  preliminary  look  at  the  
features  using  principal  component  analysis   pca   suggested  at  least      of      
components  are  needed  to  cover       of  the  variance   to  make  progress   we  mostly  
used  only  the  unperturbed  forecast  of  the  gef  ensemble   for  some  kaggle  
submissions  we  averaged  outputs  over  all      forecasts   we  investigated  subtracting  
the  annual  fourier  component   see  figure      from  the  data   and  in  the  process  we  
discovered  further  structure  that  might  be  analyzed  by  e g   wavelets   not  shown    
ultimately  we  simply  included  time of year  as  an  added  feature   for  simplicity  we  
did  not  include  gef  site  elevation   latitude  or  longitude  as  features     
  
we  used  gradient  boosted  regression   gbr   code  from  the  open  matlab  forum  
      with  simply  the  default  exponential  loss  function   the  gbr  training  step  was  
very  slow   therefore  it  was  difficult  to  use  cross validation  to  choose  the  number  of  
trees   number  of  leaves   and  learning  constant   a  series  of  models  at  single  mesonet  
stations  suggested  that  using       trees  and     leaves  gave  good  errors  without  
obvious  overfitting   when  averaging  the  four  nearest  gef  sites   a  learning  constant  
  of         i e       of  information  discarded   appeared  optimal   whereas  when  using  
all  features  at  the  four  nearest  gef  sites             seemed  better   speculatively   this  
could  be  due  to  correlation  between  the  features  at  the     gef  sites   i e   possibly  the  
algorithm  works  better  when  discarding  redundant  information  more  quickly   

results  and  discussion  
  
as  figure     shows   pca  suggests  at  least      fairly  distinct  components   since  the  
spread  between  models   and  kaggle  scores   was  not  large   a  few  percent  of  error  
mattered   so  we  did  not  pursue  pca  further     figure     shows  mae  at  a  single  
mesonet  station  for  selected  models   using  only  the  unperturbed  forecast  from  the  
gef  ensemble   figure     show  errors  for  selected  holdout  cross validation  and  k    
fold  cross validation  errors  for  models  at  all  mesonet  stations  using  the  
unperturbed  gef  forecasts   the  figure  shows  that  using  the  four  nearest  gef  sites   
instead  of  the  nearest  site   decreased  all  errors   
  

  
figure     pca  gef  features  lat      lon        right   cumulative  latency   left   features  in  pc  space   

  

fi    model  
    linear  regression   nearest  gef  site   
middle  time  
    linear  regression   nearest  gef  site   
middle  time  of  day   plus  time  of  year  
    linear  regression   mean  of     nearest  
gefs  sites   middle  time  of  day   time  
of  year  
    linear  regression      nearest  gefs  
sites   middle  time  of  day   time  of  
year  
    mae fminsearch  on  output  of      
    gbr      trees      leaves               
nearest  gef  sites   all  times  of  day   all  
features   time  of  year  

mae  
 mj  m     
        
      
      
      
      
      

  
figure     predictions  of  selected  models  run  with  the  unperturbed  gef  forecast   ensemble  forecast  
        left  side   mae   right  side   model  minus  output  for  a       years  of  the  training  time  domain   

  

  
figure     cross  validation  generalization  error  for  selected  models  run  with  unperturbed  gef  forecast   
left  side   holdout  cross  validation   gj  m      right  side   k    fold  cross  validation   different  scaling    

these  error  assessments  illustrate  how  a  series  of  changes  to  the  feature  set  
incrementally  improved  the  predictions   based  on  these  results   and  on  focused  
evaluations  using  the  complete     forecast  gef  ensemble   not  shown    we  chose  a  
set  of  models  that  were  feasible  to  train   and  ran  them  on  the  full  set  of      mesonet  
sites  using  the  gef  test  features   we  submitted  the  outputs  to  kaggle  for  scoring  
against  the   hidden   test  output   figure     shows  some  of  our  kaggle  errors  and  
rankings   for  model       the  full  gef  forecast  ensemble  was  used   for  all  others   only  
the  unperturbed  forecast  in  the  gef  ensemble  was  used   notably   gbr  performs  
very  differently  when  we  change  the  learning  constant  from        to        with  the  
number  of  trees  and  number  of  leaves  fixed    it  is  not  obvious  to  us  why  this  change  
made  such  a  large  difference   we  note  that  a  large  improvement  in  rank   if  not  
error   was  achieved  by  running  mae fminsearch  on  the  output  of  linear  regression   

  

fi   

description  of  our  model    

    mae fminsearch  on  output  of  linear  
regression   averaged  over  gef  ensemble  
    gbr      trees      leaves             nearest  gef  
sites   all  features   time of year  
    mae fminsearch  on  output  of  linear  
regression         
    linear  regression  on  nearest  gef  site  
    gbr      trees      leaves              nearest  gef  
sites   all  features   time of year  

kaggle  rank  
out  of       

kaggle  error  
 mj  m     

kaggle  error     
winning  error  

    

      

      

    

      

      

    
     

      
      

      
      

     

      

      

  
figure     kaggle  rank  and  error  for  selected  models  

conclusions  
  
we  addressed  a  real world  problem  that  is  significant  in  its  own  right   but  the  data  
and  methods  are  of  broader  interest   for  example  to  airborne  hyperspectral  sensing  
       the  data  was  right sized   allowing  both  decent  statistics  and  evaluation  of  a  
range  of  models   feature  selection  and  aggregation  required  some  thought   as  
physicists   we  spent  less  time  thinking  about  the  content  of  the  features  than  we  
expected  to   since  the  machine  learning  models  accepted  most  sets  of  features  
without  issue   our  basic  skills  from  cs     took  us  within       of  the  winning  kaggle  
score   however   the  winning  approach  was  very  similar  to  ours   suggesting  the  extra  
effort  to  reach      follows  an         rule   it  appears  using  gbr  requires  some  
experience   efficiently  tuning  and  using  gbr  probably  requires  parallelization   this  
is  difficult  with  the  matlab  license  structure  on  the  machines  we  could  easily  use   
gbr  should  probably  be  run  outside  matlab    the  kaggle  winner  used  r    

references  
  

     kaggle  website  http   www kaggle com c ams      solar energy prediction contest  
     mesonet  website             http   www mesonet org   
     u s   department  of  commerce   national  oceanic     atmospheric  administration  website  
           http   esrl noaa gov psd forecasts reforecast    
     kaggle  website    data             http   www kaggle com c ams      solar energy 
prediction contest data  
     kaggle  website    leadership  board           http   www kaggle com c ams      solar energy 
prediction contest leaderboard  
     lagarias   j c    et   al    siam  journal  of  optimization   vol      number      pp                    
     friedman   j   h    greedy  function  approximation   a  gradient  boosting  machine     february          
     friedman   j   h    stochastic  gradient  boosting     march          
     hara   kota   boosted  binary  regression  trees       august          
http   www mathworks com matlabcentral fileexchange       boosted binary regression trees  
      hyperspectral  sensing   http   www csr utexas edu projects rs hrs hyper html  

fi