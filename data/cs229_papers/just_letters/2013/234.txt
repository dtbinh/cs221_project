automated essay scoring using machine learning

shihui song
jason zhao

shihui stanford edu
jlzhao stanford edu

abstract
we built an automated essay scoring system to score approximately        essay from
an online machine learning competition on
kaggle com  there are   different essay topics and as such  the essays were divided into
  sets which differed significantly in their responses to the our features and evaluation 
our focus for this essay grading was the style
of the essay  to which we extended by adding
the category of maturity  we evaluated linear regression  regression tree  linear discriminant analysis  and support vector machines on our features and discovered that
svm achieved the best results with an average         

   introduction
the automated essay scoring model is a topic of interest in both linguistics and machine learning  the
model systematically classifies the quality of writing
and can be applied in both academia and large industrial organizations to improve operational efficiency 
     motivation
each year  thousands of students take standardized
tests with the same essay topics  hand grading these
essays is tedious and subjective  instead  many organizations have already turned to automated essay grading to improve consistency and efficiency  accurate
models will not only reduce the amount of human error variance in essay grading but could also save school
boards and teachers many precious hours that could
be used to improve the educational system 

   data set
the training and test data were acquired from a past
competition from kaggle com   sponsored by hewittpackard  we had approximately        number of essays ranging from         words each provided for us 
we split the essays into a       training and validation scheme  which results in a size of       essays for
the training set and       essay for the test set  this
divides further into around       training essays and
    test sets per essay set 

   feature generation
python was utilized for the pre processing of the data
into matrices that were then fed to matlab for supervised learning  the parts of speech tagging from the
natural language toolkit  nltk  was the sole library
used for the assignment  most of these style categories
are based of a recent acl paper classifying goodness of
scientific new york times articles  louis   nenkova 
      
there were five categories of features that were considered and generated for this project for style 
     its visual nature
a descriptive sentence awes the reader and prompts
their imagination  the source of imagery and meaningfulness used for this data set is derived from the
british natural corpus where each word has a imagery score between   and      kilgraff         the
features derived from this set include the proportion
of words that are visual  proportion of unique visual
words  average imagery scores for those words  the average imagery score for the essay  and all of the above
for every third of the essay by dividing the essay into
an introduction  a middle  and a conclusion 
     inclusion of pronouns
just as scientific articles which explicitly reference
people or experiments would most likely be more re 

cs   n final project  shihui song  jason zhao

 

http   www kaggle com c asap aes

fiautomated essay scoring

spectable and concrete  we imagined that similar essays in our data set with external reference would score
better  the kaggle essays already contained name entity tags from the name entity recognizer from the
stanford nlp group  name entities for person 
organization  and location were categorized
as proper pronouns  there were also counts of personal
pronouns such as he  myself  etc  and the last pronoun count was for relative pronouns  which were noun
phrases  tagged by the python nltk tagger  followed
by who  which  and where 
we then divided all people pronouns into animate and
organizations and location into inanimate under the
assumption that the use of people would be more engaging than locations and organizations 
     its beautiful words
beautiful word choices are thought to increase an essays elegance  thereby its score  only words above  
characters in length are considered beautiful for this
project  two factors are considered for individual
word beauty 
 high perplexity letter model   how unlikely
the combination of characters in the word is  for a
word  find the product of its character frequencies
 cornell math cryptography         the lower
the product the more complex the word 
 high perplexity phoneme model   we created
a   gram for syllables to determine phoneme frequency  cmu         for every word in the essay 
check to see if there exists a pronunciation for it 
and if so  then also find the likeliness of its   gram
combination 
the ultimate features for this category were the average letter and phoneme frequencies per beautiful word
and per essay  as well as the top         and    average phoneme and letter frequencies where the top is
the lowest frequency 
     its emotive effectiveness
a very dry and emotionless essay is not powerful  the
subjectivity lexicon from mpqa provides a list of
words and their sentiments  positive  negative  neutral  or both  and the strength of those sentiments
 mpqa         the resulting features are proportions of sentiments and strength individually and combined for the entire essay or for given sentiments and
strength  in addition  we also calculated the proportions of different emotions to one another 

in the end however  we realized that exhausting every
combination of sentiment and strength proportional to
another was actually noise that hurt the kappa score 
therefore  we removed all proportions of sentiment
and strength to other sentiments and strengths 
     its maturity
our vocabulary expands as we grow older  therefore 
in a sense  as we mature  so does our vocabulary  this
is particularly poignant for this set of essay  as they
are written by adolescent students 
although this isnt included style  we thought this
would be a good additional category  the age of acquisition is the average age when a person learns the
certain word  kuperman         for every essay  we
found its average maturity  top mature tokens  and
vocabulary maturity 

   learning algorithms
for the project  we evaluated several different classes
of learning algorithms which will be described below 
most of the algorithms we evaluated are regression
based where we treat the essay scores as a range of
values and predict a floating point value within that
range 
     linear regression
we used a simple linear regression model implemented
by the statistics packet in matlab  the mathworks 
       the linearmodel class fits a linear function
h  x    t x   c to a design matrix x in order to
minimize the least square error as discussed in class 
     svm
we used the  svm algorithm presented by  scholkopf
et al          it is a regression svm algorithm based
on the  svm which introduces the i slack variables
for capturing error  vapnik         specifically  the
 svm attempts to solve the following problem 
 
l
 x
 
 

  
 i   i  
min   w           w     c   
 
l i  
s t    w  xi     b   yi    i
yi    w  xi     b      i
  

i

       

we chose the  svm because it reparameterizes the
loss sensitivity term  in the tradition c svm  this
is desirable because  is very hard to tune in practice whereas  is simply an upper bound between the

fiautomated essay scoring

training error and the number of support vectors 
     multiclass linear discriminate analysis
we used matlabs classificationdiscriminant class
to train a multi class linear discriminant classifier the mathworks         the classifier predicts
new examples using the following rule 
y   arg min

k
x

y      k

p  k x c y k 

k  

where k is the number of classes  p  k x  is the posterior probability of k given x and c y k  is the cost
of classifying y when true class is k  we choose to use
the default cost matrix for the milestone but may investigate other cost matrices for the final report  the
posterior probability is estimated using a multivariate
gaussian distribution where the mean k and covariance matrix k are approximated from the training
set 


 
 
t  
exp

 x


 

 x


 
p  x k   
k
k
k
 
  k     

the weight is the squared difference of the i and j
scores over the square of the size of the set minus one 
e is just a calculation based on the users given vector
based on score frequency  and oi j is the number of
occurrences when score i is assigned by grader one and
score j is assigned by grader    kaggle com       
     algorithm performance
we evaluated the different algorithms using essay set   
for the initial feature vector  we used the words that
appear more than   times across all essays because
certain algorithms such as linear regression and linear discriminate analysis cannot handle matrices with
      columns  for plotting the learning curve  we
measured the kappa error as we increased the training set size 

     regression trees
matlab also supports a binary splitting decision tree
that can fit to some response variable y   this is an
interesting algorithm because it uses a recursive partitioning model to divide the feature space into simple buckets  we used the mean squared error as the
splitting criterion with a minimum leaf size of a single
observation  the tree is post pruned to generate the
optimal sequence of subtrees  the resulting geometric interpretation is that the feature space is split into
linear boxes  since this is a binary regression tree  
therefore the end result similar to an unsupervised
clustering algorithm but the training phase is drastically different 

   experimental results and algorithm
selection
     error rates
the measure of error rate utilized is the quadratic
weighted kappa  the quadratic weighted kappa measures the agreement between the automated essay
grader and the human scores  scores typically range
between    random agreement  and    total agreement   although scores less than   can occur when
theres less agreement than random 
p

i j
   p

i j

wi j oi j
wi j ei j

figure    the kappa error for each different learning algorithm over set   

as shown in figure    svm performed the best over
essay set   with naive bayes following in second  for
these   algorithms  it seems training error decreases as
training set size increases which is desirable  for all of
our subsequence evaluations  we decided to focus only
on the svm 

   evaluation and analysis
below is the kappa score comparison for the feature
categories of style and their scores for each essay set 
please note that essay set   and   are excluded because essay set   asked for two separate ratings  which
were furthermore dependent on specific trait rubrics 
essay set   was also not included due to the matlab
machine learning algorithms becoming rank deficient
during the process and we felt that it was not a whole 

fiautomated essay scoring

some measurement 
     individual features

stricted by a necessary rubric to follow 
when reviewing the categories as a whole  we discovered that the prompt of the essay has a huge affect on
the score of the categories  essays that need to address a certain topic or make a point  does not need to
have style to score high  other prompts with freedom
of expression often has style factored in to the final
score  thus  our style score depended on the prompts 
     algorithm tuning
in order to achieve the lowest training error  we performed a    fold cross validation over essay set   in
order to find the best values for c and   the plots
generated below shows the effect of these   variables
for the kappa score 

figure    individual style category and their kappa scores
across each essay set

of the various individual feature sets in the graphs
above  imagery has consistently the highest kappa
score  beauty and maturity are both respectable while
subjective is dismal  the worse is pronouns  which
does as worse as random at time  these results were
not on par with our original assumptions 
pronoun was a complete disappointment  leading to
our conclusion that in these adolescent essays  references to entities does not matter 
we did not have too much hope for beauty  as the
phonemes and character frequency collected by us were
not thought to be as great statistically as those given
by the other feature categories  however  we were
pleasantly surprised with the results  upon closer
analysis  we realized that often  beauty scores are often synonymous to word frequency scores  which we
had already proven to be a good metric 

figure    effect of the parameter c on svm prediction accuracy 

subjectivity scores depends entirely on the prompt 
essay set   asks the reader to describe how the setting affects the character  thereby asking for emotions 
when we delved into the results further  in many of
the persuasive essay sets  negative essays scored better
in general than positive ones  this leads us to believe
that criticism is more highly regarded than praise 
the maturity scores  aoa  were also delighted to have
maturity be such a good indicator  the prompts with
more freedom such as narrative essays in essay sets
  and   achieved higher kappa scores with maturity 
this could very well be caused by the range that students are allowed to express themselves in  not re 

figure    effect of the parameter nu on svm prediction
accuracy 

fiautomated essay scoring

this shows that we perform better with a c value of
around     and a  value of around       this means
we penalize incorrect entries greatly and would like to
approximate a hard margin svm as closely as possible 
we also experimented with various kernel functions
and found that a radial basis function performed the
best for essay prediction 
     final result
we used our optimal training parameters to classify
the essays in each of the sets  our results are shown
in the table below 
essay set
 
 
 
 
 
 

kappa score
      
      
     
      
      
      

url http   www speech cs cmu edu cgi bin 
cmudict 
cornell math cryptography  group 
english letter frequency       
url http 
  www math cornell edu  mec           
cryptography subs frequencies html 
kaggle com 
the hewlett foundation 
atumated essay scoring   evaluation  february
      url http   www kaggle com c asap aes 
details evaluation 
kilgraff  adam  bnc database and word frequency
lists        url http   www kilgarriff co uk 
bnc readme html 
kuperman  v  age of acquisition ratings for       
english words 
behavior research methods 
      url http   link springer com article 
         fs                 fulltext html 

   conclusion and future work
while our project has had certain success in this domain of automated essay grading for adolescents  it is
not without pitfalls  judging by style itself is not a
good indication of the paper addressing the prompt 
to truly judge an essay we must understand at an artificially intelligent level both the prompt and the essay 
however  this project takes strides in what we believe
to be understanding the style of adolescents  our inclusion of maturity has proved to be a dependable category feature to judge by  an addition that could also
perform very well in any other essay grading 
this project has also followed closely on the features
of the style according to the louis and nenkova paper 
but perhaps in the future  more explorations of what
composes of style would be a good venture  as an
extension  it would be worthwhile to explore not only
the style by token content  as this project has mainly
done so so far   but also on the structural and syntactical style 

louis  a  and nenkova  a  what makes writing
great  first experiments on article quality prediction in the science journalism domain  transactions of the association of computational linguistics        url paperhttp   www transacl org 
wp content uploads         paper    pdf 
mpqa  subjectivity lexicon        url http 
  mpqa cs pitt edu lexicons subj lexicon  
scholkopf  bernhard  smola  alex j   williamson 
robert c   and bartlett  peter l  new support
vector algorithms  neural comput             
      may       issn            doi          
                    url http   dx doi org 
                           
the mathworks  inc  supervised learning       
url http   www mathworks com help stats 
supervised learning html 
vapnik  vladimir n  the nature of statistical learning theory  springer verlag new york  inc   new
york  ny  usa        isbn               

references
automated
essay
grading
using
machine learning 
     
url http 
  cs    stanford edu proj     
mahanajohnsapte automatedessaygradingusingmachinelearning 
pdf 
cmu 

cmu pronounciation dictionary 

     

fi