context based re ranking of web documents
 crewd 
arijit banerjee  jagadish venkatraman
graduate students  department of computer science 
stanford university
 arijitb stanford edu  jagadish stanford edu 

abstract  in this paper  we introduce crewd  a machine
learning system for context based re ranking of web
documents  crewd tackles personalization by considering both
the short term and long term history of a user when
personalizing urls to them  crewd proposes metrics that
could be used as features for re ranking search results by future
ir systems as well as systems that do rule based re ranking 
crewd introduces a novel user interest model and an algorithm
for candidate selection to generate a list of users who could be
potentially similar with this user  we evaluate the performance
of crewd with respect to an unranked baseline and show that
crewd significantly out performs it and improves the
normalized discounted cumulative gain  ndcg  of the search
engine result pages displayed to the user 
furthermore in crewd  we provide support for combining
several approaches to ranking search results borrowed from
machine learning  information retrieval theory and evaluation
into a single unified framework  crewd approaches the
problem of re ranking as several independent modules each
generating their own ranks for the urls and then proceeds to do
rank aggregation on each of them  we envision crewd as a
system that can be used to rapidly prototype combine and
evaluate several such approaches to re ranking of search engine
results 

i  introduction
users query search engines for obtaining results to their
queries  each user has his own information needs  there is a
growing need to understand user search behavior  an
understanding on the same will enable us build better userinterfaces  for example  for a query like machine learning
assignment  we want a student from stanford to be shown the
cs    web page first in his list of re ranked results  an
analyst at an investment bank might want to look up the stock
quote of facebook instead of the facebook home page for the
query on facebook  one global re ranking algorithm would
not satisfy the information needs of every user           one
limitation of conventional information retrieval models is that
the retrieval decision is based solely on the query and the
document corpus rather than an integrated approach that takes
user context into account      a single query gives the ir
system only very limited information  or almost no
information in some cases  about a user  hence  one must use
contextual data to enhance retrieval accuracy  context 

sensitive ir is among the major challenges in todays ir
research    
earlier models of ir  as those in      were primarily based on
explicit user feedback  they relied on the users to provide
feedback on whether a search result was relevant to them and
specify in advance the domains in which they were interested 
not all users are enthusiastic enough to provide feedback  so 
the accuracy of such models is limited  hence  implicit
feedback  relying on the users previous history and clickthrough data as a measure of how effective the ir system
performed  has been studied exhaustively in      previous
works have actively explored several sub fields on this
research area  log analysis  probabilistic query expansion     
scaling personalization in search      these primarily apply
feedback to search results and not to re ranking  which
motivates crewd 
the remainder of this paper is structured as follows  section ii
introduces the problem that crewd tries to address and why
it is relevant or challenging  we then proceed to enumerate the
key contributions of this paper  the subsequent sections
address each of our contributions  a user model  a similaritydetection algorithm  and combining several approaches to reranking  we also then demonstrate our preliminary results on
running crewd and re ranking on a dataset of   million users
of a search engine 
ii  crewd
crewd  context based reranker for web documents 
addresses the problem of re ranking an already ranked search
result  crewd relies on implicit feedback and proposes
metrics which could be used by ir systems when re ranking
their results  while a lot of work has been done on implicit
feedback  those in              not much has been done in
unifying several approaches to re ranking and building a
system that allows developers to effectively combine several
of their re ranking algorithms coherently  we implement   of
our proposed models in crewd and combine ranks across all
of them  crewd is addressing a challenging problem in ir as
our dataset does not have the actual words or urls
themselves but only the ids for words and urls  so  applying
nlp based methods for query refinement and processing 
stemming  using knowledge bases or ontologies to detect

fisurrounding context  domains  re querying or doing query
expansion by querying the actual web pages are not possible 
crewd takes as input a user query  as a collection of term
ids without any information on the actual terms themselves 
and an already ranked list of url ids  we assume the
underlying search engine has already applied several features
when ranking the urls  including page rank and query
analysis  the problem is  given this list of ranked url ids
from a search engine  to apply the current users previous
history and the activity of all other users to re rank these
urls 
the key contributions of this work are the following   the
crewd model considers both short term history   in the form
of the current session of the user  and long term history  in the
form of activity by the user over a longer period of time and
the interaction of similar users   the metrics of relevance
proposed in our model have broad applicability across several
re ranking systems  section iii a   crewd proposes a novel
algorithm for discovering users that are similar to a user
without exhaustively searching all candidate users  section
iii b   finally  crewd allows combining of several
algorithms for re ranking and generates an optimal re ranked
list from all of those results  section iii c 

for a session
for a url
as the maximum dwell
time of the user on the url in the session over all records
corresponding to that session  the notion of
is useful when we want to model users who click the same
link more than once in a single session 
 

 

where r is the set of all log records in session where user
clicked on
 
the above formalism of a
 
 
allows us to associate a per session per url relevance score
for every user  the crewd model of relevance captures  
relevance scores     in which case the
was irrelevant
to   a relevance score of   which implies that
was
irrelevant to
and a relevance score of   implies that
was highly relevant to
  the values of relscores are
assigned based on the
metric we defined
before  the choice of these values of maxdwelltimes were
motivated by the evaluator at kaggle which we use for our
performance evaluation 
 

 

 

 

 

 

 

 

 

iii a  a model for re ranking
dwell time based personalization is currently accepted as the
state of the art in information retrieval research      cikm        informally dwell time is the amount of time the user
spent on a clicked search engine result page  the dataset we
use  obtained from an ongoing contest on kaggle  contains
time stamps at which each individual query was made  the
results shown to the user and the user clicks 
a search engine log record

  is of this form 

the
 
  is set to   if
was the last
url that user
visited in session
we argue that this
assumption is reasonable  since we dont have another record
in the session to record the user activity  we assume that the
user stayed for a long time in the last link that he clicked in his
session 
let
follows 

be an indicator random variable defined as
 

where

is the logical time stamp  a monotonically
increasing counter  when the event was recorded 
is the
user for which the event happened 
is the event meta data
descriptor that identifies the type of the event  it is set to in
case the event is a query event and to
if the event is a click
event  a
event also contains an event descriptor that
describes
the
terms
in
the
query
and a ranked list of
urls that the search engine displayed to the user 
where
is
the domain of
  if the event meta data descriptor is set
to   it means that the event is the click event and it specifies
the
pair the user clicked on  if the event
after the c event occurred at time epoch 
  then the
dwell time of the record
calculated as
we define our notion of
of a user

also define
follows 

to be the indicator random variable as

 
note that

captures only the recent urls shown to

user
armed with the above definitions we define a cumulative
relevance score of a given url for a particular user as 

 

 

 

 


the cumulative relevance measure defined above captures
how important an url
is to user
across all sessions

fifurther  let us assume that users are clustered into various
groups  the exact details of clustering and our notion of
similarity of two users follows in section iii b 
let u  
  be the set of all users that are related
to user
our formulation of the cumulativerelevance
measure allows us to derive an expression for the global
relevance of a url 
with respect to the  u  other users
in his cluster 


 

 



our model expresses the globalrelevance of a user as a
weighted sum of the cumulativerel scores of the related
individual users in the cluster  the weights are used to denote
how similar users
are  the globalrelevance function
introduced above models the users own global search history
and the history of other related users in the same cluster as  
we now  give an expression for the local relevance of a user
for a url  since our definition of
 
 
captures the relevance of a url w r t the user for a session  it
can effectively be used to track the local relevance of a url
for a user for a session 
 

 

the expressions defined above help us to formally express our
totalrelevance as a weighted combination of the local and
global relevance scores 

where the parameter represents the weight applied to the
local relevance  ie  the immediate session history for the user 
and the global relevance  that captures interactions of the user
and all the related users in his cluster   for each url in the
unranked set of urls  we compute this totalrelevance score
for the current session  we then order the original list of urls
from the search engine by this totalrelevance score  the
resulting permutation of the urls is considered to be the reranked list of urls generated by this step 
the power of crewd lies in its ability to combine multiple
algorithms  for example  we propose another algorithm in
section iii c for learning the totalrelevance function and
how crewd combines the permutations generated by both
the algorithms   this feature in crewd helps other
developers to test or combine several algorithms and measure
the effectiveness of the ranking 
iii b  computing similarity across users
in this section we present our implementation of similarity
based clustering across various users and how users are

grouped into clusters  we also propose an optimization
involving candidate selection to speed up the clustering  
computing similarity between two items represented as
feature vectors is a widely researched area in statistics  for our
notion of similarity we use the pearsons coefficient  which is
a correlation based approach 
user interest modeling
crewd models every user as a vector of cumulative
relevance scores of the urls they have clicked on  we
compute the cumulativerelevance vector  for each user u
and use this as an efficient representation of u  we formalize
our definition of
as 
 
since crewd is engineered to be a web scale system that
processes millions of urls  users and clicks and that each
user could have only clicked a few of the millions of urls 
is implemented as a sparse matrix  now  we can compute the
correlation between users u and v with vectors
and
with means
respectively as follows 
 




 

 


 

conventional algorithms like correlation based clustering
compute the similarity score across each pair of users u and v 
however  at the scale at which crewd is engineered to
operate  such an approach with o 
time complexity and
o 
space complexity clearly would take several days to
cluster users  we propose a faster algorithm for candidate
selection to find similar users 
a key realization to speeding up clustering by an order of
magnitude is to realize that two users
are potential
members of the same cluster c if and only if there exists some
url
such that
 
     and
 
      this indicates that both
found
relevant  the existence of one such a
indicates that
and uj could be potential candidates
for computing similarity 
for each url 
  crewd maintains an index of a list of
users who have clicked on the url  this approach is scalable
as since each click event is completely independent  this index
could be computed in parallel by a pipeline of map reduce
jobs   having obtained the index of a list of users who have
clicked on each url  crewd scans each url in the list  let
s  
  be the list of users who have clicked
on a url 
  let of each user be the set of candidate
users similar to   for each pair of users
in s  add
to the candidate set of
since in practice  s is expected to be
small  we expect this approach to perform better  our
algorithm is efficient in time as only  
users are examined

fifor potentially similar candidates instead of running an
exhaustive search over all the n users 



for each user 
crewd computes the top k users who
have them highest similarity measure with
by only
examining   potential candidates  thus  we derive the list of
users similar to  



a further optimization can be made here  some links are
visited by a lot of users and on a daily basis  since  a lot of
people visit such links  these urls do not contribute any
value to the clustering  hence  we neglect those urls having
more than a threshold number of users visiting them 
iii c  learning a url relevance function
for performing reranking
in this section  we describe a supervised learning based
approach to learn a relevance function for a  url  user  pair 
the next section combines the ranks generated from this
approach and the ranks generated from iii a  using rank
aggregation 
we cast ranking as a supervised learning problem of learning
a relevance function of a url u  as a weighted sum of the
individual feature values using a classifier 

average dwell time of all similar users on the
url  the above two capture a global similarity
across related users 
url clicked on the same session  session level
feature to model user interaction   also captures
immediate local similarity 

here similar users refers to the list of users who are in the
same cluster as u as determined from the previous step  we
performed supervised learning on our dataset to learn whether
a document was relevant or not given the above features  the
coefficients of the hyperplane resulting for classification gives
us the individual weights of the features which can then be
used to calculate totalrelscore 
the training data was generated by scanning through the
search engine logs and extracting the above features  urls
were given a classification label of either relevant or
irrelevant based on the dwell time of the user on that url in a
particular session  urls not clicked on during the session
were considered irrelevant 
c was performed using the scikit learn library provided in
python  we used a linear svm performing l  regularization
for classification  we mention our results in a later section 
iii d combining several rank permutations

totalrelscore url u 

 



feature selection
crewd uses a mix of dwell time information  number of
clicks at a session level and cluster level  note that we cluster
user from our algorithm in section iii b  as features  we list
the features and an intuition for having each of them 








  clicks by user u on the url  models a click
event on the same url before  a common use case
is people forget the full url of a site and instead 
frame a query to a search engine to get a serp and
clicking on their desired url  for example  a cs   
student visits the assignments page multiple times
and might use a search engine to find it 
  clicks by user u on the domain  captures if a user
had clicked on the same domain before  a common
use case is a stock analyst who is interested in
finance yahoo com facebook
and
not
in
facebook com  when he searches for facebook 
similar domains visited before are an indication that
they should be ranked higher in the search result 
average dwell time of the user u on the domain 
this feature models the time that the user usually
spends on the domain of the url  we want urls
with higher dwell times to be ranked higher 
average dwell time of all similar users on the
url domain 

in this section  we describe how several permutations
generated in previous steps  iii a and iii c  can be combined
together  rank aggregation is a method of combining several
rank outputs resulting from previous stages  the idea of rank
aggregation is that we can consider the output from each
independent re ranking algorithm as a signal and output a
permutation closest to all these permutations under some
metric  crewd implements a simple rank aggregation
algorithm called the borda ranking method 
assume that there are n ranking algorithms each with their
own permutation of urls each of the permutation of size k 
let s   
   s  k  be the unranked
permutation and let
  be the n permutations of s 
we update rank score of an url per permutation
as
follows  if an url appears in rank r in any permutation then
we set rank score url  to k r 
similarly  we estimate scores of each of the urls  we then
order the urls by the decreasing order of their scores and
generate a new permutation  the generated permutation gives
us an ordering which is the combined rank of the n
individual permutations 
evaluation and results
below are the feature weights learned by our linear classifier
after training on    million log records  the weights we
obtained for the following features were as follows 
 classification accuracy was     

fino  of clicks by user on url  f  
no  of clicks by user on domain  f  
average dwell time on url  f  

    
    
    

a record was found to be relevant iff        f           f   
       f            
our results suggest that average dwell time is a much stronger
metric for relevance determination followed by number of
clicks on the url  surprisingly  number of clicks on the
domain does not seem to be as important 
we evaluated the performance of our supervised learning
algorithm combined with rank aggregation and clustering on a
kaggle dataset consisting of    million unique urls    
million clicks     million unique queries      million unique
users       million training sessions and      million test
sessions   our experiments were carried out on the madmax
machines in the infolab with intel xeon processors     ghz   
tb ram     virtual cores  
the metric used by kaggle for evaluation was the normalized
discounted cumulative gain  ndcg   a common metric used
for search engine result evaluation     in the dcg method the
relevance contribution of a document is decreased by an
amount proportional to the log of the position of the result in
the page  the measure allows for more grades of relevance as
opposed to a binary grade  as relevant or not 
the dcg of a particular re ranked test sample is computed
using the formula 
 

 



we see that the ndcg obtained by our algorithm           is
not that far from the top ranked ndcg         
due to memory constraints  we could not perform all pairs
correlation and find the users most similar to a given user  we
sampled      users randomly for each user and chose the top
    while calculating correlation  after clustering  the number
of urls we could re rank went up from      to       since
we had more information about a users preferences 
however  we expect that our re ranked results could have
been more relevant had we been able to perform an all pairs
correlation computation to find the most similar users for
every user 
conclusion
we described crewd a system for reranking ranked search
engine result pages  we also introduced novel metrics  section
iii a  that could be leveraged by modern ir systems or
machine learning systems that perform ranking  we also
proposed an optimization for candidate selection to avoid an
exhaustive while finding similar users  we finally described
how crewd performs rank aggregation on both our
supervised and unsupervised approaches  we see that our
rank aggregated result gives a higher ndcg than rule based
learning 
future work
it would be interesting to perform an all pairs correlation
computation between users  we are also curious about how
other rank aggregation schemes such as those based on
inversion distance would perform  we may implement some
of these approaches in the winter break to see if our ndcg
increases further 
references

algorithm
random baseline
default ranking baseline
rule based classifier with local history      
supervised learning with clustering and rank
aggregation  crewd 
top position in kaggle leaderboard

ndcg

       
       
       
       
       

we evaluate our ndcg performance against a default ranking
baseline and a random baseline  the default baseline does not
do any re ranking  instead it relies just on features at the query
level and features like page rank to do ranking  for the default
ranking baseline  the re ranked list is the same as the ranked
list  the random baseline returns an arbitrarily random
permutation of an url  the top ndcg in the kaggle
leaderboard is the maximum ndcg obtained by any
contestant  we implement bordas rank aggregation
 implemented by combining the re ranking algorithms in
section iiia and section iiic   with different values of  for
tuning local and global history weights 

  hongning wang  xiaodong he  ming wei chang  yang song  ryen w 
white  and wei chu        personalized ranking model adaptation for
web search  in proceedings of the   th international acm sigir
conference on research and development in information retrieval
 sigir      
  r  fidel and m  crandall  users perception of the performance of a
filtering system  in sigir     volume     pages        acm       
  belkin  n j   ingwersen  p  and leong  m  k   ir evaluation methods for
retrieving highly relevant documents    eds   proceedings of the   rd
annual international acm sigir conference on research and
development in information retrieval  new york  ny  acm  pp     
   x  shen  b  tan  and c  zhai  context sensitive information retrieval using
implicit feedback  in sigir      pages             
   j  rocchio  relevance feedback information retrieval  in the smart
retrieval system experiments in automatic document processing 
pages         kansas city  mo       prentice hall
  h  cui  j  r  wen  j  y  nie  and w  y  ma  probabilistic query expansion
using query logs  in proceedings of www           
   g  jeh and j  widom  scaling personalized web search  in proceeding of
www           

fi