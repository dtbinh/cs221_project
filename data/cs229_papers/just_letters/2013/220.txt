reinforcement learning for bicycle control
bruce cam  chris dembia  johnny israeli
december         

 

introduction

in       randlov and alstrom showed that reinforcement
learning can be used to control a bicycle      since then 
their work has enjoyed minor popularity as a benchmark
problem               in part because it is harder to solve
than the popular cart pole problem  we implemented
randlovs original work using the pybrain     machine
learning library  we were particularly interested in how
difficult it is for a human to balance the bicycle model using keyboard inputs  how much of a feat is the reinforcement learning algorithm accomplishing  to this end  we
created a small game using the panda d library 
as randlov and alstrom did  we also attempted to
learn a controller that could drive to a goal destination 
to extend their work on shaping  we tried our own suite
of complex reward functions that would work well for
arbitrary goal destinations 
our code is available online at https   github com 
chrisdembia agent bicycle 

   

background and previous work

since the state of a bicycle is continuous  randlov and alstrom needed a method to generalize the learners observations from specific states to new states the learner had
not seen  they chose to discretize the five dimensional
state of their bicycle into      bins  the learning that occurs for any state in a given bin is used for all other states
within that bin  to evaluate the action value q   this
generalization method is called linear function approximation with tile coding       though randlov and alstrom
use different words in their description  this rudimentary
method would require a significant tuning of the generalization scheme  number of bins  etc  if a different bicycle
model were used 
lagoudakis  et al  used a linear function approximation with    basis functions that were nonlinear functions
of the state  e g            etc        their method  least
squares policy iteration  lspi   does not require the state
to be discretized  but still requires a discrete action space 
lagoudakis and parr have also used svms for the same
bicycle in order to store q s  a       ernst  et al  use treebased supervised learning methods      ng and jordan
learn a markov decision process  which allows for a continuous action space  contrary to randlov and alstroms

work       of course  many of the generalization methods  or reinforcement learning algorithms that come with
a form of generalization  have not been applied to the
bicycle problem  for example  neural fitted q iteration
 nfq  is designed to be less sensitive to any specific experience  because it is offline and q s  a  is stored using
a neural network with a hidden layer      we thoroughly
explored both the lspi and nfq methods  but had limited success  in this report  we only discuss results from
using randlovs original method 

   

controlling a bicycle

why does it make sense to use reinforcement learning
to control a bicycle  we see three reasons  as is evident in      reinforcement learning allows the learning
of a control law that causes qualitatively different behavior depending on the state  the agent can learn to focus
on riding towards a goal when the bicycle is upright  but
must focus on balancing when the bicycle starts to tilt
significantly  secondly  reinforcement learning allows for
a control law that achieves a much more high level goal
than that obtainable via typical optimal control methods 
while others have studied lane change maneuvers with
conventional control methods       the task of driving to
a goal that is at a right angle from the bicycles heading
would be a challenge for such methods unless the entire
trajectory is specified that is  reinforcement learning can
be used for trajectory optimization 
the last reason relates the bicycle problem to other
problems typically approached with machine learning algorithms  the bicycle is not so different from the cart
pole  in both cases we attempt to keep an unstable system upright  however  the control in the cart pole problem more directly affects the angle of the pole  move the
cart so that it is under the pole  in the bicycle problem 
we have the same rough objective  move the bike in the
direction it is falling in order to catch it  however  the
control to move the bike under itself is not so simple  indeed  to turn a bicycle to the left  one must first steer to
the right  this is called counter steer  indeed  this is reminiscent of the mountain car problem  in which we have
the tricky result that the car must first go in the direction
opposite to its goal destination 

fi 

randlovs model

the agent first needs to gain experience balancing before
attempting to navigate toward the goal 

using randlov and alstroms bicycle model and environment  we trained a learner using the sarsa   algorithm  we implemented randlovs balance and go to
tasks with pybrain  a machine learning library for python
     the episodic learning framework is illustrated in figure    to evaluate the execution of the task during learning  we will additionally define a rehearsal sequence  in
one rehearsal sequence  the agent learns by executing   
episodes  after which  the agent performs one episode
greedily without learning  we use the discounted sum
of rewards achieved over the performance episode as a
success metric 
we used randlovs reward function for both tasks  for
the balance task  a reward of    was given when the absolute tilt angle exceeded     and   otherwise  for the
go to task  the following reward was given 
if abs    

figure     a  learning curve for the balance task   b  the trajectories of the bicycles wheel contact points in each of several
thousand episodes  simulation was stopped after      seconds 
so trajectories may appear to terminate  the agent also found
several stable circular trajectories  as the one highlighted above 


  

r    
else
r                   
figure   shows the results from randlovs balance
task  our learner was able to balance indefinitely after
approximately      trials  we observed cases in which
the agent would fall travel in stable  circular trajectories 
this was also observed by randlov  figure   shows the
resulting implementation of randlovs go to task  the
agent lfirst learns to balance  and it first reaches the
goal after approximately      trials  both of the tasks
are very sensitive to learning rate annealing  randlov
does not mention this  despite the fact that seems to be
well known in the field       without annealing  neither task will result in a converging policy because the
current learning continually discards all the learning that
had already been done  the go to task required a much
slower annealing rate than did the balance task because

figure     a  learning curve for the go to task   b  the trajectories of the bicycles wheel contact points after several thousand episodes  here the goal was placed at a position  x  
  m  y      m  relative to the bicycle starting location  after
about      episodes      rehearsals   the bicycle reaches the
goal with increasing frequency 

now  building on randlovs implementation  we explore our topics of interesthuman ability to control a
virtual bicycle  and shaping of the reinforcement reward
function 

figure    learning methodology  in pybrains terminology 

 

fi 

game

 

shaping

one of the key factors in any reinforcement learning application is the reward function that is being maximized 
to optimize the learning  the reward function provide
positive reward for actions that help attain the goal and
negative reward for actions that hinder the bike from attaining its goal  this process of reward engineering is
called shaping  here we describe a systematic shaping of
reward functions for a go to task 

   

balance task

in order to shape an appropriate reward function for
balance  we first determine what behavior in the bike
model can help avoid a fall  if the bicycles tilt  satisfies          then we want to reward actions that reduce
tilt and punish actions which increase tilt  for tilt  at
timestep t we define    as the tilt at time t     and a
balance reward function as follows 

figure    we simulated our model using a python game engine  panda d      here  the users may attempt to balance the
bicycle by applying a torque to the handlebars or by shifting
the center of mass via keyboard inputs  additionally  the game
engine was used to visualize the agents learning process  this
aspect was useful for demonstration and debugging 

 

rb        

to explore how humans control a bicycle  we created
an interactive simulation environment using panda d  a
game engine for python  we simulated randlovs bicycle
model  and provided users control over the same actions
available to the reinforcement learning agent  in testing
the game  we quickly discovered the reflexive nature of
human control of a bicycle  successful coordination of
body mass displacement and torque control was difficult
to accomplish  in fact  we struggled mightily to balance
the bike for more than a few seconds  far worse than what
our trained agent was able to achieve  this indicates that
the use of apprenticeship learning for this problem would
require that the observations come from humans riding
real bicycles  not from keyboard inputs 



a                       
b                       

   

where a is defined as the reward factor and b is defined as the punishment factor 

   

go to task

in the case of a go to destination task  the agent must
both balance the bicycle and navigate to the goal  we
measure navigation performance through g   the error
in the heading at time t  as in balance  we define g   
the heading error at time t     and a navigation reward
function as follows 

during the poster session for this project  we considfi fi
fi fi

ered allowing observers to try their own hand at balanca  g    fifig  fifi    g     fifig  fifi
 
ing the bicycle  however  since we found it to be so chalrn  g   g    
   
b  g    fig  fi    g     fig  fi
lenging  we decided instead to allow observers to apply
a perturbation to the bicycles steering torque  using keyfinally  we combine rb and rn for a composite reboard input   the learned controller was able able to
ward
function rt   rb   crn   where c determines the
reject disturbances that were five times greater than the
degree
of emphasis on balance versus navigation  and a
steering torque it has available to itself for control 
and b have the same values as in rb   we suppose that
in future studies  we would like to collect user input
there exists an optimal ratio between balance and navidata for a simple balance task and compare this to an
gation that is characterized by c   now  we expect c to
optimal reinforcement learning policy  our preliminary
scale inversely with the expected converged values of rn  
experiments suggest that the simulation may need to be
implying the following behavior for c  
sufficiently slowed for usability  further  we found great
utility in visualizing the learning process using our game
 
 
model  it helped illustrate behaviors that were being re
c 

   
cum rn  
arctan xy
warded  and it was a valuable debugging tool  games
such as this can be used as educational or illustrative
tools 
where the target location is  x  y  
 

fi   

future work

we have two major future goals in the area of shaping 
first  we plan to more thoroughly characterize the behavior of c by repeating the trials presented here for
statistical confidence and testing more perturbations  in
particular  we plan to test a series of targets at with a
constant xy and varying distance  our second goal is to
develop a more sophisticated reward function  the current reward system has produced functional controllers
for various targets  however  it linearly combined rn
and rb which implicitly treats them as orthogonal  but
we know that this is not true since  and g are coupled
in our model  thus  we plan to explore reward functions which include cross terms between  and g   by
developing more sophisticated reward systems and characterizing the behavior of the coefficients in the reward
function we hope to develop a systematic method of deriving reward functions  this will eliminate most of the
fine tuning that is usually required in controller design 
randlov and alstroms bicycle model is highly simplified  and does not capture all the interesting dynamics
of bicycle motion  for instance  their steer axis is vertical  realistic bicycles have a slanted steer axis that is designed so that the point where the front wheel contacts
the ground in fact lies behind the point where the steer
axis intersects the ground  it had long been supposed
that this feature contributes to the stability of bicycles  in
fact  bicycles are self stable  they wont fall over if pushed
sideways  between about   and   m s  forward speed  
thus  it seems the utility of their problem is as a toy
problem for the comparison of reinforcement learning
methods  with limited applicability for actually controlling a bicycle  significant work has been performed to
develop accurate models of bicycle dynamics     

figure    path trajectories to a target centered at          after
     episodes for c         left   c        center   and c      
 right   both c        and c       reached the goal once but
c       reached the goal twice  thus  we expect c to be near
c       

to test this hypothesis we set a         b       
and tried several c values for a target at          and concluded that c        is in the vicinity of c        figure    
next  we kept the same a  b  and c values for a target at
         and we found that c        is the best estimate of
c          this confirms the hypothesized behavior which
predicts that c          c         

figure    path trajectories to a target centered at          after
     episodes for c         left   c        center   and c      
 right   both c       and c       reached the goal once  c      
reached the goal three times and thus a better estimate of c 

references
    damien ernst  pierre geurts  and louis wehenkel  tree based batch mode reinforcement learning           
     
    mike goslin and mark r  mine  the panda d graphics engine  computer                 october      
    michail g lagoudakis  parr cs  and duke edu  reinforcement learning as classification   leveraging modern
classifiers       
    michail g lagoudakis  ronald parr  and michael l littman  least squares methods in reinforcement learning
for control  pages              
    jason keith moore  human control of a bicycle  phd thesis  university of california  davis       
    andrew y ng and michael jordan  pegasus  a policy search method for large mdps and pomdps  in uncertainty in artificial intelligence       
    jette randlov and preben alstrom  learning to drive a bicycle using reinforcement learning and shaping 
proceedings of the fifteenth international conference on machine learning       
    martin riedmiller  neural fitted q iteration   first experiences with a data efficient neural reinforcement
learning method  in   th european conference on machine learning       
 

fi    tom schaul  justin bayer  daan wierstra  yi sun  martin felder  frank sehnke  thomas ruckstie  and jurgen
schmidhuber  pybrain  journal of machine learning research       
     tom schaul  sixin zhang  and yann lecun  no more pesky learning rates       
     robin s  sharp  on the stability and control of the bicycle  applied mechanics reviews                     
     richard s sutton and andrew g  barto  reinforcement learning  an introduction  the mit press  cambridge 
ma       

 

fi