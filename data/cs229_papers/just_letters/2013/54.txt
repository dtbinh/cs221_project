stumbleupon evergreen classification challenge
 website classification problem 
sumit roy  sumitroy stanford edu 
shailin saraiya shailin stanford edu 
abstract
web classification is a very important machine learning problem with wide applicability in tasks
such as news classification  content prioritization  focused crawling and sentiment analysis of web
content  in this project  we primarily focus on developing prediction model using machine
learning techniques for one such problem that classifies if a web posting is of eternal relevance 
known as evergreen or is short lived  we finally apply the optimized model to a different web
classification problem  namely the subject classification of an website content 

   introduction
classification plays a vital role in many
information management and retrieval tasks 
on the web  classification of page content is
essential to focused crawling  to the assisted
development of web directories  to topicspecific web link analysis  to contextual
advertising and to analysis of the topical
structure of the web  web page
classification can also help improve the
quality of the advertising  and help improve
the quality of web search  customer reviews
or opinions are often short text documents
which can be classified to determine useful
information from the review   automated
methods can be very useful for news
categorization in a variety of web portals 

classification is made on the basis of users
ratings  if machine learning algorithms are
used then such distinctions can be made
ahead of time which in turn improves the
quality of recommendation engine  the rest
of the project report is organized as follows 
section   describes our initial model based
on nave bayes classification  followed by
feature selection and several other enhanced
model  in section    we present a pipelined
model that improve the accuracy over
models in previous section  in section    we
apply the model developed in section    to
the subject classification problem  we
conclude our report in section   along with
plans for future 

the goal of our project was to study a
specific instance of this broad and vital web
classification problem and developing a
successful prediction system  we selected
the stumbleupon evergreen classification
challenge  that requires building a classifier
to categorize webpages as evergreen or
ephemeral  most news articles or seasonal
recipes are relevant for only a short period of
time while others are relevant forever and
can be recommended to users anytime 
therefore  based on the relevancy of a
webpage  it can be classified as evergreen
or non evergreen  in general such

   nave bayes
before presenting our classification models 
let us first explain the dataset used for all our
experiments  stumbleupon com    provided
the parsed dataset of      webpages along
with user defined labels whether they are
evergreen or not  the dataset contains  
types of data
 textual  boilerplate that includes
title  keyword and body
    meta data  common link ratio 
spelling error ratio  html tag ratio 
subject classification  etc

fifor comparing our models  we used   stratified cross validation for training and
testing on these datasets  we measured the
progression on our improvement using
roc auc score  a receiver operating
characteristics  roc  graph is a technique
for visualizing  organizing and selecting
classiers based on their performance  rocauc score computes the area under the
curve for roc      accuracy score is based
on one specific cutpoint  while roc tries all
of the cutpoint and plots the sensitivity and
specificity  so roc auc gives a more robust
metric compared to accuracy score  hence
we used that for qualifying all our models 
as suggested in the class      we started by
implementing the multinomial nave bayes
using the boilerplate as the feature set 
figure    shows the results using the nave
bayes running on       tokens 

cross validation scores using
multinomial naive bayes
    
   
    

i

my
recipe

that

it

sugar

you
food

into

cup

or

figure    tokens with highest log   
    feature selection
we implemented mutual information based
feature selection to filter out the stop words
as mentioned in the class  it did show some
improvement in filtering out most of the
stop words  but several of the stop words
 like my  i  you  still got into top     log 
score  the key thing that was missing from
mutual information metric is the importance
of word frequency in a given document as
well as the rarity of occurrence of that work 
hence we looked at tf idf  term frequencyinverse document frequency       we
performed feature selection using tf idf
and then applied that to the multinomial
nave bayes classifier  figure    shows the
comparison between that and the one
without tf idf filtering  the average
roc auc score improved from       to
      

   

improvement using tf idf

    
    

   
 

 

 

 

 

  

           

   

figure    results using nave bayes

    

the average roc auc score was        the
first step in improving our model was to
improve the feature selection  we started by
analyzing the tokens with the largest positive
log  and negative log     which is shown in
the figure    below  the tokens in red  stopwords  are not good for distinction but nave
bayes gave high weightage to them 

   
    
 

 

 
nb

 

 

  

           

nb tf idf 

figure with
   results
using tf idf
    model
regularization

fiafter finalizing the feature selection
methodology  our next step was to start
analyzing ways to improve the model  we
reviewed the boilerplate of the elements that
was classified as false negative  we found the
following types of information 
o empty body      
o advertisements      
o tabloid news       
o bogus police report       
where the element in the bracket is the item
number that displays the behavior
mentioned  having an empty body or a
tabloid news labeled as an evergreen clearly
shows that there are human errors
introduced during the manual classification 
given the noise in the dataset  we need to
inspect models that perform regularization 
like svm or logistic regression  we applied
logistic regression  referred as lr in rest of
the report  with l  estimator and tuned the
parameter c  regularization factor  using
gridsearch  figure   shows the results
compared to nave bayes with tf idf  as
you can see  the roc auc score improved
using lr for most of the cross validation
point  average roc auc improved from
      to       

that certain subjects has more false positives
than others and certain others have more
false negative than others 
since the training data had subject
classification  we plotted the lr classified
probability of each item against positive and
negative label per subject  figure   shows
that classification  where each red line
displays probability for all items with label  
for each subject and similarly green
represents for label    one can clearly see
that each subject should have a different
classification point  we decided to
implement a gaussian discriminant analysis
 gda  on the output of the lr fitted for each
subject  the hope being that gda would be
able to identify the mean and variance to
decide the classification point as well as the
confidence level  probability of the
classification  better  so the classification
system was tf idf on the boilerplate 
followed by lr on tokens for a given subject 
followed by gda per subject 

improvement using logistic
regression
    
   
    
   
 

 

 

 

nb tf idf 

 

  

           

logistic regression

figure    results using lr
we continued our analysis to improve the
model further  while investigating the falsepositive and false negative items  we noticed

figure    subject vs lr probability
unfortunately  the average roc auc score
did not improve  as shown in figure   
although the accuracy was better because of
the subject specific classification point  the
probability of the false positive negative data
was pushed to the extremes    and    

fisubject based lr gda
    
    
   
    
    
    
    
 

 

 

 

 

lr gda

  

           
lr

figure    subject based lr gda vs lr

next we started investigating if the model
can be improved using a subset of the   
non textual feature set  we plotted the
probability of the lr classification of positive
labels  red  and negative labels  green 
against each of the non textual feature set 
figure   shows the plot using common link
ratio  y axis  vs the probabilities of
boilerplate using lr x axis   visually  it
clearly showed that none of the non textual
feature set had any additional discriminative
power  we verified that by running a lr with
featureset as the classification probability of
the boilerplate based lr and each of the
non textual dataset  the average roc auc
scores from the    fold cross validation was
much worse than just the simple lr  this
clearly demonstrated that non textual
features are not very useful feature for our
model  in the next section  we will talk about
pipelining strategy using multiple textual
feature set 

   pipelined model
given that the non textual features lacked
differentiability  we started investigating
meta data within the boilerplate that could
potentially improve the classification
accuracy  we generated a lr model with

figure    link ratio vs probability of lr
keywords as the feature set  the roc auc
score for the model was good  in the range of
       which indicates that keywords have
good potential of classifying the dataset  we
performed similar experiments based on
words in the title of the webpage as feature
set and got similar roc auc score 
encouraged by this  we implemented a
combination of ensemble and pipelined
model as shown in figure    we start with  
distinct feature sets  namely title  body and
keywords of the webpage  transformed it
through tf idf filter and then trained   lr
models for each respectively  finally  we took
the classification probability of each of those
models and trained the final lr model to
provide the final classification  we used our

body

title

keyword

tf idf

tf idf

tf idf

lr 

lr 

lr 

lr 

figure    final classification model

firoc auc score

webpage subject classification

best

    
     
    
     
    
     
    
     
    

figure    comparison of various models
dataset to train and test this model and the
results are shown in figure    as can be seen 
this combination of feature set along with
pipelining and ensemble strategy gave us the
best roc auc score 
   subject classification
as mentioned in the introduction section 
the goal of our project was to develop model
for a specific web classification problem and
then extend it for other website classification
problem  the problem that we decided to
explore was classifying the subject of the
content of a webpage  the stumbleupon
dataset had that classified as one of its metadata  we used that as the label and applied
our model developed in section    the
results are shown in figure     we observed
that subject contents that are very precise
like health and sports were classified
with more than     roc auc score  but
subjects like business and entertainment 
which have more broader definition ended
up with lower scores 

 
   
   
   
   
 
 

 

 

 

 

  

           

health

sports

entertainment

business

figure     pipelined lr model
   conclusion
the webpage classification problem is vital
to many web mining applications and we
presented a method to effectively solve the
stumbleupon evergreen challenge  we were
encouraged by the results seen on applying
the model to a different webpage
classification  namely subject classification 
in future  we plan to extend our model for
sentiment analysis of webpages and emails
as well as study ways to model noise better 
   reference
    andrew ng  cs      class lecture topic
    j  ramos  using tf idf to determine
word relevance in document queries 
icml      
    tom fawcett  an introduction to roc
analysis  pattern recognition letters   
      
    www stumbleupon com

fi