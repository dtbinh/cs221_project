large vocabulary continuous speech recognition with
linguistic features for deep learning
cs        n joint final project
peng qi

abstract
until this day  automated speech recognition  asr  still remains one of the most challenging tasks in both machine
learning and natural language processing  asr research
faces data with high variability  which requires highly expressive models be built  recently  deep neural networks
 dnn  have been successfully applied to various fields  including speech recognition  in this course project  we would
like to investigate what are some possible linguistic features
that would contribute to speech recognizers  as well as what
machine learning techniques can be applied to such tasks 

  

introduction

deep neural networks have made great contribution to
speech recognition research recently  pushing one step further the state of the art  a speech recognizer generally consists of two parts  an acoustic model that converts acoustic
input into phonemes  and a language model that combines
these phonetic information to form words and sentences 
deep neural network have been shown to improve the performance of both tasks  see  e g      and     
from preliminary analyses of our dataset  detials in section     we observed that if the trigram language model is
given perfect output from the acoustic model  the system
word error rate  wer  is about     while the state of the
art on this dataset is about      that comprises the basic
motivation of focusing our project on improving the acoustic model  specifically  we trained and analyzed a handful of
different deep learning models on the dataset  and investigated how additional linguistic features such as conversation
topic  speaker gender  as well as others would further affect
the performance of acoustic modeling 

  

literature review

in      hinton et al  introduced dnn hmm hybrid systems for speech recognition  which achieved considerable
improvements over the traditional gmm hmm systems  in
      goldwater et al      conducted a thorough research
on how various acoustic and linguistic properties might affect the performance of such systems  in that paper  the
authors evaluated the effect of a myriad of properties including speaker gender  position near disfluency  pitch  etc 
while in that work the authors evaluated feature effectiveness with independent word error rate  iwer   we will stick
with senone  accuracy thoughout this project 
 
senones used in this project roughly correspond to triphone states of the successive hmm in the language model 

while reviewing related literature  we also found that a
specific type of neuron activation function  namely linear
rectifiers  are widely applied and achieved state of the art
performance in a number of recent publications  hence in
this project  well adopt a variant of linear rectifiers for our
deep neural networks proposed in     

  

dataset

in this project  the switchboard speech recognition corpus  was chosen as our study dataset mainly because of two
reasons  first  with about       telephone conversations
from     speakers  this dataset contains a large amount of
data that are highly diverse  which allows large deep neural
networks trained supervisedly without the concern of heavy
overfitting and poor generalization  the size of the corpus also relieves the burden to build a sophisticated language model  this allows us to focus on the acoustic model 
and hopefully reducing the system wer by improving the
senone  or frame  accuracy 
another major reason for our choosing switchboard  swbd 
over other datasets is that swbd contains a number of welldocumented linguistic features that were collected alongside
the speech data  which would significantly help in verifying
the idea that such features might help improve the performance of acoustic models  below we will briefly describe
the features used in our project  the rationale behind using them  and some basic statistics across the dataset  before listing the linguistic features  it is worth noting that
the input acoustic features should have been projected following a standard procedure to a subspace where speakerdependent information are supposedly removed  however 
due to the  conceptually  high nonlinearity of speech information with regard to its variability  we believe that some
speaker dependent information still exists within the acoustic features  and by introducing the corresponding linguistic
features we can cancel out these residuals with highly nonlinear deep neural networks to achieve better performance 
 speaker gender  speakers of different sexes tend to
present significant differences in pitch change  speaking
speed  which affects the presense of senones related to
repetition deletion insertion   as well as word choice
 which affects the probability of presence of different
senones  
 speaker dialectic region  speaker dialect tends to
significantly affect the their pronunciation of phones 
 

http   www isip piconepress com projects switchboard 

fispeaker gender

dialectic region

in fig     we have drawn a number of statistics of the
above stated properties across the dataset  from the figure
we can see that most linguistic features have a relatively
even distribution  which is a good property for informative
features as none of them will provide virtually zero information to the deep neural network 

propagated from  the training set statistics of the senone
labels is shown in fig     log scale  
from fig    it is evident that the senone labels follow
a very skewed distribution in the training set  for which
multiclass classifiers usually fail to achieve high accuracy 
as a start  we trained standard softmax deep neural networks  dnns  with cross entropy cost function  cenet  on
about     hours of speech data and tested on a separate
    hours  with stochastic gradient descent  we trained the
network on the whole training set with minibatches of    
training examples  in the meantime  we considered it a good
idea to attempt large margin cost function  svmnet   which
conceptually should work better on multiclass classification
tasks than cenet because it is purely discriminative rather
than generative  then  to account for the skewed distribution of the labels  we also tried to modify cenet with hierarchical classification  specifically  after sorting the labels
in decreasing order by their frequencies  we progressively
classfied the top        hcenet  k  or        hcenet  k 
senones against the rest until all labels are classified  and
added the cost functions of these classifiers together to optimize with the dnn  finally  we also attempted another
scheme to address the skewness  reweighing cost functions 
by reweighing the cost function softmax and large margin
networks with reciprocals of label frequencies  we obtained
two final baseline networks rwcenet and rwsvmnet  the
results of these baseline networks are shown in table   after
  epochs of training  usually took    days for each model
with gnumpy    
surprisingly  cenet alone is capable of working well  while
svmnet  which ideally would have been better as a discriminative rather than generative model  turned out to be
a lot worse  however  by looking at the reweighed models 
we can see that rwsvmnet improves significantly based
on svmnet  which probably suggests that svmnets failure resulted from the imbalancement of training examples
within each mini batch of stochastic gradient descent    in
which case the parameters for rare classes hardly got updated with enough positive examples  while reweighing the
cost function alleviates this problem in gradient computation  on the other hand  reweighing didnt seem to help
cenet  which is predictable as softmax classifiers are generative models  which works best if the prior knowledge of the
data is correctly exploited  also surprisingly  hierarchical
classification scheme didnt work well on this dataset  this
might suggest that the major challenge of the dataset is the
distinction between some frequent class versus some infrequent ones  rather than among classes with similar frequency
in the training set  in the sense that compared to cenet  the
drop in performance resulted from the networks feature extraction capability misued on minor discriminations  these
observations lead to potential future work directions on this
dataset described in section   

  

  

   

   

   
   
   
   

   
  

   

  
   
  
  

  

   

  

   

n
w
no

ed

yc
n

nk
u

n

nd

er

la

m
ix

en
g

th
or

ew

n

nd

th

n

id

la

er
m

nd

te

h

education level
n

n
or

ut
so

speaker age

ut
h

la

es

id

w

so

 

male

m

female

rn

  

 

   
  
   
  
   
  

   

 
 

  
 
  

  

  

  

  

  

  

  

  

  

 

 

speaker activity

 

 

 

conversation topics
  
calling
called

  

  

  

  

  

  
  

  

  
  
  
 
 

  
  

   

   

   

   

   

   

   

   

 
 

   

  

  

  

  

  

  

  

figure    linguistic features statistics of the switchboard
dataset
 

senone label frequency

  

 

  

 

  

 

  

 

    

    

figure    senone label statistics of the switchboard dataset
 sorted by frequency 
 speaker age   education level  both might contribute to word choice and or pronunciation convention of the speaker 
 speaker identity  apart from the information above 
speaker identity might convey information about some
speaker specific habits or personal marks of word choice 
etc 
 conversation topic  apart from its evident effect
on word choice  conversation topics might also affect
speech speed  pitch change  etc 

baselining

before introducing linguistic features  we briefly analysed
the property of the dataset  and performed baseline training
on several different deep neural networks that we will elaborate below  to balance between performance and training
speed  the networks used in our project shared the same
basic structure with       acoustic input units  three linear
rectifier hidden layers of       units each  and a classification
output layer with       senone classes where errors are back

incorporation of linguistic features   analyses

after baselining  we chose the standard softmax network 
amongst others  as the baseline model for further analysis
 

http   www cs toronto edu  tijmen gnumpy html
for svmnet we also attempted to use larger minibatches 
but increasing minibatch didnt improve the performance of
the network  either  before we ran out of gpu memory 
 

fitable    baseline model performances
model

train accuracy     

test accuracy    

cenet
rwcenet
hcenet  k
hcenet  k

     
     
     
     

     
     
     
     

svmnet
rwsvmnet

    
     

    
     

  the training set accuracies are estimated on the fly during training  with               minibatch  where  is the
overall accuracy estimation and minibatch the minibatch accuracy for the last seen minibatch  the same technique is
also applied to experiments in section   to reduce computation time  same procedure applied to models with linguistic
features 
table    model performances with linguistic features
model
cenet
cenet a
cenet a 

train accuracy     

test accuracy    

     
     
     

     
     
     

with liguistic features  to assess the contribution of linguistic features that we introduced  we started with a basic augmented model  where the linguistic features are appended
to the acoustic ones and fed together into the deep neural network  cenet a   specifically  the linguistic features
were translated into binary or categorical features  and numerical feature like age is binarized by thresholding with its
median value to ensure compatibility with the deep neural
network model  to further ensure that the linguistic features take part in the training process of the dnn  we also
developed a second network structure where the linguistic
features were fed into each hidden layer as well as the output layer of the dnn  forcing each layer to accommodate the
raw linguistic features when trying to minimize the model
cost function  cenet a    the results from the models with
linguistic feature incorporation are shown in table    where
the cenet results are also shown as a baseline 
finally  we also attempted to train a dnn model that
also predicts the linguistic feature themselves alongside the
senone labels  which resembles an autoencoder in some ways 
with the hope that this kind of structure can help us make
sure that linguistic features are taking part in the representation of the dnn  technically speaking  such models are
called multitask learning systems  mtnet   which generally
should reduce overfitting and improve model generalization
ability  the potential logic behind such systems is that the
local optima the softmax network alone achieves is possibly not as good as that for the multitasking network  or
the dynamics of the latter could lead to a better local optima faster for the classification task with the help of extra
information  this might not be generally true for most models  but for highly non linear models such as dnns where
first order gradient descent based methods are applied  it
seems more reasonable to assume the existence of better local optima unreachable with simple optimization algorithms 
however  by the time of the completion of this report  the

complicated multi task cost function significantly worsened
the performance of the network on the original senone classification task  though not much substantial improvements
were achieved  this part of the project did suggest one of the
future direction of our work 
from table   it can be seen that the extra linguistic features did improve the classification accuracy of the senones 
but it would be of interest to more closely examine how the
features worked  and how much each individual type of extra
information helped 
to perform error analysis on our models  the       senones
are mapped back to their    different center phones  and
the confusion matrix of these phones are shown in fig   
top row  left   with this confusion matrix for the baseline
cenet model  we can tell that the dnn is already performing impressively to correctly classify most of the phones  although some major anomalies do attract our attention  the
most significant anomaly is that a major number of classification errors happened when spoken noise  spn   non spoken
noise  nsn   as well as in word pause  lau  were misclassified
as silence  sil   and in fact it is observed that a lot other
phones are misclassified to silence as well  this is likely
to result from the imbalanced distribution of the phones in
speech  where silence appear in most utterances while specific phones appear much less  some other observations include misclassfications of en as n  confusion among k  g  p 
and d  between eh and ae  between z and s  as well as other
common mispronunciations and mishearings that occur in
speech  after the incorporation of linguistic features  the
major results  confusion matrix  are similar  the change of
the confusion matrix is analyzed instead  as it turned out 
one of the improvements is that ahs are significantly less
recognized as ae  other improvements include better differentiations between s and z  among eh  aw  ay  and ae  and
among tailing consonants  t  d  n  m  etc   while intuitively
the confusion of vowels might be majorly related to dialectic
regions  the pronunciation habit of tailing consonants might
also trace back to the speakers age or educational level 
next  we analyzed the feature effectiveness of the ceneta model by plotting the average squared second norm of the
weights for each class of linguistic features that were fed into
the network  with the average value of all first layer features
plotted in dashed line and its one standard deviation range
plotted in dotted line  it can be shown that age  dialectic region  and educational level are the most contributive linguistic features in this network  which endorses our reasoning in
the analyses of confusion matrices  identity and topical information helped less in this task  which probably results
from their sparsity accross the dataset compared to the top
three  to our suprise  gender information seems very unhelpful in this task  which suggests that the acoustic features
that we use have successfully removed gender related information in the transform  or that gender related variabilities
in the input is less of a problem given the representational
power of deep neural networks 

  

conclusion   future work

in this course project  we examined the effectiveness of
various deep learning models with controlled experiments 
and applied linguistic features to the softmax network  improving its performance in acoustic modeling  a crucial part
and performance bottleneck of state of the art speech recognition systems  weve demonstrated that with the incorpo 

firation of linguistic information when available  the performance of acoustic models can be improved  and analyzed
the importance of each of the features 
one of the next steps of this project should intuitively be
applying the linguistic feature augmented deep neural networks to the full model of speech recognition  and examine
whether word error rate could be lowered as a result 
another potential future direction comes from our experience and observations during the project  while undertaking experiments for the project  the major bottlenecks for
us were the efficiency for learning the deep neural networks 
for which stochastic gradient descent is applied in line with
the field of active research  however  our discoveries with
large margin cost functions as well as multi task networks
might suggest that we should research for more efficient and
effective learning algorithms for deep learning models with
a large number of parameters on such huge amount of data 
finally  a potential future direction specific to speech recognition  and perhaps machine learning tasks with similar natures  is to apply structured classifiers to the dnn acoustic model  as was observed in our hierarchical classification task  if the representational capacity of the network is
wasted on non significant discrimination tasks  the model
effectiveness would deteriorate  however  we would expect
substantial improvement if such hierarchical information is
used correctly  specifically  we would like to apply the intuitive hierarchy that exists among the senone classes as a treestructure classification target  where not only the task of the
network is the discrimination of the senones themselves  but
also their center phones can be taken into account  we expect to see an improvement in senone classification accuracy
as a result 

acknowledgements
we would like to thank prof  ng for his in class instruction  and the tas for their feedback on this project  we
would also like to thank andrew maas  awni hannun  and
chris lengerich from the stanford deep learning for speech
recognition group for providing source of data  for their insightful comments as well as helpful discussions 

  

references

    sharon goldwater  dan jurafsky  and christopher d
manning  which words are hard to recognize 
prosodic  lexical  and disfluency factors that increase
speech recognition error rates  speech communication 
                   
    geoffrey hinton  li deng  dong yu  george e dahl 
abdel rahman mohamed  navdeep jaitly  andrew
senior  vincent vanhoucke  patrick nguyen  tara n
sainath  et al  deep neural networks for acoustic
modeling in speech recognition  the shared views of
four research groups  signal processing magazine 
ieee                   
    andrew l maas  awni y hannun  and andrew y ng 
rectifier nonlinearities improve neural network acoustic
models  in icml workshop on deep learning for
audio  speech  and language processing       
    frederic morin and yoshua bengio  hierarchical
probabilistic neural network language model  in
proceedings of the international workshop on artificial
intelligence and statistics  pages              

topic
identity
gender
age
dialect
education
   

   

   

   

   

   

 

   

figure    analyses of the effect of introduced linguistic features

fi