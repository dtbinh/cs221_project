w vs  qcd jet tagging at the large hadron collider
bryan anenberg  anenberg stanford edu  cs   
december         

problem statement
high energy collisions of protons at the large hadron
collider  lhc  produce massive particles such as w  z 
higgs bosons  and top quarks  a key task in the search
for physics beyond the standard model is to study the
kinematic configurations of these heavy particles  the
massive particles are observed indirectly by the energy
signature they generate  the heavy particles decay into
quarks and gluons which deposit energy on the atlas
calorimeter after hadronization  the collimated stream
of particles produced by the hadronization of a quark or
gluon is referred to as a jet  the goal of the project is
to discriminate between jets that originate from boosted
electroweak bosons such as w boson and top quarks  referred to as w jets  and those originating from light
quarks or gluons  referred to as qcd jets   the motivation to improve jet tagging  classification  is due to
the decrease in performance of standard techniques for
reconstructing the decays of heavy particles with a large
background of ordinary qcd jets  in this project compares the performance of a number of supervised learning algorithms such as svm  fisher linear discriminant 
and random forest in their ability to distinguish between
w boson and qcd jets 

figure    hadronic decay sequence

jet image
the data we consider is a monte carlo simulation of a
top quark decay  the data is formatted as    by    pixel
  d images where the pixel intensity corresponds to the
transverse momentum  pt  of the particle as detected by
the calorimeter  in unrotated image  the axes correspond
to eta and phi  however  pre processing rotates each
image so that the clusters align vertically  as a result  the
axes no longer exactly correspond to rapidity azimuth
plane  eta phi   but to a spatial dimension unique to each
image 

the diagrams on the left illustrate the typical decay sequence of either a w boson or a qcd jet  a  w boson
decay  b  qcd event  note that the  a  w jet is typically composed of two distinct pt peaks  whereas the  b 
qcd jet deposits its energy over a wide region of the
calorimeter as a result of splitting observed in the event 
image curtosy of source   

the first of the following images depicts a w boson
 

fidecay into two quarks  w boson jets exhibit a two prong
structure where each prong corresponds to a quark generated in the decay  the second image is an example of
a qcd jet  qcd jets typically display an asymmetric
intensity pattern  qcd jets typically have a single high
pt peak and a variable number of lower pt peaks 
   

   

   
    

   

    
    

   

   
    

   

   

   

   

q 

   

   

    cell    

   

    
    

   

    
   

   

    

   

q 

q 

   

    

   

    

   

    

    
   

learning curve for svm w  guassian kernel

   

error

    

figure    learning curve for gaussian kernel svm

    
   
   

coefficient

   

   

q 

   

   

   

    cell    
coefficient

the data analyzed in this paper relies upon preprocessing algorithms written by josh cogan  graduate researcher with the atlas group at slac 

   

    

    

    
training size

    

     

blue 
training data  green  testing data  training size of k
indicates that the linear svm was trained on a data set
consisting of k w jet samples and k qcd jet samples 
the total training set size is  k 

classification performance on raw
data
support vector machine
to establish a baseline for classification accuracy  we
study the support vector machine  of the choices
of kernel  the gaussian
kernel svm with k x  z   

exp   x  z    performed the best  the optimize
the choice of parameters  c for l  regularization and
the  coefficient of the kernel  we perform hold out
cross validation  hold out cross validation on c 
                          and                            
resulted in parameters c       and        yielding the
lowest error when tested on the hold out cross validation
set  the learning curve for the gaussian kernel svm is
displayed below 
at best gaussian kernel svm classified the jet image
data with        error 
the learning curves generated for the gaussian kernel
svm illustrate that the model possesses high bias  the
test and training error are both high and at comparable levels  to improve the classification of the svm  we
generate additional features 
the goal of the classification algorithm is to maximize
the ability to distinguish between the signal  w jets  and
background  qcd jets   to evaluate the performance
of a classification algorithm we introduce the measure

known as significance s   t p  f p where t p is the
true positive  the number of times the classifier correctly
classified the signal and f p is the false positive  the number of times the classifier
 classifies the
 background as
signal  we consider the f p because f p is the rms
of the poisson distribution with mean f p   a classifier
that achieves good discrimination between the signal and
background should maintain a high significance value 
the significance value of the gaussian kernel svm with
c       and        is s        

fisher linear discriminant

the classification of quark initiated vs  gluon initiated
jets parallels the gender recognition problem  the similarity motivates the use of the fisher linear discriminant
 fld  facial recognition algorithm on the classification of
jets  the fld objective is to perform dimensionality reduction by finding the direction by which the classes are
most separate  the algorithm generates a classifier with
a linear decision boundary by fitting the class conditional
densities to the fisher criteria of maximizing between
class scatter while minimizing the within class scatter 
for jet classification we only consider the two class fld
algorithm  in this case we have   and   the mean vectors of the two classes  m  and m  are the total number
of samples  x i  for either p
class p
the within class scatter
 
mi
matrix is given as sw   i   j  
 xj  i   xj  i  t
p 
and between class scatter sb   i    i    i   t
where                the goal is to find an orthonormal
projection matrix wopt given by the optimization object

 w sb w  
tive wopt   argmax  w
t s w     when trained on the raw
w
w

jet pixel intensities  the fld produced the following projection onto the one dimensional subspace  in this example we train the fld with a        sample training set 
the green curve corresponds to the quark initiated jets 
blue curve is qcd jets 
 

fifisher  d projection  m       

    

   

learning curve for random forest w      trees

    
    

   

frequency

    
   

   
error

   
   

   

   
    

   

   

   

   
   
discriminant value

   

   

   

receiver operating characteristic  roc 

   

true positive rate

   

    

    

    
training size

    

     

with n t rees       the random forest classifier
performed with       test error and significance of s  
       random forest classifier performed better than
both the svm and fld algorithms 

   
   
   
      

   

   
   
false positive rate

   

feature expansion

   

in order address the bias of the model  we attempt to extract additional features from the data  historically jet
tagging techniques attempt to classify a jet by analyzing
its substructure  this technique attempts to capitalize
on the fundamentally different energy patterns of w jets
and qcd jets  we employ a number of image processing
techniques with the goal of extracting meaningful features from the jet images  for example  we would like to
quantify the number of subjets present in each jet image
where a subjet refers to a clusters of pixels with a pt
much larger than neighboring pixels and the relative pt
of the subjets 

the receiver operating characteristic  roc  curve
which illustrates the performance of fld as the discrimination threshold is varied  when setting the decision
boundary corresponding to signal efficiency of      fld
algorithm performed with       error and s          at
best  this is notably worse performance than the svm
with gaussian kernel 

random forest
the third classification algorithm that performed well
when classifying the jets was random forests  the random forest classifier builds a model by constructing
n t rees decision trees by repeatedly resampling the
training data with replacement  the random forest classifies the test data by returning the consensus vote of
n t rees  every node of a decision tree corresponds to
one of the input features  the edges between a node and
its children give the possible values of that input feature 
each leaf of the tree corresponds to the binary classification of the total sample given the features represented by
the path from the root to the leaf  when constructing
the member trees of the random forest  each node shares
an edge with the best random subset of the features  initial tests revealed the extratreesclassifier implemented
in sklearn performs even better than the standard randomforestclassifier  the extratreesclassifier adds in
an additional layer of randomness by choosing the best
threshold among a set of randomly generating thresholds that are used to determine the best random subset
of features to connect to each node 

canny edge detection

figure    canny edge detection

canny edge
detection identifies the two subjets of a w jet 
the first technique we apply to expand the set of image
features is canny edge detection  by applying edge detection to the images  we hope to more clearly distinguish
the subjets from the background  w jets should exhibit
edges around their two subjets where as qcd jets could
 

fi   whether the jet image contains two subjets 

figure    peak local maximum filter

binary value    if subjet contains two subjets  local maxima     if it contains a different number of
subjets 
   pt of the largest local maxima 
   pt of the second largest local maxima 
image    w jet  the red points denote the coordinates of
the local peaks of the image  the maximum filter merges
regions within the rectangular region to identify the local
maximum 

   difference in pixels between the two peaks with the
greatest
pt values  the difference is given as r  
p
 x   x        y   y     where x and y are the axes
of the image  qcd jets are known to often have
  high pt subjet and one lower pt subjet at wide
angle  thus we might expect that the r for qcd
jets is on average greater than the r for w jets 
   ratio of the pt of the largest peak j  and the second
largest peak j    jj     this feature quantifies the relative size of the two subjets  this measure is useful
since the two subjets of a w jet should have similar
pt values  a qcd jet is more likely to have a single
high pt peak and a second smaller pt peak 

image    qcd jet 
display a variable number of edges  the edge detection
scheme assumes that there is an underlying continuous
intensity function which is sampled at the image points 
the edges are found by computing derivatives of this intensity function   varies the width of the gaussian used
to reduce the effect of noise present in the image  the
choice of      yielded the a filtered image with the most
well defined edges  training the gaussian kernel svm directly on the image filtered by the canny edge detector
or with the additional features appended onto features of
the original image did not improve in the classification of
jet images  in fact  the testing error rate for the gaussian kernel svm  c                when trained on
     samples increased from        to         since the
raw data contains very little background noise  the edges
distinguishing each subjet were already clear  drawing
edges with the edge detector could reshape or rescale the
subjet edges inaccurately  edge detection would generate
more meaningful features when considering jet images
that contain background pt due to energy deposited by
additional proton proton collisions in the event 

the gaussian kernel svm  fld  and random forest
algorithms all demonstrated a decrease in performance
when trained with only the   feature extracted above 
svm performed with       testing error s         
fisher with        testing error s          and random
forest with       testing error and s          however 
appending the additional   features onto the original image data yields marginal performance improvements for
svm and fld  svm performed with       testing error
s         fisher with       testing error s          and
random forest with        testing error and s         
the results indicate that there is potential in extracting
additional features from the image data  however further
efforts are required for significant improvements in performance 

feature selection

we suspect that the intrinsic dimensionality of the data
is much lower than     since all of the jet images from
either class look similar  this insight motivates the discussion of how to reduce the dimensionality of the trainpeak local maximum filter
ing set to leave only those features that are critical to
to ascertain information about the substructure of the the jet classification 
jet image  we employ scikit images peak local max
function to find the coordinates of local peaks  maxima 
principle component analysis  pca 
of the image  peak local max identifies the local maxima by first applying a maximum filter to identify the the goal of pca is to identify the subspace in which the
pixels with large values  potential maxima that are lo  data approximately lies  by projecting the data on the
cated within a pre selected radius are merged together  k principle components  this procedure can potentially
the coordinates of the merged maxima is returned as the extract the most characteristic features from the data 
coordinates of the local maxima of the original image 
performing hold out cross validation on k  the number of
we extract the following   features by using the coor  principle components  we find that optimal choice is k  
dinates of the local maxima 
    however  even with this choice of k  after applying
 

fipca on the expanded data set the testing error of the references
gaussian kernel svm is         which is notably higher
    thaler  jesse  and ken van tilburg  identifying
than the error measured without pca 
boosted objects with n subjettiness  cambridge 
center for theoretical physics  mit 

recursive feature elimination

    scikit learn python library
beyond pca  we attempt to reduce the dimensions of
the features by a number of techniques  foremost  we     scikit image python image processing library
apply recursive feature elimination by using a linear kernel svm to assign weights to each feature  at each iter      sakarkaya  mutlu  fahrettien yanbol  and zeyneb
kurt  comparison of several classification algoation  this feature elimination procedure eliminates the
rithms for gender recognition from face images  isfeature with the smallest weight from a trained svm 
tanbul  yildeiz technical university  n d  web    
the procedure is recursively repeated on the pruned feadec       
ture set until the desired number of features to select is
eventually reached  in addition to being computationally
    belhumeur  peter n   joao p  hespanha  and david
expensive for a feature size of over      recursive feature
j  kriegman  eigenfaces vs  fisherfaces  recognielimination does not achieve any noticeable improvement
tion using class specific linear projection  n p  
in classification  when trained on the reduced data set
ieee transactions on pattern analysis and machine
the svm only manages to achieve        testing error
learning  n d  web     dec       
with s         and fld performs with        testing
error and s          however  random forest observes
an infinitesimal improvement to achieve        testing
error with s         

tree based feature selection
the random forest can also be used to determine the
most relevant subset of features by using the average information gain achieved during the construction of the
n t rees voting decision trees  the sklearn package
is used to implement this procedure  when trained on
the extended training samples  the tree based procedure
reduced the number of features to      this reduction in
features allowed the classifiers to run more quickly and
still demonstrate strong performance  in fact  the classifiers on performed better when trained on the reduced
data set  the svm performed with        testing error and s          fld performed with        testing
error and s          and random forest performed with
      testing error and s         

conclusion
in this project the gaussian kernel svm  fisher linear
discriminant  and the random forest classifiers were compared in their capabilities to discriminate between w and
qcd jets  results indicate that feature expansion techniques motivated by insight into the physical data can
improve classification  however  identifying the most significant features still presents a problem  feature selection techniques demonstrated a variety of results  although most feature selection procedures did not yield
significant improvement in classification  they still improve the speed of the classifiers  overall the random
forest and gaussian kernel svm classifiers performed the
best 
 

fi