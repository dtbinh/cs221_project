see click predict fix
aileen  chen  
achen   stanford edu  

introduction
    is a method for people to file complaints
for local issues such as graffiti and potholes for
the local government to fix  citizens may view 
vote for  and comment on issues that they are
interested in or find important  municipal
governments are interested in predicting the
number of views  votes and comments on a
given issue  so that they can better respond to
and prioritize important problems that people
are facing  in this project  we used machine
learning techniques to extract features and
predict the values of these three counts  we
then evaluated our model by comparing our
results to other teams results on the kaggle
data prediction platform 

data set
the datasets that we worked with are provided
by
kaggle
 www kaggle com c see clickpredict fix   each example represents an issue
posted to the website and contains the
following fields 
   longitude  the longitude where the
issue occured  e g           
   latitude  the latitude where the issue
occured  e g            
   summary  a short text summary of the
issue  e g  very dangerous and deep
hole in sidewalk
   description  a longer text explanation
of the issue  e g  this massive hole in
the sidewalk continues to trip people
and causes many falls and near falls on

david  ye  
davidye stanford edu  

a daily basis  it continues to get bigger
and only gets more dangerous with each
passing day
   source  a categorical variable indicating
where the issue was created  e g 
iphone
   time created  the time and date when
the
issue
originated 
e g 
                 pm
   tag type  a categorical variable
 assigned automatically  of the type of
issue  e g  pothole
the training examples also contains these target
values 
   number of votes  the number of usergenerated votes
   number of comments  the number of
user generated comments
   number of views  the number of views
the training set has         examples that
were created between            and            the test set has         examples that
were created between            and            the distribution of the number of
views  votes  and comments in the training set
are heavily skewed towards the lower counts 
with     of the training set having   views 
    having   vote  each issue has at least  
vote   and     having   comments  there is a
long tail in the number of views  with the
maximum being        similarly  the
maximum number of votes is     and the
maximum number of comments is    

fievaluation metric
the evaluation metric defined by kaggle was
the root mean squared logarithmic error 



n is three times the number of test
examples  since there are three values to
predict for each issue 
pi is our predicted value
ai is the actual value

   summary  categorical feature  there
were    summaries in the training set
that were found in over     samples 
the summaries were normalized by
lower casing and removing punctuation
and spaces  these were typically short
summaries describing common issues 
such as tree debris and graffiti on
private property 

we modified the given dataset by applying the
function f x    log x      to the target values 
this allowed us to use the more standard
rmse evaluation metric when training our
models  to convert back into the original target
values  we applied the inverse function g x   
ex   before submitting our predictions to
kaggle 

   description  categorical feature  there
were    descriptions in the training set
that were found in ove     samples  the
descriptions were normalized by lower
casing and removing punctuation and
spaces  many of these were descriptions
created from automated processes  such
as this issue was reported to the city
of oakland public works agency via
phone
               
email
 pwacallcenter oaklandnet com  
or
web  www oaklandpw com  

basic features

   description
length 
continuous
feature  the number of characters in the
description 




we extracted a few basic features from the
dataset that were used with little to no
processing 
the
distinction
between
categorical and continuous features that we
describe below only apply when transforming
these into binary features 
   source  categorical feature  there were
   distinct sources in the training set 
   tag type  categorical feature  there
were    tag types in the training set with
over    samples 
   longitude and latitude  continuous
feature 
   creation time  categorical feature  we
used the day and hour of the week 

advanced features
we also constructed more advanced features
that required additional processing 
   k means location cluster  categorical
feature  we applied k means clustering
to the longitude and latitude coordinates
of the issues to cluster issues together
by physical location  with a k      we
identified
 
distinct
cities
 
neighborhoods where the majority of
the issues came from  the k parameter
was determined by running the
algorithm with different values for k
until we found clusters that appeared
physically distinct from each other 

fi   naive bayes text classification
probability  continuous feature  to
capture information from the remaining 
unique summary and descriptions  we
built a multi class naive bayes text
classifier with a multinomial event
model and laplace smoothing  we split
the value for views  votes  and
comments into buckets  these buckets
were used as the classification classes 
the words in the summary and
description fields were combined to use
as features for the classifier  ultimately 
this outputs three integer features for the
regression learner  each integer
represents the naive bayes bucket
prediction for the number of views 
votes  and comments 
when extracting the text features from
the summary and description fields  we
first converted the text to lowercase
and
removed
punctuation
and
stopwords  another consideration was
how to create the buckets for the
classification classes  a small number
of buckets  around three to five 
worked best because the predicted
posterior probabilities for each
individual class werent too small to be
meaningful  we also tried creating
equal sized buckets  but this performed
slightly worse than creating buckets at
equally spaced percentiles in the
training counts distributions  since they
were heavily skewed towards the lower
numbers for votes and comments 

feature importance
using decision trees  we were able to get
estimates of feature importances for a set of
features  this allowed us to iterate quickly by
trying
new
features
and
removing

uninformative features  these were the feature
importances for vote predictions on the final set
of features we ended up using 
feature
source
tag type
longitude
latitude
creation day of week
creation hour of day
summary
description
description length
k means location cluster
nave bayes text classification

importance
      
      
      
      
      
      
      
      
      
      
      

dataset pruning
since our task is to predict future values based
on past data  we hypothesized that most recent
training examples would be more relevant and
that we should prune or discount older data 
for example  the number of views  votes and
comments may increase steadily over time as
the number of     users in a community
increases  to test this hypothesis and to find
the optimal number of months of training data
to use  we used the most recent   months of
training data as our validation set  and trained
on the previous   to    months of data 

fiusing the most recent   months of training data
achieved the lowest rmsle on the validation
set  it also improved our rmsle on the test set
dramatically by       

regression models
linear regression
the first model we applied to our data was
linear regression  for the categorical features 
we transformed each categorical value into its
own binary feature  for the continuous features 
we transformed each feature by bucketing them
into ranges where each range contained roughly
the same number of examples  the bucket ids
were then used as binary features  this model
achieved an rmsle of         on the training
set and        on the test set 
support vector regression
we also tried support vector regression with an
rbf kernel on the same binarized feature set
that we used in linear regression  this model
achieved an rmsle of        on the training
set and        on the test set 
gradient boosted decision trees
finally we then tried applying gradient boosted
regression trees  since decision trees can
naturally segment the feature values for us  we
did not need to binarize the features as we did
in the previous two models  using     trees
with a depth up to   for each tree  we were able
to obtain a training set rmsle of        and a
test set rmsle of        
for all the models the test error was lower than
the training error  we hypothesize that this is
because the test examples have a different
distribution than the training examples  and

contain a larger proportion of examples that are
easier to predict 

results summary
the gradient boosted regression tree model
performed the best out of the three models  the
official kaggle competition ended over
thanksgiving break but we were still able to
submit results after the competition  here is
how we would have done relative to other
teams modeling the same data  for reference 
the winner of the competition achieved a test
rmsle of        
train
rmsle
gradient boosted       
regression trees
svm
      
linear
      
regression

test
rmsle
      

kaggle
ranking
        

      
      

        
         

fi