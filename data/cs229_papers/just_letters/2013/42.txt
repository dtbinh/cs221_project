automatic product classification and clustering solutions in a retail context
rohit kaul and rajiv bhateja
abstract  in this report we propose a methodology to automatically classify products and cluster similar
products together  this enhances user interaction and performance metrics for e commerce  product shopping 
websites  with applications in product search  site navigation  product comparison  etc  for this project  a variant
of the multinomial naive bayes algorithm was used for classification  we present our findings of key aspects that
demonstrate its accuracy  a clustering methodology was developed to handle large data sets  overcoming the n  
complexity  in a two step approach  the first step of clustering utilizes locality sensitive hashing  the second
step uses a shingles matching method  we describe a methodology that allows trading cluster size versus
similarity of items within a cluster and propose a way to use clustering to increment classification accuracy 

  introduction
the topic of product classification and clustering is central to providing a good user experience  intelligent search 
and revenue for e commerce web sites  companies like ebay and amazon do not require the merchants to provide
a product category  it is automatically deduced  allowing scalability and reducing deliberate or unintentional
mis classification  thus enhancing user experience  customer satisfaction and higher transaction rates 
there are several challenges that need to be overcome in order to build a robust product classifier and clustering
algorithm  starting with developing a consistent taxonomy and a data set sufficient for validation  clustering is an
o n   problem  and requires computationally efficient solutions for large data sets  small clusters  though very
strongly tied  are not as interesting  or useful   but larger clusters run the risk of introducing odd balls that make
the cluster inconsistent 

  data collection
product data was obtained by crawling an existing e commerce website until we obtained approximately a million
items from downloaded web pages  after writing parsers to extract title  description and product categorization
from the downloaded pages  the extracted data was then cleaned  stemmed  converted to lower case  and cleansed
of stop words   downloaded products were not unique since multiple merchants could provide the same
products  and also due to overlap of products across different urls  all the downloaded products  including
duplicates  were used for clustering in order to verify performance on a large dataset  for classification  we
limited the data sets to approximately        items for training and        products for cross validating the
classifier accuracy 

  classification
    i methodology
for classification  our objective was to extract the product taxonomy from the downloaded products  form the
taxonomy tree  and build a classifier to map products to the top category level  l   with    classes and to the
second level  l   with     classes  we started with a simple multinomial naive bayes algorithm  kib   and
evolved to an extended model  puu    with each experimental step  we updated the feature set to incrementally
improve classifier accuracy 
our simple multinomial naive bayes model takes the form 

p t ic    p  wnc f

n

n

where  p ti c  is probability of a test document i in class c  p wn c  is the probability of a word n  given class c  and
fn is the count of word n in the test document  the extended model uses tf idf  term frequency  inverse

 

document frequency  in the form  f n log   

wn
a 

s  w 

 

   

 
m
log max    a  
 a 
mn
s  w 

  

 

where  s w  is the number of unique words in the document  m is the number of training documents  mn is the
 

finumber of documents in which word n occurs  and a 
effects length scaling and a  affects idf weights  for the
results below  a  and a  were set to   and    respectively 

evaluating bias

using a training set of    k  we evaluated bias  and the
classifier showed excellent accuracy on the training data 
adding higher number n grams was marginally useful  so
we limited our validation to bi grams  based on concerns
of over fitting and computational cost 

     

    ii validation

l      

l      

classification accuracy

 
     
     
     
    
     
 

 

 

for validation  we started with a set of         products 
n grams
the most notable trials to improve l  and l 
classification accuracy are described below  a key factor was the fraction of tokens not seen during training 
trial

description

l  l  test
acc  acc  size

 

video games
toy s games
sports gear

 

single tokens  title  simple mn bayes  smnb 

               

 

unigrams  bigrams  title  simple mn bayes

               

 

unigrams  bigrams  title  description  smnb

               

jewelry  watches

   unigrams  bigrams  title  description  mn bayes
tf idf

               

health beauty

 

same as    but limit to products with unknown
unigrams  bigrams less than    

               

 

same as    but limit to products with unknown
unigrams  bigrams less than    

               

 

same as    but limit to products with unknown
unigrams  bigrams less than    

              

petsupplies
off icesupplies
musical instruments
home garden
gif ts  flowers
electronics
computers sof tware
clothing
babies kids
automotiv e
appliances

 

               

 

l  classification accuracy for trial   

  clustering
n  clustering over a million items is prohibitively expensive  therefore we researched various approaches that can
first segment the products into large  reasonably similar clusters  in order to segment the data into first level
clusters  super clusters   we experimented with using k bit locality sensitive hashing  lsh   ull   on product
titles to produce a consistent hash even if some of the terms used to produce the hash were different  lsh
represents similarity  s  between product titles  ti  using probability distributions over hash functions  h ti  

s  t    t     ph h  h t    h t    
lsh is computed by combining hashes for different tokens at a bit level  for lsh  we used each keyword as a
token  each bit place holder is multiplied by  weight if the bit     and  weight if the bit      for this project  we
used a    bit hash  this resulted in an array of doubles of size     which we then mapped back to an   byte
integer  sum weight          bit and sum weight          bit for all    bits   the hamming similarity  vil  
between two n bit vectors x and y is defined as  hs x y    count   xi   yi   and the hamming distance is given
by     hs x  y    our goal was to minimize the hamming distance for related product titles so that they could be
clustered easily  in an ideal case  the hamming distance between related products would be zero 
for computing lsh  a gaussian weight function based on the token frequency  f  across all products was used 
 

fithe intuition here is that if a token was very specific  e g   measure of size  or material type  we did not want it to
increase clustering distance from other similar items  at the same time  if a token was too common  e g   color  

 
w  x  k e


 log  f   
 
 

 

that would also skew the clustering  therefore  the weight of a token x  was chosen as
 
where k is a scaling constant  and  and  affect the offset and the shape of the gaussian curve  respectively  if
the gaussian weight function is fatter  then the lsh hash is influenced by a larger number of tokens  and if its
narrower  then effectively a smaller number of tokens affect the final hash 

    top level clusters
the primary motivation behind super clusters is to reduce the n  problem from n o  m  to   o  k   the
clusters produced need to have related items  but produce as many large clusters as possible  which can then be
subsequently used for producing sub clusters  however  as the cluster size increases  the similarity of products
inside the clusters reduces  and needs to be balanced with the desire for larger clusters  therefore   and  in the
a 
a 
above equation need to computed to maximize the following score   clustersizeavg clustersimilarity avg  

    i training
if  a     a    it will produce super clusters that are large with fewer similar items  and vice versa  as a measure of
cluster similarity  we use average shingles based similarity  see     below   the values of  and  to use is
determined numerically  by finding the maximum score on a training data of        random examples  

avgerage category entropy

 super clusters 

   

   

   

   

   

   

   

avg entropy

avg entropy

 super clusters 

average category entropy

level  
level  

   
   
   

level  
level  

   
   
   
   

 

 
 

                                        
cluster size

a       a       l  entropy avg       

 

   

   

   

   

   

   

cluster size

a       a       l  entropy avg       
 

fi    ii testing
in order to measure the outcome of super clusters  we can compare the entropy of category distribution 
 pi log  pi    entropy max   a higher entropy implies lesser similarity within a super cluster  we test
icategory

two cases  a    a      and in the second case  a      and a       the max cluster size in first case is         not
shown in graph to reduce clutter  compared to       for second with     higher entropy than the second case 
given the primary motivation behind super clusters  we decided to use a    a      

    second level clusters  sub clusters 
the sub clusters are the final clusters  and need to be very similar products  subclusters are computed within each
super cluster using shingles matching approach  its is comprised of n min hashes of a given title  run with a
different hashing function each time  the similarity between two shinglehashes is a measure of jacccard
similarity  ab  ab   for this project  we used   hashing functions  all pairs of items within a super cluster
were compared  and the ones that exceeded a threshold were used to form a sub cluster 

    i training
the key parameter is to determine the ratio of shingles match between two items  for this testing  sub clusters
from all super clusters were computed for a varying degree of shingle match  and the average l  category entropy
across clusters was computed to choose minimum number of matching shingles such that the entropy is below a
certain threshold  as the number of matching shingles decreases  the entropy increases and the cluster size
distribution gets a fatter tail 
    

cluster distribution
 function of shingles match fraction 

    

    
     
   
     
    

      

   

      

    
 
 

 

 

 

 

 

 

matching shingles

 

 

 

number of clusters

average entropy

   

      
      
      
     
 
 

 

 

 

 

 

 

 

 

  

cluster size

    ii testing
given the primary motivation behind sub clusters  we decided to conservatively choose at least     to match  the
testing approach was the same as for super clusters  but from the graph below  it is clear that even the l  category
entropy within each sub cluster  on an average  is extremely low  which is a good measure of the quality of
similarity produced 

 

fiaverage category entropy
 sub clusters 

level  
level  

    
     

avg entropy

    
     
    
     
    
     
 
 

  

   

   

   

   

   

   

sub cluster size

average sub cluster category entropy

final cluster sizes  log scale 

    future work
for this project  clustering and categorization have been independent of each other  one of the key factors for
accuracy of the classification are the unknown tokens present in the product title and description  one option is to
increase the training data  or alternatively  to add related tokens from existing training set  given that sub clusters
have near zero l  category entropy  the tokens within a cluster can be mined to reduce the unknown token ratio
when required  also  classifying a product into an l  category first and then restricting l  categories to only the
subset of l  category can also be explored 

  references
 puu   puurula  antti  combining modifications to multinomial naive bayes for text classification  springer
link  lecture notes in computer science volume             pp         
http   www cs waikato ac nz  asp   publications puurula   c pdf
 kib   kibriya  a m   frank  e   pfahringer  b   holmes  g   multinomial naive bayes for text categorization
revisited  in  proceedings of the   th australian joint conference on advances in artificial intelligence  ai    
berlin  heidelberg  springer verlag        pp        
 vil   villaca  r s   paula  l b   pasquini  r  and magalhaes  m f   hamming dht  taming the similarity
search http   www facom ufu br  pasquini artigos ccnc      pdf
 ull   rajaramanhttp  anand and ullman  jeffrey david  mining of massive datasets
http   i stanford edu  ullman mmds ch  pdf

 

fi