personalized  web  search  re ranking  
santanu  dey                      conrad  roche  

abstract  
the   intent   of   a   search   is   context   dependent   and  
cannot   be   fully    realized    by   the   search   term   used  
by  the  user  for  performing  the  search   adding  more  
context   by   using   a   users   search   history   and  
additional   details   can   help   us   to   personalize   the  
ranking   of   the   search   output   and   help   the   search  
results   to   be   more   relevant   for   the   user    search  
engines   should   not   rank   the   results   the   same   for   its  
entire  user  base   but  must  factor  in  and  personalize  
the  ranking  based  on  the  user   

introduction  
the   source   of   the   project   is   a   kaggle   challenge   by  
yandex  and  is  publicly  available        
the   challenge   is   to   personalize   the   rankings   of   the  
urls  of  a  search  result  based  on  the  search  history  
of   user    the   search   history   considered   would   be  
both   the   long term   user   search   history   and   also   the  
short term   current   search   session   context   history   
the   purpose   of   the   re ranking   is   to   present   the  
search   results   most   relevant   to   the   user   based   on  
the  user s  intent   

related  works  
several   researchers   have   worked   on   techniques   to  
personalize  web  search  result  ranking   many  of  the  
related  works  are  listed  on  kaggle        

data  set  description  
yandex   has   provided   datasets   containing   user  
session   information       including   the   user   queries   
urls    url   rankings   and   clicks    the   relevance   of   a  
search   result   is   measured   by   the   user s   dwell   time  
on   the   page    if   the   dwell   time   exceeds   a   specific  
threshold    the   result   is   considered   relevant    the  
data  contains  about     m  records     

the  dwell  time  is  the  time  between  a  click  and  the  
subsequent   click   or   query    a   higher   dwell   time   is  
proportional   to   the   result   relevant    a   dwell   time  
higher   than   a   specific   threshold          time   units   in  
this  case   signifies  a  satisfied  click   
the   search   data   is   sampled   from   users   of   a   single  
large  city  and  is  around  two  years  old   the  top k   k  
unknown   most  popular  queries  are  removed  from  
the   data    additionally    queries   with   commercial  
intent   are   also   removed   from   the   data   set    the  
training   data   contains        days   of   search   and   the  
test  data  contains     days  of  activity   
the   data   is   provided   as   a   user   activity   log       which  
contains  session   query  and  click  action  data  for  all  
users    the   data   is   anonymous   to   maintain   user  
privacy  and  only  contains  numeric  identifiers   each  
type  of  record  uses  a  different  format  adding  to  the  
challenge  of  extracting  the  data   

analysis  of  dataset  
basic  characteristics  of  the  dataset   
characteristic  
days  
session  cnt  
unique  users  
query  count  
unique  terms  
unique  domains  
unique  urls  
click  count  
total  records  

all  
    
            
           
            
           
           
            
            
             

train  
    
            
           
            
           
           
            
            
             

test  
   
         
         
         
         
         
           
         
           

in   order   to   understand   that   dataset    the   figures  
below  show    
a  number  of  sessions  against  the  number  of  users  
b  number  of  sessions  against  number  of  queries  
c  number   of   sessions   against   the   number   of  
search  engine  results  page   serp     
d  number  of  serps  against  the  number  of  clicks   

fi

 a 

 b 

as     ranking   svm    list wise   ranking   is   out   of  
scope   
given  that  the  training  and  test  data  volume  is  
large    we   would   want   to   scale   our   models   and  
algorithms   to   be   run   efficiently    whenever  
applicable   data  will  be  sampled  randomly   

the   ranking   result   is   evaluated   by   yandex   internally  
using  the  ndcg   normalized  discounted  cumulative  
gain   score  which  uses  the  ranking  of  each  url  for  
each   query   against   the   known   grade   and   then  
averaged  over  all  the  queries   

 c 

 d 

we   observed   that   more   than   half   of   the   queries  
       have  three  or  less  terms   around       of  the  
sessions  contain  more  than  two  queries     

methods  
we   used   multiple   machine   learning   techniques   to  
determine   the   best   ranking   for   the   users   current  
search   some  of  the  approaches  we  thought  of  are   








determine   users   historical   engagements   with  
respect   to   a   domain   and   urls   within   the  
domain    the   goal   is   to   represent   user   as   a  
vector   of   interests   where   each   element  
represents   interest   in   some   aspect   such   as  
domain   url     
determine   the   association   between   query   and  
domain  and  urls  within  the  domain   historical  
performance   is   manifestation   of   intrinsic  
quality  of  the  domain  and  url   
finally   we   would   like   to   come   up   a   set   of  
feature  vectors  based  on  everything  mentioned  
above      we   define   the   loss   function   based   on  
clicks  as  feedback     
learning  to  rank   we  plan  to  use  both  pairwise  
and   point   wise   approach   with   loss   function  
modifies   for   ranking    for   point wise   approach   
we  plan  to  employ  logistic  regression  and  svm    
in   subset   ranking       for   pairwise   ranking    we  
would   use   svm   based   approach   such  

work     results  
training  set  generation   
the  intuition  is  that  historical  click  through  rate  and  
dwell  time  can  predict  how  users  are  going  to  be    
first   we   generate   features   to   represent  the   search  
results  vectors   
variable  name  
user  domain  
historical  click  
thru  
user  url  
historical  click  
thru  
query  domain  
historical  click  
thru  
query  url  
historical  click  
thru  
user  domain  
historical  average  
dwell  time  
user  url  average  
dwell  time  
query  domain  
average  dwell  
time  
query  url  
average  dwell  
time  

description  
this   ratio   represents   historical   click  
through   rate   of   a   use   given   the   domain   
this  is  orthogonal  to  query  
this   ratio   represents   historical   click 
through   rate   of   the   user   given   the   url   
this  is  orthogonal  to  query  
this   ratio   represents   the   historical   click  
through   rate   of   the   query   given   the  
domain   this  is  orthogonal  to  users   
this   ratio   represents   historical   click 
through   rate   of   a   query   given   the   url   
this  is  orthogonal  to  users  
total  time  spent  by  users       count  of  clicks  
that  ended  in  that  domain  
total  time  spent  by  users       count  of  clicks  
that  ended  in  that  url  
total   time   spent   in   the   domain   by   all  
users       count  of  clicks  from  the  query  by  
all  users  
total  time  spent  in  the  url  by  all  users       
count  of  clicks  from  the  query  by  all  users  

mean  average  precision   map     
the   training   does   not   have   any   relevance   labeling  
except   click   through   information    so   the   binary  

firelevance  model  is  assumed  as  a  basis     this  means  
if  a  search  results  page  has  n  clicks   the  best  ranking  
function   will   have   those   n   urls   will   be   in   the   top     
as   part   of   point wise   approach    the   clicks   are  
modeled   and   predicted    since   binary   relevance   is  
assumed    mean   average   precision   is   used   to  
evaluate   the   different   approaches    in   the   process  
broader  metrics  such  as  precision  and  f   measures  
are  looked  into     
as   a   starter    map   of   the   data   is   calculated   to   be  
      and  it  establishes  baseline  value  of  map   

point  wise  approach  
as   explained   by   cossock                  a   regression  
based   approached   is   used   to   predict   likelihood   of  
clicks   given   the   search   result   page       in   this  
simplified   approach    the   relevance   of   a   page   is  
solely   determined   by   whether   the   user   clicked   on  
the   link   or   not       it   is   also   expected   that   search  
engine  will  rank  urls  based  on  likelihood  of  a  page  
being  clicked  by  users   thus   if  the  click  probability  
can  be  calculated   it  can  be  used  to  rank  the  urls     
there  are     separate  approaches  implement  point 
wise   ranking    first   a   logistic   regression   model   is   fit  
to   predict   click   probability    second    a   maximum  
margin  classifier   liblinear  svm   is  used  to  separate  
clicks   with   non clicks   urls       then   the   distance   from  
the   separation   line   is   converted   into   probability  
measure  for  ranking        
in   both   cases    the   target   was   set   to       or       based   on  
click   or   non clicks       if   there   were   multiple   clicks   in  
the  page   all  of  them  are  considered   once  labeled   
each   urls label   combination   becomes   an  
independent  observation  within  a  page     

classification   situation    measuring   predicted   labels  
with  actual  labels  for  all  the  urls  in  the  page     even  
though  this  approach  is  simple  and  straightforward   
it   has   shortcoming   in   ranking   in   web   search   since  
no   of   clicks   in   a   page   is   going   to   be   limited   even  
though   the   predicted   labels   can   all   be   highly  
probable   on   the   page       this   is   addressed   in   pair 
wise  approached     
in   summary    point wise   approach   is   measured   by  
the  following  metrics    




accuracy  
precision recall  
f   score  

role  of  sample  size  
given  that  the  big  data  volume  and  high  complexity  
in   terms   of   space   and   computation   time    data   has  
been  sampled  for  both  training  and  testing       
sample  
size  

svm  

logistic  regression  

f  score  

precision  

recall  

f  score  

precision  

recall  

         

        

        

        

        

        

        

         

        

        

        

        

        

        

         

        

        

        

        

        

        

         

        

        

        

        

        

        

         

        

        

        

        

        

        

         

        

        

        

        

        

        

  
as  specified  below    a   shows  f  scores  vs   sample  size   
green  line     logistic  regression  and  blue  line  is  svm     

evaluation   
when  prediction  is  made  on  test  set   first  the  urls  
are   sorted   by   click   probability    probability   for  
logistic   regression   and   distance   from   separator   for  
svm        evaluation   can   be   done   as   a   typical  

          

            
 a   

as   specified   below     b    shows   recall   vs    sample   size   
green  line     logistic  regression  and  blue  line  is  svm   

fi                

it   minimizes   the   number   of   pairs   of   training  
examples   that   are   swapped   w r t    their   desired  
order   
in   this   approach    prediction   will   limited   to  number  
of   clicks   in   actual   data    this   means   if   we   have   n  
number   of   clicks   in   the   page    there   will   be   only   n  
predicted  labels  set  to     and  those  are  expected  to  
be  in  the  top   this  is  similar  to  point wise  approach  
except   output   labels   are   restricted   to   underlying  
data     

  
 b   

tuning  hyper parameters  of  the  model  
in  order  to  find  best  hyper parameters  for  both  logistic  
regression   and   svm    a   grid search   has   been   conducted  
with     fold   cross validation    heres   the   summary   of   the  
final  model  parameters   
logistic  regression   stochastic  gradient  descent   





alpha    e       constant   that   multiplies  
regularization  term   
loss     modified  huber  loss     instead  of  log     
regularization l 

the  

                                                  source     scikit learn  website  
  

then  based  on  no  of  clicks  in  the  actual  data  of  the  
page   test  data  is  labeled  with  clicks   for  instance   if  
page  had     clicks   it  is  expected  the  top     positions  
in   the   ranked   page   will   have   clicks   label   set   to       
other  will  be        

  

ranking  svm  was  not  used  with  default  parameters   

pairwise  approach   
ranking   svm    svm rank   software   developed   by  
joachims               has  been  used   svm rank  solves  
the   quadratic   program   of   the   roc area  
optimization     

          

precision     recall     f  score  

information   retrieval    specifically   binary   relevance  
model    measures   such   as   mean   average   precision  
 map    would   be   better   suited   to   determine   the  
quality   of   prediction  in  this  case    map  takes  ranked  
position   into   account   which   is   very   critical   given  
that   we   are   expecting   all   the   clicks   to   happen  
starting  from  the  top  of  search  result  page   

kernel   linear  
loss l    squared  hinge  loss     
regularization l   
c         penalty  of  the  error  term   

                

since   only   output   labels   are   restricted   to   match  
input   click   counts    no   of   clicks   and   no clicks   in  
actual   data   will   be   same   as   predicted   data    so   in  
this   case    precision    recall   and   f    score   will   have  
same  values  


svm   liblinear   





evaluation    

  

pairwise   approach   needs   to   distinguish   between  
pairs   of   urls   in   the   page    a   simple   approach   would  
be   dividing   the   urls   into       sets    one   for   click   and  
other   for   no click    then   pairs   are   composed   by  
taking   one   url   from   each   set    this   is   will   be  
functionally   same   as   point wise   approach   as   that  
was  dealing  with     sets  of  urls  and  trying  to  find  a  
line   of   separation   between   them   in   process       as   it  
turns   out   a   majority   of   pages   have   more   than      
click   and   dwell   time   for   each   click   can   be  

fidetermined  unless  its  the  last  click  in  the  page   so  
we  can  use  dwell  time  as  a  proxy  of  relevance  and  
differentiate  between  clicks   for  the  last  click   it  can  
be   assumed   to   be   relevant   as   the   user   got   what   is  
needed  and  left  it  afterwards   
so   all   the   clicks   can   be   ranked   by   dwell   time   and  
non clicks   are   assumed   to   be   all   equal       with   this   
ranking  svm  is  constructed  with  pairs  of  urls     

results  
in  case  of  point wise  approach   without  considering  
click   restriction     svm   seems   to   do   better   in  
precision   and   logistic   regression   does   better   in  
recall   
in   overall   comparison    based   on   the   results    it  
seems   like   point wise   logistic   regression    with  
modified  huber  loss   can  get  the  best  result  given  
the  dataset  and  the  problem     
model  
type  
point 
wise  
point 
wise  
pair wise  

model  

mean  
average  
precision  

precision f
  score  

accuracy  

logistic  
regression  

        

        

            

svm liblinear   

        

        

        

ranking  svm  

        

        

          

training  size          million  test  size        k  

conclusion  
as   far   as   click through   modeling   is   concerned   
point wise   approach   gives   better   results   than   pair 
wise   this  would  be  a  good  thing  if  the  goal  of  the  
search   engine   is   to   generate   as   many   clicks   as  
possible   but  in  reality   often   the  goal  is  to  provide  
relevant  information  so  that  the  user  can  get  what  
is   needed   in   the   first   click       but   objective   function  
should   consider   broader   relevant   signals   such   as  
dwell   time   and   optimize   the   model   based   on   it   
which  is  what  ultimately  matters  to  the  users   

references  
      yandex     n d      personalized   web   search  
challenge    retrieved   from   kaggle   website   

http   www kaggle com c yandex personalized 
web search challenge  
      yandex     n d      personalized   web   search  
challenge      related   papers    retrieved   from   kaggle  
website   
http   www kaggle com c yandex 
personalized web search challenge details related 
papers  
        milad   shokouhi    ryen   w    white    paul   n   
bennett    filip   radlinski    fighting   search   engine  
amnesia    reranking   repeated   results    sigir          
         
      carsten   eickhoff    kevyn   collins thompson    paul  
n   bennett   susan  t   dumais   personalizing  atypical  
web  search  sessions   wsdm                  
      hongning   wang    xiaodong   he     ming wei  
chang    yang   song    ryen   w    white    wei   chu   
personalized   ranking   model   adaptation   for   web  
search   sigir         
      chris   manning    pandu   nayak    prabhakar  
raghavan    cs      lecture   notes    retrieved   from  
stanford  website   http   cs    stanford edu  
      david   cossock    tong   zhang       subset   ranking  
using  regression  
  
     john  c   platt     probabilistic  outputs  for  support  
vector   machines   and   comparisons   to   regularized  
likelihood  methods          
      t    joachims    training   linear   svms   in   linear  
time    proceedings   of   the   acm   conference   on  
knowledge  discovery  and  data  mining   kdd           

fi