is this a joke 
jim cai   and nicolas ehrhardt 
  jimcai stanford edu
  ehrhardn stanford edu

december         

abstract
the deconstruction of humor in text is well studied in linguistics and literature  computational techniques center around
modeling text structure and hand crafting features surgically designed to emulate linguistic tendencies in specific types of
jokes  we take a more hands off approach  leveraging light linguistic features in conjunction with learned semantic word
vectors in a neural network to learn humor  the system performs at a reasonable level and the extensions are promising 

i 

introduction
d     

he role of humor in text is arguably different
than in speech  what is lacking in intonation
and timing is made up for with an emphasis
in meaning  there are many reasons why a certain
sentence can be humorous but rather than conduct a
linguistic analysis  perhaps we may be able to computationally determine a humor metric 

t

ii 
i 

 ten  first  two  home  living    
 husband  wife  friend  actress  wrestling
 cold  single  big  heavy  upper
d      
 ten  five  six  seven    
 husband  father  wife  mother  son    

features

 cold  natural  sea  big  earth  heart    

embeddings

while still not entirely perfect upon inspection it
is conceivable that these words are related  especially
on the kinds of co occurences with other words that
they share  this is what is endowed by the neural net
that they were derived from  turian et al      also
study these embeddings extensively and find that
they perform well in various word representation
tasks 

collobert et al      trained a set of    dimensional
word embeddings using a neural network with hidden layers  the scoring function compared pairs of
phrases of a fixed size  where the negative example was the same as the positive example with the
target word replaced with a random word  before
we utilized this embedding for our task  we were
investigated the degree to which these embeddings
captured semantic meaning 

ii 

inspired by erk and pado     we intend to derive a
sense of words into a sentence with respect to the
parts of speech of adjacent words  to do so  we model
a words contextual information alongside its word
embedding as 

we applied pca on the set of word embeddings
to reduce the computation time  then  we ran
the nearest neighbors algorithm which provided a
straightforward method to visualize the data structure  k nn was run on the first d      principal
components  with d chosen empirically  for comparison  the results for using the first      principal
components are as follows 
 cs
 cs

context

 
  
  r
x   rx   vx  

where v x  v   our word embedding space and
    r   are applications of r  v  role space
r
x
x

   n  cs    
   

 

fi vector space   the role space consists of an ndimensional vector that represents the contribution
of a particular tag before after a certain word  depending on a particular words neighbors  we extract
the mean contribution of occurences with such tags 
this will vary depending on the training corpus 

where
c
r     
 m

nd

h

h

  wk i     uk 

i    k   

 

k   

adding r is equivalent to placing a gaussian
prior on the parameters to help with overfitting 

mitchell and lapata     discusses various tradeoffs in combining semantic word vectors  they argue
that composition by dot product is effective because
it captures the commonality between sentences and
accentuates it  however since we would like to keep
different sources of variability in the vector  we have
chosen to take the mean of the word vectors that
have a particular tag  for this work we utilize the
brown corpus and pos tagger provided by pythons
nltk library 

ii 

hidden neural network

taking the word level features seen before as raw
input  we utilize a   layer hidden neural network to
perform the classification  the topmost neuron is a
softmax layer  outputting a classification          the
middle layer is the hidden layer  and we illustrate the
paths of the input layer with only the first two words 

one issue with this framework is data sparsity
more specifically for a word to have useful values of
this feature it would require having been observed in
its different contexts  if applicable   another issue is
in the evaluation stage  if the test sentence exhibits a
new pos tag in this case we utilize a naive back off
model that looks for the minimum euclidean distance
to the vector of any tag to use as the feature 

iii 
i 

model

one layer hidden neural network

goal
as such  the parameters to our hidden neural
network model are as follows 

in this section  we follow a similar problem formulation to      given parameters  for our model  we
arrive at an estimated probability of a sentence being
funny  we seek to maximize the following probability 

  u  rh
  w  r h nd

m

  b   r h

i

  b   r

l       log  p y i    x  i      
which simplifies to
m

 y 
i

 i  

 i  

 i  

h is the number of hidden layer neurons  n is
the number of input words that have d dimensional
feature vectors  for a particular neuron in the hidden
layer  it receives information from the input layer and
outputs 
a    f  w   x   b   

 i  

log h   x         y   log     h   x   
 z
 
li     

where h   x   is the output of our classifier 
since we will utilize stochastic gradient descent
 sgd  to estimate our parameters  we thus seek to
minimize
j       

 
m

where f   x   is a nonlinear sigmoid like function 
the softmax layer then takes each hidden neuron as
an input and produces a classification according to 

m

 li        r     

z   u t a   b 

i   

 

fiwhere a is a vector of outputs from the hidden
layer  finally  the topmost neuron returns 
g z   

our final dataset consists of     high quality puns 
negative examples are sentences taken from the constitution by nature of it being a serious document 
this is our smaller evaluation datasetgiven the dimensionality of our features  this amount of data is
too prone to overfitting especially considering that
some of it is omitted for a test set  in our initial
testing  we obtained results that were too good to be
true 

 
    exp z

which has the same form as logistic regression
and a   class maximum entropy model  we classify
as positive if it passes the score threshold of     

iii 

update equations
with the advent of twitter  it is relatively simple to access curated feeds  most notably for this
experiment  we leverage this aspect for training examples  we obtain funny examples from joke feeds
  thecomedyjokes   funnyjokebook   funnyoneliners   and unfunny examples from authoritative news feeds   breakingnews   theeconomist 
 wsj   some examples of each type of sentence
found in our dataset are 

the sgd update equations follow  we include the
partial update to a positive example  the negative
example equations are analagous  
we define the following equations  tanh is applied
for each element  
h   x     g  j  x           exp  j  x    
t

j  x     u f   x     b 

  r 

  r 

f   x     tanh wx   b   

  r h  

f     x        tanh   wx   b   

  pun  i tried looking for gold  but it didnt pan
out 

  r h  

our update equations for a single data point are
as follows 
j    
u
j    
w
j    
b 
j    
b 

  h   x   f   x    

  constitution  to prove this  let facts be submitted to a candid world

r   
u

  h   x     f     x    u    x  
  h   x   f     x    u

  joke  pizza is the only love triangle i want 
r   
w

  news  artwork purchased by city of detroit
valued at up to      million 

   

our balanced dataset contains      examples of
each with the words in the sentences stemmed and
retweets omitted  we then truncate the sentence
length to a maximum of    words  with the blank
word appended if there are fewer than    words in
the sentence 

   

  h   x  

where in equations        we use the element wise
product operator  and the outer product operator
  the regularization gradients are
j    
  cu
u
j    
  cw
w

iv 
i 

in sgd we essentially alter the parameters by
their respective gradient of the cost function whenever we make an incorrect classification  the importance of each example is controlled by the parameter
  which downweights each gradient  we also have
the regularization parameter c from equations       

   
   

experiments

ii 

data

initially we set out to predict the punniness of certain
sentences  the original hypothesis was that sentences
are punny because specific words in the sentence
are used in more than one context and as such  are
jarring enough to be funny to the reader  we began
labeling puns but it soon became too time intensive 

results

we include a plot detailing the accuracy of the classifier with varying amounts of training and evaluation
data on the full word vectors with contextual features 
in these experiments we have    hidden states  we
observe that additional data does not help the performance after      or around      sentence pairs 
 

fiscores with dataset utilization

scores with gradient descent step size 

we also vary the number of hidden states  if we
include too many hidden states  it is theoretically
possible to predict the training data exactly and thus
overfit  since the size of the weight matrix depends
on the number of neurons in the hidden layer   after
an initial dip that is probably due to the variance of
sgd  performance remains roughly constant 

we vary the regularization parameter c and for
this particular problem it does not seem to matter
much  intuitively  since we are dealing with such
high dimensional data it should be rather sensitive to
regularization  however if the features are relatively
evenly distributed across the dimensions regularization will not have that large a contribution 

scores with number of hidden variables h

scores with overfitting parameter c

we also vary the learning parameter   we observe that making  too small for this task kills precision  which is expected  if  is too small  we wont
reach a local optimum 

we vary the number of iterations of sgd  k and
observe that there are but marginal performance
gains after k   iterations 
 

fitor and the data set upon which to utilize for training
and evaluation 

v 

future work

in our initial literature review  we came across recursive neural networks      as a good way of modeling sentence meaning because it allows for arbitrarily long sequences of words  rather than truncating
sentence length or appending blanks   however we
could not formulate a strong scoring function and
were stranded in the swamps of implementation details 

references

scores with sgd iterations k

iii 

    collobert  ronan  et al           natural language processing  almost  from scratch journal
of machine learning research

analysis

we evaluate our derived parameters on our dataset of
jokes and news sentences on a training set containing
sentences from the constitution and puns  using the
same parameters above when we varied the size of
the training set 

    mihalcea  rada and carlo strappavaral         
learning to laugh  automatically   computational model for humor recognition computational intelligence  volume     number        
    kiddon  chloe and yuriy brin          thats
what she said double entendre identification proceedings of the   th annual meeting
of the association for computational linguistics shortpapers  pages     
    taylor  julia and lawrence mazlack         
computationally recognizing wordplay in
jokes
    turian  joseph et al            word representations  a simple and general method for semisupervised learning
    erk  katrin and sebastian pado          a structured vector space model for word meaning in
context

scores with dataset usage  testing with constitution puns

    mitchell  jeff and mirella lapata           vectorbased models of semantic composition  in proceedings of acl         

the scores dropped by a small factor  especially
recall  the prediction became more conservative 
precision
   

recall
    

    manning 
chris and richard socher
cs   n 
pa 
assignment
sheet 
http   nlp stanford edu  socherr pa  ner pdf

f 
    

    bengio  yoshua et al           a neural probabilistic language model journal of machine
learning research                  

it is interesting that even though we trained on
jokes we are still able to generalize our predictions to
the domain of puns  however it is difficult to make
incremental progress on this model due to the nature
of hidden neural network  instead  one must start
afresh in thinking of the structure of the feature vec 

     socher  richard et al            recursive deep
models for semantic compositionality over a
sentiment treebank
 

fi