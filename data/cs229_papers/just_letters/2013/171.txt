cs     final project   using machine learning to
enhance a collaborative filtering
recommendation system for yelp
chris guthrie

abstract
in this paper i present my investigation of machine learning as a tool to augment collaborative filtering in
a recommendation system learned on a sparse dataset  although methodological flaws and scaling
difficulties limited the conclusiveness of my results  i believe lessons learned will enable me to
successfully extend the project on my own in the future 

introduction
yelp
yelp offers users high quality local search and reviews for businesses  in particular restaurants and
retail outlets  users engage with the app by searching  writing reviews  rating businesses  connecting
with other users and checking in at businesses  two factors make yelp an ideal product for a
recommendation system analysis  first  yelps product depends heavily on search and business
discovery  making ratings prediction valuable from a business standpoint  and second  yelp users
generate a large amount of relevant  highly structured data 

project objectives
the goal of the project was to implement a ratings predictor for yelp  i e  a machine capable of taking a
user and a business and predicting how the user would rate that business   a good ratings predictor
potentially would have product implications for yelp   for example  it could form the basis for a
recommendation system or more personalized search results rankings 
the standard method for recommendation systems is collaborative filtering  explained below in
detail   i wanted to see if i could enhance collaborative filtering predictions by incorporating 
   multiple collaborative filtering methods
   signals derived from structured content
into a sophisticated  multi input predictor 
i implemented the predictor using a selection of out of the box regressors  where features are
drawn both from the outputs of a collaborative filtering process and from structured yelp data 
i measured the ratings predictor through the mean square error metric  i chose mse because it
is a highly balanced metric that requires little explanation to understand  additionally  all standard
regressors efficiently support mse  for yelps specific purposes  it could be interesting to evaluate
different metrics that perhaps have different business implications   e g  finding the roc curve on the
proportion of predictions that are off by more than   rating star or adjusting the error metric to punish
false     star ratings more severely  experimenting with error metrics is something that id like to explore
in the future 

 

fithe yelp dataset
yelp offers a freely available challenge dataset consisting of data for the phoenix  az area  with highly
structured data on businesses  reviews  users  and user checkins  since we focus our efforts on
collaborative filtering  the review data has the most implications for us 
the dataset has        reviews spanning       businesses and       users  the
distribution of reviews per user and reviews per business is heavily skewed toward the low end 

this presents challenges for collaborative filtering  since reliable vector based similarity metrics depend
on high numbers of shared ratings  for users who have rated less than   businesses  for example 
collaborative filtering becomes impossible using the metric i chose  explained in the next section  

features
collaborative filtering
collaborative filtering typically attempts to predict ratings based on a ratings matrix indexed by user and
business  which well refer to as the user business matrix  the collaborative filtering algorithm is very
simple   it uses the matrix to compute a similarity score between any two users or businesses  the
similarity metric can vary  and in my case i chose to compute the pearson correlation of the
corresponding row column vectors weighted by a significance factor based on the jaccard similarity of
the vectors  this metric was a poor choice  since the vectors were very sparse and the majority of
vectors had jaccard similarity and pearson correlation of zero 
independent of the metric used  typically the top  k  similar users or businesses  that have been
rated by the user or business in question  are selected  and the output of cf is a similarity weighted
average of the ratings given  i computed collaborative filtering averages under a user user cf scheme
as well as a business business cf scheme and used both outputs as inputs to my regressors 

content based features
i had planned to integrate a number of content based signals from the yelp data  but
the only feature i had a chance to analyze was the users average rating for businesses that share a
category with the business in question  due to time constraints  other content based features that could
be analyzed are suggested under the future areas of exploration section 

 

fisummary features
i computed average rating scores for each business and user  the distribution of user rating scores  and
the number of ratings for each business and by each user  for every rating  i added these features as
signals to the regressors 

regressors
i chose to use two different classes of regressors in my exploration  since multiple regressors increased
the chance of positive results and could have given me better insight into the most effective ways
machine learning can enhance collaborative filtering 

linear regression
i chose to use a linear regressor for two reasons 
   there was an a priori reason to think that most of the features i chose  such as average user
rating or collaborative filtering guess  should be linearly related to the output rating
   linear regression allows for easy interpretability of features  so i could easily separate the
explanatory power of collaborative filtering features as opposed to content based features or
summary statistics

decision tree regression
decision trees are a topic i had heard about before entering      and i was looking for an opportunity to
learn more about them and explore their effectiveness as regressors  i chose to evaluate three
regressors based on decision trees  all from scikit learn    a simple decision tree regressor  a random
forest regressor  which randomly generates a set of trees and averages over the results to create a final
prediction   and an adaboost regressor  which combines multiple decision trees and uses the statistical
technique of boosting to reduce variance 

method
tools
i implemented the project entirely in python  representing datasets and the user business matrices in
sparse scipy matrices  i used scikit learn for learning and model optimization 

dataset creation
for the entire course of my investigation  i created training and testing datasets by randomly splitting the
set of review data  this was inadvertent  i had intended to select test and training sets that were
complete sub matrices of the user business matrix   i e  by including ratings of the form  user  business 
where both user and business were in a specified set of values  but i made a technical error  because i
randomly split the data based on individual reviews  i significantly undermined the effectiveness of the
regressors and of the collaborative filtering step  by systematically biasing many key features and
making collaborative filtering less statistically significant 

feature extraction
i built a sparse user business matrix  stored in multiple ways to optimize access   in addition to similarity
matrices and appropriate methods to efficiently query these matrices in parallel  i performed cf on

 

fithese matrices  and computed some summary statistics for users and businesses that i used as inputs to
the regressors 

challenges with data size
my largest struggle with scaling my working small scale regression model to the entire yelp dataset was
with computing similarity scores between users and between businesses  despite using highly efficient
representations of the data  my exact algorithm was too computationally complex to feasibly run on such
a large dataset  instead of trying to optimize speed  memoize every computation i made  and parallel
program through multithreading  i should have instead searched out algorithms that wouldve made it
easier to approximate a similarity metric  alternatively  i should have seeked out computing resources
with enough memory to support my computations across multiple training iterations 
the result of these struggles was that i limited the number of reviews  users  and businesses i
included in my training testing sets  unfortunately  i mis implemented this limitation and inadvertently
made the review data significantly more sparse  compounding my troubles 

optimizing collaborative filtering parameters
i tested across different values for  k user and  k business   the basic parameters for collaborative filtering  with
cross validation  i used a single scale for both  based on difference from median reviews per user and
median reviews per business  

model optimization
for the tree based regressors  i ran cross validation tests to optimize for the maximum tree depth
hyperparameter  ultimately  due to lack of results for reasons relating to feature selection  i did not
further explore model optimizations 

results
baseline
i found that regressors based solely on summary features performed at an average cross validated
mse of      on a dataset with   k ratings samples  a simple linear regression model performed best
with cross validated mse of      

effect of collaborative filtering
in sum  i found that on data sizes of less than   k  for given user and business sizes   collaborative
filtering had practically no effect on the regression fit compared to the   k baseline  this was confirmed
by an analysis of the coefficients and significance of the linear regression fit on the collaborative filtering
features   the coefficients were consistently      orders of magnitude smaller than comparably scaled
features  and the fit found that the cf features explained no variance in the data 
i attribute the absolute ineffectiveness of collaborative filtering to sparseness of the data and the
lack of significant correlations between users or between businesses 

effect of sample size
due to a lack of regularization in my fits  as data sizes increased up to   k  cross validated mse
increased as well  since the regressors were based almost entirely on summary statistics  

 

ficonclusion
getting positive results on a machine learning project has more hurdles than i had anticipated 

future areas of exploration
other methodologies
faster similarity metrics
instead of computing an exact similarity metric for every user user and business business pair in every
dataset  it could be productive to use nearest neighbor search via locality sensitive hashing or
clustering to find similar users businesses 

graph based collaborative filtering
i saw another student who had solved the sparseness problem i encountered by constructing a bipartite
user item  or in my case  a user business  graph with edges representing a review  then performing
network analysis on this graph to find similar users  even if the users have no overlap in rated
businesses 

weight ratings by votes and recency
since these signals are in the dataset and give a good idea of the reliability of a rating  some sort of
weighting mechanism would likely improve overall performance 

add an svr regressor and optimize it
this would make the analysis more complete 

other features to explore
review text as a content signal
it would be interesting to mine review text to extract content features for businesses  and possibly for
users  to construct a more complete content profile  there are a number of interesting algorithms
 dimensionality reduction algorithms  nlp algorithms  that would be very cool to investigate 

review text as a similarity metric
it would be possible to build a collaborative filtering mechanism around review text in addition to one
built around ratings  using review text as a similarity metric would help solve some of the sparseness
problems i encountered  since you can come up with a metric between any two users that have written a
review or any two businesses that have been reviewed 

business location as a similarity metric
as with review text  business location solves the sparseness problems i encountered and could be a
strong signal for user preference 

 

fi