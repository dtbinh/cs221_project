predicting the betting line in nba games
bryan cheng

kevin dade

michael lipman

cody mills

computer science
stanford university

electrical engineering
stanford university

symbolic systems
stanford university

chemical engineering
stanford university

abstractthis report seeks to expand upon existing models
for predicting the outcome of nba games  using a time varying
approach  the model proposed in this report couples standard
machine learning techniques with weighted causal data to predict
the number of points scored by each team in an attempt to beat
the spread 

i 

i ntroduction

in the nba  thirty teams comprise two conferences 
throughout the regular season these teams will each play   
games  for a total of      nba games played per season  to
the enterprising sports gambler  this means      opportunities
to beat the odds made by expert nba analysts and cash in  to
the data analyst       games provide a wealth of player and
team data for modeling complex trends in the performance
of individuals and franchises  this report is concerned with
exploring prior research in the field of sports prediction 
specifically with the goal of predicting nba game outcomes
more accurately than the nba experts who set the betting line
for each game  before diving into our models and predictions 
we want to provide an overview of the various types of bets
you can make in the nba 
a  the spread  when betting against the spread  the
gambler bets either that the favorite will win by more than
the spread  or the underdog will lose by less than the spread
 or win   the favorite is the team with the negative number 
because they are essentially handicapped a number of points 
in order to place a bet of this form  the gambler would place
a bet of      with a payoff of       the amount paid for a
chance to win      is denoted in parentheses 
e g 
miami heat            
denver nuggets           
b  over under  in this form of betting  the gambler
bets whether or not the total points scored in a game will be
greater than or less than a given amount  as with the spread 
the amount paid for a chance to win      is also listed 
e g 
miami heat           
denver nuggets           
c  other  there are additional forms of betting  but this
report is concerned only with the betting schemes discussed
above 
prior research
a prevalent approach in the field is to use a combination of
models  as opposed to a single prediction algorithm  to output
a more robust prediction  for example  the teamrankings com
website  founded by stanford graduates mike greenfield and

tom federico and featured by espn  combines the predictions
of six different models to shape their expected outcomes of
college basketball games  the intuitive idea of a combination
of models is that each model can capture a different aspect of
the game or correct for a poor assumption made by another
model  due to time constraints  the methods covered in this
report are all singular in their approach  it is possible that some
combination of our individual predictors would yield better
results  and is worth further exploration in the future 
also  zifan shi et al  suggested normalizing team statistics
with respect to the strength of the opposing team  though
they did not propose an algorithm for doing so  this seemed a
logical approach and we incorporated it into our model  in this
report we first apply basic feature reduction and polynomial
regression with team averages over a long period  these
approaches are useful for becoming familiar with the data 
and picking out any obvious trends that our more involved
models might use to their advantage  we then use the classic 
tried and true support vector machine technique with various
feature sets to predict game outcome  finally  we propose
a model for estimating a teams time varying offensive and
defensive strengths in order to normalize the team statistics
with respect to their opponent in each game  these normalized
statistics are used to predict the number of points scored by
each team in the coming game  and we then make a bet against
the spread under the assumption that our predicted point spread
is more accurate than the actual spread 
ii 

data

for our project  we needed actual nba game statistics 
but there are no publicly available datasets  instead  we used
public box scores on the popular sports website espn  on
espn  the data we wanted are organized per game in box
scores  we wrote a scraper that traversed all the dates of the
regular season  found if there were any games for that date 
and if there were  stored the box score html links  with
all the box score links  we issued an html request to those
html pages  received the html response  and parsed the
corresponding data from those pages  we scraped the past   
years of data from espn  making small modifications year to
year for html formatting differences  we figured    years
would be sufficient for our purposes  as for the betting data 
probably to prevent people like us from running tests like these 
we were unable to find betting data from past games listed
on any site  however  via an espn insider account  and by
changing the url in the browser with the correct game id  we
managed to get the betting data from all of last years games 
data from prior years were not available via this method  so
we were limited to testing spread and over under results on
only last years data  after collecting this data  we organized

fithe data into   tables in a mysql database 
table i 
date
away team
technical fouls
spread

g eneral p er g ame data

time
home team score
officials
over under

table ii 
team id
 nd quarter pts
 st ot pts
 th ot pts
three pointers made
free throws attempted
rebounds
blocks
points
total team turnovers

home team
flagrant fouls
game length
away roi

t eam p er g ame data

game id
 rd quarter pts
 nd ot   pts
field goals made
three pointers attempted
offensive rebounds
assists
turnovers
fast break points
points off turnovers

table iii 
player id
minutes played
three pointers made
free throws attempted
rebounds
blocks
plus minus

location
away team score
attendance
home roi

 st quarter pts
 th quarter pts
 rd ot pts
field goals attempted
free throws made
defensive rebounds
steals
personal fouls
points in the paint

p layer p er g ame data

team id
field goals made
three pointer attempted
offensive rebounds
assists
turnovers
points

game id
field goals attempted
free throws made
defensive rebounds
steals
personal fouls
did not play reason

with mysql  we easily manipulated or merged the data
into smaller more applicable datasets that we wanted to perform our training on  for example  we could take the averages
for each team in games played in the         nba season 
instead of analyzing the entire dataset 
iii 

basic m odels

to start off  we looked at some basic data among teams 
to determine wins  the most obvious stat is points scored or in
other words  which team scores more than the other team  we
wanted to know if we could make any simplifying assumptions
based on how teams scored points  for one season  we plotted
the histogram of points scored per game  we see that this is a

fig     distribution of points scored per game for all teams in nba
         

nearly perfect gaussian distribution  further  by breaking this
down per team per year  we can see similar results with smaller
datasets 
for most of our tests  we make the assumption that the
teams have fairly consistent performances  we do not take
into consideration for example major injuries   our first test
was running a basic linear regression using four predictors 
home team points per game  ppg   home team points

fig     distribution of points scored per game for all teams in nba
         

allowed per game  papg   away team ppg  away team
papg  this model trained on season total averages for the
full season  then testing on the training set predicted correctly
     unfortunately  if we hold out data  the testing accuracy
was only      
for the next step  we incorporated more stats outside of
simply points per game averages  we found the season long
averages for every team stat listed in the teams stats table 
some stats exactly predicted others     fgm tpm      tpm
  ftm   pts and oreb   dreb   reb   so we had to
reduce the number of predictors  we used a lasso approach to
reduce the coefficients of certain predictors to   or effectively
remove the predictors  using cross validation  we trained on
a random half of the data then tested on a held out dataset 
with this  our model again predicted with     accuracy  but
on data it had not seen before 
to play around with different predictors in the model  we
tried using raw game numbers instead of the season averages
to train the model  and then tested on the same data  this
produced worse results at      lastly  we obviously do not
have the season end averages until the end of the season 
to compensate for this  we calculated the running averages
 using a small python script since mysql does not have this
capability   one thing to note is that the running averages can
have large changes over the course of the season  to see where
they stabilized  we plotted each teams points per game average
over    games 
from the graph  we can see some teams have very stabile
and constant averages  most of the teams seem to stabilize
by     to     of the way through the season  only a couple
  for the regression  the home teams ppg and away teams papg were
more significant than the other   predictors  put into the basketball context 
this means an offensive team performs better at home  while a defensive team
performs better on the road  interestingly  this seems to indicate that a team
is more consistent defensively on the road  while at home  their offensive
production can feed of the crowds energy 

fifig    

average points per game per team over    games

kept changing until the end of the season   when we trained
only on the running averages in the  st half of the season and
tested on the running averages in the  nd half of the season 
we managed a get a test accuracy of     
the next step we tried from this was using svms to
classify the game as win or loss 
iv 

s upport v ector m achine

using the full game data from the database  implemented
a support vector machine model to classify each game sample
as a binary variable    win  or    loss   we used the svm in
conjunction with a lasso regression and bootstrap aggregating
to achieve our final prediction  before implementing the algorithm  we pre processed the raw data was had scaled and
calculated the average of various team statistics from each
season  rebounds per game  points per game  etc    further 
for each game we created a sample point with the averages
for both teams as features 
even in this reduced form there were still    variables
features  to infer which of the predictors were most significant
we ran a lasso regression on the data  the lasso  a type of
shrinkage regression  set several of our predictors coefficients
to zero  which justified their exclusion from our model because
they were insignificant  our lasso model was tested over a
range of shrinkage parameters and was then    fold cross
validated  we chose the best model after cross validation to
select our best subset of predictors  though the data for our
lasso model was of varied size and character throughout our
experimental trials  the lasso generally left approximately   
predictor coefficients nonzero 
using the set of predictors that had been found to be most
significant  we fit a polynomial kernel support vector classifier
to the data  we implemented the svm and tuned the result to
find the optimal cost parameter and polynomial degree from
a range using simple cross validation  this model was able
to predict the win response for whole seasons of games with
  if you look at indianas steady increase in average points per game  it can
be attributed to paul georges meteoric growth as a player  our models never
account for player or team growth 

accuracy in the range of        
to further optimize the prediction of the svm we also
implemented bootstrap aggregating  bagging  over the svmlasso model  we bootstrapped over our entire model so that the
lasso would be fit onto each bootstrap re sample and decide
which sample were significant in that bootstrap set  then we fit
the svm to the re sample and predicted the results for the test
set  averaging the predictions over    bootstrap re samples 
we set the sample with an average of less than    to   and the
rest to    in the best case we were able to bag over a model
trained on the         season and predict the results of the
        season with       accuracy and the results of the
        season with       accuracy 
we also explored fitting the model on a larger training
set  which led too a small improvement in test accuracy  we
fitted and bagged a model trained on the         and        
seasons and predicted the         results with       accuracy  this was our best non training error from the lasso svm
model  this is a strong result in context  espns accuscore
algorithm for nba odds  win loss  bets had an accuracy of
      last season  though a consistent expert picks panel does
not formally exist at espn for basketball  in nfl football the
accuscore football equivalent has been more accurate than any
espn expert for the past   years 
to examine the real effect of our model we calculated
the real time average of a teams statistics for the        
season and ran our bagged lasso svm model on a training
set that used the previous two seasons as well as an arbitrary
number of elapsed games in the         season  when we
predicted the remain games of the season we consistently
achieved accuracies of            of games remaining  or
greater with the accuracy increasing to       toward the end
of the season      of games remaining   we also ran the
predictor on the entire running averages set and got an accuracy
of      though this test of the true use case yielded weaker
results than the full season retrospective classifications  we
believe that given more time we could improve the model 
first  we could calculate running averages for each season
and use these samples to train the data  we could also go
a level deeper in detail and try to use a construction of player
statistic contributions  in real season time  to construct the
team average statistics and then make predictions 

a  boosting
one of the methods cited by teamrankings com as an
element of their prediction formula was a decision tree 
because of his success we also pursued a decision tree to
try to classify our data  however  instead of simply fitting a
decision tree regression to the data we implemented a boosting
method to combine a myriad of weak learner decision trees
to form a strong learner tree at the end  we optimized by
simple cross validation over three different tuning parameters 
the depth of the tree  the number of trees  and the shrinkage
parameter lambda  unfortunately  the accuracy of the model
plateaued at        boosting is a particularly slow and computationally heavy method so it was difficult to run a cross
validations optimizations over many combinations of depth 
tree count  and shrinkage parameter 

fib  spread and over under analysis
we ended up developing full models to predict the spread 
but to give us an initial idea on how spread and over under
lines are set  we ran a simple regression using teams averages
as predictors and the following graphs are the result 

normalized statistics  we use polynomial regression along with
svm and naive bayes techniques to further enhance the
models predictions  the following steps describe the full
method we devised  running this algorithm over a season
algorithm   calculate time varying defensive and offensive
strengths by win propagation
require    ko   kd
os        
ds        
for all game  allgames do
m  numberof gamesw innerhasp layed
p  numberof gamesloserhasp layed
 m   

 p 

 m 

oswinner  oswinner        ko
 m   

 m 

dsloser

 m 
oswinner
 p 
osloser
d
 m 
dswinner
 m 
  dswinner
d os  p 
loser
 m 
  oswinner
o ds  p 
loser

dswinner  dswinner        k
 p   

 p 

 p   

 p 

fig     prediction of spread and over under using basic linear regression

osloser  osloser        k

from the graphs  we can see a clear linear relationship
between what we predicted and what the final betting lines
were set at  the variance could come from betting houses
adjusting to how the public is betting or variation in the models
that our linear model did not account for  such as injured
players  fatigue  etc   we then looked at how the vegas spread
and over under lines compared to actual game results 

dsloser  dsloser        k

fig    

spread and over under vs  actual game results

from these graphs  it is clear that from the cloud shape
that there is a lot of noise in the actual games compared to
the predictions  this could be the underlying nature of sports
that sports are inherently very noisy and there is nothing we
can do  on the other hand  the betting lines do a good job
splitting their predictions in half  essentially meaning that they
do a good job themselves over the course of the season setting
the lines such that they are guaranteed to win 
v 

c ausal w eighted r egression

in our final approach  we attempted to create a model that
captures both the natural fluctuations in a teams performance
throughout the season  and also adjust their statistics to more
accurately reflect their performance in a game  for instance 
a team that puts up a lot of points against the best team
in the league should potentially have their rating increased 
even if they lose  we introduce the causal limitation somewhat
artificially to this method  although our reasoning stems from
the fact that we have a time varying model  and it would not
make sense to incorporate future data  one thing to note is
that this is a hybrid approach  and once we obtain the causally

end for

will produce an estimated offensive strength  os   m   and a
defensive strength ds   m   where m  m gamesperseason
is the number of games played for each team  the constants
ko and kd control the relative weighting of defense vs 
offense  and  is the forgetting factor   i e  how much past
game results should affect the current strengths  these values
are then smoothed with a polynomial interpolator to get an
estimate of a teams offensive and defensive strengths relative
to the other teams at any point in the season  one important
thing to note is that we do withhold the test set from the
training set when we apply the smoothing  as failure to
withhold this data would cause the test data to leak into to the
training data  the estimations of this method provide a fairly
decent relative representation of all the teams capabilities 
at least when compared retrospectively to their performances
last year  the defensive strengths of a team are then used in

fig     in the      season  the miami heat were regarded as the best
defensive team in the league  the los angeles lakers were more or less in
the middle of the league both offensively and defensively 

every game to normalize their stats to get an estimation of
their effective stats relative to the league at that point  these
values are similarly smoothed to reduce the high variance that
is inherent in sports data 
these normalized statistics are then used in a linear regression over the training set to map a given statistic to
normalized points scored in a game  we found that a derived
statistic called effective field goal percentage  efgp  was the
most highly correlated with points scored  and so our model
uses only the regression fitted to this statistic in predicting
the normalized number of points that will be scored  once

fia normalized point prediction is made based on a teams
smoothed normalized efgp going into a game  the prediction
is de normalized with the opposing teams estimated defensive
 numgamesopponenthasp layed 
strength  dsopponent
  to get a real score
prediction  we can then compare the predicted scores for each
team to get a predicted spread  and by comparing this to the
given spread for the game  we are able to bet one way or
another 

allows for better estimation of a teams true value at any
given time  we think that expanding upon the existing causal
weighting algorithm to include more variables would significantly improve the strength estimation aspect of the model 
it was disappointing that incorporating the extra classification
step did not significantly improve the model  but given that
our predictions are hovering at around     anyway  it is
not surprising that the predictions have only a very small
correlation with the outcomes  if any   and no amount of
classification can fix that 
vi 

fig     this regression is fitted for each team in the hopes of capturing any
differing offensive paradigms between teams 

to further improve our models ability to beat the spread 
we incorporate both svm and naive bayes classifiers trained
on a feature set of the game statistics in addition to our
predictions  the classifiers are used both to directly predict
whether or not a team will beat the spread in a given game 
and also to predict whether or not the prediction given by
the model will succeed or not  the first case is referred to as
method    and the second as method    here are the results of
the model under various test schemes  k fold cross validation
with k       and random holdout cross validation with
    of the data withheld as the test set  the rhcv values are
obtained as the mean of    iterations  though the above figures
table iv 
method
normal predictor
svm method  
svm method  
nb method  
nb method  

s pread p rediction m odel r esults

   fold cv i
      
      
      
      
      

   fold cv ii
      
      
      
      
      

   rhcv i
      
      
      
      
      

   rhcv ii
      
      
      
      
      

seem to imply that this would be a reasonable approach  our
results show that it was not any more successful at predicting
game outcomes than the brute force bulk approaches discussed
earlier  win loss prediction results not shown in table as they
have been discussed at length in prior sections   however 
since this model was designed specifically with the goal of
predicting the point spread  it is not surprising that it would
perform worse with regard to winner prediction  beating the
spread is not necessarily the same problem as predicting the
winning team  we also tested this model under more stringent
conditions imposed due its causal nature  so this probably
accounts for the lower performance in win loss prediction
accuracy  with our results  we are reluctant to believe that
our model achieves significantly better than     prediction
accuracy vs  the spread  and although multiple iterations of
the two cross validation schemes did show a slight favorable
edge towards     and      we are certainly not beating
the requisite       prediction accuracy required to enact a
financially rewarding betting strategy 
this modeling approach makes more rigid assumptions
about the time varying nature of team performance  but also

c onclusion

in this report we have shown that machine learning techniques can be successfully applied to nba games to predict
the winner of any given game with around     accuracy 
this level of accuracy rivals that of professional analysts and
basketball experts  however  in our endeavor to predict the
spread outcomes with an accuracy greater than        the
model we developed fails to meet this goal under testing  
there are several contributing factors to this 
once our model was trained  it resulted in a simple
deterministic predictor  ideally  since we are trying to model
the interactions of complex entities  we would like to add more
layers to our model and incorporate an element of stochasticity 
this in conjunction with batch simulation would probably
converge on a more accurate estimate for game outcomes than
our train once  predict once method  successful sports betting
companies such as accuscore claim to use this batch simulation approach  implementation of a more complex stochastic
model would require greater access to specialized data  and
also more computational power to run many simulations  these
are limitations that an individual wishing to beat the spread
will always face  and it is no surprise that the predictors with
more resources at their disposal are able to perform better 
we conclude that in order to make predictions about such a
complex interaction  the quality and complexity of the data
used is perhaps the most important factor in determining the
success of the model 
acknowledgment
the members of this project team would like to thank
andrew ng and his teaching staff for sharing their wealth of
knowledge on the related subject material  we should also like
to thank yuki yanase for helping manually collect betting data
from the espn website 
r eferences
   

   
   
   
   

adams m  ryan p   george e  dahl  and iain murray  incorporating
side information in probabilistic matrix factorization with gaussian
processes     mar        arxiv org 
four factors  basketball reference  n d  basketball reference com 
hess  david  under the teamrankings hood  part    models  models 
everywhere  teamrankings     mar        teamrankings com 
shi  zifan et al  predicting ncaab match outcomes using ml techniques   some results and lessons learned     oct        arxiv org 
walsh  pj  how to bet over unders  espn insider    dec       
insider espn go com 

  for the poster presentation  we made a prediction for that night
that orlando would beat the spread in their game against charlotte  we
just wanted to say  that our prediction was right  orl    cha    http   espn go com nba recap gameid          

fi