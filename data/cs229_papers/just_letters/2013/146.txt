speech recognition using deep learning algorithms
yan zhang  sunet id  yzhang 
instructor  andrew ng
abstract  automatic speech recognition  translating of spoken words into text  is still a
challenging task due to the high viability in speech signals  deep learning  sometimes referred as
representation learning or unsupervised feature learning  is a new area of machine learning 
deep learning is becoming a mainstream technology for speech recognition and has successfully
replaced gaussian mixtures for speech recognition and feature coding at an increasingly larger
scale  the main target of this course project is to applying typical deep learning algorithms 
including deep neural networks  dnn  and deep belief networks  dbn   for automatic continuous
speech recognition 
   introduction
automatic speech recognition  translating of spoken words into text  is still a challenging task due
to the high viability in speech signals  for example  speakers may have different accents  dialects 
or pronunciations  and speak in different styles  at different rates  and in different emotional states 
the presence of environmental noise  reverberation  different microphones and recording devices
results in additional variability 
conventional speech recognition systems utilize gaussian mixture model  gmm  based hidden
markov models  hmms         to represent the sequential structure of speech signals  hmms are
used in speech recognition because a speech signal can be viewed as a piecewise stationary
signal or a short time stationary signal  in a short time scale  speech can be approximated as
a stationary process  speech can be thought of as a markov model for many stochastic
purposes typically  each hmm state utilizes a mixture of gaussian to model a spectral
representation of the sound wave  hmms based speech recognition systems can be trained
automatically and are simple and computationally feasible to use  however  one of the main
drawbacks of gaussian mixture models is that they are statistically inefficient for modeling data
that lie on or near a non linear manifold in the data space 
neural networks trained by back propagation error derivatives emerged as an attractive
acoustic modeling approach for speech recognition in the late     s  in contrast to hmms  neural
networks make no assumptions about feature statistical properties  when used to estimate the
probabilities of a speech feature segment  neural networks allow discriminative training in a
natural and efficient manner  however  in spite of their effectiveness in classifying short time units
such as individual phones and isolated words  neural networks are rarely successful for
continuous recognition tasks  largely because of their lack of ability to model temporal
dependencies  thus  one alternative approach is to use neural networks as a pre processing e g 
feature transformation  dimensionality reduction for the hmm based recognition 
deep learning        sometimes referred as representation learning or unsupervised feature
learning  is a new area of machine learning  deep learning is becoming a mainstream technology
for speech recognition         and has successfully replaced gaussian mixtures for speech
recognition and feature coding at an increasingly larger scale  in the course project  we focus on
deep belief networks  dbns  for speech recognition  the main goal of this course project can be
summarized as 
   familiar with end to end speech recognition process 
   review state of the art speech recognition techniques 
   learn and understand deep learning algorithms  including deep neural networks  dnn   deep
belief networks  dbn   and deep auto encoders  dae  
   applying deep learning algorithms to speech recognition and compare the speech recognition
performance with conventional gmm hmm based speech recognition method 

fiacoustic
models

pronunciation
dictionary

speech signal

language
models

recognized words
decoder

feature extraction

fig    a typical system architecture for automatic speech recognition
   automatic speech recognition system model
the principal components of a large vocabulary continuous speech recognizer        are
illustrated in fig     the input audio waveform from a microphone is converted into a sequence of
fixed size acoustic vectors                this process is called feature extraction  the decoder
then attempts to find the sequence of words               which is most likely to have
generated   i e  the decoder tries to find
   

      


however  since      is difficult to model directly  bayes rule is used to transform the above
equation into the equivalent problem of finding 
   

        


the likelihood     is determined by an acoustic model and the prior    is determined by a
language model 
for any given   the corresponding acoustic model is synthesized by concatenating phone
models to make words as defined by a pronunciation dictionary  the parameters of these phone
models are estimated from training data consisting of speech waveforms and their orthographic
transcriptions  the language model is typically an  gram model in which the probability of each
word is conditioned only on its     predecessors  the n gram parameters are estimated by
counting n tuples in appropriate text corpora  the decoder operates by searching through all
possible word sequences using pruning to remove unlikely hypotheses thereby keeping the
search tractable  when the end of the utterance is reached  the most likely word sequence is
output  alternatively  modern decoders can generate lattices containing a compact representation
of the most likely hypotheses 
a  feature extraction
in automatic speech recognition  it is common to extract a set of features from speech signal 
classification is carried out on the set of features instead of the speech signals themselves  the
feature extraction stage seeks to provide a compact representation of the speech waveform  this
form should minimise the loss of information that discriminates between words  and provide a
good match with the distributional assumptions made by the acoustic models  a popular feature
vector mel frequency cepstral coefficients  mfcc   which provides a compact speech signal
representation that are the results of a cosine transform of the real logarithm of the short term
energy spectrum expressed on a mel frequency scale  mfcc coefficients are generated by
applying a truncated discrete cosine transformation  dct  to a log spectral estimate computed by
smoothing an fft with around    frequency bins distributed non linearly across the speech
spectrum  the nonlinear frequency scale used is called a mel scale and it approximates the
response of the human ear  the dct is applied in order to smooth the spectral estimate and
approximately decorrelate the feature elements  after the cosine transform the first element
represents the average of the log energy of the frequency bins  this is sometimes replaced by
the log energy of the frame  or removed completely 

fib 

hidden markov models

predominantly  hmms are used in asr  a hmm is a stochastic finite state automaton built from a
finite set of possible states               with instantaneous transitions with certain probabilities
between these states  each of these states is associated with a specific emission probability
distribution        thus  hmms can be used to model a sequence x of feature vectors as a
piecewise stationary process where each stationary segment is associated with a specific hmm
state  this approach defines two concurrent stochastic processes  the sequence of hmm states
modeling the temporal dynamics of speech  and a set of state output processes modeling the
locally stationary property of the speech signal 
in speech recognition  we have to find the hmm  which maximizes the posterior probability
    of the hypothesized hmm  given a sequence x of feature vectors  since this probability
cannot be computed directly  it is usually split using bayes rule into the acoustic model  likelihood 
    and a prior    representing the language model             

fig    the dbn is composed of rbms 
   deep belief networks
deep belief networks  dbns  are neural networks consisting of a stack of restricted boltzmann
machine  rbm  layers that are trained one at a time  in an unsupervised fashion to induce
increasingly abstract representations of the inputs in subsequent layers 
    restricted boltzmann machines  rbms  and training
as shown in fig     a   each rbm has an input layer  visible layer  and a hidden layer of
stochastic binary units  visible and hidden layers are connected with a weight matrix and no
connections exist between units in the same layer  signal propagation can occur in two ways 
recognition  where visible activations propagate to the hidden units  and reconstruction  where
hidden activations propagate to visible units  the same weight matrix  transposed  is used for
both recognition and reconstruction  by minimizing the difference between the original input and
its reconstruction  i e  reconstruction error  through a procedure called contrastive divergence
 cd   the weights can be trained to generate the input patterns presented to the rbm with high
probability  the rbm pretraining procedure of a dbn can be used to initialize the weights of a
deep neural network  which can then be discriminatively fine tuned by back propagating error
derivatives  the recognition weights of the dbn become the weights of a standard neural
network  in cases where the rbm models the joint distribution of visible data and class labels  a
hybrid training procedure can be used to fine tune the generatively trained parameters 
    dbn structure

fifig     b  shows the structure of a dbn  a dbn consists of a stack of rbms  trained one at a time 
each layer of hidden units learns to represent features that capture higher order correlations in
the original input data  in dbns  subsequent layers usually decrease in size in order to force the
network to learn increasingly compact representations of its inputs  the training procedure is
sometimes augmented to optimize additional terms  such as the l  and l  norms of the weight
matrices  or sparsity constraints on the unit activations  weights are initialized from a normal
distribution with zero mean and small standard deviation  weight updates are applied after the
presentation of a number of samples in a minibatch  after a number of training cycles through the
full training dataset  the stack of rbms is unfolded  such that first recognitions are computed
through all subsequent layers  and next reconstructions through all layers in reverse order  the
recognition and reconstruction weights are uncoupled  and can then be fine tuned with gradient
descent  either to become better at reconstructing the inputs  or  in combination with other
supervised or reinforcement learning methods  to form features relevant to the task at hand 
    applying dbns for speech recognition
to apply dbns with fixed input and output dimensionality to phone recognition  a context window
of n successive frames of feature vectors is used to set the states of the visible units of the lower
layer of the dbn which produces a probability distribution over the possible labels of the central
frame  to generate speech sequences  a sequence of probability distributions over the possible
labels for each frame are fed into a standard viterbi decoder 
   performance evaluation
timit acoustic phonetic continuous speech corpus dataset      is used for performance
evaluation  the speech was analyzed using a    ms hamming window with    ms between the
left edges of successive frames  the data were normalized to have zero mean and unit variance
over the entire corpus  all experiments used a context window of    frames as the visible states 
th
the     order mel frequency cepstral coefficients  mfccs  and energy  along with their first and
second temporal derivatives are extracted as speech features  the word error rate for gmmhmm based speech recognition system is about      the word error rate for a deep neural
network hidden markov models  dnn hmms  speech recognition system with five hidden layers
is around      the word error rate of a dbn speech recognition system with three hidden layers
and      hidden units per layer is about     
   conclusion
in this course project  typical deep learning algorithms  including deep neural networks  dnn  
and deep belief networks  dbn  have been learned understood  further  a dbn has been
implemented for automatic speech recognition  the speech recognition performance evaluations
on three speech recognition systems  namely  gmm hmm  dnn hmm and dbn  have been
performed with timit acoustic phonetic continuous speech corpus dataset in terms of word error
rate  the results have shown that the dbn based speech recognition system beats other two
speech recognition systems 
references 
    
    

    
    

s  young  large vocabulary continuous speech recognition  a review  ieee signal
processing magazine  vol      no     pp             
j  baker  l  deng  j  glass  s  khudanpur  chin hui lee  n  morgan  and d 
oshaughnessy  developments and directions in speech recognition and understanding 
part    signal processing magazine  ieee  vol      no     pp        may      
hinton  g   osindero  s   and teh  y  a fast learning algorithm for deep belief nets 
neural computation  vol      pp                  
yoshua bengio  pascal lamblin  dan popovici and hugo larochelle greedy layer wise
training of deep networks  in j  platt et al   eds   advances in neural information
processing systems     nips        pp           mit press       

fi    

    
    
    
    

     

     
     
     

     
     
     
     

     

marcaurelio ranzato  christopher poultney  sumit chopra and yann lecunefficient
learning of sparse representations with an energy based model  in j  platt et al   eds  
advances in neural information processing systems  nips        mit press       
bengio y  learning deep architectures for ai  in foundations and trends in machine
learning  vol     no           pp        
bengio y  deep learning of representations  looking forward  in  statistical language and
speech processing  pp         springer       
bengio y   courville  a   and vincent  p  representation learning  a review and new
perspectives  ieee trans  pami      a 
li deng  a tutorial survey of architectures  algorithms  and applications for deep
learning to appear in apsipa transactions on signal and information processing 
cambridge university press       
mohamed  a   dahl  g   and hinton  g  deep belief networks for phone recognition  in
proc  nips workshop deep learning for speech recognition and related applications 
     
l  deng  m  seltzer  d  yu  a  acero  a  mohamed  and g  hinton  binary coding of
speech spectrograms using a deep auto encoder  interspeech       
g  dahl  d  yu  l  deng  and a  acero  large vocabulary continuous speech recognition
with context dependent dbn hmms  icassp       
g  dahl  d  yu  l  deng  and a  acero  context dependent pre trained deep neural
networks for large vocabulary speech recognition  ieee trans  audio  speech  lang 
proc   vol      pp             
mohamed  a   dahl  g  and hinton  g  acoustic modeling using deep belief networks 
ieee trans  audio  speech    language proc  vol          january      
mohamed  a   hinton  g   and penn  g   understanding how deep belief networks
perform acoustic modelling  proc  icassp       
morgan  n  deep and wide  multiple layers in automatic speech recognition  ieee
trans  audio  speech    language proc  vol          january      
li deng  jinyu li  jui ting huang  kaisheng yao  dong yu  frank seide  michael seltzer 
geoff zweig  xiaodong he  jason williams  yifan gong  and alex acero  recent
advances in deep learning for speech research at microsoft  in proc  of ieee
international conference on acoustics  speech  and signal processing  icassp   may
     
j s  garofolo  l f  lamel  w m  fisher  j g  fiscus  d s  pallett  n l  dahlgren  darpa
timit acoustic phonetic continuous speech corpus  u s  dept  of commerce  nist 
gaithersburg  usa       

fi