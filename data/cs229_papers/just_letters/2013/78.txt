predicting gene function and localization
by ankit kumar and raissa largman
cs     fall     
i  introduction
our data comes from the      kdd cup data mining competition  the competition had two tasks 
to predict the function and localization of genes  thus we are presented with a classification problem 
the data set consists of a row per gene with the following information about each gene 
   gene id
   motif

   essential

   class

   chromosome number

   complex
   function

   phenotype
   localization

all the attributes are discrete variables  with most related to the proteins that are coded by that
gene  the function of the gene describes the tasks operations that the associated protein performs or
is involved in such as protein synthesis  metabolism  and transcription  the localization attribute is the
region of the cell in which the protein resides  such as ribosome  nucleus  cytoplasm  etc  additionally 
the data set contains interaction data for the genes  describing the relationships between pairs of genes 
the data presents a challenge in that many genes have missing variables and it is not obvious how to
best utilize the interaction data for the genes 

ii  data
each gene in the data set can have one of    localizations  and one or more of    functions  to
classify the test data  the main goal is to find an appropriate feature set  the first data set  with the
gene information  provides a starting point  each gene has the above   attributes  however  many of
the attributes can take on several discrete values  which are often strings  therefore  we transformed
all possible values of the   attributes into binary valued features  furthermore  for the   attributes 
many genes have   as a value because data is missing  so we add a an unknown feature for each of the
attributes  this leaves a total of     features for each of the     training genes 
we now have a large feature set compared to the number of training vectors  the data vectors are
very sparse because we are given only   attributes for each gene  not including gene id   therefore
feature selection could improve accuracy  refer to section iv  
 note  each gene can have more than one function  so although only   attributes are given besides gene
id  the data vector for each gene can have more than   non zero feature values

iii  baseline implementation
a  classifiers
we first decided to implement naive bayes and softmax with our binary feature set using only the
relational gene data  no interaction data yet   we decided to use naive bayes because the data is
 

fics    

project milestone

ankit kumar  raissa largman  

binary rather than continuous  and furthermore  naive bayes and softmax classification provides a good
benchmark for basic training and testing error  we tested two types of naive bayes classifiers and two
types of softmax classifiers 
   multinomial naive bayes which does not penalize the nonoccurence of features
   bernoulli naive bayes which explicitly penalizes the nonoccurence of features
   softmax with an l  norm
   softmax with an l  norm
we expected these to have different results  considering our feature vectors are very sparse  therefore 
penalization of features that do not occur could affect classification output  we trained both classifiers
on the training set of     genes and then tested them on the testing set of      genes 
for function prediction we implemented    different classifiers  one for each different function  this is
because each gene can have more than one function  and the scoring of success for function classification
included all possible functions for a given gene  the score for function predicition is as follows 
score  

tp   tn
fp   fn   tp   tn

b  results
classifier type

training error

testing error

multinomial

     

     

bernoulli

     

     

softmax l 

     

     

softmax l 

     

     

table    results of classifiers on localization prediction

classifier type

training score

testing score

multinomial

     

     

bernoulli

     

     

softmax l 

     

     

softmax l 

     

     

table    results of classifiers on function prediction

c  analysis
classification with multinomial versus bernoulli naive bayes classifier led to slightly lower training
and testing error with mutinomial naive bayes  this makes sense intuitively because the data vectors are
sparse due to the fact that there are     features but each data vector will have only   to approximately
   non zero feature values  therefore  penalizing for all the features that do not occur penalizes the vast
majority of features and would not improve performance 

 

fics    

project milestone

ankit kumar  raissa largman  

however  the softmax classifiers did much better than naive bayes  likely because the features are
not actually conditionally independent  therefore  we decided to use the softmax l  classifier for the
remainder of our project 
the classifiers do much better on the function prediction as opposed to localization prediction because 
as previously mentioned  a gene can have multiple functions  which increases chances of predicting at
least one correct one 

iv  clustering and interactions
a  graph clustering
our first approach after the baseline implementation was to try and incorporate the interaction data
into the classification  we created a graph representation of the interactions data such that the nodes
were the genes and the weight of connection between two nodes was the absolute value of the correlation
coefficient between those two nodes  we then ran a community detection algorithm that was developed
by ankit kumar for his cs   w final project to cluster that graph representation into    clusters 
after clustering the interactions graph we added the cluster numbers as additional features in the data
set  we then performed classification with these additional features and were able to gain a substantial
increase in performance  this is likely because genes that interact are in similar localizations and are
involved in the same functions 
classifier type

training error

testing error

softmax l 

     

     

table    results of classifier with graph clustering on localization prediction

classifier type

training score

testing score

softmax l 

     

     

table    results of classifier with graph clustering on function prediction

b  k means on features
our second approach involving clustering was to perform k means clustering on the existing feature
set  and then add those cluster numbers as additional features as well  this did not have any noticeable
effect on performance  due to the high dimensionality of the feature space  we performed pca on the
data in order to reduce the dimensionality  and then clustered the data  however this only led to a
marginal increase in performance  this is likely becuse the clusters werent meaningful  with only
binary features the vector space created by our features was high dimensional and a point either had
distance in a dimension  i e the feature was    or didnt  running pca transformed the vector space 
but likely did not find meaningful principal components to transform it into a space in which k means
clustering in this way would be effective 

 

fics    

project milestone

ankit kumar  raissa largman  

classifier type

training error

testing error

softmax l 

     

     

table    results of classifier with pca and kmeans on localization prediction

classifier type

training score

testing score

softmax l 

     

     

table    results of classifier with pca and kmeans on function prediction

c  knn with localization
the final approach was to implement a variation of k nearest neighbors as a feature for our classifier 
in particular  we added k features to the genes that represented  for each k  the localization of that genes
kth nearest neighbor  the results were surprising  as the new classifier grossly overfitted on the training
set and hurt test set performance considerably  we made sure that we werent allowing a genes nearest
neighbor to be itself  so that in the training set we found a genes k nearest neighbors by finding its k
nearest neighbors in the training set minus that gene  therefore  we conclude that the only explanation
for this massive failure of k nearest neighbors is that in the training set  a genes nearest neighbor is very
indicative of its localization  but in the test set  a genes nearest neighbor is not very indicative  this
could be the case if  for example  all the genes in the training set have neighbors that are very similar to
it with the same localization  but genes in the test set arent very similar to many genes in the training
set  so its nearest neighbor actually is not so near  and thus it is a bad predictor 

v  feature selection
a  feature ranking
the discrepancy between training error and testing error in the above tests  as well as the large
number of features  suggested that the classifier was overfitting  thus feature selection seemed like a
natural next step  we employed a feature ranking algorithm which chooses the k highest scoring features
based on univariate statistical tests  we chose k     as our final number of features since lower than this
led to decreased performance  and higher than this had no change  however  feature selection was not
very succesful because overfitting was not substantially reduced and performance was only marginally
increased 

b  results and analysis
classifier type

training error

testing error

softmax l 

     

     

table    results of classifier with feature selection on localization prediction

 

fics    

project milestone

ankit kumar  raissa largman  

classifier type

training error

testing error

softmax l 

     

     

table    results of softmax classifiers with feature selection on function prediction
we likely did not see great improvement in performance or a decrease in overfitting because many data
vectors are missing many features  which means that removal of features can cause some data vectors to
have very few reamining features 

vi  conclusion and future directions
we were able to achieve results that were similar to those of the top scorers in the      kdd cup
data mining competition  the top scorer in localization classification had an accuracy of        or a
testing error of        the top scorer in fuction classification had a testing score of        these results
were not achieved with machine learning as taught in      but instead with other techniques such as rule
based learning 
in working with our data  including information from the graph of interactions data was the major
contributer in helping improve accuracy  this is likely since genes that interact often has similar function
and localization  however  the high dimensionality of the feature space led to a discrepancy in training
and testing error that suggested overfitting  our attempts to implement feature selection were only mildly
successful due to the binary nature of our data and that many data vectors were missing a substantial
number of features 
future directions to take to improve performance could include natural language processing techniques
since our data is similar in nature  as bag of word models have binary features as well   and also has high
dimensionality 

vii  references
    kdd cup       kdd cup       kdd     sept        web     dec       

 

fi