sentiment analysis on email archives using deep learning  advised by
sudheendra hangal 

abstract
i have attempted to use deep learning as a
feature in svm to classify email archives into
positive or negative categories  i have primarily
focused on the email body of users inboxes 
the concept of deep learning has been used at
each sentence level   which includes labels for
every syntactically plausible phrase in thousands
of sentences  allowing us to train and evaluate
compositional models  this in turn  is applied to
svm as a feature in order to reduce the error 
   introduction
sentiment analysis is a key topic and has become
very instrumental in a large majority of fields such
as social media  human resources within
corporations  customer relationship management 
amongst others  manning and socher have
successfully applied deep learning to movie
reviews  here we had certainty that the content of
these reviews will contain opinions and words that
will most definitely point us in the direction of
ascertaining a positive or negative sentiment  i have
leveraged rntn deep learning as a feature in
svm  the end goal is to try to utilize it in
conjunction with the algorithms we learned in class
to achieve the least possible error in the
classification of emails into positive or negative  i
have done so by analyzing email body of users and
then extracting some sort of positive or negative
emotion from it  muse  memories using email   a
system that combines data mining techniques and an
interactive interface to help users browse a longterm email archive  muse analyzes the contents of
the archive and generates a set of cues that help to
spark users memories  communication activity with
inferred social groups  a summary of recurring
named entities  occurrence of sentimental words  and
image attachments 
   dataset
muse is installed and run locally from each user s
machine  each user specifies one or more sources of
email to muse  including online pop imap servers
or mbox format files stored on a local file system 
the user can select folders to analyze from each
source and optionally apply filters by date range  or
tell muse to only analyze their email messages  in

the test dataset  i had access to approximately       
public emails of sarah palin  out of which       
emails have been labeled as positive or negative by
muse  this data was already labeled and made
available for testing  out of these        were
negative and positive was        greif category
accounted for     emails  anger was     
upon running muse  the email data is successfully
categorized into following categories 


















anger
congratulations
family
festive
grief
life event
love
medical
memories
milestones
negemo
posemo
racy
religion
superlative
surprise
vacations

muse inherently maintains its own lexicon and
simply applies the bag of words model for
classification  it has a lexicon for each of the
categories listed above  during classification  it is
simply performing a search query for each word of
the lexicon on the email body  if a successful match
is found  then it classifies the email into that specific
category 
   multinomial nave bayes
as a first step  i ran nave bayes on the same dataset
that muse ran on  i was able to do using a simple
set up in matlab  to classify our email messages  i
used a multinomial naive bayes model  further  i
have used simple cross validation  i have chosen to
split the data as     as training data  traindata  and
remainder     as test data  testdata   as we know 
one drawback to this approach is wastage of data 
in our case  the wastage is about     
    methodology

fii used the test data set for sarah palins emails
already provided  the first step was extracting the
email body from the given dataset  metadata
associated with the email such as sent date  email
headers  from  to  received by  return path etc   had
to also be removed and only email body was taken
into consideration  the data was then preprocessed
in order to help with the classification effort and
accuracy 
    data pre processing

the email body had to be preprocessed in order to
help make the algorithm more accurate and also run
efficiently  to some extent 
stop word removal  stop words such as the  and

etc  have been removed since they provide no useful
information about the overall sentiment of the email 
thus  they will not help us determine whether an
email is positive or negative 
removal of non words  i have removed numbers and

punctuation  all white spaces  tabs  newlines  and
spaces  have all been trimmed to a single space
character 

positive   posemo
my main motivation behind choosing these
categories was due to the obvious nature of these
emotions  as in  other categories such as marriage 
life events were a bit ambiguous and were mostly
falling into the neutral classification  with anger and
grief categories  the strong words used in these
categories clearly eliminate the possibility of these
emails being neutral 
upon manually analyzing the results of muse
categorization  i found there to be a lot of duplicates
in classification  for example  an email containing
the phrase this is a state problem was categorized
correctly as a negative email due to the word
problem  the same email was categorized as
positive due to the word proud     txt 
i also found the classification to be incorrect in a lot
cases  for example  there was a clearly negative and
sarcastic email including the phrase sarah pailn
must be killed  yet  this email has been labeled
by muse as positive  this is most likely due to the
presence of the word proud      txt 

    feature representation

thus  it was easily identified there is most certainly
room for improvement upon the existing
classification model to reduce error 

i ve taken the uniform row approach  representing
the features in the way  we are able to have uniform
rows whose lengths equal the size of the dictionary 
in essence  my feature count is about        which
is basically the size of the vocabulary 

it can be calculated that muse inherently generates
an error of about          i calculated this error by
counting how many of the total muse files were not
present in the ground truth files for positive and
negative categories  as established above 

    results for nave bayes

here  i used     misclassification error  we have
established that the error rate is about     
after a quick analysis  i was seeing a lot of duplicate
data  same email categorized as positive and
negative  thus  i ran nave bayes again and tried to
be more aggressive with the smoothing but was not
getting good results as the error rate was still
approximately        with smoothing parameter set
to    when i set it to      the error was         as i
increased the value of the smoothing parameter  i
found the error rate was not decreasing further 
   establishing ground truth
in an effort to establish ground truth  i manually
labeled all of       the test emails and categorized
them into two main categories of positive or
negative emotion  thus  i encapsulated some of the
existing muse categories into these two categories
as well  below are the categories i considered 
negative  negemo  anger  grief

   rerun nave bayes and svm
once i had correctly labeled data  i re ran nave
bayes and svm on this data  this time  i used the
scikit as i wanted to leverage its built in methods
and features such as cross validation  i have used
multinomial nave bayes and svc with linear
kernel  here again  i used     misclassification
error  i used traindata to generate our training and
cross validation errors  using   fold cross validation
in grid search  in scikit   we were able to identify
optimized parameter values for model selection 
using the bag of words model  i found the
following 
linearsvc c       

misclassification
cross validation error



 note  this error is calculated based on the logic
explained in this report  this error is not derived
from any published data on muse and hence should
not be quoted  
 

fi          least 
svm
with
kernel c      

model

linear misclassification
cross validation error
         least 

nave bayes

misclassification
cross validation error
        

parameter
value

misclassification
validation error

linear

c      to
      with
  
increments

      

rbf

gamma
           
      
       

      

polynomial

c      to
      with
  
increments

      

linearsvc

c      to
      with
  
increments

      

svc with
kernel

svc with
kernel

   deep learning
thus  we look into deep learning algorithm in an
effort to get better results  the goal of deep learning
is to explore how computers can take advantage of
data to develop features and representations
appropriate for complex interpretation tasks  this
model works best on individual phrases sentences 
thus it shows great results for analyzing tweets  our
analysis of emails falls in the middle of analysis of
large documents and analysis of phrases  i have used
the actual email body and broken it down into
unique sentences  i have then applied the rntn
deep learning algorithm to each sentence  this
gives us a rating for each sentence in the email body
and we can represent the data as follows 
file
name

very
negative

negative

neutral

very
positive

positive

      txt

 

 

 

 

 

      txt

 

 

 

 

 

thus  the overall positive or negative score of an
email is calculated based on adding all the columns
above  in order to improve upon these results  i also
then took into account the first   sentences and the
last   sentences of the email  if the initial result is
neutral  then we calculate the rating for the first and
last   sentences  the resulting rating is then assigned
to the email  applying this methodology  i found
that the result set still did not improve 
as i saw  there is a large amount of data for
individual sentences  i was faced with the problem
of aggregating this data and in turn classifying a
whole email as positive or negative  thus i decided
to use svm and implement rntn deep learning as
a feature in svm  i ran it against the same dataset
that we ran nave bayes on  the    
misclassification cross validation error was        

cross

thus  upon analysis  i found that i was seeing a high
bias problem as my data was getting under fitted 
thus  i started exploring more features 
   analysis
as we know  the bag of words model is a simple
classification disregarding grammar and even word
order but accounting for word frequencies  this
model works great when we have documents with
large amounts of text  emails do not necessarily
classify as large size documents and thus our
accuracy applying simple bag of is not very high in
section   and upon re run in section   the    
misclassification error was reduced to      
as we see from the results thus far  the error   is
fairly high and we need to reduce it down 
application of rntn deep learning algorithm as
an svm feature was not sufficient to improve the
results for better accuracy  one conjecture i could
draw was that the inaccurate results could be due to
the fact that deep learning is meant to be applied
individual sentences and hence does not to provide
reasonable results on larger text such as emails 
thus  i decided to utilize email meta data  along
with deep learning as features in svm algorithm 
the meta data features were primarily day  month 
year of email  muse lexicon usage  with this
approach  i was able to reduce my features from
       to    thus  i was able to capture the sentence
level analysis of deep learning and bag of words
model used in muse 

fi   sentence sentiment features  bag of words 
lda and more features
i have applied linear svm to our dataset and
implemented the following features 


day
of
email
 mon tue wed thur fri sat sun 



month
of
email
receipt
 jan feb mar apr may jun jul aug sep oct
 nov dec 

receipt



year of email receipt            



muse positive lexicon  posemo 



muse negative lexicon  negemo  anger 
grief 



deep learning positive sentiment



deep learning negative sentiment



lda

day   month   year  changing these features
seemed to have a significant impact on the
result set  it was observed that the number of
emails  positive and negative  was
significantly increased as the announcement
of vp ticket came in 
muse  positive negative  lexicon   we were able to
leverage the already established lexicon in
muse that is maintained by emotion  for the
purposes  of our classification  i have used
negemo  anger and grief to represent a
negative sentiment  for positive sentiment  i
have used muses posemo 
deep learning  positive negative  sentiment  this
was applied at each sentence of the email and
it seemed to have a big impact on the overall
result set 
lda  we know  given a set of documents  lda
tries to learn the latent topics underlying the
set  it represents each document as a mixture
of topics  generated from a dirichlet
distribution   each of which emits words with
a certain probability  upon running this on my
email dataset  i was able to extract    topics
from this dataset  further  i was also able to
establish a mapping of all the documents by
topic weights  i then used this as a binary
feature   which will simply identify whether
the email represents a specific topic or not  i
found that this feature helped decrease the
error a fair amount  prior to it  i was getting
an error of        and with this feature in
play i got         thus  it was a great boost 

model

parameter
value

misclassification cross
validation error

linear

c      to
      with
  
increments

      

rbf

gamma
           
      
       

      

polynomial

c      to
      with
  
increments

      

linearsvc

c      to
      with
  
increments

      

svc with
kernel

svc with
kernel

   cross validation
regularization

 

grid

search

and

i used traindata to generate our training and cross
validation errors  using   fold cross validation in
grid search  in scikit   we were able to identify
optimized parameter values for model selection 
here  i used     misclassification error  the
optimized parameters are listed below 
linearsvc

c       

least
cross
validation error  
      

svc with
rbf
kernel

c          gamma    

least
cross
validation error  
      

svc with
linear
kernel

c        

least
cross
validation error  
      

polynomial

c       degree    

least
cross
validation error    
    

analyzing the different parameters from grid search 
i then validated the parameter options by calculating
the training error and validation error on the
traindata  in this case  i chose the rbf kernel with
svc which seems to be performing the best  i also

fiobserved that rbf kernel was giving the lowest
error with gamma set to    next  i tried to establish
whether i have a bias or variance problem by
analyzing the values of the training and cross
validation errors 
by plotting the cross validation error and training
error against c  we can infer that our optimized
parameter values are indeed correct  for svc with
rbf kernel 

results while at the same time leveraging
muses current lexicon  we started out with
muse misclassification error of about       
and we ended with a final error of
approximately       thus improving muse
results significantly 
    conclusion and next steps
thus  we can conclude that a mixture of models
yields the best results in the context of
sentiment analysis on muse  using lda
helped for topic modeling significantly helped
reducing the error  now that we have applied
rntn deep learning successfully using the
sub categories of anger and grief  we should
extend this further to the other categories of
muse as well  currently  we have done a
high level sentiment analysis for positive or
negative  future extension would be possible
if we had more labeled data for each category 
further  we can use lda to learn topics that
correlated to muse categories 
references
http   nlp stanford edu sentiment 
http   nlp stanford edu  socherr emnlp     rnt
n pdf

here  the blue line represents the training error while
the red dotted line represents the test error for svc
with rbf kernel 

http   mobisocial stanford edu muse musepapers html
http   suif stanford edu  hangal hangal thesis pdf

we had set aside the remainder     data to report
the generalized testing error  the final error reported
is the generalized testing which is calculated by
using the optimized parameters identified above and
analyzing the training and cross validation error  i
found that the generalized error was varying
between        to        for svc with rbf kernel 
for linearsvc  after analysis of cross validation and
training errors  we updated the value of c from
       to      the final generalization error was
      
    summary
thus  it can be seen that applying svm in
conjunction with rntn deep learning  along
with the features in section    as a feature  we
were able to get very good results  our error
was
reduced
significantly
and
the
classification was fairly accurate  i believe
this is a great use of rntn deep learning
algorithm in the context of sentiment analysis
of emails  it has helped us improve upon
muses existing bag of words model and its

http   nlp stanford edu irbook html htmledition support vector machines thelinearly separable case   html
http   scikitlearn org stable modules cross validation html
http   scikitlearn org stable modules svm html classification
http   nlp stanford edu downloads tmt tmt     
https   github com echen sarah palin lda
acknowledgements
i would like thank sudheendra hangal for
introducing me to muse   providing all the
necessary source code and his valuable input in the
project  i have used the deep learning algorithm
developed by richard socher  alex perelygin  jean
y  wu  jason chuang  christopher d  manning 
andrew y  ng and christopher potts 

fi