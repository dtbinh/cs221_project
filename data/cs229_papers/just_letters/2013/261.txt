a computational model for multi instrument music transcription
cs    final project report  autumn     
zhengshan shi  tony yang  huijie yu
kittyshi  tyang    huijie   stanford edu

for source separation basing on a statistical analysis of
the training data  we take a supervised learning
approach in building such model to apply to our
training data  and the model is updated in an iterative
process through construction  furthermore  we smooth
the separated temporal matrix to get the note matrix as
a form of music transcription  the whole process is
illustrated in the following flow chart 

abstract
the aim of our project is to build a model for
multi instrument music transcription  automatic music
transcription is the process of converting an audio wave
file into some form of music notes representations  we
propose a two step process for an automatic multiinstrument music transcription system including timbre
classification and source separation using probabilistic
latent component analysis 

   introduction
automatic music transcription for polyphonic
music is a difficult task in digital signal processing
given the fact that frequency partials of notes from
different instrument will mix up the spectrogram and is
thus difficult to segregate  different approaches have
been proposed in order to get pitch information 
including multi pitch analysis using human auditory
periphery  klaupuri        and source separation  in
the project  we propose a new two step method using a
pre processing stage of instrument classification along
with a traditional statistical approach of source
seperation for decoding the incoming music signal 
given a sound mixture of an instrument ensemble  for
example  a flute cello duet   our system is expected to
recognize and generate the music transcriptions  in
forms of a note matrix  for each instrument voice
respectively 
the whole project is divided into two parts 
first  we implement the instrument classification stage
in a supervised setting  training models of different
instrument spectral characteristics are used for
estimating the component in a sound file assuming that
they are unknown  second  based on the result from the
instrument classification  we pick up corresponding
basis vectors from a pre trained basis vector library and
implement the source separation  in order to get the
note matrix for each instrument layer from a
polyphonic music piece  we implement the plca
 probabilistic latent component analysis  algorithm

fig   flow chart

   stage i   instrument classification
we use the mel frequency cepstral
coefficient  mfcc  as representations for instrument
timbre features  by comparing the feature vectors  we
comprise the classification method that categorizes a
piece music played by unknown instruments to match
a known instrument in the training data set  and were
using k nearest neighbor classifier and softmax
regression for the classification process 

fig   process of feature  mfcc  extraction

 

fi    feature extraction  mfcc

vectors  then we listed the k  we tested the case of
k       and     nearest neighbors of the testing
vector and claim it has the same instrument type as the
majority of its k nearest neighbors 
the softmax regression builds a hypothesis
based on the training set of feature vectors  and then
each of the testing vectors is evaluated by the
hypothesis  we then take the most probable guess to
estimate the instrument category of that testing vector 

a first step of our approach of music
transcription is instrument classification  timbre is a
multi dimensional sound perception which enables us
to distinguish the difference between multiple
instrument classes and sound quality  timbre spaces
project the sound file into a low dimensional space
which describes the spectral envelope of musical
instruments  we choose the mfcc feature  which is
widely used in speech recognition  we performed a
discrete cosine transform to the mel scaled log
frequency spectrum  and took the first    coefficients
that captures best the spectral shape and envelope of a
musical instrument as the feature vector  our time
frame is chosen to be   second long  meaning that a   dimensional feature vector is extracted for each second
of the audio wave file 

after categorizing each testing feature vector
independently  we combine the classification results of
all testing vectors for a single piece of solo music  in
our case  we group    feature vectors for a    second
piece of music  and claim that the current piece of
music is played by the instrument type which
corresponds to the majority of the    feature vectors 
the results are evaluated by calculating the hit ratio 
i e   the ratio of feature vectors that provides us the
correct instrument type  which was judged by
musicians 

   stage ii  source separation using plca
the plca algorithm does a non negative
matrix factorization on the audio spectrum  v   and
decomposes it into a spectral basis matrix  w  and
temporal weight matrix  h   each column in the audio
spectrum v is the frequency distribution of the piece
of music at a certain time frame  and we name it     
each column of the spectral basis matrix w is
composed of a set of spectral basis vectors represented
by      and each column of the spectral basis
matrix is a spectral basis vector  the spectral basis
vector    is the frequency distribution of a pitch
played by one instrument  each column of the
temporal weight matrix h is a distribution of weights
   for spectral basis vectors at a certain time frame 
so when we multiply the weights on the spectral basis
vectors and take the sum of the products  we will get
one column of the audio spectrum  in mathematical
notation  we have  

fig   mfcc for the training set

    training and testing process
there are two classification algorithms
implemented in this stage  k nearest neighbors  k nn 
and softmax regression  both algorithms treat the
feature vectors as points in a    dimensional space  and
we label each training feature vector with a number
corresponding to an instrument type  for the testing
process  we divide the testing data into   second long
frames  and each of them is represented by a   dimensional feature vector  we then evaluate each of
the testing vectors    second long frames 
independently  and take the most frequent guess as the
overall estimation for that piece of testing data 
the k nn algorithm measures the euclidean
distance between the testing vector and all training

      

 

     

to get the spectral basis vectors     and
their respective weights      we use an em
approach that estimates the posterior distribution
    in the e step  with    and     in the m
step  in mathematical notation  we have 

 

fie step 

test case i 

     
       

       
m step 
      

      

    
        

hit ratio

flute

clarinet

trombone

cello

piano

violin

k nn

    

    

    

    

    

    

softmax

    

    

    

    

    

    

   
 

 

test case ii 

        
        

the incoming audio signal is first transformed
into an audio spectrum through short time fourier
transform  stft   with fft length of       hop size
of     and a sampling frequency of       samples per
second  which gives us a time resolution of     ms 
the initial spectral basis vectors are fixed they
are extracted from a pre trained library of basis vectors
for different instruments and pitches according to the
instrument label detected in the first stage  in the
construction of the basis vector library  notes from a
frequency range of c  to c  played by    different
instruments were synthesized using cakewalk sonar 
we keep     seconds for each note  and perform a
stft on the basis note to extract the spectral feature
vector as the basis vectors 
once we load the spectral basis vectors into the
algorithm  temporal weights and spectral basis vectors
are iteratively updated based on the original audio
spectrum  the resulted temporal weight matrix h
represents the notes played by an instrument along the
time axis  since the probability of a pitch shift in a
short amount of time frames is low  to eliminate the
systems over sensitivity to transients and noise in the
temporal matrix  we implement a moving average filter
to smooth the detected pitch among neighboring frames 

hit ratio

flute

clarinet

cello

flute cello

clarinet piano

k nn

    

    

    

    

    

softmax

    

    

    

    

    

based on our preliminary result of instrument
classification  we see that our algorithm was more
robust on string instrument  cello   violin   a
possible reason for that is that string instruments have
a smoother spectral envelope  which makes the feature
extraction part more accurate than the woodwind
family  since our aim for the final project is to
separate a duet of a woodwind   string combination 
the algorithm for detecting the woodwind   string case
works efficiently 
after the pre processing instrument
classification stage  we apply plca algorithm on a
piece of duet music played by flute and cello  we take
the first     seconds of the music  and the spectrum is
generated with stft  then we combine our
synthesized basis vectors into a spectral basis matrix 
the calculated temporal weights matrix represents the
notes played by an instrument  the temporal weights
for the first five basis vectors is illustrated in the
following figure  with the horizontal axis for time  and
vertical axis for intensity 

   evaluation
we were having two testing cases for
instrument classifications  test case i includes music
pieces from   different solo instrument types  and test
case   includes three solo instruments and two
instrument ensembles  the hit ratio of instrument
classification with two different algorithms were
characterized in the following table 

fig   temporal weighted matrix for cello part
 

fithe accuracy of the temporal weights matrix
compared to a musician annotated ground truth is
originally      with a moving average filter applied
on each row of the matrix  we have an improved
accuracy of     
   conclusion
this project uses a fixed based plca to
factorize the audio spectral mixture into music
transcription in form of note matrices  compared with
the traditional statistical method  our approaches use
an additional pre processing stage of instrument
classification for basis vector estimation  an incoming
audio mixture is first coming through our instrument
classification system  the basis vectors are then
picked up from a pre trained library containing
different basis vectors of notes played by various
instruments  based on the basis vector  we perform a
plca algorithm to factor out the matrix into a
spectral matrix and a temporal weight matrix  we then
applied the moving average filter for post processing
of the temporal weight matrix to get a cleaner note
matrix  which is a representation of the polyphonic
music transcription  our experiments show a robust
result for detecting woodwind   cello instrument
family duet  future work will include an unsupervised
approach of instrument classification based on timbre
space analysis  and a more complete basis vector
library including note features from different playing
techniques for a particular instrument 

fig      separated cello note matrix

fig      smoothed cello note matrix after applying the
moving average filter

reference
    m  shashanka  latent variable framework for
modeling and separating single channel acoustic sources 
     
    p  smaragdis  b  raj    m  shashanka  supervised and
semi supervised separation of sounds from singlechannel mixtures       
    j  bello  g  monti    m  sandler  techniques for
automatic music transcriptions       
    t  hoffman  unsupervised learning by probabilistic
latent semantic analysis       
    d  lee    h  seung  algorithms for non negative
matrix factorization       
    m  shashanka  b  raj   p  smaragdis  probabilistic
latent variable models as non negative factorizations 

fig      musician hand annotated ground truth

    e  benetos  s  dixon  d  giannoulis  h  kirchhoff    a 
klapuri  automatic music transcription  breaking the
glass ceiling 

 audio source  j s  bach suite en si mineur 
polonaise et double      s 
 

fi