identifying demonstrative lovers in tango texting app
nicolas poulallion

jean yves stephan

stanford university
nicolasp stanford edu

stanford university
jstephan stanford edu

abstractthis paper aims at identifying the demonstrative
lovers in tangos chat interface  i e  the users sending each other
romantic tango surprises 
our initial approach was to use a bag of words representation
of the conversations as well as additional user features that we
defined in cooperation with tango operational team  we then
conducted a svm classification with a linear kernel  reaching
    accuracy but a bias vs  variance diagnosis revealed that
there was potential in increasing the model complexity  a svm
with a rbf kernel gave us indeed better results  but with
unacceptable computation time 
therefore  our second approach was to focus on an industrially
operational solution  using only the conversations logs  the use
of n grams combined with optimized features normalization and
selection enabled to build a program that could both accurately
and efficiently identify demonstrative lovers based on their realtime conversations 

keywords  tango  text classification  supervised leaning 
support vector machine  texting app 

i  e xperimental d esign
in this section  we present the datasets used for our experiments and the features we constructed 
a  datasets
we received   different  txt files from tango 
 a file consisting of more than         casual conversations that took place in the us during the first week of
november  with encrypted user ids 
 another file consisting of more than        love conversations  with the same geographic and temporal attributes 
 a file with users statistics  such as the gender and the
number of messages sent as well as the number of tango
surprises sent each day 
 a file with  for each conversation  the number of messages and tango surprises sent each day 
b  features extraction

i ntroduction
tango is a social networking app enabling its      million
users to connect with friends by sending free text messages 
making phone video calls  and playing games  to personalize
a text conversation  the users can send each other animated
emoticons called tango surprises that pop up on the screen 
they are widely used to help deliver your speech in a fun
and illustrative way  some of them are free  but there is also
a large variety of premium surprises that can be bought in
thematic packages for       each 
the objective of this project is to predict whether two people
are likely to send each other romantic tango surprises based
on their recent conversation and additional user features 
eric setton  co founder and cto of tango  explained us
there would be great value in targeting those specific users
in marketing campaigns for premium surprises packages 
it could also be a first step toward real time suggestion of
content related tango surprises  through weekly exchanges
with tango data analysis team  our project aims at providing
a concrete  industrial oriented solution 
throughout this paper  we will use the notation love to refer
to the parameters related to the conversations containing the
selected romantic tango surprises  the notation casual will
refer to the other conversations 

   conversations selection  first  in accordance with
tango  we decided to keep only the significant conversations 
i e  conversations with at least   messages and    words in
total in our datasets  this first filter reduced our datasets
to about        and        conversations respectively  in
average  there are    messages per conversation and   words
per message 
   conversations logs 
a  words selection  using c    we used a stemming
library  see      to extract the root of the words  we introduced
five additional tokens to capture non verbal information 
 sadsmiley corresponds to sad emoticons  such as   
 happysmiley corresponds to happy emoticons  such as   
 heart corresponds to   
 brokenheart corresponds to    
 url replaces all the words containing http or www 
we decided to focus only on the       most frequent words
in the entire corpus of conversations   after stemming  this
decision does not affect our accuracy  as established in     
but considerably improves our efficiency  each conversation
will then be represented as a vector of size       containing
the number of occurrence for each word in our dictionary 
for reference  we implemented a quick naive bayes to get an
informal guess of the most relevant words  even if the naive
bayes assumption is clearly not verified  for example  two
different spanish words cannot be considered as conditionally

fiindependent   the    words with highest relative frequency
ratio were  amor  quiero  love  baby  mi  mucho  babe  por 
kiss and te 
b  language  a striking observation was that the spanish words seemed to be really significant  even those lacking
a romantic overtone  this led us to add the language to our
features  we created an additional boolean feature testing if
the conversation is in spanish  the percentage of spanish
conversations varies between      casual conversations  and
     love conversations  
c  number of words  for each conversation  we counted
the total number of words 
d  timestamp  for each conversation  we created a vector of size    containing the hourly number of messages sent 
fig    shows the frequency for each hour for love and casual
conversations 
   
newtimestamplove
newtimestampcasu

    

    

    

    

    

    

ii  s upport v ector m achine   a ll f eatures
we conducted a svm classification on all the features 
in section a we use a linear kernel since linear kernel
svms are recommended when both numbers of instances
and features are large as in our dataset  see       we used
the liblinear svm library  see      to implement a l regularized support vector classifier 
in section b we implement a gaussian kernel using the
libsvm library  see      in order to reduce the bias 
a  linear kernel
   implementation and parameter optimization  to ensure
the efficiency of our algorithm while keeping a good accuracy 
we used the l  loss primal solver 
we also chose to normalize our features to a        scale by
dividing each feature by its maximum  compared to other
normalization methods  this scaling has the advantage of
preserving the sparse property of the design matrix  which has
a major impact on the algorithm efficiency  normalizing by
mean and variance multiplies computation time by   while
giving similar results  
in order to set a good trade off between efficiency and computation time  we tested the    fold cross validation accuracy on
exponentially increasing values of the cost parameter c  the
curve on fig    ends with an asymptotic value  as indicated
in literature  see theorem   in       proving that c      is a
fair choice 

    

  

    

  
    

chosen parameter
c     

  
 

 

  

  

  

  

fig     timestamps frequency

   users statistics 
a  gender  for a given conversation  we indicated the
gender of the two users  the data gives us   different genders 
unknown  gender   and gender    according to      we decided
to convert those categorical attributes into numeric data  i e   
dimensional vectors defined as follows 
           for unknown 
           for gender   
           for gender   
b  conversation statistics  we indicated the number of
messages and tango surprises sent each day  as well as the
number of active days for each conversation 
c  users activity  for the two users involved in a given
conversation  we stored the number of messages and tango
surprises sent each day  as well as the number of active days
on tango 
our final dataset therefore consists of         examples
and       features 

  
accuracy    

 

  

  

  

  

    
 

  

  
  
parameter c  log  scaling 

   

   

fig     cost parameter optimization

   results and performance  we split the dataset randomly
into a training dataset         examples corresponding to    
of the total dataset  and a test dataset         examples or
    of the total dataset  and then computed the classification
  in   s with matlab  classification outputs on the train
dataset are shown on table i 
the accuracy rate reaches      improving significantly
our initial quick naive bayes approach        the recall

fipredicted

actual

casual

love

casual

       

      

love

      

       

  

table i  classification outputs  accuracy        

rate  measuring the frequency of love conversations that our
algorithm successfully identifies  is of      this rate is our
principal metric  since tango does not want to miss any
potential love tango surprises customer  the precision rate
        is also an important metric because tango wants
to avoid flooding its users with marketing offers that do not
concern them 
to make a bias vs  variance diagnosis  we repeat the same
analysis on increasing  random  sample of our dataset  the
learning curve we obtain is shown on fig    
   
test error
training error
    

   

 fold accuracy    

  

  

  

  
    
   
  
    

   
   

    

   

gamma

c

   

   
   

fig     c and  selection

shown on fig    
although this result was slightly better than the one we
previously obtained with the linear kernel  the computational
time    hours  cannot be used by tango in an integrated
industrial system  the next section will therefore focus on
how to achieve an accurate and easy to compute classification 

    

iii  o perational s olution   c onversations l ogs

   

    

   

    

 

 

  

  

  

  
  
  
  
dataset size  as a   of total data 

  

  

   

fig     learning curve

the test error stabilizes at about      our dataset is large
enough to train the algorithm so gathering more data is not
an issue  however  the gap between the training and test error
reduces down to     which is very small  it suggests that our
model may underfit the data  bias issue   and that we should
increase the model complexity to try to get better results 
b  radial basis function kernel
using a more complex kernel appears to be a good way
to increase our model complexity  since it allows to grasp
more interactions between our features  keeping the same normalization as previously  we tested the   fold cross validation
accuracy of our training set over a grid of model parameters
 c    
this analysis showed that the best couple  c    was
           with a cross validation rate equal to         as

to create an industrially operational solution that tango
could easily integrate to its system  we will now only use the
conversation logs  i e  the conversation itself and the associated
metadata  we will not use the users statistics anymore  as it
requires accessing another database  which cannot be done in
real time  furthermore  the users statistics we used in the
previous model probably introduced a bias  in particular  the
number of tango surprises sent each day is correlated to the
probability of sending a love tango surprise 
this choice will decrease a little bit our accuracy  but allows
us to save computation time and thus makes it more suitable
to a real time application 
a  n grams
we switched from the naive bag of words representation to
a more complex one  using contiguous sequences of n words
or n grams  unigrams  bigrams and trigrams  
we fixed a threshold to focus only on the         most
frequent trigrams  out of more than            and the       
most frequent bigrams  out of more than          and kept
our previous       unigram dictionary  here again  setting
this threshold notably increases our efficiency without really
lowering our accuracy  cf       
b  features selection
using such a large set of unigram  bigram and trigram
features requires a good selection method so as to prevent
overfitting issues  since traditional methods such as forward

fiii showed that the bag of words model underfitted the data 
however  a saturation phenomenon appears when more and
more bigrams  respectively trigrams  are used  as shown on
fig     the reason being that adding more features above the
optimal threshold leads to overfitting issues 
  

  

  

  

  
  
  

accuracy    

accuracy    

or backward search are computationally intractable  we
chose to filter the top k   respectively k    k    best unigrams
 respectively bigrams  trigrams  according to a score function 
we chose the bi normal separation metric  which
substantially outperforms most other metrics as indicated in
     if tpr  respectively f pr  denotes the true  respectively
false  positive rate i e  the frequency of love  respectively
casual  conversations containing a given n gram  then the
bns score is simply f    tpr   f    f pr  where f  
is the standard normals distribution inverse cumulative
probability function  for intuition  this score measures the
distance between two hypothetical thresholds  such that a
random draw from the normal distribution exceeding this
threshold would trigger the occurrence of the n gram in a
love  respectively casual  conversation  as shown on fig    

  

  

  

  
  
  
  

  
  

 

    

    

                        
number of bigrams taken in account

    

          

  

 

   

   

   
   
 
   
   
number of trigrams taken in account

   

   

 
 

x   

fig       fold cross validations led independently on bigrams and
trigrams reveal the optimal number of features to select

c  normalization

fig     bi normal separation metric

table ii gives an example of the    most significant love
n grams according to the bns metric  a high score is usually given to n grams that are semantically and statistically
significant  improving our classification accuracy  
unigrams

bigrams

trigrams

beso
amo
amor
kiss
lindo
corazon
bello
love
tokenheart
quiero
babe
encanta
miss
babi
hermoso

te amo
mi amor
love you
si amor
ok amor
te quiero
a kiss
amor no
love u
un beso
mi corazon
hola amor
miss you
quiero mucho
ok bb

te amo mi
mi amor te
the amo y
i love you
i love u
te mando un
babi i miss
love you too
good morn my
love u so
tenga un lindo
babi i love
call me i
ya me voy
i miss you

table ii  most significant love n grams using bns

having computed that score for each n grams feature  we
then used   fold cross validation to determine the right k   respectively k    k    number of unigrams  respectively bigrams 
trigrams  to select  optimizing these values independently  i e 
by doing the classification using only unigrams  bigrams or
trigrams  led to choosing k            k           and
k             the whole unigrams dictionary is therefore
selected   this is no surprise since our analysis of section

before performing feature wise normalization to a       
scale as in section ii  we used the term frequency inverse
document frequency  tf idf  statistic for conversation wise
normalization  this metric takes the raw term frequency of
a given n gram in a given conversation  and scales it by a
multiplicative factor that depends on the occurrence of the ngram over all conversations  precisely  for a n gram w in a
conversation c  c  the tf idf weight is calculated by 
tf idfw c   fw c  log

 c 
  c  c   w  c  

the second term can be seen as a discriminatory power
factor that emphasizes the rare words and diminishes the nonsignificant common words  the combination of these two
normalization steps constitutes an important improvement in
our approach  increasing accuracy by at least     it also
ensures an efficient computation on the liblinear library 
while preserving the sparse property of the matrices  which is
absolutely necessary considering the scale of our problem 
d  results
over our complete         examples dataset  using the
previously described normalization and features selection
method  we searched the optimal values of the number of
unigram  bigram and trigram features to select by computing
  fold cross validation over three nested loops  it resulted
in keeping the entire       unigram dictionary and adding
the top       bigrams and       trigrams according to the
bns score  with this choice of parameters our classification
algorithm reaches an accuracy of       on the test dataset
     of total data  see table iii   which outperforms the
section ii approach even though the informative user features
were dropped off  computation time is about one minute on
a m atlabx   on the corn cluster 

fipredicted

actual

casual

love

casual

       

      

love

      

       

table iii  classification outputs  accuracy        

the feature selection approach has the nice advantage of
setting a good trade off between bias and variance issues 
whatever the dataset size  if more examples were given  more
bigrams and trigrams would be used in the classification  fig   
shows the training and test error of the classification with
these fixed parameters  on an increasing random portion of
the dataset size  average over   runs   compared to fig    
the test error now only stabilizes when the whole dataset is
used  which shows an intermediate stage between a definite
bias issue and a definite variance issue 

fig     accuracy improvement through increasing model complexity

    
test error
training error

c onclusion

   

    

   

    

   

    

 
   

   

   

   
   
   
   
dataset size  as a   of total data 

   

   

 

fig     final learning curve  average over   runs 

finally  let us give an overview of the approaches of
increasing complexity that we developed in this paper  see
fig      the initial naive bayes classification gave but
little accuracy improvement compared to the     reference
 random guessing taking into account the frequency of love
and casual conversations   the switch to a svm approach 
based on words count and on conversation logs  metadata 
has the most significant impact  adding bigrams and then
trigrams mitigates the underfitting issue that we diagnosed in
section ii  allowing a significant    increase in accuracy  but
the tendency shows that adding   grams would probably have
little impact for a very high computation cost  tango was
pleased with the     final accuracy  given the biased nature
of the dataset  all lovers do not send each other romantic
tango surprises  

in this project we implemented a supervised binary text classifier  using a very large dataset of real conversations between
tango users  we were able to predict which users are likely
to send each other romantic tango surprises  with an    
accuracy  we particularly focused on building an operational
solution  which can process real time conversations logs and
identify the demonstrative lovers in tangos chat interface 
our progression led us to tackle many of the main topics
and practical techniques in the field of text classification 
support vector machines   where the linear kernel combined
accuracy and efficiency  as well as features normalization  tfidf metric  and selection  bi normal separation scoring   we
moved from an initial bag of words approach which suffered
from bias issues  to a combination of n grams  n      which
solved the under fitting issue by adding the right amount of
significant information to our model 
acknowledgment
we would like to thank e ric s etton  co founder and cto
of tango  and g uillaume s abran  data analyst at tango
for their precious help and interest in our project  we are also
thankful to a ndrew n g and the cs     teaching staff for
this great class 
r eferences
    c hih  w ei h su   c hih  c hung c hang  and c hih  j en l in  a practical
guide to support vector classification       
    rong  e n fan   k ai  w i c hang   c hih  j en h sieh   x iang  rui
wang  and c hih  j en l in  liblinear  a library for large linear
classification       
    g eorge f oreman  an extensive empirical study of feature selection
metrics for text classification       
    s  s athiya k eerthi  and c hih  j en l in  asymptotic behaviors of
support vector machines with gaussian kernel       
    oleander stemming library  copyright       oleander software  ltd  all
rights reserved   available at http   www oleandersolutions com stemming 
stemopensource html  

fi