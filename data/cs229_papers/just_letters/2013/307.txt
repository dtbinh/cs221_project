cs    machine learning

 

classifying ephemeral vs evergreen content on the
web
li wei chen  lwc stanford edu 

i  i ntroduction
ne of the strengths of the internet is the proliferation
of content available on virtually any topic imaginable 
the challenge today has become sorting through this wealth
of content to locate the information of greatest interest to
each user  many sites today implement recommender engines
based on expressed and learned user preferences to direct users
towards new content that the engine believes they will most
enjoy  the relevance of such content can either be highly
topical and short lived  such as last nights sports scores  or
enduring and long lived  such as an introductory tutorial to
machine learning algorithms   the former content is termed
ephemeral  while the latter is called evergreen 
an interesting challenge is to attempt to predict a priori if a
new piece of content will be in the former or latter category 
not only would it be useful for recommenders attempting to
classify different news stories based on type  this information
could be used for other applications also  such as for archival
projects to determine what web content merits inclusion  or
for content sites interested in capacity planning for hosting
different pages based on expected longevity 

o

ii 

p roject d esign

a  dataset
this project uses the dataset provided by stumbleupon as
part of the stumbleupon evergreen classification challenge
competition on kaggle      the dataset consists of a training
set of       urls that have been hand labelled as evergreen
or not  and an unlabelled test set       urls  we evaluate
the performance of several different classification algorithms
in accurately predicting the evergreen status of different pages 
the metric for evaluation used will be the one chosen by
kaggle for the contest  the area under the receiver operating
characteristic curve  roc auc           the roc is a characterization of the true positive rate against the false positive
rate of a classifier  the area under the roc curve is equal to
the probability that the classifier will rank a randomly chosen
positive instance higher than a randomly chosen negative
instance 
b  feature selection
the basic information available for each training example
is the url of the page  along with the url itself  kaggle
provides a snapshot of the html retrieved from the url
in raw form  the first challenge is to transform the html
page into features that can then be processed by classification
algorithms 

html pages today include much more than the content
text of the main topic of the page  as an illustrative example  consider the page http   www howsweeteats com      
   cookies and cream brownies   which is an example of an
evergreen page for a brownie recipe drawn from the training
data  the page consists of the recipe itself  some anecdotal
descriptions about the authors experiences baking with the
recipe  and photographs of the food in question  all relevant to
the interest level of the viewer to the page  however  it also
contained a drug ad and an automotive ad  as well as verbose
javascript implementing a user tracking system  which is likely
not relevant to user interest  as well as generic items such as
a commenting system and links to various locations on the
parent site which  while they might contain relevant content 
are common components of both evergreen and non evergreen
sites 
some basic intuition on preprocessing the html to perform
feature extraction can be obtained by training a regularized
logistic regression classifier on the raw html and examining
the words with the lowest predictive weight  one insight
from this exercise is that the raw tags themselves contain
very little predictive power  likely because they appear in
virtually all the documents  similarly  the javascript code was
also typically not related to the document contents and not
predictive  preprocessing the html to strip the tags and
javascript and keep only the contents of the tags themselves
both reduced the amount of data that the algorithms needed to
process as well as reducing the noise in the input 
some of the features that were included in this project based
on their predictive possibilities 
 url  page url
 body  body text
 links  body href links
 outline  title and header node contents
c  preprocessing
the extracted features were preprocessed to transform them
into feature vectors  first  the contents of each page were
transformed to standard ascii encoding  the body text and
title and header node contents were common english language
words  and were stemmed using a porter stemmer      the
url features were stemmed by extracting the domain
from each url  the bag of words model was applied to the
stemmed text and domains     
two approaches were used to transform the resulting bag ofwords data into input features for the classification algorithms 
the first computes a document term matrix where the rows
correspond to different training examples  and the columns

fics    machine learning

 

 d 
idf t  d    log
      d  d   t  d  

table i 

naive bayes cross   validation roc auc on trimmed
data

  

  

  

   

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

naive bayes  full 

training
test

    
    
    

    
     

    

    
    
samples

    

naive bayes        

    

where d is the set of training examples  documents    d  is
the number of training examples  and   d  d   t  d   is
the number of documents where the word t appears      the
inverse document frequency will be small when the same term
appears in a large proportion of the documents  and multiplying
it into the term frequency will decrease the weighting on terms
that appear commonly in the majority of documents  and thus
are unlikely to have much predictive power  

a  classifier selection
three different classification algorithms were investigated 
naive bayes  regularized logistic regression  and support vector machines  svms  
the naive bayes classifier was trained on the documentterm frequency matrix  the dictionary was sorted in order of
term frequency  and varying numbers of the most and least
frequent words were discarded to investigate the effects of
trimming the dictionary  trimming removed words with low
predictive strength from the dictionary and helped prevent
overfitting on noisy data 
table i shows the roc auc results evaluated via crossvalidation  it indicates that while trimming low frequency
words degrades the metric  trimming high frequency words has
little to no impact  this suggests that some of the infrequent
words do have predictive power so there is value in retaining
them  but that the high frequency filler words can be discarded
without penalty  this would be useful for controlling the
size of the dictionary and reducing run times for a classifier
based on naive bayes that was being used in a production
environment 
figure   sheds more light on the effect of trimming the
dictionary  the divergence between the training and test error

  

 
 
 
 
  

    

   

iii  r esults
in this section  the performance of several different types of
classifiers is evaluated  the predictive potential of each of the
feature sets is also investigated 

  trim

roc auc

indicate the term frequencies of the different words in the
dictionary  we construct the dictionary by computing the term
frequencies of all words appearing in the entire training set 
and discarding the most and least frequently appearing words 
the rationale for this approach is to discard the filler words
in the english language  such as a or the  which have
high frequency but little information  and also the very lowfrequency terms which do not occur often enough to be
generally useful for prediction 
in the second approach  we use the term frequency inverse
document frequency  tf idf  of each word  the tf idf is the
product of the term frequency  indicating the number of times
a word appears in a given document  and the inverse document
frequency  which measures how commonly the word appears
across all documents  the inverse document frequency is
computed as

    
training
test

    
roc auc

    
    

    
     

fig    

    

    
    
samples

    

    

learning curves for naive bayes

table ii 

l ogistic regression cross   validation roc auc on
trimmed data

  trim

  

  

  

   

 
 
  
  
  
  

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
     

for the full dictionary shows that naive bayes is overfitting
due to the addition of the low content words  the training
and test learning curve convergence indicates that the removal
has reduced the overfitting  although it has not resulted in an
improved score  
table ii shows the roc auc when training the logistic
regression classifier on the with varying amounts of the dictionary trimmed  figure   plots the learning curves for the
training and test set curves for the full dictionary as well as
the         trimmed dictionary  the training curves show
that logistic regression is more sensitive to overtraining  and
the reduction of overfitting seen in the         dictionary
translated into notably improved test error  the gap between
the two curves shows that there is still residual overfitting in
the final solution  despite the improved score 

fics    machine learning

    

training
test

    

 

logistic  full 

svm        

    
    
    

roc auc

roc auc

    

    

    

    

    

    

    
     
    

    
training
test

    

    
    
samples

    

training
test
    
                                                
samples

    

logistic         

fig    

    

learning curve for svm

roc auc

table iv 

p redictive p otential of f eature s et c andidates

    

    
    
     

fig    

    

    
    
samples

    

    

training roc

xval roc

precision

recall

body
outline
links
url

     
     
     
     

     
     
     
     

    
    
    
    

    
    
    
    

learning curves for logistic regression
logistic  tf idf 

training
test

roc auc

    
    
    
    
    
    
    
    
    
     

fig    

feature set

b  feature sets

    

    
    
samples

    

    

learning curves for logistic regression with tf idf features

table iii 

sizes seen for logistic regression  which likely explains why
switching to tf idf features did not have the same impact here 

l ogistic regression cross   validation roc auc on
trimmed data

  trim

  

  

  

 
  
  

     
     
     

     
     
     

     
     
     

since the word frequency dictionary showed that the performance of logistic regression could be improved by reducing
overfitting  the use of tf idf values in the document word matrix as another approach to feature reduction was investigated
next  this approach yielded an improved roc auc of       
with the learning curve shown in figure   
finally  table iii shows the roc auc for an svm classifier
operating on document word frequencies with varying amounts
of the dictionary trimmed  for the svm  trimming highfrequency words had more of an impact than low frequency
ones 
applying svm to the tf idf features resulted in a roc
auc of        which was not an improvement on using the
trimmed dictionary  figure   shows the learning curve for the
      trimmed dictionary  note that the svm does not suffer
significantly from the overfitting problem for smaller sample

to evaluate the relative predictive potential of the various
feature sets  a logistic regression classifier was trained on each
feature set  and the roc auc for each classifier was evaluated 
the precision and recall for the evergreen classifier based on
each feature set was also computed to provide more insight
into the nature of the errors of each classifier  table iv shows
the performance of the classifier based on the different feature
sets 
the first thing of note is that the text based features exhibited generally better performance than the url based features
  not surprising  as the text was generally longer and more
descriptive  especially the body   table v shows the top
text predictors as identified by the classifier  while table vi
shows the top url based predictors  examination of the word
fragments that were most strongly associated with evergreen
samples showed topics related to food  health and fitness 
and finance  topics related to technology  fashion  music  and
sports tended to be associated with ephemeral samples  the
classifiers trained on the url based features effectively associate
different sites with either evergreen or ephemeral content  the
url feature set  in particular  predicts a samples class based
purely on the site on which it was hosted 
while working with the body text yielded the best results 
each classifier was able to correctly predict some samples that
others did not  using n fold cross validation  the percentage
of samples misclassified by classifier a that were also misclassified by classifier b was estimated  table vii shows the
results 
since there is a significant subset of errors for all classifiers
that are correctly predicted by other classifiers  it seemed
likely that training an ensemble classifier using the outputs
of the individual classifiers could further improve the score 

fics    machine learning

table v 

t op e vergreen and e phemeral p redictor w ords

 

table vi 

t op e vergreen and e phemeral p redictor url s

top evergreen predictors

top evergreen predictors

marthastewart gt signag wing frog delight gina breath tasti crack profil cover
pastri halloweekend eco prefer lucyphon tender concentr input meal acn    
arm hate wheel hazelnut scream cookbook gmo pmid tooth mexican asparagu
     hangov leftov ball fondu illustr tension toss hello skin weird tsp teeth
random actress yogurt semenya hungri associ kb parchment pet quinoa     
tin mouth orang wonder function box cord friend     damari salsa submiss
ping chick submit gsc crock campi cooksplu text nexeon ricotta way cannot
strength artichok coupon seriou cafepress cardiovascular rep lifehack extract
dumpl apron african massag heat risk urcrowd hummer dessert direct videoinfo
oat granola goat someecard stokk crossbow jar parkour gram clock alyssa tuna
sina hydropon veri bite ad salmon    ramkissoon espresso period crispi notic
chocolat pong decis    crunchi peirc feta zest cut rest partnership write technic
macaroni elearners phyllo dessertstalk ivillag root formal monsanto stockist bone
photographi guru womenswear enjoy brow teaspoon diesel subscrib averag heavi
never maegan dinner start sauna sinu method timelin programm soy culinari
suggest help herb html  entertain sesam fred johnni lifestyl mayb margin
toothpast databas lasagna three water warrior carbon nuclear diego almond
contain broke fit spaghetti omega humorbash chip u  bd sushi pad soft treatment
psn hand cracker butterscotch practic tissu boil remov frozen pancak threadless
yum stretch dna trainer beat rack tbsp plant pyramid golf shrimp wikihow wall
sketch librari milkshak wrote orgasm vegetarian hz melatonin bon cucumb simpl
toy indian tabl increas zucchini

tammysrecipes com tandao com tartelette blogspot com tarteletteblog com
tastebook com tastyhuman com tedmed com teknogamia com tektuff com
templeofthai com testsounds com texastypeamom com thatskinnychickcanbake com the  thavenue com thebewitchinkitchen com thebittenword com
theblackoven blogspot com
theblackpeppercorn com
theburlapbag com
thecandidappetite com
thecherryblossomgirl com
thecookinghusband com
thecookingphotographer com
thedailyspud com
thedentistauthority com
thefamilyhomestead com thefightins com thefrugalgirl com thegraciouspantry com
thegunnysack com
thegutsygourmet net
thehealthysnacksblog com
thehungrymouse com
theidearoom net
thejewelsofny com
thekindlife com thelifeofrylie com thelittleteochew com themeaningofpie com
therealfoodchannel com thesneeze com thespiffycookie com thestar co uk
thesweetslife com thethreecheeses com thevelvetdoll com theyummylife com
thisiswhyyourefat com thismamacooks com tie timelessinformation com
tiphero com
tonychor com
triathlontrainingschedule org
tulsaworld com
turkishcookbook com ultimatejujitsu com unh edu unitedstatesofmotherhood com unplggd com uptowntwirl com upworthy com utahdealdiva com
valetmag com
vanillagarlic com
various
veganpeace com
vegetarian
vintag es embryo com visualnews com vivawoman net wanderplex com
warpbreach com warriordash com washingtonsgreengrocer com weather com
wechangebeauty com weddingclan com weeklycupcake com weheartfood com
weightlossresources co uk weightlosswand com weightlossweapons com

top ephemeral predictors

top ephemeral predictors

fashion news said he hi iphon ha team smile app price gift easter compani model
which video look pumpkin devic dress say show their man boot halloween year
that london style on costum sport real imag world product mobil call makeup
custom browser de android      system ll googl servic technolog like fall who
leagu internet music via trend      than gif hair holiday phone sell fan regist
email million hilari america clip govern shirt featur come head rose  d flight
robot stori do wear shoot peopl web back latest outfit cloth qualiti woman bird
la see      be print polic digest vaccin    fiction view job nike off doe gadget
worst basketbal innov    offer diamond flu perform insur will melon case report
been shoe secur nfl march jean seen inspir anoth street york wood coke event
clutch footbal twitter shot social self sunglass softwar humor we script airport
color daili facebook game dad geek survey watch usb accessori code must send
him china meme cancel screen passov olymp raw break tablet prank mini stay
sexi act cute map award friday bracelet here pictur front cloud ill shop allow
highlight microsoft polit vehicl leather youtub collect career printer pro credit
pay cat mike jersey ladi chic tech patent celebr wors ridicul vodka marijuana
star spring everyth amus fame father corpor candi manag soon expens in item
crash island where joke hairstyl ndash silk feder hockey believ categori support
u  e  ray ever indi someon machin batteri old church snicker they player boy
cost    chew    rumor lo her court tag announc jami      popul jacket forc
sodium suck ski hat decor select jello open cigarett reveal five pretti bra address
diabet truffl suffer farmer tenni fli run

wired com cool webpronews com dailymail co uk bleacherreport com
popsci com
sports yahoo com
midwestsportsfans com
itechfuture com
collegehumor com sportsillustrated cnn com vii     com cbssports com
forbes com
nydailynews com
technologyreview com
thesun co uk
techcrunch com wimp com globalpost com splicetoday com urbanog com
geek com
pcworld com
gawker com
guyism com
thumbpress com
fashionserved com mymodernmet com news com au pcgerms com style com
mashable com nymag com gadgetrance com content usatoday com mo no com
pleated
technologyinnovationsite com
acecabana com
extremetech com
freep com newser com vice com techflesh com alternet org etsy com
polyvore com
threadsence com
washingtonpost com
laweekly com
telegraph co uk refinery   com humor si com engadget com rawstory com
reuters com bucogo com espn go com formyhour com globalgoodgroup com
kecole com notalwaysright com techworldtop com worldlifeexpectancy com
empowher com fastcompany com newseum org pcmag com realbeauty com
smbc stumbleupon com stylelist com syracuse com theglobeandmail com
westword com
gizmodo com
cnn com
villagevoice com
mirror co uk
collegefashion net cbsnews com wimp com break com upi com  jokes com
blog metmuseum org blogs discovermagazine com dornob com fashion elle com
hautemacabre com icanhascheezburger com iwastesomuchtime com moreoo com
newsfeed time com
politicalhumor about com
seattletimes nwsource com
shopruche com sports break com sports espn go com the video mpora com

table vii 

an ensemble classifier was implemented consisting first of
processing the feature sets with individual classifiers  and then
constructing a new feature set consisting of the output probabilities from each individual classifier that a given example
is in the evergreen class  a regularized logistic regression
classifier was then trained on this derived feature set 
the ensemble classifier achieved a k fold cross validation
roc auc of        with a precision of      and a recall of
       effectively identical to the performance of the classifier
trained on the body features alone  this was a surprising
result in light of the error correlation analysis  one possible
explanation is that the ensemble classifier is overfitting the
training data  the training roc auc is much higher  at
       some evidence for this can be seen from the fact that
when the regularization constant was tuned empirically for the
ensemble classifier  the optimal value called for a significantly
higher penalty coefficient than the classifiers for the individual
feature sets  this suggests that obtaining additional samples to
reduce the overfitting could result in better performance for

e rror c orrelation of f eature s et c lassifiers

  overlap
a 
a 
a 
a 

body
outline
links
url

b  body

b  outline

b  links

b  url

     
    
    
    

    
     
    
    

    
    
     
    

    
    
    
     

the ensemble classifier 
c  error analysis
table iv showed that the leading classifier  logistic regression on the body text features  shows somewhat better
precision than recall  indicating that errors are slightly biased
towards false negatives in the evergreen class 
to better understand the nature of the classification errors 
an investigation of some of the misclassified examples was
conducted  this turned up the interesting result that the labels
in the training set themselves are fairly noisy  for example 

fics    machine learning

the following two urls were two different training examples
in the dataset 
 http   news menshealth com touch at your own peril 
           cm mmc facebook   menshealth content mhnews    dirtiestplaces
 http   news menshealth com touch at your own peril 
           
the urls actually referred to the same page  the first
link simply contained an extra query string parameter  likely
representing the referring page  which did not affect the
retrieved content  however  the first url was labeled as
ephemeral  while the second was labeled as evergreen  there
were numerous other examples of effectively identical or
similar pages that had conflicting labels  since the examples
were hand labeled  this likely represents either human error or
a difference of opinion on the part of the humans performing
the labeling 
a systemic analysis of the urls shows that      samples
have the same url and differ only in the query string  but
are classified differently  spot checking several such urls
suggests that they are most likely misclassifications of the same
sort as the menshealth com example  there are also additional
cases that are somewhat more ambiguous but also likely misclassifications  for example  two receipes for different dishes
hosted on the same domain  since it is difficult to distinguish
these cases from pages hosted on the same domain that are
legitimately different classes  no attempt has been made to
quantify these cases  but it suggest that the      figure is
likely a lower bound 
the misclassified examples affects the accuracy of the
classifier since they add noise to the input  they also affect
the roc auc estimate  since examples which the classifier
correctly labels but which for which the training label is incorrect will be mistakenly considered to be in error  increasing
the number of training examples gathered would help address
both these problems 
iv  c onclusions and f uture w ork
several popular classification algorithms were implemented
and applied to the evergreen classification problem  from the
results  we observe that logistic regression is well suited to
this text classification problem  achieving the best roc auc
score of the classifiers investigated 
feature extraction for the classification problem was examined  and low content features such as javascript and portions
of the html markup were identified and removed in preprocessing  salient features such as the body text  document outline  and relevant links urls were extracted and their predictive
power analyzed by training independent classifiers on them 
the body text and document outlines were more successful
than the link based features  with predictions based on just
the outline able to achieve a roc auc within   percentage
points of the full body text  tf idf proved to be effective in
further extracting the most relevant terms within the text based
features 
while a comparison of the misclassified samples from
classifiers trained on the different feature sets suggested that

 

somem benefit could be achieved by combining the information from the separate predictions  attempts do so via an
ensemble classifier did not significantly improve the result 
training curve analysis showed a gap in convergence between
training and test roc auc  indicating that the ensemble
classifier is overfitting  error analysis also indicated that the
training set itself is subject to mislabeling  introducing a further
source of noise in the dataset 
for both these reasons  a fruitful area of future work is likely
to gather additional examples to expand the training set and
reduce the overfitting  this will assist with overfitting  as well
as to help compensate for mislabeling errors  the ensemble
classifier should be revisited in this context  as the analysis of
error corrleation and learning curves suggested that it could
improve prediction if the overfitting issues were addressed 
a second avenue for investigation is using alternatives to
hand labeling the samples  since the hand labeling is dependent on the opinions of a small number of human labelers 
if stumbleupon is able to add tracking of click throughs to
articles presented to real end users  the tracking data could be
used to establish the longevity of the content in question more
directly  which should help reduce mislabeling 
r eferences
    kaggle  stumbleupon evergreen classification challenge  http   www 
kaggle com c stumbleupon
    t  fawcett  an introduction to roc analysis  pattern recognition
letters  issue           pp        
    wikipedia  receiver operating characteristic  http   en wikipedia org 
wiki receiver operating characteristic
    m f  porter  an algorithm for suffix stripping  program        pp       
    wikipedia  bag of words model  http   en wikipedia org wiki 
bag of words model
    wikipedia  tf idf  http   en wikipedia org wiki tf e       idf

fi