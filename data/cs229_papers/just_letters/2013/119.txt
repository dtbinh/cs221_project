using twitter to gauge news effect on stock market moves
sam paglia

   abstract
the rapid rate of data creation  interconnected graph structure and variety of data captured make twitter an ideal
candidate for characterizing the complicated and rapidly changing investor sentiments that move financial markets  in their
     work on the subject  bollen et al  claimed that sentiment analysis on a broad twitter corpus can lead to       accuracy
in predicting daily changes in the dow jones industrial average 
in this project  i use a dierent method  specifically  i take a news based approach and aim to examine the specific eect
of news releases via twitter on market moves  given that all major news outlets have twitter feeds that they run in parallel
to their other distribution channels  this project aims to explore two key questions  the first is whether an aggregated body
of news tweets can be used as the basis for a viable prediction strategy  the second is an analysis of what key phrases in
tweets tend to be associated with directional market moves
my results indicate that  while news feed analysis lacks significant predictive power as a binary classification tool of market
directional moves  it does show promise as a multinomial classifier of market moves into discretized buckets  in this case 
appropriately tuned multinomial naive bayes and multiclass svm models show promise as twitter news based predictors
of market moves
   dataset schema and feature choice

     dataset composition  the twitter rest api makes
only a users      most recent tweets available  in con     data sources  while the specific tweets i chose will be
structing my dataset  i faced a tradeo  breadth  including
explained more fully below  i used the twitter rest api to
feeds from many sources  versus depth  the maximum amount
construct my dataset of tweets  from each tweet record  i
of samples from each feed   for this analysis  i accumuparsed the tweets text  authorship date  number of retweets
lated a dataset of        tweets  representing the full     and the id of its origin feed  i also added two additional featweet timelines of    news sources  among them the finantures  one was the date of the next scheduled trading day afcial times   f inancialt imes   reuters   reuters   the
ter tweet authorship  for tweets before market close      pm
wall street journal   w sjeconomy   forbes   f orbes  
est  this is just the authorship date  while for tweets after
and bloomberg news   bloombergn ews 
market close  it is the next trading day  the other was the
these timelines are of unequal length  some span little
number of minutes between the tweets authorship date and
more than   months  while some span almost   years 
the end of the next scheduled trading session 
for the market data  i downloaded the past three years of
s p     closing prices  pt   from yahoo finance  and cal   binary classification
culated the daily index returns rt     ppt t     i chose the
the initial problem we consider is simple binary classifis p     as opposed to the dija because it is has more incation
problem  given a tweet  can we predict whether the
dex components and is widely perceived to be a more broadly
market
rose or fell during its associated trading day  we
based measure of market movement  use of alternative fitransform
our existing tweet labels li to li      li      and
nancial time series is explored in the final section  iterating
through my tweet set  i labeled each tweet  denote tweets consider various classifier models 
labels li   with the percentage return rt associated with the
     naive bayes  we begin by fitting a bernoulli naive
next scheduled trading close
i converted the text of each tweet into a count vector of bayes to our data  we use a laplacian smoothen parameword occurrences using a bag of words approach  for my cor  ter      to help guard against the inherent sparsity of our
pus  i use a corpus of all words found across my tweet dataset  dataset  the results are summarized in the table below  note
while i previously intended to use pre existing corpuses com  that  for this and all other models used in this paper  we emposed of common english words  the frequent occurrence of ploy a       train test split of our dataset
abbreviated terms or proper nouns  which often grade out as
naive bayes diagnostics
strong market movers  made it advantageous to structure our
metric
value
corpus around the terms present
p  li      
     
additionally  i also excluded    extremely common english
training error
     
words  such as is   which  due to the short nature of the
test error
     
text being sampled  appeared disproportionally often and had
precision
     
a skewing eect on our sample 
recall
     
date  submitted december   th       
 

fi 

sam paglia

as we can see  the substantial gap between training and
test error indicates that this model is likely high variance 
and would benefit from additional training data  when
we isolate the    words most indicative of a market increase  words wi with largest coefficients i    our list includes
such phrases as china  recession  more  up  stocks 
fed  trading and economy  while there are some entries that are difficult to explain      for example   overall
this model  while not performing particularly well  does show
some promise
     weighted naive bayes  we also consider one nuance
of this model  the addition of observation weights w i    we
would like to weight our examples so as to leverage two of the
primary advantages of twitter  its network structure and the
near constant flow of timestamped data  in a      paper on
the subject of modeling influence decay of news events    kong
et al posit a formula for modeling news decay over time that

rather frustratingly  there is no combination of weights
  that yields superior performance to our basic nb implementation  as such  we conclude that memory retention
based model weighting is not an eective predictive strategy
and move on to other model classes

   perceptron
we now fit a perceptron model to our dataset as well  results displayed below 
perceptron diagnostics
metric
value
p  li      
     
training error
     
test error
     
precision
     
recall
     
unlike the previous case  the similarity between training and
test error indicates that this is likely a high bias model  furthermore  the fact that the test error is substantially larger
than our naive bayes implementation indicates that this is
likely not a model class that is worth pursuing further 

is based on the ebbinghaus forgetting curve    which states
that memory retention for arbitrary phenomena  r  is equal
s
to r   e t   where s is strength of memory and t is time 
one premise of kongs paper is that the relevance of a
news item  or a tweet  should be determined by the degree
to which it has been retained in memory by its readers  in
that sprit  we alter ebbinghaus formula somewhat to yield
r   w i    n i  ln  t i     where n i  is a tweets retweet count
and t i  is the time in minutes until the close of the next trading day  the reason we have altered the formula in this way
is that we would like to capture the fact that many retweets
is likely indicative of a tweets perceived relevance to those
that view it 
given this weighting methodology  we attempt to select
the optimal   for model performance  we use grid search
to optimize over pairs       recording the test error for each
pair  results shown below 

   svm
the final classifier we fit is a linear kernel l   regularized
svm  the sparse featuresets  few irrelevant features and linear separability of most classification problems makes svms
an ideal candidate for text classification    diagnostics shown
below 
svm diagnostics
metric
value
p  li      
     
training error
     
test error
     
precision
     
recall
     
while the test error is higher than a naive bayesian implementation  the greater precision and especially recall indicates
that our svm implementation performed well in identifying
positive market moves with greater accuracy  furthermore 
the large gap between training and test error indicates that
the model could improve considerably with continued expansion of our dataset

fiusing twitter to gauge news effect on stock market moves

     binary classification wrapup 

 

l       l       l       l        where a tweet attains label li if
its associated market move is contained in partition pi   the
reason we have set l  l      as opposed to opting for a simple li   i   labeling is that we would like our error metric
to reflect the fact that directional errors are more severe than
directionally correct magnitude errors  increasing l  l  will
allow mae to appropriately penalize models that commit directional errors more frequently  now that we have defined
our multinomial estimation framework  we move on to our
actual implementations  we implement two model classes 
naive bayes and linear kernel svm  but use an unweighted
and weighted variant of each
     unweighted multinomial naive bayes  once again
using      are our laplacian smoothing parameter  we fit an
unweighted multinomial naive bayesian model to our data 
diagnostics shown below 

as the summary chart above indicates  while naive bayes
and svm implementations do show some promise  there does
not appear to be a significant enough uptick in performance
above a naive always predict   strategy to consider textbased classification a viable alternative to sentiment based
methods  especially if bollen et als results are indeed correct 
admittedly  more better data and more expansive timelines
could partially remedy this issue  but for now we move on to
our next topic  multinomial classification

this model appears to perform well  with accuracy     
better than any individual class prior probability  all told 
our unweighted model makes a directional error  predicting that yi     when the opposite is true or vice versa  only
      of the time  more accurate than a naive strategy  however  there are ways it can be improved as we show in the
following section

     weighted multinomial naive bayes  for a weighted
model  we do not use a memory retention weighting model as
one area in which news based approaches show much more we did in our classification task  as it similarly unhelpful  
promise is the area of multinomial classification  we begin by instead  we use a value based weighting scheme as follows 
outlining the discretization technique which we use to cluster if li         wi   w   where w is a fixed parameter  otherwise li      essentially  w controls the degree to which
our observations
we consider extreme observations more or less heavily than
     observation discretization  error metric and la  non extreme observations  we perform a linear search over
beling methodology  looking at the descriptive statistics w to find the optimal value over our test set  the results of
of the three year daily return history  jan           de  which can be seen below
cember           daily return history  rt   of the s p     
we compute the following interquartile boundaries q      
       q              q              we would like any
partitioning to partition our dataset somewhat equally  as well
as use a symmetric partitioning bound to avoid overfitting 
we use these interquartile boundaries as the basis for the following   class partition  p      rt         p            
rt       p           rt         p             rt   
of more interest is how we label tweets depending on inclusion in these respective classes  in a      paper  fok et
al p
  posit that the mean absolute error  defined as m ae  
n
 
yi    is a suitable error metric for stock price prei    yi
n
diction  furthermore  since we are dealing with discrete and
not continuous output  mae is a much better choice than
other common error metrics such as rmse  hence  mae will
be our error metric of choice for the remainder of the paper
having said this  we choose the following labeling schema
   multinomial classification

fi 

sam paglia

as we can see  a value of w        meaning that extreme
observations have     the weight of non extreme observations  minimizes our mae at        setting w        we
fit a weighted multinomial naive bayes model  diagnostics
below 

as we can see the addition of weights does substantively improve our models performance  but still leaves it below our
naive bayes benchmark
     multinomial classification wrap 
as we can see  despite a higher test error  the mae of this
model is significantly lower than in the unweighted case  furthermore  the probability of directional error is now only     
less than in the unweighted case  digging in to the specific key
words isolated by our model  we see that our model isolates
words such as earnings  debt  down and recession as
indicators of a strong negative market move  li       and
words such as growth  up and sales as indicators of
strong positive move  li       finally  a few phrases  such as
fed  trading and economy are key predictors of both
types of extreme market moves
in summary  weighted multinomial naive bayes can be a
viable predictor for discretized market moves  while the partial overlap in predictive keywords between classes  as well
as the fact that our current model has a notable bias toward
labels l    l   predicting market declines  indicate that there is
room for improvement  the promise is notable  we now move
on to multi class svms

as we can see from the chart above  given our current dataset 
naive bayes models outperform linear svms for our prescribed multinomial classification task  both with and without weights  however  the more uniform nature of the svmderived fit matrices and the large gap between svm training and test error indicates that our svm models may still
be plagued by variance issues  and that with an even larger
dataset  svm models could outperform all other implementations in predicting discretized market moves 
regardless  it is clear that tuned multinomial models are
much more eective at predicting directional market moves
than simple classification strategies  furthermore  the introwhile the overall performance is worse than that of our naive duction of value based weights can further improve model acbayes  it appears that our confusion matrix is more uniform curacy  reducing both mae and the probability of directional
that in our naive bayes implementation  indicating that this error 
model likely has fewer inherent prediction biases  furthermore  this model has very low training error        indicating
   prediction of other financial time series
that its performance may well improve with additional data
now that we have completed our full analysis  turn our attention to one small aside  while we have seen that a newsbased twitter predictive approach can be eective in pre     weighted svm  if we apply a similar value based dicting movement of the equity market  we were curious how
weighting method to our svm implementation  and opti  this performance would generalize to other market time semize our weighting parameters w   we find that our optimal ries  the reason for this curiosity is thus  the obvious shortw        using this value of w   i fit a weighted svm  diag  fall of a news based approach to market prediction is that
nostics below 
it ignores much of the sentimental signs aecting the human
     unweighted svm  we now fit an one vs all multiclass
linear kernel svm to our data  results shown below

fiusing twitter to gauge news effect on stock market moves

beings driving the markets  thus  logically speaking  newsbased strategies will perform more poorly in markets that are
dominated by sentiment versus rationality versus those that
are not  to this end  we analyze three new data series  the
yield on the    year treasury note  the price of gold  as tracked
by the etf gld  and the overall value of the us dollar  as
tracked by the etf uup   it would stand to reason that
that  given that treasuries and gold are both safe haven assets that are typically purchased when investors are worried
about the state of the economy  that these two assets would
be excessively sentimental and difficult to predict  while the
us dollar  which is typically the domain of more seasoned
investors  would be more rational and easier to forecast
our methodology in each new market is the same we used
in the equity market  we give tweets a label li               
based on whether the tweets associate market movement falls
into one of four partitions defined by boundaries   t     t   
where t roughly corresponds to the   th percentile of the

 

data series daily return set  we then cross validate an optimal weighting parameter w   and fit a weighted naive bayes
to the data  the results for each market series  along with
the optimal w and chosen t  are shown below
other indicator performance
index
test err mae w
t
s p    
     
             
   yr t note
     
            
gold
     
              
us dollar
     
              
looking at the above table  it appears that our results are
partially borne out in that gold does appear to be difficult
to predict  while equities  treasuries and the dollar all lie in
similar range  furthermore  the contrast in optimal w between the s p     and other assets is interesting to note 
in summary  while it is not a hard and fast relationship  it
does appear that news based approaches are more eective in
markets that are less influenced by sentiment

   summary
my results show that  while not particularly compelling in binary classification problem  news based approaches can indeed
be useful in multinomial classification of market moves  both in the equity market and otherwise  while  for our sample size 
appropriately tuned naive bayes models show the most predictive power  with a test         and m ae          there
is some evidence to suggest that  with a larger dataset and perhaps more fine tuning of observation weights  that a linear
kernel one vs many multi class svm could surpass naive bayes models as a accurate and informative predictor of discretized
market moves
   works cited
in addition to the papers and resources cited below  i would like to thank andrew ng and the entire cs     teaching sta for their invaluable help and instruction  furthermore  special mention is needed for the python sklearn module  which i used for my o the shelf model fitting  documentation for the module can be found here  http   scikitlearn org stable documentation html 
   bollen  j   mao  h   and zeng  x  j        twitter mood predicts the stock market journal of computational science         
   http   finance yahoo com q hp s   egspc historical prices
   kong et al  ranking news events by influence decay and information fusion for media and users        cikm     pg
         
   h  ebbinghaus  memory  a contribution to experimental psychology  teachers college  columbiauniversity       
   manevitz  l  m  and yousef  m        one class svms for document classication  j  mach  learn  res            
   dr  wilton w t  fok  iaeng  vincent w l  tam  hon ng  computational neural network for global stock indexes prediction   proceedings of the world congress on engineering        vol ii wce       july              london  u k 
other
   c  ornes and j  sklanskkita et al  application of bayesian network to stock price prediction        doi          air v n p   
   ke tao  fabian abel  claudia hau  and geert jan houben  what makes a tweet relevant for a topic  making sense of
microposts    msm       page        

fi