musical structure in irish traditional tunes
matthew staib  lennart jansson  and edward dai

 

introduction

most irish traditional folk dance music contains two musical themes  the tune begins with one
musical theme which is usually repeated  then progresses to another theme with similar musical
structure and motives  which is usually also repeated  the first section is typically called the a
section while the second is called the b section  our goal is to understand  at a quantitative level 
the musical relationship between a sections and b sections  see fig    for an example 
in working towards this goal  we need to understand both the relationship between the a and b
sections of a given folk song as well as the differences between a and b sections of the tunes
in general  in the long term we wish to  given an a section  automatically generate a musically
appropriate b section 

 

dataset

we are using the tunes dataset from the session      the session is an online community of people
who are interested in playing irish folk music and cataloguing traditional irish tunes for others to
learn  these tunes include various dance forms  such as jigs  reels  waltzes  and slides  available on
their website is a set of roughly        dance tune settings in abc notation  a human readable symbolic music data format in plain text  the abc files are easily parsed and manipulated symbolically
for feature extraction 

 

feature extraction

first we select from our dataset tunes which have the standard form of an a and a b section  we
consider only tunes with a number of bars in               an individual a or b section usually has
  or    measures  or    or    after manually reduplicating out music that was contained in repeat
signs  we want to restrict to tunes that split evenly into two sections  e g  not three sections  which
could mean abc  aba  etc   
after splitting tunes into a and b sections  we turn our attention to feature extraction  our dataset
is loosely in the form of sequences of pitches representing melodies  however  as tunes may have
different lengths  we need to generate a feature vector of fixed length 
we define an n gram as an ordered list of  in our case  n pitches of notes in a melody  ignoring
accidentals  in total  we created three different feature vector designs 
   record counts of all    and   grams 
   split melodies by measure into eighths and store counts of all    and   grams separately
for each measure 
   use the features from      also adding counts of notes of particular rhythmic length 
since these feature vectors have nearly       components  before further processing  we used pca
to reduce the number of components  this helped make metric learning computationally more
feasible 
 

fi
 








 
  



a section









b section



figure    cooleys  reel   an example of a traditional irish tune from the session      this reel
has the standard form of an   bar a section which is repeated  then an   bar b section which is also
repeated  the sections are labeled in the notation above  notice how the a and b sections contain
distinct melodic lines  but still have musical similarity  for example  they end with the same      bar
ascending then descending motif 

    
   

percentage correct

   
   
   
full feature vector  test 
full feature vector  train 
no rhythm data  test 
no rhythm data  train 
no bar by bar differentiation  test 
december          no bar by bar differentiation  train 

   
     

  

  

  
  
number of features in pca

  

  

figure    accuracy of running our svm classifier to classify musical phrases as either a or b
sections  the performance on the testing dataset with various feature vector designs is shown with
solid lines  and the performance on the original training set is shown in dotted lines 

 

svm classifier

using our labeled pool of a and b sections  we used an svm with gaussian kernel to attempt to
classify a given segment of a tune as an a section or a b section  we used   fold cross validation 
and experimented with our three different feature vectors and various numbers of pca components 
to train the svm  we used the relevant scikit learn libraries     
our svm classification results are presented in fig     for all three feature vector designs  performance on the test set generally improved when we kept a greater number of features when running
pca on the data  accuracy on the training sets was much better than the testing setsprobably  this
is due to overfitting the svm to the particular training sets in each case 
the feature vector design that gave us the least difference between training and testing performance
 so probably represents the feature vector design with the least overfitting  is the no rhythm data
 

fifeature vector  that contains pitch n gram counts specific to each bar of the tune but not overall
counts of notes with particular rhythmic values  interestingly  the full feature vector with rhythm
information caused the svm classifier to perform worse  we believe this is because the corresponding a and b sections of most tunes may have different melodic figures  but if they are to have
reasonable stylistic similarity they are likely to have similar rhythm value counts  for example  a reel
whose a section is mostly eighth notes is likely to have a b section which contains mostly eighth
notes   meanwhile  having bar by bar differentiation of features understandably greatly improved
performancewe believe this is a consequence of how in many tunes  the last bars of the a and b
sections contain nearly the same notes  thus our svm might have learned to give greater weighting
to the musical features of earlier bars  which are likely to be distinctive to a and b sections 

 

metric learning problem formulation

we also want to learn what makes the a and b sections of a particular tune musically related  we
attempt to learn a metric d x  y  where  if x  y  rn are the feature vectors corresponding to the
a and b sections of a tune  d x  y  should be small  for this task  we use the mahalanobis metric 
given by
q
 x  y t m  x  y 

d x  y    kx  ykm  

for m  rnn positive semidefinite  and attempt to learn a suitable value of m  
usually  in metric learning  one learns a metric by supplying both pairs of points  x i    x j     s
that are similar and should have small distance  and points  y  i    y  j     d that are dissimilar and
should have large distance  we then solve the following optimization problem for m  from      

 
x
  i 

minimize 
x  x j  
m

 x i   x j   s

subject to 


 
  i 

y  y  j      

x

m

 y  i   y  j   d

m    
however  in our problem  it is unclear how to select pairs of a and b sections that are dissimilar 
intuitively  one might argue that the a and b sections of different tunes should be dissimilar  however  two arbitrary tunes may be variations of each other  and hence have a and b sections that are
related 
note that by simply removing the dissimilarity constraint results in an optimization problem whose
optimal value m is the zero matrix  instead  we enforce det m     as a reasonable seeming
regularity constraint  we do this by noting that det m is concave and so  det m    n    
det m    is a convex constraint      then  the optimization problem 

minimize 

m 
 
x
  k 

x  y  k  

m

k  

subject to  m    
 det m    n    
where m is the number of training examples  produces m with det m      as the objective function
scales with the determinant of m  

 

metric learning implementation and results

after using pca to reduce our feature vectors to n      components  we applied this method using
  fold cross validation to learn such a metric d  we used cvx to solve the optimization problem
that determines m      to evaluate the success of our metric  we consider the following problem 
 

fi    

matching a sections to given b sections

matching b sections to given a sections

percentage with rank

   
   
   
   
   

full feature vector
no rhythm data
no bar by bar differentiation

 
  
  
  
rank of correct matching section in nearest neighbor list

 

 
  
  
  
rank of correct matching section in nearest neighbor list

figure    results of metric learning based matching algorithm  showing the percentage of tune sections whose matching other section appeared at a given rank in a ranked list of nearest neighbors
by the learned metric  for example  the first column in the left chart represents the percentage of
b sections whose matching a section was the very closest by the metric out of all possible other a
sections  here the full feature vector with rhythm information gave by far the best performance 

split apart the a and b sections of our test data        tunes   for each a section  order
all the b sections by increasing distance from this a section  under the metric d   record
the rank in this list of the actual corresponding b section  ideally  the actual b section
would be the first element of this list  closest to this a section  alternatively  we could
match an a section to a given b section 
see fig    for a cumulative histogram of our results  if we were to randomly guess the closest b
section  we would expect the rank of the correct b section to be   with probability       in our
results  the correct b section was the first ranking option     of the time  moreover  the correct b
section was in the top     of the ranking     of the time  we conclude that there is a significant
amount of information shared between a sections and b sections  and the mahalanobis metric we
learned is able to identify some of the musical similarities 

 

composition

now that we have shown the viability of our metric  we can use it to try to  given an a section 
compose a musically related b section  we employed a simple algorithm 
   select a specific tune  and split it into its a and b sections 
   randomize a b section  bcur   using the rhythm of the a section 
   repeat n times 
for every i              k  
generate bi by randomly making pitch edits to bcur  
set bcur    arg min d bi   a  
bi

as an example  we tried to compose a new b section for the a section of cooleys reel  the original
correct version of which can be seen in fig     our new created b section can be seen in fig    
while the resulting b section is musically far from ideal  it is notably better than the random starting
b section  for example  the starting note is the same as that of the tune and the intervals between
consecutive notes are  on average  smaller 

 

further work

at a broad level  all the work we have done could be improved by better feature selection  given the
complexities of music     and   grams are far from ideal features  at the very least  we could store
 

firandomized notes used as a starting point for the optimization 


 

 







the new b section  converged upon in optimization 


 

 







figure    a new b section composed for cooleys  see fig      using its distance by the learned metric
to the original a section 

counts of    or     or more  grams  or subsets of these  thus encapsulating more of what a melody
looks like on a larger scale  there are other obvious musical features that we have not looked into
using  for example  time and key signatures  even more musically complex features may prove
fruitful  for example  based on the notes in the melody at each point in time we could infer the chord
progression  there is a lot left to explore in this regard 
as for improvements specific to our automated composition  one idea is to use our dataset to construct a model for the likelihood that a given sequence of pitches is musically significant  at the
moment we are easily able to converge to a b section that  with respect to our learned metric  is
actually closer to the given a section than the actual b section of the tune is  we produce a sequence
of pitches that is similar to the a section but that  when viewed independently of this a section  still
seems musically unlikely  it is does not seem unreasonable  for example  that we could learn from
our set of tunes that smaller intervals between notes are much more likely than large ones  one
other complicating issue is that when performing pca to reduce our feature vectors to only have
at most    features  as is necessary for the metric learning implementation   we may lose essential
musical information that is necessary for adecember
tune to   
seem
    plausible to a human  as features that are
unrelated musically may be represented by a single principal component in the transformed dataset 
we might find improvements to the composition process by using a metric or likelihood estimation
that is able to consider a larger number of features  or perhaps the untransformed original feature
vectors themselves 

references
    s  boyd and l  vandenberghe  convex optimization  cambridge university press       
    m  grant and s  boyd  cvx  matlab software for disciplined convex programming  version    
beta  http   cvxr com cvx  september      
    f  pedregosa et al  scikit learn  machine learning in python  journal of machine learning
research                
    the session  thesession org  
    e  xing  a  ng  m  jordan  and s  russell  distance metric learning  with application to
clustering with side information  nips       

 

fi