cs     machine learning
project report
an ensemble classifier for rectifying classification error
cheuk ting li

ctli stanford edu

december         

 

introduction

in the field of classification  apart from building a single powerful classifier  efforts have been made on ensemble
methods which combine several classifiers to give results better than any one of them  some notable examples are
boosting  freund and schapire       and stacking  wolpert        in this project  we propose a new ensemble
method in which each constituent classifier would focus on correcting errors made by the previous ones 
to explain this method  first focus on the case where there are two constituent classifiers  the first classifier 
called the assorter  would perform classification on the training data to produce a ranking of the classes for each
training instance  rank   is the class which is the most likely  rank   is the second likely  etc   for each instance 
we find the rank of the correct class  rank is   if the prediction of the assorter is correct  rank is   if the prediction
is wrong  but the assorter decides that the correct class is the second most likely  etc   then the second classifier 
called the rectifier  would use the rank as the class variable  and discard the original class variable  to perform
classification  intuitively  the rectifier would decide whether we should accept the prediction made by the assorter 
or pick from the classes which are determined as less likely by the assorter  to classify a test instance  run the
assorter to obtain a ranking  and pick the position given by the rectifier  this method can be generalized to
more than two constituent classifiers  where each subsequent classifier would try to rectify the results made by the
previous classifiers 
experiments were conducted to investigate the classification accuracy of the assorter rectifier method  the
results suggest that the assorter rectifier method  with suitable choices of assorter and rectifier  outperforms its
constituent classifiers  and many other state of the art classifiers 

 

assorter rectifier method

in this section  we would describe the assorter and rectifier  and how the outputs of the two classifiers are combined 

   

assorter

the assorter serves as the base classifier  which gives the initial prediction of the class  the assorter has to be
able to produce a distribution  or at least a ranking  of the classes for a test instance  simple parametric classifiers
are more suitable choices for the assorter 

   

rectifier

the rectifier would try to correct the prediction made by the assorter  the rectifier does not need to produce a
distribution of the classes  it only needs to pick one class as the prediction  local non parametric classifiers are
more suitable choices for the rectifier  since the rectifier has to capture small clusters near the decision boundary
of the assorter which are incorrectly classified by the assorter 

 

fics     machine learning

project report

cheuk ting li

ctli stanford edu

figure    illustration of assorter and rectifier on a hypothetical data set

   

combining the results

k
 
suppose there are k different classes
 labeled
         k  let ha  x   r denotes the distribution of classes predicted
by the assorter for instance x  i e    ha  x    p  y   i   x    let hr  x            k  denotes the rank of the correct
i

class guessed by the rectifier  define the function r i   h  to be the index of the element in h         hk ranked the i th
 r     h    y if hy is the largest among h         hk   etc   define the function r   y   h  to be the rank of hy among
h         hk  r   y   h      if hy is the largest  etc  

 
to train the classifier  we first train the assorter with the original training data x i    y  i  i       m   then we
would replace the n
class variables y  i  by theo
rank of the correct class r   y  i     ha  x i      and train the rectifier on
the modified data
x i    r   y  i     ha  x i    
 
i       m

to obtain the prediction for a test instance x  we first use the assorter to obtain a distribution  ha  x   and then
output the class corresponding to the rank guessed by the rectifier  i e   the predicted class is r hr  x    ha  x    figure
  illustrates the idea of assorter and rectifier 
note that if the rectifier just outputs the class which has the highest frequency in the training data for any test
instance  then the assorter rectifier method would reduce to using the assorter alone  assume the assorter has     
accuracy   intuitively  if the rectifier would not perform worse than just choosing the most frequent class regardless
of the test instance  then the assorter rectifier method would not be worse than using the assorter alone 

   

appending assorter output to feature vector

in the current method  to train the classifier  the output of the assorter is only used in finding the rank of the
correct class  the rectifier does not know the prediction made by the assorter  we can supply the prediction by
the assorter to the rectifier as an additional feature  i e   append it to the vector x i     so that the rectifier can use
the prediction made by the assorter 

 

experiments

we have performed experiments on different combinations of assorter and rectifier  the choices of assorter are 
naive bayes

naive bayes is a simple parametric classifier with low variance  a suitable choice for assorter 

tree augmented naive bayes  tan   friedman and goldszmidt       tree augmented naive bayes
is a bayesian network searching algorithm  which finds the best tree shaped bayesian network on the attributes
 conditioned on the class   compared to naive bayes  it has higher variance but lower bias  it usually outperforms
naive bayes for moderately sized datasets 
averaged one dependence estimator  aode   webb et al        the averaged one dependence estimator is a method which aggregates several bayesian networks  where each network is a tree where one attribute

 

fics     machine learning

project report

cheuk ting li

ctli stanford edu

is taken as the root  and all other attributes are children of the root  conditioned on the class   compared to naive
bayes  it has higher variance but lower bias 
logistic regression logistic regression is a simple discriminative parametric classifier  all the classifiers above
are generative  with low variance  a suitable choice for assorter 

the choices of rectifier are 
k nearest neighbors a simple non parametric classifier which takes the average of k nearest training instances
of the test instance in order to produce the prediction  we would use k     and   
decision tree

we used the c    decision tree learning algorithm  quinlan       

support vector machine support vector machine  when used with a high dimensional kernel  can be used
separate small clusters  and thus is a suitable choice for rectifier 

we have implemented the method in java  the method was tested using weka  witten et al         we used
the    datasets recommended by weka  except the datasets letter  mushroom and waveform due to size
constraint   these datasets were taken from the uci repository  frank and asuncion        and were downloaded
at the website of weka  ten fold cross validation was performed 
the average classification accuracy for each combination of assorter and rectifier  together with the average
accuracy when using each assorter rectifier alone  is given in table    we found out that appending the assorter
output to the feature vector always improves the accuracy  except for aode   svm  where the accuracies are
nearly the same   the combination of aode as assorter and decision tree as rectifier gives the best average
accuracy 
we then tested the assorter rectifier method with aode and decision tree against nb  tan  aode  logistic
regression  nearest neighbor    nearest neighbors  decision tree and svm  the classification accuracy for each data
set is given in table    paired t tests were performed for each dataset  we can see that the average accuracy of
assorter rectifier is the highest  and is significantly more accurate for many datasets 
table    average accuracy for each combination of assorter and rectifier  the numbers next to the assorters rectifiers
are the accuracies when using those assorters rectifiers alone  the first number in each cell is the accuracy without
appending the assorter output to the feature vector  and the second number is that with appending 
rectifiers
accuracy  without append  with append 

  nn

decision tree

svm

     

     

     

     

nb

     

            

            

            

            

tan

     

            

            

            

            

aode

     

            

            

            

            

logistic regression

     

            

            

            

            

assorters

 

nn

conclusion

in this project  we have proposed a new ensemble classifier  called the assorter rectifier method  in which the first
classifier  the assorter  would give an initial prediction of the class  and the second classifier  the rectifier  would
 

fics     machine learning

project report

cheuk ting li

ctli stanford edu

focus on correcting the mis classified instances of the assorter  experiment results suggest that the assorter rectifier
method outperforms its constituent classifiers  and many other state of the art classifiers 
several variants of the method may be investigated in the future  for example  we may add more rectifiers to
the method  each subsequent rectifier would try to correct the mis classified instances of the previous assorter and
rectifiers  the rectifiers should be ordered from higher bias to lower bias 
another possible improvement is to break down the classes supplied to the rectifier  instead of only using the
rank of the correct class as the class variable for the rectifier  we can also retain the original class variable and add
it to the new class variable  for example  if there are two classes a and b  then there will be three classes for the
rectifier      a   b   where   stands for the case where the prediction of the assorter is correct   a stands for the
case where the prediction of the assorter is incorrect and a is the correct class  and  b stands for the case where the
prediction of the assorter is incorrect and b is the correct class  although this will produce extra classes  hopefully
the instances within the same classes will be more similar to each other 

references
frank  a  and asuncion  a         uci machine learning repository 
url  http   archive ics uci edu ml
freund  y  and schapire  r  e         a decision theoretic generalization of on line learning and an application to
boosting  journal of computer and system sciences                 
friedman  n  and goldszmidt  m         building classifiers using bayesian networks  proceedings of the thirteen
national conference on artificial intelligence  pp           
quinlan  j  r         c     programs for machine learning  morgan kaufmann publishers inc   san francisco 
ca  usa 
webb  g  i   boughton  j  r  and wang  z         not so naive bayes  aggregating one dependence estimators 
machine learning            
witten  i  h   frank  e  and hall  m  a         data mining  practical machine learning tools and techniques   
edn  kaufmann  burlington 
wolpert  d  h         stacked generalization  neural networks                

 

fics     machine learning

project report

cheuk ting li

ctli stanford edu

table    accuracy for each dataset
dataset

ar

nb

tan

aode

logistic

nn

  nn

c   

svm

anneal orig

     

     

     

     

     

     

     

     

     

anneal

     

      

     

     

     

     

     

     

      

audiology

     

     

     

     

     

     

      

     

      

autos

     

      

     

     

     

      

      

     

      

balance scale

     

     

      

     

      

      

      

      

     

breast cancer

     

     

     

     

     

      

     

      

     

wisconsin breast cancer

     

     

      

     

      

     

      

     

     

horse colic orig

     

     

     

     

      

     

     

     

     

horse colic

     

     

     

     

      

     

     

     

     

credit rating

     

     

     

     

     

     

     

     

     

german credit

     

     

     

     

      

      

      

      

     

pima diabetes

     

     

     

     

     

      

      

     

     

glass

     

     

     

     

     

     

     

     

     

cleveland

     

     

     

     

     

     

     

     

     

hungarian

     

     

     

     

      

      

     

     

     

heart statlog

     

     

     

     

     

     

     

     

     

hepatitis

     

     

     

     

     

     

     

     

     

hypothyroid

     

      

     

     

     

      

     

     

     

ionosphere

     

     

     

     

      

     

     

      

     

iris

     

     

     

     

     

     

     

     

     

kr vs kp

     

      

      

      

     

      

      

      

      

labor

     

     

     

     

     

     

     

     

     

lymphography

     

     

     

     

     

     

     

     

     

primary tumor

     

     

     

     

     

      

     

      

     

segment

     

      

      

     

     

     

      

     

      

sick

     

      

     

      

     

     

     

     

     

sonar

     

     

     

     

     

     

     

      

     

soybean

     

     

     

     

     

     

     

     

     

splice

     

      

     

     

      

      

      

      

      

vehicle

     

      

     

     

     

      

     

     

     

vote

     

      

     

     

     

     

     

     

     

vowel

     

      

      

     

      

      

      

      

     

zoo

     

     

     

     

     

     

     

     

     

average

     

     

     

     

     

     

     

     

     

   statistically significant improvement or degradation

 

fi