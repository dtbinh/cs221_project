identifying tags from millions of text question
chintan parikh  chintanp stanford edu
abstractidentifying tags or keywords from text has been a
very important class of application of text data mining  in the
case of questions and answer sites such as stack overflow or
quora tagging allows users to explore more related content 
build and showcase expertise in a given area and in general get
more visibility to the question at hand  in this paper i take on
the problem of identifying tags for the questions asked at
stack exchange sites based on title and text of the question 
for this problem vowpal wabbit is used as a tool to build set
of discriminative classifiers for each of the tags in the training
set  the resulting tags for each of test questions are predicted
using running through each of the classifiers 

the problem statement is to predict the tags for the test set
of over   million questions using only the model learned from
the training set 
top ten tags sorted by their frequency are shown in table   
the top    tags account for        of all the tags  similarly
when analyzing top     tags they account for around     total
tags  this follows perfect power law distribution as shown in
figure   

index termsmachine learning  clustering  keyword
extraction  text analysis
i  background and motivation
tagging has become popular way to categorize text and
non text information  with the advent of twitter hashtags 
people have started promoting usage of tags in order to
categorize and find related content easily  stack overflow is a
popular site for discussing programming related questions  and
now they have dataset of over   million questions  on stack
overflow a user can tag each questions up to five tags to
categorize a question  using existing tags or create a new tag in
certain cases  although they restrict ability to create a new tags
by having requirement on certain reputation 
in this problem  i look at dataset obtained from kaggle
competition     which contains questions and related tags from
stack exchange sites in the training set  the training dataset
has over   million questions with associated        unique
tags  each question has average of     tags associated with it 
figure   shows one example of training set with its associated
tags 

figure    tag distribution

tags

times occurrence

c 

       

percentage
occurrence
    

java

       

    

php
javascript
android

       
       
       

    
   
    

jquery

       

    

c  

       

    

python
iphone

       
       

    
    

asp net

       

    

table    top ten tags
figure   question with tags  c   asp net mvc  linq  lambda 

fiii  approach
this section details the approach tried out for predicting the
tags for each of the question  in this discussion i take top   
tags into account as they account for       of all the tags and
also it speeds up the process of testing multiple hypothesis 
a  setting up the baseline 
kaggle has supplied with multiple baselines for comparing
the results  one with mean f  score of      where it predicts
top   tags  c   java  python  php and javascript  for all the tags 
for setting up another nave baseline  first the data set was
cleaned removing xml tags  punctuation text and common
stop words in english language using natural language
toolkit      this was tokenized and feed through a simple
classifier  which searches the post for the tag keywords and if
they are present then predicts that tag  this is then aggregated
and then finally selects the top three tags for each of the
question  this approach results in mean f  score of      
b  building discriminative classifiers
two approaches were identified for tag prediction for the
questions  one was to build a global multiclass classifier and
then use method such as one vs all to select the final class 
the second approach was to build a discriminant classifier for
each of the tags and then predict the final tags choosing the
most likely tags  since we need to predict more than one tags
for each of the question  it was decided to use the second
approach  the block diagram below explains the approach 

the features names to        space  there in reducing the
dimension and allows faster lookups 
feature selection 
the feature selection was done using vw wrapper called
as vw varinfo  which exposes all variables of models in a
human readable form  the output includes the input variable
names  including name spaces where applicable  the vw hash
value  the range  min  max  of the variable values in the
training set  the final model  regressor  weight  and the
relative distance of each variable from the best constant
prediction 
using this  we learn the relative importance of the words
and we can remove the features with    relative importance 
so as to reduce model size  table below shows the feature
vectors for the model for c  
feature
c 
initquestion
winform
copypathlist
xmldocument
addin
button neutral
csharp
enumerable
containskey

rel score
       
       
      
      
      
      
      
      
      
      

feature
qt
autoslideinterval
andriod
printf
xcode
rails
wikiversity
gcc
boost
java

rel score
       
       
       
       
       
       
       
       
       
       

table    relative importance of features for c  classifier
choosing loss function 
for the classification task  vw has support for two loss
functions namely hinge  svm  and logistic  the
parameters the default values of regularization and learning
rate of     gives the best result for svm classifier  figure  
and    compares the performance on svm and logistic
classifiers  on the metric of precision and recall for the
classifier built for top     tags 

figure x  training of classifier for each tag
to build a discriminative classifier vowpal wabbit  vw     
was selected  it provides several loss functions as well as
learning algorithms  vw provides a sparse matrix input format
which easily allows a bag of words models  also vw hashes

choosing input samples for building models
the svm classifiers are sensitive to the ratio of positive  m 
training documents and negative  k  training documents  a
previous study     suggests that a discriminative model
produces the result for a class that has    positive and     
negative examples  in the initial phase of training  this fixed
value was being used to create training set for each of the
classifiers  but the total sample size of      was not sufficient
to capture and it has high variance 
the approach which was implemented in the final classifier 
which gave the maximum precision and recall is discussed
below 

page  

fifigure    logistic loss function  mean p         r       

figure    hinge loss function  mean p         r       
                                                                  for  each  of  the  tags   
   get  the  occurrence  value   o      
   choose  m  to  be  o     k  to  be     m  
   each  m  and  k  are  chosen  from  the  question  which  has  
more   than         characters   so   as   to   have   enough  
information  in  the  model     
                                                                  c  tag suggestion
once we have the models built for each of the top   
tags  next step is to predict multiple tags for each of the
question  for this we use the algorithm as below 
                                                                  step      for  each  question  in  the  test  set   
   run  through  all  the  classifiers  in  top     set  
   add  the  svm  output  of  each  classifier  to  a  list      
step      now  sort  the  list  to  get  the  maximum     output  
for   particular   question    and   assign   these   tags   to   the  
question     
                                                                    

the output of vw classifier is between    and    we choose
the top   values which are above a fixed threshold  set to     
in the list as our final tag output 
iii  results
from the above section it is clear that hinge loss function
performs much better than the logistic loss function  hence it
was used in the final classifier 
a  evaluation metric 
mean f  score is used as evaluation metric  which
measures accuracy using statistics precision p and recall r 
precision is the ratio of true positive  tp  to all predicted
positive  tp  fp   recall is ratio of true positive  tp  to all
actual positives  tp   fn  
f     pr   p   r
so in order to maximize the f  score  the algorithm
should maximize both recall and precision simultaneously 

page  

fib  some results from the tag suggestion
original tags
php  image proecessing  fileupload  upload  mime types
firefox
r  matlab  machine learning
c   url  encoding
php  api  file get contents
core plot
c   asp net  windows phone 
 net 
javascript 
codegeneratio
visual studio  makefile  gnu
html  semantic  line breaks

suggested tags
image  file  php
firefox  windows
ubuntu  apache  networking
c   string  json
php  api  file
ios  iphone
windows  asp net  c 
javascript  c   linq
visual studio  file
html

table    original and suggested tags for questions from
test set
note in the above results  the tags are predicted from the
top    tags classifier set 
we can set that the top tags such as c   php are being
predicted with a very high accuracy  where as lower
occurring tags which are not part of the top    tags are
missed or being predicted with some synonym from the
top    set  the example for that being makefile    file 
windows phone      windows 
c  classifiers performance
from the figure   we see that we can build a fairly
accurate classifier with a mean precision of      and recall
of       this value is obtained from test set of        
samples which were not part of the training set 
the tags for which the precision was lowest in the
top    set were  file  windows  forms  list  api  oop  class 
the precision and recall for them is shown in the table
below 
tag
file
windows
forms
list
oop
api

precision
      
      
      
      
      
      

recall
      
      
      
      
      
      

table    tags with lowest precision values in top   
the classifiers in the above list have a noticeably low
precision and higher recall values  this could mean that the
algorithm is bit too liberal in making the classification
leading to lower precision values  this could be true for the
tags  which occur generically in the context of multiple tags 

for example  api  list  file  this could have multiple
connotations and dont particularly belong to a particular
language or a tag set 
d  suggested tags performance
to measure the performance of the entire algorithm to
predict the suggested tags  we run through the test set of
        samples through top    classifier set  following is
the result from the run
tp        
fp        
fn         

p         
r         
f          

table    final result of f score
the low recall is some what expected  as we are not
classifying from the entire set of   k tags 
e  kaggle submission 
the algorithm described in the above section was
submitted to the kaggle  where the test set contains over  
million test question  the competition is particularly intense 
as facebook is conducting it for recruiting  the competition
ends on            and as of       the algorithm described
above had the standing of   th out of     total teams 
the mean f  score of the submission using the methods
describe above was         compared to the top of         
note the high values of f  score compared to above result
are largely due to overlap of test set in the training data set 
thus for the questions in test set if they belonged in
training set  then the same tags were predicted for them 
iv  conclusion
vowpal wabbit was used extensively in the development
of classifiers and its sparse input format  hashing trick and
particularly vw varinfo wrapper had been very useful to
debug the models and come up with valid features  the
hinge loss function works much better than logistic loss
function 
as discussed in the earlier sections  it is possible to build
highly accurate classifier for each of the tags in the training
set  the precision is higher for specific tags such as php 
python and it decreases for generic tags such as file  java etc 
the results show that average precision of      is obtained
for the tags in top    set  the recall is particularly low in
the results  since we are not predicting tags from the entire
tag set 
v  future work
the next immediate thing to try out it to build a set of
top      top      and all the tags and see how the

page  

fiprecision and recall values vary  the expectation is that
mean f  score should go up by few percentage points 
the other thing to try out could be to add more features
so as to improve accuracy of the existing top    tags  also
could look at techniques such as lda to give us a list of
topics for documents  which could be  then used a feature 
references
    kaggle competition facebook  keyword extraction
http   kaggle com c facebook recruiting iii keywordextraction
    kaggle
leaderboard 
http   www kaggle com c facebook recruiting iiikeyword extraction leaderboard

    bird  steven  edward loper and ewan klein        
natural language processing with python  oreilly
media inc 
    j  langford  l  li  and a  strehl  vowpal wabbit online
learning project  http   hunch net  p           
    wang jian  davidson brian  explorations in tag
suggestion and query expansion  in proceedings of the
     acm workshop on search in social media  ser 
ssm     new york  ny  usa  acm        pp 
    saha a  saha r  schineider k  a discriminative model
approach for suggesting tags

page  

fi