solving differential equations using neural networks
m  m  chiaramonte and m  kiener

  introduction
the numerical solution of ordinary and partial differential equations  des  is essential to many engineering fields  traditional methods  such as finite elements  finite volume  and finite differences  rely on
discretizing the domain and weakly solving the des over this discretization  while these methods are
generally adequate and effective in many engineering applications  one limitation is that the obtained
solutions are discrete or have limited differentiability  in order to avoid this issue when numerically
solving des  i e   obtain a differentiable solution that can be evaluated continuously on the domain  
one can implement a different method which relies on neural networks  nn   the purpose of this study
is to outline this method  implement it for some examples  and analyze some of its error properties 

  formulation
the study is restricted to second order equations of the form
g x   x    x      x        x  d 

   

where x  rn is the independent variable over the domain d  rn   and  x  is the unknown  scalarvalued  solution  the boundary of the domain is decomposed as d   d d   d     d d   d 
where d d is the portion of d where essential boundary conditions  bcs  are specified  this study
is restricted to problems with only essential bcs  for a given function  x    x     x   x  d d 
to approximately solve the above using an nn  a trial form of the solution is assumed as
t  x  p     x    f  x n  x  p  

   

where n  x  p  is a feedforward nn with parameters p  the scalar valued function f  x  is chosen so
as not to contribute to the bcs  f  x       x  d d  this allows the overall function t  x  p 
to automatically satisfy the bcs  a subtle point is that  the single function   x  must often be
constructed from piecewise bcs  see section     furthermore  for a given problem there are multiple
ways to construct  x  and f  x   though often there will be an obvious choice 
the task is then to learn the parameters p such that eqn    is approximately solved by the form in
eqn     to do this  the original equation is relaxed to adiscretized version and approximately solved 
more specifically  for a discretization of the domain d   x i   d  i              m   eqn    is relaxed to
hold only at these points 
g x i     x i      x i        x i          i              m 

   

note this relaxation is general and independent of the form in eqn     because with a given nn it may
not be possible to  exactly   satisfy eqn    at each discrete point  the problem is further relaxed to find
a trial solution that nearly satisfies eqn    by minimizing a related error index  specifically  for the
error index
m
x
j p   
g x i    t  x i    p   x t  x i    p    x t  x i    p     
   
i  

 

fisolving differential equations using neural networks
the optimal trial solution is t  x  p     where p    arg minp j p   the optimal parameters can be
obtained numerically by a number of different optimization methods     such as back propagation or
the quasi newton bfgs algorithm  regardless of the method  once the parameters p  have been
attained  the trial solution t  x  p    is a smooth approximation to the true solution that can be evaluated
continuously on the domain 
a schematic of the nn used in this study is shown in fig    
input
layer

hidden
layer

output
layer

x 
  
 

n  x  p 

xn
bias 

 

figure    schematic of nn with n     input nodes  h hidden nodes  and   output node 
there are n     input nodes  including a bias node  and a single hidden layer of h nodes with sigmoid
activation functions  the single scalar output is thus given by
n  x  v  w     v t g w x  

   

where v  rh and w  rhn   are the specific nn parameters  replacing the general parameter
representation p   the input variable is x    xt     t   where the bar indicates the appended   used
to account for the bias at each of the hidden units  the function g   rh  rh is a component wise
sigmoid that acts on the hidden layer 
given the above  the overall task is to choose the discretization d and the number of hidden nodes h 
and then minimize eqn    to obtain the approximation t  x  p     assuming a given numerical method
that reliably obtains the solution p    this leaves the discretization and the hidden layer as basic design
choices  intuitively  it is expected that the solution accuracy will increase with a finer discretization and
a larger hidden layer  i e  nn complexity   but at the expense of computation and possible over fitting 
these trends will be explored in the examples  ultimately  one would like to obtain an approximation
of sufficient accuracy by using a minimum of computation effort and nn complexity 

  examples
the method is now showcased for the solution of two sample partial differential equations  pde  
in both examples  n     and the domain was taken to be the square d                  with a
uniform grid discretization d     i k  j k    i              k   j              k   where m    k        
both backpropagation and the bfgs algorithm were initially implemented to train the parameters  it
 

these methods may reach a local optimum in eqn    as opposed to the global optimum 

  of  

fisolving differential equations using neural networks
was discovered that bfgs converged more quickly  so this was ultimately implemented for these final
examples  furthermore  through trial and error it was discovered that including a regularization term in
eqn    provided benefits in obtaining parameters of relatively small magnitudes  without this term  the
parameters occasionally would become very large in magnitude  this regularization term also seemed to
provide some marginal benefits in reducing error and convergence time compared to the unregularized
implementations 

    laplaces equation
the first example is the elliptic laplaces equation 
   x       x  d 

   

 x       x    x    x     d   x       x       or x      

   

the bcs were chosen as

 x    sin x    x    x    x     d   x       
the analytical solution is
 x   

e


 
sin x  ex   ex   

e

   

using the bcs  the trial solution was constructed as
t  x  v  w     x  sin x    x   x     x   x     n  x  v  w   

   

for the case of k      and h      the numerical solution and the corresponding error from the
analytical solution are shown in fig     the numerical solution is in good agreement with the analytical
solution  obtaining a maximum error of about         
   

   
   

   e   

   

   e   

   

   

   e   

   

   e   

x 

x 

   
   

   

   e   

   


   

   e   

   

   

   e   

   

   

   e   

   

   
   
   

   

   

x 

   

   

   

 a  the computed solution t  x  v  w   

   t 

   

   e   

   

   
   

   

   

x 

   

   

   

   e   

 b  the error of the computed solution from the analytical solution    x   t  x  v  w    

figure    solution to laplaces equation  eqn     for bcs in eqn    

  of  

fisolving differential equations using neural networks

    conservation law
the next example is the hyperbolic conservation law pde
x  x   x    x   x    x  x    x  d 

    

where the bcs were chosen as
 x    x     exp x      x           x      

    

the analytical solution is
   x 

 x    x   x        x   e x    ex  e

  x  ex   

    

using the bcs  the trial solution was constructed as
t  x  v  w     x     exp x       x  n  x  v  w   

    

the network parameters were again obtained for k      and h      and the solution and error is
shown in fig     the numerical and analytical solutions are in good agreement  with a maximum error
of about           
although the errors in both examples are small  the error for laplaces equation is about an order of
magnitude smaller than that of the hyperbolic equation  while this may be due in part to the different
nature of the solutions  the different bcs may also have an effect  in laplaces equation the bcs
constrain the solution around the entire square domain  since it is second order in both variables   while
in the hyperbolic equation the bcs only constrain the solution along the bottom edge  since it is first
order in both variables   because the solution will automatically hold at the bcs due to the construction
of f  x   the bcs along the entire boundary in laplaces equation most likely contributes to overall
smaller error throughout the domain 

   

   
   

   e   

   

   e   

   

   

   e   

   

   

   e   

x 

x 

t

   
   

   

   e   

   

   e   

   

   

   e   

   

   

   e   

   

   
   
   

   

   

x 

   

   

   

 a  the computed solution t  x  v  w   

   t 

   

   e   

   

   
   

   

   

x 

   

   

   

   e   

 b  the error of the computed solution from the analytical solution    x   t  x  v  w    

figure    solution to the hyperbolic conservation law  eqn      for the bcs in eqn     

  of  

fisolving differential equations using neural networks

  error properties
as discussed previously  it is intuitively expected that refining the discretization and increasing the
size of the hidden layer will increase the accuracy of the solution  to study this  laplaces equation
was solved for a number of choices in k and h  and the maximum error over the domain   x  
t  x  v  w   max for each solution was recorded  to assess the dependence on h  solutions were obtained
for h            and    for a fixed k      to assess the dependence on k  solutions were obtained
for k         and    for a fixed h      the results are shown in fig     from the first figure  the
error steadily decreases for h         and   but plateaus for h       this suggests that for the given
discretization  a network complexity greater than h     yields diminishing returns in reducing error 
from the second figure  the error steadily decreases with increasing mesh refinement  it is unclear how
this trend continues for even finer discretizations of k      

 

   
   

 

   
log    t max

log    t max

 

   

 

   

 

   

 

 
   

   

   

   

   
log  h

   

   

   
   

   

   

   
log  k

   

   

 a  plot of maximum error versus hidden layer sizes h   b  plot of maximum error versus discretization sizes k  
         and    for fixed mesh size k     
      and    for fixed hidden layer size h     

figure    error trends for laplaces equation 

  conclusions and future work
in this study  a framework for the numerical solution of des using nns has been showcased for several
examples  the benefit of this method is that the trial solution  via the trained nn  represents a smooth
approximation that can be evaluated and differentiated continuously on the domain  this is in contrast
with the discrete or non smooth solutions obtained by traditional schemes  although the method has
been implemented successfully  there are several areas of possible improvement  because there is a
considerable tradeoff between the discretization training set size  and solution accuracy  and the cost
of training the nn  it could be useful to devise adaptive training set generation to balance this tradeoff 
also  this study used a uniform rectangular discretization of the domain  so future studies could explore
nonuniform discretizations  this could be especially useful in examples with irregular boundaries  where
more sample points might be needed in some regions of the domain compared to others 
  of  

fi