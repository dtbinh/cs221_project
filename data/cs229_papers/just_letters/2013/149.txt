alternate equivalent substitutes 
recognition of synonyms using word vectors
tri dao  sam keller  alborz bejnood
stanford university  cs      machine learning
december         

 

abstract

tions of all the different senses of the word  of
or at a relatively low temperature  lacking affection or warmth  a common viral infection   the
part of speech  adjective  noun  adverb   along
with some knowledge of the surrounding text  he
stopped cold   
the system we used represents these traits using word vectors  real vectors that represent
the word in n dimensional space  the vectors
are learned using a feed forward neural network  a classification algorithm that passes an
input through several layers of processing units
until reaching an output layer  we used the
open source tool word vec to generate these
word vectors  which in turn were used to investigate the potential of utilizing a combination
of machine learning techniques with natural language processing ideas in order to recognize synonomous words 

the task of computationally learning the linguistic application of a word requires a robust analysis of both the words semantic presence and
its independent characteristics  word vectors
are learned  functional representation of words
which can then be analyzed for linguistic regularities  we examine whether the synonyms of a
word can be recognized from their word vectors
by training a neural network on a large corpus
of text  then implementing k means clustering to
check whether synonyms have a statistically significant word vector similarity  we illustrate the
output using a simplified version of the vectors
generated by principal component analysis  results suggest that increasing vector length  which
improves syntactic and semantic accuracy when
performing comparisons of words  negatively correlates with synonym recognition 

 
 

introduction

data

we obtained and processed the following data 

effective representations of lexical tokens are de text corpus
pendent on a perceptive consideration of the
multiple characteristics that define words  these
 list of words
characteristics range from more concrete at list of corresponding synonyms
tributes  including but not limited to the definition  part of speech  and tense  to the seman text representation framework
tic relations of the word resulting from the conin order to effectively incorporate a diverse set
text in which the word is used  for example 
consider the polysemous word cold  a compre  of words  we used a corpus text from the weshensive understanding would include the defini  bury lab wikipedia corpus  comprising over  
 

fi   

million documents and approximately a billion
words  we extracted synonyms from wordnet  a
large lexical database for the english language 
by using a provided file consisting of      of the
most commonly used english words  removing
lexically meaningless tokens such as the and
a  and cross checking the remaining terms with
the         sets of synonyms in the wordnet
database in order to find the sets containing each
word 

 

skip gram model

the idea behind the skip gram model is an extension of the idea used in the single n gram
model  we want to look at not only the set of adjacent words  but also sets of words where some
 up to the order of the skip gram  are skipped 
the parameter that controls how many words
the model can skip is called the window parameter  in this paper  we are primarily interested in
the skip gram model with the window parameter
of          and     the architecture is illustrated
in figure    taken from      

models

we generated word vectors based on the following two models  bag of words and skip gram 

   

bag of words model

the bag of words text representation model
consists of treating a sentence as a map from
words to the frequency of that word in a sentence  data is not stored about the grammatical
nature of the sentence or word order  thus information about the semantic meanings of words is
lost  however  this is a useful baseline approach
to determine general similarities between words 
the architecture is illustrated in figure    taken
from      

figure    skip gram model  predicts surrounding words based on current word 

 

k means clustering

the clusters were calculated using a reduced corpus of more common words  with initial values
randomly selected from that corpus 
k means clustering
   initialization  choose a random word
   iteration 
mean

assign each point to nearest

   move mean to the center of the cluster

figure    bag of words model  predicts current
word based on surrounding words 

we implemented our version of the k means
clustering algorithm  as written above  and used
 

fiit to test synonym recognition as described below 
synonym recognition algorithm

  

  

  
total accuracy    

   run k means clustering on small set of
words       
   assign each word in large corpus to a cluster
   for each word in small corpus  compute 

  

  

  

 percentage of synonyms in same cluster

  

 ratio of percentage to expected percentage

 

bagofwords
skipgram  
skipgram  
skipgram  
skipgram   
 

  

   

   

   
   
vector length

   

   

   

to calculate the ratio of percentage to ex  figure    overall accuracy of different models
pected percentage  for every word vector we compute the probability of one of its synonyms being
assigned to the same cluster as itself  and then of a word being in the same cluster as the word
find the average probability by summing over all itself  compared to the percentage of any randomly chosen word to be in the same cluster as
words 
the word itself  larger number indicates better
synonym recognition 

 

results

length

we first computed the semantic accuracy and
syntactic accuracy of the word vectors obtained
using the internal testing tool that came with
word vec  which consists of       analogy questions  for example  a semantic question may
have the form of man woman king queen  if
vec man   vec woman    vec king  is closest
to vec queen   then the word vector model is
consider to be correct  a syntactic question
may have the form possible possibly quick
quickly  the accuracy is the percentage of
questions that the model get correctly  figure
  shows graph of overall accuracy  the average
of semantic accuracy and syntactic accuracy  for
different models  bag of words and skip gram
with window parameter of          and     this
measures how much the word vectors can capture the meaning and grammatical information
of the words they represent 
the k means clustering results are given for
bag of words model in table   and skip gram
 with window parameter of    in table    this is
the ratio between the percentage of a synonym

 
 
  
  
  
   
   
   
   

number of clusters
 
 
 
 
                       
                       
                       
                       
                       
                       
                       
                       
                       

table    k means clutering result for bag ofwords model
we also computed the average distance between a word and its synonyms  this distance
is then normalized by dividing by the distance
between that word and any randomly chosen
words  the graph of this normalized distance
is plotted in figure    lower distance means
better synonym recognition 
 

filength
 
 
  
  
  
   
   
   
   

number of clusters
 
 
 
 
                       
                       
                       
                       
                       
                       
                       
                       
                       

 

    

   

    

   

bagofwords
skipgram  
skipgram  
skipgram   

    

   

 

  

   

   

   
   
vector length

   

   

   

table    k means clutering result for skip gram figure    average distance between a word and
model  window of   
its synonyms

 

large dimensional vectors to   d space  figure  
corresponds to a skip gram model with vector
length of      the words in red are beautiful and its synonyms  and the words in black
are several randomly selected adjectives  we
see that synonymous words do not necessarily
cluster together  figure   shows the same skipgram model with vector length of     mapped
for cold and its synonyms  this time  synonymous words cluster closer together 

analysis

from the graph is figure    it is clear that
increasing the vector length yields significantly
higher accuracy  larger vector length means the
word vectors are able to capture more semantic and syntactic information of the words they
represent 
to investigate the data in table   and table   
we should note that a larger ratio implies a larger
percentage of synonyms that were mapped to the
same cluster as their original word  which implies
affordable
affluent
better synonym recognition  coupled with the
plot of the normalized average distance between
a word and its synonyms  we see a reverse trend 
acceptable
fair
smaller vector lengths significantly improves the
beauteous
probability of detecting synonyms  these results
are statistically significant since  for example  usaggressive
beautiful
ing the skip gram model with window of   words 
handsome
gorgeous
the synonyms of a given word are about     times
more likely to be in its cluster as a randomly choagile
sen word 
however  the length of the vector representation cannot be too small  any length less than
   decreses the likelihood of detecting synonyms  figure    beautiful and its synonyms  along
the best vector length varies from    to    
with randomly selected adjectives
an illustration of the clustering is shown with
an application of the pca algorithm which maps
a longer vector length means having more fea 

fi 

agreeable
brisk
bland

brooding

future work

these results suggest several extensions to generating a more effective classification system  we
propose the following two alternatives which are
likely to improve synonym detection using clustering and distance computation of word vectors 

breezy

abysmal

   train the neural network with a large vector size to increase syntactic accuracy using word vectors  however  maintain another model with a small vector size  preferably from    to     which yield a much better probability of synonyms of a given word
having a vector representations close to the
representation of the word itself 

frosty

cold

icy
frigid

figure    cold and its synonyms  along with
randomly selected adjectives

   traing the neural network with a large vector size as before  however  during training  add another class of parameters  relative weights for each dimention of the vector
representation  i e  for each feature   train
the neural network to optimize these weights
so as to minimize distance between words
and its synonyms  as given from a lexical
database  since we are not losing any feature  the semantic and syntactic accuracy
of the model is maintained  however  synonyms recognition will be much improved 

tures  some of which may have no bearing on the
concept denoted by the word  instead  they may
capture the syntactic information or the context
of how the word is used  this is in fact how the
neural network is trained  it has no access to the
meaning of the words  only the context of how it
is used  when we then use this high dimensional
data to compute distance between a word and
its synonyms  the conceptually irrelevant portion
of the representation causes the distance to be
much larger  when we limit the number of features in the internal representations  more of the references
vector representation contains conceptually relevant information  as such  the distance between     mikolov  kai chen  greg corrado  and jeffrey dean  efficient estimation of word repa word and its synonyms might be smaller 
resentations in vector space  in proceedings
using word vector representations can give a
of workshop at iclr       
meaningful result since the synonyms of a given
words are likely to be in its cluster  however  this     tomas mikolov  ilya sutskever  kai chen 
greg corrado  and jeffrey dean  distributed
is not necessarily a good enough metric to detect
representations of words and phrases and
synonyms  these cluters are large and contain
their compositionality  submitted to nips
hundreds of words  if we only rely on the clu     
tering data or distance data  we cannot conclude
definitely that two words are synonyms  how    j  turian and l  ratinov and y  bengio 
ever  since this is a statistically significant result 
word representations  a simple and general
it can be used to improve synonyms recognition
method for semi supervised learning  acl 
when paired with other methods  for example 
     
syntactic and semantic analysis 
 

fi