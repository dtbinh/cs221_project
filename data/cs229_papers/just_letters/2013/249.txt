exploring potential for machine learning on dataset about k   
teacher professional development
devney hamilton

keziah plattner

december         

 

problem and goals 

y value that was equal to the average of these indicators 
our samples were clustered tightly around the
mean score of      so we converted the continuous y
values into two classes with class   being below average  score less than      and class   being above
average  score greater than or equal to         we
threw out the    samples that fell directly on the
average 

evaluation of k    teachers in the united states involves intense political and philosophical debates 
data on teacher professional development  pd 
practices is only just emerging in digital form  as
pd management for k    moves to online platforms  and teachers are rated on numerically scaled
rubrics  education leaders hope to identify generalizable factors in pd practices  previous experience  training  and institutional support that predict teachers improvement  but first we mustdetermine the viability of currently available data for
predicting teachers ratings 
we began this project with a dataset recording
teachers professional development activities and
ratings of their performance in one large charter school network  drawn from a commercial pdmanagement platform  our goal was to find which
available features are relevant to predicting ratings  identify barriers to meaningful analysis of the
dataset  and make recommendations to interested
education organizations on data collection for future analysis 

 
   

   

features

we used subsets of these per teacher features
throughout our experiments 
a  volume of evidence in their evaluations
b  number of professional development activities
they have engaged in
c  the number of teachers their coach works with
d  number of rubric items they were rated on
e  number of resources teacher attempted
f  completed resources per teacher
g  goals completed
h  number of goals made
i  the average length in days of all their observations
j  what month of year they were observed
k  which week of the year they were observed

setup

   

data and y values

cross validation strategy

we tried several different strategies with training
on our data  first  we did random cross validation
holding out     of the data  then iterated over
many runs of this to find the average error on our
training and testing data  next  we tried leave one
out  and got very similar error rates  ultimately  we

some data points were associated with a teacher 
some with a coach  and some with an observation
event  we decided to define a sample as a teacher
because there were more teachers than coaches 
and more features associated with teachers  each
teacher is rated on a scale of     for some subset
of    rubric items  each of which describe one aspect of teaching  we assigned each teacher a single

  we used this split with all our experiments unless otherwise specified

 

fidecided on one iteration of cross validation holding and the specificity of teachers goals were not avail    of our data  so we could analyze the training able as we hoped 
in addition  many of the additional features were
and testing data in order to better understand our
present for only some of the teachers  we replaced
error rates  
the counts of missing features with zero  in order
to maintain our larger sample size  our baseline results for features a   k using imputation of missing
  initial attempts
data was 
we initially started experiments with only features
dataset 
train dev
a e  we tested linear regression with the raw avmnnb error
   
   
erage scores  but quickly found out that the data
svm error
   
   
was not linear 
class   prior
   
   
svm and naive bayes  nb  were readily availsample size
   
   
able in matlab  and were our next starting point 
given that most of our data involved counts of tokens  such as number of goals  resources  etc   a
multinomial distribution made the most sense  in
addition  the multinomial distribution makes an assumption that the total number of tokens is independent of the response class  this fit the best with
our data  since each teacher has a different amount
of information about them  but it should not impact how they are scored in the end  with this
logic  a sample is a teachers collection of pd actions  the prior on the data set as a whole is    
class   and     class    and our initial experiments
with svm  and multinomial nb  mnnb  left us
with a minimum development error of     
this low performance left us to explore in our
further experiments whether it was due to an insufficient number of samples  bad feature selection 
or a bad modeling of the problem  in these exper  figure    features with weights most different beiments we faced a constant trade off between more tween classes are highlighted in blue  most are
complete information per sample and large sample available for only a subset of the data  the relate
to the teachers and the their coaches professionsize 
alism and involvement in pd activities  most have
values so small they are likely to be overridden by
the prior 
  further experiments

   

the weights are reported in figure    where n
is the number of samples for which a feature value
was available 
only a small subset of teachers had features like
the number of completed goals  and the number of
times they had used professional development resources  however  these features had the greatest
distinction between above average teachers and below average teachers  for example  the number of

additional features

our next step was to exhaust our database of features  resulting in a total of    features  unfortunately  some information like teachers years experience  communication among teachers and coaches 
  the

reported results in this paper come from this last
strategy 
  unless otherwise noted  svm was always run with a
linear kernel  as our experiments with other kernels provided
no improvement 

  see interpolation experiments for more details on our
attempts with generative classifiers and imputation 

 

figoals completed was     higher for teachers who
were above average  the number of goals created
was     higher  and the number of resources completed was     higher  unfortunately  this data
was rarely available for the samples we were predicting  resulting in a higher error rate in the development data set compared to the training dataset 
these significant differences in weights despite
our high error rate implies that these are key features in determining teacher performance  but are
either only a subset of important features needed
to more accurately predict teacher performance  or
need to be present in all our data in order to accurately predict teacher performance on testing data 

   

the influence of feature weights in the larger set
decreased so that all the weights for class   and
class   were nearly equal  this suggests that the
presence of those additional features  while incomplete  is significant in classifying teacher performance  even if we must fill in missing data for the
rest of the samples  our following imputation experiment suggests the absence of some data is itself
meaningful 

   

we had choices about how to handle missing feature values  because our features were counts from
a mysql database where the count starts as soon
as something is created  a goal  an activity  etc   we
set null values as   at first  this is consistent
in what we observed in      where the absence of
goal and resource data in samples mattered more
than variations within the subset where that data
was available 
we wanted to test if this   count assumption was
breaking existing correlations in the features  so we
also tried using imputation by filling missing values
with the average for that feature  unfortunately 
we did not find a performance improvement over
replacing missing values with    comparing to our
baseline in      this is consistent with our assumption that our features are inherently counts of actions 

try redefining y values

teachers are evaluated on rubrics  our initial attempts ranked teachers on the average of all their
scores  however  the rubrics are broken into   specific categories  we tried defining a teachers label
based on their score in each category  we ended
up getting better results with the category labeled
professionalism  averaging around     development error with mnnb and     with svm     
samples      prior on class    
this makes sense for two reasons  features
that show a difference between below  and aboveaverage teachers are related to professionalism
practices  goal setting  resource usage   another
explanation is the       prior split  making this
performance little better than what we observed
with our more general definition of a y value 

   

dataset 
mnnb error
svm error
class   prior
sample size

limiting the dataset

we were curious if we could get better results on the
subset of data for which we had values for features
that had distinguishable weight values  requiring
all samples to have completed goals and completed
resources would have restricted our datasize to so
small that randomness would have overridden results  instead  we used the subset  n        for
which simply the count of goals and count of resources were available 
dataset 
mnnb error
svm error
class   prior
sample size

train
   
   
   
   

imputation

train
   
   
   
   

dev
   
   
   
  

we also experimented with latent variables by
adding a boolean for whether or not features were
available  in this initial attempt we saw no noticible difference  but this needs further investigation 

 

model fit

in our experiments with more features and different
sample sizes  we tried svm with various kernels 
but were never able to bring its performance above
that of mnnb  we used this as a suggestion that
our choice of mnnb as a model was not a limiting
factor in our performance 

dev
   
   
   
  
 

fi 

however  we later tried classifying only extreme
examples  defining class   as an average score of less
than      and class   as an average score of greater
than      we replaced missing feature values with
the average of that feature over included samples 
we were were surprised to find that it was easier
for svm to classify a subset of this dataset where
the labels are extremely different  though svms
performance on the full dataset had been indistinguishable from that of mnnb  our results are summarized below 
dataset 
mnnb error
svm error
class   prior
sample size

train
   
    
    
  

over all analysis

dev
   
    
    
  

the low performance in mnnb may be in part due
to the small sample size imposed by the restrictive definition of the classes  on the training set 
mnnb performed equivalently to assigning all samples class    since the error is equal to the prior on
class          on the development test set  it performed no better than a coin flip  for the training
set  svm performed better than the prior  with
the development test set  svm also performed better than mnnb 
svms superior performance suggests that with
classifying extreme examples  we can find a model
that is better than than the multinomial model 
since in general svm classifies with fewer assumptions than mnnb  the dataset with extreme examples is more likely to be separable than the dataset
including samples close to the mean  see over all
analysis   since svm did better than mnnb in
this case  and better than mnnb ever did on a
fuller set including samples closer to the mean 
it suggests that the extreme examples are better separated with an svm model than with an
mnnb model  in this case of more likely separable
data  it seems that perhaps the mnnb assumptions
did pose a barrier to performance when compared
svm 
this experiment also suggests that it would be
useful to model latent variables that might help
distinguish samples whose labels fall closer to the
mean  and expose variance in teachers intrinsic
performance level 

figure    plotting features that have distinguishable weights in our baseline       and have large
sample sizes creates a pattern filling the lower right
half of the plot  this helps us understand why were
were more successful classifying only extreme examples  and the need to find other variables  latent
or observed in other domains  affecting teachers
ratings 
we had somewhat more success classifying extreme examples  mnnb found differences in the
counts of certain pd related activities for below
and above average teachers  however  these differences were not enough for successful classification 
we decided to plot these more interesting features
against teachers scores  average over their rubric
ratings  before classifying into class   and class
    what we found is that more pd activities predicts a higher score  however  a high score does
not predict more pd activities  as demonstrated in
figure    in this figure  we see that three of the five
most influential weights  difference between classes
is more then     in our baseline  in our baseline fill
the lower right triangle of the plot  the number of
pd activities  red   the number of goals  magenta  
the number of resources used green   the number
of resources completed blue  also falls roughly in
 

fithe lower right triangle  but was not a feature that
got noticably different weights  perhaps because of
the relatively low number of samples for which that
information is available 
this supports our suspicion that there are other
factors that have an important influence on teachers scores  these may be latent in pd data  and
discoverable via more complex models  or they may
be observable in other domains such as teachers
experience  training  and institutional support  it
is also consistent with our finding that it is easier
to classify more extreme examples  since very low
scoring teachers generally have taken very few pd
actions 

and qualifications  the organization could not provide them  to make meaningful conclusions about
teacher performance  organizations should prioritize combining relevant information for statistical
analysis  this conclusion is confirmed by several
education research sources    also  pd management providers should be aware that there is a difference between the data necessary for managing
pd and that for describing pd practices  timestamps are important for scheduling  but fortunately had little relationship with teachers scores 
other expected features were not present  for example the platform had the potential to measure
teacher coach interaction and collect teacher metadata  but since users chose to not use that functionality  the tables for this data were nearly empty  in
  conclusion
this case  the database optimized for personal applications rather than global queries  so there was
with the currently available data and preliminary significant per feature overhead  resolving these isexperiments  we were able to identify which fea  sues requires cooperation with schools to combine
tures were more and less relevant to teachers rat  data sources and allowing for data processing overings  we saw in     that certain features had a head 
big difference between different classes  but our error rate was still high  this means that these fea    next steps
tures are valuable in understanding teacher performance  but are only a subset of the total features the next steps to predicting teachers ratings are
needed in order to accurately predict teacher per  to include more relevant  observable features  we
formance  this conclusion fits with our other ex  also recommend further exploring latent variables
periments  extreme examples are easier to classify  with models such as a mixture of gaussians or hidsince they dont need the extra nuances of other den markov models    the distribution of scores
features in order to correctly predict teacher per  suggests there is social pressure to rate teachers
formance  since extreme examples are more likely near the mean  since deviations have funding and
to be linearly separable given observable features employment implications   obscuring naturally octhan the dataset as a whole  there may be latent curing variance in teachers effectiveness  however 
variables at work in examples closer to the mean  the implementation overhead suggests this should
the missing feature data in some of the teachers wait until a richer dataset is available 
is not necessarily problem  since just the absence
our later experiments suggest that logistic reof relevant features can help predict scores  lastly  gression or further tuned svm could better for
our experiments in redefining of our y value indi  classifying these samples than mvnb 
cates that the observed features we were able to
work with are mostly relevant to a specific area of
teacher performance  professionalism 
overall  it was both exciting and frustrating to
work with real world  unexplored data 

   

  national education policy center research based
options for education policymaking   teacher
evaluation  william mathis  university of colorado  boulder  september      
  developing and selecting assessments of student
growth for use in teacher evaluation systems  joan l 
herman  margaret heritage  and pete goldschmidt

implications

confidentiality issues in education can impede statistical analysis  when we requested more relevant features such as teachers years experience
 

fi