predicting stack exchange tags
rose perrone
 dated  december          
this paper shows a method of predicting the tags assigned to stack exchange questions  given only the title and description of millions of stack exchange questions 

i 

introduction

stack exchange is the a popular question and answer
website  when someone asks a question  they must provide a title  description  and tag the question with    
tags  note the three tags on the question above 
answerers use the tags to find questions on topics in
their expertise  this task of predicting the tags is a kaggle competition  the competition provides  gb training
data and  gb testing data 
this was a fun learning project becasue i tried out
different algorithms  features  and tuning parameters and
the results of changes often surprised me  to get a much
higher f  score  however  i would use a deep learning
method like a neural net 



rose cs stanford edu

ii 

methods

my latest algorithm has four major steps  which ill
discuss in turn 
   data purification
   feature extraction
   logistic regression
   tag count selection
note that when running experiments to tune parameters whose results you see in the plots below  i assigned
three tags to every post  because there are an average of
three tags per post  each plot is titled with the number
of training examples  i typically tested on     that number  i found that the f  score plateaud at around     
traning examples 
the only algorithm i didnt implement myself was cosine similarity  for which i used scikit  because finding
the cosine similarity was the most time intensive step in
my algorithm  and scikit uses a c compiler 

fi 
a 

data purification

i stripped out html markup  unimportant punctuation  capitalization  and stopwords  i hand selected over
    stopwords from the list of most frequent words that
werent themselves tags  below is a plot of the number
of stopwords vs  f  score 

i also found that tag prediction based only on cosine
similarity using only one most cosine similar document
gave an f  score of       compared to my algorithms f 
score of      

  

b 

feature extraction

i tried out a variety of features  but kept only those
that my optimization algorithm gave significant weight 
these four features are described in the following sections  the following weights were learned by logistic regression 

probability of the text given the tag

i computed the probabilitiy of each ngram found in
the posts given each tag  i summed these probabilities
for each post per tag  giving me an estimation of the
probability of the text given my tag  this feature is
similar to the naive bayes score  except that i include
the probability of the tag in a separate feature  unigrams
gave a better f  score than bigrams and trigrams  as
shown in this plot 

feature
weight
probability of the text given the tag     
frequency of tag in title
    
tag popularity
    
cosine similarity score
    

  

cosine similarity

i generated what i call a cosine similarity score for each
tag by converting each document to a tf idf vector and
finding the two documents that are most cosine similar 
and then for each tag  i summed the cosine similarity
scores of the documents to which it was assigned  i chose
the two most similar documents because of the following
finding 

there is a discrepancy between this trend and the success of trigrams when i run my implementation of naive
bayes by itself  i have not resolved this discrepancy 
when i run naive bayes  i assign three tags per post 
as i do in my full algorithm  when i run naive bayes
on unigrams  bigrams  and trigrams  i get the following
results  the best result  an f  score of       is still much
lower than the best f  score i get on the full algorithm 
which is      

fi 

  

popularity of the tag

the popularity of the tag is the number of times it
is assigned to training samples  i was curious if removing the least popular tags from my candidate set would
improve the f  score  but it didnt  as shown below 

  

tag frequency in the title

this feature is simply the number of times the tag
appears in the title of the post 

i got to see how weights and covergence changed on
each iteration of gradient ascent when i varied the learning rate  convergence constant  data size  and different
features  i set the convergence constant to  e    and the
learning rate to  e   
i tried squaring each feature  testing the effect of squaring a single feature one feature at a time  but i found no
squared feature significantly improved the f  score  as
shown below  the    is the baseline where no features
are squared 

key
x  
x  
x  
x  

shows
shows
shows
shows

probability of the text given the tag
tag frequency in the title
tag popularity
cosine similarity score

d 
c 

logistic regression

i normalized the design matrix to mean zero  standarddeviation   for each feature  i gave each feature vector
a supervised output of   if the tag was assigned to the
post on which the feature vector was generated  and  
otherwise  because i pared my number of features down
to    my algorithm was tolerant to changes in l  regularization as shown below 

tag count selection

i could choose to assign between   and   tags to each
post  to choose that tag count that would most improve
the f  score  i kept an online average of the top five tag
scores per post  where a tag score is the dot product of
the tags feature vector and the learned weight vector 
for each post  i assigned the top k tags  where
k   max    number of tags with a score above     
 average tag score   i chose     because of the following
result 

fi 
iii 

results

the baseline was assigning the three most popular tags
to every post  that gave an f  score of        my full
algorithm gave an f  score of       a comparison of the
algorithms i used is shown below 

fi