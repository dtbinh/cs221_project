cs     final project
sentiment causation extraction
desmond c  ong  dco stanford    wen hao lui  whlui stanford 
i  i ntroduction
in state of the art sentiment analysis  text is analyzed
for a single unidimensional sentiment or opinion score 
this unidimensional sentiment  however  cannot capture the
nuances of emotions  for example  two texts that respectively
convey anger and sadness will both have a negative sentiment associated with them  while carrying very different
connotations  thus  we require a tool that allows more
sophisticated analysis of emotional text 
this need for more sophisticated relation extraction is
relevant outside of sentiment analysis as well  for example 
causal relation extraction has important applications for
building question and answer systems  relation extraction
remains an incredibly difficult problem to solve  but offers
many promises such as constructing abstract knowledge
representations or constructing narrative sequences from
newspapers     
in this work  we propose building a causal relation extraction tool specifically for extracting the cause of emotions 
although we focus on causal extraction of emotions  the tool
is generalizable to causal extraction in other domains 
ii  p rior research
in recent years  there has been an exponentially increasing
number of papers in sentiment analysis  most of which
use simple bag of words models and a few that have made
extensions by using bigrams     and deep learning over
annotated sentiment of parse trees      these unidimensional
sentiment scores do not give insight into the different nuances
of similarly valenced emotions  e g  within negative emotions
 anger  sadness  disgust  disappointment  anxiety   or within
positive emotions  happy  excited  content   
a next class of research involving emotion analysis in text
has focused on identifying a set of basic emotions in text 
these have mainly focused on training a classifier  using
text that has been annotated into one of these emotional
categories   and has been used to classify newspapers    
and blogs     
several papers go a step further      reported an emotion
extraction system with a parser that accounts for auxiliary
verbs  referent  tense  conditionals  etc  while     proposed
a fuzzy semantic typing method to analyze affect  and the
system generates an affect profile based on analyzed words 
our goal in this paper is not to identify the emotion in
text  although that is a component of our model that can
be improved upon using previous research   but  given an
already identified emotion in the text  identify the potential
cause of the emotion  that aspect of our work is more similar

fig     example of a sample problem  given the two parse trees in this
paragraph and the emotion happy  we want to be able to identify the
correct reason published his paper 

to work in causal relation extraction  such as using annotated
causal pair examples to extract linguistic features 
iii  m odel
a  statement of research objective
the goal is to build a system that  given a paragraph of
text  consisting of    sentence  and an emotion word in
the paragraph  identify the cause of the emotion  candidate
phrases from a dependency parse of the paragraph are
considered  and the most likely phrase is chosen as the cause 
none of the causes can also be chosen  if they all do not seem
likely 
b  corpus
the corpus we used is from the experience project  ep  
which is a social networking site where users join groups
based on shared experiences  e g  i am a military wife 
or i am a cancer survivor  or even i am lonely  users
can  among other things  post stories and comment on others 
the portion of the corpus we used for this work consists of
short paragraphs of free form text from feb       through
june        and consists of approximately         short
paragraphs 
c  generation of labeled dataset
from the    k paragraphs  we selected a subset of  k paragraphs  and applied simple filters to discard the paragraphs
with no identified emotions and to weed out potentially
offensive material to get a set of       paragraphs  of
these  we had workers on amazon mechanical turk label
      paragraphs  while we hand labeled the remaining    
paragraphs to build intuition for later feature engineering 
for each paragraph  an emotion word was identified using
a simple word matching lexicon  which took into account

fisynonymous words using synsets  or synonym sets  constructed using thesauri  e g  joyful and happy would
fall under the same synset  the most frequently occurring
emotion synset in the paragraph was chosen  human labelers
were asked to identify the cause of the given emotion by
copying the cause phrase into a text box  or by indicating
if no cause was found  two raters provided ratings for each
paragraph  if there was disagreement  we took the longest
common substring between the two labels 
next  we used the stanford dependency parser to parse
all the paragraphs  using the dependency parse trees  subtrees that contain more than   word  to avoid terminal
nodes unigrams  were identified as candidate phrases  the
explanations labeled by human raters were tagged to the
smallest subtree that contains the explanation  this subtree
was then marked as a positive example for the explanation
of the emotion in the paragraph  all other subtrees were
marked as negative examples  in some paragraphs  there was
no cause found  and so all the subtrees were marked as
negative  thus  our final labeled dataset consisted of a set of
paragraphs     sentence  and thus    parse trees   a list
of candidate phrases for each paragraph  all subtrees with
   word within the paragraph   an emotion word within the
paragraph  the  phrase  paragraph  emotion  input tuple is
labeled with a simple     indicator to indicate if the candidate
phrase is a positive or negative example 
d  linguistic features
we designed    features in total  spread over the following
categories 
   bag of words  given the dominant emotion in the
paragraph  we create  word  emotion  indicator features for
each word in the phrase 
   part of speech tags  we included indicator features
that coded for whether the candidate was a noun phrase
 np   a verb phrase  vp   an adjective phrase  adjp  
an adverb phrase  advp   a quantifier phrase  qp   or a
prepositional phrase  pp   in addition  we had a feature that
coded for wh clauses  who what when where why  
   proximity features  we coded several features that
accounted for semantic cues in proximity  we added features
that indicated if the candidate contained causal conjunctions
like because  so  that  some other examples 
 indicator feature for whether the candidates left sibling
contains because at the end  e g  if the candidate
phrase is  i fell down   then the feature fires if the
sentence contains    because  i fell down     
 indicator feature for whether the candidates left sibling
contains i am  emotion  that  for example  i am sad
that  i fell down  
   distance features 
 indicator feature for whether the candidate is in the
same sentence as the emotion
 indicator feature for whether the candidate appears
before the emotion in the paragraph
 indicator feature for whether the candidate appears after
the emotion in the paragraph  the only case where



both candidatebeforeemotion and candidateafteremotion return   is when the emotion is within the candidate
phrase  
  distance metrics that code the absolute distance between the emotion word and the start end  whichever is
closer  of the candidate phrase  in units of characters 
words and sentences  we scale these features appropriately to keep the maximum value below   

e  learning
we break our learning problem into two phases  we first
train a linear svm classifier  using liblinear      using
a simple bag of words model  each inputs feature vector
for this step is purely based on the bag of word indicators
described earlier  the classifier then infers a score on the
candidate phrase and the score is fed as a single feature into
the second phase  the sparsity and large cardinality of the
 bag of words  feature set makes a linear kernel well suited
for the learning in this phase 
we formulate this as an svm with    regularization for
the learned weights w 
 
  w     c  i
 
i

min
w

yi  wt  xi        i

s t 

i   

i

i

where  xi   is the feature vector comprised of bag ofword indicators for xi   with a corresponding yi label where
   is a positive example and   is a negative example  c
is an adjustable hyper parameter which encodes the trade off
between error margin and minimizing the weight  c is set
to   in our experiments  the last factor i encodes the slack
for each misclassified example 
in the second step  we train a radial basis function  rbf 
kernel on the linguistic features  including the bag of words
score  of the input  phrase  paragraph  emotion  tuple  we
have    linguistic features for this phase  which is a much
smaller and more manageable set  this allows us to use the
rbf kernel  which runs much slower than a linear kernel
but has an infinite dimensional kernel feature space and is
well suited for exploiting non linear relationships between
the various features 
we present the rbf svm objective in the dual form 
k x  z    exp kx  zk   
 
max  i    i  j yi y j k xi   x j  

  i j
i
s t 

   i  c

i

 i yi    

i

i

of the      labeled examples  we set aside     for testing
and trained our model on the remaining      examples  we
chose to include more paragraphs for our training because
we plan to scale up the training set using semi supervised
learning later  and wanted to reduce the amount of bias
present in the seed training set 

fi   inference 
given
a
list
of
 phrase  paragraph  emotion  inputs  where paragraph
and emotion remain constant  the svm classifier returns a
list of scores corresponding to how confident it thinks each
input is a positive example  we take the input phrase with
the highest score and choose it as the output cause phrase 
   no cause found scenario  we define a score margin as the difference in scores between the chosen cause
phrase and the second best cause phrase  we initially formulated the no cause found situation as a simple threshold
problem  where the inference algorithm would return an
empty string if the score margin during inference was too
low  however  there was no easy way to estimate a good
threshold value that works across various examples  we thus
decided to fold this component into our learning algorithm
as well  by adding an empty string as one of the candidate
phrases for each training example  the empty string would
then activate its own unique indicator feature  allowing the
svm to effectively train on it as well 
f  scoring metric
we initially used a simple accuracy metric to keep track of
the algorithms performance  but some preliminary analysis
revealed that the number of false negative cases was disproportionately higher than false positives  this is primarily
because the number of positive examples is much less than
the number of negative examples  for each paragraph  there
are many subtrees  but there is only at most one correct
cause   we use a bleu score as a more useful metric
instead  we also included a confusion matrix in our analysis
to provide another perspective for understanding our models
performance 
   bleu  the f  unigram bleu scoring mechanism
takes into account both precision and recall  only unigrams
are considered because higher n grams are typically used to
measure the fluency of the output  which is not a concern
for our inference method  we are only considering subtrees
in the parse trees extracted from the paragraph as possible
outputs  and since all subtrees can be converted back to a
original phrase or sentence in the input paragraph  we do
not anticipate problems with fluency  a guess phrase that
perfectly matches the correct label phrase will receive a
bleu score of   
as an example  take the guess phrase the lottery winner
against the correct phrase he won the lottery  the unigram
precision is p        and the unigram recall is r       
leading to an f  unigram bleu score of  pr  p   r        

kernel
linear
radial basis function   svr 
radial basis function  csvc 
radial basis function  csvc 

training set
s
s
s

bleu score
     
     
     

s u

     

table i  bleu results for various training configurations  s  seed 
       initially labeled training examples  u        unlabeled training
examples  relevant for the semi supervised approach 

actual positive
actual negative
total
table ii 
labeled set 

predicted
positive
   
    
    

predicted
negative
  
    
    

total
   
    
    

confusion matrix for the svm model trained only on the

formulation  c svc  also performs better than the regression
formulation   svr   primarily because of our formulation
of the objective as a binary classification problem  the data
input thus lends itself well to classification  while regression
on it will require large slack margins that can skew the result 
we next look at the confusion matrix in table ii and
calculate some basic statistics 
precision   t p  t p   fp        
recall   t p  t p   fn         
speci f icity   t n  t n   fp         
accuracy    t p   t n   t p   t n   fp   fn         
where tp   true positive  fp   false positive  tn   true
negative  fn   false negative 
we see that although we have a reasonable performance in
classifying both true positive and true negative examples  the
precision statistic is remarkably low simply because there are
a lot fewer positive examples than negative examples  recall
is significantly higher than precision  although they are both
measuring the correctly classified positive examples 
the accuracy statistic is also misleading for our case 
if we have a classifier that labels all of the test data as
negative  the accuracy will be                          
even though it is not making any useful predictions  this
misalignment is the primary motivation for choosing the f 
unigram bleu as our metric for performance  since it is
not affected by data where one label greatly outnumbers
another  we also compensated for the relative low numbers
of positive examples by making their weights    times that
of the negative examples in our learning step 

iv  r esults
we experimented with different formulations of the svm 
and found that the best performing configuration was the
classification svm using a radial basis function kernel 
as seen in table i  it significantly outperforms the other
classifiers with a bleu score of        the rbf kernel is
superior to the linear kernel because it can map non linear
relations between features  which is useful in our small feature space  excluding the bag of words   the classification

v  s emi   supervised l earning
we performed training on an iteratively larger subset of
the corpus in order to make use of the large amount of
unlabeled data  given that the initial labeled set of paragraphs
is of size m  we first train and obtain the weights on that
labeled set  we then perform labeling on another disjoint
and unlabeled set of size    m and obtain the labels for
them  keeping the most confident    m predictions based on

filabeled set  seed set  
while unlabeled set is not empty do
take     size of labeled set  examples from
unlabeled set  
classify taken examples  
pick top     with highest score margin  add to
labeled set  
return remainder to unlabeled set  
end
algorithm    semi supervised learning

fig     illustration of a semi supervised learning approach  the black
and white circles indicate labeled examples  while the gray circles are
unlabeled  using only labeled examples for learning may result in a
decision boundary that does not fit the underlying structure of the data 
by inferring labels on unlabeled data to grow the training set  we can find
a better decision boundary  image sourced from http   en wikipedia 
org wiki semi supervised learning

the score separation of the correct pair from the secondbest pair  we then perform the optimization of weights
again on the combined    m examples  repeating this process
until we have trained on a sufficiently large set or reached
convergence  see algorithm   for the pseudocode 
the main strength of this approach is that it allows us
to make use of the unlabeled set  which is about     times
larger than our labeled set  although we are using our own
inferred labels for further training  which risks magnifying
errors and biases in our original training set  in practice this
approach has proven to work well as long as we can identify
a useful heuristic for the metric we are measuring against
 in this case  the score difference of the top phrase from the
second best one   figure   shows how adding more unlabeled
examples  paired with their inferred labels  can considerably
help with forming a better decision boundary
a  semi supervised results
looking at the results in figure    we see a clear upward
trend in both the accuracy and the bleu score  the gains
in accuracy are due to the reduction of false positives  as
seen in table iii  the bleu score plateaus at around      
which is significantly higher than that of the original       
we believe that good directions for future work would be to
find more discriminative features and start with a larger and
more diverse seed set to help to raise the upper limit on the
semi supervised approach 
one interesting feature in the learning curve is the significant drop in the bleu score for the first iteration  this
is probably an artifact of the algorithm that we are using 
which chooses the top     inferred labels with the highest
score margin  since the score margin has a lower limit of   

actual positive
actual negative
total

predicted
positive
   
   
   

predicted
negative
  
    
    

total
   
    
    

table iii 
confusion matrix for the svm model trained on     
labeled and      unlabeled training examples  the number of false positives
is greatly reduced compared to training only on the labeled set 

fig     the learning curve for the semi supervised approach we are taking 
we see significant gains in both the bleu score and the accuracy as the
size of the training set increases with the number of iterations 

positive examples tend to have much higher score margins
and are selected disproportionately 
examining the confusion matrix for the first iteration  we
see that our classifier increases the amount of true positives
labeled  but reduces the number of true negatives identified 
since the negative examples outweigh the positives by about
   times in our test set  the bleu score understandably
decreases  however  the positive examples added are still
valid  and therefore when we add more training examples in
subsequent iterations we tend to increase the bleu score as
well 
vi  e rror a nalysis
the largest source of remaining errors is incorrectly predicting the presence absence of causal phrases  even though
we encoded this as a feature  the classifier still cannot
differentiate between the two cases sufficiently  abstracting
this away as a separate problem  such that our input data
only consists of paragraphs with an identified causal phrase 
would significantly improve performance in identifying cause
phrases 
however  the problem of identifying if there exists a cause
phrase in a paragraph in itself is not trivial  and using a
feature based classifier for this simplified problem may not
yield significantly improved performance over our current
implementation 
another common error that our tool makes is that the
guesses it makes often contains the emotion itself  e g  that
your life makes you happy when the gold label is your
life  more often that not  the candidate phrase that contains
the cause usually does not contain the emotion  and although

fiwe have features that account for that  perhaps there needs
to be additional features that code for dependency structures
more  in this case  the correct answer is actually contained
within the guess  but one level down 
we also noticed a drop in performance when the labeled
cause is in a different sentence from the identified sentiment 
this is because we lose quite a bit of structural information
in the features due to the dependency parser operating on
a sentence level instead of a paragraph level  one interesting extension would be to try and encode some kind of
dependency relationship between sentences as well  instead
of just words within a sentence  for example  establishing
temporal relations between sentences can help the classifier
focus on sentences with events that occur before the events
corresponding to the given sentiment  we can also include
some basic co reference resolution  to help establish links
between sentences 
lastly  the cleanliness of the input also posed some
challenges  because we were taking data from a live website
with user postings  there were often misspellings and punctuational acrobatics that challenged the stanford dependency
parser and led to unpredictable features for those examples 
for example  some users do not leave spaces between their
sentences  which causes the dependency parser to interpret
the two sentences as an usually long one  this led features
like those based on part of speech tags to misfire  given
more time  we would have written more code to clean the
data and do some sanity checks before sending it out for
labeling 
vii  l imitations
because the main goal of our project was relation extraction and not emotion identification  we used a very
naive system to identify the emotions in text  namely  wordmatching against a dictionary of emotion terms  we considered only unigram emotion words without context  and hence
our tool identified examples like happy family and upset
stomach  where the emotion word does not convey a state
of feeling a particular emotion  but just an adjective  happy
family  which sometimes has a semantically different meaning  upset stomach   it also identified examples where the
emotions were part of phrases or idiomatic expressions  such
as happy birthday  sad to say  im afraid that  and
just as i feared  this was only a small subset  estimated
to be       and they were labeled as no cause found
by human raters  we can probably greatly improve our
system by improving our emotion identification tool with
more contextual features 
a tricky case that we came up with is the case of
conditionals  for example  the sentence i am afraid of x 
sometimes means that x caused  me to feel  fear  e g 
spiders   but might also means that the possibility of x
happening causes fear  e g  death   this also made us
rethink our definition of causality and whether a conditional
event causes an emotion  for the purposes of this paper 
we said yes   this should be an important point to consider
in future work on causal relation extraction 

viii  c onclusion
in conclusion  we have developed a tool that performs well
at identifying the causes of an identified emotion in a multisentence paragraph  we have also shown that  given only
labels for a small fraction of the dataset  we are able to bootstrap our way using a semi supervised learning algorithm
that improves the prediction of the classifier tool  future
work would be to include other types of relations  such as
identifying the agent who is experiencing the emotion  and
in applicable cases  identifying the target of the emotion 
as mentioned  this work has many applications to more
nuanced emotion analysis of text  in addition  the tools
developed here can be generalized to other domains  for
example  the emotion identification step of our algorithm can
be generalized to event identification  and the causal relation
extraction part can be generalized to other relations  the
framework of this tool offers many promising applications
for more powerful natural language processing 
ix  acknowledgements
we thank christopher potts for sharing the corpus data
with us  noah goodman  dan jurafsky  christopher manning  and justine kao for helpful discussion  and emily yeh
for working on the emotion synset classification tool  we
also want to thank sebastian schuster  spence green  milind
ganjoo  and bharath bhat for useful advice 
x  m iscellaneous n otes
wen hao is taking cs   n and cs     and is using the
project for both of those classes as well 
desmond is not taking cs     but will be submitting the
project for cs   n 
r eferences
    chambers  n     jurafsky  d         august   unsupervised learning
of narrative schemas and their participants  in proceedings of the
joint conference of the   th annual meeting of the acl and the
 th international joint conference on natural language processing
of the afnlp  volume   volume    pp            association for
computational linguistics 
    wang  s     manning  c  d         july   baselines and bigrams 
simple  good sentiment and topic classification  in proceedings of the
  th annual meeting of the association for computational linguistics 
short papers volume    pp          association for computational
linguistics 
    socher  r   perelygin  a   wu  j  y   chuang  j   manning  c  d  
ng  a  y     potts  c          recursive deep models for semantic
compositionality over a sentiment treebank  emnlp 
    strapparava  c     mihalcea  r         march   learning to identify
emotions in text  in proceedings of the      acm symposium on
applied computing  pp              acm 
    aman  s     szpakowicz  s         january   identifying expressions
of emotion in text  in text  speech and dialogue  pp            springer
berlin heidelberg 
    zhe  x     boucouvalas  a  c         july   text to emotion engine
for real time internet communication  in proceedings of international
symposium on communication systems  networks and dsps  pp          
    subasic  p     huettner  a          affect analysis of text using fuzzy
semantic typing  fuzzy systems  ieee transactions on                
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin 
liblinear  a library for large linear classification  journal of
machine learning research                   

fi