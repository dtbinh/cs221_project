i m different  yeah i m different  classifying rap
lyrics by artist
stephanie guo and scott khamphoune

introduction
rap songs can sound radically different from one another because each rapper is unique  or at
least they claim to be  because hip hop artists and rappers often rap about their personal
stories  current challenges  and hopes or dreams  every rap song can be considered a personal
reflection of its artist 
there has been significant work in text classification and stylometry on problems such as
literary author classification and song genre classification  our project seeks to extend off
these past works to investigate author classification within a song genre  specifically in rap or
hip hop  although most humans can subjectively classify hip hop and rap songs lyrics based on
knowledge of popular artists and their backgrounds  the question of whether or not this can
be transformed into a text classification problem has not been widely explored yet 
thus  the question we seek to address is  can we design a model to predict the artist of a hip
hop or rap song based on the songs lyrics 

data collection and formatting
to narrow the scope of our investigation  our dataset will contain songs from the following
four artists  eminem  nicki minaj  kanye west and nas 
we chose these four artists because of the topics they tend to rap about tend to be distinct
from one another  eminem talks about his negligent mother  his love for his daughter  and how
his race has distinguished him in the rap world  kanye west  on the other hand  raps about a
broad range of topics  from dealing with fame and fortune to themes of race and consumerism 
nas often raps in the form of first person narratives  focusing his experiences growing up
surrounded by gang violence  drug use and poverty  finally  nicki minaj often touches on her
gender and feminism in the hip hop rap community and her relations with other female
rappers 
we obtained approximately       songs per artist  evenly distributed across each artists
album  all song lyrics were obtained from rapgenius com  a popular crowdsourcing platform
for posting  correcting  and annotating song lyrics  each song lyric was preprocessed so that
lyrics from a featuring artist and annotations indicating a repetition or hook were removed 
we formatted the text by tokenizing across whitespace and lower casing all words 
for testing  we randomly chose   songs from each artist to be used for testing and used the

firemaining songs as training examples 

artist

number of training examples

number of testing
examples

eminem

  

 

nicki minaj

  

 

kanye west

  

 

nas

  

 

total

    training examples

   testing examples

our feature vector consisted of key words and terms resembling each artist  we used two
different approaches to generate the feature vector 
 in our first approach  we used a bag of words model on our dataset to create a
frequency table for each artist  using the frequency table  we selected    of the most
frequently used words for each artist  if a word was used by multiple artists  as is
most often the case   the word would only count towards the artist who used it the
most  we called these our objective feature words  in total  we had    objective
feature words 
 in our second approach  we used our common knowledge to generate a list of
significant words frequently used by each artist  this approach was motivated by the
fact that there was a significant amount of overlap among most frequently used words
across artists in our first approach  the bag of words model also many words that
were not very descriptive  and used by all artists  such as never and dont  for each
artist  we had approximately    of their most frequently used significant words  we
called these our hand selected feature words  in total  we had    hand selected
feature words 

models used
we used the   following models  courtesy of the scikit machine learning library  
   multinomial naive bayes  naive bayes is widely regarded as the most efficient and
effective machine learning algorithm for text classification  despite its untrue
assumption of conditional independence  it is still able to perform exceptionally well
    
   support vector machine  svms work well with text classification problems because
they acknowledge particular properties of text  including high dimensional feature
spaces  few irrelevant features  and sparse instance vectors     
   decision trees  lastly  we chose decision trees because we thought it would be
interesting to compare an algorithm not traditionally used for text classification with

fitwo algorithms that are known to perform well on text classification problems  naive
bayes and svm  
for our first approach  we trained each model on the dataset using the    length objective
feature vector and observed the percent error on the testing examples  then we iteratively
removed a certain number of feature words per artist from the feature vector and observed
the resulting percent error 
for our second approach  we used forward search feature selection on the    length
hand selected feature vector and observed the subset of the    feature words that resulted
in the least percent perror 

results
best percent error for each feature vector for each model

percent error as result of feature selection of hand selected features
model

  of features after feature
selection

percent error

naive bayes

  

   

svm

  

   

decision tree

  

   

fipercent error of objective features

fidiscussion and further work
our results overall were somewhat satisfactory considering the     error of naive bayes 
implementing forward search feature selection yielded the permutation of a feature set that
performed the best and had the lowest error  feature selection  however  did not improve
the percent error of naive bayes as the subset of feature words that performed the best was
the entire feature vector  all    feature words   therefore  future work should expand the
feature vector to include either more words or more sophisticated features for naive bayes 
the svm model performed equally well for both the hand selected feature set and the
objective feature set        due to the svms ability to work well in high dimensional feature
spaces and fulfilling the need for feature selection      it follows that results for both the
hand selected feature set and the objective set were the same  as they had    and    features 
respectively  thus  feature selection did not play a significant role in improving the
performance in naive bayes and svm  which is more evidence that the feature vector should
have been expanded 
we observed that the performance of objective features followed a consistent pattern  as the
number of features were decreased steadily  the percent error would start to decrease  and
then start increasing  removing objective features steadily allowed only the most significant
words to be used as identifiers  until a threshold was hit  and then the models would start to
underfit  experiencing too much bias 

references
    zhang  harry   the optimality of naive bayes   proceedings of the flairs conference  vol    
no    
    joachims  thorsten  text categorization with support vector machines  learning with many
relevant features  springer berlin heidelberg       

fi