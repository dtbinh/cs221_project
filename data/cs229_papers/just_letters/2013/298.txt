the next generations personal file system management

iru wang
xuan yang
yi chen tsai

iruwang stanford edu
xuany stanford edu
ytsai stanford edu

abstract
the current file systems are hierarchical 
which can cause duplicate storage and cannot represent humans mind map  in this
paper  we explore the possibility of a heuristic  relational personal file system  regarding each file as a node in the graph  we implement k means  em  lda and tree bagging algorithms respectively to group the related files  in this way  we convert the current hierarchical file system to relational file
system  we compare the results of these algorithms  the error of k means  em  lda
and random forest are                      
and     respectively  among all the unsupervised learning  lda   a popular and generative model in topic modeling   gives the
best accuracy  but still does not surpass supervised learning  therefore we propose to
combine lda and tree bagging algorithm 
using the semi supervised learning to classify
the files in the future  in the end  we also
discussed the potential of combining latent
fator model with above methods to classify
a large scale of file sets  thus extending our
method from personal file system to corporate file system 

   introduction
text mining is an important application in the fields
of machine learning and natural language processing 
various topic modeling techniques have wide spread
use in text mining applications  in this paper  we implement these techniques onto personal files classifications  we attempt to gain an insight on how a personal
file system should cluster and organize files  a bagof words model is implemented  in which word order
is ignored wallach         the iterative optimization
proceedings of the    th international conference on machine learning  atlanta  georgia  usa        jmlr 
w cp volume     copyright      by the author s  

clustering algorithms have been demonstrated reasonable performance for clustering nathiya   punitha 
       a simple method would be k means clustering  which is preferred in many large scale applications brian kulis         however  since our matrices
are sparse  the error we get is         the expectation maximization  em  algorithm is also common
and effectual when clustering data  we improve our
result to an error of        latent dirichlet allocation lda  has become a standard tool in document
analysis d  m  blei   jordan         we describe the
implementation of this algorithm and get its result to
be         better than em and k means  however 
the error rate of lda is still not satisfying  so we
implement a popular supervised learning method in
classification problems  which is tree structured classification  using bootstrap aggregating to improve the
accuracy  we then compare the result of above three
algorithms with the result of tree bagging  whose performance is the best  with optimal error rate to be    
  the detailed implmentation and analysis of above
algorithms are in section    according to the above
results  it is better to use random forest classifier for
the personal file system  however  since the time complexity of random forest is relatively high  the classifier
will be very inefficient if the filesystem is very large 
in order to solve the issue with a large scale of files  we
can adopt factor analysis  so it will be unneccessary to
parse all the files and build the dense matrix  thus the
algorithm can still be running at an accepted speed
with only a little accurcy sacrificed 

   motivation
the current hierarchy file system cannot accurately
classify personal files  each file may fit into many
hierarchical locations  for example  a school project
file may belong to school  as well as project   or even
school project   causing misleading search and redundant duplicates  furthermore  a hierarchical file system does not represent how human think and interact
with files  here we explore the possibility of a heuristic  relational personal file system  by exploring the

fimachine learning final project

algorithms in text minig field  a novel file system can
be created to help human classfiy the files automatically  and each time user create a new file or download a new file  the file system can group this file to the
most suitable group according to personal perference
for user to access next time  in addtion to the above
features  the file system can also help user to manage
and clean up the messy desktop and imporve users
working efficiency on computer 

plain texts from microsoft word documents  pdf documents  microsoft outlook emails by some tools too 
c  too  a  too  b   there are     text files in our initial dataset  the dataset was labelled by the owner
before we applied the classifier to the dataset  in the
end  we compare the tag defined by user with different
algorithms results to find each algorithms error rate 

the personal file system can furthur be utilized by
companies to manage all files on the server 

first  we use online ngram tool to parse files and get
all the words and its frequency  second  we compare
words with a list of tokens  which serves as features
for files  here  we have chosen our tokens to include
only the medium frequency word stems  assuming that
words that occur too often or too rarely do not have
much classication value  examples of very frequent
tokens might be the  and  which may appear so
commonly that they are not indicative  words were
stemmed using a standard stemming algorithm  which
means that house  houses  housing may be recognized by the same token  hous  being treated as the
same word 

   related work
a similar problem with the file system classfier is spam
email problem  which has been studied by many people for decades  there are many effective methods
proposed to filter junk emails  one study is based on
a bayesian approach  by casting the problem in a decison framework  they make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters  considering domain specific features in addition to using gibbs
em algorithm on raw text of email messages m  sahami   e horvitz         in the topic modeling field 
one study is based on a hierarchical generative probabilistic model that incorporates both n gram statistics
and latent topic variables by extending a unigram topic
model to include properties of a hierachical dirichlet bigram language model wallach         we found
there are many studies about file system classifier or
the way to change from a hierarchical file system to a
relational file system  one study about file classification is predicting the properties of new files as they are
created  by exploiting the strong associations between
a files properties  names  and attributes assigned to
them m  mesnier   seltzer         however  it can
not help user manage the file system automatically
based on their relations 

   the dataset

     parsing data set

using this method  we construct a matrix with file
id as row id  token id as column id  and matrix file
id  token id  represents the number of occurance of the
correspoding token in that file 
     spliting dataset
for unsupervised learning  such as k means  em and
lda  it is unnecessary to split data into training set
and testing set  simply have all text files serve as training set  for supervised learning  such as tree bagging 
we randomly choose     of text files as training set 
and the others as testing set 
for the latent factor analysis to be adopted to corporate filesystem  we need a large scale of dataset  in
which case we would use    newsgroups  splitting data
into training and testing sets  then perform hold out
validation on them 

     inital date set

   the details of learning and results

throughout history of topic modeling  a lot of data
sets have been used for testing documents  such as the
   newsgroups dataset  however  due to our focus on
personal file system and the variety of each persons
preference  we decided to test on documents that solely
belong to one person 

     k means clustering algorithm

a user may have all kinds of files  including music 
picture  text  etc  as a starting point  we decided
to focus on classifying text based files  we extract

k means clustering is a method of vector quantization originally from signal processing  that is popular
for cluster analysis in data mining  k means clustering aims to partition n observations into k clusters in
which each observation belongs to the cluster with the
nearest mean  serving as a prototype of the cluster km  b  

fimachine learning final project

figure    em categorization results

figure    kmeans categorization results

the k means clustering algorithm is as follows  k m 
a 
  
initialize cluster centroids            k
rn randomly 



   repeat until convergence   
for every i  set
c i     argj min  x  i   j    

   

for every j  set
pm
 
 
i    c i    jx i 
j    p
m
 
i    c i    j

   

 
in the algorithm above  we set k to be    which is the
number of group we want to classifiy to  we use greedy
algorithm to find the most likely label matching  assuming that the bigger the quantity this label matches
that group  the higher the likelihood that this is the
correct group for that label  due to our matrices being
sparse  most of the files are categorized into the same
group  therefore  the error rate we get is as high as
       
in the figure    we see that the correct categorized
files account for only about     of the total files  for
each category  there are a certain amount of files that
is miscategorized to other categories  we can see that
the category for ee emails has the highest discrimination rate of        which indicates that it is the easiest
category to be discriminated 
     expectation maximization  em 
algorithm
em algorithm is an iterative method for finding maximum likelihood or maximum a posteriori  map  estimates of parameters in statistical models  where the
model depends on unobserved latent variables  the

em iteration alternates between performing an expectation  e  step  which creates a function for the
expectation of the log likelihood evaluated using the
current estimate for the parameters  and a maximization  m  step  which computes parameters maximizing the expected log likelihood found on the e step 
these parameter estimates are then used to determine
the distribution of the latent variables in the next e
step em   
we partition the data into   groups and use the same
greedy method to compare them with how the user
label them  we improved our result to an error rate
of        it is still very high due to the fact that our
matrices are sparse  and each label may have different
meanings between the user and the machine  causing
miscategorization 
in figure    we see that the correct categorized files
also account for only about     of the total files  we
can see that in this case  the category for cs    has
the highest discrimination rate of        which indicates that it is easier than other categories to be discriminated 

     latent dirichlet allocation lda 
latent dirichlet allocation lda  is a generative probabilistic model that reshapes the topic modeling community  it allows sets of observations to be explained
by unobserved groups that explain why some parts of
the data are similar  lda is a three level hierarchical bayesian model  in which each item of a collection
is modeled as a finite mixture over an underlying set
of topics  each topic is  in turn  modeled as an innite mixture over an underlying set of topic probabilities  d  m  blei   jordan       

fimachine learning final project

figure    lda categorization results

figure    tree bagging training error

the probability of a corpus is given by 
p d       

m z
y
d  

p d    

x

p zdn  d  p wdn  zdn     dd

zdn

   
after running lda  we use the highest topic probability for a file to be its category  and get the error rate
as low as         as shown in figure    if we account
for the second high topic probability to be also its category  we can reduce the error rate to        it only
decreased a little  which means that the correlation
between topics may be low  we see that the most discriminated category is once again ee emails  having
      discrimination rate  and the topmost indicative
word for this topic is staff  giving us an insight on
what this topic is mainly about 
     tree bootstrap aggregating tree
bagging 
bagging and boosting are general techniques for improving prediction rules  both are examples of what
breiman        refers to as perturb and combine
 pc  methods  for which a classification or regression method is applied to various perturbations of
the original data set  and the results are combined to
obtain a single classifier or regression model  bagging and boosting can be applied to tree based methods to increase the accuracy of the resulting predictions sutton        
we randomly choose     of files as training set  after
the decision trees learned from the features and the
labels of these     files  we use the rest     files as
the testing set  and compare the predicted result with
the user defined label  we ran it for different number
of decision tree               up to     after running
   times for each case  we get the lowest average error
rate to be      figure   and figure   are showing the
relation of trainig error and testing error for different
number of decison trees respectively  we can see that

figure    tree bagging testing error for different number
of files

the training error does not change much after we have
more than    trees  but at    trees  testing error gets
to a local minimum  this is probably because that as
the number decision trees grows to some extent  each
tree loses its correctness due to the lack of nodes in
the tree  in figure   we also see that as we increase
the number of files in the dataset  our testing error
rate may decrease largely  therefore  it is important
that we have sufficient datasets  figure   shows that
testing error may depend on how many files are in
training data 
by using tree bagging  the error rate is much tolerable  however  we need the user to label most of the files
for us  which is somewhat against our goal of machine
automatically categorizing files for users  moreover 
the time cost for tree bagging of training data with
all features is large  a typical run of    trees using    
training files and      features may take around    
seconds  this is a disadvantage compare to k means 
em and lda  which at most take tens of seconds for
a run 

   conclusion
for unsupervised learning  the algorithm that gives us
the best result is latent dirichlet allocation  lda  
which has an error rate of         for supervised

fimachine learning final project

and when the unsupervised error rate exceed certain
threshold 
   in order to reduce computation complexity  we propose to use factor analysis algorithm to learn based on
sparse matrix instead of dense one  therefore only a
small portion of the files are required to be parsed and
extra the token inside for us to classify all the files 
we can extend the personal file system to corporate
file system 
figure    error rate of different percentage of total files
serving as training data

learning  tree bagging gives us an error as low as
       but we need the user to label most of the files
for us  also  tree bagging is time consuming  therefore  it would be best if we can combine the two and
use a semi supervised method  this method can be
implemented as below 
first we use the unsupervised lda to categorize all
the files  and use the topic of highest probability
as the files label 
second as user encounters a miscategorization  we
make adjustments to our files label  we then run
tree bagging as a background application to hide
its run time 

after achieving more desired performance of our classifier  we can further design ui to make this a public
product for user to use on their computers or phones 
or for the company to use on their servers 

   acknowledgement
thanks for professor andrew ng s great lectures and
all the help from tas 

references

http   cs    stanford edu notes 
cs    notes a pdf  a 
http   en wikipedia org wiki k means 
clustering  b 

third when there is a new file to be categorized  we
use our built tree to predict which category this
file belongs to 

http   www pdfmate com pdf to text html  a 

by doing this  we have a good starting point and also
a smooth process of modifying labels as well as adding
in new files 

http   www mobileread com forums showthread 
php t         c 

   future work

brian kulis  michael i  jordan  revisiting k means 
new algorithms via bayesian nonparametrics  in
proceedings of the   th international conference on
machine learning  edinburgh  scotland  uk       

since tree bagging already produces relatively good
prediction accuracy  we plan to further reduce error
rate  computation complexity and user label workload 
there are multiple things we can do to achieve our
objectives 

http   www outlookextractor com         
convert outlook email text html  b 

d  m  blei  a  y  ng and jordan  m  i  latent dirichlet allocation  the journal of machine learning research                  

   we can increase dataset by using larger number of
files from one person  or we can train the model based
on large number of customers personal files  thus we
can learn more from users varied preferences and accordingly customize the classification 

m  mesnier  e  thereska  g  r  ganger d  ellard and
seltzer  m  file classification in self storage systems 
in proceedings of the first international conference
on autonomic computering  icac      new york 
n y       

   we can develop a system combining lda and tree
bagging  so that user do not need to label the group
for each file all the time  they only need to provide
the information at the beginning of the training phase 

m  sahami  s  dumais  d  herkerman and e horvitz 
a bayesian approach to fitering junk e mail  technical report  aaai workshop on learning for text
categorization  madison  wisconsin       

fimachine learning final project

nathiya  g  and punitha  s  c  an analytical study on
behavior of clusters using k means  em and k means
algorithm  international journal of computer science and information security             
sutton  clifton d  classification and regression trees 
bagging  and boosting  handbook of statistics     
     
wallach  hanna m  topic modeling  beyond bagof words  in proceedings of the   rd international
conference on machine learning  cambridge  uk 
     

fi