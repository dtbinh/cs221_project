application of graph clustering on scientific papers subject classification
yutian liu  zhefei yu  qi zeng
in this paper  we realize the equivalence between k means clustering and graph spectrum clustering  and implement a multi level algorithm which combines the advantage of fast computation
from k means and global maximization from spectrum clustering  then we apply this algorithm on
the scientific paper graph which is obtained out of self defined crawlers over inspirehep databse
and perform the clustering  an evaluation is performed and the reason for the imperfect clustering
is discussed 
i 

a 

introduction

the problem that we are interested in here is the clustering of scientific papers within a database  more specifically  due to the existence of citation and reference  papers are actually closely connected to each other if they
belong to a same category  topic  research field  etc   
furthermore  the concept of category really depends
on the scale we observe this network  all these network
features motivate us to think about papers as a graph
object with nodes being the paper and edge being the
citation reference relation  specifically  we want to use
state of art graph clustering technique to explore detailed
structure of scientific paper graph  aiming at subject classification that they belong to 
ii 

methods used in graph clustering

in a graph g        with vertices  and all edges  
we can define an adjacent matrix to represent it 
aij   ij

   

where ij is the weight of edge connecting vertex i and
j  notice that in general aij is not a symmetric matrix
because we dont expect the link to be directed in general 
based on this  we can then further define a link function
between two clusters 
x
links a  b   
aij
   

spectral method

to do the maximization minimization of the objective
function defined above  we still needs to do a lot of derivation to transform it into a problem of maximizing the
trace of a quadratic form  and then  from linear algebra 
we only need to perform a diagonalization of the matrix
in the middle and pick those k eigenvectors with descending eigenvalue order  due to the limitation of pages  we
have to omit detailed mathematical derivation but one
could refer to reference  will be appended in the final
project summary   specifically  we have eventually
w assoc g    maxy trace y t w     aw     y  

   

where a is adjacent matrix  w is a matrix formed by
our weight function and y is related to indicator vector
 yc  i      if i is in cluster c  
it turns out spectral method is very powerful in graph
clustering in that it is guaranteed to reach global extreme point by principle  however  since the eigenvectors
are not indicator vectors in general  we have to do some
rounding here  one big problem for the spectral method
is that it will be very expensive on a large graph simply
because of the complexity of solving eigen problems  as
we will show  though spectral method is the optimal one
when node size not too size  we have to resort to other
techniques when our graph is too large 

ia jb

which represent the connection between two subset
within a graph  and thus we can define the degree of
a subset a to be degree a    links a    where  represents all vertices 
objective function
pinp analog to minimizing
 
as we did in k means clusc
ic   pic  ic   
tering algorithm  one also needs to define a proper
objective function in graph clustering  there are many
different but quite similar definition of objective function
used in graph clustering and we can write them in a
general form called weighted association as following 
w assoc g    maxl    k

k
x
links c   c  

 c  

c  

where l is each cluster and  c    
weight of each vertex  

p

ic

   

i  i is the

b 

equivalent weighted kernel k means method

one way to tackle this problem is utilizing the equivalence between k means method and spectral clustering
which is delivered and proved by inderjit s  dhillon etc 
in our reference  the k means method first needs to be
generalized to weighted kernel k means as changing objective function to be
d 

k x
x

i    ai    mc    

   

c   ai c

p

i  ai  
  here we map our data from
ai c i
original feature space to a hyper space through function
 x   if we plug mc in and expand all quadratic form  we
where mc  

ai c

p

fics    project
will find that only inner product between  ai   remains
and thus we can simply replace it with kernel 
the proof of equivalence between weighted kernel kmeans objective function and graph clustering objective
function basically only involves several algebra tricks 
though it is not intuitive to build such connection  first
we define a n  k matrix z
zic  

i ai  c  
   

c 

multi level strategy is an approach proposed in the reference to combine advantages of speed in k means algorithm and global optimization in spectral method  this
strategy can be divided into three steps  coarsening  base
clustering and refinement 
   coarsening  first  we need to coarsen our graph
into a much smaller graph with nodes size only
about one order larger than number of clusterings 
this could be done level by level  in each level  for
some vertex x  we look for the unmarked vertex y
edge x  y 
edge x  y 
that maximize
 
and merge
 x 
 y 
them  and mark them afterward   when all points
are marked  we move to next level with merged
graph as input and them do all the iterations 

   

sc

p
where sc   ai c i   realizing that columns of z is
mutually orthogonal  we can then get that
d 

n
x

i   i   w zzit     

   

i  

where i    ai   and w is diagonal matrix with weights
on it  then we can get
d     w

   

 w

    e

y ye t    f

   base clustering  when the graph finish coarsening 
we run standard spectral clustering on it  since
graph size is small now  the spectral clustering is
much efficient 

   

where ye   w     z and   a   f is the frobenius norm of a
matrix  with the feature that   a   f   trac at a  and
some algebra  one will eventually get

   refinement  then we just follow the same path
down to original graph level by level  in each level 
we release points merged in this level and assign
them to the same cluster as merged points initially 
then we run the equivalent k means clustering on
this refined graph  since we have a very optimized
initial clustering here  the result will be much less
sensitive to local minimum 

d   trace w     t w      trace ye t w     t w     ye  
   
and thus
miny d   maxy trace ye t w     kw     ye  

    

by comparing this to the objective function defined in
graph clustering  the spectral method and k means turns
out to be equivalent by doing some proper transformation
between adjacent matrix and kernels  one subtlety here
is that a kernel transformed from adjacent matrix is not
necessarily a valid kernel  to do so  it is proposed in the
paper that we can boost our obtained kernel to be
k     w     w   aw  

multi level strategy

iii 

data acquisition

since we cannot find a prepared good enough data  especially with tags   we decide to obtian the data by ourselves through webpage crawler  the database we use is
inspirehep net which is a compiled database primarily
for papers in high energy physics  one great advantage
of using this database is its closure  most of the citation reference of each paper is also in this database 
it turns out the web page crawling is much more challenging than what we expected originally  so far we have
only been able to implement single crawler that starts
from an initial paper and crawls down the citation using
breadth first search  the maximal depth is severely restricted due to the bandwidth and time  if we sends out
too many concurrent http request  the connection lost
rate will increase a lot  or even worse  rejected by the
server   what is worse  data crawled by single crawler
will be just a tree structure rather than graph structure 
first  we start from some paper with a reasonable citations  then we look for all citation links and do a
breadth first search  during the search  paper with too
low citation         also  during the search  crawler will
collect seed papers for next round of crawling  the seedpapers should have at least     citations and a different

    

where  is a parameter that should be large enough to
make k   positive definite  further  in case a is not symat   a
and everything
metric  one can replace it with
 
is just the same under the context of taking trace  actually  as we try in our experiment  the choice of parameter
 is very subtle in that it cannot be too large in order
to avoid sensitivity to the local minimum while it should
make kernel valid at the same time 
by such equivalence  in case the graph is super large
and running it with spectral method is expensive  we can
simply transform it to a k means problem which is much
faster to compute  conversely  since k means algorithm
is sensitive to the local minimum  and initial clusters  
we can also transform a k means problem into a spectral clustering problem to achieve optimization when the
computation is affordable 
 

fics    project
tags  a standard subject classification given by database 
from initial paper of current crawling  when crawler
reaches maximal depth set by user  it will jump to a random paper within seeds collected in this round and then
do the whole crawling again starting there  such whole
procedure is repeated until number of iteration reaches
maximal value set by user or there is no seed left 
it turns out this routine works to some extend  as we
can see in the figures below  however  we can still infer
from the graphical structure that this is a very typical
tree structure in that we can see clear boundaries between
levels  furthermore  this method costs too much time in
crawling  which makes the data acquisition unrealistic
when one wants to get a larger sample of the network or
when some star paper has thousands of citations  one
way to treat this is doing a random sampling throughout
the crawling  explicitly speaking  when one is doing the
width first searching of citations  once the collection of
next level is established  rather than crawling them over 
one pick a random sub sample and only crawls that part 
this simple trick turns to be very powerful in getting a
reasonable citation graph 
figure   is a comparison between two graphics with
and without this random sampling procedure  the difference is very obvious  without random sampling  links between articles are actually pretty low  articles within one
level basically are independent with each other  the ratio between links and nodes is about        meaning this is
basically just a binary tree structure  on the other hand 
when random sampling is adopted  the link to node ratio
becomes about         which is pretty reasonable  from
the graph we can also see that the connections increase
significantly  therefore  this one is fixed as the data we
use in the following analysis  the total number of nodes
is      and total number of edges is       for test purpose  we also include the tag given by the website and
there are in total four labels for our sample  more details
will be discussed immediately below 

iv 

experiments on crawled citation
graph

in the inspirehep database  there is already a labeling system which assigns a certain category to each
paper  such classification is based on the whole understanding in physics and thus we believe it is a truth reference in our test  in the data sample we crawled  there
are in total four labels  hep ph  high energy physics
phenomenology   hep exp  high energy physics experiment   hep th  high energy physics theory    astroph  astrophysics  including cosmology  and x category standing for unknown category which represents error in the website scraping  after the data is set up  we
are able to apply our algorithms on it and see the effect
of clustering 

a 

initialization

as we discussed earlier  the multi level algorithm design is to avoid the great uncertainty brought by k means
method itself  after a multi level coarsening process and
a spectrum clustering at the most simply graph  we have
actually already obtained a roughly correct guess of the
final clustering  which make the further equivalent kmean clustering in refinement steps much more easier
and reliable  to see this effect of initialization directly 
we do such a comparison between multi level and k mean
algorithm initialized state  for k means  it is just random
assignment  for multi level algorithm  it is the state after
coarsening and base clustering  the comparison plot is
shown in figure    the graph here is the data from facebook  downloaded from snap stanford edu  because we
want to test the initialization effect independently of sample used 

fig     left  initialized state of k mean  right  initialized
state after base clustering
fig     left  sample graph without random sampling  right 
sample graph after random sampling

as we can see  in the normal k means clustering  random category assignment is pretty bad  as we will show
 

fics    project
later  such huge uncertainty results in significant unreliability of k means method  for the multi level design 
it is much more robust in that the spectrum clustering
actually have done some clustering here 

b 

assert that k means is effectively random clustering  but
it is true that  as mentioned in the initialization part 
a random initialization in k means severely degrades its
performance  especially in such a complicated graph clustering application 
the visualization of two clustering results can be found
in figure   which is consistent with statement above 

level decision

another issue in multi level algorithm is the determination of levels in coarsening  and thus refinement   in
our reference paper  a criterion of stopping coarsening is
delivered  however  when we test this on our sample  we
realize that this method might not in general apply to
any problem  initially we get    levels and this gives a
very low objective function value          therefore  we
manually change the coarsening level to           

c 

d 

comparison between multi level and spectrum
clustering

now  we turns to comparison between multi level and
spectrum clustering  the percentage distribution is
shown in table iii 
category multi level spectrum
truth
 
      
               hep ph 
 
      
               hep ex 
 
      
     
       hep th 
 
      
             astro ph 

comparison between multi level and weighted
k means clustering

now  let us take a look at the final result of clustering 
first we could make a comparison between multi level
and k means algorithm  we set the number of clustering
to be   and below table i is a chart showing the fraction of articles subscribed to each category  sorted from
highest percentage to lowest one  

table iii  percentage distribution

both multi level and spectrum clustering gives pretty
similar result to either each other or the truth tags  which
is good because spectrum clustering is guaranteed to
reache global maximization in principle  the table on
objective function is table iv 

category multi level k means random
truth
 
      
                      hep ph 
 
      
                      hep ex 
 
      
                     hep th 
 
      
                     astro ph 

multi level spectrum
objective function
     
     
table iv  objective function

table i  clustering and truth tag percentage distribution

the objective function table further confirms that
multi level and spectrum basically gives the same result 

note  there is an additional component in truth tag
which is around        this part represents unknown
category out of the flaws in crawlers  since they are very
small we will simply neglect them 
from this table  we can see that multi level algorithm
is way much better than k means in that it can better reproduce the percentage distribution in truth tags  furthermore  we put a column called random which is
generated by random clustering for further comparison 
similarly  we can also make a comparison table of objective function as shown in table ii 

objective function

e 

multi level evaluation

the canonical way of doing evaluation is the estimation of efficiency and fake rate  these could be inferred
by looking at the clustered category distribution within
one truth tags  for example  we can see the truth tag
hep exp and see how many of them is really assigned
to the correct category in clustering  similarly  we can
also do this in the inverse way  one looks at the clustering and estimate the distribution of truth tags within
each clustering  both way can give us a sense how the
clustering is doing in an equivalent way 
a summary of the distribution of truth tags within
each multi level clustering is shown in the table v 
from this table  one can see that each multi level clustering is dominant by the hep ph articles  considering
that hep ph papers are dominant in our sample  this is
not so surprising though it is definitely not good  what

multi level k means random
     
     
     

table ii  objective function

from this table  in terms of objective function maximization  multi level algorithm does the best while kmeans method is actually not that bad  at least it is
still far from random clustering  therefore  it is unfair to
 

fics    project
one of the most important reason responsible for this
is the data sample we are using  frankly speaking  the
graph that we are using is still not complete  in the sense
that it is not a good representation of the graph structure  for example  according our crawler rules  the truth
tag between different iteration is not the same  we use
this rule because we want to get a sample with diverse
possibilities  however  this apparently bias the whole
structure because we know the original structure in this
local region should be dominant by the hep ph paper
while we pick several connected local regions with different truth tag  statistically speaking  we are picking
statistical fluctuation rather than the dominant structure itself  but due to the limit on our resources  it is
impractical for us to really get a complete enough graph
sample this moment 
if we go one step further  we could infer that the clustering algorithm that we use might not be suitable for a
sample with strong statistical fluctuation  to show this 
we can take a look at the objective function out of each
clustering algorithm and the truth tag itself in the table
vi 
k means spectrum multi level truth tag
objective function      
     
     
     
table vi  summary table of objective function

its surprising to see that all clustering method actually reaches a objective function value much larger than
the one obtained from truth tag itself  this implies that
we might be choosing an inappropriate objective function whose maximum does not correspond to the actual
solution to our problem 

fig     lefttop  k means  righttop  spectrum  leftbottom  multi level  rightborrom  truthtag
multi level clustering hep ph hep exp hep th astro ph
 
   
  
   
  
 
   
  
 
 
 
   
 
 
 
 
  
  
 
 

v 

conclusion

eventually we conclude that  though the graph clustering algorithems perform well on maximizing the objective
function  they do not work very well on categorizing the
sample scientific papers  it is because the basic ratio
association cut may not be the best objective function
for this problem  also  the sample we collected is biased
by our crawling routine 

table v  summary table of clustering

is worse  for each truth tag  the dominant population also
is concentrated in multi level tag    all these means
that our clustering is not very ideal because it cannot
really discriminate between these four categories 

    n  cristianini  j  shawe taylor  and j  kandola  spectral
kernel methods for clustering  proc    th advances in
neural information processing systems       
    inderjit s  dhillon  yuqiang guan  and brian kulis 
weighted graph cuts without eigenvectors  a multilevel
approach  ieee transactions on pattern analysis and
machine intelligence  vol      no          
    h  zha  c  ding  m  gu  x  he  and h  simon  spectral

relaxation for k means clustering  proc  neural information processing systems       
    i  dhillon  y  guan  and b  kulis  kernel k means  spectral clustering and normalized cuts  proc    th acm
knowledge discovery and data mining conf   pp               

 

fi