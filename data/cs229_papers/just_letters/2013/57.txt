the recipe learner

the recipe learner
doug safreno  yongxing deng

  

introduction

cooking is difficult  particularly when not following a specific recipe  often times a cook wants to know how much of
a common ingredient  e g  salt  to throw into their mix of other ingredients  in particular  given n ingredients and
n    amounts  the cook wants to find the nth amount  the probability of getting an exact match in a database of
ingredients is low given these requirements  thus it is a good fit for a learning algorithm  we discuss several different
approaches to solve this problem below 

  

the dataset

our dataset initially consisted of        recipes  rows in our m matrix  with       ingredients  columns in our m
matrix   the value in each cell was specified as the amount in grams of that ingredient  this matrix was very
sparse due to each recipe consisting of relatively few of these ingredients  the data was crawled from allrecipes com 
which states that it has over        recipes  though our crawler only found        unique recipes   we found after
crawling that       of ingredients occurred less than    times in all recipes  making up a total of       of ingredient
occurrences in recipes   thus  we first removed all ingredients with fewer than    occurrences  and then removed recipes
that became empty  after filtering we had        recipes and       ingredients  each of which occurs in at least    recipes 
our recipes are distributed by how many ingredients they require in the chart below 

the majority of the recipes require fewer than fifteen different ingredients  verifying our claim that m is a sparse
matrix  moreover  our ten most prominent ingredients are 
rank
 
 
 
 
 
 
 
 
 
  

count
    
    
    
    
    
    
    
    
    
    

name
salt
butter  melted
white sugar
egg
all purpose flour
water
onion  halved and thickly sliced
garlic  minced
black pepper to taste
milk
 

fithe recipe learner

for the purpose of our project  we attempted to predict the amount of butter  melted  this is because the ingredient
appeared frequently and because we suspected that recipes that contained this ingredient were more similar to one
another  dessert   to accomplish this goal  we only looked at the recipes that contained the ingredient butter  melted 

  

approach    linear regression

to predict the amount of butter to put in  we first attempted a linear regression  we took away the row that represented
butter and used the other ingredients as features in this supervised learning setting  as a baseline  we computed the
average of the target variables in the training set  and predicted this value for every testing example  we divided up all
the examples into training  x   and tesing       x   where x varies from    to    
we used mean squared error to measure the result 

the plot shows that the training error and testing error converge at the baseline error as the percentage of training
examples goes to    a naive linear regression algorithm gave us a baseline result  from which we improved upon 

  

the multimodal problem

one reason why the naive linear regression approach did not work is the multimodal issue  to illustrate this problem 
consider two ingredients  chicken and vegetables  then  consider two dishes that consist of these ingredients  chicken
caesar salad and roasted chicken with mixed veggies  the main item in the former dish is the vegetables  while the
main item in the latter is chicken  this discrepancy causes a plot of chicken vs  vegetables to result in a bimodal
distribution  one mode for chicken salad dishes and one for grilled chicken dishes with veggies  it is easy to see how
this could be extended to many modes across many recipes  each mode can be considered a dish  the proportions
between ingredients within a dish are more consistent with one another 

  

approach    clustering and linear regression

to address this problem  we used a k means algorithm to cluster all the recipes  then filtered the ingredients  and then
applied linear regression to predict our target  we hoped that the clusters would correspond to dishes  and that the
linear regression would then predict the correct amount for that dish 

     clustering
we first subsetted our dataset to the       recipes that have butter  we chose to subset our data in order to improve
the running time of k means as well as to remove examples that dont have any information about butter  after
normalizing all the features     we then applied k means to the training set to produce    clusters  we use    as the k
  so

that mean is    and standard deviation is   for all features 

 

fithe recipe learner

in our k means because it seemed to produced the most subjectively accurate clustering  a few of these clusters are
shown below  displayed examples are randomly chosen from the clusters  
cluster  
white chocolate     cookies i
white chocolate     cookies
white chocolate     cookies ii
white chocolate blondies
secret chocolate fantasy cake
red velvet cake iv
macadamia     cookies
white chocolate     cookies iii
swiss white chocolate cake
    white chocolate cookies
  
 

cluster   
homemade pan rolls
mall pretzels
    bread machine rolls
    cheese bread
clone of a cinnabon
cinnamon raisin bread ii
ds whole wheat challah
best rolls ever
pams bierocks
serendipity bread
  
 

cluster   
    macaroni and cheese
    cheesecake
    seafood lasagna
lasagna spinach roll ups
    macaroni and cheese ii
spinach enchiladas
baked spaghetti  
ginas creamy     lasagna
taco queso dip
    red beans lasagna
  
 

    more 

     more 

    more 

     directly running linear regression in clusters results in overfitting
our first algorithm with linear regression is as follows 
p whole   l i n e a r   r e g r e s s i o n   w h o l e   t r a i n i n g   s e t  
for i                   k 
i f s i z e   c l u s t e r   i      min cluster size  
p i   l i n e a r   r e g r e s s i o n   c l u s t e r   i  
u s e p i t o p r e d i c t c l u s t e r   i
else  
u s e p whole t o p r e d i c t c l u s t e r   i
here  min cluster size is a constant thresold      that we determine if a cluster is meaningful enough to run
linear regression on its own  however  we found that the training error was far below the testing error  this is shown
in the plot below 

 

fithe recipe learner

as the number of training examples in each cluster increases  the testing error actually goes up  this means that
the linear regressions in clusters do not predict the amount well  clusters are effectively representing different types
of recipes  so why isnt the algorithm effective  by using clustering  we are reducing the number of training
examples  but we are not reducing the number of features  the ratio of number of features to number
of examples increases significantly  the combination of the two results in overfitting  as a result  we need
to find a way to reduce the number of features 

     some attempts at dimensionality reduction   
attempt    independent component analysis   failed  if there are two ingredients that are linearly correlated to each
other  then independent component analysis would help us remove one of them  unfortunately the dataset
does not have feature pairs like that  we suspect this is because columns like  butter  salted  and  butter 
unsalted   which would initially seem linearly dependent  are in fact most often mutually exclusive  and
thus very independent 
attempt    principle component analysis   failed  if all the examples actually lie in a subspace of rn   then principle
component analysis could reduce the dimension  after standardizing the matrix  we ran principle component
analysis 

latents  or the eigenvalues of the co variance matrix  are proportional to the variance     since the latents
are close to each other  if we only use the first few principle components  then we lose a very high
proportion of the variance of the dataset  as a result  we cannot use principle comoponent analysis to
reduce dimensionality 
attempt    cluster specific dimensionality reduction   success  since each cluster represents a different dish  the
principal features in each are different  as a result  dimensionality reduction should be done within
each cluster  with   of examples in a cluster    of ingredients  we suspect that the simplest algorithm
will work the best  for a cluster i  we only consider ci ingredients that have the highest variances within
that cluster 
but how do we decide ci   since our original reason for dimensionality reduction is to prevent overfitting 
the larger a cluster is  the more ingredients we should allow our algorithm to consider  thus  for a cluster
with ti recipes  let ci   ti  d ingredients  where d is a pre determined constant 

     putting it together
this is our final algorithm  combining the ideas of clustering  dimensionality reduction and linear regression 
p whole   l i n e a r   r e g r e s s i o n   w h o l e   t r a i n i n g   s e t  
for i                   k 
i f s i z e   c l u s t e r   i      min cluster size  
c i   s i z e   c l u s t e r   i      
  see

http   www mathworks com help stats princomp html

 

fithe recipe learner

r e d u c e d   f e a t u r e s   i   c i f e a t u r e s with h i g h e s t v a r i a n c e i n c l u s t e r   i
p i   l i n e a r   r e g r e s s i o n   r e d u c e d   f e a t u r e s   i  
u s e p i t o p r e d i c t c l u s t e r   i
else  
u s e p whole t o p r e d i c t c l u s t e r   i

  

results and conclusions

our results show that our algorithm performs noticeably better than the baseline  and thus also better than our
linear regression  it also removes the overtraining problem that we experienced before reducing the columns  as is
evidenced by the convergence of the training and testing errors 

 

fi