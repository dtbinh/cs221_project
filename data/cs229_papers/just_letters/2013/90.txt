predicting short term stock returns
chase lochmiller

yuan chen

school of engineering
stanford university

school of engineering
stanford university

abstractas the capital markets evolve and expand  more
and more data is being created daily  this explosion of data has
made the flow of information much more efficient  as market
participants act on this information flow  it drives market prices
to more efficient values    one of the driving forces in this march
to efficiency  is the application of various algorithmic learning
techniques on both market data and news sources  this paper
seeks to examine a number of different forecasting techniques to
predict future stock returns based on past returns and numerical
news indicators provide by raven pack 

i  i ntroduction
the capital markets serve as a place for investors to efficiently distribute capital to entrepreneurs and businesses so
that they can effectively provide their services to society 
in order for this process to work best  markets need to be
both liquid and efficient  this means that an investor can
buy or sell a reasonable quantity at a fair price  long term
investors are not the only participants keeping the market
working efficiently  there are many other participants that
help in the process of price discovery  thus making the market
more continuous and efficient  short term supply and demand
imbalances are what drive stock prices to fluctuate in the shortterm  which provides an opportunity for short term market
participants in two ways  first is in the event a short term
supply and demand imbalance pushes a stock out of line with
fair value  its an opportunity to profit by taking a position
contrary to the recent price movement and profiting from
reversion of the price to true fair value once an environment
of normal supply and demand resumes  in taking a position  it
also leads to balancing out the supply and demand  which also
helps drive the prices back to efficient values  strategies such
as this are called mean reversion strategies  the second is for
a short term participant to forecast such supply and demand
imbalances before they are fully realized in the marketplace 
in which case they would be able to take a position in the
direction of the imbalance and benefit from the short term
price appreciation resulting from the imbalance  strategies
such as these are typically referred to as momentum strategies
or trend following strategies  one factor that is effective in
forecasting such imbalances is the news  as news comes out 
it changes peoples set of information and hence changes the
intrinsic value in the securities  this change can often cause
one sided imbalances in the supply and demand for individual
securities  a second factor that can help short term investors
forecast supply and demand for a particular security is the
past return history of that particular security  as well as for

other related securities  for instance  if an investor observes
the price appreciating in ford motor company  it is likely
that many of the causes driving that price appreciation  i e 
consumer spending  new car sales  oil prices  etc  would also
benefit general motors  and so we would expect some related
price increase in gm stock as well  this project seeks to use
such news factors to predict the price of stocks over a two hour
window  from  pm et until  pm et when the market closes 
an effective abstraction of factors can lead to both a profitable
trading strategy  as well as pushing stock prices more in line
with fair value 
december         
ii  data
the dataset used for this investigation was from the closed
kaggle contest entitled the big data combine engineered by
battlefin      within the dataset we are given     days worth
of trading data including     training days and     testing
days on     different stocks and     different news features 
the news features are meant to be numerical reflections of sentiment and are provided by ravenpack  a company specializing
in providing such sentiment data  each day contains the stock
returns from the previous nights closing price in   minute
windows up until  pm  and the corresponding news feature
values at each point in time as well  our target variables are
the final returns at the end of the day   pm  for each stock 
these end of day returns are given to us for the training data 
but not for the testing data  and solutions must be submitted
on the kaggle website in order to determine efficacy while
testing out of sample  our goal is to build a learning model that
incorporates the features and finds the lowest mean absolute
error return from  pm to  pm compared to the actual return
from  pm to  pm 
a  feature reduction
one of the primary challenges of this forecasting problem
is selecting features to incorporate into our model  for each
trading day  we are given a time series of data points across
    instruments  each of which has    measurements in the
time series  additionally  we are given     news factors that
also have    measurements in each day  if we treat each of
our news and return observations independently  note  they
are far from being independent   we end up with a training
feature space that has     observations with        features
for all of the observed returns and        features for all
of the observed news indicator values  giving us a total of

fi       features for each of our     observations  however 
given we are working with forecasting within a time series 
the data points from our time series observations are highly
correlated  which means if we treat them independently 
our problem is highly subject to overfitting  this leaves us
with the challenge of feature reduction  for this reason  it is
very helpful to model the problem at a higher level before
attempting any learning methods 
   time series models  before trying to model and incorporate many different data points from within the time series 
we will start by looking at our problem from a higher level 
intuitively  if we are trying to predict the return of stock i at
 pm  the most meaningful observation is our most recent one 
this effectively describes the ar    auto regressive model  in
this model  we have 

and reduces our overall feature space from        features to
    features 
   clustering features  one critical observation to make
in analyzing financial time series is that stock returns tend to
be very correlated  similarly  so do news indicators  as such 
even with a significant amount of feature reduction  to one
feature per stock or news indicator  we are still very much
subject to over fitting because of the intrinsic correlations
between our features  to help address this problem  we use
a couple of different clustering and grouping techniques to
aggregate feature information 

where  is a smoothing factor  determining how fast we
should decay old information in our current estimate of fair
value 

one very tricky aspect to clustering techniques is choosing
the number of clusters to reduce your feature set into  one
technique weve employed to help us in this process is using
pca to give us a statistical picture of how much variance
is explained by the first n eigenvectors  in our application 
pca is a process that performs singular value decomposition
on our observation matrix  but before doing so  it normalizes
pn our data set so that our singular values sum up to   
i   i      in this way  the corresponding eigenvectors
represent an orthonormal basis for defining our vector space
our singular values represent the percentage of variance in our
dataset explained by those eigenvectors  looking at a plot of
this is helpful for us to estimate how many clusters we should
be reducing our data to  plotting variance explained by the
principle components of our set of stock returns  we see very
clearly  that most of the variance is explained in the first few
principle components  so we can group this into a very small
group  namely  the first   components explain     of the
variance  and the first   components explain over      for our
stock return data  this gives us an idea that we should probably
experiment with somewhere between   and   groupings for our
data  for the news indicators  the data was even more skewed 
with closet to     of the variance being explained by the first
principle component and over     being explained by the
first   components  this tells us we should experiment with
between   and   groupings 
once we have estimated the number of groups we want
to reduce our features into  we explored two different
approaches for grouping the features  the first is to use linear
combinations of the features with the weighting set based
upon the first n principle components  this method can at
times add noise  as it does not produce sparse solutions 
so even if two products are very uncorrelated and they
end up mostly being represented in two separate principle
components  you still often end up with the products being
partially represented in each component 

while we have been discussing applying these time series
models to our stock return time series  they can just as easily
be applied to the time series for our news indicators  applying
these methods gives us the most up to date representation of
our returns at  pm  and thus we can reduce each of our
individual time series into a single value  this reduces our
feature space from    values per stock or news indicator to   

the second approach we explored was in using k means
to cluster our features into n groups  the k means algorithm
can be described as follows 
   initialize cluster centroids            n  rk
   repeat until convergence   
for every i set
c i     arg minj kx  i     j k 

rt         rt    t

   

where rt is the return at time t  rt  is the return at time t  
  and   are constants and t represents our noise term with
zero mean and constant variance     for our purposes  the
  term represents a systematic drift in our return  modeling
our returns as a random walk  our expected return is zero  thus
making our systematic drift zero as well  in our application  we
experiment fitting our model above inclusive of the constant
drift term as well as the random walk hypothesis  fitting the
model 
rt     rt 
   
however  the ar    model only incorporates the last return
value for our target stock and we have        other feature
values to consider  for individual returns  we can employ
another time series model to get an aggregate estimate of fair
value at  pm  stock returns are tricky  as the observed return
value does not necessarily equate to fair value  short term
shocks in supply and demand can push the observed return
values out of line with fair value  the theory is  that if we can
more accurately represent the fair value at  pm  it will be a
more accurate predictor for the return and fair value at  pm 
one such model to help smooth shocks in supply and demand
is the method of exponential smoothing  in this model  we can
represent our estimate of fair value at time t  f vt   based upon
the returns to be 
f vt   rt         f vt 

   

fiiii  m ethods
we employed a number of different forecasting methods to
this data set  as part of a trial and error process to find the best
forecasting method  the details of the methods are described
below and the results are discussed in the next section 
a  baseline predictions

fig     plot of the stock return variance explained by principle components

to get a couple of baseline predictions  we look at the
problem in two ways  the first is to use the random walk
hypothesis discussed earlier  which says that our expected
return is zero and our mle for our return at  pm is simply the
same as the return at  pm  remember  these are all relative
to the previous nights closing price  so our effective predicted
return from  pm to  pm is zero  i e  were saying the stock
price will not change over the next   hours   this gives us
training mean absolute error of        and test error of
         the second is to predict that the stock price return
will be the average of the returns for the day  this predictor is
predicated on the notion that stock prices tend to revert to the
average  and is the mle if we were to assume that the time
series values were not serial  using this baseline prediction
our training mean absolute error is        and our test error
is         
b  linear regression
linear regression is one of the most standard tool for predictive modeling because it is stable and simple to implement 
we used linear regression in a number of different ways 
namely regressing various feature sets to try to predict the
future return  with our naive predictor of the last return  we
actually found this to be a very difficult benchmark to beat 
one effective method was instead of trying to predict the future
return  predict the change in return from the last return and
then add back the last return value to get your predicted return 
so our regression equations become 

fig     plot of the news indicator variance explained by first    principle
components

for each j pset
m
 i 
 j x i 
i     c
 j     p
m
  c i   j 
 

i  

k means is a simple algorithm to aggregate features  and it
produces sparse solutions  thus if we have two wholly uncorrelated features  each feature will only be represented in one
cluster  which helps to filter out noise that can be introduced
to our analysis  we will discuss further the application of kmeans versus pca in the results section 
   further feature reduction  one final method we employed to attempt to reduce our feature space was to take the
correlation matrix between all our stocks and news indicators 
and set a cutoff for minimum correlation to be included in
the regression  this is one blunt way to cut the feature space
down 

   r pm  r pm   t x

   

linear regression is an effective technique  but it has its
shortcomings  one that affects this problem most directly is
that if you are solving a linear regression problem using the
normal equations  and there are many more features than
observations  the observation matrix becomes singular  we
have approached this by trying a number of different feature
reduction techniques as described above 
c  poisson regression
the poisson regression is another member of the family of
generalized linear models with linear regression  it is typically
used to predict things counts of random variables  like counts
of customers that come into a store  in our case  log of returns
has a number of different attractive properties that make this
regression particularly interesting  just like counts of random
variables  returns of stocks have a lower bound  i e  we could
never have a return of less than        just as we could never
have a negative customer enter our store   given this however

fiis not true of the log of the return  one common assumption is
for prices to be log normally distributed instead of normally
distributed  for the poisson regression  regression equation
becomes
log     r pm     t x

table i
p rediction r esults k ey
p redictionalgorithm
multinomial nb
random normal distribution

accuracy
       
       

   

d  softmax
finally  we experimented with implementing a family
of classification algorithms in predicting our stock returns 
namely we used softmax  it is very interesting to see the
application of classification algorithms performance in the
time series domain where regression plays a dominant role 
there are   questions need to be answered when applying
the classification algorithms  first we must frame the problem
properly  to apply classification algorithm to a continuous
time series problem  we need to discretize the problem space 
to make the prediction tractable  we would like to perform
discretization on high probable space  given our random walk
hypothesis  our mle for the stock return is the most recent
return  so we chose to frame the problem as prediction of
the stock return at     pm relative to the stock return at  pm 
to apply classification algorithms  we need to generate class
labels  initially we define  bps as   label  for example  if the
return at  p m  is       and we are going to have     labels 
then our prediction of the return at     pm ranges from     
       bps  which catches most of the return variation in the
training data 
second  we must define the feature space  we picked two
feature spaces to study the correlation between the features
and the end of the day price  the first feature space choice is
the provided by the     news features at  pm  the reason why
we only pick the news features at  pm and dont include the
news feature before it is because we believe in efficient market
theory in the weak form  first of all  the news features at  pm 
should have included all the information and impact of the
news features in the previous time like     pm  so including
the past features would be redundant  secondly  we hope the
market efficiency is weak such that the market at      p m  still
reaction to the news feature at  pm  the second feature space
choice is the return sampled by   minute interval ranging from
the open to  pm  giving us a total of    features 
third  and finally  we must prove that classification algorithms
are applicable  to test this hypothesis  we performed a control
group experiment by applying multinomial nave bayes to
classify     labels and random number generator to generate
    labels according to normal distribution with return at  pm
as mean  we tested     stocks on     days and the results
show that classification algorithms has strong prediction capability and proves that classification algorithms are applicable
to tackle time series problem  our results are shown in table
i 
we switched to softmax as our major classification methods
due to its robustness  the cost function of softmax with
weighted decay to tame over fitting is defined as follows 

fig     plot of mae from softmax on the news indicators with reduced
feature space



m x
k
n
o
jt x i 
x
 
e
    
j      
  y  i    j log pk
lt x i 
m i   j  
l   e
k

 

n

 xx  

   
  i   j   ij

to minimize this cost function we used a package in scipy 
l bfgs to minimize the cost function 
iv  r esults
our general approach in producing results was trying a
whole bunch of different combinations of fitting techniques
with various sets and subsets of features by employing numerous feature reduction techniques  implementing softmax
on the news indicator space  we tried to find a more efficient
solution by reducing the feature space using pca  but the
results actually got worse as seen in fig     this speaks more
broadly to the news indicators having very little predictive
power  so in our case  the issue was not with overfitting
our news indicators  it was just in the fact that they did not
have a lot of information to predict upon  additionally  using
our softmax implementation  we analyzed various numbers of
return features to include for each stock  by analyzing  fig 
  we can conclude that the prediction of the stock return at
    pm can be distorted by including the stock returns in the
morning  as using more and more recent stock returns  the
mae consistently reduces 

fitable ii
p rediction r esults

fig    
space

plot of mae from softmax on the returns with reduced feature

for softmax specifically  to further reduce mae  we investigated another factor that may impact the prediction accuracy 
just as we may be overfitting with the features  we may
be discretizing the prediction space with too fine granularity 
what we discovered is that if we attach   label to  bps change 
we have     labels  and with only     days training days
for each stock  most of the labels are never exercised during
the training  it is counterintuitive  but when we increase the
descretization granularity  we actually improved our accuracy 
in addition  we also decreased our prediction range from       bps to   bps to ensure all our labels get hit during the
training and we use  bps per label with    labels to achieved
         the best result we have for the classification methods
to this continuous time series problem 
the table below shows some of our results for various
combinations of features and regression methods  and there
were many more combinations that were attempted and are
not recorded here  there were a few notable results and
conclusions that we came to  the news factors as a whole
tended to have very little predictive power  in all cases  the
results tended to be better without the news factors than
with the news factors  as we have no insight into how they
are being produced  a likely conclusion is that either the
market has already absorbed all of the information being
transmitted in the news analytics  or the news information
affects the market over a different time horizon compared to
our prediction time horizon  a second very notable outcome
was that pca tended to add more noise in the feature reduction
process than k means  and generally speaking k means was
a more effective factor reduction technique  a third point that
became very clear as we experimented with more and more
approaches was that predicting the change in return over the
final   hours of the day  turned out to be a much more effective
way to produce our forecasts  one final point that is very

strategy
random walk baseline
average return baseline
ar   
ema returns
lr ema returns
lr news  stock k means clusters w ema
lr news  stock k means clusters 
stock return w ema
lr delta stock k means clusters 
last stock return
lr delta stock pca clusters  last
stock return
pr delta stock k means clusters 
last stock return
pr delta news  stock k means clusters 
last stock return
pr delta stock k means clusters  stock
return w ema
pr delta stock pca clusters  last
stock return
lr news  stock correlation cutoff
lr stock correlation cutoff
sm     news features    pm
sm    return features for each stock
sm last    return features for each stock

mae train
      
      
       
        
       
       
       

mae test
       
       
       
       
       
       
       

       

       

       

       

       

       

       

       

       

       

       

       

       
       
       
       
       

       
       
       
       
       

table iii
p rediction r esults k ey
abbreviation
lr
pr
sm
delta

what it means
linear regression
poisson regression
soft max
regress on the change in return from  pm to  pm

notable  is that the market is really efficient and beating our
most naive prediction of the last return value turned out to be
really challenging  using all of these advanced data analysis
techniques  only a handful were able to outperform this very
naive approach  our most effective approaches were to use
linear regression on   stock return clusters that were clustered
using k means to estimate the delta between  pm and  pm 
we used both linear regression and poisson regression  and the
poisson regression outperformed slightly in test   bps   as far
as a benchmark goes  the linear regression technique would
have put us into   th place in the competition and the poisson
regression technique would have put us into   th place 
v  c onclusion
the stock market is a very efficient environment where
information is reflected very rapidly in prices  however  there
are still ways to process information and produce forecasts that
can add value and create more efficient predictions of prices
than the current market values 
r eferences

    https   www kaggle com c battlefin s big data combine forecastingchallenge
    http   cs    stanford edu materials html
    http   mdp toolkit sourceforge net 
    http   scikit learn org stable modules generated sklearn naive bayes multinomialnb html
    http   ufldl stanford edu wiki index php softmax regression

fi