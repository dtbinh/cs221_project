hapbeer  a beer recommendation engine
cs     fall      final project
bryan offutt
december         
  abstract
this project looks to apply machine learning
techniques in the area of beer recommendation and
style prediction  the first portion of the project deals
with recommendations  with the recent boom in the
popularity of craft beers  it has become increasingly
difficult to navigate a liquor or grocery store in order to
discover new beers that one might enjoy out of the
hundreds of options  with so many gimmicky names
and brightly colored labels  it has become tough to
discern the good from the bad  often resulting in wasted
money and a dissatisfied drinker  this project looks to
ease this process by providing users with personalized
beer recommendations based off of their personal
preferences 
the second portion of this project is aimed at
helping home brewers rather than general consumers 
when making your own brews  people often ask you
what kind of beer is it   a question which can be
difficult to answer  the style prediction portion of the
project classifies a beer into one of thirteen categories
based off of the beers alcohol by volume  international
bitterness units  ibus  and ingredients with the hopes
of giving home brewers a quick and easy way to
automatically classify their creations 

 
   

recommendation engine
data collection

beer were based off of four pieces of information 
alcohol by volume  international bitterness units 
category  and style  each style belongs to a particular
category and contains a subset of the beers in that
category  for example  the category north american
origin ales contains styles such as american style
india pale ale and american style strong pale ale 
while including both category and style adds some
redundancy to the feature vector  this redundancy was
seen as justified because style is usually a very
important factor in a persons beer preference  these
four features were represented by a binary feature
vector of length      where each of the     styles and
   categories were given a binary feature  and ibus
and alcohol by volume were discretized into features
based on ranges    ibu         ibu      etc   
user data was obtained via a simple online
survey that asked participants to list beers that they did
and did not like  the survey received    responses 
however  of these    responses only    of them listed
enough beers to comprise what was determined to be a
sufficiently large dataset     or more beers   this
subset of    users was used in the training and testing
of the following algorithms 

   

likes dislikes classifier

the first step in the beer recommendation process was
to create a classifier that could predict
whether a given user would or would not like a given
beer based off of the users past likes and dislikes  in
order to solve this task i implemented both a nave
bayes classifier and a support vector machine  svm  
error for each user was calculated by running leave
one out cross validation on each model separately 
once cross validation had been run on every user in the
system  the average of the individual user errors was
calculated in order to give an overall classification
error for each model  by looking at the average
classification error per user rather than the overall
classification error we hope to create a system that
performs well for all users 

the data used in this project came from an open source
online beer database called brewerydb     
brewerydb provides a simple api that allows one to
obtain a wide variety information about a large number
of beers  this information includes things such as
international bitterness units  alcohol by volume  the
brewery the beer comes from  photos of the beers label 
and much more  the database includes information for
       beers  but only the       beers that included
ibu information were used in the recommendation
portion of this project 
once the beer information was obtained  the     nave bayes results
next step was to transform this information in order to
create a feature vector for each beer  the features for a my first attempt at a likes dislikes classifier utilized the

finave bayes algorithm using the multi variate bias value
bernoulli event model  this initial implementation
followed the traditional nave bayes paradigm of  
calculating the following two values 
p   likes   beer   p   beer   likes  p  likes 
p   dislikes   beer   p   beer   dislikes  p   dislikes 

classifications were then made based off of which of
these two probabilities was larger  this particular
implementation also utilized laplace smoothing with a
smoothing value of       nave bayes is known to
work surprisingly well for small data sets  and since we
were working with limited user data  it seemed a
natural fit 
while this model worked relatively well  see
figure     i swiftly realized that this was not an ideal
system to solve the problem of beer recommendation 
when we think about the problem of recommending
something  we realize that we are not really worried
about classification error  but rather what can be
thought of as recommendation error 
recommendation error  

if      beer   likes      likes    bias   
 
   beer   dislikes      dislikes    
classification   dislikes

classification   likes
 
where     biasterm   

recom 
error

     

     

  

     

     

  

     

     

   

     

     

    

     

     

     

     

     

      

     

     

       

     

     

        

     

     

figure    classification and recommendation error
 of loocv  for assorted bias terms

false positives
total classifications

recommendation error gives us the probability that we
would recommend someone a beer that they would
dislike  limiting this number is really the goal of
recommendation  even if it means potentially not
recommending someone a beer that they may have
actually liked  the logic behind this is that if we do not
recommend a beer that a the user would have liked 
they will never know the difference  conversely 
recommending a user something they end up hating
may leave them saying this recommendation thing
doesn t work right  
with this in mind it became clear that i needed
air on the side of caution and push the classifier
towards making negative  dislike  classifications  in
order to accomplish this  i added a bias term to my
estimation of p  likes   beer   the classification logic
now looked like this 

 
else  

average user
classification error

figure  

while the addition of this bias term raised classification
error and false negatives  as expected  see figures   
    it considerably lowered the classifiers performance
in terms of recommendation error  furthermore  by
picking a bias term of around       you can obtain the
advantages of this great recommendation error while
still maintaining solid classification performance 

   b support vector machine
the svm portion of this project was implemented
using a combination of the liblinear     and
libsvm     matlab libraries  after trying a variety
of kernels  i found that a linear kernel was the most
effective on the training data  since we have a large

finumber of features and a very small number of training
examples  it makes sense that we would not want to
map our features into an even higher dimensional
space 

  

nb  no bias 

svm  lin  kernel 

  

average user class 
error

     

     

  

recom  error

     

     

  

figure    svm and nb errors

feature vectors corresponding to beers the user
likes 
divide the resulting vector by the total number
of beers the user identified as a like  creating
that users average beer  or a user centroid 
identify which of the    k means centroids this
user centroid was closest to 
choose a random beer from the list of beers
that are assigned to the centroid you found in
part   
run the likes dislikes classifier on this beer  if
it is determined the user will like this beer 
recommend it  otherwise repeat step   

by first using k means to cluster beers and then
recommending beers from the cluster which our user
centroid is assigned to  we assure that the beers we run
through our likes dislikes classifier are similar to the
users past preferences  this saves us the time of
running our classifier on beers that are very different
from the users preferences  and thus likely to receive a
negative
classification  
while
simultaneously
providing our recommendation with an additional layer
of assurance 

   

figure    graph of svm vs nb errors

from the results we see that the svm does a slightly
better job properly classifying a beer as a like or
dislike than the nave bayes classifier does  given
the simplistic nature of nave bayes  this is not a very
surprising result  however  it is worth noting that with
the addition of the bias term nave bayes is able to
significantly outperform the svm in terms of
recommendation error 

   

final recommendation

once the the likes dislikes classifier had been trained
and tuned on the training data  it became time to
actually make a recommendation  this process
followed the following steps 
   run k means using all of the beers in the
database in order to identify clusters of beers
that have similar features   ten centroids were
used  resulting in    distinct beer clusters  
   create a vector that is the sum of all of the

discussion

overall  i think the recommendation engine performed
fairly well  the cross validation error that we obtained
for the likes dislikes engine was surprisingly low 
even when using a relatively simple algorithm such as
nave bayes  in addition  after running the full
recommendation system for a number of friends  it
seems that this too performs fairly well  however  this
is tough to test quantitatively  since the odds of the
person having had the recommended beer is relatively
small 
with that said  this recommendation system is
far from perfect  while the average classification error
and recommendation error was relatively low  there
were still a few users on whom the system performed
rather poorly  after examining the data  i determined
that the system performs much better  as you d expect 
on people who s likes and dislikes have very distinct
features  users who s likes contained a variety of
different beer types tended to do much worse in terms
of classification error  another very common issue was
that a lot of users had two beers that were incredibly
similar feature wise but opposites in terms of
classification  for example  some users would list bud
light as a like and coors light as a dislike when  from
an objective perspective  these two beers are almost
identical  having two nearly identical beers with

fiopposite training labels can make things very difficult
on the learning algorithm 
furthermore  because the svm performed
better on classification and the nave bayes classifier
with a bias term performed better on recommendation 
we are faced with the question which one is better  
in the long run it really depends on how adventurous
you are  take the safe nave bayes route and
potentially miss out on great beers  or take the svm
route and potentially get some beers you don t like 
while they both have their advantageous  i decided that
nave bayes was the better option in this situation
because it makes it incredibly easy to obtain high
quality recommendations  which is the end goal of this
project 

 

style prediction

   

data collection

similar to the recommendation portion of this project 
the style prediction portion also utilized data from
brewerydb      however  this portion used only the
    beers that had ingredient information in the
database  the information about these beers was
broken up into a binary feature vector based on alcohol
by volume  ibus  and ingredients  there are a number
of ingredients listed in the database that are not actually
in any of the beers  and these ingredients were left out
in order to decrease the size of the feature vector 
because these ingredients are not in any of the    
beers in the training data  these features would always
be zero  and thus would not add any real value to our
system 
in order to make the features in our vector
binary  the alcohol by volume and ibu features were
discretized by range just like in the recommendation
feature vector  additionally  each ingredient type
 admiral hops  amarillo hops  etc   was given it s own
binary feature which was   if that ingredient was
present and   otherwise  the resulting vector had a
total of     features      of these features were
ingredients  while the remaining    were the
discretized features for abv and ibus 

    method
in order to solve the problem of style prediction  i
utilized a multi class nave bayes classifier  this
classifier worked in the same way as the one used in
the likes dislikes classifier  except it chose from   
different categories  figure    rather than simply  
 like or dislike   the category with the highest

likelihood as determined by the classifier was selected
as the prediction 
british origin ales

irish origin ales

north american origin
ales

german origin ales

belgian and french origin
ales

international ale styles

european germanic lager

north american lager

other lager

international stylrs

hybrid mixed

mead  cider  and perry

other origin
figure    possible styles

to control for unseen features  laplace add one
smoothing was initially implemented  this initial
algorithm performed very poorly  and i swiftly realized
it was due to this smoothing value  because each
category has only a small number of training examples
with it s respective labeling  the number of beers in this
category that have a given feature is fairly low for the
majority of features  in addition  due to the size of the
feature vector  a large number of features are never
seen in a beer with this labeling  in this case add one
smoothing places too much value on these never before
seen features  giving them almost as much weight as
features that have been seen  though only a small
number of times  thus it became apparent that a
smaller smoothing value was in order  in order to solve
this i ran the algorithm using a number of different
smoothing values and chose the best one using      
cross validation   figure   

    results
considering the large number of categories from which
the algorithm had to choose from and the simplistic
nature of nave bayes  it is not surprising that the
classifier struggled  on the held out validation set  the
classifier was able to correctly guess the category in
one guess        of the time  in two guesses       
of the time and in three guesses        of the time
 figure     the drastic increase in accuracy with each
additional guess is likely due in part to the fact that
    of the training examples were north american
ales  thus  the prior probability of north american
ale was very high while the prior of the rest of the
categories was relatively low  as a result  north
american ale was very frequently  and often
incorrectly  the classifiers first guess  while the correct

ficategory was the second or third guess  another
potential source of error is the fact that some groups of
categories  such as north american origin ales and
irish origin ales  have relatively similar
characteristics  which can make it difficult for the
learning algorithm to distinguish between the two 

another great way to improve the
recommendation system would be to add information
about the users themselves into the recommendation 
one piece of information that i believe would be
especially useful is the users geographic location 
looking through the data  and considering my own
preferences  it became apparent that people have a lot
of hometown pride when it comes to beers  the quality
of your home city s breweries is a point of pride for a
lot of beer drinkers  and their likes often reflect this 
finally  there is one key feature i would add to our
feature vector  price  price seemed to be a huge
factor in peoples likes and dislikes  with cheaper beers
tending to end up in the latter group  this feature could
be a small but powerful addition to our
recommendation system 
in addition to recommendation improvements 
there are also a number of improvements that could be
made to style prediction  while my nave bayes
system was able to at least give a general idea of what
category a beer would be in  the results were certainly
not up to what you would want in a deployable system 
trying a new  more complex model such as soft max
regression may very well yield better results  in
addition  obtaining a larger training data set with more
figure    style prediction held out error using a variety of
diverse training labels would be a great help  as i said
smoothing values 
earlier  the majority of the beers  with ingredients 
from brewerydb were north american ale s  this is
  conclusions and future work
likely a result of the fact that brewery db is an
american based website  and thus favors american
while the beer recommendation and style prediction craft brews  many of which are some sort of ale  it
engines described in this paper are a good start  there is would be nice to have a bit more variety in our data 
still much that could be done to improve them  the
first and most obvious improvement would be to obtain
more training data  particularly in the user department    acknowledgements
while it was great to have    people respond to the
survey  it was a real shame that so few of them    
brewery
db  
                
web 
included enough beers to comprise a decent data set     http   www brewerydb com 
users is simply not enough to be sure that we have a
recommendation system that is reliable  having a     r  e  fan  k  w  chang  c  j  hsieh  x  r  wang 
larger number of users would be incredibly helpful in and c  j  lin  liblinear  a library for large linear
assuring the system performs as well as possible on a classification  journal of machine learning research
        
          
software
available
at
variety of different preference types 
http   www csie ntu edu tw  cjlin liblinear
in addition to simply obtaining more user data 
it would be great to have a more varied group of users 
    chih chung chang and chih jen lin  libsvm   a
the majority of respondents to my survey were       library for support vector machines  acm transactions
year old white males  which is not representative of on intelligent systems and technology                
society as a whole  data on users of different genders       
software
available
at
ages  and ethnic backgrounds would help paint a more http   www csie ntu edu tw  cjlin libsvm
holistic picture 

fi