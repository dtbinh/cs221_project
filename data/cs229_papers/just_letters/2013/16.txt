scalable deep learning for image classification with k means and svm
alexandre vilcek
 vilcek gmail com 
abstract  deep learning has got a lot of attention recently in the specialized machine learning community
and also in common media  the latter mainly due to research activities of large technology companies  a
large part of that research is applied to image classification tasks  and is based on techniques such as deep
belief networks and convolutional neural networks  although those techniques yield state of the art
results  they are known to be hard to tune and scale and  as of today  there is very few software packages
available to allow for a simple and quick implementation  in this report i will show an implementation of a
deep learning framework based on recent research          proposing k means as the algorithm for the
unsupervised learning step  i will also show some techniques that allows for implementation of scalable
versions of the proposed framework 

  

introduction

the proposed framework will be implemented in r  the r project for statistical computing  running on a
standard linux desktop  specifically  for the unsupervised feature learning task  i will implement the
spherical k means algorithm in r  porting it from matlab code used in      for the image classification task  i
will use an svm implementation in r provided by     
to test the implementation  i will use the mnist dataset      the goal is to learn a new feature
representation for the images that is suitable for a linear classification task  in the standard pixel
representation a good classification is possible  but demands a much more complex non linear classifier 
such as a deep neural network or an svm with a non linear kernel  a non linear classifier turns out to be
harder to implement in a scalable way 
i will present this work as follows  in chapter   i depict an overview of the proposed architecture  showing its
main processing steps and how they are composed to build a complete deep learning framework  in
chapter   i present results obtained by applying this framework for a handwritten digits classification task
using the mnist data set  this data set was chosen due to its low dimensionality and large number of data
examples  also  it is extensively used in image recognition benchmarks  which helps when comparing this
solution to alternatives  in chapter   i discuss possible implementations that allows this proposed framework
to scale and be suitable for very large datasets  finally  in chapter   i conclude this report and suggest
further investigation 

  

architecture overview

the architecture for the proposed framework is shown in figure    it is implemented according to the
following steps 
   pre processing of the data set 
here we need to perform brightness and contrast normalization  as well as whitening  so that all the feature
remapping processing provide good results  i followed the corresponding procedures as described in     
   extracting random patches from the data set 
these patches  extracted from the original data set  are the input for the unsupervised learning procedure
 next step   again  i followed the suggestions presented in     and     for the necessary amount of patches
and the patch size  in my setup  i extracted          x  random patches from          x   images from
the training data set  each one of those are an image of a handwritten digit    to    



due to lack of computational resources available  i implement only a single layer of the framework

fi   learning a dictionary of features 
here we perform the unsupervised learning step through k means  which attempts to learn a dictionary of
image features that will be further used to build a new representation from the original dataset  the setup
for this step followed the suggestions from     and      concerning k means implementation  an
implementation known as spherical k means in this case   necessary number of clusters and processing
steps  my initial intention was to use an of the shelf implementation of the spherical k means algorithm 
such as      but it turned out to demand too much memory  surpassing what i had available for the tests  so i
decided to port to r an implementation in matlab provided by      which performed much better with
limited memory resources  for this setup i run it with       centroids  figure   shows two learned feature
dictionaries  on the left  we see the dictionary learned without pre processing of the data set  clearly we see
that it fails in learning good stokes representations of the handwritten digits and it is full of empty
clusters  on the other hand  the image on the right shows another dictionary  now learned after
pre processing of the data set as described earlier and yielding much better results much more similar to
actual pen strokes with clear edges 
   mapping the original features  pixels  of the dataset into a new feature space  which is a
function of the learned dictionary 
now we compute a new feature representation of the original data set  which will hopefully turn it linearly
separable  as suggested in      i implemented the soft threshold encoding scheme for k means  in this step 
each original    x    pixel image  represented in a   x     vector  is now encoded in a   x      vector 
   optionally repeat steps   to    if more than one layer of feature representation is desirable 
in this experiment  due to limited computational resources available  i tested only a new feature
representation in one layer 
   classify the data set  now represented in the new learned feature space  using standard linear
classification algorithms 
in this experiment  i tested it with the svm implementation provided by      i performed    fold
cross validation to choose the c parameter  which turned out to be c    the best choice 

fig     overview of the proposed architecture for the framework implementation

fifig           centroids learned from          x  pixel patches  left  no pre processing  right 
brightness and contrast normalization plus whitening

  

tests and results

the mnist database of handwritten digits  used to test this framework  is composed by        labeled
training images and        labeled test images  k means was tested using          x  patches extracted
from all available training data set  the encoding and classification steps used     of both training and test
sets  drawn randomly from their respective complete sets 
after running an svm classifier using a standard linear kernel  i got the results depicted by the following
confusion matrix 
truelabels
prediction          
             
             
             
             
             
             
             
             
             
             

this result corresponds to a test error rate of        which is better than all results published for   or   layer
neural networks trained without cross entropy loss function and without deskewing pre processing  it
suggests that  if training the system with      of the data set  and applying deskewing to it  it could
improve significantly 
the complete source code in r is available in     

fi  

scalable implementations

the implementation presented here is suitable for running in a commodity desktop computer in relative few
time  few hours  for relative small dimensional data        to       pixels per image  and about        to
        images  it was not possible  due to lack of enough computational resources  to apply this framework
to more complex images  such as those used in     
in that case  a scalable implementation would be necessary  a possible approach for implement one would
be to use a scale out processing framework  such as hadoop      there are several ways to implement it on
top of hadoop  all using the mapreduce      programming model  an option for such an implementation in
r would be to use the hadoop streaming framework  or several available products that allows for writing
map and reduce function in r code 
there are several processing blocks that would benefit from the distributed  parallel processing of such an
implementation 
   data pre processing  for both brightness and contrast normalization as well as whitening  the
parallelization is straightforward  a map only hadoop job can split the data matrix in several chunks  each
chunk being processed independently and in parallel with each other 
   patch extraction  this is also straightforward  again  a map only hadoop job taking care of several
independent chunks of the data matrix in parallel 
   k means processing  there is a classic and also straightforward k means algorithm expressed in
mapreduce  in the map step  perform the centroid assignment to the data set  and in the reduce step 
compute new centroids and check for convergence 
   feature encoding  again  an straightforward implementation through a map only job that processes
several chunks of the data matrix in parallel 
   liner classifier  an alternative to svm which is easier to implement in mapreduce is logistic regression
through gradient descent  in this implementation  several partial gradients would be computed in parallel 
for independent chunks  mini batch approach  of the data matrix  then  a reduce process would aggregate
those partial gradients for the updating of the parameters 

  

conclusion and further work

in this work  i ve shown an working implementation  in r  of the framework proposed in     and     to
construct a deep learning architecture for classification  as future work  it would be interesting to
implement an scalable version to run on top of a hadoop cluster that could be expanded to two or three
layers of features  such a framework could then be applied for more complex image classification tasks  such
as     

references
    coates  a   ng  a y   learning feature representations with k means  g  montavon  g  b  orr  k  r  m uller  eds    neural
networks  tricks of the trade   nd edn  springer lncs           
    coates  a   ng  a y   selecting receptive fields in deep networks  in  advances in
neural information processing systems     pp                 

fi    coates  a   lee  h   ng  a y   an analysis of single layer networks in unsupervised
feature learning  in    th international conference on ai and statistics  pp     
          
    the mnist database of handwritten digits  yann lecun  courant institute  nyu  corinna cortes  google labs  new york 
christopher j c  burges  microsoft research  redmond
http   yann lecun com exdb mnist 
    http   cran stat unipd it web packages e     vignettes svmdoc pdf
    kurt hornik  ingo feinerer  martin kober  christian buchta         spherical k means clustering  journal of statistical
software                url http   www jstatsoft org v   i    
    https   github com vilcek image classification
    http   www kaggle com c dogs vs cats
    http   hadoop apache org 
     mapreduce  simplified data processing on large clusters  by jeffrey dean and sanjay ghemawat  from google
research

fi