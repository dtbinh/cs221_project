discovering evergreen content on the web
ushnish de  xiaotong suo mcclure  konstanin stulov

abstractin this work we explore a dataset of websites
which have been classified as evergreen  interesting in the
long run  or ephemeral based on the content of the page 
the objective is to build a classifier which focusses on key
attributes of the content on a page to determine whether
users would have rated it one way or the other  we used
various algorithms which were implemented on different
representations of the text  and while we attained up to
    accuracy in some cases  we could not exceed this
percentage by any single method  we believe that the
accuracy can be improved by more careful preprocessing
of the raw data and applying an optimal combination of
classification algorithms as an ensemble 

i  i ntroduction
stumbleupon is a user curated website that aggregates
content which users have up voted as interesting or
engaging  while some popular content is considered
ephemeral or interesting for only a temporary amount
of time  like seasonal recipes or news articles  other
content can be considered evergreen or interesting in
the long run  such as advice forums or how to websites 
while users themselves can flag content as evergreen 
stumbleupon is interested in knowing ahead of time
which websites are likely to be evergreen and show these
prominently  the main aim of this project is to apply
techniques of machine learning to build a classifier
that is best capable of predicting how users would label
the websites as evergreen or ephemeral  in the process 
we want to determine which attributes of a website
have the greatest significance in whether it is considered
evergreen by users 
ii  background
the data set consists of the raw html content
scraped from approximately        websites  as well as
a  tsv text file with information aggregated from these
websites  possibly using an html parser  there are
essentially two types of fields in the aggregated data set
that we can use to determine what evergreen websites
have in common   text fields and numeric fields  at
the start  we tried experimenting with both the numeric
and the text fields by applying classification algorithms
 e g  naive bayes  decision trees  etc    however  due

to a high number of missing values and the presence of
clearly wrong entries in the numeric fields  we decided
to mainly use the boilerplate text only  the boilerplate
text field consists of the title and body text of the
website in a json string format  in other words  it looks
like  title easy coconut rice recipe body body
text  
iii  p re  p rocessing s teps
   tokenization  cut sequence of characters into word
tokens 
   normalization  transform different formats of different words into the same word 
   stemming  match words from the same root 
   stop words  remove common words such as a 
and the 
iv  f eature r epresentation
the first task in text classification is to transform the
document to a representation on which we can perform
classification algorithms  the format used for text representation can have an impact on the effectiveness of
these algorithms  hence  care must be taken in choosing
an appropriate feature representation 
the vector space model is a widely used document
representation model     in this model  every document
is represented as a vector and each entry of the vector 
denoted wt d   is a real number  a weight  that corresponds to each term t in each document d  different
values can be used for the weights wt d   in this paper 
we explore three different representations of the text
documents   binary representation  tf model and tfidf model  all of these representations of documents
ignore the ordering of the words  for example  the phrase
mary likes cake has the same representation as cake
likes mary  research in information retrieval has shown
that the ordering of words in a document has minor
impact for many tasks      which leads us to adopt the
bag of words model  we first introduce the following
notation 

fivi  e valuation

a  binary representation
this is the simplest model  we use binary values for
the real valued function 
 
  if word t appears in the document d
wt d  
  otherwise 
b  t f model
the term frequency  t f   model takes into account the
number of times that a word appears in the document 
this model assumes that words that appear more often in
the document should be given more weight  tft d denotes
the number of times term t appears in the document
d  however  if one term appears ten times more than
another term  it does not mean that the term is ten times
more informative than the other term  therefore  we use
log   to dampen this effect and define 
 
    log   tft d if tft d    
wt d  
 
otherwise 
c  t f  idf model
this model takes both term frequency and document
frequency into account  besides the assumption of the tf
model  this model also assumes that if a term appears in
too many documents  it is less informative than a rare
term  we use dft to denote the document frequency of
the term t  i e  the number of documents that contain the
term t   which gives an inverse of the informativeness of
the term t  thus  given a total of n documents  we define
idf   log  

n
 
dft

where log   was used to dampen the effect of idf   our
tf idf model is then defined as 
n
wt d   log     tft d    log  
 
dft
v  f eature s election
feature spaces that come up in text classification
problems tend to have very high dimensions  hence 
feature selection is used to reduce the size of these
feature spaces  research has shown that aggressive reduction of the feature spaces leads to little loss in
accuracy and  in many cases  to significant improvement
in performance     rogati and yang found that   statistic
is more effective for optimization classification results
in text classification  we applied   statistic to feature
selection and analyzed the results with and without the
  statistic 

in text classification  precision  recall  f  measure
and accuracy are often used for validation of results  we
denote tp as true positive  f p as false positive  tn as true
negative  and f n as false negative  where positive and
negative refer to the output of a classification algorithm 
tp   f n
accuracy  
 
tp   f p   tn   f n
p   precision  
r   recall  

tp
 
tp   f p

tp
 
tp   f n

 pr
 
p r
vii  c lassification a lgorithms
f   

we applied naive bayes  svm  logistic regression 
decision tree and k  nn classification algorithms on
this dataset 
viii  d iscussion and r esults
a  naive bayes text classifier
our first pass at classifying the websites was using
the words in the boilerplate text directly to classify the
websites  we were hoping that the occurrences of certain
words would determine whether or not a website was
considered evergreen or not  we ran a text classifier
on the data with and without preprocessing  deleting
stop words  spelling errors and stemming   after the
model was trained on this set we computed the accuracy
percentage of the model on the examples which were
withheld from the model  i e  the test set  figure   shows
how the training and the test accuracy of the naive
bayes model is changing with the number of training
examples  the two curves are converging as the number
of examples is increased  the peak performance was
achieved when the number of examples was      and the
test accuracy was        beyond      examples there
was no more learning since the classification accuracy
stopped improving 
this implementation also allowed us to analyze which
specific keywords were the most indicative of a website
being from a particular category  figure   shows the
words sorted by a score of how indicative they were of
a category  from this preliminary analysis itself we can
see that the users have tended to rate websites related
to food as evergreen and websites related to sports 
technology or news as ephemeral 

fiincorporate numerical features  e g  number of links in
the web page  fraction of misspelled words  etc   into the
model 
a major source of difficulty with dt model was model
over fitting and high variance in predictions  the training
error was consistently         and the test error was        some of the dt variants that were implemented
included numerical feature reconstruction  using median
values in place of missing values   feature vectors with
full and reduced vocabulary and tf idf values 
the best results       classification accuracy  were
produced by using tf idf valued feature vectors with
the extra trees ensemble method  extremely randomized
dt   which builds each tree in the ensemble by sampling
with replacement  i e   a bootstrap  from the training set 
fig     training and test accuracy   vs no  of training examples

d  decision tree with naive bayes
evergreen
recipe
tablespoon
creamy
olive
chill
peanut
soften
oven
shred
preheat
mixture

ephemeral
upload
download
application
swimsuit
nfl
director
generation
analysis
url
court
device

fig     table of words sorted by indicativeness of category

in order to reduce over fitting of the decision tree
classifier  we attempted another approach that used the
list of informative words which were output by the naive
bayes text classifier trained on the same data set  this
list of informative words was used as the vocabulary of
words for the decision tree instead of the full vocabulary 
we believed that this could make the decision tree algorithm focus on the occurrence of words which are most
strongly indicative of whether a website was labelled
evergreen or not  however  this approach did not give any
statistically significant improvement over the previous
approach  we believe that this could be because the overfitting of decision trees masks any potential gain that we
get from using the most informative features produced
by the naive bayes classification 

b  svm
we implemented svm with both linear and gaussian
kernel  we used    fold cross validation to choose the
parameter c   we determined that c       with l 
penalty and gives the best result for both models  since
svm with gaussian kernel gives worse results than linear
svm in every case for this dataset  the results of gaussian
svm are not included in this report  we found that svm
with non linear kernel causes overfitting to happen on
our dataset 
c  decision tree
decision trees are used to select informative features
based on an information gain criterion  and to predict the
categories of each document according to the values of
the features in each document  in this case the features
are words and the values are   or   depending on whether
the word exists in the document  dt also allowed us to

e  k nearest neighours classification
we implemented k nn classification using different
choices of k and used    fold cross validation to determine how the accuracy of the model varies with
k   as figure   shows  the accuracy improves until k
reaches     and plateaus from then on  this figure also
demonstrates how the accuracy of the k nn model
fluctuates with the parity of k  
f  n grams
we ran all of the above algorithms using n grams of
different lengths  and these were essentially the terms in
our tf idf model  it turned out that in every algorithm 
  grams provided the best accuracy  figure   shows how
each algorithm performed for different sizes of n grams 
although this is counter intuitive  this may occur because
there is not enough text within the boilerplate texts of

fialgorithm
naive bayes
logistic regression
linear svm
random forest  n    

precision
    
    
    
    
table i

f  score
    
    
    
    

recall
    
    
    
    

accuracy
    
    
    
    

a lgorithms applied on full t f   idf model

precision recall
    
    
    
    
    
    
    
    
table ii
a lgorithms applied on top    

f  score
    
    
    
    

algorithm
naive bayes
logistic regression
linear svm
random forest  n    

accuracy
    
    
    
    

features

fig     accuracy of k nn vs the number of neighbors  using a
uniform and weighted distance metric

ix  m ore r esearch

fig     performance of classification algorithms for different sizes
of n grams

most websites and the tf idf vectors get rapidly sparse
for even small values of n 
g  feature selection

we have implemented a number of different classification algorithms  but our results have not improved
past     regardless of how we changed the parameters
for each algorithm  therefore we propose some obvious
extensions to this project which could be implemented
at a later time  firstly  we have been relying on the
data which has been provided to us by the html
parser  but we believe that we might have attained better
classification accuracy if we had used the text from the
raw html files for each website  secondly  since a lot of
numeric features had missing or invalid values  we might
have implemented other software to compute those fields
for each website and incorporated them into our analysis 
thirdly  instead of relying on classification algorithms
being implemented in isolation  it is possible that computing different linear combinations of the outputs of all
the algorithms might have given us better accuracy 

we also used   statistic to reduce the feature space 
as figure   shows  the accuracy increases until the
feature size is       and stabilizes afterwards 
h  validations and comparisons
table i shows that naive bayes is the best classification algorithm when we use tf idf model on this dataset 
we also compared this result with binary representation
model and tf model  and tf idf model indeed gives us
the best accuracy  table ii shows the results of using
the top     features after feature selection using  
statistic  in comparison with two tables  we can see that
the accuracy does not change significantly between the
full model and the reduced model 

fig     feature selection of the top k features vs accuracy

fix  c onclusion
we have determined that the user classification for this
dataset has been heavily biased towards food and recipe
related websites  and biased against websites related
to news articles  technology and sports  it is possible
that this particular group of users believes that recipe
websites are more likely to be of interest in the long run 
an unexpected trend we noticed was that  in none of our
algorithms did cleaning up the text  by removing stop
words  spelling errors  stemming  removing duplicate
words  improve the prediction accuracy  in fact in some
cases the prediction accuracy actually became slightly
worse  this could be because latent factors in the original
text  such as the stop words and different forms of a
word may have influenced a users opinion of whether
that website can be considered evergreen or not 
as an after thought we generated a word cloud which
displays that words related to food  sports and news show
up prominently in the data 
fig     word cloud of the top     informative words after tf idf

acknowledgment
we would like to thank victor minden for his help on
python 
r eferences
    thorsten joachims        a probabilistic analysis of the rocchio algorithm with tfidf for text categorization  in proceedings of the fourteenth international conference on machine
learning  icml      douglas h  fisher  ed    morgan kaufmann publishers inc   san francisco  ca  usa          

    ma zhanguo  feng jing  chen liang  hu xiangyi  shi yanqin 
an improved approach to terms weighting in text classification  computer and management  caman        international
conference on   vol   no   pp            may     
    monica rogati and yiming yang        high performing feature
selection for text classification  in proceedings of the eleventh
international conference on information and knowledge management  cikm      acm  new york  ny  usa
    yiming yang and jan o  pedersen        a comparative study
on feature selection in text categorization  in proceedings of
the fourteenth international conference on machine learning
 icml      douglas h  fisher  ed    morgan kaufmann publishers inc   san francisco  ca  usa          
    george forman        an extensive empirical study of feature
selection metrics for text classification  j  mach  learn  res   
 march                  

fi