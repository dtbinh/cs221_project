large scale social tag prediction
lorenzo lucido
stanford university
scpd   computer science department
remotely from   sheung wan  hong kong

llucido stanford edu

abstract
social tagging  also known as social bookmarking or
folksonomy  is the process of annotating a text
published on the web  usually on community forums  with specific keywords related to the underlying topic of the document 
in this paper  we re visit the famous task of social tag prediction with two perspectives  first  we
discuss in depth what challenges one has to face
while trying to predict social tags  both in theory
and practice  then  we take a practical and  most
importantly  scalable approach of the task  with the
potential objective to develop a high performance
automated tagging software  we propose an efficient algorithm based on linear svm and ensemble
methods and we test it on a real machine learning
competition  finally  from a more research oriented
point of view  we analyze how deep learning applied
to natural language processing can be used in order to approach the end goal of social tagging  topic
discovery 

  

introduction

the dataset used in this paper is issued from the kaggle
facebook competition named keyword extraction    the
training set consists in approximately   millions messages
provided by the well known stack exchange community   
each message includes a title part  a body part  and of
course the associated tags  the testing set  which is an
additional   millions untagged messages  serves as benchmark for the competition  the number of different tags in
the dataset is greater than         with names as various as
python  astronomy or stanford nlp  it is important to mention that while such massive dataset certainly makes data
manipulation relatively more complicated  it also constitutes
a very valuable asset 
the performance metric used in order to assess the predictions is the mean f  score  also known as micro averaging  
this measure    which is common in information retrieval  is
essential in any classification task  such as social tag predictionthat involves imbalanced classes  which reduces most of the
time to a low number of positive examples in binary classi 
lorenzos comment   please note that any information in
this document must remain confidential in order for my participation to comply with the competition rules 
 
www stackexchange com
 
for more details  the reader can refer to the mean f score
explanation page on kaggle website 

fication  and has been subject to a large amount of research
due to the fact that it is not directly maximizable  on balanced datasets  maximizing the accuracy is equivalent to
maximizing f  score 
most practitioners tend to train their algorithms on accuracy
performance and use cross validation in order to maximize
the f  score 
the particular task of social tag prediction been approached
in many ways  for example  heymann  ramage   garciamolina        have trained a svm algorithm on a bag ofwords feature model  while budura   al         have looked
into a pagerank like algorithm  using tags as links between
documents  our implementation can be seen as an improvement of the former  however we do consider the interesting
fact that the latter model is class free and therefore not limited to predict a finite number of tags 

  

large scale tag prediction

the first challenge in our task is to define and extract a set
of features that would be directly used to train the algorithm  in our case  we adopted a similar view as heymann 
ramage   garcia molina        and used a bag of words
feature model  we have drawn a particular attention to the
title of each message and used a technique called parameter
tying  in other words  we upweighted words appearing in
the title  by a factor of   in our case  we also normalized
the word counts by the total number of words in each forum
post  in get comparable features regardless of the size of the
message 
this brings us to the second challenge  which is the number
of features and how they can fit in memory  the training set
has more than     millions words which generate more than
   millions features  while most  if not all  are dealing with
this problem by using mutual information  or similar tests 
to perform feature selection  we chose to keep them all  we
justify our choice by the necessity to have a unique set of
features  for all tags and all weak learners   as we discuss
later  which allows to get a fast running algorithm  we can
realize this by  first  exploiting the sparsity of the features
 much less than      of the total number of features used
per training example  and second  using feature hashing   
social tag prediction is an application of text classification
that involves  as we mentioned  highly imbalanced datasets 
 
feature hashing is the process of assigning hash values to
features  words here   potentially generating collisions  but
with the noticeable advantage of avoiding the necessity to
keep a large vocabulary into memory 

fi 

    
    
    
    
    

max achievable f  score    

stackexchange forums most frequent tags
c 
     jquery     
mysql
java
    
c  
    
html
php
     python     
 net
javascript      iphone     
ios
android
     asp net      objective c

table      of positive training examples
indeed  table     illustrates the percentage of positive examples across the popular tags  and even the tag the most
frequently used  namely c   does not have more than    
of positive examples 
it is important to note that training a machine learning algorithm with imbalanced classes is still a challenge today  the
usual techniques to cope with such imbalances are based
on dataset resampling  either by under sampling  removing
samples from the majority class  or over sampling  duplicating samples from the minority class   in order to restablish
balance between both classes  we  again  chose a different
path by leveraging on our large labeled dataset  instead  we
are using an aggregation of classifiers with almost no regularization in order to make sure each of them outputs a very
high f  score  even with imbalanced data 
finally  we split the social tag prediction into two consecutive subtasks  the first one  named popular tag classification  is the prediction of the most popular tags with the
usual bag of words feature model  each one of the prediction
is considered independent of the others  therefore  we train
as many binary classifiers as the number of tags we want to
predict  in the second subtask  individual tag extraction  we
need to predict non popular tags  since popular ones have
been handled in the first subtask   and therefore we assume
that the tags to predict are likely to appear in the related
message  thus  we want to use a completely different set of
features 
in order to isolate the popular tags  we sorted them by frequencies and retained the n top ones  we then studied the
maximum achievable mean f  score  denoted f    as a function of n  defined more formally as  
f   n   

p n 
p n   

n  n 
 

where p n  is the number of positive tags within the set
of tags we are trying to predict for the full training set and
n  n  is the number of positive tags not included in this set
of tags  directly counted as false negatives   one can easily
verify that f   n  is equivalent to the usual micro averaged
f  score with perfect predictions on the first n tags and no
prediction at all on the remaining ones 
while we could only focus on predicting existing tags for
the purpose of the competition    we consider that trying to
match human inputs   although being an interesting challenge   diverges slightly from the actual topic discovery purpose of social tagging  indeed  an adequate set of tags should
give useful information about the topic of the message but
also avoid useless and misleading information  while we  as
humans  usually tag words which are relevant  we tend to
make two mistakes  first  we omit very important tags  as
 
top competitors have achieved a mean f  score as high as
    only by predicting existing tags

   

   

   

  

   

     

      

number of tags to predict  logscale 
figure    max f  vs number of tags

an example  we have in our dataset a message tagged with
boost  which is a famous c   library  but not c    see the
message extract   second  we often do not consider wordsense disambiguations as shown by the popularity of the tag
post  used in more than         messages  which has many
different meanings 
this argument favors our two step approach  in the first
one  we use the bag of words features and tag labels to get
a first set of tags for each message  then  we use this representation and a different set of features to construct our
final set of tags 
also  we believe that setting n       is a good trade off
between performance and efficiency  although it is for sure
up to discussion and might need to be optimized at a later
stage 
message extract  
compiling with boost c  
im using boost for the first time and having a problem
compiling my program  so far all i have done is include
the tokenizer hpp header file and its failing with a load of
errors  im using using g          boost      on centos
      bit  here is the output       
tags   linux  boost  gcc   

  

popular tag classification

we now focus on our first subtask in more details  let us
denote mtest the number of messages for which we are trying to predict the tags  and t    t    t            tn   the set of
n most popular tags to be predicted  we then try to build
the mtest  by n matrix y which values are either    predicting absence of tag  or    presence of tag   this matrix is 
again  sparse since the average number of tags per message
is about   
our first approach was to build a simple naive bayes model
in order to get a first idea of what percentage out of the max
achievable f  score is reasonable  using only the first    
tags and cross validation to set the laplace smoothing parameter  we managed to get an overall performance of      

firealized f  score  reported by kaggle 

 on the go learning curve as the population of weak
learners increases

   
    

using our algorithm to predict the first     most popular
tags and    weak learner for each tag  we managed to reach
almost     on the mean f  score for the competition with
the computing power of a single laptop    we report the
learning curve of our algorithm as a function of n  the number of predicted tags in fig     the number of weak learners
     corresponds to the full training set  excluding the crossvalidation set  also  algorithm   gives a pseudocode of our
implementation  

   
    
   
    
 

   
   
   
   
number of tags predicted

   

figure    learning curve
out of        this result shows that a discriminative model
 such as svm   should be reasonably able to get more than
    of max achievable f  score even when training substantially more classes  given the large size of the training set 
it is also important to note that naive bayes has the advantage of being gradually parameterized by using subsets
of the training set that can fit into memory  in our implementation  each subset si is represented as a sparse ms  byn matrix  ms being the size of each subset  we are using
         messages by subset  and n the number of features   
our proposed algorithm  is the combination of linear svm
and bagging  bagging  or bootstrap aggregating  is an ensemble algorithm usually employed to build a set of classifiers by randomly sampling a subset of the training set  it
has a lot in common with k fold cross validation  except that
in cross validation  we try to estimate of the generalization
error  while in bagging  we average the predictions  across
all the classifiers  also called weak learners  in order to obtain a more robust model overall  each of our weak learner
cjk is a linear svm model trained to predict a tag tk  t on
a subset sj of the training set  inspired by the wisdom of
the crowd and again taking advantage of abundant labeled
data  each subset sj is disjoint  therefore guaranteeing the
diversity of the classifiers  the regularization parameter of
each linear svm is set to an extremely low level in order
to guarantee a high f  on the training set even with unbalanced data  we then perform a majority vote 
our resulting implementation has several strengths  
 easy to implement
 highly parallelizable   horizontally  tags  and vertically  disjoint subsets 
 low memory consumption  adjustable with the size of
the subsets 
 
in practice  n is the maximum number of hash values resulting from feature hashing      millions here 
 
in classification  each weak learner votes to either predict
a positive or negative label 

data  training set   bag of words features x  tags y
result  tag predictions on test set x
split x and y into m subsets sj    xj   yj  
for k     to number of tags to predict  n  do
y   k th column of yj
for j     to number of subsets  m  do
train linear svm classifier cjk  xj   y 
predict on test set cjk  x 
end
majority vote across predictions
end
algorithm    svm   bagging for popular tag prediction
both loops are parallelizable in the above algorithm 

  

individual tag extraction

our second task is now to improve the results previously
obtained by looking to extract the words within the message which are relevant to the topic of the message  indeed 
having predicted the tags mostly used  we consider that this
new representation gives a first idea of the message content 
however  it is only based on human inputs  which  as discussed above  are relevant for the kaggle competition  but
not ideal for summarizing the topic of each message 
in this particular step  we are looking to enhance the set of
tags by adding relevant words  these words must be either
in the body or in the title of the message in order to be
selected as tags  therefore  while still not exactly summarizing the content using new words  this approach has the
advantage of choosing words which have potentially never
been tagged before but still gives useful information 
we introduce a modified version of the tf idf statistic 
called tf icf  term frequency   inverse class frequencies 
and defined by  
 x
n
 c  i   
td
log
log t  t  d    n log
 d  i  
tc  i 
where td is the frequency of the term t in document d 
 d  is the total number of terms within the document d 
tc  i  is the number of documents of class i where the term
t appears   c  i    is the frequency of the class i with the
corpus of documents  the classes here correspond to the
tags predicted in the previous step 
this statistic is designed to be  for each class  increasing
with the probability of the term t within the document d
 
for the latest rankings  please check the kaggle facebook
rankings

fiand decreasing with the probability of the term t within the
class i  it is closely linked to the multinomial naive bayes
log likelihood  due to the assumption that the classes are
independent 
the tf icf statistic allows us to rank all the term in all
the documents relatively to the additional information they
bring on top of the general idea of the topic predicted in the
first step  that is  for example  knowing that the message
seems to be about java and eclipse but not linux  is the word
void relevant to the overall topic of the message   is the word
mahout relevant  
the tags predicted in the previous step are here very important  because for a new untagged message  we need to
provide our algorithm with a basic set of tags  also  the
computation of tf icf can be implemented very efficiently
using some optimized vector operation library and is much
simpler than a pagerank like algorithm 
the only parameter in this second step is in fact the unique
threshold used to select words that should be flagged as tags 
one way would be to set it in order to maximize the mean
f  score on a cross validation set  but since we consider that
human inputs might not be ideal  supervised learning should
not be part of this step  we suggest instead to target a mean
number of tags around    similarly  it is also fairly difficult
to assess the quality of topic discovery 
another choice for the second step could have been to consider building a totally new set of tags  i e  considering the
predicted tags from svm   bagging only as classes  these
classes would then be used to compute a slightly different
td idf like statistic that emphasizes on inter class relevance  rather than intra class  
an actual implementation with examples of the second step
will be subject of further work 

  

discussion

being able to automatically summarize a message  either by
selecting words relevant to the topic  or building short sentences  is still today one of the most difficult task in machine
learning  and more specifically natural language processing due to several reasons  namely  
 the quality of the output is very subjective and hard
to assess quantitatively
 due to unreliable human labelling  supervised learning
is not sufficient
 the features used can be considered as very noisy  since
several people can write the same idea in different
ways  and sometimes mispelling words
kaggle is definitely the kind of initiative that can really help
the development of machine learning  it is indeed very
interesting to compete against the best practitioners  in a
result driven challenge  as for our participation in the facebook competition  we have carefully looked into explaining
the top score of      while our actual score is about     
the top scorer announced that he has achieved it using only
popular tag prediction  first step of our method   it means

initial dataset   document corpus

parsing   pre processing

word vector space representation

document vector space representation

optimization problem

output
figure    deep learning pipeline for topic discovery

that his solution lies between predicting perfectly     tags
and predicting       tags with a weighted     f    we think
the score difference  and in the meantime  the key ideas in
order to improve our score  can be explained by  
 a significantly higher number of predicted tags
   better predictions
 a better parsing script with manual inputs related to
main programming languages  should show significant
improvement  due to the tag popularity of such languages 
   better features
 a faster implementation in a programming language
suitable for large datasets  allowing for example ensemble methods algorithms to achieve a lower generalization error
   better training algorithm
we have then highlighted the fact that trying to match human inputs may not be the end goal of social tag prediction 
indeed  we consider the task successful when the tags associated to a text are giving a good overview of its content   
individual tag extraction  through tf icf statistic  presents
a way to select words eligible to be tags within the associated
text 
finally  we open the discussion on how deep learning could
be applied for social tag prediction  and especially how the
previous work from socher  pennington  huang  ng   manning        connects to this specific task  deep learning 
although very demanding in terms of computing power    
seems to be the next step towards better social tagging 
 
additionally  we could say that proper tags are to be reused
across the corpus  and therefore must be both words relevant
to the topic and popular within the community 
  
note   it did not seem to be wise training a sparse autoencoder on a   millions documents dataset with a single laptop 
which is why we looked into more computationally efficient
algorithms  but the initial subject was deep learning for
social tag prediction

fimore specifically  we can train a deep learning algorithm to
map words and documents onto the same vector space  in
which a large euclidean distance between two words or documents translates to very different meanings  and inversely
a close distance into similar meanings  then  after a clustering step that will help us define the tags  i e  words with
close meanings are grouped under the same tag  the social
tagging  or more generally  topic discovery task reduces informally  to the constrained optimization problem 
 minimize the number of tags
 s t    sum of tags   target document     meaning dist
where       is the l   norm and meaning dist is a constant
that defines how close to the actual meaning of the document the tag summarization should be  that is  we want
to restrict the meaning of the selected tags to be within a
meaning range  which  in the word vector space  is simply
the euclidean distance  this pipeline is fully unsupervised 
in order to be consistent with the fact that human social tagging is not necessarily relevant  an overview of the pipeline
is given in fig    
an extensive study about this application of deep learning
will be the subject of a further work 

  

references

    p  heymann  d  ramage and h  garcia molina  social
tag prediction       
    a  burdura  s  michel  p  cudre mauroux and k 
aberer  neighborhood based tag prediction       
    d  yin  z  xue  l  hong and b  davison  a
probabilistic model for personalized tag prediction
      
    a  hotho  r  jaschke  c  schmitz and g  stumme
information retrieval in folksonomies  search and
ranking       
    a  ng  cs    lecture notes       
    c  manning  p  raghavan and h  schutze 
introduction to information retrieval       
    s  bird  e  loper and e  klein  natural language
processing with python       
    a  rajaraman and j  ullman  mining of massive
datasets       
    m  byrne  predicting tags for stackoverflow posts
      
     h  guan  j  zhou and m  guo  a
class feature centroid classifier for text
categorization       
     r  socher  j  pennington  e  huang  a  ng and c 
manning  semi supervised recursive autoencoders for
predicting sentiment distributions       
     e  bauer and r  kohavi  an empirical comparison of
voting classification algorithms  bagging  boosting
and variants       

fi