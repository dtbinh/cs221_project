model reconnaissance  discretization  naive bayes
and maximum entropy
sanne de roever  spdrnl
december      

fidescription of the dataset
there are two datasets  a training and a test dataset of respectively     
and      observations with    features  and one label feature  the has
training dataset has     positives  the test dataset has     positives  the
classes are skewed  the training dataset will be used to develop a suitable
prediction model  cross validation  k fold  where k     on the training
dataset will used to assess model performance during development and to
tune meta parameters  the test dataset is used to test the final model 

discretisation of features
 yang and webb        present a comparative study of nine different discretisation policies  not all policies perform equal  the following features
in the dataset are pre discretised  age  income indicators  insurance policy
contribution indicators  no clear discretisation policy can be detected for
the features  this is a disadvantage 

first learning curve for a naive bayes model

figure    learning curve for all  l  and non noise  r  features
a first learning curve  homegrown in python  for a naive bayesian model
with all features is presented in figure    the error is defined as the fraction
of positives that is not part of the dense sub sample  the parallel gap is
an indication of overfitting variance  adding more data  if possible  does
not seem to be the answer given that the lines are parallel 

compression  selection or regularisation
having a lot of noisy features can lead to overfitting and high variance  running a pca on categorical data is not common  for ordinal categorical data
 

cs     autumn      spdrnl

fia polychoric correlation matrix can be constructed  but in this context most
of the features are not ordinal  a tetrachoric correlation matrix could be
constructed on binarized features  as far as overfitting and feature selection
is concerned  regularisation can provide an outcome 

noisy data
using a   test of independence  p         and additional bonferroni correction  on the total of    features the null hypothesis of independence of
   features can not rejected  not considering interactions effects these features could be titled noise  a lumber jack type of backward selection to see
the trees in the forest  the tests indicate that the other features are likely
to contain signal  the noisy and non noisy are actually alike semantically 
chances are not high to loose possible interactions by filtering noisy features 
the remaining features concern four types of information  social economic status  owning a car  renting or buying a home  contribution to or
number of a selection of other insurance policies   these feature clusters
could make candidates for factors   if the noisy features are filtered  the resulting learning curve looks better  the variance seems to be under control 
applying the chi square tests provided extra information that regularisation
does not provide  which is welcome at this point 

collinearity
finding interaction effects is a nphard problem  having a lot of features does not help this  the data
is highly correlated  likely a lot of
features are still redundant  in figure   the results of a chi square
test of independence  p         and
additional bonferroni correction between the remaining features and
the label is shown  the upper triangle gives the p value  the lower triangle indicates if the null hypothesis
figure    collinearity
of independence was rejected  red
indicating true   in the lower left the social economic indicators can clearly
be recognised 
a downside of using a chi square is that it does not give direct insight into

 

cs     autumn      spdrnl

fithe strength of the dependency  usually social economical features correlate
mildly  the notion of collinearity of categorical features is actually not well
defined since categories can be split and merged arbitrarily  the best way
of tracing collinearity in this case is by evaluating  additional  prediction
power using for example forward of backward selection 
in the next section domain knowledge will be introduced 

domain knowledge
the domain knowledge concerning consumer spending is  almost disappointingly  straight forward  the most policies are bought by consumer that buy
insurance policies and have the means to do so  this analysis is confirmed
by  perlich         perlich build advertisement auction models  information
relevant to predicting purchasing behaviour of online consumers consists of 
did this consumer buy items online   what category of products   what
was the maximum spending   how often does this consumer buy online  
how long ago was the last purchase   did pears buy a similar product   the
feature clusters found in this project were similar 

the trouble with collinearity and naive bayes
in a naive bayes model adding a lot of near similar features gives these features to much weight in the model  given the assumptions of the model 
a naive bayes model and cross validation does not seem to be the way to
check for high collinearity  to check for collinearity  as far as the predictive value of that feature is concerned  see the earlier note on this  logistic
regression model selection with aic in r is a reasonable option  of the
social economical features customer main type  average income and
purchasing power class have all the prediction power  lower level education and social class a if added can improve the performance very
marginally based on aic  aic though is not critical on model complexity  hastie et al          for now we ignore these  removing the redundant
features reduces the number of social economic indicators to    the remaining features indicate three things  income  the number of policies and the
policy contribution  how expensive   these features seem to be the base
ingredients for the final model 

the trouble with bayes
bayesian networks in general capitalise on breaking down the full joint under
the assumption of independence  once this assumption falters  the elegance
 

cs     autumn      spdrnl

fialso seemingly disappears  applying bayesian networks requires firm prior
knowledge   let it be noted that i have not learned pgms yet   although
the coil      winning model was bayesian  the data seems to be cramped
to fit  reviewing several bayesian models it seems that a variation on latent class models called latent tree models  zhang et al         could fit the
bill  using latent factors to capture the signal in noisy factors is common in psychometrics  the model allows for latent factors in a bayesian
network  in this context the factors could be social economic status  and
insurance mindedness  created using the number of bought policies and
insurance contributions  interactions are not supported though  other contenders could be tan or ban networks  but these models seem to focus
on dealing with faltering assumptions   in this context maximum entropy
 softmax   or in this context logistic regression  seems a good class of models   ng and jordan        it is argued that although the generative naive
bayes converges faster  but discriminative logistic regression actually outperforms naive bayes given enough samples  given that the current model
may not be sparse  regularisation can be applied if necessary 

figure    learning curve regularised logistic regression with c     

insights from chi squared automatic interaction detection
chi squared automatic interaction detection  chaid  is an algorithm that
builds decision trees on categorical data using chi  test and bonferroni  an
observation is that the categories of categorical features up to some point
are arbitrary  as mentioned earlier  and could be merged if the categories
contain no signal  where the number of possible mergers can be computed
by bells number  ritschard         chaid proposes the following method
 

cs     autumn      spdrnl

fito find the right splits in a feature  using an rx  table  r is   in this context 
to find a category that is not significant and the least significant  merge this
category with the second worst category too see if the combined categories
are significant  if so  see if one of the categories can be split out without
loosing significance  if not  repeat the procedure  chaid is not a goal
currently  but looks very promising

logistic regression with interactions
logistic regression can handle categorical data if the data is binarized  using
a   test of independence  p         and additional bonferroni correction 
out of the    binary features there appear to be    significant predictors 
after creating interaction effects  a total of    interactions effects are identified using a similar test  these numbers are large  the regular r glm
package cannot fit a model to this amount of variables  the data is ill conditioned  regularisation can help out  its relationship to ridge regression
eases optimisation  and prevents overfitting  the plan for forward selection
is canceled since im not that familiar with liblinear  the learning curve
above is generated with liblinear  the results look reasonable 

final result
the model picked     out of     candidates  that is    less than the winning
model  the result is good enough for a top   position 

conclusion
the end game of the project proved harder then expected  error analysis
on observations too  with some more r and liblinear experience i might
have gotten a nicer result by forward selecting the interaction features  due
to this inexperience i had to use heavy regularization  which gives less information  but finally  getting    parameters to work on several hundred
positives is amazing  regularization is as big as a life saver as naive bayes 

post mortem
as often is the case  prior knowledge is often generated after the facts for
the next time  if generalisation applies 

 

cs     autumn      spdrnl

fibibliography
 hastie et al         hastie  t   tibshirani  r   and friedman  j          the
elements of statistical learning  data mining  inference and prediction 
springer    edition 
 ng and jordan        ng  a  y  and jordan  m  i          on discriminative vs  generative classifiers  a comparison of logistic regression and
naive bayes 
 perlich        perlich  c          data science and predicitive modeling 
 ritschard        ritschard  g          chaid and earlier supervised tree
methods  research papers by the department of economics  university
of geneva          dpartement des sciences conomiques  universit de
genve 
 yang and webb        yang  y  and webb  g  i          a comparative
study of discretization methods for naive bayes classifiers  in in proceedings of pkaw       the      pacific rim knowledge acquisition
workshop  pages        
 zhang et al         zhang  n  l   yuan  s   chen  t   and wang  y         
latent tree models and diagnosis in traditional chinese medicine  artif 
intell  med                

 

fi