twitter sentiment analysis
jon tatum
john travis sanchez
https   github com jnthntatum cs   proj  

abstract
as  social media is  maturing and growing  sentiment analysis of online communication has
become a new way to gauge public opinions of events and actions in the world  twitter has
become a new socialpulpit for people to quickly  tweet  or voice theirideasina   characters
or less  they can choose to retweet  orshareatweet topromoteideasthattheyfindfavorable
and elect to follow others whose opinion that they value  specifically  we studied sentiment
toward tech companies in twitter  we tried several methods to classify tweets as positive 
neutral  irrelevant  or negative wewereabletoobtainhighoverallaccuracy withthecaveatthat
thedistributionofclasseswereskewedinourdataset 

dataset
we used a handcurated twittersentimentdatasetpublishedbysanderslab itcontainstweets
from          that mention one of four  major tech companies  sanders lab manually
assigned labels for each tweet as either positive  negative   neutral  or irrelevant  
positive  and  negative indicated whether or not the tweet showed a generally favorable or
unfavorable opinion toward the mentioned company aneutrallabellingindicatedthatthetweet
was either purely informative or the opinion of the tweet was otherwise ambiguous  an
irrelevant  labelling indicated that the tweet could not be determined to fit into any of the other
classes thisoftenindicatedthatthetweetwasnotwritteninenglishorthatitwasclearlyspam 

we  hoped to see whattechniques we could use in order topredictthe labelling of an example
tweet only using transformations of the tweet text  with this goal in mind  we spent some time
analyzing the tweetsthemselves  these plots show  as expected from empirical evidence the
distribution  of token frequencies roughly follows a power law distribution  note that much of the
right tail is cut off in  each graph  a property of power law distributions is  that the tails of the
distribution areextremelyheavycomparedtomostcommonprobabilitydistributions    giventhat
a large proportion of tokens appear in only one or two  examples  we can quickly reduce the
complexityofourrepresentationbyfilteringoutthosetokens 
 

sanders niekj  twittersentimentcorpus  sandersanalytics sandersanalyticsllc       web   
nov      
 
powerlawwikipediaweb   nov     

 

fimodeling
the first challenge was finding a reasonable way to represent the tweet text  language data is
messy   considering each token as a distinct dimension in the feature space isntviable  there
are many words that are ubiquitous in text  and  therefore  arent significant if discovered in a
tweet  there are also words that only appear inoneortwotrainingexamplesacrossourcorpus
and dont provide a good estimate for how strongly associated they are with their labelling 
furthermore  there are many variations on words that dont change their meaningsignificantly 
butrequireadifferentformforthesakeofgrammaticalcorrectness 
in  order  to best represent the tweet text  we used  several techniques shown to work well in
document similarity problems  specifically  we used a method loosely based on pubmeds
related citation algorithm    we stemmed tokens of the tweets using the natural language tool
kits   implementation of the porterstemmer  built a distribution over the frequency of stemmed
words  and removed tokens that fell in the left tail of the distribution  we used thecount of the
filtered stemmed tokens ineach input example as features in the vector representation of each
tweet 
while  it is reasonable to consider removing tokens from the right side of the distribution  ie
tokens that appear extremely often   there is an a priori notion that some of the more common
tokens are modifiers and are more significant in kgrams as opposed  to single tokens  one
example is  not  or the stemmed  separated contraction form from our stemmer  nt    which 
seemingly shouldinverttheresponseofanypredictor 

feature selection
to  tune the amount of words that we removed from the left tail  infrequent tokens so that our
representation retained a reasonable amountofinformationfromthetweets weran foldcross
validation on a multinomial bayesian classifier  we tried different differentthresholdsatwhichto
retain tokens  we foundthatthedatawasskewed veryheavily towardstheneutralandirrelevant
classes biasingthepredictionstowardsthoseclasses 

pubmed help  internet   bethesda  md   national center for biotechnology information
 us          pubmed help   updated      nov     available from  
http   www ncbi nlm nih gov books nbk    
 
bird  steven  edward loper and ewan klein         natural language processing with
python  oreilly media inc 
 

 

fihowever  a very slight peak can be seen  manually inspecting theperformance  we found that
ignoring terms that appear four times or fewer provided a good  balance across the different
metrics we considered seetablebelow  inconsideringwherewedrewthecutoff wechosethe
threshold that gave us the highest
sensitivity for  positive and negative
examples  without causing too much of
a drop in specificity  values for positive
predictivevalue alsocalledtruepositive
rate and precision  arealsoshown this
reduced the number of features used
from       to       in choosing this
metric  we considered the use case of
using a sentiment classifier as a first
pass filter for  analysis of the overall
opinionofapopulation 
neg
sens

neu
sens

pos
spec

neg
spec

neu
spec

min
freq

vector
len

pos
ppv

neg
ppv

neu
ppv

pos
sens

 

    

      

      

      

      

      

      

      

      

      

 

    

      

      

      

      

      

      

      

      

      

 

    

      

      

      

      

      

      

      

      

      

 

    

      

      

      

     

      

     

      

      

      

 

    

      

      

      

     

      

      

      

      

      

 

    

      

      

      

      

      

      

      

      

      

pca
we  investigated the use of principal component analysis to  reduce the feature size  we
implemented pca in numpy using the linear algebra module  we projected the unigram token
counts using a threshold of two document occurrences  to a     dimensional feature space 
using an offtheshelf machine learning workbench  weka  we used the lowerdimensionaldata
set to  train a neural network classifier  for thistest  we used the unigram token count vectors
generated using a minimum occurrence threshold  of     we were  limited by memory
requirements  of the library we used  this was the lowest threshold that we could run on our
machines 
visualizing the data projected onto the  lower dimensional space showed promise  viewing the
first    principal components as stackedhistogramsindicatedthatthedistributionofeachofthe
classes for some of thefeatures was noticeably different  this gaveanindicationthatageneral
purpose classifierwould perform well  since we didnt have anystrongnotionsoftheunderlying
relations between the projected features and the classes  we went with a neural network  a
modelthatperformswellwithmanyfeatures 

 

fi figure wekavisualizationoftrainingdataprojectedontofirstprincipalcomponents 

classification
having settled onsome detailsof our model wetesteditwithseveraldifferentalgorithms using
off the  shelf libraries rather than implementing our own  we wanted  to get an idea for what
methods worked well and which didnt  we generated training and testsets from ourcorpusby
randomly permuting the examples and using     of examples fortraining set and     for test
set 
we  found that the results between the different models didnt vary greatly  all were able to
achieve on the order of    accuracyandaprecisionofaround    theconfusion matricesfor
the neural network and the multinomialnaivebayesclassifiersshowsomeofthestrengthsand
drawbacks of each  naive bayes performs better on correctly classifying negative and neutral
tweets but doesntperformverywellonclassifyingpositivetweets onthe otherhand theneural
networkhasmoreevenresultsacrossallclasses 
neural network
 weka using pca reduced design matrix 
accuracy      
rocarea      
precision      

multinomial bayes
 weka using bigram counts threshold at   
accuracy      
rocarea     
precision      

negative

neutral

positive

 prediction

negative

neutral

positive

 prediction

  

  

  

negative

  

  

 

negative

  

   

  

neutral

  

   

  

neutral

  

  

  

positive

  

  

  

positive

the svm classifier provedtohavehighaccuracyfornegativeandpositivetweets whilehavinga
loweraccuracyforneutraltweets italsoshowedlowsensitivityforthepositiveand

 

fisvm
 scikit in python 
accuracy      
precision      
kfoldvalidationoutput 
threshold label
accuracy
    
negative     
    
neutral
    
    
positive
    
    
negative     
    
neutral
    
    
positive
    
    
negative     
    
neutral
    
    
positive
    
    
negative     
    
neutral
    
    
positive
    
    
negative     
    
neutral
    
    
positive
    
    
negative     
    
neutral
    
    
positive
    

sensitivity
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

specificity
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

precision
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
l    
    
    

negative tweets which meant that the
svm  correctly identified a low number
of the positive and negativetweets on
the other hand  the svm maintained a
high specificity for the negative and
positive tweets which meant that the
classifier did often correctly classified
negatives alternatively hadmaintained
a low false  positive rate   this means
that our svmmodeloverallprovidesan
useful way for finding some of the
positive and negative tweets  and
providing a high accuracy rate  but
may often fail to identify many of the
positive and negative tweets  while it
would likely  not provide a holistic view
of  public sentiment on a topic it could
work reasonably well filtering many of
theuninformativetweets 

conclusion
results from this method arent particularly compelling  we thought that the limited space
permitted  in tweets would allow for purely statistically based techniques  however that
assumption seems to nothold acrossthreemodelsthatareknowntoworkwellforthisclassof
problem  we found poor performance  thisindicatesthatourtextrepresentationisntparticularly
good   given that our feature space is still quite large  a more intelligent method for feature
selection could lead to better results  his is somewhat corroborated by the most informative
bigramsfromthemultinomialbayesclassifier usingthemetric logp bigram class 
p bigram   
we found that many of the top terms are entity names which  while       devic  
likely strongly related to the classes  probably dont convey much   microsoftstor   http  
information on the underlying opinion of the tweets  while our project   offer   up  
   microsoft   microsoftstor  
hasnt shown a particularly great model for sentiment classification     store   offer  
hasprovidedsomesuggestionsonfutureapproachesthatmaywork     devic   microsoft  
   up   free  
   n t   wait  
         i  
    phone      
    replac   my  
    thank   for  
    love   the  
    i   love  
    googl   nex 

 

fias requested in ourmeeting withsameepontravissrolein theproject hereisaroughdivision
oflaborfortheprojectatthispoint 
jon 
procuringdataset
scrapingtweets
textprocessing conversiontocountvector
featureselection codeinfrastructureforkfoldcrossvalidation
pca
classifiertestsinweka
exportingdataforusewithpythonlibraries
majorityofwriteupsformilestone presentation andfinalreport
travis 
researchonsvmlibrariesforpython
runningsvmclassifier
   helpwithpresentationwriteup
   helpwithfinalreport
travis plans to extend the project as he completes the course material over winter break and
january 

 

fi