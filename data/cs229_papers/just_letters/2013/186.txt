feature reduction for unsupervised learning
meng wu  yang zhao

abstract

clustering methods   a  k means   b  em with mixture
of gaussians  and  c  spectral algorithm on two different
datasets 

in this project  four unsupervised feature reduction algorithms for clustering problem were investigated and experimented upon two sets of data  handwritten digits
data set and the functional magnetic resonance imaging
 fmri  resting state data set  ratio of sum of squares
 rss   leverage score  lev   and laplacian score  lap 
were used to rank the influences of the features in the
clustering  similarity based method were implemented
to find largest groups of features that dominate the clustering result  clustering results were evaluated and compared using both accuracy score and average fisher score 

 

   

notation

in the report  we represent the n samples data with ddimensional features as matrix


 x t 
  xt  
 
     f    f         fd    rnd      
x 


   
 xnt 
where xi  rd denotes the ith data point  and f j  rn
denote the jth feature vector  the ith data point belongs
to the kth cluster ck   if i  ck  

introduction

an important problem related to machine learning and
large dataset mining is selecting a subset effective features from the original data  the goal of feature reduction is to use less number of features while achieving
high clustering accuracy or similar clustering results as
using all features  preprocessing the data to obtain a
smaller set of representative features  retaining the optimal salient characterstics of the data  not only decreases
the processing time but also leads to more compactness
of the models learned and better generalization     feature reduction also helps in improving the prediction performance of the predictors  providing faster and more
cost effective pre dictors  and providing a better understanding of the underlying process that generated the
data       when class labels of the data are available  supervised feature selection becomes possible and has been
studied widely  however  selecting features in unsupervised learning scenario is a much more difficult  it often
becomes a np hard problem    
in this project  we investigated four feature reduction
method      ratio of sum of squares  rss       leverage
score  lev       laplacian score  lap  and    feature
similarity based method  and experimented with three

 

clustering methods

numerous clustering methods have been proposed  and
different methods have different pros and cons  in the
project  we focused on three almost most commonly used
clustering method  k means  expectation maximization
with mixture of gaussian  em       and spectral algorithm          different algorithms make different assumption of the data have different clustering results  in
the same reason  the feature reduction algorithms may
also have different effects on the clustering resulting
when the clustering algorithms are not same 

   

k means clustering

given an unlabeled data set  k means is the first clustering method we have chosen  k means is a popular clustering method because of its simplicity and easiness to
implement  in our project  we first tried k means with
different number of clusters i e  k   we used standard
 

ficost function 
n

 ick  

j c       kxi

  k k 

there are several interesting properties in the spectral algorithm that may be useful to us  the laplacian matrix l
can be used to compute score for each features resulting
a efficient feature selection  the eigenvalues l indicate
how many block like structure are inside the data 

   

i  

because k means algorithm may fall into a local optimal  we repeated k means for     times with random
initialization  and select the clustering result with the
lowest cost function 

 

   

we assume the clustered data followed the model as

feature reduction methods

   

expectation maximization with mixture of gaussians

ratio of sums of squares  rss 
 ick  

xi

assuming our dataset is a mixture of gaussians  we further used expectation maximization em  algorithm to
cluster our dataset  em is another important clustering
method in machine learning  which involves iteration in
two steps  expectation e  step and maximization of expected log likelihood m  step  we assumed the hidden
variable zi   which follows a multinomial distribution  indicate which of the k gaussians each xi had come from 
the log likelihood defined as following 

   k   i

   

where  k  rd denotes the mean of the kth cluster  and
the i  n       id   is the random effect of the data  then
the sum of squares  ss  for the jth feature have the relation as
ss j total   ss j between   ss j within
k

 

n

 ick  

  ck    j   kj        x ji
k  

  kj   

   

i  

then ratio of sum of squares  rss  is defined as
n

 ick  

l c            log p xi

   k   k     log  k

   

rss j  

i  

 ic  

where is p xi k    k   k   is the probability of a gaussian vector  similar to the above k means method  we
repeated it     times with random initialization 

   

 
k  ss j between
 
 
nk ss j within

   

large rss means the feature has better correlation with
clustering results  therefore  we could used the clustering results from all the features to compute the rss
scores  then select the features with the largest rss values 
note that  if we assume the  kj is another random variable with model n      j    the the rss value has an fstatistic  fk  nk   for testing h      j          the pvalue of f test for rejecting the h  implies the significance of the jth feature in the clustering process  the
rss methods are applied the clustering results from the
three clustering algorithms discussed in the previous section 

spectral algorithm

spectral algorithms         uses information contained
in the eigenvectors of a data affinity  i e   item item similarity  matrix to detect structure  such an approach has
proven effective on many tasks  including information
retrieval  web search  image segmentation  word class
detection and data clustering  the construct a similarity
matrix s  rnn   we used the gaussian kernel to comkx x k 
pute the similarity si j   exp  i  t   j   between the ith
and jth data 
to cluster the frame in to k groups  the spectral algorithm is following 

   

leverage score  lev 

boutsidis et  al  presented a novel feature selection algorithm for the k means clustering problem      their
algorithm is randomized and uses the  normalized  leverage scores to assign probability to the features  the jth
leverage score equals the square of the euclidian norm
of the jth row of v 

 compute d   diag s   
 compute the laplacian l   i  d    sd   
 compute the first k eigenvectors of l  u            uk

 j   k v j   v j        v jk  k   k 

   

 normalized the each row of  u            uk  
where v    v    v         vk   is the matrix that contains the
first k right singular vectors of x  the jth leverage

 cluster using k means
 

fi 

score characterizes the importance of the jth feature with
respect to the k means objective  in the original paper  these scores form a probability distribution over the
columns of x      in this project  we simplified it by
selecting the features with the largest the lev scores 

   

   

laplacian score  lap  is fundamentally based on laplacian eigenmaps and locality preserving projection  the
basic idea of lap is to evaluate the features according to
their locality preserving power              the laplacian
score of the jthe feature is defined as     
t
fj d fj
 
t
fj l fj

   

   

where the vector fj is
fj   f j 

f jt d 
 t d 

 

feature similarity  fs 

several unsupervised feature selection algorithms based
on measuring similarity between features have been proposed         those algorithm aim to remove the redundant features by clustering the features using some
similarity measurements  we used two common similarity measurements  mutual information and correlation coefficients to construct feature similarity matrices
ami  rdd and acorr  rdd  
ami
i j   mi  f i k f j  
 p
acorr
ij

cov  fi   f j  
 
var  fi  var  f j  

fmri data

in the last few years  many machine learning algorithms
have been studied to investigate the brains functional activities and connectivities on the function magnetic resonance imaging  fmri  data      recent studies showed
that the spatial patterns of the temporal blood oxygenation level dependent  bold  signals even without explicit stimulation correlations have strong resemblance
with many established brain networks  which are generally referred as resting state network  rsn   the nonstationary correlational structure of rsn provides an opportunity to extract useful information about human brain 
however  there are little effort in investing the potential
network changes with the resting state temporal variability 
liu and duyn      recently reported a method using
point process analysis  ppa  to extracting the rsn patterns from relatively brief  a few seconds long  periods
of coactivation or codeactivation of regions  they extracted a few fmri time frames from the representative
participant at time points where fmri signal in the posterior cingulate cortex  pcc  was high  notably  these
single fmri frames already show coarse resemblance to
the dmn pattern  they also extend their method to select the frames based on the two signals  pcc and medial
prefrontal cortex  mpfc  
for this project we used fmri data at resting state
from    subjects  the total amount of fmri frames is
      each frame are further decompose into    features
signals  as suggested in lius paper  we obtained    
sample frames when the pcc signal is the     largest
among every subjects  therefore  the data can be presented in a    by      matrix  with each column representing a sample frame and each row representing a feature  to avoid the bias caused by different amplitude unit
of different features  we first normalized each feature to
zero mean and one standard deviation  this may help us

   

and l and d are defined in the spectral algorithm  for a
good feature is to feature  the laplacian score  j tends
to be big  therefore  we simply selected the features with
the smallest  j values 

   

handwritten digit data

in this data set       handwritten digits from around
   persons were scanned  stretched in a rectangular box
  x   in a gray scale of     values then each pixel
of each image was scaled into a bolean       value using a fixed threshold  each person wrote on a paper all the digits from   to    twice  the commitment was to write the digit the first time in the normal way  trying to write each digit accurately  and
the second time in a fast way  with no accuracy  
 availabel at http   archive ics uci edu ml datasets  semeion handwritten digit 

laplacian score  lap 

j  

data description

    

we used the feature similarity matrices to find several
feature communities  groups  using a modified spectral
algorithm      there are two way to select the features based on the founded groups  the first one is to
used select the representative feature from each feature
group  the second one is select the feature groups with
the largest sizes  we chose the second method  because
we want to let the clustering result with reduced features
close to the results using all features  thus  the feature
groups with the large size are likely to dominate the clustering  the advantages and disadvantages between two
approaches need further studies which are beyond the
scope of this project 
 

fi   

get each feature evenly weighted  and the energy of each
feature could contribute evenly in our clustering process 

we chose k     for clustering the fmri data using the
three different algorithms as described above  the clusters in all three algorithms have very obvious pattern
within each cluster  the clustering agreements between
three algorithm are about       figure     the top
    and top     significant features  brain region  in
   slices are also shown in figure   
using similar analysis approach  we show the relation
between the feature reduction and average fisher score
of fmri data in figure    again  the combination of
fs based methods and k means em algorithm achieves
a more reasonable average fisher score  demonstrating a
more preferable approach in real world application 

results and discussions

the evaluation of our clustering result is based on two
metrics  accuracy for labeled handwritten digit data  and
average fisher score for both two data sets  accuracy is
the ratio of correct clustering to the total data points  the
averages fisher score is defined as
d

ss j between
j   ss j within

    



handwritten digit data

  
features

   

 
d

based on the above description  we first plot thefeature
scores of the handwritten digital data as a heat map  figure     different colors in the heat map represents the
significance of different feature in terms of the corresponding feature score  i e  blue  lowest significance 
red  highest significance   from the figure we can see 
different feature scores vary differently  but most of them
indicate the distinguishable difference between different
features  i e  different colors   this figure intuitively
shows us the possibility to adopt meaningful feature reduction in our data set 
variance

rsskmeans

rssem

 

 

 

 

  

  

  

  

  

  

  

  

  

 

lev

  

  

 

lap

  

  

 

 

 

  

  

  

  
  

  

  
 

  

  

  

  

   

   
frames

   

    

    

   

 a  original signal
features

   

   
frames

   

    

    

    

    

 b  k means

  

  

  
  
  

  
  
  

   

   

   
frames

   

    

    

 c  em

   

   

   
frames

   

 d  spectral

figure    clustering results of the fmri data 

  

  

  

 

  

conclusion

in this project  we focused on a particular unsupervised
feature reduction problem  and have investigated four
unsupervised feature reduction algorithms  rss  lev 
lap  and fs on three unsupervised learning methods 
k means  em  spectral algorithms  experiment on the
labeled human handwritten digit data has proved the effectiveness of our feature reduction algorithms  and the
application on the fmri data presents a promising application in real world 

  
 

  

  
   

fscorr

 

 

  
  

 
 

fsmi

  
  

  

rssspetral

  

 

  
features

afs  

features

 

fmri data

  

figure    feature scores for handwritten digit data 
then  we experimented with the three different clustering methods by gradually reducing features  based on
the different feature scores  figure     reduction rate is
defined as the number of features removed to the number of total features  from the figure  we can see when
reduction rate is below      the accuracy remains quite
stable around      which is reasonable for unsupervised
clustering methods  similarly  we also use the average
fisher score to evaluate the accuracy  the result is similar 
moreover  we notice that the fs based methods remain
a more stable accuracy and average fisher score in kmeans and em method  which may be a more preferable
combination in the real world application 

 

acknowledgments

the fmri data used in this project is provided by
jingyuan chen from the radiological science lab  stanford school of medicine 

references
    b ilmes   j  a  a gentle tutorial of the em algorithm and its application to parameter estimation for gaussian mixture and hidden
markov models  international computer science institute       
            

 

fi    

    

   

   

    

    

   

   

   

accuracy

accuracy

accuracy

   

   

   

   

   
rss
lev
lap
fsmi
fscorr

    

   

 

   

   

rss
lev
lap
fsmi
fscorr

    

   

   
   
reduction rate

   

   

   

   

 

   

   

   

   
   
reduction rate

   

   

 

   

    

    

     

    

    

    

    

     

     

    

    
     
    

     

ave fisher score

    

    
     
    

rss
lev
lap
fsmi
fscorr

    

 

   

   

    

   
   
reduction rate

   

   

   

     

   

   

   
   
reduction rate

   

   

   

   

   

   

    
    
    

rss
lev
lap
fsmi
fscorr

     

   

   

 c  spectral

     

     

 

 b  em

ave fisher score

ave fisher score

 a  k means

rss
lev
lap
fsmi
fscorr

   

 

   

   

 d  k means

rss
lev
lap
fsmi
fscorr

   
    

   

   
   
reduction rate

   

   

    

   

 

   

   

   

 e  em

   
   
reduction rate

 f  spectral

    

    

    

     

     

     

    

    

     

     

    
     
    

    
     

    

 

   

   

rss
lev
lap
fsmi
fscorr

     
    

   

   
   
reduction rate

 a  k means

   

   

   

     

   
     
    

   
rss
lev
lap
fsmi
fscorr

     

     

ave fisher score

    
     

ave fisher score

ave fisher score

figure    clustering accuracy and average fisher score with feature reduction using handwriting digits data set 

 

   

   

rss
lev
lap
fsmi
fscorr

     
    

   

   
   
reduction rate

 b  em

   

   

   

     

 

   

   

   

   
   
reduction rate

   

   

   

 c  spectral

figure    average fisher score with feature reduction using fmri data set 

    b outsidis   c   d rineas   p   and m ahoney  m  w  unsupervised feature selection for the k means clustering problem 
in advances in neural information processing systems        
pp         

    p eng   h   l ong   f   and d ing   c  feature selection based on
mutual information criteria of max dependency  max relevance 
and min redundancy  pattern analysis and machine intelligence 
ieee transactions on                        

    f ortunato   s  community detection in graphs  physics reports                      

    p ereira   f   m itchell   t   and b otvinick   m  machine
learning classifiers and fmri  a tutorial overview  neuroimage    
          s   s    

    g uyon   i   and e lisseeff   a  an introduction to variable and
feature selection  the journal of machine learning research  
                 

     von l uxburg   u  a tutorial on spectral clustering  statistics
and computing                      
     w eisberg   s  applied linear regression  vol       wiley  com 
     

    h e   x   c ai   d   and n iyogi   p  laplacian score for feature
selection  in advances in neural information processing systems
        pp         

     x iao l iu   j  h  d  time varying functional network information extracted from brief instances of spontaneous brain activity 
proceedings of the national academy of sciences of the united
states of america        

    m itra   p   m urthy  c   and pal   s  k  unsupervised feature
selection using feature similarity  ieee transactions on pattern
analysis and machine intelligence                      

     z hao   z   and l iu   h  spectral feature selection for supervised
and unsupervised learning  in in icml        

    n g   a  y   j ordan   m  i   and w eiss   y  on spectral clustering  analysis and an algorithm  in advances in neural
information processing systems         mit press 
pp         

 

firandom

rsskmeans

rssem

rssspectral

lev

lap

fsmi

fscorr

 a  top      regions
random

rsskmeans

rssem

rssspectral

lev

lap

fsmi

fscorr

 b  top      regions

figure    most significant brain regions using differenct
selection methods 

 

fi