web content extraction through machine learning 
ziyan zhou
ziyanjoe stanford edu

abstract
web content extraction is a key technology for enabling an
array of applications aimed at understanding the web  while
automated web extraction has been studied extensively  they
often focus on extracting structured data that appear multiple times on a single webpage  like product catalogs  this
project aims to extract less structured web content  like news
articles  that appear only once in noisy webpages  our approach classifies text blocks using a mixture of visual and
language independent features  in addition  a pipeline is devised to automatically label datapoints through clustering
where each cluster is scored based on its relevance to the
webpage description extracted from the meta tags  and datapoints in the best cluster are selected as positive training
examples 

keywords
content extraction  svm  dbscan  web scraping

  

introduction

content extraction algorithms have been well studied and
there are many interesting and useful applications        a
survey by laender et al     systematically covers a wide
range of techniques used for this purpose  the techniques
range from wrapper development to nlp based and modelingbased approach  miao et al     proposed a fully automatic
tag path clustering approach to extract structures on a single webpage  sun et al     applied a language independent
approach by looking at text density at each area of the document 
we wanted to take a different approach to this problem  our
goal is to extract content of a webpage in a language independent way  we wanted to understand how we  humans 
can identify the main content of a webpage even if we do
source code for this project is avaiable on github under
the mit license at https   github com ziyan spider

muntasir mashuq
muntasir stanford edu

not recognize the language of the website  the relevant information for us for those webpages are  visual features like
font size  color and style  line spacing and block size  layout
of the webpage  text density in different parts of webpage 
density of links etc  in our algorithm we have used these visual features to train a svm classifier to find out the content
of a webpage 
another aspect of our approach is that we wanted to run
the complete pipeline automatically  from data collection
labelinglearningtesting  specially we tried to avoid
manual labeling of the dataset  so we devised a method
using clustering to label the dataset 
our overall algorithm looks like follows  label the collected
dataset using clustering  train svm with the labeled dataset 
use svm model to extract content from new webpages 

  

terminology

lets define a few terminology that would help us to understand the rest of the paper 

   

webpage

a webpage is a document that is rendered by a browser
corresponding to an url 

   

block

a block is part of a webpage that contains text and have
both height and width  for example  a paragraph in the
content of a webpage  a navigational link in menu  title of
an articleeach of these are separate blocks  the blocks are
highlighed in figure   
each block is an input datapoint to our clustering and svm
algorithm 

   

content

content is the main text of a webpage that we aim to extract  in content extraction literature  it is often referred as
gold text 

  

data collection

the dataset is extracted from several popular english  simplified chinese  and bengali news article websites on the
internet   the chaos dataset urls are collected from sun
 
the collected dataset is available at https   github com 
ziyan spider tree master data

fitable    collected dataset
website
language
  of
npr
english
prothom alo begali
qq
simplified chinese
sina
simplified chinese
techcrunch
english
the verge
english
usa today
english
chaos
mixed  includes rtl

webpages
  
  
  
  
  
  
  
   

et al     this dataset contains urls from diverse websites
across the internet  such as  personal blogs  articles  news
etc  see table   for some information about our dataset 
for all the dataset  we collected the urls and then extracted the webpage ourselves  as various existing dataset
often cleans up the html and removes css properties that
we need  each webpage is downloaded and rendered in a virtual webkit browser     restoring its original layout intended
for human audience  javascript is then injected into the
webpage to efficiently inspect dom elements  text enclosing blocks are identified  for each dom element containing
text  the algorithm finds its closest parent element that are
displayed as block  inline links and other text decorators are
not individually identified  instead  their common parent element  which might be a paragraph or a div  is identified as
a whole  see figure   
for each chosen block  a set of features are then extracted 
including the size and position of the block  the contained
text  the font configurations  color  line height  tag path  etc 
in fact  there are over     different css properties for each
block  our algorithm automatically extracts those with a
non default value  therefore  the number of features varies
from block to block in the data collection phase 

   automatic labeling
    clustering blocks
the cluster shape of similar blocks on a webpage  i e  navigational buttons  are not necessarily well shaped  i e  spherical   also there can be random noises that appears on some
of the webpages  furthermore  it is unclear that how many
clusters will there be  since it depends heavily on the design and layout of the webpage  therefore  we have chosen
dbscan     density based spatial clustering of applications
with noise  as our clustering algorithm  this algorithm handles the problems of unknown number of clusters  unknown
shape of clusters and noise quite nicely 
the clustering result shows that  the body text of the articles
across multiple webpages  which usually consist of multiple
blocks  are clustered together in a single cluster  the navigational links are also clustered together  as well as the links
in the footers  other common elements  like similar sidebar 
advertisement  comments  etc   are also successfully clustered  given all the visual features represented by the css
properties  the algorithm is quite effective in discerning the
different visual elements on the webpages 

figure    webpage rendered by virtual webkit
browser with text blocks identified by red boxes

fitable    separate classifier
website
precision
npr
       
prothom alo
       
qq
       
sina
       
techcrunch
       
the verge
       
usa today
       

   

for different website
recall
f 
               
               
               
               
               
               
               

labeling clustered block

although all the blocks have been clustered quite precisely
based on their visual properties  it is not trivial to find out
which cluster contains the content text  the dbscan algorithm views clusters as areas of high density separated by
areas of low density  cluster number varies from webpage to
webpage  and the cluster containing content can be any of
those  for our dataset  the number of output clusters ranges
from a few clusters to above    
fortunately  most websites have some metadata stored in
meta tags in order to accommodate web crawlers  so we
can extract the description of an article by parsing the meta
tags in the webpage  the description usually contains a brief
summary or just the first few words of the article content 
with that  we are able to calculate the similarity between
each cluster and the description using the longest common sub sequence  lcs  algorithm  the longer the common sub sequence  we call this number relevance score  
the more likely is the cluster to contain the content text 
note that we used lcs instead of term frequency or inverse
document frequency to ensure the language independence 
as word segmentation varies from language to language 
at first we tried to automatically label the blocks by finding
the best cluster local to each webpage on a website  after training the svm on this labeling  we found that for
some websites  the precision on the test set was not ideal 
on closer examination  we found that on some rare pages 
the best cluster according to that webpages description is
comment instead of the main text  to fix this issue  we implemented algorithm   which scores the blocks across the
entire website  rather than working locally on a single webpage  
score    array of zeros with length equal to   of clusters 
for each cluster i of a website do
for each block j under that cluster do
score i     score i    relevance score of block j 
end
end
pick the cluster c  with the highest similarity score 
label all the blocks of in the same cluster c as   
label all other blocks as   
algorithm    labeling from global score in a website
this algorithm performs very accurately and removes the
occasional labeling hiccups on a single webpage 

table    testing on previously unseen websites
dataset precision
recall
f 
chaos
                    

table    using random mixture of webpages
dataset precision
recall
f 
chaos
                    

  

svm and cross validation

using the collected and labeled text blocks  the web content extraction problem can be formulated as a classification problem  where the goal content consists of multiple
text blocks that are classified as content while the other
text blocks are classified as non content 
we have constructed a support vector classifier with a linear
kernel to perform text block classification  due to the imbalance between the number of content blocks and the number
of non content blocks on a webpage  we applied class weights
to give our positive  content  examples higher weights 
we have employed three different approaches to the classification problem  for each approach  we have performed
  fold cross validation to evaluate its performance 
in our first approach  we have trained separate classifier for
different website  for each given website  the collected webpages are shuffled then divided into four groups  one of
the groups is chosen as evaluation group  while the other
three are used as training examples  intuitively  the support
vector classifier in this case tries to learn the underlying
structure and template used by the particular website  this
approach worked very well  see table   
in our second approach  we have trained one classifier model
using webpages from multiple websites in the chaos dataset
and then tested the model on webpages from the rest of the
dataset  the classifier in this case learns some structures
from only a subset of websites then tries to generalize the
model on previously unseen websites  this approach produced the worst results  see table    it make sense that
specific visual characteristics of one website rarely appears
the same in another website 
and lastly  we have trained one classifier model using random webpages chosen from the chaos dataset then tested the
model on the rest of the dataset  in this approach  since the
webpages used in training are picked randomly  our training
examples included a wide variety of websites  therefore  it
is likely that the classifier have previously seen at least one
example webpage per website for the websites in the evaluation set  this approach worked much better than the second
approach  see table   
the results from the third approach has shown that our algorithm is able to learn and generalize more than one template
at a time  combined with results from the first approach 
these results indicate that our algorithm will work well for
any website including websites using multiple different templates as long as the classifier is trained on a representative

fitable    using text lengths as features
website
precision
recall
f 
npr
                    
prothom alo
                    
qq
                    
sina
                    
techcrunch
                    
the verge
                    
usa today
                    

table    using tag paths as features
website
precision
recall
f 
npr
              
      
prothom alo
                       
qq
              
      
sina
              
      
techcrunch
                       
the verge
              
      
usa today
                       

mixture of example webpages 

  

feature selection

in order to develop deeper insights into our classification
problem  we experimented with different feature sets 

   

text length

a common naive approach to web content extraction is to
find the longest contiguous block of text in a webpage  the
intuition is that paragraphs in an article are usually long 
to establish some baseline  weve experimented with using
only text length as feature  the text length of each text
block is simply the number of non whitespace characters in
the block  we normalize this number by scaling it to have
a zero mean and a variance of one 
the results show that this approach works for some websites
but performs quite poorly for others  see table    due to
the variation of paragraph length in the real world  text
length is not a very good determinator of article content 

   

tag path

the second approach is to use tag path of the block  tag
path is simply the path it takes to navigate the dom tree
from its root to the text block  it consists of element names
for each node along the path  for example  a paragraph
inside a div container in the body of the html webpage
would have a tag path like body   div   p  we treat
each different tag path uniquely and vectorized this discrete
feature 
the results show that this approach works quite well in general but fails for some particular website  we found that
in the failure cases  the tag paths are indeed indistinguishable for content and some non content text blocks  therefore  this feature alone cannot sufficiently distinguish the
two classes of blocks  see table   

table    using css selectors as features
website
precision
recall
f 
npr
                       
prothom alo
       
      
      
qq
              
      
sina
              
      
techcrunch
                       
the verge
       
      
      
usa today
       
      
      

table    using css visual
website
precision
npr
       
prothom alo
       
qq
       
sina
       
techcrunch
      
the verge
       
usa today
       

   

properties as features
recall
f 
               
               
               
               
       
      
               
               

css selectors

css selectors are essentially decorated tag paths  we have
incorporated css class name in addition to the element
name into the css selectors  these selectors have the potential to discriminate previously indistinguishable text blocks 
thus allowing the classifier extract more concise template
information for a given website 
the results shows that this approach produces improvement
for most of the websites compared to the tag path approach 
but also introduces some regressions for other  see table   
upon closer examination  we learned that some css selectors can be overly unique and tied only to a particular text
block  its idiosyncrasy presents a challenge for the classification model to generalize to other text blocks 

   

css properties

the third approach is to use all visual related css properties  color  font size  font style  line height etc   as features 
these features are also treated as discrete features and vectorized 
the results show that similar to tag path and css selectors 
these features work quite well in general but not ideal in
some rare cases  especially for websites that do not visually
distinguish content with other parts of the webpage  these
visual features alone are not sufficient  see table   

  

conclusion

we have developed a language independent pipeline to extract web content  our pipeline collects data  labels examples  trains support vector classifier  and evaluates learned
model in an automated manner  our learning algorithm can
achieve perfect labeling when trained on a single website 
even for websites with multiple different templates  by analyzing features  we have found that some of our featurestag
path  css selectorscontributed to the near perfect classification results in many websites  but they also fail in some
cases  css visual properties work particularly well across

fimost websites  this reinforces the intuitive reasoning about
how a person identifies the content in a website  tag path
and css selectors are technical details of presenting html
documents  which often matches with human intuitive reasoning but may break in times  see table   

  

future work

sun et al     showed the interesting idea of density sum in
his paper  density sum performs very well in chaos dataset
 f  score          it would be interesting to see how combination of density sum and css features perform  but we
need to refactor our code extensively to incorporate density
sum  so we were not able do it in the current time frame 

  

references

    diffbot  extract content from standard page types 
articles blog posts  front pages  image and product
pages  http   www diffbot com  
    phantomjs is a headless webkit scriptable with a
javascript api  http   www phantomjs org  
    readability turns any web page into a clean view for
reading now or later on your computer  smartphone  or
tablet  http   www readability com  
    m  ester  h  p  kriegel  j  sander  and x  xu  a
density based algorithm for discovering clusters in large
spatial databases with noise  in kdd  volume     pages
             
    a  h  f  laender  b  a  ribeiro neto  a  s  da silva 
and j  s  teixeira  a brief survey of web data extraction
tools  in acm sigmod record  pages            
    g  miao  j  tatemura  w  p  hsiung  a  sawires  and
l  e  moser  extracting data records from the web
using tag path clustering  in proceedings of the   th
international conference on world wide web  pages
        april      
    f  sun  d  song  and l  liao  dom based content
extraction via text density  in proceedings of the   th
international acm sigir conference on research and
development in information retrieval  pages        
acm       

fi