applications of machine learning on keyword
extraction of large datasets

 
 

meng yan
my    stanford edu

 

abstract

 
 
 
 
 
 
  

given a large text dataset composed of greater than    k training sets and
multiple classifications  various machine learning algorithms were used to
train and predict tags and keywords  this project also explores various
techniques in pruning and managing a large  unwieldy dataset in order to
produce a practical training point  nave bayes and svm are the algorithms
focused on in this project  with various contours of the dataset tested to
examine the practical effects of dataset manipulation 

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

 

in trod u cti on

keyword extraction is a common problem that exists today due to the rapid growth of data
available online and in databases  in order to make such a large body of information readily
accessible to the general public  it is pertinent that accurate and effici ent text classification
algorithms are used to sort and index this information 
for this project  i utilize a large set of data provided by kaggle from their competition
facebook recruiting iii  keyword extraction  this competition involves training on an  
gb training set to predict the tags for a     gb testing set  broken down  the data consists of
roughly   million training examples with   million testing examples  and a total of      
different tags  with the possibility of multiple tags associated with a single example  as
such  the data management and computer workload management is a nontrivial task  for the
purposes of this project  i decided to limit myself to    k examples from the data set 
training on the    most prevalent tags 
first  the raw data used in this project will need to be processed and pruned into a
manageable and useable format  this will involve applying various techniques learned in
class  such as stemming  stop words  and filter feature selection  after parsing the data 
nave bayes and support vector machine algorithms will model the data and be compared
against each other for both accuracy and computation time  finally  different contours of the
data will be explored to see the effects on accuracy and computation time  in hopes of
finding an approach that can be extrapolated to the entire    gb data set 

 

dataset man i p u l ation

each of the training sets contains four components  index  title  body  and tags  the tags are
based on information derived from the title and body  which are plaintext components of
arbitrary length  based on a rough analysis of only the first        trai ning examples  there
are around     million unique words in their titles and bodies alone  with        unique tags 
clearly  the data must be pruned in order to create a usable dataset 
in this project  only the first    k training examples were used and categorized based on the

fi  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

top ten most used tags  c   java  php  javascript  android  jquery  c    python  iphone 
asp net   simple cross validation was used  with a         split between training and
testing examples  the idea is that the time to test each model for the full set of data will be
roughly proportional to the smaller test set used here  the time to train the models  however 
will drastically increase with the size of the dictionary  fortunately  the actual act of training
has to be done only once  and can be prepared in advance  as such  computation time will
examine mainly testing time with the models as opposed to training time 
the following pruning techniques were applied to create a manageable dataset 
  only title information was used  and body information was ignored as it drastically
increased the dictionary size 
  non alphanumeric characters were removed 
  all characters became lower case 
  repeat instances of a word in a tag or body were removed  as there are no plans for
multinomial event modeling 
  solely numeric words were removed  ex  cs    still remains a valid word  but     
is pruned  
  word stemming was applied when applicable 
  stop words were pruned away from the title and body 
to handle stemming and stop words  the natural language toolkit  nltk      for python
was used  stop words provided through the nltk libraries were used as a baseline  and
additional words were added to the list to handle word and symbol fragmentation due to the
initial parsing  ex   p  is pruned away to p  
filter feature selection has the potential of greatly reducing the dictionary of unnecessary
words  as is easily applicable  for instance  in the set of   million training examples  there
are instances where words in the dictionary are not used more that       times  a
straightforward way to parse the dictionary would be to set a lower threshold in which the
occurrence of a word must exceed before being used as a feature  in depth analysis is
required to understand what a reasonable threshold would be  however  without filter feature
selection  using the title and body information  even with the pruning  will still produce a
dictionary that is too large to process  as such  two contours of the data will be explored 
one with no filter feature selection  dictionary size of       for the    k data set   and one
with a filter feature selection threshold of     dictionary size of       for the    k data set  

 

na ve b ayes

using nltk     nave bayes classifier  with a    k training set and   k test set and no
filter feature selection  i obtained the following results for the top ten most used tags 
tag
c 
java
php
javascript
android
jquery
c  
python
iphone
asp net

positive
examples
     
     
     
     
     
    
    
    
    
    

negative
examples
      
      
      
      
      
      
      
      
      
      

accuracy
       
       
       
       
       
       
       
       
       
       

testing time
 seconds 
     
     
     
     
     
     
     
     
     
     

  

table     naive bayes baseline  no filter feature selection        dictionary size

  

the total training time for nave bayes without filter feature selection is        seconds 

fi  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   

note a separate model per tag was needed  as we cannot apply the discretized nave bayes
algorithm in this instance  that is  the tags are not mutually exclusive  a t raining example
has the possibility of being tagged with multiple tags  a cursory glance at the results
suggests that nave bayes is a good predictor even with only using the title as the dictionary
 and completely ignoring the body information   below are the results after applying filter
feature selection  note that the number of positive and negative examples remains constant  
tag

accuracy

c 
java
php
javascript
android
jquery
c  
python
iphone
asp net

       
       
       
      
       
     
      
       
       
      

increase in
accuracy    
         
        
        
         
         
         
         
         
         
         

testing time
 seconds 
     
     
     
     
    
     
     
     
     
    

speed up    
        
        
        
        
        
        
        
        
        
        

table     naive bayes  filter feature threshold           word dictionary

the total training time for nave bayes with a filter feature selection threshold of    is
       seconds 
it is surprising to see that accuracy slightly increased with filter feature selection  which
implies the low frequency words did not have any prediction value  at least for the selected
tags  and noise was effectively removed by filter feature selection  as for testing time and
total training time  the timing methodology was to simply compute the execution time to call
the classification function of the nave bayes algorithm  though the same machine was
used for all runs  not all transient variables were accounted for  as such  the compute time
comparisons serve only as a rough estimate  therefore  between the two different nave
bayes contours  their testing times were roughly equivalent  though the smaller dictionary in
the filtered nave bayes run did produce a noticeably faster training time compared to the
baseline 

 

s vm

below are the results from the baseline svm results with no filter feature selection  the
total training time for svm with no filter feature selection was        seconds  the data is
compared against the nave bayes baseline 
tag

accuracy

c 
java
php
javascript
android
jquery
c  
python
iphone
asp net

       
       
       
      
      
       
       
       
       
       

increase in
accuracy    
         
         
         
         
         
         
         
        
         
         

testing time
 seconds 
     
     
     
     
      
     
     
     
     
     

speed up    
       
        
        
        
        
        
        
        
        
        

   

table    svm baseline  no filter feature selection        word dictionary

   

to perform svm  the liblinear library was used  v       this first involved converting the

fi   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

python generated datasets into a matlab friendly format  and then formatting the data into
sparse matrixes to pass into the train and predict functions  while the conversion to python
to matlab format is not counted against the training time for svm  the creation of the sparse
matrixes is  as such  there is a dramatic increase in training time compared against the nave
bayes baseline  almost   times as long   mainly due to the un optimized generation of the
large sparse input matrixes 
however  the testing times are orders of magnitude faster than that of nave bayes  granted 
some of the speed increases may be due to the nature of the python nave bayes and c based
svm implementations  but in general  the efficiency of svm can clearly be seen in the
comparison  extrapolate the results of the baseline algorithms to the full data set  assuming
nave bayes average test time is       seconds and svms average test time is      
seconds  for       separate tags classifications  it would take nave bayes roughly   days
       hours  to classify all tags while svm would only take      minutes  note that this is
only on       test points too  for the full test set of   million data points  the actual testing
computation time may be drastically higher  so much so that nave bayes is infeasible  as
such  for very large data sets  svm is clearly the optimal solution  regardles s of its increased
training time 
filter feature selection was also performed for svm  below are the results  the comparisons
are against the svm baseline  the total training time for filter feature selection with a
threshold of    is          seconds 
tag

accuracy

c 
java
php
javascript
android
jquery
c  
python
iphone
asp net

      
       
       
       
      
       
       
       
       
       

increase in
accuracy    
          
          
          
          
          
          
          
          
         
          

testing time
 seconds 
      
      
     
     
      
      
      
      
      
      

speed up    
        
        
        
        
        
        
        
        
        
       

   

table   svm  filter feature threshold           word dictionary

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

as opposed to the nave bayes results  the filter feature threshold of    word occurrences
caused a decrease in svms accuracy compared against the svm baseline  however  svm
with filter feature selection is still more accurate that nave bayes  the testing times roughly
stayed the same  as such  adding filter feature selection for svm decreased the training time
by roughly      but also decreased accuracy by roughly    overall  given the situation and
circumstances  the drop in accuracy may be reasonable for the faster model generation 

 

fu tu re i mp rove me n ts

the use of filter feature selection at a word frequency threshold of    reduced the dictionary
size roughly by half  however  a dictionary of       words is still very large for a training
set of    k examples  also  as seen with filter feature selection on nave bayes  a higher
threshold may still be used to further reduce the dictionary size without any negative impact
on accuracy  this implies that further reductions of the dictionary may be desired  to
achieve this  principle component analysis seems like the best choice  not only will this
remove the trial and error approach associated with guessing a proper threshold for filter
feature selection  but it will also remove frequent redundant features that will be ignored by
solely using filter feature selection 

fi   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

another improvement would involve exploring ways to reduce the training time for the data
set  this is especially pertinent for svm  as the creation of the sparse matrixes for liblinear
is a very time consuming process  though this goes beyond the scope of this project 
exploring efficient sparse matrix generation algorithms is a practical consideration that is
definitely needed in real world usages of keyword extraction 

 

con cl u si on

keyword extraction on large data sets introduces a large number of practical issues which
render certain machine learning algorithms more desirable than others  in particular  svm is
seen as a much faster and feasible method of keyword extraction compared to nave bayes 
however  this is only true for very large data sets  as the initial cost of training svm is much
greater than that of nave bayes  to reduce training times  various methods of pruning away
unnecessary information can be applied and formalized  such as filter feature selection and
pca  as such  with a properly managed feature set and efficient algorithm  keyword
extraction can be performed accurately and within a reasonable amount of time and with
fairly inexpensive hardware 

 

ci tati on s

bird  steven  edward loper  and ewan klein        
natural language processing with python  oreilly media inc 
r  e  fan  k  w  change  c  j  hsieh  c  r  wang  and c  j  lin  liblinear  a library
for large linear classification  journal of machine learning research                    
software available at http   www csie ntu edu tw  cjlin liblinear
facebook recruiting iii  keyword extraction  kaggle         web 
http   www kaggle com c facebook recruiting iii keyword extraction

fi