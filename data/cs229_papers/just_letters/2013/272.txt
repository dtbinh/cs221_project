reconstruction of human voice for impersonation 
final report
amritha raghunath  amrithar stanford edu 
gunaa arumugam veerapandian  avgunaa stanford edu 
vignesh ganapathi subramanian  vigansub stanford edu 
   november      

fiintroduction
the project we are doing deals with the idea of reconstructing of human voice  as we had mentioned
in the project proposal  the aim is to learn the voice of an existing voice  and then to be able to
convert any given sound input  into that particular voice  this idea consists of   voices  one of which
is the source voice  and the second being the target voice  the target voice is the voice in which form
we try to observe the required input  the source voice is the voice which contains the information
that we need to have reconstructed 
our implementation consists of three major stages  filter analysis  voice defiltering and voice
conversion  the broad outline of each of these methods is as follows  in the first stage  we find using
machine learning techniques such as minimizing the mean squared error  the components unique to
a human voice  and this is what we call the human voice filter  in the second stage  we use speech
signal processing techniques such as the z transform  to obtain the speech content from the given
speech signal  by defiltering the unique voice content of the particular human voice  in the third
and final stage  we now pass this defiltered voice into the human voice filter of the target voice  and
obtain the final speech in the target voice 
a rough idea of this is given below in the form of a block diagram 
gu  z 

v  z 

s z 

in the above block diagram  the filter v n  refers to the discrete time filter which models the
human voice  and the filter gu n  refers to the discrete time input which saves the words and other
sounds in speech in some form  the output s n  consists of the exact speech samples recorded by us 
now  by some basic discrete time filter analysis ideas  we know that s n    gu n   v n   this is the
basis behind our three stages 

linear regression techniques on a z transform implementation 
in this section  we talk about a popular technique for voice conversion  which uses plain linear
regression as a tool  here  the filter is a plain filter employing the idea of a ztransform  and we
use linear regression to determine the coefficients of this filter  this method has been explained in
detail 

filter analysis
in this stage  we compute the feature values obtained by minimizing the mean squared error  this is
p
done as follows  we try to minimize the minmum mean squared error of s n   ki   s n  i ai   since
minimizing this would leave us with the value of gu n   which consists of the speech content in the
p
pk
 
speech signal  so  we try to minimize the value of n
n    s n   k  
i   ai s n   k     i     this is
done by the standard machine learning technique of getting the vector a    x t x    x t y   where
x and y are obtained from trying to optimize the above summation  so  in this part  we obtain the
values of ai   which are our features  and the human voice filter is ready 

 

fivoice defiltering
in this stage  we first convert the various signals to their z transforms  in the z domain  we get
 
  therefore  we have
gu  z v  z    s z   from the previous section  we know that v  z    a z 
gu  z    a z s z   so  that gives us gu n    s n   a n   and therefore  we are able to obtain the
defiltered speech  which contains words and other related information 

voice conversion
this is the final stage of our project  here  we use the fact that the defiltered speech of various people
speaking the same stuff must be identical  that gives us gu  z    a   z s   z    a   z s   z   since
we know a   z   a   z  and s   z   we can obtain s   z   which effectively models the speech content
in the target voice 
we have been able to obtain a dictionary of around     input voice samples  this dictionary
has been replicated to compare with output observations  the linear regression output currently
is extremely noisy  and we are trying to better it  or obtain a different machine learning algorithm
which helps us minimize the mean squared error all the same 
we implemented this approach  and observed that the output we obtain from this method is
not only very noisy  but also has a strong influence of the source speaker on the target voice  one
implication of this result is that the filter is not really a linear one  also  we do not consider the
effect of causality on the output voice  we try to implement these ideas in the following approach 

auto regressive techniques for voice conversion
this technique implements ideas of auto regression on stationary time frames  the auto regression
is also customized to the properties of the time frame we consider  this is explained in detail below 

stage     dynamic time warping
dynamic time warping is an algorithm for measuring similarity between two temporal sequences
which may vary in time or speed  in our problem  we use this algorithm to resize both sound
sequences to the same size  when we are given the source and target training data  we use this
technique to resize the size of the source and target data frames  to a uniform value  which is used
for further processing purposes  henceforth any usage of the words source and target sound frames
would refer to the dynamically time warped versions of the frames 

stage     kmeans clustering
in our technique  we use the fact that sound samples are stationary for relatively small time frames 
this is justified by the fact that for small time frames  of the order of   ms  the sound does not
really vary much  each of the frames would have auto regressive techniques performed on them  as
would be explained in the next subsection  we try to cluster each of these frames into one of many
cluster sets  this clustering is done by the kmeans clustering algorithm  this is an unsupervised
algorithm  on the dimension which is the number of frames  it is to be noted that this clustering
process would take a fair bit of time  which would cause our training time to shoot up  but the
advantage is that the training time would be amortized over our use of the machine for testing  and
so it does not really matter to spend time on training 
 

fithe initial idea here was to use clustering based on gaussian mixture models  this would have
been extremely time efficient  but it turns out that the source speech is not normally distributed 
and this sort of a model did not really provide us with an output matching expectations 

stage     auto regression for time frame
we use the auto regression technique on stationary frames  the auto regression technique proposes
that output samples are dependent on a few previous output and input samples  p and q samples to
be more specific   this uses a feedback from output to determine the future output samples  the
following block diagram models the auto regression in the zdomain 
i z 

g z 

o z 

a z 

the input is given by the input sound sequence i n   and the output is given by the output
sound sequence o n   the above diagram can further be written in equation form as follows  o n   
pp
pq
i   ai o n  i   
j   gj i n  j   this is the auto regressive equation we use for training and output
generation on a stationary sound frame  in the above block diagram  the ztransform is the input
p
transform filter is given by g z    qi   gi z i   similarly  the ztransform of the feedback transform
p
filter is given by a z    pi   ai z i   the effective transform filter from the input voice to the output
g z 
voice is given by   a z 
  our aim is to find these coefficients ai   gj for    i  p     j  q  this is
planned to be accomplished by using linear regression on each of these time frames 

stage     training phase
the training phase consists of two stages   clustering and obtaining coefficients  once we are get the
input samples of source voice i n  and target voice o n   the first stage requires us to cluster the input
sound frames into a fixed number of clusters  this is done because if we do not cluster the input
sound frames into a fixed number of clusters  the coefficients we obtain for each sound frame would
be different  and storing all the coefficients would become an issue  clustering is helpful because
we need to store coefficients for only the various clusters  and not for every frame irrespective of its
properties 
in this first stage  we do the clustering by using the plain k clusters algorithm  each of the
input vectors is a vector of size      which keeps the time frame of length lesser than   ms  thereby
justifying our assumption of stationarity   and so the clustering algorithm is of dimension      this
is time consuming  but justified in the second section where we talk about amortizing the time spent
on training clustering over the various testing data 
the final stage of the training phase is to find the auto regressive coefficients of each of the
clusters  each of the auto regressive equations of each cluster  across time frames  are put together
in the form of a linear equation x   y  where  refers to the auto regressive coefficients  this can
be obtained by a least squares approximation using the formula  x t x   x t y  which is the solution
to the linear regression equation 
 

fithis phase is crucial because we need enough data samples to obtain a tractable output  if the
number of data samples are low  the output might have a noisy quality  the main idea here is the
two pronged approach to machine learning by initially applying a kmeans clustering followed by
linear regression to obtain the actual mime output  it is also to be noted that we do not influence
the output in any other manner since the kmeans clustering is an unsupervised learning method 

stage     testing phase
the testing phase  similar to the training phase  consists of two stages  once the source speakers
voice sample is obtained  it is first split into stationary time samples  as in the training phase  these
stationary time samples  in the first stage of our testing phase  are detected to be part of a cluster 
among the set of clusters obtained in the training phase  the cluster obtained is then further used
to obtain the output in the next stage 
the second stage consists of predicting the output frame given the cluster the input frame belongs
to  once the cluster has been obtained  we pull out the coefficients corresponding to the cluster  and
use it to linearly generate the samples which mimic the output 
here  to obtain the first few samples  we pad the output vector with the previous output samples
 this doesnt really matter since we are anyway considering stationarity to be present  and using p
extra previous samples from the output shouldnt typically affect our output   this ends our testing
phase  and the obtained output across frames can be concatenated to give the required output 

analysis
the outputs obtained from the various techniques we employ have to be compared  the output
obtained from the first method  which uses plain linear regression clearly has a very poor voice
quality  which has a strong influence of the voice tone of the source speaker  therefore  we do not
provide any measures of comparing that with the actual speech of the target speaker 
the output obtained from the auto regression technique can be compared to the actual speech
of the target speaker  this can be done as follows  we employ the log mean squared error of
the actual speech to the target speaker as an error metric for measuring how well  or badly  our
p
 o i o i   
algorithm performs  this is given by log  n
   where o n  is an estimate of our target
i  
n
speaker obtained through the algorithm  and n refers to the total number of voice samples of the
output signal  this is expected to be as low as possible  and preferably lesser than    for a favorable
output 
the two main features we use in our algorithm are p and q  the number of output and input
delay elements affecting the output sample  in our problem  we try varying the values of p and q
and observe how that affects the output observed  and the error metric 
the error metric  which is the log mean squared error  is varied with three parameters primarily 
those being the number of clusters n   the auto regressive components p and the delay components
q  figure  a plots the log mean squared error varying with the number of delay coefficients q  as q
increases  the log mean squared error reduces  and this makes intuitive sense  as we have more data
to predict the future output samples  and that explains the reduction in error  figure  b shows the
variation of log mean squared error with the number of clusters  the increase in error with number
of clusters might at first seem unintuitive  but it is true because we seem to be overcorrecting for the
number of phonemes  the ideal cluster value would be around     where we have all the phenomes

 

fi a  log mean squared error versus the number of feed  b  the log mean squared error versus the number of
back coefficients p for varying values of delay coeffi clusters for fixed number of auto regressive coefficients
cients q 
p q

figure    results of analysis
covered for sure by the sample data  and we have also made up for any missed data in the    necessary
phonemes 

future work to be done
the machine learning algorithm has to be smoothened  so as to observe lesser noise in the output 
this algorithm can be extremely potent if cleaned up so as to reduce the noise content in the speech 
another factor that has to be looked into is to better the voice quality  the current output sounds
more like a growl in the voice of the target speaker  while it is aimed to get the target speakers voice
to be mimed as speaking properly 
the work could also be used eventually to break into voice recognition systems  and for that kind
of an implementation  it is necessary that the output voice quality and timbre match identically that
of the target speaker  for that to happen  we need to drastically tighten the algorithm up  so as to
obtain a voice conversion system close to perfection 

references
    ye  h  and s  young         perceptually weighted linear transformations for voice conversion  eurospeech       geneva 
    ganvit y lokhandwala  ma and bhatt  ns         implementation and overall performance
evaluation of voice morphing based on psola algorithm  international journal of advanced
engineering technology

 

fi