predicting time spent with physician

jim zheng
jimzheng stanford edu
stanford university  computer science dept       serra mall  stanford  ca       usa
ioannis  yannis  petousis
petousis stanford edu
stanford university  electrical engineering dept       serra mall  stanford  ca       usa
ye  scott  cheng
scottcheng stanford edu
stanford university  computer science dept       serra mall  stanford  ca       usa

abstract
the goal of this project is to predict how
much time a physician should be spending with a patient  given the patients injury  referral status  and other reported  nonidentifiable medical information  this may
be used as an additional factor for measuring
and predicting physician efficiency alongside
existing methodology  we implement linear
regression and a variety of classification techniques on a national dataset of emergency
room  er  patients and outpatients  we
use cross validation to select the optimum
set of features  in addition  we implement
the k means algorithm to generate new features for our supervised learning algorithms 
our final models provide good predictions of
time spent with physician with a mean error
of around   minutes 

   introduction
current literature on measuring clinical efficiency measure cost and patient feedback efficiency  both of which
have been shown to be correlated with a patients
length of stay  kroch et al          among the many
subdivisions of the length of stay  one of the most important for quality patient care is how much time a
patient spends with a physician  techniques in current
literature for predicting length of stay include multilabel  multi class for supervised learning  specifically 
length of stay is transformed into a binary labeling of
short or long with the separation between short
and long set specific to the study  snyder        
studies also try to discretize by hourly intervals and
use decision trees to discretize bins  existing studies
do not attempt  however  to directly predict length of

stay in minutes  possibly because there is no need for
such precision in existing studies where the variation
in length of stay is high  the goal of our project is
to predict  given a set of features that describe the
circumstances of a patient visit  how much time  in
minutes  a physician is likely to spend with them 

   data and features
our data comes from the national ambulatory medical care survey  collected by the national center
for health statistics  this annual survey samples
physician patient interactions inside hospital er and
outpatient departments across the us  data is collected via a predefined sampling method provided by
the center for disease control  cdc  that spans
a wide variety of regions and clinical environments
 namcs       namcs        this is a rich dataset
with over        rows per annual report  dated from
     back to        and over     features recorded 
     raw data structure
some of the features captured by the dataset include 






age of patient
day of week the appointment took place
is the physician the patients primary physician
major reason for visit
location  metropolitan or not  and which us region 
 has a computer for viewing patient info
 drug type prescribed
 patients body mass index  bmi 
its worth noting that some of these features are continuous  age of patient  bmi   some are discrete  multivalued  day of week  drug type   and some are boolean
 e g  is primary physician   this meant that the type

fipredicting time spent with physician

of regression that we ran could operate only on a subset of all features  and extensive feature processing was
necessary to obtain the best set of features 
with respect to time spent  several facts are also noteworthy  first  time with physician is rounded to be
between   and     minutes  records with times spent
over     minutes are capped at      second  since
entries are manually recorded  times are frequently arbitrarily rounded to the nearest   minutes 
     preprocessing
data was collected by downloading and extracting a
set of features listed in the documentation for each
year  each dataset contained a slightly different set of
features  this required us to search for a set of features
that stayed consistent from year to year 
binomial smoothing  after running some early
data analysis  we saw that quite a few features that
were semantically binomial  e g  is patient referred 
were in fact represented as multinomials with values
    to indicate types of errors in the recording process 
for these features  we unified error values as    so that
  indicates existence  and   indicates that the feature
was either not present or untrue for the patient 
label smoothing  we also realized that the distribution of the time spent with physician field was skewed
towards large values at multiples of   minutes  and
large dropoffs for non multiples of    this reflects the
fact that a good amount of time  on the field recording
of time spent with physician is estimated and rounded
to the nearest   minutes  to mitigate this concern  for
logistic regression  we discretize time with physician
into multiples of    to smooth out our dataset  we
also found that more than     of the data examples
have one or more invalid feature values  which are generally unrecorded data  if we include these features
or training examples as they are  the invalid values
would largely disturb the linear models  we came up
with two strategies to deal with this issue  in the first
approach  we take out all the examples with invalid
values  and only train and test on the examples where
all values are valid  this way  we make sure that all
the data examples being used are real and meaningful  in the other approach  instead of dropping invalid
examples  we try to fix them by filling in the average
value of the attribute over the entire dataset  we did
two things to improve the second approach  first  to
ensure the representativeness of the average attribute
values  we remove the features for which too many
examples have invalid values  for example  if     of
the examples have invalid values for a particular feature  we would not look at this feature  second  we

remove the data examples with relatively more invalid
attribute values  for example  if a data example has
more than      of invalid features  we consider it as
unreliable and therefore remove it from the dataset 
     unsupervised learning and feature
generation
we implemented the k means algorithm to identify if
the hospitals can be grouped into clusters based on
parameters such as 
 geographical region
 public or private practice
 percentage of income derived from the state   research institutions   private insurance   patient
payments
 the provision of electronic billing and electronic
prescriptions 
the number of centroids was varied and the total error
 i e  the sum of euclidean distances of each example
from its closest centroid  was calculated  we found
that k     and above gives a low error  we will
subsequently try to define new features by calculating
the euclidean distance of a new training example from
each of the clusters  the new features will be fed to
the supervised learning algorithms 
     principal component analysis
the principal component analysis  pca  algorithm
was used to exploit any dependencies in the input data
by projecting them on a lower dimensional space  the
number of dimensions was varied   they were then
fed into the supervised learning algorithms and the
predictive performance was compared to that with the
original data 

   linear regression models
our first approach is to apply linear regression models
on our dataset to predict the time with physician  the
prediction involves the following steps 
   incorporate custom features  including non linear
transformations and features learned from kmeans
   set up linear regression model
   select features using the linear model
we will now describe each step in more detail 
     adding custom features
to improve the results of linear model predictions  we
added some custom features to the dataset  the first

fipredicting time spent with physician

type of custom features is non linear transformations
of continuous features  since linear regression models only look at linear relationships between the target
variable and feature  in order to reflect non linear relationships in the data in linear ways so that linear
models can capture them  we apply non linear transformations on the continuous feature values of data examples  specifically  for continuous features  e g  age 
height  weight   if the value is x  we add the following
values
as additional features to the example  x    x   
 
 
x  x 

   classification

another class of features that we add to the data for
linear models are learned from k means clustering 
similar method of combining supervised and unsupervised learning is adopted in previous works  coates
et al          specifically  after computing k clusters
out of the dataset  we add k additional features to
each data example  where each feature represents the
examples distance to a cluster centroid 

we used the following algorithms in our analysis 
multinomial naive bayes  weighted logistic regression  random forest classifier  forward feature selection  univariate feature selection  using anova
f value as a scoring function   we implemented naive
bayes and forward feature selection by ourselves using numpy  we used pythons scikit learn library  pedregosa et al         for all other algorithms  weighted
logistic regression is done by scikit learn using an one
vs all  ova  approach  rather than true multiclass
logistic regression 

     linear regression models
we applied four types of linear regression models  linear regression  ridge regression  lasso regression  and
elastic net  we use basic linear regression as a baseline  and try different variations to see which one fits
the data best  in particular  since there are so many
features in the dataset  we use    features for linear
regression models  which does not include custom features from non linear transformation and k means  
chances are that not all of the features are relevant to
the target variable  therefore we try lasso regression
which predicts sparse coefficients 
in ridge regression  lasso regression and elastic net 
we determine the parameters using cross validation 
for example  in ridge regression  we try to optimize
minw   xw  y         w      and we set  by running
cross validation using ridge regression model with different values of  and take the optimal value 
we will see the details of how these models perform in
section   
     feature selection
we apply feature selection before training the linear
model  which prevents overfitting and makes training
and predicting faster  specifically  we apply cross validation to determine the number of feature to maintain 
and use recursive feature elimination to choose the set
of features to use in our model 

our second major approach for supervised learning
was to discretize physician time into bins  first  we
designated a number of bins and a distribution of labels in each bin  and ran feature selection to reduce our
feature set  next  we iteratively refined our features by
smoothing  combining certain common features  and
generating new ones via unsupervised learning techniques to get better predictions  we also experimented
with other classifiers to try and eliminate feature bias 

     naive bayes
as an initial benchmark  we implemented discretized 
multinomial naive bayes with laplace smoothing  we
trained our model on the      dataset and tested using the      dataset  and initially chose our number
of bins to be     i e  our interval was     and ranged
from   to      naive bayes was quick to implement and
gave us a first baseline for error  moreover  the naive
bayes assumption is a fair assumption to make for a
large subset of our features  since the dataset combines patient  physician  region  and facility information  all of which are expected to be weakly correlated 
for discretized naive bayes  our approach can approximate the use of the true probability density function
given the appropriate selection of interval and number
of classes contained in each interval  yang   webb 
      
naive bayes yielded poor initial performance
 accuracy         on the initial set of features for
the      dataset  predictions were skewed right  even
though the dataset was skewed left  using feature
selection eliminated the drugtype features  which 
when included  were responsible for the right bias 
our own implementation of forward feature selection
 selecting n      features  produced an accuracy of
about       we suspected that this had to do with
a latent bias in the features we were selecting for 
to test this suspicion  we ran scikit learns random
forest classifier on the initial set of features and
produced an accuracy score of       this prompted

fipredicting time spent with physician

us to focus on feature generation as a next step in
improving classification accuracy 
     feature generation and error analysis
since initial benchmarks gave rather poor performance  we attempted to gain better insight into the
features  we found that many features were unable
to distinguish between multiple labeling  since there
was much noise  thus  we eliminated these features
altogether  next  since we were uncertain about the
target number of features for forward feature selection 
we opted for filter feature selection using the anova
f test statistic as our scoring function  using this type
of feature selection  we reduced our analysis down to
only four features  feeding this narrowed feature set
into naive bayes boosted our accuracy to       and
random forests accuracy to      
residual analysis showed we were predicting only three
distinct intervals                and       bucket  this
meant that we were mispredicting the              and
       which accounted for about     of our data 
here  we tried a combination of techniques to reduce
this bias and boost accuracy with varying degrees of
success 
combining binary features  we tried to combine
several features into a binomial feature in order to create better separation of labelings among our feature
set  this boosted our accuracy by      
combine train and testing dataset  in an effort to
reduce randomization  we combined the      and     
datasets into one dataset  and randomly chose a      
split for our training and testing datasets  respectively 
this yielded a change in accuracy of about      on
average  depending on the permutation 
weighted logistic regression  logistic regression
was tried with class weights of          hresti     in an
effort to reduce overfitting to the       bucket  this
decreased accuracy by about       as true positives
with the       labeling were now being mispredicted 
binary labeling  our final attempt was to discretize
times into only two buckets  and by hour  according to
existing literature  using these weaker predictions  we
were predictably able to improve our performance to
over     accuracy with labelings 

   results
this section discusses our results from both linear regression models and classification models  as a baseline  in our dataset  if we predict all the data examples
using the mean of target variable  the resulting mean

include k means features
linear regression
ridge regression

include
    
    

not include
    
    

table    linear regression models  mae  minutes  from
   fold cross validation  k    

absolute error  mae  is      minutes 
     linear regression models results
linear regression models are able to make predictions
with an mae of around     minutes  we now discuss the different parameters and factors that affect
the performance of linear regression models 
we first compare the performance of different linear
models  lasso regression and elastic net try to establish the model on fewer features  while we initially
thought this could work well since our data has a large
number of features  not all of which are directly relevant to the target variable  it turns out that lasso
regression and elastic net yield poor performance  for
the other two linear regression models we tried  ridge
regression produces better results than linear regression 
we found that including features learned from kmeans dramatically improves the performance of linear regression  the improvement is particularly significant when k is relatively large  table   compares the
results with features learned from k means and the
results without these features 
     classification results
table   compares the results of different classification
models  where bucket size is set to    minutes  as
mentioned in section    increasing bucket size will significantly improve classification accuracy  but we are
more interested in more fine grained predictions 

multinomial nb
random forest
logistic regression

accuracy
 percent 
    
    
    

mae
 minutes 
    
    
    

table    classification  accuracy and mae

for classification  it was difficult to see how accuracy
correlated with strength of prediction for a continuous
variable  we wanted to see  for example  whether our
classifier made lots of mispredictions that were off by
   and a few off by    vs consistently off by     we
thus used mae as a metric of performance in classifi 

fiminutes

minutes

ratio

predicting time spent with physician

test error

test error

test error
training error

training error
training error

m

m

figure    ridge regression 
error and test error over m

training

m

figure    multinomial naive bayes 
training error and test error over m in
terms of accuracy

cation 
m

  x
  yi    yi   e yi    yi  
m i  
in addition  we assume a roughly gaussian distribution over the bins  which is a roughly good approximation since most data points are concentrated in multiples of     thus  the expectation is the mean of the
bin 

   conclusion and future work
our study represents a first attempt at directly trying
to predict time spent with physician  to minutes accuracy  our attempts at making stronger predictions
with linear regression yielded an average error that was
better than the average error obtained from predicting
the mean  in addition  if we weaken our predictions
to match those found in current literature  we are able
to obtain over     accuracy in classifying time spent
with physician 
further studies could continue improving upon our
classification and regression techniques by investigating the following 
interval and interval distribution selection  we
experimented with several interval numbers of varying
weakness  the weakest we tried was   minutes intervals  the most     hourly   we also tried to restrain
and randomly sample examples that fell into each
bucket to ensure an even distribution  a more principled approach would have automated over a range of
possible intervals and samplings and calculated scorings for each pair of  interval  sampling  
training on larger dataset at the time of testing  we were unable to access documentation for the
datasets starting from      to       which prevented

figure    multinomial naive bayes 
training error and test error over m in
terms of mae

us from gathering more training data  using more
training data could potentially improve our linear regression predictions  since our linear regression model
eliminated mostly null rows and thus had only about
    of the original training set with which to train 

references
coates  adam  ng  andrew y  and lee  honglak  an
analysis of single layer networks in unsupervised feature learning  in international conference on artificial intelligence and statistics  pp               
kroch  e a   duan  m   silow carroll  s   and meyer 
j  a  hospital performance management improvement  trends in quality and efficiency       
namcs       national ambulatory medical care survey       
namcs       national ambulatory medical care survey       
pedregosa  fabian  varoquaux  gael  gramfort 
alexandre  michel  vincent  thirion  bertrand 
grisel  olivier  blondel  mathieu  prettenhofer  peter  weiss  ron  dubourg  vincent  et al  scikitlearn  machine learning in python  the journal of
machine learning research                    
pope  g c  and kautter  j  health care financing review  pp             
snyder  ashley m  data mining and visualization  real
time predictions and pattern discovery in hospital
emergency rooms and immigration data  technical
report  dtic document       
yang  ying and webb  geoffrey i  discretization for
naive bayes learning  managing discretization bias
and variance  machine learning                   

fi