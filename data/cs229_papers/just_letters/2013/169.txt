subhalo prediction for dark matter halos
pinnaree tea mangkornpan  dennis wang  marc williamson
abstract  we explore machine learning techniques for predicting the number of dark matter subhalos  nsub  
belonging to a main halo using only those characteristics of the parent halo that are independent of subhalos  such
a model can be used to save computation time by enabling accurate predictions of nsub from lower resolution
simulations  due to a lack of previous work in this area  we thoroughly test multiple types of machine learning
models  these include variants of linear regression  lr   a support vector regression  svr   and a linear regression
trees  lrt   we find that locally weighted linear regression with a novel use of principle component analysis performs
the best out of all the models  obtaining an average test error of about    

   introduction

   physics background

in the past few decades  cosmology has become an extremely active area of research with landmark surveys
such as wfirst  lsst  euclid  and des  one of the
more prominent areas of research in the field is the so
called missing satellite problem  the missing satellite
problem refers to the dierence between the large number of dark matter subhalos that are predicted to belong
to main halos  and the much smaller observed number of
galaxy satellites     this disagreement indicates either a
lack of precision in current observational techniques  or
a flaw in physicists understanding of the current models
of the universe  in this paper  we investigate the possibility of using machine learning to predict the number
of subhalos belonging to a dark matter halo in order to
gain insight into the missing satellite problem and to
improve computational eciency in current simulation
techniques 
in section    we will provide background information
on dark matter and the simulations that are used to
study dark matter halos  this background will illuminate the connection between subhalos and the missing
satellite problem  providing motivation for certain design choices that we made in our machine learning models  section   will briefly describe the dataset that we
used for our models  in section    we will describe in detail the machine learning algorithms that we used  this
will include motivation for each type of model  along with
preliminary results in order to obtain a basic idea of the
eectiveness of each algorithm  section   will detail two
methods of feature selection that we applied to the models in section    in section    we will present an analysis of weighted linear regression and detail how principle
component analysis   pca   can be applied to improve
the model  section   will be dedicated to fully presenting
our results  with a focus on the algorithm that performed
the best at predicting nsub   finally  in section    we will
analyze our results and discuss applications of the work 
along with possible future directions to take this research 

dark matter composes     of the energy density of
the universe  compared to only      that is contributed
by baryonic matter     in order to understand the universe  it is necessary to understand dark matter  although it is unknown what exactly dark matter is  physicists can infer some of its properties  physicists model
dark matter as very weakly interacting particles  that
only interact gravitationally  in the early universe  the
density distribution of dark matter was approximately
uniform  with small density perturbations  as the universe evolved  the slightly overdense regions attracted
more particles  eventually growing into dark matter halos  which are generally orders of magnitude more massive than galaxies  dark matter halos thus create large
gravitational potential wells which baryonic matter falls
into  thus forming galaxies  each large dark matter halo
hosts one galaxy  in addition to having many smaller subhalos that are gravitationally bound to the main halo    
these subhalos are host to galaxy satellites  like the magellanic clouds  this connection between subhalos and
galaxy satellites is why studying dark matter subhalos a
promising approach to solving the missing satellite problem 
in order to study dark matter halos and subhalos 
physicists use n body simulations to model the entire
evolution of the universe     one of the most important properties of these simulations is the resolution  a
simulation will be run using a fixed number of particles 
accordingly  each particle must be given a mass in order
to accurately simulate the entire universe  if more particles are used  then the individual particle mass is lower 
allowing physicists to see smaller structures in their simulations  if a simulation is run with a resolution chosen to
study main halos  the obtained nsub might not be representative of the actual number of subhalos  which can be
orders of magnitude smaller in mass than the parent halo 
because the low resolution might not allow such small
structures to form  therefore  in order to accurately

fisubhalo prediction for dark matter halos

find nsub   it might be necessary to use a much higher following matrix
 
resolution than is required for studying the main halo 

running these high resolution simulations can be com 
  
putationally expensive and therefore limit the amount of
x  
 
available data  thus  it is desirable to have a predictive
 
model for nsub that only depends on properties of the

parent halo 

 

definitions   
x   
x   
  
 
x m 

t
t

t

 

 
  
 
 
 


 

 
 
y   
 

y    
y    
  
 
y  m 

 
 
 
 
 

     unweighted linear regression  unweighted
linear regression simply fits a straight line to the rhap   dataset
sody data  we used the standard ordinary least squares
we used the results of the rhapsody  k simula  regression model with the cost function   
m
 
tions to train and test our machine learning models      
  x    i  
j    
h x
y  i 
we obtained this data courtesy of professor risa wech  i  
sler at stanford university  the rhapsody simulation contains    cluster sized dark matter halos  with this model is extremely ecient to implement because
virial mass in the range of mvir              h   m   it has a normal equation  which optimizes  in closed
each halo was simulated at two dierent resolutions  form  unsurprisingly  unweighted linear regression is not
      particles  corresponding to a resolution of      a very good model for nsub   it has training error of   
    h   m   and       particles  corresponding to a res  and a high test error      see the table in section    
olution of          h   m      we used the results of the we hypothesize that linear regression underfits the data
high resolution simulation   k  so that our models would because it cannot fit nonlinear relationships between the
have the most accurate training data  in the data  we features  we check this hypothesis by using principle
predict the feature n  vmax         vmax       is one component analysis to reduce the entire dataset  withcriteria that is used for defining a subhalo  and it is the out the output feature  to one dimension  and plotting
most inclusive such definition in the rhapsody simu  the scatter between nsub and the reduced dimensional
lation  the rhapsody simulation keeps track of over data  there is no scatter  which indicates there is no
    halo features  however we had to eliminate about simplistic model for nsub  
half of these features due to an implicit dependence on
knowing the number of subhalos  using these features      locally weighted linear regression  locally
in our models would defeat the purpose of creating the weighted linear regression  lwlr  combines the simplicity of linear regression with the ability to model nonlinear
model in the first place 
overall behavior in a data set  this made it our next logical step up complexity from linear regression  intuitively 
given a testing instance  lwr weights the training ex   machine learning models
amples so that training instances closer to the testing
we chose to use regression based models instead of instance are more important than those farther away 
discretizing our data and using classification because we the definition of closer is arbitrary  to start  we use
wanted to build a model with the lowest possible test standard euclidean distance in the high dimensional feaerror  first  we implement a simple unweighted lin  ture space  the weight  w i  corresponding to training
ear regression in order to get a feel for the data  we example x i  is gaussian   
then move on to locally weighted linear regression to in  
x x i 
crease the complexity of our model  for these models 
 i 
w   exp
we also explore two dierent types of feature selection 
   
forward search  a standard algorithm   and tree search
 defined later   finally  we use principle component anal  where  is a parameter that describes the size of the
ysis  pca  to modify our models  in order to train and region where highly weighted training instances can extest our models  we use the technique of cross validation  ist  small  corresponds to only considering very close
splitting the data into two groups  where     of the data training examples  while large  corresponds to a larger
is used for training  and the remaining     is used to test range  thus  smaller  models nonlinear behavior betthe model  the training and testing subsets were ran  ter  we experimentally determined         to be the
domly selected from the    halos  and error ages reflect ideal value  and the preliminary results were       training error and        test error  see table in section    
average values from several trials 
this is a significant improvement over unweighted linear regression in both testing and training error  but the
     notation  x i    y  i  is a training example  where test error is still unacceptably high  at this point  we
i                    and x i  is the feature vector while y  i  hypothesize that we are overfitting the data due to the
is the output variable that we are predicting  we use the large number of features  this is addressed in section   

fisubhalo prediction for dark matter halos

     support vector regression  we use the generalization of the support vector machine for data classification to continuous outputs in order to build a support vector regression model to compare with out linear
regression     this model finds a function  f   that has
at most  deviation from each training example  this
amounts to solving the primal problem   
 
minimize k w k 
 
 


y  i 
w  x i 
b 

subject to 
 i 
 i 
w  x
 b y


we use the liblinear implementation of svr  with l regularized  l  loss svr to solve the dual problem    
the results of the svr model are almost as good as using locally weighted linear regression  but the test error
of        is higher than desired  this paper will focus on
using and modifying locally weighted linear regression  so
we do not fully analyze the possibility of using svr  our
further research in this area will involve a much more rigorous treatment of svr as a possible machine learning
model 
   feature selection
we implement two forms of feature selection  standard forward search and a simple method based on linear regression trees  the advantages of feature selection
are two fold  first  it ameliorates overfitting and reduces
test error  second  it helps physicists understand which
halo characteristics are most influential in determining
the number of subhalos 
     forward search  the forward search feature selection algorithm works by greedily choosing features
that yield models with the best test error performance 
the algorithm begins with an empty list of chosen features  it then iterates over all unchosen features  adding
each separately to the running list of chosen features and
training a corresponding model  the feature that yields
the best model  ie lowest test error  is permanently added
to the list of chosen features  this algorithm is repeated
until the desired number of features has been reached 
for our models  we chose to use    of the roughly   
features 
     tree search  this method of feature selection
refers to using a regression tree to select the best predictive features from the data     in order to predict
an output for an input vector  the algorithm starts at
a root node and follows decision branches until a leaf
node is reached  the leaf node contains the prediction
for the input vector  at each node in the tree  a feature
from the data is selected  and two branches are made
leaving the node  the branches have an associated decision rule  for example take the left branch if xj   c  but
take the right branch otherwise  the regression tree is
constructed from the training data where the outputs are

 

known  there are three characteristics of regression trees
that require further explanation  the feature selection 
the optimization criterion  and the stopping rule     we
utilized the matlab implementation of regression trees 
so we will focus on matlabs specific implementation 
at each node  a feature needs to be chosen from the
data  in order to do this  every binary split on every
feature is considered  and the feature with the best optimization criterion is selected     the optimization criterion refers to how the tree chooses a nodes associated
decision  in matlabs implementation  the split is chosen
in order to minimize the mean squared error of predictions on the training data  the stopping rule refers to
the condition that stops a node from branching  for
regression  this is when the mean squared error of the
nodes prediction is less than some tolerance level proportional to the mean squared error for that response in
the entire data set  as a regression model  we found that
the regression tree did not perform very well  most likely
because it bucketized the training data into the nodes of
the tree  which requires a large dataset in order to be
most accurate  however  the regression tree did identify
a set of    features as the most predictive of nsub   so we
compared this set of features against standard forward
searching for    features  using weighted linear regression with forward search we had the following results 
test error           training error          using the
tree search  we had test error          and training error      forward search performs much better on the
test set of data  so we will use it as our method of feature
selection 
     results of feature selection  having identified
forward search as the better method of feature selection  we apply it to our locally weighted linear regression
model  the results are much improved  with an average
of       test error and      training error 
   improving lwlr with pca
in locally weighted linear regression  each training instance  x i    is assigned a weight  w i    based on the euclidean distance between the test case  x and x i    however  this presents a problem because the euclidean distance scales upwards with the number of features in x
and x i    specifically   
d i 

 
 

k x x i  k 
v
u k 
 
ux
 i 
t
xj x
j

j  

where k is the dimension of the data  this is a significant problem for weighted linear regression because if
the distances between points scales upwards  then the
tau parameter would need to be scaled upwards in order to keep the model at an optimal fit  during forward search feature selection  features are chosen based
on making a lwlr model with the current number of

fisubhalo prediction for dark matter halos

features  and adding the feature that produces the best
model  so k is changing  however  we cannot vary 
during feature selection  since we want to pick a single
tau value and then find the best model  to address this
problem  we use a modified definition of distance that is
independent of the number of features in order to calculate the weights for lwlr  given a dataset  x  we
use pca to reduce all of the data to a one dimensional
representation  x  then each x i    x corresponds to
x i    x  we split the dataset x into training and testing subsets  and we put the corresponding xs in training
and testing groups  then the distance between x and
x i  from x corresponds to the distance between x and
x i  from x  we have   
d i     x x i   

 

which has the desired behavior of being independent of
the number of features in the original dataset  x  the
new weights for lwlr are then   
w

 i 

  exp

x

x i 
   

  

the results of this lwlr model using pca were  test
error          and training error           adding
pca to our model helps performance on the test set 
but it causes the training error to increase  compared to
the lwlr without pca model  the increase in training
error can be attributed to a loss of information in computing the euclidean distance from the one dimensional
dataset resulting from pca 

   results
the following table organizes the results of each of the machine learning models that we used on the rhapsody
dataset 

ml model
unweighted lr
weighted lr
weighted lr
weighted lr
weighted lr
svr
regression tree

pca fwd search
no
no
no
no
yes
no
no
yes
yes
yes
no
yes
no
no

   analysis
the above table represents a roughly chronological
ordering of the machine learning algorithms that we
used  the results involving the various types of linear
regression match our expectations  specifically that unweighted lr has the worst performance due to its simplicity  and the various algorithmic improvements that
we implemented   feature selection and pca distance 
improve the test error compared to the basic locally
weighted linear regression  surprisingly  support vector regression does not perform better than lwlr with

test error   train error   tau
     
    
n a
     
    
    
     
    
    
    
    
    
    
     
    
     
     
n a
     
    
n a

pca and feature selection  although the focus of this
project is mainly modifying lwlr  so it is possible that
further work with svr models could yield better results 
one important feature of the results is that modifying
the distance in lwlr with pca causes training error
to increase from the lwlr method with only feature
selection  the explanation for this increase is discussed
in section    despite this increase in training error  we
believe that lwlr with pca and feature selection is
a better model for predicting nsub because it has lower
test error  and it represents a more reasonable trade o
between training and test error 

fisubhalo prediction for dark matter halos

the above plot shows the results for a sample run of
lwlr with pca and feature selection  one interesting question is whether or not the model is biased  this
would manifest itself in a scatter plot of the error versus
the true number of subhalos  for example  if the model
consistently underestimates nsub when nsub is higher 
then the error is biased  this would indicate that there
is behavior that our model is failing to accurately predict  this analysis is one course of action that we will
take in pursuing our research 
the features that were identified as the best predictors of the number of subhalos  listed best to worst 
are  rvir  vpeak  bcgdom st  vdfv   sigmav  alpha step  log  mvir  vdfp  pvalign dm   pvalign vmax  
b a  pvalign vpeak   gammanfw like  c a  and b a  v  
some of these features are expected to be good predictors  for example  rvir is a measure of the size of a halo 

 

and log  mvir measures its mass  it is unsurprising that
the size of a halo is helpful in determining how many
subhalos become gravitationally bound to it  however 
we do not know exactly how these chosen features interact  nor do we fully understand all of them  one focus
of future our future research will be pursuing a better
physical understanding of the features and subhalos 
   acknowledgements
we would like to thank professor risa wechsler for
her guidance in helping us choose this topic of research 
we would also like to thank her graduate student  yao
mao  for his help in explaining the more dicult physics
concepts  finally  we warmly thank professor andrew
ng for providing us with the machine learning skills necessary to conduct this research 

    references
    h y  wu et al   rhapsody  i  structural properties and formation history from a
statistical sample of re simulated cluster size halos  kavli institute for particle astrophysics
and cosmology  stanford  ca  sept       
    h y  wu et al   rhapsody  ii  subhalo properties and the impact of tidal stripping
from a statistical sample of cluster size halos  kavli institute for particle astrophysics and
cosmology  stanford  ca  mar       
    a j  smola and b  sch olkopf  a tutorial on support vector regression  sept       
    mathworks  classification trees and regression trees  online  
available  http   www mathworks com help stats classification trees and regression trees html bsxg kc 
    c j  lin  liblinear  online   available  http   www csie ntu edu tw  cjlin liblinear 
    x  xu et al   a first look at creating mock catalogs with machine learning techniques  mar       
    j  bullock  notes on the missing satellites problem  uc irvine  sept       
    b  griswold  what is the universe made of   online   available  http   map gsfc nasa gov universe uni matter html 

fi