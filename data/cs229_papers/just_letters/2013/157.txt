 

live handwriting recognition
using language semantics
nikhil lele  ben mildenhall
 nlele  bmild  cs stanford edu

i  i ntroduction
we present an unsupervised method for interpreting a
users handwriting using language semantics  we collect
time series data of a users handwriting using a tablet
and create clusters of the written symbols  or glyphs  
then we use language semantics to determine the most
likely written text 
ii  l anguage s emantics and
s ubstitution c iphers
we depart from the norm by not using prior data
about the absolute shapes of characters  nor comparing
a users input to previous samples of handwriting from
that user or any other user  we have the user input a
sentence of glyphs and spaces s    g    g            gm  
that represents some natural language phrase  usually
      words   given that the user is writing with
alphabet a in language l  our ultimate goal is to
recover the users intended mapping from glyphs to
characters in a using only a sample corpus of text
written in l 
in order to do this  we first use a comparison function
to find clusters of glyphs we think should map to
the same character in a  if the true mapping from
these clusters to characters in the original sentence
is bijective  then we have reduced the problem to
solving a traditional substitution cipher  a substitution
cipher is an archaic technique for encrypting messages
where the key is some permutation of the characters
in the alphabet  all characters in the original message
are mapped to their image in the permutation  for
example  the phrase the cat in the hat could be
encoded as can iec th can aec using the permutation
 a  c  e  h  i  n  t    e  i  n  a  t  h  c  
a substitution cipher retains much of the semantic
information of the original phrase in the form of letter
frequencies and the relative positions of characters in
each word in the phrase  this information is enough to

solve the cipher quickly and reliably for phrases of    
characters or more and to provide a list of very good
guesses for shorter phrases  speed and accuracy depend
on the length of the words and how many times letters
repeat in the phrase  
our process for analyzing input sentences involves the
following steps 
   record a sentence of n glyphs 
   evaluate the similarity of all n  pairs of glyphs 
   create k clusters of the most similar glyphs 
   solve the ciphertext corresponding to that clustering 
iii  r ecording g lyph data
we used a wacom tablet to collect sample data  each
data point is a single glyph  which is a vector of n
  tuples  ti   xi   yi   where ti is elapsed time since the
first point recorded in the glyph and pi    xi   yi   is a
two dimensional point in screen space  once the user
indicates they have finished inputting the glyph  the
time data is normalized so that t      and tn     
in addition  the glyph is scaled and translated so that
its centroid is at            and the point farthest from
the centroid lies on the perimeter of the square ranging
from        to        
we save this normalized data but run all comparison
operations on smoothed versions of each glyph 
smoothing is a two step process  first  we upsample
the glyph  each glyph has between       points when
recorded  and we resample the glyph at     evenly
spaced intervals from t     to    to upsample a glyph
g at time ti   i      we find the value of j between
  and n inclusive such that tj    ti  tj   we then
linearly interpolate the coordinates pj  and pj of those
points using t i    ti  tj     tj  tj    to get the
sample     t i  pj    t i pj for time ti  
after upsampling the glyph  we downsample to    
points by taking a weighted average of all samples in

fithe high resolutionpversion of the glyph  our sample
   
at ti   i     is
 ti    pj   the function
j   n  t
j
 
 
n  x    exp x           is the probability density of a normal distribution with mean   and variance
     used to weight the points closest in time to ti most
highly  we set         lower values of  preserve
more detail from the raw sample such as sharp turns
and twists in more pointy letters  but higher values
are more effective for smoothing out the noise in the
raw data  since we do not make note of when the pen is
lifted  smoothed glyphs usually interpolate some points
between pen up and pen down points  which connects
consecutive strokes in letters like t  x  j  etc 

b  spatial comparison
the spacial feature is simply the list of screen space
coordinates in a glyph  spatial comparison has meaning
since we normalize the glyphs to all have the same
centroid and diameter  in order to compare two glyphs
spatially  we sum together the squared distances between
points corresponding to the same time value in each
glyph  this feature works well to distinguish glyphs with
gross spatial differences  such as t and s  but is not as
successful at differentiating more spatially similar letters 
such as t and f 
c  curvature
the idea of matching glyphs using curvature was
taken from zitnick      the curvature feature is a list of
the change in direction at each point in the glyph  more
specifically  for some index i  it is the angle between
the displacement vectors pi    pi and pi  pi    to
compare two curvature lists  we add up the squared
difference of each pair of curvature values  we also
compared the integrated curvatures of glyphs  created
by accumulating the partial sums of the curvature lists 
the noisiness of the data and unpredictable behavior at
cusps in characters  like the point or turn at the top
of the stem of a d  made comparisons using curvature
more unreliable than the other two features 

iv  c omparing g lyphs
we tested multiple features for measuring the
similarity between glyphs  since we collected live data 
all of our features are time series  this allows for
easy comparison between glyphs  since we avoid the
expensive and tricky process of trying to match spatial
points in different glyphs 
this assumption means that any feature we use will
not correctly match two examples of the same character
drawn in different ways  if a user draws some glyphs
representing the character d starting from the loop and
some from the top of the stem  the matching in time
will not be the best possible matching in space  we will
address this problem in section vi 

v  m inimum s panning t ree c lustering
we use a linear combination of the features from
the previous section to create a symmetric cost
function c gi   gj   that takes in two glyphs gi and
gj and outputs a real number in the range       
c gi   gi       for all gi   and c gi   gj     c gi   gk  
if gi is more similar to gj than it is to gk   as measured
by our features 

a  shape contexts
the richest and most successful feature we use is the
shape context for each point in each glyph  as presented
by belongie  et al       the shape context of a particular
point p is a rough description of how the shape looks
when viewed from p  it is a representation of the polar
coordinates  ri   i   of the vector pi  p  i e  the distance
and angle from p to pi for all points pi in the glyph  we
store discretized histograms of these polar coordinates
as viewed from every point in the glyph  splitting the
data uniformly into   bins based on log r and    bins
based on   the values used in      

given this cost function  we can treat the sentence
as a fully connected graph with one glyph at each
vertex  where the edge between glyphs gi and gj has
weight c gi   gj    we then find the vector of edges
in the minimum spanning tree of this graph  sorted in
ascending order  to find k clusters of glyphs  we simply
remove the k    most expensive edges from the tree
and take the remaining connected components of the
graph as our clusters 

to compare the shape contexts of two glyphs 
we match the spatial points corresponding to times
t    t            t    in the smoothed versions of those glyphs 
as in      we use the    test statistic as a measure of
similarity between the i th histogram in the two glyphs 
the overall similarity of the glyphs is taken to be the
sum of the histogram comparisons at each time step 

we prefer using the minimum spanning tree over
k means for clustering for two reasons  first  averaging
feature vectors from multiple glyphs does not always
make sense  depending on the features  it certainly
 

fidoes not always produce a meaningful visual result 
in addition to averaging feature vectors  we tried
lumping together the space time points from all
glyphs in a cluster and using the weighted average
downsampling technique from section iii to produce
a cluster average  this method produces something
that usually looks like the glyphs in the cluster but
with its extremal points smoothed out  with either
scheme of averaging  the cost score of any glyph in
a cluster with the cluster average is disappointingly high 

have a low cost score  then they may end up in the
same cluster  we call this under segmenting the
sentence   in this case  solving the resulting ciphertext
is guaranteed to give an incorrect answer  because two
characters that should be different will remain the same
in the solution  it is also possible that the ciphertext
will have no solution at all in the language l  we wish
to avoid under segmentation at all costs 
if glyphs representing the same character have a high
cost score  then they may end up in different clusters
 we call this over segmenting the sentence   luckily 
over segmentation does not necessarily mean that the
resulting ciphertext will be incorrect or unsolvable  to
remedy this issue  we relax the assumption that the
mapping from clusters back to original characters needs
to be a bijection  this means we allow multiple clusters
to map back to the same character  for example  the
e glyphs might end up in two clusters instead of just
one  

by comparison  using min spanning trees for clustering preserves more information about the individual
glyphs  if any two glyphs are a strong match  have an
extremely low cost   then we probably want to pair them
in the same cluster  the min spanning tree approach
supports this intuition  since any edge of the graph with
an extremely low weight will appear in the min spanning
tree with high probability 

the calculated costs may be inaccurate either
because the cost function is bad or because the users
handwriting is inconsistent  consistency is not the same
as legibility  it is possible for a user to have illegible
but consistent writing  as mentioned in section iv  if
a user draws a character in two different ways  our
cost function will not cluster those two glyphs together 
allowing for over segmentation can allow these two
glyphs to still map back to the same character  at which
point the cipher solver can figure out that they should
map back to the same character 
the more we over segment  the longer it takes to
solve the cipher  for this reason  we start with a low
number of clusters          then try to solve the cipher 
we increase the number of clusters only if the cipher
is unsolvable  since this implies under segmentation 
typically the ciphertext is only solvable if we have at
most     more clusters than there are unique characters
in the original sentence  beyond that point  too much
semantic detail is lost to recover the original sentence in
a feasible amount of time 

fig     visualization of a minimum spanning tree for
a sample sentence  the    pink edges are the most
expensive  so they are cut to create    clusters  one of
these edges is between two f glyphs and another is
between two p s so there is slight over segmentation 

vii  s olving c iphertext
we initially based our ciphertext solving method on
hasinoffs paper      this process involves performing
a stochastic search over the space of all permutations
of the alphabet  the value of a particular permutation
key is the product of the probabilities of all resulting  grams of letters in the ciphertext translated with that key 

vi  a ddressing c lustering e rror
regardless of the clustering method  problems arise if
the calculated costs between glyphs are not completely
accurate  if glyphs representing different characters
 

fifig     symmetric cost matrices for the poison from a black widow spider is about fifteen times more potent
sample data  upper left  the raw costs of matching glyphs  i  j  using shape contexts  plotted with       in the
lower left corner  colors are scaled so that black is zero  spaces matched with any glyph are also black  and white
is the maximum cost of matching any two of the glyphs  upper right  the correct matching function  with black
where glyphs are the same and white where they are not  this clustering is achieved by our system using mst
clustering with    clusters  lower left  mst clustering using    clusters  demonstrating under segmentation  red
entries where different glyphs are incorrectly clustered together  lower right  mst clustering using    clusters 
demonstrating over segmentation  green entries are where glyphs representing the same character were not clustered
together 

 

fiwe implemented a simple stochastic search to
solve substitution ciphers that worked but was slow 
inconsistent and unable to solve over segmented ciphers 
we wanted a procedure guaranteed to find a solution to
the cipher if it existed  so instead we used a deterministic
brute force search over all possible solutions 

glyphs   the clustering depends on the cost function 
so it is also quite good 
there is the most room for improvement in the
cipher solver  the solver should be probabilistic
rather than deterministic  and should not need clusters
as input  instead  it should use the cost matrix to
probabilistically guess which glyphs should correspond
to the same character  then the cipher solver itself
could adjust when it thinks some pair of glyphs has
been incorrectly matched  rather than having to wait for
over segmentation to cut the incorrect edge  this would
make the cipher solver robust against an imperfect cost
function  paragraphs of handwriting could be solved
extremely reliably because of the high amount of
semantic information they contain 

we use a recursive procedure that takes a cipher with
n words already fixed and loops over all possibilities
for the  n      th word  the procedure chooses which
word to loop over next based on which unsolved
word in the ciphertext has the fewest matches in
the vocabulary of language l  we sort the possible
matches in descending order by frequency  so the most
common words are always tried first  for each word
in the list of possible matches  we fix that word in
place and modify the lists of possible matches for all
other unsolved words to reflect the updated cluster to
character mapping  then recursively call the procedure
again  if some word in the ciphertext has no possible
matches in l  the procedure recognizes this partial
solution as a dead end and terminates  if all words in the
cipher have been fixed  we score the solution and save it 

another step to make our system more generally
applicable would be to segment entire words into
characters  currently  the user indicates the end of each
glyph  this is an artificial constraint  since many people
connect some of the letters in their handwriting  the
ability to segment written words would make the system
much more useful 

to score solutions  we use the sum of the frequencies
of single words and pairs of words     and   word grams 
in the solution  frequency data is extracted from a corpus
of sample text written using language l  this helps the
cipher solver return the most likely solution  assuming
the user wrote a sentence with some semantic meaning
rather than a list of unrelated words 

it may seem somewhat contrived to perform handwriting recognition while completely ignoring the significance of the absolute shapes of characters  our goal
was to demonstrate the power of semantic analysis alone 
one advantage of using semantic analysis only is that it
is entirely independent of language  our method would
work for any language whose alphabet has a number
of characters similar to english  it would not work for
chinese or japanese since there would not be enough
repetition of characters in a single sentence  more generally  semantic analysis could be added to existing handwriting recognition systems to increase their accuracy
when recognizing semantically meaningful passages of
writing 

viii  r esults   i ssues   and f uture w ork
we implemented all parts of this system from scratch
in c    using opengl to collect the data and visualize
the intermediate results  we used the american national
corpus to collect data about the frequency of words
and word pairs in english 
the system as a whole is not yet amenable to
mass testing  first and foremost  we did not manage to
acquire any dataset appropriate to the particular problem
we were trying to solve  also  since the system relies
on three components  comparing  clustering  and cipher
solving   it will break if any one of them fails  so far 
it has succeeded in reading the five data sets we have
made  all similar in length to the poison from a black
widow spider     

r eferences
    s  belongie  j  malik  and j  puzicha  shape context  a new
descriptor for shape matching and object recognition  in advances
in neural information processing systems  vol      mit press 
      proceedings paper  pp         
    c  l  zitnick  handwriting beautification using token means 
acm transactions on graphics  vol      no          
    s  hasinoff  solving substitution ciphers  department of computer science  university of toronto  tech  rep        

we know that the cost function using shape contexts
is very accurate         testing error on large sets of
 

fi