prediction of user intent to reply to incoming emails

andy bromberg
kevin shutzberg

abromberg stanford edu
jkevin stanford edu

abstract
we aim to develop a practical solution to predict whether a user will reply to incoming
emails by designing a model that draws on
insights derived from classical machine learning algorithms and their basis in statistical
methods and convex optimization  our algorithm is optimized to handle the specific
challenge of user email organization by incorporating heuristics about key behaviors and
consequences 

   introduction
email overload is an increasingly serious problem in
todays society  as our inboxes become more and more
saturated  it is becoming important to have methods
and technologies in place to help us parse the mountain
on email we face every morning 
so far  there have been three attempts at solutions to
this general problem  first is the implementation of
spam filters  which is a very well understood problem
at this point  pantel   lin         second is the use
of software to categorize emails by topic  which ultimately doesnt answer the question of what a user
should focus on  dredze et al    the third is the use
of simple heuristics to tell the user whats important 
but these typically incorporate just a couple features
and often arent personalized  aberdeen et al   
we aim to take steps towards solving this overarching
problem by focusing on a component of it  we get a
lot of email that isnt spam but still doesnt warant a
reply  while we  hopefully  read most of the emails in
our inbox  we only reply to a small fraction of those 
in this project  we create a system and algorithms to
isolate and highlight those emails so that a user can
take action on what needs a response quickly and efficiently 

   dataset representation
we gathered a corpus of received personal emails  from
a single account  and labeled them with whether they
had been replied to  yi      or not replied to  yi      
we represented all the emails as feature vectors x  
 xi   where each xi represents one of the features we
extracted  as follows 
feature
sender
number of words
question marks in subject
question marks in body
links
attachments
sent to me  not ccd 

representation
unique id 
integer
tf idf  weight
tf idf  weight
tf idf  weight
      
      

table    initial feature representations

we then discretized all of our features in order to allow
them to work easily with our machine learning algorithms  for the features that were real valued initially 
we binned them by dividing them up into deciles  so
our final features were as follows 
feature
sender
number of words
question marks in subject
question marks in body
links
attachments
sent to me  not ccd 

representation
             n  
                  
                  
                  
                  
      
      

table    final feature representations
 
we assigned all senders from whom we hadnt previously received an email a sender id of     this served to
gather data about how we generally respond to emails from
unknown senders with whom we have no history 
 
tf idf weight is the term frequency inverse document
frequency  used to measure the abundance or scarcity of an
item in a document in a corpus relative to its presence in
the other documents in that corpus 

fiprediction of user intent to reply to incoming emails

   methodology
in this section  we detail our benchmark classifiers and
then the process by which we designed our own algorithm  as well as our general methodology  the results
are presented in section   where we evaluate all of the
algorithms relative and absolute performance 
     features
the features we outlined in section   are accessible
and calculable from the individual email documents
we gathered from the email account  the sender
ids are assigned sequentially and chronologically  with
the exception of one off senders  as detailed above  
the number of words is simply the count of words in
the body of the email  and subsequently binned by
deciles   the features requiring tf idf scores can be
calculated  as in algorithm   and binned by deciles 
the presence of attachments and the presence of the
recipient in the to  field is extracted from the email
metadata 
algorithm   tf idf calculation
input  corpus  d  of documents  dj   with list  t  
of terms  ti  
for ti in t do
idf  ti   d    log count d  count d with ti   
for dj in d do
tf  ti   dj     count ti in dj  
tf idf  ti   dj   d    tf  ti   dj    idf  ti   d 
return tf idf  ti   dj   d 
end for
end for

our other algorithms 
     support vector machine classifier
the support vector machine classifier is a nonprobabilistic linear classifier that creates the hyperplane in the feature space that best divides the training data  cortes   vapnik         as our data was
not separable with either a linear or a gaussian kernel  we had to use a regularized version of the svm
algorithm 
initially  we tried an svm algorithms with gaussian
and linear kernels to provide a baseline for our developed algorithms  which would also be based on svms  
we used a base regularization parameter of c      
for all of our svm based algorithms 
     asymmetric support vector machine
after getting some baseline results  we realized a key
heuristic for this particular problem  which is that false
positives are better than false negatives  i e  theres
low marginal cost for a user to read an irrelevant email
but high marginal cost for a user to miss an important
email  and as such  we should optimize for recall  possibly at the expense of precision and accuracy 
to do this  we changed the cost function of the svm
algorithm to penalize false negatives more than false
positives  the original  regularized  svm algorithm
is formulated as 

m

minimize
w b 

     naves bayes classifier
the nave bayes classifier is a probabilistic classifier predicated on bayes theorem  the usage of a
nave bayes classifier assumes that each feature in
the dataset is conditionally independent of each other
feature  mitchell         we are comfortable making
this assumption because of our usage of frequencies as
opposed to absolute counts  that is  if we were using
absolute counts  the count of questions marks would
certainly be conditionally independent on the length of
the email  however  with our use of frequencies  the
length of a given email should not affect the feature
measuring question marks or links 
we chose to use nave bayes as our baseline algorithm
against which we could measure the performance of
 
where n is the number of senders who sent mail to the
inbox  this varied depending on our initial training set 

x
 
i
kwk    c
 
i  

subject to y  i   wt x i    b      i              m

   

i     i              m
in its primal form  our asymmetrically regularized
svm is formulated as 

minimize
w b 

x
 
kwk    cf n
i   c f p
 
 i 
i y

  

x

i

i y  i    

subject to y  i   wt x i    b      i   i              m
i     i              m
   
we let cf n      p
cf p to perform the penalization  we
constrained c   i ci  m   c       so that we have
the same amount of total regularization as we did in
our previous svms  the corresponding dual problem
is 

fiprediction of user intent to reply to incoming emails

     svm with linear kernel

minimize


t q 

m
x

i

i  

precision        
recall        
accuracy        

subject to i     i              m
i  ci   i              m
qij   y  i  y  j  k x i    x j   

where

ci   cf n  y  i         cf p  y  i      
   
to perform our a svm with a linear kernel  we simply
solved  the primal form of the problem on our dataset 
but to perform the a svm with a gaussian kernel  we
had to use the dual form of the problem  to allow us
to swap out the kernel   in practice  the dual problem
was much more computationally complex  to the point
where it was not practical to solve using our entire
dataset  given our limited time and resources  instead 
we trained on a smaller portion of our dataset 

predicted

positive
negative

actual
positive negative
   
   
   
    

table    support vector machine  linear kernel  confusion matrix

     svm with gaussian kernel
precision        
recall        
accuracy        

     reducing feature input rank
we then moved to futher optimize our model by using
pca to reduce the rank of our feature inputs  this
served to eliminate noise from the data as well as dramatically increase the speed of our algorithms 
initially  the matrix describing our   feature dataset
had a condition number of           after dropping
the three lowest singular values  which were roughly
an order of magnitude below the others   we had a
condition number of          we then re ran our asvm algorithms with gaussian and linear kernels and
the modified feature inputs 

   results

predicted

positive
negative

actual
positive negative
   
  
   
    

table    support vector machine  gaussian kernel  confusion matrix

     asymmetric svm with linear kernel
precision        
recall        
accuracy        

     nave bayes
precision        
recall        
accuracy        

predicted

positive
negative

predicted

actual
positive negative
   
   
   
    

positive
negative

actual
positive negative
   
   
  
    

table    asymmetric support vector machine  linear
kernel  confusion matrix

     asymmetric svm with gaussian kernel
table    nave bayes confusion matrix

 
we used cvx to solve all of our convex optimization
problems  grant   boyd        

precision        
recall        
accuracy        

fiprediction of user intent to reply to incoming emails

predicted

positive
negative

actual
positive negative
   
   
   
    

table    asymmetric support vector machine  gaussian
kernel  confusion matrix

     asymmetric svm with linear kernel and
reduced rank feature inputs

asvm l pca

precision        
recall        
accuracy        
asvm g pca

predicted

positive
negative

actual
positive negative
   
   
  
    

asvm l

table    asymmetric support vector machine  linear
kernel  pca  confusion matrix

     asymmetric svm with gaussian kernel
and reduced rank feature inputs

asvm g

precision        
recall        
accuracy        
svm l

predicted

positive
negative

actual
positive negative
   
    
   
    

svm g

table    asymmetric support vector machine  gaussian
kernel  pca  confusion matrix

     overall results

precision
recall
accuracy

nb

a graph of our results for all seven algorithms is in
figure   
 

   discussion
in the end  the value in this project came from optimizing our algorithms for our specific use case of classifying emails as to be replied to or not  while the use
of libraries like scikit learn or nltk to implement
models like normal nave bayes or svm can get decent results  they are limited in that they only tackle
a small set of specific objective functions  while these
functions have widespread applications  in our case  we

  

  

  

percentage
figure    results for all   algorithms

  

fiprediction of user intent to reply to incoming emails

could dramatically improve them by specific designing
an algorithm for our problem 
we crafted our own machine learning algorithms using the general support vector machine paradigm and
the principles of convex optimization to create an objective function matched to the task at hand  our
ability to do so came from knowledge about how the
svm algorithm functions on conceptual and mathematical levels  we were then able to implement these
algorithms using cvx  grant   boyd        
finally  we were able to use some unsupervised learning to condition the feature space by applying principal component analysis  this marginally improved
our performance on our tasks  but it also greatly improved the runtime of our algorithms due to the reduced rank of the feature inputs 
with these three steps  we hit upon mathematical and
conceptual understanding of both supervised and unsupervised learning and were able to get excellent performance on our key metric  accuracy  

   future work
our primary extension to this would would be the
application of asymmetric penalization  to favor false
positives  to other algorithms such as nave bayes 
wed love to see the results of these improvements
as compared to our asymmetric support vector machines 
we would also like to examine a larger dataset  this
would give us a better sense for which features are commonly important  we would still segregate the larger
dataset by receiver inbox  and simply train multiple
instances of our model and then analyze the distribution of parameters that define our models  we could
imagine using the enron dataset for this  enr        
the heart of our algorithm for this project was our
objective function  while we addressed the main issue
with regard to email repliesthat the marginal cost
of missing an important email is much less than the
marginal cost of reading an extra emailthere is a lot
of room to further improve the objective functions to
tackle other nuances of replying to email 
beyond just looking at the reply status of an email 
we would also like to work on problems such as label or folder categorization and how to preempt that
process  this has already been worked on but there
are still many extensions possible to previous work
 dredze et al   mock  

   conclusion
ultimately  we were able to attain good performance
on our key metric  recall  in determining whether
emails would be replied to  our algorithm had the best
performance  an asymmetrically regularized support
vector machine with a linear kernel and pre processed
feature inputs  which were rank reduced by principal component analysis   our recall reached        
which is a leap beyond nave bayes at        
we consider this endeavor a success  our models function well enough that we can imagine them being used
in practice to predict whether actions would be taken
on incoming emails and we were able to optimize them
specifically for this application 

references
enron email dataset  august       url https   
www cs cmu edu  enron  
aberdeen  d   pacovsky  o   and slater  a  the
learning behind gmail priority inbox 
url
http   static googleusercontent com media 
research google com en us pubs archive 
      pdf 
bekkerman  r   mccallum  a   and huang  g 
automatic categorization of email into folders  benchmark experiments on enron and sri
corpora 
technical report ir      university of massachusetts  amherst       
url
http   management haifa ac il images info 
people ron bekkerman files email pdf 
cortes  c  and vapnik  v  support vector networks 
machine learning                september      
dredze  m   schilit  b n   and norvig  p  suggesting email view filters for triage and search 
url
http   www cs jhu edu  mdredze 
publications dredze ijcai    pdf 
grant  m  and boyd  s  cvx  december       url
http   cvxr com cvx  
mitchell  tom m  machine learning  chapter    mcgraw hill       
mock  k 
an experimental framework for
email categorization and management 
url
http   www math uaa alaska edu  afkjm 
papers emailcat pdf 
pantel  p  and lin  d  spamcop  a spam classification   organization program  march      
url http   www patrickpantel com download 
papers      aaai   pdf 

fi