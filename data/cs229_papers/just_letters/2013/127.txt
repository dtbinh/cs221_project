machine learning classification of kidney and lung cancer types 
vivek jain  weizhuang zhou  yifei men
stanford university
cancer type identification is often critical in disease management and extending life expectancy of patients  conventional classification
through pathological analysis is invasive and costly  with advancements in microarray tools  whole genome methylation profiling can
now be performed quickly and cheaply  since dna methylation profiles are known to be different between normal and cancerous cells 
they hold promise as a scalable avenue for cancer classification  in this project  we used machine learning algorithms to classify  
different kidney and lung cancer types based on their methylation profiles  and show that our two best performing models have an
accuracy exceeding      we further demonstrate that this high level of prediction accuracy can be achieved with only   
 transformed  features  a        fold reduction in feature space as compared to raw input from genome wide methylation information 

introduction 
cancer cells exhibit a number of characteristics distinct
from their healthy cellular counterparts due to
misregulation of specific cellular pathways  these changes
manifest in differential expression of genes  cancer
classification became arguably the first application of
machine learning in medicine at the turn of the century 
when both supervised and unsupervised learning were
applied on gene expression data  these approaches achieved
considerable success in discriminating between types of
cancers  and in detection of cancer subtypes with clinical
significance   while some of these alterations in gene
expression of cancer cells are due to changes in the dna
sequence  methylation changes of dna molecules are
increasingly acknowledged as key contributors   the
interest in methylation profiling has also been fueled by
modern molecular biology technology that allows for high
coverage analysis of dna methylation sites  and profiling
of cells at the level of the whole genome 
in this paper  we apply both unsupervised and supervised
machine learning methods to whole genome methylation
data for lung and kidney cancers  we demonstrate that  i 
methylation profiles can be used to build effective
classifiers to discriminate lung and kidney cancer subtypes 
and  ii  classification can be performed efficiently using

low dimensional features from principle components
analysis  pca  
by demonstrating that different cancer types have
distinct methylation profiles  the findings of this paper are
not only relevant as classification tools  but also set the
foundation for targeted and specific epigenomic therapies
for cancer  we also narrow down the list of methylation
sites that show distinctly different profiles in different
cancers  providing the potential for future work on the
biological significance of the sites 

methods 
data 
methylation profiles for the following   cancer types  kidney
renal clear cell carcinoma  kirc  n       kidney renal papillary
cell carcinoma  kirp  n       lung adenocarcinoma  luad 
n       and lung squamous cell carcinoma  lusc  n     
were obtained from the cancer genome atlas  tcga   for
every sample  the degree of methylation at         positions
across the whole genome was quantified numerically as beta
values  assayed using illumina humanmethylation    beadchip  
imputation by mean values was used to fill up missing values
in the dataset  and quantile normalization was performed to
reduce batch related variance noise across the different samples 
as per standard treatment   a balanced data set was achieved by
picking     samples randomly for each cancer type  for a total of
    samples 

k means 
k means was implemented in r using   and   centroids
separately  fig     k means is a non parametric  unsupervised
machine learning algorithm which clusters given samples to k
centroids by minimizing within cluster sum of squares  where
each   represents a         dimensional vector containing the
methylation profile of a sample  and   is the mean of the points
assigned to cluster    
 

arg min
 

      

 

        

the algorithm was run till convergence or a maximum of   
iterations 
fig    outline of main processes used in study 

fifeature selection 
principal components analysis  pca  is a transformation that
maps data to new dimensions  principal components  or pcs 
that capture the greatest variance when the data is projected on it 
pca was performed on the data using pythons scikit learn
package   yielding a total of     principal components 

supervised learning 

coefficients  probes whose contribution to the different cancer
classes differed greatly  more than   standard deviations above
the average range across all probes  were identified  and grouped
by hierarchical clustering  these set of selected probes were then
graphically depicted using a heatmap   fig   

results 

all of the following supervised learning algorithms were
performed using lower dimensional  n        features using
scikit learn packages  run time analysis was performed for
varying number of features in   fold increments  fig    
additionally  knn and gnb were trained and tested using the
full untransformed feature set 

k nearest neighbors  knn  
knn classifies each test sample based on the majority label of
the k nearest neighbors  as determined from euclidean distances
to the test sample  we used k   for our model 

fig    composition of cancer types in k mean
centroids   a  cancer samples are well clustered based on
organ type with   centroids   b  cancer subtypes in the same
organ system are not well separated using k means 

gaussian naive bayes  gnb  
gnb builds a bayesian probability model based on the
frequency of observed features  assuming independence and
gaussian distribution  laplace smoothing with a smoothing
parameter       was used 

support vector machine  svm  
svm works by finding points that can be used as support
vectors to define the classification boundaries between the
cancer types    svm models were generated  using linear and
gaussian kernels respectively  the gaussian kernel is based on an
infinite dimensional feature mapping  both svms were trained
with penalty term c        gaussian svm was trained with
 
kernel coefficient   
 
n features

logistic regression 
logistic regression models the probability of each of the  
cancer subtypes as a linear sum of the features using a logistic
function  the model was trained using l  norm regularization
with penalty c     

fig    computational tractability of different
machine learning models  runtimes based on models
trained on     samples  running times of all models appears to
scale linearly with number of features 

model evaluation  
for evaluation of generalization error  a random set of   
samples were withheld from each cancer type  while the
remaining data was used to train predictive models 
generalization and training errors were analyzed with different
numbers of training examples in increments of     with equal
training samples per class  fig    
the effect of feature size on model performance was evaluated
by varying the number of pca features used to train different
models  features were selected from top ranked principle
components  the number of features used was incremented by  fold from   till      fig        fold cross validation was used to
assess performance 

selection of significant probes set 
we recovered the contribution of each probe to eventual
predictions  this was done by using a square matrix with
        rows  where diagonal terms were maximum observed
value for the corresponding probe  row  in the training set  offdiagonals were mean values of observations for given row from
the training set  matrix was transformed to lower dimensions
using pca loadings  then multiplied by logistic regression

fig    fea tu re sel ection u si ng p ca  a   cancers
from different organs are well separated spatially using the
first   principle components   b  scr ee p lot  the first
few principle components explain a large proportion of
variance observed in data 

fifig    dissection of errors in svm with gaussian

fig    training and generalization  testing  error

kernel  between organs refer to misclassifications between
cancers from different organs  while within kidney lung
refer to misclassifications between cancers of the respective organ 
beyond    features  error was dominated by between organ
and within lung misclassifications  the breakdown of errors
did not change significantly with change in number of features 

of different models with varying number of samples 
logistic regression and linear svm  not shown  have similar
profiles  models were trained on     pcs and tested on a holdout set 

fig    performance of svm models  confusion matrix
comparing accuracy of predictions using  a  svm with
gaussian kernel and  b  svm with linear kernel  models were
trained on     pcs and tested on a hold out set 

fig    contribution of individual probes to logistic
regression model       probes  rows  with significantly
different contributions to the logistic regression classification of
the cancers subtypes are shown  a distinct profile is observed for
each cancer type 

discussion of results 
unsupervised learning 

fig    accuracy of models with varying number of
features 

prediction error of   models against the number of pca
features  knn  logistic regression and linear svm have
similar observed trends 

we used k means as an exploratory tool to determine
whether there was an underlying nesting structure in the
dataset  if the feature vectors of different cancer types were
indeed different  k means using   starting centroids should
be able to achieve relatively good clustering according to
organ systems and subtypes  we obtained encouraging
results using   centroids  where the cancer types are wellsegregated by organ types in the   clusters  fig  a  
demonstrating that lung and kidney cancers have distinct
methylation profiles  however  the cancer subtypes are not
well segregated when   centroids were used  fig  b   the
given clustering result suggests that the   lung cancer
subtypes and   kidney cancer subtypes cannot be easily

fidistinguished using an unweighted linear combination of all
        features 

feature selection 
to achieve better discrimination between the cancer
subtypes  we applied supervised learning methodologies to
the data set  using the full feature set for all     samples 
we were able to achieve an average of       accuracy using
the gaussian naive bayes model and a       accuracy
using the knn model based on    fold cross validation 
however  a simple benchmarking of the runtime of
common machine learning algorithms on our dataset
indicated that using the full         dimensional feature
set was computationally expensive  and especially so for
svm  fig     with improving technologies in wholegenome methylation profiling  we expect the number of
methylation probes assayed to further increase in the near
future  using all probe values as training features will
become increasingly challenging with time 
given that many of the methylation sites are correlated
with each other due to physical proximity in the genome
and commonality of regulation pathways  the data lends
itself naturally to dimensional reduction techniques  which
will collapse covarying features  the reduced dimensions
may reflect methylation hotspots or biological pathways
that may be used for further research 
an even greater motivation to perform feature selection
was based on the fact that the dimensionality of feature
space was much greater than the number of training
samples        which may result in overfitting during
model construction 
we adopted principle component analysis  pca  for
feature selection as it offered an easy way to transform our
data into lower dimensions  n       pca performed well
for our dataset and the cancer samples were well separated
by organ types based on the first   pcs  fig  a   with
some separation also observed between the subtypes  the
first    pcs were also found to account for     of the
variance in our data  fig  b  

supervised learning 
after obtaining a transformation of our data into a    dimensional subspace using the principle components  we
applied several supervised machine learning algorithms to
classify cancer subtypes using this new set of features 
due to the short runtime and easy interpretation  we
built a gaussian naive bayes  gnb  classifier as our
baseline supervised  parametric model  despite gnb having
good performance when trained on the full feature set
consisting of         probes  the new model trained using
the     pcs demonstrated high bias  fig     with an
unacceptable generalization  testing  error across all ranges
of training size  this suggests that the pcs were not ideal
features for training the gnb 
as an alternative baseline  we chose the non parametric
knn classifier  the knn model performed well on our
new feature set  achieving a generalization error of   

when trained on all training samples  fig     an
improvement in accuracy as compared to the earlier knn
trained using the full feature set 
as a non parametric algorithm however  knns runtime
and storage size increases with the number of training
samples  the model is also contingent on the assumption
that new samples would be very similar to the known ones 
biological data is inherent noisy  and although we reduced
variation via data pre processing  the nature of knn
implies that it is sensitive to the differences between new
test samples and our current data set  impairing its
generalizability  furthermore  knn does not provide
information on whether there are any methylation sites that
are strongly correlated with the cancer types 
to gain better insight into the data  we applied linear
discriminative models  logistic regression and svm with a
linear kernel  both models gave similarly good performance 
with zero training error and a low generalization error of
    beyond a training size of    samples  fig     zero
training error observed indicates that the given training data
is linearly separable  although minute  the decrease in
generalization error with increasing training samples
suggests that the models performance can be further
improved with more training data 
we also experimented with a non linear svm using a
gaussian kernel in an attempt to achieve even better
classification accuracy  unfortunately  this model
performed poorly  although the training error was zero  it
had a high generalization error         figure   shows a
comparison of predictions generated by both svm models 
high intensity in the off diagonal reflects high rates of
misclassification in the gaussian svm model  in particular 
the gaussian svm classified most samples wrongly as
luad  this is likely due to over fitting from high
dimension feature space of the gaussian kernel  while this
can perhaps be resolved by more extensive parameter
tweaking or further feature reduction  likely by exploring
techniques other than pca   we found this to be
unnecessary due to the outstanding performance of our best
models  logistic regression and linear svm  

optimizing prediction model 
the logistic regression and linear svm models achieved
good prediction accuracies on the test set  and the training
curves do not suggest overfitting  however  in our analysis
of pca components  we noted that the lower ranked pcs
did not account for a large proportion of variance of the
data  intuitively  this means that the lower ranked pcs
should not have significant predictive value and could
possibly be eliminated from the feature set to simplify
models  to investigate this  we tested the accuracy of all
previously described models with feature space restricted to
the first  n pcs  with n varying from   to    models with
good performance previously  knn  linear svm  logistic
regression  achieved or approached their optimal
performance with as few as    features  first    pcs   the

figaussian naive bayes  gnb  model also achieved its best
performance with    features  but errors increased when
additional features were used  this suggests that the gnb
model is overfitted when more than the    features are
used 
in the previous section  we inferred that gaussian svm
was possibly overfitted when trained on all     pcs  as
evident from its high generalization error  thus  we
expected the models generalization error to be lowered
when fewer features are used  however  the models
performance remained consistently abysmal  fig    
to understand the reasons for its poor performance  we
examined the types of error made by the model  we
grouped the errors as intra organ  within kidney lung 
and inter organ  between organs  misclassifications  the
breakdown of errors  fig    is consistent with varying
number of features  within lung and between organs
misclassifications accounted for nearly all errors observed 
since the performance of the gaussian svm did not
improve with varying feature or sample size  it is likely that
the set of features was unsuitable for this model 
alternatively  some form of regularization might be
necessary to improve the performance of the model 

selection of significant probes set 
we found that the classifications made by linear svm
and logistic regression were in perfect agreement  we thus
feel justified to focus only on the logistic regression model
when selecting significant probes  we obtained a set of
     probes  fig    that were found to be significant 
using the procedure described in the methods section  we
note that approximately three quarters of all         given
probes were not predictive  with variation of contribution
across the   classes for these probes being less than the
average range for probes 

conclusion 
in this paper  we show that logistic regression and linear
svm models can achieve      accuracy in classification
of   different cancer types  using as few as    features
obtained from top ranking pcs  as compared to the full
feature set of         probes  a    feature subset
represents a        fold reduction in feature space  which
drastically reduces computational overhead and complexity 
in particular  we note that both models were in full
agreement on all predictions  which suggests that
misclassifications by these two models are likely due to the
inherent nature of the data  rather than models bias or
variance 
future work would involve uncovering the biological
significance of the identified methylation sites  we would
also extend the classification model to include control
 non cancer  patients and more cancer types 

reference 
    golub  todd r   donna k  slonim  pablo tamayo  christine huard 
michelle gaasenbeek  jill p  mesirov  hilary coller et al   molecular
classification of cancer  class discovery and class prediction by gene
expression monitoring  science      no                       
    alizadeh  ash a   michael b  eisen  r  eric davis  chi ma  izidore s 
lossos  andreas rosenwald  jennifer c  boldrick et al   distinct types
of diffuse large b cell lymphoma identified by gene expression
profiling   nature      no                       
    laura  b   epigenomics  the new tool in studying complex
diseases   nature education    no           
    schumacher  axel  philipp kapranov  zachary kaminsky  james
flanagan  abbas assadzadeh  patrick yau  carl virtanen et al 
 microarray based dna methylation profiling  technology and
applications   nucleic acids research     no                    
    du  pan  xiao zhang  chiang ching huang  nadereh jafari  warren
kibbe  lifang hou  and simon lin   comparison of beta value and
m value methods for quantifying methylation levels by microarray
analysis   bmc bioinformatics     no                
    bolstad  benjamin m   rafael a  irizarry  magnus strand  and
terence p  speed   a comparison of normalization methods for high
density oligonucleotide array data based on variance and bias  
bioinformatics     no                    
    troyanskaya o  cantor m  sherlock g  brown p  hastie t 
tibshirani r  botstein d  altman rb  missing value estimation
methods for dna microarrays  bioinformatics      jun 
            
    scikit learn  machine learning in python  pedregosa et al   jmlr
    pp                  
    model  fabian  peter adorjan  alexander olek  and christian
piepenbrock   feature selection for dna methylation based cancer
classification  bioinformatics     no  suppl           s    s    

fi