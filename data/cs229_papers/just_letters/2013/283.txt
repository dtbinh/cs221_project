prediction using prospect theory
david walsh
december         
abstract
in this report  we consider prediction of an agents preferences over
risky monetary outcomes using prospect theory  we suppose that for a
given agent we have data on previous prospects that the agent has accepted or declined  based on this information  we would like to predict
whether the agent will accept some new prospect x    this amounts to
learning a value function v and probability weighting functions w    w
for the agent in question  and using these functions to establish whether
the prospect will look attractive to them 
in general  we do not expect to have sufficient data on a single agent
to learn  v  w    w   from scratch  instead  we assume that we have a
population of agents with observed decision histories  on which the algorithm may first be trained  each training agent has different values for
 v  w    w    we assume an arbitrary parameterisation of these functions 
controlled by some   then  following the approach taken by chajewska
and koller        for prediction using expected utility theory  we model
 as having some population wide distribution p   using p as a prior for
the parameter value specific to the agent on whom we wish to make prediction  we may define a probability p that the agent is someone who
would accept x   
we present an algorithm that uses a fully bayesian framework to learn
a posterior distribution for p   and hence a posterior distribution for p  
this could be highly valuable in any competitive context where we expect
an opponent to follow prospect theory  as it tells us which offers x  they
are likely to accept 
this approach differs from how pt is typically implemented in the
literature  most studies have been primarily descriptive and have sought
to explain specific phenomena  usually  heterogeneity in the functions
 v  w    w   features as random effects terms and the model is fit using
maximum likelihood 
our algorithm is then made more robust by allowing a small proportion
of agents to deviate from pt  that ensures that the posterior for p cannot
be biased by a small number of training agents whose behaviour does
not correlate with the predictions of pt  further  if the agent on whom

 

fiwe make prediction is not well described by pt  there is a risk that we
conclude either p     or   with great confidence  this refinement ensures
a more conservative posterior distribution for p that avoids this 

 

introduction

prospect theory  tversky  kahneman             is a descriptive model for
decision under risk that is commonly used across numerous applications  the
key tenet of pt is that decisions are reference specific  so that agents focus on
immediate gains and losses  it also allows non linear probability weighting  so
that agents may be overly sensitive to tail events 
for a detailed exposition of the contexts to which pt has been successfully
applied  see barberis         in general  pt has been applied most extensively
in finance and insurance  for instance  many recent papers have used the overweighting of tail probabilities associated with pt to explain why stocks with
positively skewed returns  such those offered in an ipo  appear empirically to be
overpriced  barberis and huang         boyer et al          bali et al          
sydnor        uses probability weighting to explain the purchase of insurance
policies that charge a disaproportionately high premium to secure against very
improbable disasters 
other applications of pt include labor supply  camerer et al          koszegi 
rabin          gambling  snowberg  wolfers        barberis        and retail
 heidhues  koszegi         
in general  this algorithm is likely to be most useful in contexts where we are
marketing prospects x  to an individual customer and would like to know what
offers they might accept  the package we are selling could be e g  an insurance
policy or a betting offer 

 

model

a prospect is defined to be finite distribution over real valued monetary outcomes  in particular  the prospect  p    x         pt   xt   represents an opportunity
to receive xi with probability pi  
we adopt specifically the cumulative prospect theory model  tverky  kahneman        under which an agent will accept a prosepct with x          xs 
    xs           xt if
s
x
r  

i v xr    

t
x
s  

 

i  v xr     

   

fiv   r  r is a value function over outcomes specific to the agent  akin to utility
in expected utility theory  it is required to be continous and strictly increasing  with v         generally it is assumed to be convex over losses and concave
over gains  reflecting empirical loss aversion 
i are decision weights  given by
    w  p     t    w   pt  
r   w  p          pr    w  p          pr   
r

 

 

  w  pr         pt    w  pr           pt  

 rs
s  r t 

where w                  are probability weighting functions  these functions
are required to be continuous and strictly increasing  with w          w      
   often they are assumed to have an inverse s shape  the larger curvature
at the end points corresponds to the notion that people are more sensitive to
differences in probabilities for events that have probabilities close to   or   
we suppose that we have training data on agents i           n  consisting of
i
i
  indi  offered and binary labels z i    z i        zm
prospects x i    x i        xm
i
i
cating whether the prospect was accepted  then  given a new agent for whom
 
 
 
 

that
we have data x      x          xm
     z    z         zm    and a new prospect x

is offered to them  we seek to predict the agents decision z  

 

parametric specifications

we now briefly consider some parametric specifications for the functions  v  w   w    
for a full discussion of the empirical evidence for alternative parameterisations 
see booij et al         
for the value function v  it is very common to use the power function
 
x
x 
v x   

 x 
x 

   

this corresponds to a crra utility function in expected utility theory  and
it was originally suggested by tversky and kahnemann         it has strong
empirical support  wakker         alternative parameterisations include the exponential function  relating to cara in eut  and an amalgamated expo power
function  see abdellaoui et al        for their properties  
there is greater variety in how people choose to model the weighting functions
w   in general  it is considered desirable to be able to control the curvature and
elevation of these functions separately  the curvature reflects an agents ability
 

fito discern between mid range probabilities  whereas the elevation reflects the
agents overall optimism in the case of w  or pessimism in the case of w  
two possible parameterisations  introduced respectively by goldstein and einhorn        and prelec         are given by
p
      p 
w p    exp   log p   
w p   

   

p

   

here  controls the curvature and  controls the elevation 
the algorithm derived in this project works for any choice of parameterisation 
in what follows  we suppose simply that  v  w   w    is controlled by some vector
    rp  

 

training

recall that an agent will accept a prospect x if     is satisfied for their particular
 v  w   w     viewing the expression in     as a function of   it follows that
they will accept the prospect if and only if   rx   where the set rx  
is defined by some inequality f        for prospects x    x         xm   and
responses z    z         zm    set
 
rxj  zj  
rx z  

rxj
c
rx
j

m
 

zj    
zj    

 jm

rxj  zj

j  

the agent will give the exact sequence of responses z to prospects x if and
only if   rx z   in other words  given data  x  z   the likelihood of  is
l         rx z  

   

often  for a particular value of   we will need to check whether   rx z for
some given prospects x and responses z  it is worth noting that rx z is defined by m simultaneous inequalities  so that is reasonably straight forward 
now the focus of training the algorithm is to learn the population heterogeneity in the functions  v  w   w     for this parameterisation  that amounts to
learning the population wide distribution p of   we adopt a fully bayesian
framework to achieve this  as a prior for p   we use a dirichlet process with
a suitably chosen measure  over   the parameter values          n for the

 

fiagents in the training set then constitute an iid sample from p   from the decision histories of these agents  we may make inference on          n using     
and hence on p  

 

fithe posterior distribution of p   given the training agents data d    x     z         
x n   z n   is derived by gibbs sampling 
for any a   
p i  a x i   z i     p i  a i  rx i  z i  

   

hence the posterior for i   p is given by
 i   p  d  p  rx i  z i

   

where p  a denotes the distribution of p conditioned to the set a  we can sample from this distribution by sampling i  p and accepting the result only if
i  rx i  z i    
due to our choice of conjugate prior for p   we have
p            n  dp      

n
x

 i  

   

i  

we can sample p from this distribution using a stick breaking process 
together     and     define a gibbs sampler that be used to sample p and
          n   from their posterior distributions  although our intention is to learn
the posterior of p   we cannot store a sample of distributions for p   instead  it
will prove sufficient to take a sample for           n   
amalgamating the two stages of the gibbs sampler  successive instances of
          n   are computed by the following procedure 
for i           n  
suppose we have already obtained stick breaking elements  p               pk   k   
   sample u  u        
pk
   if u   r   pr   generate  pk                pk    k    until k   is minimal such
pk  
that u  r   pr as follows 
for j   k      k           
i  sample j  beta     r    n   set pj   j

qj 

r     

 pj     

  if r
x i  z i is small  this rejection sampling can be very slow  that issue is addressed in
section   

 

fiii  sample j from the distribution
 
  r    n 

 

n
x

 
 i

i  

i e  with probability n   r    n  we sample j uniformly at random
from          n   else we sample j from the distribution   r  
 


   set    k    if   rxi  zi   set i     else return to   
 
we allow for some burn in  then construct a sample for           n   by selecting
distantly spaced instances from this sequence 

 

prediction

now for prediction we suppose that we have a new agent on whom we have
historic data x     z     we are interested in the probability they will accept a
suggested prospect x 
p   p z      x    x     z    

   



the randomness modelled by p here is whether the given agent is the type
person who would accept this prospect  the decision z  is determined by the
agents parameter   and we consider   to be drawn from p   conditional on the
information  x  z   in particular  p is fixed given p  
there is a second layer of randomness relating to our uncertainty in p   encapsulated by our posterior for p given d  we will let p inherit a posterior
distribution given d also  treating this randomness seperately allows us to assess our uncertainty due to imperfect training of the algorithm  as distinct from
the fundamental uncertainty resulting from population heterogeneity 
z      if and only if    rx   
from      we have
p   p    rx       rx    z     
  p  rx    z    rx   

    

then  by     
c
p            n  beta   k rx   rx    z      k rx
  rx    z      

 

    

fiwhere k a     a    i   i  a   note that k is staight forward to evaluate
for these sets  it simply requires us to check whether each i is in the sets rx 
and rx    z    
this shows how the posterior distribution of p may be inferred from our sample
for           n    for each           n    we sample one instance of p from       and
so obtain a sample for p   in fact  values for the mean and variance of p   d are
likely to be sufficient  as an estimate of the desired probability and a measure
of our training uncertainty  these may be estimated directly from the sample
for           n   using

 
e p   d    e       n e p            n     d


k rx   rx    z    
  e       n
k rx    z    

 
v ar p  d    v ar       n e p            n     d

 
  e       n v ar p            n     d


k rx   rx    z    
  v ar       n
k rx    z    


c
k rx   rx    z    k rx
  rx    z    
   e       n
k rx    z       k rx    z         

 

    

    

robusting the algorithm

as with any model  some agents will be poorly described by prospect theory 
indeed  pt does not constitute a perscription for optimal decision making under
some criteria  rather  it is a psychological assertion about how typical agents
perceive their situation and it is inevitable that some agents will behave differently 
this section focuses on refining the algorithm to handle agents who deviate
from pt  such agents present a number of problems for the algorithm  firstly 
consider an agent in the training set whose responses z i to prospects x i do not
fit with pt  in that case  the region rx i  z i is likely to be very small or even
empty  using the likelihood for  given in      we note that the likelihood for p
is
l p   

n
y

p  rx i  z i  

    

i  

if rx i  z i is small  the posterior for p will be biased towards distributions that
assign significant mass to this erronenous region  if the set is empty  this likelihood is everywhere    so the posterior is not even defined 

 

fifurther  while training the algorithm  the gibbs sampler must sample values
for i until we obtain i  rx i  z i   if rx i  z i is small  this will take a very long
time  indeed  if it is empty the algorithm will not terminate at all 
on the other hand  during prediction suppose that the agent in question does
not follow pt closely  then of course we cannot predict their actions well  so
our distribution for p ought to be centred near     with a large variance  in
fact  since the agents previous responses z   to prosepcts x   will not fit with
pt  we are likely to have a region rx    z   that is very small  consequently  we
expect one of the following two situations to hold approximately 
  
rx    z    rx 
  
c
rx    z    rx


it follows from      that the posterior for p will be clustered closely around
either   or   
a number of recent studies have addressed the issue that different agents are
better described by different decision models using mixture models  bruhin et
al          harrison and rutstrom         conte et al           for instance 
bruhin et al  uses a mixture model that classifies agents as either following
pt or eut  they find most agents have a very high posterior probability of
belonging to one of the two classes  with about     of agents following pt 
in principle  our predictor could be combined with other predictors based on
alterative decision models in this way  in fact  we will presume that in the context under consideration almost all agents follow pt reasonably well  and let
only some small proportion  deviate from it  we will not specify an alternative
model for those agents  rather  we assert that they follow some reference model 
which we do not attempt to learn  that predicts each decision correctly with a
given probability               we will consider  to be the same for all training
agents  however  it will be convenient to use a different value of  for prediction 
to model this  we endow each agent with a binary variable y that indicates
whether the agent deviates from pt 
y  bernoulli  

    

independently across agents 
when we make prediction  it will be important that pt is compared only against
an imperfect reference model  for  even if the agent in question follows pt with
  we suppose       so that the reference model is at least as accurate as guessing at
random 

 

fisome  exactly  the population heterogeneity means that pt will not be able
to predict their decisions perfectly until we have learnt   meanwhile  when
compared against a perfect model  any decision history  x     z     would appear
as evidence that the agent deviates from pt  once m  is sufficiently large  we
are sure to conclude that y     for this agent 
for training  however  the details of the reference model are less important and
we are free to use       in this case  our choice of  essentially controls the
extent to which our inference on p is weighted towards training agents for whom
we have more information  reducing  moves the posterior for p away from
those agents where m is large 
for observed data  x  z   we now have the conditional sampling distributions
p z   x   y               rx z  

    

p z   x   y           m

    

after marginalising out y   this gives the robust likelihood for   cf     
l              rx z     m 

  

    

fi  

fi   

training

using       we obtain the robust likelihood for p  cf      
l p   

n 
y

p  rx i  z i    

i  

m 
 


    

now p  rx i  z i   can be small for some agent without the likelihood approaching zero  this shows that no single agent can bias the posterior for p too heavily 
the algorithm is trained using the same gibbs sampler as before  the difference
is that during the rejection sampling we must consider the possibility that y    
for the given agent  step   is replaced by

  set    k    if   rxi  zi   set i    


i

  if  
  rxi  zi   sample t  bernoulli m      if t      set i     else
return to   
i

note that the rejection sampling now requires at most o   m   attempts  so
the run time is bounded even if rx i  z i is small 

   

prediction

equation      describes how the reference model fits observed decisions  however  since we do not learn the reference model  given a new prospect x  we do
not know it should predict the response z    we define a random variable v for
this predicted response and model it as
v  bernoulli     

    

independent of x     z     d 
since our inability to predict v reflects a limitation of the algorithm  we want
to include this additional source of randomness as part of our posterior uncertainty over p   namely  we now consider p to be fixed given p  v   p then
inherits a posterior distribution from our posterior for p and the variability in v  
to evaluate this distribution  we first compute
 
m 

 
   m   

p y         x     z              
 p y         x     z             

   rx    z  

  
  rx    z  

 
p  rx    z    
       m   

then we have
  m i 

    

 


is the probability that y     and the observed response z is given 

  

    

fip   p y         x     z         p    rx     y             rx    z     

 

  p y         x     z         v

n
o
 
   z    
   v
p
 r
p
 
 r
 v
x
r
x
 
 
 
x  z
       m  

    

by     
c
p  rx    z                n  beta k rx    z      k rx
   z    
 

n

p  rx    z    rx                beta   k rx   rx    z     



c
k rx


    
 rx    z           

hold independently 
by sampling from            and      for each           n   in our training sample  we can generate a complete sample for p from its posterior distribution 
alternatively we may compute the posterior mean and variance of p as before 

 
e p   d    e       n e p            n     d
 


k rx    z    
 
  e       n
        m   
k r 
 



k rx   rx    z    
 
 

 
k rx    z    
 
 


 
 
   
 
  r    n           m    


 
e       n k rx   rx    z      k rx    z    
 

    

a similar  but very messy  formula can be obtained for the variance 
it is worth noting that if rx    z   is very small  then p  v   rather than concluding that the agents decision may be predicted with great confidence  we
find that p is centred around     with maximum possible variance     
finally we address the issue of choosing  for prediction  supposing that the
agent is well described by pt  the region rx    z   will shrink with increasing m 
as we learn their value for   from      we see that  conditional on p   the population heterogeneity ultimately

 leads us to conclude that the agent deviates
 
 m
from pt if p  rx    z       o 
as m    
 

consequently  we would like to choose   so that p  rx    z       m for typical
distributions p under the posterior  this could be achieved  perhaps  by examining how p  rx z   decays for agents in the training set  alternatively  to be
  

fisafe  we could just use         

 

references

abdellaoui  m   bleichrodt  h   paraschiv  c          loss aversion under
prospect theory  a parameter free measurement  management science        
          
bali  t   cakici n   whitelaw  r          maxing out  stocks as lotteries and
the cross section of expected returns  journal of financial economics       
        
barberis  n   huang  m          stocks as lotteries  the implications of probability weighting for security prices  american economic review                 
barberis  n          a model of casino gambling  management science       
      
barberis  n          thirty years of prospect theory in economics  a review
and assessment  journal of economic perspectives  forthcoming 
barseghyan  l   molinari  f   odonoghue  t   teitelbaum  j          the nature of risk preferences  evidence from insurance choices  ssrn working paper         
booij  a   van praag  b   van de kuilen  g          parametric analysis of
prospect theorys functionals for the general population  iza dp no       
boyer  b   mitton  t   vorkink  k          expected idiosyncratic skewness  review of financial studies                
bruhin  a   fehr duda  h   epper  t          risk and rationality  uncovering
heterogeneity in probability distortion  econometrica               
camerer  c   babcock  l   loewenstein  g   thaler  r          labor supply
of new york city cabdrivers  one day at a time  quarterly journal of economics                 
chajewska  u   koller  d          utilities as random variables  density estimation and structure discovery  proc  uai        
chajewska  u   koller  d          learning an agents utility function by observing behavior  proc  uai        

  

ficonte  a   hey j  d   moffatt  p  g          mixture models of choice under
risk  journal of econometrics            
gelfand  a  e   kottas  a          a computational approach for full nonparametric bayesian inference under dirichlet process mixture models  journal of
computational and graphical statistics             
harrison  g w   rutstrom  e e          representative agents in lottery choice
experiments  one wedding and a decent funeral  experimental economics     
       
heidhues  p   koszegi  b         regular prices and sales  unpublished paper 
hey  j  d   c  orme         investigating generalizations of expected utility
theory using experimental data  econometrica               
koszegi  b   rabin  m          a model of reference dependent preferences 
quarterly journal of economics                   
maceachern  s  n          computational methods for mixture of dirichlet process models  practical nonparametric and semiparametric bayesian statistics 
       springer 
neal  r  m          markov chain sampling methods for dirichlet process mixture models  journal of computational and graphical statistics            
sethuraman  j          a constructive definition of dirichlet priors  statistica
sinica            
snowberg  e   wolfers  j          explaining the favorite long shot bias  is it
risk love or misperceptions   journal of political economy                 
starmer  c           developments in non expected utility theory  the hunt
for a descriptive theory of choice under risk  journal of economic literature
          
teh  y  w   jordan  m  i   beal  m  j   blei  d  m          hierarchical dirichlet
processes  technical report      uc berkeley 
tversky  a   kahneman  d          prospect theory  an analysis of decision
under risk  econometrica         pp          
tversky  a   kahneman  d          advances in prospect theory  cumulative
representation of uncertainty  journal of risk and uncertainty           

  

fiwakker  p p          separating marginal utility and probabilistic risk aversion  theory and decision         
wakker  p p         explaining the characteristics of the power  crra  utility
function  health economics                

  

fi