express recognition
exploring methods of emotion detection
yasemin ersoy
yasemin ersoy ti com
stanford cs     autumn     
abstract computer  robotic and mobile interfaces are
beginning to use expression recognition to give a more human
experience  to make an interface more dynamic and seamless to
human interaction  understanding of emotions is key  a robust
application to recognize certain facial expressions in real time
has many obstacles starting from correctly identifying a face and
extracting necessary features of the face to then mapping these
features to the right expression  there has been a thorough study
of how to extract key points on faces and opencv even provides
this code freely      a topic that has been over looked is how to
best manipulate a minimal number of facial key points efficiently
or even if facial key points are the fastest route to facial
expression identification  the goal will be to compare different
methods of extracting expressions and gain more intuition about
this system to further improve how expressions are defined and
modeled 

i 

introduction

facial expression recognition has been studied since the
    s and has been spreading to a variety of fields such as
psychology  art  robotics  and computer vision  though
initially studies targeted smaller subsets such as those with
impaired understanding of emotions and lie detection  in our
current technology these valuable expressions can be utilized
to create a more fluid user interface  cameras surround our
world  and with the development of image processing even the
lower resolution cameras on laptops or phones are enough to
robustly extract the necessary features for expression
recognition 
the majority of expression recognition research involves
using a grand assembly of facial key points to mimic and
understand the entire face  i e  representing the eyebrow using
at least five points to digitally visualize the curvature  my goal
is to utilize fewer points and still capture the expression  the
   points that will be
examined are  top center of
face  middle of eyebrows 
sides of the eyes  tip of the
nose  corners of the mouth 
center bottom of the bottom
lip  and the tip of the chin
 figure     since the focus
will be on the expression side
of events  the algorithm will
be inputted features directly 
unfortunately this means
manual feature extraction
which is time consuming 
figure    key points

ii 

understanding emotions

artists refer to triangles of shadowing and warping these
triangular areas surrounding the eyes nose and mouth to reflect
emotions in drawings  as pamela davis at stanford university
explains  these triangles can also be viewed as angles between
different corners of the face such as between the chin and the
corners of the mouth  the happy  sad  angry  and surprised
emotions have been studied in the greatest detail so these four
emotions will be tested and detected along with the neutral
face  the lower face angles surrounding the mouth are more
relevant indicators for happiness and sadness and the upper
face angles regarding the eyebrows are more prominent in
detecting anger and surprise     
using angles rather than distances  figure       allows
different sized faces and rotated faces to be treated the same
and is a novel and speed optimized method based on past
expression detection methods which look at the direction of
face flow  distance of features  and relative facial key point
location training            if angles are not used then
normalizing the features using top and bottom of face is
crucial 

figure    neutral face angles

figure    surprised face angles

the neutral face is an important foundation when
examining expressions  the variation based on a neutral face
on each subject rather than a global neutral face is much
smaller  so it is best to calibrate each user with his her own
neutral face then look at the distance of the expression face
from the relative neutral point to detect the expression from
this difference  plus each persons neutral face is slightly
different depending on the shape of their facial features so this
method makes up for facial variations in users  below is an

fiexample of expression angles of a single subject  figure    and
variation of   different subjects  figure    

figure    sample surprised subjects

given the face angles and using the calculated thresholds for
each expression  facial expression at each instance is calculated
as follows 
figure    five subject variation

using the correlation of the current face to the neutral face 
a reliable model can be created for expression recognition 
iii 

modeling and machine learning

a  brute force modeling
since angles between features appear to be more stable
reference points than exact locations  it is best to get a deeper
understanding of the expression model using these angles to
create an initial algorithm 
based on the troubles or
observations of this initial brute force algorithm the best
method of continuing with machine learning can be chosen 
the thresholds for each expression were estimated by
experimenting using a collected database of faces  examples in
figure     the most prominent angles for each expression are
given the heaviest weighting for that expression  where
prominent is defined as the least overlapping points with
another expression and greatest change from neutral  the
collected data showed that combinations of angles are also
useful identifiers such as the first   angles in the lower face of
surprise being positively different with respect to neutral while
the last two are negatively placed  figure    

   add chin and center top of forehead to key points using
face bounding box  eyebrow center and lip corner keypoints 
   calculate facial expresion angle using all key points
   check if past expression was inputed
 yes  if past angles are within a threshold to
equal new input  expression has not changed output past expression
 no  continue to face detection
   check if calibrating
 yes  record current expression angles as neutral
 figure    and check if previous average neutral
angles were imported  if so average the two
neutral angle sets and output as new neutral
expression angle set
 no  continue to face detection
   detect expression
a  upper face
a  compare upper face thresholds to
calculated emotion thresholds of
eyebrows
b  if anger and surprise are detected in
upper face give a higher weight

fic 

check if movement is in all negative or
all positive direction
b  lower face
a 

c 

weigh mouth corners to nose heavily for
happiness
b  weigh mouth corners to bottom lip middle
heavily for sadness
check for angle thresholds for surprise and if the

maximum weighted expression is not strong
enough label face neutral
d  if contrasting emotions are detected  ie angry
eyebrows with smiling face  label as confused
e  if two emotions are equally rated  which happens
in anger and sadness quite often  further
differentiate the two expressions
a 

b 

shape of eyebrow movement relative to
eyes and top of forehead to differentiate
sadness and anger
general direction of mouth relative to nose
and chin to differentiate happiness from
surprise and sadness from anger

   output detected expression along with the probability
of detecting each expression 
using the estimated algorithm above a small sample set of
nine separate collections were tested to give the results shown
in table i  indicating that sadness is the most difficult emotion
to detect 
table i 
detected
emotion

initial expression recognition accuracy
actual emotion
happy

sad

anger

surprise

    

 

 

 

sad

 

   

     

 

anger

 

   

   

 

surprise

 

 

 

    

confused
or neutral

 

   

     

 

happy

b  machine learning
the sample algorithm that was used to test the dynamics of
the expression model is not a convex model to optimize so it is
best to start with simplifying the recognition model  the input
vector can be considered as the distance from neutral
normalized to face size     points for x and y coordinates  or
the difference of the    chosen angles between features from
the neutral angles  the output will be a classification of neutral 
happy  sad  angry  or surprise 

where m is the number of inputs 
is the matrix of
normalized distance of the features from the neutral of size m
by     and
are to be estimated to capture the outputs y  a
size m vector indicating the expression 
is the matrix of the
distance of the angles from the neutral angles of size m by   
and are to be estimated to capture output classification y 

the two input methods above can be tested using different
classification methods to see the most robust system  since the
data is sparse  nave bayes should give intuitive results 
discriminant analysis could capture the characteristics of the
limited data input more accurately for better results  and knearest neighbor can give a localized estimation of the data
since there are overlapping emotions that we do not
accommodate for in the simplified algorithm  the features are
not independent so the assumption is that nave bayes
performs the worst        and   nearest neighbors are checked
for k nearest neighbor and give surprisingly similar results
most likely due to the need for further depth in the training set 
in general  the prediction before testing was that angles would
give better results than distances  and k nearest neighbors
would give superior fitting in comparison to discriminant
analysis which in turn would be more accurate than nave
bayes 
iv 

results

the data used to test the best fit and input method was
derived from    separate subjects creating the four proposed
expressions plus neutral 
the features were recorded
manually and multiple collections per subject were made to
test variation as shown in figure    the mean variance of
angles was     and the average variance for normalized face
distances was     using this information  a larger pool of
data was created by varying each subject output angles by
    which created   possible inputs from a single collection
         and each x and y coordinate for distance by   
creating   possible inputs from a single collection   x y  
 x  y    x y      x  y    x y      x  y     x   y      
therefore the total data set for distance training was   
subjects with   variations and   separate faces      with input
vectors of size    and for angle training only   variations     
with input vectors of size    
table ii 

initial classification testing

method

error rate
distance

angle

brute

n a

     

naive

  

    

discriminant

  

    

k nearest

  

  

initial results were just to see how well the three
classification methods fit the entire data set and also to see
how the brute force method reacted to the variations 
displayed above in table ii  next  the training set was
decreased by     of the data  used for testing  with    separate
combinations of testing and training data  the training set and
the testing set were then made of equal size again with   
separate random combinations  finally the training set was
dropped to     of the data and     of the data was used for
testing  again with    combinations of training and predicting 

fitable iii 

classification testing train test ratio

predictions using angles of these facial features which allows
for a distance and rotation robust algorithm 

error rate
method

distance
   

angle
   

distance
   

angle
   

distance
   

angle
   

nave min

  

     

  

    

     

     

discr  min

  

     

  

    

     

     

k near min

  

  

  

  

     

     

nave max

     

      

      

      

      

      

discr  max

     

      

      

      

      

      

k near max

     

     

     

     

      

    

nave mean

     

      

     

      

     

     

discr  mean

     

      

     

      

     

     

k near mean

     

     

     

     

     

     

the simplified clustering results are shown in table iii  as
expected  k nearest neighbor performed the best  however 
nave bayes and discriminant analysis led to the same
solution  which was not predicted  plus surprisingly  for these
two weaker solutions interpreting the feature points with
normalized distances created a better fit for the data  adding
further rotational variations to the key points would cause the
need for a larger training set for distance measurements and
create errors that the angle method can bypass  k nearest
neighbor not only proved superior results but also was
efficiently able to use the angle training set  even by using
only a third of the data to train the maximum error was still
below     and the mean below     the only problem with
this method is that it is memory based  the more data that is
stored the more accurate the system will  be up to a plateau  
overall  if memory is limited and a quick solution is
desired then nave bayes is optimal  but if memory is
available and robust predictions are preferred then k nearest
neighbor is definitely the right choice 
v 

conclusion and future work

expression detection is certainly achievable by means of
a few number of key points and can handle slight variations in
the key point locations  without using a large database  a knearest neighbor system can be quickly trained to calculate

figure    facial frame

the next step for expression recognition is in two
directions  one problem is calibrating to a neutral face and
another is facial feature detection accuracy  which drops
dramatically in poor lighting  a generalized neutral face can
be determined using an extended database but this would
cause an increase in false detection because of the variation of
natural facial features  removing facial features for emotion
detection all together can be a solution to both of these
problems 
the face can be simplified even further than the minimal
facial key points by local averaging  all humans have eyes 
nose and mouth located in relatively similar locations  for
expression recognition using feature extraction precise
locations of these features are vital  but if gradients of the face
were taken then the general trend of the face can be captured 
a crude example shown in figure    the averaged shades of
the face can then be used to train a new system that can
calculate thresholds for emotion detection 
expression recognition has plenty of room to grow in our
ever increasingly technology intertwined world 
by
continuing research to model facial expressions effectively
and discover efficient solutions with the help of machine
learning  soon the line between cold electronics and warm
emotional beings will begin to blend 
acknowledgment
thanks to pamela davis for artistic expression modeling
support 
references
m  uricar  v  franc and v  hlavac  detector of facial
landmarks learned by the structured output svm 
visapp      proceedings of the  th international
conference on computer vision theory and applications 
    
    neeta sarode  shalini bhatia  facial expression
recognition  international journal on computer science
and engineering          p            
    wu  yuwen  hong liu  and hongbin zha   modeling
facial expression space for recognition   intelligent
robots and systems        iros       
    yang  y   s  ge  et al           facial expression
recognition and tracking for intelligent human robot
interaction   intelligent service robotics               
   

fi