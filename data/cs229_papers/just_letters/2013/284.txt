predicting answer quality using post and domain based user information
lorraine fan  alexander hsu  and christina kao
abstract
stack overflow has a well established reputation system that gives users incentives to ask
and answer questions  as well as to evaluate content generated by other users  in this study  we
predicted whether an answer in a specific domain  java  would receive at least two net upvotes from
other users  and whether an answer would be accepted by the user who had posted the question  our
feature set consisted of answer post specific measures  answerers history in the predicted domain 
and the answerers history in the domain most related to the predicted domain  we obtained test
accuracies of    to      and identified that a set of just three featuresresponse order  number of
answers in the question  and the relative rank of the number of accepted answers of the answerer
among those who responded to the same questionperformed nearly as well as a full set of features 
we also discussed the importance of accounting for competition among answers to the same question 
motivation
we predicted whether an answer in a specific domain on stack overflow would receive at
least two net upvotes from other users  and whether an answer would be accepted by the user who
had posted the question  the general interpretation of this prediction was  given a question in the
specific domain  how likely the answerer would give a quality response 
past research on stack overflow has suggested that post specific predictors  such as the
arrival time of the answers  are associated with reactions to answers      other research on another
developer forum has proposed that user specific characteristics are positively correlated with human
assessment of domain expertise     we explored whether proxies for the domain expertise of
answerers are also associated with the reactions to answers 
with respect to user expertise  we hoped to improve the stack overflows current reputation
model  which relies on various user actions that may not necessarily relate to actual skill levels  in
particular  we believed user expertise can be better proxied by certain user history measures in the
domain of the question  and user history in the most related domains 
one possible application of our prediction problem is system recommendation or
highlighting of high quality answers to enhance the knowledge creation process of the website 
another application is to gauge whether identifying the level of domain expertise of each user is
viable in a peer assessment environment  after the characterization of domain expertise is refined 
such an application may be useful for assessing online participation in classes 
data
we obtained a complete trace of all the actions on stack overflow from its inception on july
         to september          which is publicly available at the website  we processed the data
using sqlite  in python 
we considered each hashtag to be a domain and focused on predicting reactions to answers in
the java domain  which is among the top five most active domains on stack overflow  descriptive
statistics on this domain are presented in figure   
figure   statistics on java since website inception   
total
users

total
questions

total
answers

total
accepted
answers

average
questions
per users

average
answers
per user

       

       

       

      

    

    

average
accepted
answers
per ser
    

average score
per user
 up downvotes 
    

fibecause we wanted to explore using answerers history in related domains as features  we
looked into cross domain activity  as shown in figure    the number of users decreases
exponentially when the number of associated domains increases  among the           users in the
   most popular domains          users have activities in only one domain  and         users have
activities in two domains     consequently  we decided to explore the one domain most similar to
java  which is android  figure    
figure    distribution of number of associated
domains   

figure    domain similarity based on intersecting
questions   

associated domains

number of domains

features
the three sets of features included were answer post specific measures  answerers history in
the predicted domain  java  between july          and march          and answerers history in the
most related domain  android   figure     for answerer history  we tabulated the history between
july          and march         of         unique answer providers  who formed           userdomain pairs 
figure    three sets of features
answer post specific measures
o
o
o
o
o
o
o

response time
answer edited or not
number of comments on the answer
number of comments on the question
question pages view count
questions favorite count
questions answer count

o
o
o
o
o
o
o
o

algorithms

answerers history java between
        and       
number of answers
number of questions
number of net upvotes from answers
average number of net upvotes per
answer
percentage of answers receiving at least
  votes
number of answers accepted  percentage
of answers accepted
number of not accepted answers 
number of net upvotes from questions
number of answers with negative net
upvotes  number of questions with
negative net upvotes

answerers history in
android
same as second column

fiwe ran logistic regression  support vector machine  nave bayes logistic regression  and
linear discriminant analysis using scikit learn in python  as well as in rs base  e      e     and
mass packages  respectively 
results and error analysis
approach  
for our first approach  we predicted whether each answer posted between march         and
september         would receive at least two upvotes using answerers history in java only  the
actual proportion of answers that met the upvote threshold was       
as shown in figure    training a svm with a linear kernel from        to        examples 
the training and test error rates were approximately     across the board  the radial basis   gaussian
kernel reduced the error rates by about   percentage points   the uniformly high rates of error
suggest that our model suffered from high bias  which in turn suggested that more features were
needed 
figure    learning curve with user in java history features only  svm using linear kernel
learning curve with svm using linear kernel
   

training error
test error

error

   
   
   

   
  
 

     

     

     

     

      

training set size

approach  
for our second approach  we predicted whether each answer posted between march        
and april         would receive at least two upvotes or be accepted using answers history in java
only  concentrating on valuable questions  which we defined as questions with at least     views 
the data set diminished to       examples  the actual proportions of answers that met the upvote
threshold and were accepted were       and        respectively 
we analyzed feature sets with    answerers history in java only     answer post specific
measures only  and    combining both  within user history measures  we noticed that three measures
about the answerers past history in asking questions had     zero entries and were worsening our
predictive performance  therefore  the three features were removed  it has been found in past
research that stackoverflow experts rarely ask questions but are active in answering questions    
with the remaining features  we checked the stepwise aics of logistic regression models  which
suggested that further removing features might not be useful 
for both response variables  ten fold cross validation test accuracies using svm and logistic
regression remained around      figures   and     post features delivered higher test accuracy than
did user features  moreover  the addition of user features to post features did not have much
influence on the outcome  we believed post features captured much of what user features had to offer 
there was a clear issue that insufficient positive test examples were classified as such  and the issue
was more serious from svm than from logistic regression 
on a separate note  our model worked better for answers of questions with higher view
counts  reaching approximately     test accuracy 

fifigures    predicting answer receiving at least two upvotes  test accuracy
figure    predicting answer accepted  test accuracy

    

user in java

post

user in java   post

   
   

                 

     

           

           
     

   
   
   
svm li 

svm gau 

logistic regression

response variable   answer accepted
ten fold cross validation test accuracy

ten fold cross validation test accuracy

response variable   upvotes     

    

user in java

post

user in java   post

   
   

                 

           

           

     
     

   
   
   
svm li 

svm gau 

logistic regression

approach  
in our last approach  we focused on predicting answers being accepted  as we were
predicting which answer got selected as the best answer by the user who asked the question  we
eliminated all answers that did not have a question with an accepted answer  furthermore  we
removed all answers that were posted after an answer had already been accepted because each
question could only have one accepted answer  to mitigate the shrinkage in the sample size  we
expanded the prediction period to include all answers posted in march   to may         for questions
posted during the same period  concentrating on valuable questions  which we defined as questions
with at least     views and had accepted answers within the same time frame  we were left with
      samples  within this constrained sample        of answers were accepted 
we introduced new features after realizing that  in order for us to better predict whether an
answer would be accepted or not  we needed to take into account the presence of other answers under
the same question  for example  user a might have really good background history in the domain
and really fast response time  but that did not mean his answer would necessarily be accepted
because he had to be compared with other answerers  user b might have even better background
history and even faster response time  therefore  where applicable  for each existing feature which
pertained to user history in java or the post  we added a within question rank feature  importantly 
within each question  we ranked each answerers number of accepted questions in the past to
represent the answerers relative standing among its competitors for the question  we also ranked
the response time of each answer within each question because response time itself was extremely
right skewed  even after taking log 
we analyzed feature sets with    answerers history in java only     answer post specific
measures only     combining user in java and post specific measures     combining user in java 
user in android  and post specific measures  and    our stepwise search result of combining response
order  number of answers in the question  and relative rank of the number of accepted answers of the
answerer among those who responded to the same question 
figure   shows that  with logistic regression  combining user in java and post specific
measures yielded the highest test accuracy of        however  it was only marginally better than the
      obtained with our selection of only three features  given the proximity of training and test
errors  we believed the problem of high bias had persisted  nave bayes and linear discriminant
analysis results were very similar to those from logistic regression 
svm with a gaussian kernel and a cost parameter of   painted a somewhat different picture 
the model was overfitting  obtaining      training accuracy but     test accuracy with large sets

fiof features  the second to fourth sets aforementioned   test accuracy benefitted from reducing the
number of features  it improved substantially to       with our selection of only three features 
figure    predicting answer accepted with original and rank features  test accuracy
response variable   answer accepted
user in java

post

user in java   post

user in java   android   post

  selected features

ten fold cross validation test accuracy

    

   

     

   

     

     

     

     
     

     

     

     

     

   

   

   
svm gau 

logistic regression

for both logistic regression and svm  we observed that false positives were the biggest
source of our test error  especially for svm  we believed a flaw of our approach was that we allowed
the model to predict acceptance for more than one answer for the same question  as it performed the
classification of each example independently from that of other examples 
conclusion and future work
for this project  we came up with several approaches in tackling the two predicting tasks and
obtained reasonable results  during our efforts in improving performance in the last approach  we
discovered several more complex problems in predicting whether an answer would be accepted or
not  if we not only looked at features of an answer post and the answerers history but also kept in
mind its competitors  we could improve our results significantly  a similar approach could be
applied to the upvotes predicting problem given the reasoning that users would probably only vote
once under the same question because casting a vote is not free in stack overflow  although we
incorporated rankings within each question  our approaches remained to suffer from not taking
account into competition constraints such as only one answer can get accepted in a question 
finally  for future work  we propose to examine features related to the contexts of the questions and
answers  such as body lengths and word associations  to improve performance 
references
    a  anderson  d  huttenlocher  j  kleinberg  and j  leskovec  discovering value from community activity on
focused question answering sites  a case study of stack overow  in acm sigkdd international conference on
knowledge discovery and data mining       
    j  zhang  m s  ackerman  and l  adamic  expertise networks in online communities  structure and algorithms 
proceedings of the   th international conference on world wide web       
    c  chan  a  hsu  and c  yea  status differentiation between sub communities in stack overflow       

fi