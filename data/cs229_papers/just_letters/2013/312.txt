distinguish hard instances of an np hard problem
using machine learning
zhe wang  tong zhang and yuhao zhang

abstract  graph properties suitable for the classification of instance hardness for the np hard
max cut problem are investigated  eigenvalues of the adjacency matrix of a graph are demonstrated to be promising indicators of its hardness as a max cut instance 

   introduction
many combinatorial optimization problems arising in various areas are np hard in computational
complexity  they are commonly believed to be intractable  given that all previous attempts of
finding a polynomial time algorithm have failed irrespective of a considerable amount of effort  a
typical strategy for tackling these problems is to use approximation or heuristic algorithms to find
solutions relatively close to the optimal value  these algorithms usually demonstrate reasonable
performance on realistic applications  however  without the knowledge of the hardness of these
applications  it is questionable whether identical performance would be achieved in general  
this project is concerned with identifying hard instances of an np hard problem according to
some features calculable in polynomial time using machine learning algorithms  the problem under
study is the max cut problem  given an unweighted graph g    v  e   where v and e denote
the set of vertices and edges respectively  the max cut problem is to find a cut  s  v   s  such
that the number of edges with one endpoint in s and the other in v   s is maximized over all
possible cuts      the best known approximation algorithm for the max cut problem utilizes
semidefinite programming and establishes the celebrated         performance guarantee      in
practice  many heuristic algorithms based on randomized local search achieve better performance
than the         approximation algorithm         since local search is subject to being trapped
into local optima  it is intuitively plausible to consider instances with a large ratio of the number
of local optima to global optima to be hard but those with a small ratio to be easy  however  since
the computation time of directly evaluating the ratio increases exponentially with the problem size 
it requires alternative efficient methods to estimate the instance hardness for large problems 
the paper is structured as follows  section   describes the data sets under study and explains
the method to determine the hardness of the max cut instances  section   explores possible
intrinsic properties of the graph to be utilized as indicators of its hardness  section   presents the
results of predicting instance hardness using the eigenvalues of the adjacency matrix of a graph 
finally  section   concludes the paper with remarks on possible future directions of the work 
   data sets
the current study focuses only on cubic graphs for the following reasons  first  the max cut
problem restricted to cubic graphs is itself np hard      therefore  hard instances of the problem
are still included in the select data sets  second  the existence of common attributes among cubic
graphs easily excludes some properties as promising features  for example  since all bridgeless
cubic graphs contain a perfect matching according to petersens theorem      the matching number
is unlikely to be meaningful for the prediction of instance hardness  third  finite number of graphs
of the same order helps reduce the bias and the required number of instances in the training sets 
 

fitable    number of cubic graphs     
order
              
  
  
  
  
cubic graphs                                          
the hardness of each instance is determined according to its ratio of the number of locally
maximal cuts to globally maximal cuts  for a cubic graph of order n  it requires to exhaust o  n  
number of possible cuts by brute force to obtain the ratio  meanwhile  the total number of cubic
graphs increases super exponentially with the graph order  as shown in table    therefore  only
graphs of orders up to    are examined due to the limited computational resources 
since the ratio tends to increase with the graph order  the hardness is relatively measured within
instances of the same order  the k means method are utilized to divide the instances into two
clusters  and those assigned to the centroid with a larger ratio are labeled as hard while the
remains are considered to be easy  fig   displays the classification result for graphs of order   
and the shift of the cluster centroid positions for different graph orders  the decision boundary is
fitted well by an exponential curve  which captures the exponentially faster increase of the number
of local optima compared to global optima with the graph order on average  if the fitted curve
already indicates the true asymptotic behavior of the decision boundary  it provides information
on the estimated lower bound for the number of local optima for hard instances of arbitrary order 

figure    left  probability distribution of the ratio of the number of local optima
to global optima on cubic graphs of order     right  positions of the cluster centroids
and decision boundary for each graph order  the data points for order    are results
of      random cubic graphs 

   feature selection
the possibility of using a particular graph property as a feature for the classification of instance
hardness has been analyzed through calculating the mutual information between it and the instance
hardness label  which is given by
xx
p x  y 
mi x  y   
p x  y  log
 
p x p y 
x
y
three efficiently calculable properties of graphs have been investigated  global clustering coefficient
 gcf       diameter and eigenvalues of the adjacency matrix of the graph  even though a highlyclustered graph is intuitively more difficult to solve than a sparsely connected graph  the mutual
 

fiinformation between gcf and the hardness label is almost    thus  it is highly unlikely to be a
relevant feature  diameter is also excluded as a suitable feature for the same reason 
on the other hand  non trivial mutual information is discovered between the eigenvalues of
adjacency matrices of the graphs and the hardness label  as shown in fig    a cubic graph of order
n has eigenvalues               n     all of them may potentially function as useful
features except for the largest eigenvalue   since it is a constant for all instances  the relatively
higher mutual information achieved by   and n than the remains can be explained from their
relations to the structures of the graph  the edge expansion of a graph is bounded by functions of
  as shown in cheegers inequality      and the bipartiteness ratio of a graph is in close relation
with n       in addition  an upper bound of the maximum cut for a d regular graph is given by
      n   d nd        

figure    mutual information between the eigenvalues of the cubic graphs and the
hardness label 
   classification results
machine learning algorithms such as logistic regression  naive bayes and support vector machine
 svm  have been applied to predict instance hardness using the eigenvalues of the adjacency matrix
of a graph  the svm algorithm with the gaussian kernel


  x  z   
k x  z    exp 
   
yields the best classification results  fig   visualizes the outcomes of using the second largest
eigenvalue   and the smallest one n to categorize the hardness of instances within the same order 
it can be easily seen that hard and easy instances appear to form clusters on the plane defined
by   and n   meanwhile  the clusterings for graphs of different orders and decision boundaries
are similar in some degree  which suggests the possibility of using learning models obtained from
graphs of lower orders to predict the hardness of instances of higher orders 
to evaluate the effectiveness of using eigenvalues to identify hard instances  training sets consisting of     hard and     easy instances are randomly generated for cubic graphs of order from   
to     it is verified that larger training sets only produce very limited improvements on accuracy 
therefore  training sets with      instances for graphs of order     and      instances for graphs of
order    and    are mainly used for the investigation 
the performance of the svm algorithm is tuned by adjusting the number of features and the
parameter  in the gaussian kernel  eigenvalues at the head and tail of the sequence are mainly
considered because they have relatively larger mutual information with the hardness label  if the
 

fifigure    svm classification results for randomly selected     hard and     easy
instances of order    and    using   and n when        and the cost constant
c      the eigenvalues are normalized to quantities with mean   and variance   

figure    prediction accuracy using different number of features and s in the
gaussian kernel  the values are obtained by performing k fold cross validation on
training sets consisting of      randomly selected cubic graphs of the same order
when k     
number of features are determined to be      and n will be utilized  and if the number is decided
to be          and n  and n will be selected  and so on  fig   shows that the algorithm
performs reasonably well with a small number of selected features and  in the interval            
the prediction accuracy decreases with the number of eigenvalues  which might be caused by the
fact that data points appear to be sparse in a higher dimensional space  meanwhile  it can also be
observed that when  is chosen to be too small  even though the learning model can achieve above
    accuracy on the training sets  it introduces large generalization error due to overfitting  on
the other hand  when  is too large  the model becomes not accurate enough even on the training
sets themselves 
table   lists the prediction accuracy of the learning models on instances of both the same and
different orders as those in the training sets  in the former case  the accuracy is approximately    
on all randomly selected training sets  although the absolute value does not appear so attractive 
the achieved result is still considered to be reasonably good given that eigenvalues and the ratio of
the number of local optima to global optima are two seemingly uncorrelated quantities  meanwhile 
it is not surprising that the accuracy decreases when learning models obtained from training sets
of other orders are utilized  as demonstrated in fig    the decision boundaries seem to be shifting
 

fitable    prediction accuracy of the svm algorithm with the gaussian kernel using
  eigenvalues as features when        and c        the same order prediction
results are the average accuracy obtained from the training sets using k fold cross
validation  whereas the cross order prediction uses the learning model obtained from
the training set of order    to classify instances in training sets of other orders 
instance order
  
  
  
same order prediction                     
cross order prediction       
      
slightly for different orders  it requires more work in the future to understand the variation of the
boundaries in order to improve the cross order prediction accuracy 
   summary
eigenvalues of the adjacency matrix of a graph are discovered to be promising features to classify
instance hardness for the max cut problem  hard and easy instances are shown to form clusters
in the space spanned by the eigenvalues  for the studied instances  the learning models based on
the svm algorithm can achieve up to about     accuracy in predicting the hardness of instances of
the same order  since the clusterings are similar for different graph orders  it is expected that better
understanding of the decision boundaries will help increase the accuracy of cross order prediction 
references
    r  m  karp  in complexity of computer computations  edited by r  e  millera and j  w  thatcher  plenum
press        pp        
    m  x  goemans and d  p  williams  j  of the acm                      
    s  burer  r  d  c  monteiro  and y  zhang  siam j  on optimization                    
    p  festa  p  m  pardalos  m  g  c  resende  and c  c  ribeiro  optim  methods software                     
    m  yannakakis  in proceedings of the tenth annual acm symposium on theory of computing  ieee computer
society  san diego  ca        pp        
    j  petersen  acta mathematica                   
    g  royle  http   www easybib com reference guide mla website  oct       
    duncan j  watts and steven h  strogatz  nature                   
    n  alon  combinatorics                    
     l  trevisan  in proceedings of the   st acm symposium on theory of computing  new york  ny        pp 
        
     c  delorme and s  poljak  mathematical programming                   

 

fi