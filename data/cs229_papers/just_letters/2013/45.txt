phonation detection system   final report
you yuan  anwen xu  and junwei yang 
 

i 

 
electrical engineering
civil environmental engineering

introduction

the analysis of human singing voice brings meaningful
insights of peoples vocal resonance and assist people to
do self vocal training  the resonance condition can be
detected through various spectrum techniques  e g  dft 
mfccs  which extract dominant frequency  harmonics
and other spectral components in the voice that are not
easily detectable in time domain waveforms 
we provide a reliable and robust phonation detection
system  which helps detecting peoples singing resonance
position based on machine learning models  svm  neural networks  decision tree  as well as spectrum analysis
techniques  dft  mfccs  etc   models are trained with
a labeled supervised learning dataset recorded from both
vocalization professionals and non professionals  and a
phonation detection and scoring system is also implemented 

ii 

data collection

to ensure good quality of the supervised learning
datasets  all voice clips are collected from stanford music professors teaching vocalization and outstanding students recommended by the professors  the samples are
recorded with the same equipment in various but relatively quiet environments in order to make our models
prediction more accurate  based on vocalization theories 
the resonance positions can vary horizontally  from head
to chest resonance  and vertically  from backward to forward resonance   thus the supervised learning dataset
covers samples of these two dimensions and are labeled
into   categories  the entire supervised learning dataset
contains     labeled vowel clips  including male and female voices and over different major scales 

a 

audio data preprocessing

in the learning dataset  each recorded voice clip is tailored into valid pieces each with only one note  and normalized to have the same magnitude scale  for testing
continuous singing clips  the recorded voice clip will be
cut into small overlapping clips for the test system which
will be explained in part v 

b 

spectrum feature analysis

once each voice clip is tailored and normalized  the
spectral features are extracted for each voice clip  after
careful evaluation of audio energy  perceptual  temporal 
spectral features  the most statistically significant dft
features and mfccs are selected for further model training 

  

dft diagram

different vocalization methods alter the position of resonance inside the body  and produce distinctly different
frequency bandwidths and magnitudes  as shown in figure    

labels backward balanced forward
head
       
      
      
middle        
      
      
chest         
       
       
table i  classification of resonance positions

iii 

feature extraction
figure    head voices dft vs chest voices dft

feature extraction includes three steps  audio data
preprocessing  spectrum feature analysis  and training
matrix preparation 

to exploit such types of differences  we apply hamming window of size      to sample each note clip with

fi 
    partial overlap  after discrete fourier transform  we
choose the median values from above spectrum  in order
to reduce the variance in dataset  we averaged the magnitudes of   neighboring frequency samples  which reduces
the resolution 
in figure    the commonness in the same group and the
difference between different group is conspicuous  head
voice often has a powerful and centralized peak  while the
energy of chest voice is distributed in a much flatter way 
although we implement a simple svm model only based
on dft features  its accuracy of head chest resonance
prediction is only      therefore  we investigated other
features to improve the overall performance  based on
p value evaluation results  we select the frequencies and
normalized magnitudes values of five toppest peaks from
dft diagrams as dominant features 
  

mfccs

mfccs are used extensively in speech and speaker
recognition  essentially  they represent the discrete cosine transform of the log spectrum of a signal analyzed
on an auditory frequency scale  the mel scale   the process creates a    dimensional vector that summarizes the
signals spectrum  we included mfccs to represent the
differences in the shape of the spectrum for different signals 
c 

training matrix preparation

as mentioned above     features     mfccs    
dfts  are extracted for each voice clip  the feature matrix is constructed with each row containing the    features of one voice clip  which is of size           in total 
the target matrix is constructed with vertical and horizontal resonance position labels for each voice clip  which
is of size           additionally  the feature and target
matrices of male and female are separated for independant model training  given the fact that male and female
demonstrate completely different vocalization characteristics  table ii below shows the actual feature and target
matrices utilized by model training 
horizontal target
vertical target
feature matrix         feature matrix        
target value       
target value       
female feature matrix         feature matrix         
target value       
target value        
male

table ii  four feature matrices and target matrices size

iv 

machine learning model

for each of feature matrix and target matrix with different gender and resonance dimension  the following ma 

chine learning models are trained and used to predict the
singers phonation condition 
   logistic regression model
   support vector machine
   neural network
   decision tree

  

logistic regression model

logistic regression is implemented using standard gradient descent  this model is based on the fact we learn
from dft diagrams that generally head voice contains
purer higher frequency components while chest voice contains wider range of lower frequency and the assumption
of linear relations between the logit of the explanatory
feature inputs and the classification results  the performance of logistic regression acts as our performance
baseline for other models evaluations 

  

svm

svms with different kernels are implemented using
libsvm package  for each of the four groups  a separate
model selection is performed separately using different
svm parameters 
parameter selection   we adopt c svc  standard
multi class classification  for all groups of dataset  in
order to discover the locally optimum value of each
parameter  an automatic svm options selected program
is created and applied  for each model  common kernel
types  number   to    degrees and    logarithmically
spaced values of  were evaluated based on cross validation results  models with maximum cv values are
chosen as final svm models 

kernal type
degree

coeff 
cross validation
kernal type
degree

coeff 
cross validation

male vertical
polynomial
 
 e   
 
      
male horizontal
polynomial
 
 e   
  
      

female vertical
polynomial
 
 e   
 
      
female horizontal
polynomial
 
 e   
  
      

table iii  svm parameters

performance analysis   as shown in table iii  svm
models for vertical resonance groups perform far better

fi 

figure    decision tree  female vertical resonance position

than horizontal resonance groups 
intuitively it is
much easier for human ears to detect head voice from
chest voice than to distinguish forward resonance from
backward resonance  because vertical resonance could
partially be differentiated by pitch while the difference
in horizontal resonance is somehow subtle 

  

neural network

given the complicated nature of vocal resonance  a
             neural network model is also considered and
implemented  which includes one input layer  two hidden
layers with    neurons and transfer function tansig
and one output layer with transfer function purelin
from matlab tool box  the number of neurons are
selected based on iterative tests with different parameter combinations  in total four neural network models
are trained separately for different combinations of gender of the singer and resonance classification directions 
male horizontal  male vertical  female horizontal  femalevertical 

  

decision trees  however  the features  which represent
the locations of different dfts peak  are not used in the
decision tree  one of the tree model is shown in figure   

  

model selection evaluation

after running all the models above  based on the performance accuracy  as shown in figure   and     the best
models are selected for prediction 

figure    vertical resonance  model accuracy comparison

decision tree

decision tree is trained via the classification tree functions inside matlab machine learning package  each
combination of the feature matrices and target matrices  male horizontal  male vertical  female horizontal 
female vertical  is trained separately to generate decision
tree models  through analysis of features used to determine the decision tree  it is shown that the first   
features  the mfccs  and the final   features  the magnitude of the peak dfts  have more importance in the

figure    horizontal resonance  model accuracy comparison

fi 
as mentioned  the logistic regression model acts as
the baseline for our model evaluation  the svm model
works best in determining the vertical resonance position 
with approximately     accuracy for female and     for
males  which is a significant improvement from the baseline      for female and     for male   but in horizontal
direction  decision tree works best of the   models with
accuracy of     for females and     for males  which are
relatively higher compared to other models  this phenomenon can also be explained by our feature choices 
mfccs and dft features are more related to the vertical resonance position of singing than to horizontal ones 
based on the best models chosen above  the misclassification rate for each of the class labels are also calculated to
evaluate and analyze the model performance  as shown
in figure         and    

figure    misclassification rate of decision tree model for
male horizontal

only    error rate  however  the class label    head
voice  has an error rate of        for the svm model to
detect the males vertical resonance position  the prediction of class label   becomes very accurate       error
rate  while class label   has an error rate of        for
the decision tree model  which is to detect the horizontal
resonance position of male and females  class label    forward voice  is minimally classified with    error rate to
predict females horizontal position and      to predict
the males horizontal resonance position 

v 
figure    misclassification rate of svm model for female
vertical

phonation detection system

a detailed flow graph of the phonation detection system and how user interface interacts with the backend is
displayed in the figure   below 

figure    misclassification rate of svm model for male vertical

figure    vertical phonation detection system

figure    misclassification rate of decision tree model for
female horizontal

from the figures  it is illustrated that for svm model
to detect the female vertical resonance position  the class
label    middle voice  prediction is very accurate with

as discussed in part iv    svm and decision tree
models are trained for vertical and horizontal classification uses  resulting four models of different resonance dimension and gender combinations  when a test sample
is recorded through ui  the voice clip is tailored to unit
test clips  such test unit clips will be classified based
on svm and decision tree models with the corresponding gender  and the prediction with highest confidence
will be selected as vocalization detection system output  also  the phonation detection system will perform

fi 
vi 

future work

the phonation detection system works well with most
testers on the poster day  but we would like to extend
the work continuously to make the system more comprehensive and accurate 
   in this phonation detection system  we didnt distinguish vowel as all testers are required to sing
with vowel ah only  however lots of relevant research have found out  different vowels represent
distinguishable harmonic content  this may raise
challenges to classify resonance position and distinguish vowel with spectrum analysis at the same
time 
   there is still lots of other meaningful phonation
classification that will be helpful in vocal training 
for example  classical vocal professionals distinguish themselves from contemporary singers by lifting their soft palate to have a more operatic
sound  such differences might be extracted through
both spectrum analysis and sound energy analysis 
acknowledgments
figure     user interface

a weighted euclidean distance calculation between testing voice clip and the most similar classified centroid 
and the distance will be projected to         range as the
performance score 

we would like to extend our gratitude to all those
people who helped and supported us in the project 
we would like to thank professor claire giovannetti 
professor wendy hillhouse  professor gregory a  wait 
yuanyuan wen and chenjie luo for kindly sharing their
music knowledge and providing their voice clips 

    zhu li and yao wang audio feature extraction and
analysis for scene segmentation and classification      
volumn     issue         
    ingo mierswa and katharina audio feature extraction
for classifying audio data     
    bryan huh and arun miduthuri vocal based musical
genre classification     
    pat taweewat detection of a specific musical instrument
note playing in polyphonic mixtures by extreme learning machine and partical swarm optimization      
volumn    issue  
    greg sell  gautham j mysore and song hui chon musical instrument detection     

    francesco camastra and alessandro vinciarelli machine
learning for audio  image and video analysis     
    polina proutskova  christophe rhodes  tim crawford
and geraint wiggins breathy  resonant  pressed   automatic detection of phonation mode from audio recordings of singing       volumn     issue          
    stefan steidl vocal emotion recognition  state of theart in classification of real life emotions     
    nathalie henrich  john smith and joe wolfe vocal tract
resonances in singing  strategies used by sopranos  altos  tenors and baritones     
     giovanni de poli and paolo prandoni sonological models
for timbre characterization     

fi