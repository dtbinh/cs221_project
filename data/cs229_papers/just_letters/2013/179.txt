detectingwebpageswithevents
matan zinger

junmin hao

mzinger stanford edu

junmh stanford edu

abstract
the semantic web is a concept being promoted by the world wide web consortium 
with the vision of allowing machines to understand webpages  without maintaining scrapers of humanvisible html 
this structured markup in webpages have recently been powering features in google search  such as rich snippets    
the project purpose is detecting web pages that contain information about events  based on the main text in the page 
thus making it possible to reach the websites owners  suggesting they add structured data for the events in their site 

   introduction
traditionally  web pages were designed to be read by
human beings  web visualization technologies have
been evolving significantly fast over the last two
decades  allowing websites to provide rich visibility
to their users  in parallel  a growing trend of
scrapers has been taking place  powering systems
that utilize information aggregated from other
sources in the web  while some of these scrapers
may be considered spammers  others actually create
a mutable benefit for both the content provider and
consumer  e g  a video store website presenting imdb
reviews and ratings before rental  and encourage
customers to add a review to imdb afterend  

owners  with a suggestion andinstructions regarding
addingeventsstructureddatatotheirwebsites 
the approach we take is building a quick basic
classifier  keeping in mind possible optimization steps
that could be taken in the future  in order to evaluate
the initial performance and choose the enhancements
that would yield the best impact on our classifier 
our preference is as high precision as possible  even
by hurting recall  since the effect of sending spam
suggestions to webmasters is very undesirable 
   data gathering   preprocessing
in order to gather a labeled set of web urls  we have
used the open directory project  odp       which
contains an hierarchical set of over        
categories  in each category  a few urls that were
placed after a human process of content evaluation 

the biggest pain is that making machines
understand information modelled in a fashion
designed for human consumption  such as html  is a
tedious task that requires frequent maintenance  an
alternative might have been having websites provide
public api to access the data presented in their web
pages  but a proprietary api would also make data
integration an effortconsuming task 

in order to gather positive examples  we have
selected all hierarchical categories where the leaf
category is events  e g 
top sports running events 
top arts entertainment events  

during the last decade  the semantic web initiative
created open standards for defining structured data
on top of webpages  as well as standards for
repositories and query languages on top of them 
making it possible to build semantic applications 

in order to gather negative examples  we have
selected all category paths that did not contain an
event topic in any of the path element  e g 
top arts crafts paper 
top business arts and entertainment photography
 photographers  

google also contributes to the semanticweb vision 
by utilizing structured markup detected in webpages
in different google search features  the most famous
of which being richsnippets  these improve both the
experience of a user going through web results  as
well as allowing website to surface further
information in its search result snippet 

once gathered the lists of positive and negative
example urls  we have taken the following
processing steps per each url 
 fetching   text extraction
the boilerpipe     open source  under apache
license      library in order to fetch the content
of these urls  parse the html  discard
boilerplate    and extract the main text content
of the url 

the purpose of this project is to utilize signals from
webpage textual content in order to detect pages
that contain information about events  making it
possible to make a mass outreach to event website
 

fi stemming
in order to avoid contribution of specific
inflections  words were reduced to their root
form  stem  using a python implementation of the
porter stemmer    
 transforming into term vector
the last dataprocessing step weve done is
transforming each document into a term vector
 by using        different tokens as features  

  support vector machine

having a set of       examples  represented as term
vectors  we splitted is it a random fashion into a
training set  with     of the documents  and a test
set  the rest of the documents   while doing so  we
made sure each odp category in use has
representatives in both training and test sets 

since both algorithms have shown similar levels of
accuracy during the first execution  we have also
measured the precision and recall  the numbers in
the table below are the numbers of positives or
negatives divided by the number of test examples  

the training set was splitted in a similar fashion into
subsets  making it easier to measure the learning
rate  as well as performing cross validations 

actual  positive

actual  negative

   classification

nb positive

      

      

the algorithms we have evaluated were 
 naive bayes with laplace smoothing 
 linear l regularized l loss softmargin
support vector machine 

nb negative

      

      

svm positive

      

      

svm negative

      

      

per each  we trained a model with the different
subsets of the training set  each produced a
classifier   we measured the error rate of both the
training set and test set with each classifier 

naivebayes precision          recall         
svm precision          recall         

   performance analysis

  naivebayes

as these are the components of our classification
pipeline  the following parameters essentially
determine the classifier quality 







webpage cleansing
text preprocessing
choice of terms list
features  except term frequency 
choice of algorithm
svmchoice of algorithm parameters 
regularization term  soft margin c 
 choice of test set
the most indicative terms for events  m    ct 
marathon  fundraiserjoin  m   

while building the initial execution  we had ideas of
how to improve all of the above  in more than one
manner   however  we intend to try and evaluate
which of these could yield the best improvement in
classification quality 

 

fia strongly noticeable problem is the   meaningless
terms out of the   most indicative terms in the
naivebayes classifier  this can be either due to the
intrinsic flaw in nb  or due to our overpermissive
policy to consider every possible term  however
extremely noticeable  we preferred further analyzing
the results  before choosing our next step 
considering our strong preference for precision  the
svm seems like a better candidate to be examined
 given the lower falsepositive rate it yielded  
judging from the learning curve  both the extremely
low training error rate  and the fact that the
marginal improvement in accuracy is negligible 
indicate a problem of overfitting  in order to reduce
variance  useful steps would be using less features
and then using more training examples 

using a      documents test set  the plot still seem
like a typical highvariance case  but the increased
set still improved accuracy by      percentages 
 additional preprocessing steps

in addition  in order to reduce the variance of the
svmbased classifier  c should be reduced 

at the previous step  weve noticed the indicative
terms seems radnsom  such as m     weve added
the following steps in order to eliminate these terms 
   filteringnondictionaryterms wehaveusedthe
englishwordslistincludedintheunixfilesystem at
 usr dict words tofilternonenglishwords 
   identificationofurlsandemailaddresses we
havedefinedregularexpressionsforthispurpose and
replacedtheseinstanceswithapredefinedtokens
 email url  
   filteringofsingleletterterms 
   filteringstopwords useingtheranks nl    list 

   performance optimization
while the previous steps were mostly about gathering
data and establishing a classification baseline  the
next purpose was improving quality  mainly 
precision   we therefore engaged in the following 
 increasing training examples set
the first action we took towards solving the variance
problem was to gather more training examples 
in order to gather more negative examples  our
boilerpipe based pipeline was fetching the data of
     additional nonevent urls  all of which were
retrieved from the odp dataset  we found it
important to increase the proportion of the negative
examples  after noticing the positive examples are
strongly correlated with stopwords  we eventually
increased the negative portion from     to     
in order to retrieve additional positive examples
 after using all positive examples found in the odp
dataset   we have used the sindice  semantic web
index       and executed a query for urls that have
already been marked up with structured data of type
http   schema org event  we suspected that using
an additional source for urls  other than odp  would
increase the diversity of texts  and might be useful in
facing the overfitting sample 

these additional steps have reduced the number of
tokens from         to         as overfitting is our
main problem  and reducing the feature set is one of
the main methods in solving it  we chose eliminating
meaningless features as the first step in this regard 

this yielded additional      positive examples  thus
concluding an increase of the dataset from      to
      documents 
 

fithis activity have also made the indicative terms list
appear to be much more relevant for the events case 

first  we implemented filter feature selection 
with two feature scorers  mutual information  and
indicativeness score  described in hw  q b   accuracy
did not improve  as seen below 

 event   festiv   toggl   ticket   particip   saturday   share  
 mathemat   password   address   venu   sponsor   descript  
 sunday   workshop   attend   twitter   date   circl   notifi   friday  
 miss   qualiti   busi   music  

 tokens appear in their poststemming format  
 tuning  reduced  value of c
another step aimed at reducing variance  we have
used the kfold cross validations technique in order to
examine different values of c  with setting k    and
calculating the error rate for each value 
 c c   

 
k

k

 s i 

     h  c c ss    x j 
     y  j 
 
i
i
i

i    j    

however  the positive outcome was raising the
necessity in a larger portion of negative examples 
since in the initial    positive data set  even a
dummy model that always predict positive yields    
accuracy  therefore weve increased the negative
proportion to      as previously described  

where 
  s i   ith slice of the examples set  out of k
equallysized slices 
  h    the hypothesis function generated when
 c s  



training the svm algorithm with c c over a subset s
of the examples set s 
  x j 
  the j th data point in the examples subset s i
i

the next method we tried was backwards feature
search  over the        features in the   k data set  
first  we removed infrequent tokens  i e  with less
than   appearances   to reduce feature set size to
  k  in order to speed the search  weve used  fold
cross validation  the fastest possible setting 

s 

  y  j 
  the label corresponding to x j 
 
i
i
using the  fold method  weve also counted the
number of truepositives  falsepositive 
truenegatives and falsenegatives  in order to
calculate the precision and recall rates for each
possible value of c  the results 
c
      
      
      
      
      
      
      
      
      
      
      

test error precision
      
      
      
      
      
      
      
      
      
     
      

      
      
      
      
      
      
      
      
      
      
      

at each backwardssearch step  the program iterates
over all features  and looks for the feature f i that
minimizes  i  

recall

 
k

k  s j  

     h  f f   ss    x j 
     y  j 
  
i
i

j   t  
 j 
 j 
xi   y i are

i

j

where s i  
as previously described  and
h  f s  is the prediction function generated when

      
      
      
      
      
      
      
      
      
      
      



training over a subset s

 s   and a subset f  f  

this feature shall be discarded at the end of the step 
after    iterations  only      of all features   the
accuracy has increased from       to        
however  each iteration took    minutes to run  and
completing the algorithm could take up to a month 
therefore we looked to speed up this process  by
executing each iteration in a parallel fashion  the
errors per each feature removal   i   shall be
computed in parallel  and when all tasks complete 
the minimal results indicates which feature to drop 
however  due to matlab licensing issues  we could
not have ran this version before the deadline 

the optimal value found is c          the accuracy
has been improved by      percentages  and precision
by      percentages 
 feature reduction
we were looking to further eliminate features 
 

fi tuning for better precision

learning theory lectures  and implemented all the
methods efficient at reducing variance  the most
effective method proven to be reducing c  which
improved accuracy by      percentages  second 
adding training examples increased accuracy by     
percentages  third  backwards search improved
accuracy by      percentages  and further iteration
could yield more improvement 

training a svm model yields values for w and b  given
a data point x  the amount    w  x   b is the
distance from x to the decision boundary  by default 
when        the model predict it as a positive  and
negative when        the larger  is  the more
confident the model is about x being positive 
therefore we can use a threshold t  different than
zero  to modify how we make the prediction  that is 
when    t yields a positive prediction  and negative
otherwise  by tuning t  we could control the
confidence for positive predictions  i e  precision  

last  weve tuned the level of confidence for
considering a webpage as being about an event  and
found a value for which precision is over      and
recall is        while in general this is not necessarily
a good result  it could very well fit the specific case
we would like to solve 
our classifier could potentially make it possible to
find half of the event webpages on the web  and
approach their owners with high        confidence 
to suggest adding semantic event markup 
that being said  testing over larger tagged webpages
sets is required in order to verify this statement 

its important to note that the threshold actually
controls a precision recall tradeoff  and by increasing
precision  the recall rate decreases 
precision tuning results 

   future enhancements
in order to further improve the classifier  the first
step we would have take is to complete the feature
reduction process  by being able to make a parallel
execution of the backwards search algorithm  this
can initially be done on the cores of a single machine 
as we described in the last paragraph under feature
reduction 
the precision seem to have an upper bound around
        where threshold        recall rate          
as the threshold further increases  the recall rate
eventually drops to       at t    at the precision
rate only increased to        

another possible enhancement is in the preprocessing
pipeline  currently phone numbers  dates and
geolocations are being filtered out on the account of
not being in the english words list  however  since
dates and locations are the major properties of an
event  we speculate that these could be indicative
signals  in order to examine that  a method to detect
such terms should be plugged in  and map them into a
constant term  similar to whats being done with
urls and email addresses  

   summary   conclusion
our work was mainly composed of   major activities 
first  we followed the practice of quickly building an
initial version of the classification system  to be
gradually enhanced based on experiments results  
weve implemented a basic platform to fetch and
process web pages into term vectors  based on
common practices   implemented the two text
classification algorithms learned in class 

references
    rich snippets  events 
http   support google com webmasters answer       
    http   www dmoz org docs en about html
    http   code google com p boilerpipe 
    christian kohlschtter  peter fankhauser  wolfgang nejdl 
boilerplate detection using shallow text features
    m f porter  an algorithm for suffix stripping 
    sindice  a semantic web index  http   sindice com 
    ranks nl stopwords list 
http   www ranks nl resources stopwords html

second  weve analyzed the results  which indicated
an overfitting problem 
third  weve applied the practices given in the
 

fi