generative models of music
mike kayser
cs    

introduction
we propose a markov model based generative model for music  latent variables in the model capture key and
chord progression information  we train the model in a completely unsupervised setting and qualitatively
investigate the accuracy of the latent variable assignments  we also quantitatively test the model on a composer
identification task  results indicate that the model can learn simple structural features such as key  but that
more complex tasks such as chord recognition may require a more sophisticated model and or the introduction
of supervised training data  both models are able to perform significantly better than chance on a composer
identification task 

motivation
markov models have been successfully applied to a wide range of sequence analysis problems  e g  in natural
language processing  generative models such as hidden markov models are attractive because they can be
easily utilized in a partially labeled or unlabeled scenario 
this is welcome for two reasons  first is the usual reason that labeled data may be hard to come by and that
there is far more unlabeled data  for example  in the form of raw midi or mp  files  than labeled data available 
second and somewhat more subtly  devising a reliable measure of ground truth for musical annotation may be
a non trivial task  for example  the same melody can often be harmonized in a variety of ways and there is often
no single correct underlying chord progression  it may therefore be more reasonable to preserve uncertainty as
a first class feature and model distributions of chord progressions  then  for example  if the task is to devise a
harmonization for a given melody  one can sample from this distribution to obtain a valid harmonic progression 
our goal in this work is devise a model which captures some of the latent structure of a piece of music  by
latent structure we refer to things like key and chord progression  our hypothesis is that models which
recognize latent structure can be more accurate for a range of downstream tasks such as melody generation 
harmonization  and composer identification 

related work
jan buys  appears to have done similar work in generative modelling of chord and note sequences  we
discovered his work too late to attempt any proper empirical comparison 

 

http   www cs sun ac za rw    files         j buys hons report      pdf

fimodels
we developed and trained two models  which we refer to as model   and model    all models are trained using
the em algorithm  first we will discuss some underlying modelling assumptions and simplifications common to
both models 

modelling assumptions















we assume that two notes separated by one or more complete octaves represent the same pitch  that
is  we collapse the set of all notes to a set of    pitch equivalence classes  for example  a high c note and
a middle c note are treated identically 
we assume that every musical piece has exactly one key which lasts for the entire piece  this is a strong
assumption and not generally correct  but it makes our models somewhat more tractable  in the future
work section we discuss potential methods of loosening this assumption 
we assume that for every root pitch class  there are two types of keys  major and minor  we associate
each of these two classes with a set of pitch offsets from root which classifies the allowable notes of
this key  for example  the major key type allows notes which are                   or    semitones above
the root key pitch  note that addition in pitch values is modular addition  as implied by the first
bulleted point above  see figure   for examples of pitch class sets defined by keys 
we therefore define a set of    possible keys     major keys  one for each root pitch  and    minor
keys  the allowable notes of any minor scale are defined by  what is music theoretically known as  the
natural minor scale  however  note that in general music theoretic terms the minor scale is more
complex than this 
we generate notes using a mixture model  for example  in model   we generate a note by first
generating a note source variable denoting whether the note was generated as part of the key or as part
of a background distribution 
when generating a note from a key  only pitches which belong to that key are allowed  all others are
fixed at probability zero  
the background distribution allows for passing tones and other non key notes  and is fixed as a uniform
distribution over all    pitch classes  in model   we introduce chords as a third type of note source 
see the section model   below for more information 
notes are generated as offsets relative to a base pitch  for example  in models   and    notes generated
from key or from background are generated as offsets relative to the keys root pitch  this allows us to
share parameters  e g   to treat the tonic note in the key of c major in one training melody as the same
event as the tonic note in the key of g major in another melody 
key
   c  minor
   g  major
   f  major

allowable notes  pitch equivalence classes 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
figure    selected examples of keys  there are    keys in total 

  
 
 

model  
model   can be thought of as a simple bag of notes model with key as a latent variable  the generative story is
as shown in figure    unless otherwise noted  all probability distributions are maximum likelihood estimated

fimultinomials  note that as stated above  the probability distribution over notes is defined over offsets relative to
the keys root pitch 
   generate a key root pitch p  from                            
   generate a key type t from  major minor 
   for each note of the piece 
a  generate the note source si from  key background  
b  if si key  generate ni using the distribution p ni p    si key  t 
c  if si background  generate ni using the distribution p ni p    si background 
figure    model   generative story 

model  
in model    we introduce the notion of chord progressions  specifically  a chord bigram model generates a chord
sequence given a key type  and each chord generates an unordered bag of notes  an important assumption is
that the time granularity of chord progressions is fixed  that is  a new chord is generated in sequence for each
fixed length time segment  in this case  the fixed segment length is defined in terms of fractions of a measure  in
our experiments we generally assumed that there was one chord per measure 
once the chord for a time segment is generated  the model generates all notes in the segment  again we use a
mixture model  in this case  there are three possible note sources  chord  key  and background  when
generating notes from a chord  only notes consistent with the chord are allowed  further  since in our model
every chord consists of   notes  we share parameters across all chords for the first  second  third  and fourth
note  that is  no matter what key or chord type  there are only four parameters to learn when generating notes
directly from a chord  in figure   below  bin ni c   is a random variable drawn from           denoting which of
the four chord notes is generated 
   generate a key root pitch p  from                            
   generate a key type t from  major minor 
   for each time segment of the piece 
a  generate the segments chord root note ci  from                              using the
distribution p ci p     ci   p    t 
 at time    the history consists instead of a special start token 
b  for each note in time segment i 
i  generate the note source si from  chord key background  
ii  if si chord  generate ni using the distribution p bin ni c    si key 
iii  if si key  generate ni using the distribution p ni p    si key  t 
iv  if si background  generate ni using the distribution p ni p   
si background 
figure    model   generative story 

qualitative investigation
before testing the trained models on a downstream task  we performed a qualitative investigation to roughly
determine the accuracy of the inferred latent structure 

fiwe trained on the bach cello suites  a selection of    pieces  we extracted note sequence information from
freely available midi files  we trained models   and   and investigated their output to determine how accurately
they determine  a  key  and  b  chord structure 

key detection accuracy
we found that in both models   and    the detected key is almost exactly     likely to be correct  in fact      
of the time in our anecdotal observation  it was either the correct key or a closely related key  the relative
major or minor  this is because of a quirk in the music theoretic definition of key  that is  for any major
key  there is a corresponding relative minor key which contains the same set of pitches  in practice  our model
decides randomly whether to use a major key or its relative minor  we thought this ambiguity would be resolved
by model  s notion of chord progressions  but in fact there is nothing strong enough in model   to break the
essential symmetry  one alternative would be to use priors to guide the model  for example  the root chord
should have high probability in both major and minor keys  such a prior would break the symmetry of the model
and hopefully lead to better convergence 

chord detection accuracy
using model    we find anecdotally that the chord detection accuracy is quite mixed  we did not perform a
quantitative study due to a lack of annotated data  but it appears that accuracy in highly melody driven 
monophonic pieces like the bach cello suites is below      it seems there are a few reasons for the low
accuracy 
   the model changes chords too frequently  getting confused by color and passing notes  it may be that
if notes were generated using an ordered markov model such as a note bigram model  the model could
learn that some notes merely serve as connectors between chordal notes  and thus that such notes
need not be explained harmonically 
   the model does not have an option to label an entire time segment as no chord 
   the chord model is music theoretically incomplete  in particular  due to the one key per piece
assumption  chords that belong to other keys  e g  the dominant key  are not directly allowed for in the
model  bach  among many other classical composers  makes extensive use of modulation and
tonicization  despite the fact that there is a natural home key for each piece   thus  the one key perpiece assumption is too strong 

experiments
we tested the utility of the learned models on a composer identification task  in particular  we trained  
different models on works by rachmaninoff  beethoven  mozart  and bach   overall  our data for training and
test consisted of    works by bach      works by beethoven     works by mozart  and    works by
rachmaninoff  we split data for each composer       between training and test  
we used these trained models to assign probability scores to the held out pieces of music  at test time  we
choose a label for the piece of music based on the highest scoring model  as shown in figure   below  the model
is able to perform at about     average accuracy  which is significantly better than chance  we are not aware of
any experimental setups which are directly comparable to this  so it is difficult to know how this result compares
to other modeling approaches 

fithe two models have significantly different composer specific performances  overall  it appears that model  
may indeed be slightly better  although it is difficult to say conclusively 
composer

model   accuracy
model   accuracy
   correctly classified 
bach
    
    
mozart
    
    
beethoven
    
    
rachmaninoff     
    
figure    model performance on composer id task 

future work
as mentioned above  there is much room for improvement in these models  these ideas can be grouped into
thematic areas such as  greater supervision  more sophisticated generative models  and non generative
modeling approaches 
   improved supervision
a  prime the chord bigram model with a repository of known chord progressions  this can be done
without specifically annotating the training midi files 
b  annotate some amount of data for hidden variables like key and chord progression 
   more sophisticated generative model
a  develop a markov model over keys 
b  allow notes to be generated by a note bigram model rather than a bag of notes model  this
might also involve introducing a new note source  passing  which allows the model to
generate a note adjacent to the previous note 
c  use bayesian methods to attempt to model self similarity in music  for example  one might
imagine a more sophisticated generative story in which a small vocabulary of themes is first
generated  and then the note sequence is generated by a process which often generates
transformations of the theme  and sometimes generates non theme related note sequences 
   use a different model  for example  it would be interesting to consider applying recurrent neural
networks to note generation  one possible approach would be to divide a piece of music into very small
frames similar to audio processing  a frame would be a vector  e g   a    dimensional vector 
denoting which notes are being played at a given very short time step  a rnn could be used as a
continuous state machine to model the dynamics of note frames  it would be interesting to investigate
whether an rnn can recover latent properties such as chord progression and key 

conclusions
we have developed a generative model of melodic and harmonic sequences  our model gives what appears to
be reasonable performance on a composer identification task  although there is much room for improvement  a
qualitative investigation into latent variable assignment indicates that some combination of better modelling or
more supervision may be necessary to correctly determine chord progressions 

fi