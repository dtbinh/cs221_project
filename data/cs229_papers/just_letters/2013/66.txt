rank hotels on expedia com to maximize purchases
nishith khantal  valentina kroshilina  deepak maini
december         

 

introduction

for an online travel agency  ota   matching users to hotel inventory is very important  as such  having the best
ranking  or sort order  of hotels for specific users with the best integration of price competitiveness gives an online
travel agency the best chance of winning the sale  imagine a hotel ranking system that predicts a permutation of
relevant hotels to a user such that a hotel at a higher position always has higher or equal probability of purchase
than a hotel at a lower position  such an order will ensure that a user sees the most relevant hotels first and increase
the likelihood that the online travel agency wins the transaction 
ranking algorithms are typically used in document retrieval  expert search  definition search  collaborative filtering  question answering  keyphrase extraction  document summarization  and machine translation applications
    
even though ranking algorithms are used extensively in many fields  their application in the online travel booking industry is new  data collected on travel booking sites are unique to the industry and present an interesting
opportunity for applying machine learning algorithms to understand what features are important and which not so 
furthermore  there is very little literature on which algorithm is best suited to obtain optimal results  in this paper 
you will find our analysis of some of the most popular algorithms and their relative performance 

 

approach

learning to rank is a supervised learning task and thus has training and testing phases  in our case  training data
consists of search history of different users  including data on     hotel characteristics      location attractiveness
of hotels      users aggregate purchase history  and     competitive ota information  each search is represented
by a    dimensional vector  the target vector for each search consists of two class variables  click and book  and
a continuous variable  gross purchase amount  
suppose that qi    qi    qi    qi            qim   is a set of hotels returned for a search query i and suppose that yij is
a relevance label for a hotel qij   let yij         where   means that the hotel was booked and   means the
hotel wasnt booked  therefore  label   is of higher relevance than label    we aim to train a ranking model
that can assign a score to a given hotel search pair  each hotel qij in search i have a set of features  some of which
are hotel specific  others search  user  and competitive pricing and inventory specific 
ranking is finding the permutation i of  i    i    i            im   for a given search i using the scores given by the
ranking model  after training the ranking model we evaluate its performance on a held out test data  the trained
model predicts rank of hotels in the test data set and the predicted order is compared against the observed relevance labels  evaluation measures  such as ndcg  normalized discounted cumulative gain   dcg  discounted
cumulative gain   map  mean average precision   are widely used for ranking algorithms  in this paper  we use
ndcg  as defined in    

 

fin dcg n    zn

n  r j    
p
j   log     j 

where n denotes position  r j  is the label at position j  zn is a normalizing factor such that a perfect rankings
ndcg at position n is    we report ndcg averaged for all queries for max position n 

 

experiments

we initially used logistic regression and naive bayes to rank instances based on their predicted class  the
challenge that we faced was presence of categorical features with large number of levels  using categorical features
without transformation produced poor results  we were able to obtain a performance gain by converting number of
categorical features to booking probabilities  we were able to further increase ndcg by selecting a fewer number
of features based on importance scores  additionally  we wanted to investigate how random selection of features
improves performance  therefore  we implemented random forest model  we also implemented ranking svm to
see if pairwise approach that it utilized fits our data better 

   

data

the data consists of records capturing expedia users search history  we used a subset of the data available at
kaggle com  brief overview of the data used to evaluate proposed methods is given in table   
num  of examples
num  of queries
num  of bookings
num  of continuous features
num  of features
num  of target attributes
percent of empty input values
average number of returned items per query

         
      
      
  
  
 
   
  

table    characteristics of the data set

   

features

since redundancy of the features can reduce effectiveness of ranking algorithms  we used methodology proposed
in     to investigate similarity between features  in     features are considered as undirected weighted graph where
each feature is node  each node is assigned an importance score and edge between any two nodes is assigned a
weight equal to similarity score  importance of a feature is defined as ndcg score if we rank based on feature
values  similarity is defined as a proportion of identically ranked items between two different features  importance
of the features is shown on fig    a   fig    b  reflects pairwise similarity  the level of lightness represents the
strength of similarity between two features  thus  there are two blocks of similar features  and much less number
of attributes with more diverse effect on ranking 
for this exercise we used a data sample of   k rows and all continuous features  as well as two additional
fields 
   delta between price of a property and average price of users purchases  derivedf eature  
   delta between rating of a property and average star rating of properties previously booked by user
 

fi a 

 b 

 c 

figure     a   importance scores  b   similarity scores  c   experimental importance scores
in our later experiments we defined an importance score for feature f   s as following 
importance f      n dcg s   n dcg s f   

   

where s is a fixed set of features  n dcg s  is ndcg value obtained by training on set of features s  fig    c 
shows importance scores of two sets of features used in different experiments described in the next section 

   
     

models
first experiments

naive bayes for naive bayes  we hand selected a few features  mostly property features  such as price  location
scores  etc   to get a basic machine learning algorithm running  we then learned the model on     of   k     k 
and    k examples and tested the resulting model on the remaining     examples and calculated ndcg values 
we then added new features based on their importance scores  we calculated target class probabilities for the
categorical features  and we normalized continuous variables based on target label values  for each attribute in
the test set  we converted its value to booking probability based on the the observed probabilities in the training
set  and finally we ranked instances using log of the sum of probabilities over all attributes  we obtained the best
ndcg of        
logistic regression our initial attempts at logistic regression didnt predict any positive values due to sparsity
of the data and abundance of negative examples  we took several corrective measures  subsampling  basic pruning
of features  normalization of continuous features  we also experimented with derived features  purchase rate for
each property  purchase rate for each property and destination  average values of various features for each property
etc   to improve the performance  we ranked features based on importance scores and trained using    most
relevant features  we see a potential future work focused on further exploration of effects of using probabilities as
features on performance 
     

second phase of experiments

random forest because linear models didnt provide us with satisfactory results  we decided to a try nonlinear classification model  we picked random forest because of its effectiveness and ease of implementation 
using the standard random forest implementation in r  we trained classification models on several subsets of
the data  for training  we used all continuous variables and all categorical variables with fewer than or equal to

 

fi   levels  a restriction imposed in the standard implementation of random forest   we used default settings of
    trees in the model with   variables tried at each split in each tree  furthermore  for computational speed  we
downsampled number of negative examples in the training set so as to maintain a ratio of     between negative
and positive examples  we believe that the benefit of increased computational speed outweighed any loss of model
performance from downsampling  using the trained model  we predicted booking probabilities for properties
within each search query in the test set and then ordered the properties from the highest to lowest value of booking
probability  we then used the predicted order to calculate ndcg for the model 
ranking svm ranking svm transforms ranking into pairwise svm classification  where instances from different groups  searches in our case  are incompatible  to train we used svm rank library based on      we used
a linear version of svm  with c       for training weve used all continuous features and two additional fields
described in section    we explored several ways of normalizing the features and only normalizing results belonging to one query slightly increased ndcg  since svm uses dot product of vectors we set missing values to   to
exclude their effect on minimization problem  ranking svm outperformed both logistic regression and naive
bayes models in terms of ndcg  our initial findings show that ranking svm performance on derived features
described in section       drops but is comparable to logistic regression performance 

   

comparison

table   lists top   features  in terms of importance score as defined in equation   in section      we can see that
random forest and svm have almost identical set of top features  while logistic regression list is diversified by
comp  rate and comp  rate  both of these features belong to the biggest block of similar features that we see in
fig    b  in section      thus  not enough diversity in the features might explain lower performance of logistic
regression compared to random forest and ranking svm  top features for naive bayes are not listed due to
complexity of calculation  but we know that after adding price usd and location score   features performance
increased the most  thus  its also consistent with what we see for other models 
importance rank
 
 
 
 
 

logistic regression
prop location score
price usd
prop star rating
comp  rate
comp  rate

random forest
prop location score 
price usd
derivedf eature  see     
prop location score
prop log historical price

ranking svm
prop location score 
price usd
prop review score
derivedf eature  see     
comp  inv

table    top   features across models
we also compared ndcg values as a function of input size  the highest ndcg value observed for naive bayes
was         therefore  naive bayes is outperformed by other models as we can see from fig     naive bayes still
performs better than random ordering which has n dcg        
also  we can see on fig    that ndcg for logistic regression is declining with increase of data size  we think
that such behavior can be explained by the way we normalize columns  in particular  with the increase of input
size derived fields that represent mean of different attributes based on target label eventually converge to the global
mean of the whole data set  therefore  our strategy to average based on target label becomes less effective 
ranking svm outperforms logistic regression but is stagnant for the last two data sets  we believe that the
optimal w to separate target labels wont be changing  because the data is even and it seems that for the last two
data sets ranking svm converged to global optimum 
 

detailed description of features is available at http   www kaggle com c expedia personalized sort data

 

fifigure    ndcg dependence on size of the input data set 

unlike other models ndcg for random forest keeps growing with the increase of input size  therefore  we
think that random forest is the most promising approach  additional advantage of random forest is that it can
more effectively account for non linear dependencies in the data than other models weve used  thus  we believe
that random forest will eventually outperform ranking svm  though the max ndcg value that we observed for
   mln examples belongs to ranking svm  n dcg           

 

conclusion

we applied most popular machine learning algorithms to solve a ranking problem with an intricate set of features
and reviewed the relative performance of each algorithm  based on our findings  we made a recommendation for
the best algorithm to solve the problem  we also proposed a list of top   most important features to demonstrate
what drives purchase behaviour on online travel booking websites  one possible direction for our future work is
to explore optimal parameters for the used algorithms  as well as perform further research on feature properties of
the data set 

references
    mariaflorina balcan  nikhil bansal  alina beygelzimer  don coppersmith  john langford  and gregory b 
sorkin  robust reductions from ranking to classification  machine learning                  
    xiubo geng  tie yan liu  tao qin  and hang li  feature selection for ranking       
    t  joachims  training linear svms in linear time       
    hang li  a short introduction to learning to rank  ieice transactions on information and systems  e  d     
     
    h  brendan mcmahan  gary holt  d  sculley  michael young  dietmar ebner  julian grady  lan nie 
todd phillips  eugene davydov  daniel golovin  sharat chikkerur  dan liu  martin wattenberg  arnar mar
hrafnkelsson  tom boulos  and jeremy kubica  ad click prediction  a view from the trenches       

 

fi