e commerce transaction anomaly classification
minyong lee

seunghee ham

qiyi jiang

statistics department
stanford university
minyong stanford edu

statistics department
stanford university
sham   stanford edu

statistics department
stanford university
qjiang stanford edu

i 

i ntroduction

b  preprocessing

due to the increasing popularity of e commerce in our daily
lives  credit card usages have dramatically increased over the
years  as credit card being the primary method of payment in
online transactions  credit card frauds have also been observed
to surge as the number of online transactions have increased 
because of the significant financial losses that credit card
fraudulent incidents can cause  credit card fraud detection drew
our interest into its investigation  specifically  we examined
anomaly detection classification algorithms that can be applied
to detect fraudulent credit card transactions  in addition  we
proposed a new type of sampling method called oversampling
via randomly imputed features  orif  and details of orif
is given in section iv  results of incorporating orif to each
basic classification algorithms are also provided in section v 
ii 

first of all  the data         observations  was split into
training data       and validation data        due to the
issue of highly imbalanced data we are facing  we took a
special treatment of sampling     of the anomalous  i e  
positive  responses into the training set and the rest     was
assigned into the validation set  as a result  our training data
consist of     of total anomalous responses and     of total
non anomalous responses  for validation set  it contains the
rest     of total anomalous and non anomalous responses 
although we are provided with the original validation dataset
with        observations  we cannot use it to test the performance of our learning algorithms because the dataset does not
have the outcome variable  therefore  we again divided the
training data obtained above into training and validation data
with the ratio of     and      for all the algorithms we
tried  we first ran and tested on the second layer of training
and validation data to find the best parameter s  of a specific
algorithm  and then applied it to the first layer of validation
data and reported the results 

data and p reprocessing

a  data
the data is from the      uc san diego data mining
contest and the goal of the data is to detect anomalous
online credit card transactions with   being anomalous and
  being normal  the dataset consists of    features with a few
continuous variables and some categorical variables such as a
state and a web address that correspond to each transaction 
the most striking characteristic of the data is that it is highly
imbalanced  about only     of the transactions are anomalous 
which represents the real world scenario  the problem is that
standard classifiers tend to bias in favor of the larger class
since  by doing so  it can reach high classification accuracy 

initially  we did data visualization to detect patterns of
each feature and correlations among them  in which we found
amount and total are highly correlated and so are hour 
and hour   therefore  we decided to drop total and hour  in
later analysis to avoid a multicollinearity problem  in addition 
we detected some skew ness in some of the features that
needed variable transformation to normalize them and applied
the box cox transformation to address the issue  meanwhile 
outliers have been found for f lag  and f ield   we adopted
capping approach for outlier treatment  which is to cap the outliers that exceed     quantile of its distribution and imputed
    quantile value for the outliers 
the features we had the most difficulty with in terms
of coming up with creative transformations were hour  
state  and domain   for hour   which indicates the time
of transaction  showed a wavy trend over    hours  so  we
proposed the idea of applying a sine and a cosine function
on hour  and created two features coshour and sinhour
to preserve continuity  we believe the linear combination of
these two will capture the wavy and repeating trend of hour  
for state   we categorized them into   groups according to
both the frequency of transactions occurred in each state and
geographical segmentations of the united states  we set the
top four states with the highest number of transactions as
four individual groups  a  fl  tx  and ny  followed by
the west  midwest  south  and northeast groups of states 
lastly  we grouped the military addresses ap and ae into
the  th category  for domain   we categorized them into  

fig     e commerce data 

 

ficategories based on both the frequency of a domain appearing
in the transactions and an industry that the domain belongs
to  shopping websites  news websites  and email sites are the
examples of three groups that we assigned domains into 
after data preprocessing  our final training dataset contained    features with    of them being categorical variables
and   continuous variables  the same preprocessing steps were
applied to the validation dataset 
iii 

oversampling via r andomly i mputed
f eatures

in figure    it presents our imbalanced data that has unequally proportioned positive and negative classes  in order to
deal with the issue that imbalanced data creates  we proposed
a new sampling method  called oversampling via randomly
imputed features  orif   what orif does is to generate
artificial instances for minority classes  in details  we imputed
the feature values for a new minority class observation by
simple random sampling from the existing feature values that
corresponding to minority classes  the intuition behind this
method was that by looking at our dataset from figure    we
observed that most of the minority data points are overlapped
with each other  which suggests the minority classes are
sharing similar feature values  a toy example is displayed in
figure    we arranged the top four observations as the   
positive classes in the whole     observations dataset  in order
to create half more of the existing positive classes  which
is   artificial positive classes in this example  we imputed
each feature values for new  and new  by randomly select
corresponding feature values from the positive classes 

fig     orif

can give us a good performance measure by balancing the
true positives
tradeoff between precision   predicted
positives and recall  
true positives
 
a
weighted
harmonic
mean could also be
actual positives
a good criterion if recall is regarded more important than
precision 
most of the basic classification rules minimize misclassification error rate  when there are not many positives  the
rules will classify all the observations as negatives to minimize misclassification error rate  therefore the parameters in
each classification rule must be carefully chosen to maximize
f  measure  here we use validation set approach to find
the best parameters  without touching the original validation
data  we divide our training data into training training data
and validation training data and train with a specific parameter
on the training training data and evaluate on the validationtraining data 

the main advantage of orif is that it does not impose any
structure to the data  compared to synthetic minority oversampling technique  smote  which adds artificial neighbors
to the minority class  chawla et al          orif does not
require the distance metric on the feature space which is hard
to define when it is a mixture of numerical and categorical
variables  also  it is very simple and fast to implement on any
dataset 

v 

we expect that orif will increase the performance of a
classification rule especially when the features are independently affecting the response  orif breaks the interaction of
features by randomly choosing the values from the whole
positive observations  so it might not work well if there are
interactions of features which are significant  if we have a
prior information about the interaction among features  we can
apply grouped orif  for the anomalies in our data  we believe
that the features themselves represent the characteristics of
anomalies more than the interactions  and therefore orif
could be a good oversampling method 
iv 

c lassification m ethods

our goal is to find a good algorithm to classify anomalous
transaction in a reliable manner  as a preliminary learning process  we tried various basic classification algorithms involving
different parameters and followed by employing orif to each
algorithm 

a  logistic regression
we first fit logistic regression  which can output probability
of an event appearing  since our dataset is highly imbalanced 
the probabilities of classifying in a positive class produced by
the learning algorithm are very small  thus  instead of looking
at the probabilities  it makes more sense to look at the quantiles
of probabilities  by experimenting with different quantiles on
training set  we found by assigning true positives among top
     to be the positive likely transactions achieved highest
f measure  the f measure and recall computed for validation
set are       and       respectively  after applying logistic
regression with orif  f measure has increased by      and
     for recall 

e valuation for classification and parameter
selection

in an imbalanced dataset  classifying all the observations
as negatives usually minimize the classification error  but
in anomaly detection  we want to penalize false negatives
more while not classifying all the observations as positives 
therefore  the f measure
   precision  recall
f  measure  
precisionrecall
 

fithe improvement was only marginal as the number of trees
increases  but achieves its highest f measure with     trees 
with the number of trees at      we applied different class
weights and found that the bigger the difference in the class
weight between misclassifying positive and negative classes 
the better results we get  therefore  with     trees and weights
at     for positive and    for negative classes on the original
dataset  f measure was       and recall was       

b  linear discriminant analysis
the f measure and recall results we obtained from lda are
      and        however  applying orif to lda does not
change the performance of the f measure and recall  although
it is a bit debatable  one study found that there is no reliable
empirical evidence that imbalanced dataset has a negative
effect on the performance of lda and that the improvement
in the performance of lda from re balancing is very marginal
 xue and titterington        

g  support vector machine
c  decision trees

before we implemented support vector machine with
different costs  sdc   we wanted to look at how svm
performs as a starting point  without any cost parameter  the
performance was very low and with an increasing number
of costs  the performance improved significantly although the
running time increased accordingly  since the original data
has almost        observations  cost higher than    took an
enormous amount of time  so  we decided to report the result
with cost     which is      and      for f measure and
recall  respectively  this performance was the lowest of all
and we do not think it is surprising given the fact that we had
a lot of categorical variables and we forced them into numeric
variables  which gave them orderings when there should not
be any 

since the competition did not provide definition of each
feature in the dataset  we practiced decision tree classification
method to get a sense of what are the most significant features
in predicting anomalies  we found out three features appeared
in the tree classification and they are transaction amount  time
of the transaction and type of transaction method  in general 
decision tree algorithms have moderate to high variances
 dietterich and kong         the f measure and recall we
obtained are a lot lower than the previous two methods with
      and       respectively  however  by using decision tree
with orif  both measures increased significantly to       for
f measure and       for recall 
d  k nearest neighbors

h  support vector machine with different costs

by experimenting with different parameters of k  we
learned that as k increases  both f measure and recall decrease 
so we chose k   as an optimal parameter  tested on the
original dataset and achieved       for f measure and      
for recall  among all other algorithms  knn turned out to
have the highest f measure  with orif  f measure went down
a little while recall went up slightly  but the change was not
significant 

support vector machine with different costs  sdc  penalizes misclassified positives more  the primal problem can be
written as
x
x
 
min    w      c  
i   c 
j
 
 i 
 j 
y

e  naive bayes

  

y

  

subject to wt  x i      b     i   wt  x j      b      j  
i     j     the cost parameters c     c  and a feature
mapping should be determined  in a highly imbalanced setting 
we put c    c  to penalizes positives more  figure  a
explains the effect of sdc  from now on  i represents an index
of positive labels and j represents an index of negative labels 
introduce the lagrangian

we applied naive bayes on the smaller training set by
starting without laplace smoothing then trying with different
smoothing values to achieve the best f measure result with
laplace smoothing of    then following by testing on the
validation dataset  it returned a f measure of      and recall of
       even though the recall is high in this setting  when we
look at precision of       it reveals a high false positives have
been made  thereby a low f measure shows a poor performance
by naive bayes classification  by applying orif to naive
bayes  we only see an improvement of      for f measure 

x
x
x
  t
w w   c 
i   c 
j 
i  wt  x i      b
 
i
j
i
x
x
x
     i   
j  wt  x j     b      j   
i i 
j j

g 

f  random forest and weight random forest

j

first  we tried random forest with different number of
trees from     to      with more than      the algorithm ran
out of memory  overall performance improved only slightly
with increasing running time as the number of trees increased 
the best result was achieved with     trees        and      
for f measure and recall  respectively  when orif was applied 
f measure and recall both improved by about    

i

x
x
g
 w
i  x i     
j  x j       
w
i
j
x
x
g
 
i  
j    
b
i
j
g
  c    i  i    
i
g
  c   j   j    
j

to weigh in the issue of imbalanced data  we tried
weighted random forest  where a larger weight is assigned
to the minority class  anomalies  when it was misclassified
 chen  liaw and breiman         similar to random forest 
 

j

ficlassification result
classification methods
f measure
      
linear discriminant analysis
      
linear discriminant analysis orif
      
logistic regression
      
logistic regression orif
      
decision tree
      
decision tree orif
      
k nearest neighbors
      
k nearest neighbors orif
      
naive bayes
      
naive bayes orif
support vector machine
      
      
random forest
      
random forest orif
weighted random forest
      
      
sdc linear
      
sdc orif linear
      
sdc gaussian
      
sdc orif gaussian

substituting and simplifying  we obtain the dual form of the
problem
 
 x
j   
i i  k x i    x i    
   
i
j
i i
x
x
 
 j 
 j  
j j   k x   x     
 
i j k x i    x j    

max l  

x

i  

x

j j  

i j

subject to
   i  c        j  c    

x

i  

i

x

j    

j

let the solution of this optimization problem        then
x
x
w  
i  x i    
j  x j   
i

table i  performance result

j

and b can be derived from the kkt conditions for an l
such that     l   c     l       thus l     and then
b      wt  x l       can also be used find b  
the classification rule is that we label a new observation as
positive  y      if and only if
x
x
wt  x    b  
i k x i    x  
j k x j    x    b     
i

j

notice that when solving the dual problem and classification 
we only need the kernel k xi   xj      xi  t  xj   instead
of  xi  s  for our data  we tried linear kernel k xi   xj    
xti xj and gaussian kernel k xi   xj     exp   xi  xj       

 a  sdc before orif

recall
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

vi 

r esults

we applied    classification methods with carefully chosen
parameters to maximize f measure  f measures varied from
  to       and corresponding recall values are in table i 
from figure    we observed that f measure of all the methods
are increased by orif except k nearest neighbors  figure  
shows the methods with largest f measures among all the
classification methods  and the recall of sdc orif with linear
kernel was relatively larger than others 

 b  sdc orif

fig     sdc and sdc orif 
fig     f measure comparison between basic classification
methods and orif applied classification methods

figure  a and figure  b illustrate the effect of orif with
sdc  the idea is inspired by smote  chawla et al         
but orif can be applied without a distance metric  nonzero errors on positive support vectors will have larger costs
while non zero errors on negative support vectors will have
smaller costs  the net effect of this is that the boundary can
be pushed more towards the negative instances  however  as
a consequence  svm becomes more sensitive to the positive
instances  therefore  if the positive instances are sparse  as in
imbalanced datasets  the boundary may not have the proper
shape in the input space  after using orif  the positive
instances are now more densely distributed and the learned
boundary is much well defined 
 

fi   

xue  j  h   and titterington  d  m          do unbalanced data have a
negative effect on lda   pattern recognition                    
     yang  h  and king  i   ensemble learning for imbalanced e commerce
transaction anomaly classification  in neural information processing
 pp            springer berlin heidelberg 

fig     f measure and recall of classification methods

vii 

c onclusion

we successfully applied a new oversampling technique on
a highly imbalanced dataset  our results suggest that orif
could be effective especially when features are unknown  with
orif  we were able to improve classification performance by
searching appropriate parameters in each classification method 
we found that k nearest neighbors  random forest and sdc
performs better than others  this means the preprocessing
process including scaling were reasonable for the distance
measure used in k nearest neighbors  and the importance of
variables in random forest can be used to analyze significant
features in transaction anomalies  in addition  noting that sdc
can be applied with various kernels which represent similarity
in observations  sdc orif can be improved once we know
more about the structure of the data  the results can also be
enhanced by ensemble learning 
r eferences
   

   

   
   

   

   

   

   

akbani  r   kwek  s   and japkowicz  n          applying support
vector machines to imbalanced datasets  in machine learning  ecml
      pp          springer berlin heidelberg 
chawla  n  v   bowyer  k  w   hall  l  o   and kegelmeyer  w  p 
        smote  synthetic minority over sampling technique  journal
of artificial intelligence and research            pp          
chen  c   liaw  a   and breiman  l          using random forest to
learn imbalanced data  university of california  berkeley 
dietterich  t  g  and kong  e  b          machine learning bias 
statistical bias  and statistical variance of decision tree algorithms 
machine learning            
imam  t   ting  k  m   and kamruzzaman  j          z svm  an svm
for improved classification of imbalanced data  in ai       advances in
artificial intelligence  pp            springer berlin heidelberg 
phung  s  l   bouzerdoum  a   and nguyen  g  h          learning
pattern classification tasks with imbalanced data sets  available from 
http   www intechopen com books pattern recognition learning patternclassification  tasks with imbalanced data sets
tang  y   zhang  y  q   chawla  n  v   and krasser  s         
svms modeling for highly imbalanced classification  systems  man  and
cybernetics  part b  cybernetics  ieee transactions on                  
weng  c  g   and poon  j         november   a new evaluation measure
for imbalanced datasets  in proceedings of the  th australasian data
mining conference volume     pp          australian computer society 
inc  

 

fi