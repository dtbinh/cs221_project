parsing domain whois information with different
patterns using one generic parser
jiale tan  xing li  and yiqing xing
 suids  jiale  xingli  and xingyq 

abstract
we design a generic parser to read the output file from more than     whois servers all over
world  there is no uniform and standard format for these files  and formats also change over time 
this leads to huge maintenance cost for firms who regularly harvest these files and read in data
using regular expression  in this project  we design a generic parser that is robust to changes in
format in different sources and over time  with a training sample of about      files  our parser
results in smaller testing errors as we are making use of more sample files 

introduction
there are more than     whois servers  worldwide who are providing the whois information of
the domains   lots of firms in different industries make use of whois information in their
business  for example  apple is interested in finding out who has registered ipad com  nike
would like to know who owns the online marketplace which sells counterfeits of their products 
and reuters can take advantage of whois information to support their news  etc  for these firms 
one of the routine jobs is to regularly submit queries to whois servers that will send back a string 
and then read the string and parse out useful information 
informative strings from whois servers have different formats both within and across servers 
and the format also changes over time  the variation in formats among servers causes a huge
maintenance cost for user firms to use standard regular expression to read in information 
because they need to detect and test different patterns for more than     whois servers and in
addition make modifications when format changes  which is also unpredictable  more
specifically  an average whois server has     patterns  and the pattern changes once every   
weeks  which will induce to about        regular expression patterns in a year 

 

for example  whois godaddy com 
whois information include registrar  registrant tech admins name organization email phone fax address  name
server  status
 

fiin this project  we try to design a generic parser to read whois strings  although our parser is not
as precise as the regular expression  it is robust enough to cater for different formats among
different servers over time 
 

 

   and

 

to express the idea in an abstract level  let
 

 

string of words

 

 

  be the training sample  where
 

 

is a

is the information we want to parsed

 

out  the parser will assign a probability
  to each of words in string and pick out the
word with the highest probability to be the information  to decrease dimensionality  we restrict
our probability function to be a function of words before  i e  
 

 

 

 

 

 

  

whois strings
the objective we are dealing with is the whois strings from more than     servers all over world 
they have different patterns within one server as well as between servers  and they changes
constantly over time  for example  the following are two strings with totally different formats 
record   

domain name  dandylion recycle com
registrar  purenic japan inc 
whois server  whois purenic com
name server  dns   jps sunfirstasia net
name server  dns  sunfirstasia net
registrant name 
hidetoshi fujita
registrant organization 
dandy lion

record   

domain isabellabrendolin com
dns   ns  serverlet com
dns   ns  serverlet com
registrant
isabella brendolin
email domain serverlet com
via corsica  
      piazzola sul brenta
italy
tel                email  elalex    yahoo com hk


in our algorithm  we assign probability to each word predicting whether this is the field we are
interested  for example  in record    we assign probability to each of the words domain 

finame  dandylion recycle com  registrar     and pick up the one with the highest
probability  if the model predicts correct  the probability assigned to dandylion recycle com
should be highest 
for simplicity  we restrict our probability to be dependent only on words before  suppose
  in our example of record    p dandylion recycle com    f domain  name   and
p registrar    f name  dandylion recycle com 

learning models
converging to email spam problem
our problem is isomorphic to the email spam problem after pre processing to the format of email
spam  specifically  for a file tokenized in a string of words  we stack that into multiple
observations where each observation consists of m words as feature and whether the word
immediately after is the content to be detected as categories  see figure   for an example 

 

 
 

file  t 
b

a
 

 

 

 

 

 




 

 

x
x
domain
name


 
 
 

 

d



 





is domain name

 

 
 

c

 

x
domain
name
 


domain
name
 
dynd


 
 

 
 

 
 

 

 

 
   

problem candidate models
a  nave bayes multi nominal model 
a multi nominal naive bayes model can be imposed to pin down the probability function
described above  say 
 

 

 

 

  

b  order specific multi nominal model

 

 

 



 

 

fione of the questions of the model above is the order invariance of the prediction
probability  which will make the prediction of domain name being the same as name
domain  one modification is to allow probability to be order specific  say
 

 

 

 



 

 

 

 



 

 

c  svm
at the end of day  we will go back to svm to pursue the best prediction 

other implementation details
 

we drop all the starting irrelevant words and start our string from domain 
we pick the dictionary of the words by taking the union of frequent words      of all
words  appeared within m words before the field of interest 

testing results
we plot the learning curve using   fold cross validation within which we fix the testing sample
and allow the training sample to grow  we report the learning curve of three fields of interest 
domain name  administration email and registrant name  the learning curve is decreasing pretty
fast  and as sample size goes  we believe our parser will work pretty well after enough training 

finext steps
there is a lot of room to reduce the error and improve performance 
   preprocessing 
a  we can pick features through filtering out irrelevant words
   modeling 
a  we can pick the maximum likely set of proceeding keywords for corresponding fields
b  we can use the n tokens instead of single tokens as keys in our dictionary so that we
can take advantage of the order of the tokens in the feature windows
c  we can optimize the number of words we want to look in a row before or even use
flexible token size based the proceeding keyword
   ml algorithm 
a  we can try svm which might reduce the error rate 

fi