musical instrument modeling and classification  cs      c  copeland  s  mehrotra

musical instrument modeling and
classification
christopher n  copeland  sameep mehrotra
cs      machine learning  stanford university
chrisnc stanford edu  sameep   stanford edu
abstract
independent components analysis is commonly used to separate different sources of sounds in a set of
recordings  ica produces the original unmixed input signals as output  but not in a known order  it is up
to the listener to identify or interpret the sounds separately  this task is easy for a human who is familiar
with the nature of the sounds being mixed  e g   human voices   but may be more difficult if the sounds are
not already known to the speaker  we propose and implement a technique to label the unmixed sounds
produced by ica  specifically  we were able to use a set of support vector machines trained to identify
each of four different musical instruments  to find a feature mapping the svms can use to label arbitrary
sounds beyond those similar to its training set  we used  yet another audio feature extractor  or yaafe 
an open source audio feature toolkit      we tested our instrument svms on randomly generated sounds
from known instruments and found that  using the appropriate set of features from yaafe  we obtained a
    prediction accuracy  we also tested the instrument svms on the outputs of ica  which have some
lingering components of other instruments mixed in due to an imperfect unmixing matrix  and found a
prediction accuracy of     

i 

overview

independent component analysis is an excellent tool for analyzing and separating multiple
sources of sounds using recordings from different locations in a room  often  the resulting
sounds from this separation are readily useful
just by listening to them  speech  for example  is easy to discern after ica if the unmixing matrix is sufficiently accurate  for some
tasks  though  further processing after ica can
give valuable information or otherwise save humans the trouble of finding information from
the sound samples by directly listening to them 
one such example is the task of identifying the
instrument that produced each of the unmixed
tracks found from ica  ica normally does not
attempt to produce any particular ordering of
the unmixed sound outputs  so ica alone is
not sufficient for this task 
to solve this problem  we used multiple
support vector machines  with each trained to
classify one of the instruments we used in our
data sets  a prediction consists of determining
which svm determines that a sound sample

belongs in its class  and returning an error if
none  or more than one  of the svms give a
positive result  we used the liblinear to
create and train these svms     
in our task of classifying instruments given
only short sound samples  we needed to choose
features that would provide enough information to uniquely identify each instrument  without containing too much  or any  if possible 
information about the notes being played  as
these will generally be different between any
two sound samples  in particular  if features
pertaining to the notes being played are captured by the feature mapping  then an instrument playing some set of notes is more likely
to be misclassified as a different instrument  if
that instrument happened to play similar notes
in the training data set  in order to solve this
problem  we looked at many different features
available in yaafe  and tested our svms using
different subsets of these features  we found
that classifying randomly generated sounds
using the appropriate subset of features gives
very good performance  while using features
that depend heavily on the notes played in

 

fimusical instrument modeling and classification  cs      c  copeland  s  mehrotra

a particular sample leads to very poor performance  even used in conjunction with the
highly predictive features 

ii 

sound generation

to produce our sound data  we generated
multiple waveforms for each of four instruments  clarinet  mandolin  saxophone  and
sitar   these waveforms consist of randomly
generated  music  in the form of chords  single
notes  and rests of varying lengths interspersed
together  the notes were randomly selected
from the one octave of    semitone notes between c   middle c  and b   inclusive  we
limited the chords to just two or three simultaneous notes  while varying the length of the
notes between roughly    ms and one second 
depending on the instrument  each waveform
is roughly eight seconds of sound data 
this data was generated in the chuck programming language  which is a strongly typed 
strongly timed concurrent audio and multimedia programming language created by ge
wang of the stanford center for computer research in music and acoustics  ccrma      
the instrument models in chuck are based off
of those found in perry cook and gary scavones synthesis took kit  stk   which is a set
of open source audio signal processing and algorithmic synthesis tools written in c    the
stk instrument classes utilize the method of
digital waveguide synthesis  which is a form of
physical modeling synthesis in which a mathematical model of the instrument is approximated based on physical features of that instrument  digital waveguides are efficient computational models for physical media through
which acoustic waves propagate  for example 
the stk mandolin uses two  twang  models
and commuted synthesis techniques in order
to model the mandolin instrument 

iii 

feature selection

our intuition about the nature of sound initially suggested that a frequency representation
is most helpful for identifying the properties of
 

a musical instrument  the strongest features
in a given sound sample will be the frequencies of the note being played  the fundamental frequency   and the other strong features
will describe the harmonics associated with
the instrument playing the note  for classifying arbitrary sequences of sounds  however 
this is not sufficient  as the frequency profile of
a sound sample depends heavily on the fundamental frequencies being played  and not just
the instrument playing them  which primarily
influences the harmonics 
a useful property of this feature mapping
is that it is time invariant  specifically  because
we are taking the magnitude of the fourier
transform  starting a sound sample with some
delay t simply multiplies the fourier domain
representation by  exp  it   which has a
magnitude of    and therefore the representation will be the same  this means that  ideally 
we should be able to detect a particular profile regardless of when the note begins in any
given sample  despite other drawbacks of using a frequency representation  any other ideal
feature mapping should also have this property  shifting sound samples in time should
not affect the instrument classification of that
sound sample 
after experimenting with pure frequency
representation as a feature mapping and finding the results to be poor  we turned to more
sophisticated signal processing techniques provided by yaafe  yet another audio feature
extractor   an efficient and easy to use audio
features extraction toolbox  yaafe was developed at telecom paristech by the aao  audio  acoustique et ondes  group  yaafe conveniently includes python and matlab bindings 
and offers a wide variety of core audio features  we found some of the most useful and
successful of these features to be the autocorrelation coefficients  the spectral shape statistics  including centroid  spread  skewness  and
kurtosis       and the octave band signal intensities  obsi       which is a rough estimator of
harmonic power distribution 

fimusical instrument modeling and classification  cs      c  copeland  s  mehrotra

iv 

instrument separation

as described in our overview  we tested our
classification technique on the outputs of independent component analysis  which contain a
single primary instrument and one or more instruments with reduced amplitude in the background  these background instruments are the

result of imperfect separation in ica  which
is not gauranteed to find the perfect unmixing
matrix  as expected  this will degrade the performance of the classification somewhat  refer
to figure   for an example of sound samples
that have gone through the mixing and unmixing process 

original instrument signals
   
instrument  
instrument  
instrument  
instrument  

   

magnitude

   
 
   
   
   
   

 

  

   

   
sample

   

   

   

mixed signals ready for ica
   
mix  
mix  
mix  
mix  

   

magnitude

   
   
 
   
   
   

 

  

   

   
sample

   

   

   

separated instrument signals after ica
   
instrument  
instrument  
instrument  
instrument  

   

magnitude

   
 
   
   
   
   

 

  

   

   
sample

   

   

   

figure    original sound samples  mixed recordings  and unmixed results of ica 

v 

results

we measured our systems performance by
counting the number of correctly identified
instruments from randomly generated sound
samples  in these eight second samples  the
simulated instrument plays random sets of
notes for random durations  in combinations
that are unlikely to appear in training data 
overall we found that some features performed very weakly and could even detract
from the performance of more predictive fea 

tures  suggesting some level of overfitting  we
achieved an excellent prediction accuracy of
    when testing our svms on randomly generated sounds from known instruments  see
figure     the task of identifying instruments
from samples obtained via ica is a more difficult one  but we were still able to reach    
accuracy when using autocorrelation coefficients  autocorrelation peaks integrator  and
the obsi  refer to figures   and   for accuracy
results using on original sound samples and
ica unmixed sound samples  respectively 
 

fimusical instrument modeling and classification  cs      c  copeland  s  mehrotra

figure    instrument prediction accuracy using different sets of yaafe features  key  acpeaks   autocorrelationpeaksintegrator  ac   autocorrelation  sstat   spectralshapestatistics

figure    instrument prediction accuracy using different sets of yaafe features  using data that has gone through
mixing and unmixing  key  acpeaks   autocorrelationpeaksintegrator  ac   autocorrelation  sstat  
spectralshapestatistics  obsi   octavebandsignalintensity

 

fimusical instrument modeling and classification  cs      c  copeland  s  mehrotra

vi 

future work

there are several opportunities to extend and
improve our project  given more time  we
would have liked to work toward more ambitious goals such as 
 exploring other features to include in
training the svm  and doing a more indepth analysis of their relative effectiveness

 replicating a simple version of instrument synthesis using parameters extracted from a generative instrument
model
 comparing machine classification of instruments to human classification  especially in the absence of an attack window 
which is well known to make this task
difficult for humans

references
    b mathieu  s essid  t fillon  j prado  g richard  yaafe  an easy to use and efficient audio
feature extraction software proceedings of the   th ismir conference  utrecht  netherlands 
     
    ge wang  the chuck audio programming language  a strongly timed and on the fly
environ mentality  phd thesis  princeton university       
    o gillet  g richard  automatic transcription of drum loops  in ieee international conference
on acoustics  speech and signal processing  icassp   montreal  canada       
    r fan  k chang  c hsieh  x wang  c lin  liblinear  a library for large linear classification  journal of machine learning research       

    s essid  g  richard  b  david  musical instrument recognition by pairwise classinacation
strategies  ieee transactions on audio  speech  and language processing 
vol           

 

fi