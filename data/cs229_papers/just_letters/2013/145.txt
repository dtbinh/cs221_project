building fast performance models for x   code sequences

justin womersley
stanford university      serra mall  stanford  ca       usa
christopher john cullen
stanford university      serra mall  stanford  ca       usa

   introduction
stoke is an aggressive low level compiler which generates complex assembly level optimizations using random search  part of this process includes estimating the performance properties of short    bit x   instruction sequences  in this project we aim to train
a machine learning model to more accurately estimate the performance of these instruction sequences 
the predictor has to be as accurate as possible  given
that cisc architecture is extremely complex  while
also being able to sustain a throughput of  m predictions second to remain useful within stoke optimization process 

jwomers stanford edu
cjc    stanford edu

has been executed           times in a loop to provide
an accurate estimate of average execution time  this
includes padding each sequence with    nop instructions in an attempt to minimize more complex pipeline
and caching effects  this data is then stored as an xml
database with code sequence and execution time  using the starter code from the x  asm tools  we created
a parser to read the xml entries and generate feature
vectors for the entire data set  this process is shown
in figure   

   background  scope   requirements
current performance estimations for stoke are simply a sum of average run times for each instruction 
this is fast  but not very accurate  on the other side
of the spectrum are cycle accurate simulators  which
give accurate preformance estimations but at the cost
of very long run times  thus our goal is to achieve
an estimation model that is both more accurate
than the current sum of averages predictions while
also fast enough to be useable in the stoke compiler 
to maintain feasibility  the scope of this project has
been limited to short code sequences of    instructions
or less  in addition  certain rare and more complex x  
instructions  like those including memory access  are
not required in the model  and thus not included in
the training or testing data sets 

   data   features
our data is in the form of short x      loop free code
sequences along with their known execution times  for
each sequence length of   to    instructions  we have
        randomly generated sequences  each of which

figure    the process of extracting the opcode data into
a sparse opcode id vector  each sequence gets translated
into      length vector  one entry for each opcode 

we then took a sample of     of the generated feature vectors          examples   and split that set into
    training          examples  and     test        
examples   both training and testing sets were then
radomized to more realistically represent real world
data  each of our chosen models were then trained
and evaluated in matlab 
top performing models were evaluated more thoroughly by partitioning the test set by sequence length
and testing the models on each partition  the best
performing model was implemented in c   to measure the prediction runtime in order to validate our
throughput requirement  for a complete overview of
our data preperation  modeling and testing process see
figure   

fibuilding fast performance models for x   code sequences

data  

opcode

registers

runtime

       
       
       

sqrtpd
sqrtpd
sqrtpd

 xmm     xmm  
 xmm     xmm  
 xmm     xmm  

       
         
         

table    three different run times for the same opcode and
register sets indicate the huge variance in our data due to
unmodeled effects  note these runtimes were achieved with
only the   instruction pre padded with    nop instructions 

bution of execution times in sequences containing only
one instruction        of sequences are            and
      of sequences are             but one sequence
had a runtime of                 thus the variability
of even the simplest of our data creates a difficult modeling problem and any successful model would have to
accurately predict these outliers 

figure    process for generating  preparing  modeling and
testing on our data

   data issues
     data variability
prior to defining features  we performed data analysis
on our generated data  examining means and variances
over the different sets  execution times were mostly
linear with the number of instructions in the sequence 
but extreme outliers raised the mean of the dataset
to be non representative of the true tendency of the
data  as seen in figure    the vast majority of execution times were in the sub           range  sloping
gradually above           for code sequences of length
      an optimal predictor would need to be able
to accurately model both the lower execution times of
the bulk of the data and the higher  by an order of
magnitude  execution times of the minority outliers 
figure   shows  with logarithmic y axis  the distri 

figure    the distribution of runtimes for varying sequence
lengths  the vast majority of runtimes were           
with a clear linear trend for these non outliers  see figure  
for a more accurate percentage breakdown or runtimes for
sequences of   instruction 

     unmodeled effects
along with high variability seen within a minority of
code sequences of the same length  there were several
cases where high variability was seen within different
executions of the same sequence  table   shows three
separate examples of the same code sequence  with
drastically different execution times  whether due to
latent processor state or other factors that our data
do not capture  this discrepancy appears only as noise
in our data  because of this  no predictor can  deter 

fibuilding fast performance models for x   code sequences

was simply not feasible 
modified linear regression
using a stochastic gradient descent on our training set
achieved good results with a small training error  however the feature weights of  often ended up negative
for opcodes that did not appear often in the training set  while this is mathematically valid   there
is no logical mapping for this to instruction runtime 
since we know all instruction runtimes must be nonnegative  we amended the stochastic gradient descent
update step to reflect this prior knowledge to the following 

 i 

j    max    j    y  i   h  x i    xj  
figure    execution time for single instruction sequences note that almost     of the runtimes fall below         
the remaining    of outliers take on values of as much as
           

ministically  perform perfectly on our data set  an
optimal predictor can only minimize error over ostensibly equivalent data 

which we will refer to as the  floor update  in addition  the outliers described in figure   often resulted in
large oscillations as theta was updated to reflect these
outliers  as a result we also reduced our learning rate
 to better cope with this  this change  in addition
to including our prior knowledge that j s cannot be
negative  reduced the oscillating effect we encountered
when predictions are far from y  i  and resulted in no
negative feature weights for  

   prediction models

outlier exclusion

feature creation

since the the small minority of extreme outliers  including known unmodeled effects described in table   
may impede the the regression to model the more linear sequences  another approach we took was to train
the regression model with a dataset that excluded any
extreme outliers  the aim was to better model the
vast majority of the data at the expense of badly predicting the       of outliers and those with unmodeled effects  for each of the      training sequences
of         we removed     of training examples that
were lying furthest from the median and then implemented the stochastic gradient descent using our floor update modification 

calculating runtime is at its most basic level a sumof parts problem with the opcodes as the main contributing factor  thus our first step was to map our sequences to an opcode frequency feature matrix  since
we had between   and    opcodes per sequence with
     opcodes  this produced a very sparse feature matrix 
linear regression
due to high throughput requirement  we could not use
any form of non parametric algorithm as prediction
throughput with such algorithms would not be high
enough  fortunately though  this strict speed requirement did not extend to training our model  allowing
us a larger breadth of training approaches and implementations in matlab 
pn
a linear regression with h x      i xi   t x using
the normal equations quickly ran into two issues  running out of memory for even moderate data sets  and
getting non invertible matrices dues to the extreme
sparsity of our data set  since our design matrix x
is a very sparse matrix  using the normal equations

additional features
lastly  to better predict the outliers we separated them
out and analyzed which  if any  opcodes and register
combinations resulted in these large runtimes  once
we had a subset of    opcodes and    indicative registers  we added these additional     features  cross
product for register pairs of each of the    registers for
any of the    opcodes  to our sparse opcode feature
matrix in the hope of modelling these more complex
runtime results 

fibuilding fast performance models for x   code sequences

   evaluation metrics
the baseline benchmark against which we evaluated
our predictions was the runtime predictor that is currently being used in stoke  this is a simple table
lookup using agner fog instruction tables     for the
haswell architecture  to evaluate our models and predictions against this baseline benchmark we used two
different error metrics 
pm

i  

average error   a  

  y  i   h x i     
m

where m is the number of testing examples  y is the
actual runtime and h x  is the predicted runtime using
our feature vector x 
pm
threshold error   t  

i  

figure    average error for different models  the best performing model  lowest error  was the linear regression with
the outliers removed during training 

    y  i   h x i       t  
m

where t is an acceptable error margin  in our case
    or      since our extreme outliers could easily
skew the average error even in a reasonable good predictor  the threshold error would in this case represent
a more useful error metric as it reports the percentage
of predictions that were wrong 
to evaluate our timing requirement we developed our
final predictor in c   and timed it making        
predictions  ensuring that it met our throughput requirement 

   results
     accuracy   error rates

figure    threshold error for different models  the best
performing model  lowest error  was the linear regression
with the outliers removed during training 

our best performing model was the modified linear regression using  floor trained on our dataset that excluded the outliers   it was able to improve the average
error by     from the baseline benchmark  and improved on the baselines threshold error by      while
also maintaining the necessary throughput  figure  
shows the overall errors for our three top performing
models  along with the baseline  figure   and figure  
show the results for each of our error metrics  partitioned by sequence length 

baseline in both metrics for all sequence lengths  any
attemps we made to model the outliers using these
register features only worsened the predictive power
of our model  linear regression on opcode frequency
with outlier removal performed best overall  and outperformed the other models in almost every metric 

linear regression and  floor linear regression on opcode frequency were both outperformed by the baseline overall  but had better error rates than the baseline as sequence length increased  this suggests that
they are useful for predictive qualities on larger code
sequences  linear regression with advanced features
such as register interaction modeling are not included
in these results because they performed worse than the

we implemented our final model as an iterative tablelookup sum computation in c    achieving an average throughput of     m predictions per second on
code sequences of length     this throughtput meets
our throughout requirement  in addition  converting our final theta from floating point values to fixed
point values would increase the throughput even further while having very little effect on prediction accu 

     timing   throughput

fibuilding fast performance models for x   code sequences
tion latencies 
throughputs and micro operation
breakdowns for intel 
amd and via cpus 
http www agner orgoptimizeinstruction tables pdf
          

figure    overall errors for our different models  both the
standard linear regression and modified  floor regression
were not able to improve on the baseline benchmark performance  while the modified regression trained on data
excluded the outliers outperformed all the other models
including the baseline benchmark 

racy 

   conclusion
execution time prediction is a notoriously difficult
problem for any modern processor  but is especially
difficult for processors using the expansive x      instruction set  we achieved our initial goal of improving upon the baseline while maintaining necessary throughput  lowering the average error rate by
    and the threshold error rate by      however 
it was frustrating that all attempts at feature generation for modeling the behavior of outliers performed
so poorly  using features to represent specific opcodes
over certain register pairs  we were able to model some
common causes of large execution time  but the inability of those features to predict consistently had a net
negative effect on the models overall performance  in
addition  the unmodeled effects essentially impose an
upper bound on any predictor without deeper information about processor state during execution  despite
the difficulties modeling the outliers in our data  the
improvement in error rates achieves our goal of developing a better performance model for loop free x  
code sequences while maintaining the required prediction throughput 
references
    eric schkufza  rahul sharma   alex aiken   stochastic
superoptimization  stanford university
    agner fog 
technical university of denmark 
       instruction tables lists of instruc 

fi