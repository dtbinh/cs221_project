learning predictive filters
lane mcintosh
neurosciences graduate program  stanford university
stanford  ca       
 dated  december          
we examine how a system intent on only keeping information maximally predictive of the future
would filter data with temporal correlations  we find analytical solutions for the optimal predictive
filters when data is gaussian distributed and numerically find the optimal filters for non gaussian
data using the broyden fletcher goldfarb shanno algorithm  we then test the hypothesis that
biological organisms process visual information to preserve only predictive information by comparing
these optimal filters with measured filters from tiger salamander retinas  these results suggest that
neural systems optimally capture information about the future  as well as give insights about how
we should pre process image data for deep learning networks 
keywords  information bottleneck  predictive information  retinal coding  image pre processing

introduction

neural systems and machine learning algorithms face
a common challenge  to learn the simplest possible models from past data that generalize well to future  unseen
data 
this problem is particularly acute in the early visual
processing of the retina  where bits about the visual world
are encoded via action potentials that cost        atp
apiece      while the strict energy budget of the retina
has led to theories of efficient coding  where information
between the visual world and the retinas output is maximized by stripping away redundancy in the visual signal  relatively little attention has been paid to the need
for retinal output to be informative about the future 
from the moment light hits the eye  over a hundred milliseconds elapse before our first perception of the signal meaning that we are constantly living in the past  since
organisms must react to the future given data collected
in the past  one conceivable function of the retina would
be to only transmit information that will be useful for
the near future 
such predictive processing is by no means beyond the
purview of the retina  the last several decades of research on the retina have demonstrated that the eye acts
as far more than a simple camera capturing light intensity
from the external world      in just a few layers of cells 
the retina detects object motion      dynamically predicts
arbitrarily complex temporal and spatial visual patterns
     reduces the temporal and spatial redundancy inherent in natural scenes         and transmits this compressed
representation efficiently        
are these retinal properties the result of individual
bug detectors crafted over the course of evolution  analogous to hand selected feature creation  or do they fall
out of general optimization principles  the theory of
efficient coding suggests that maximizing mutual information between stimuli and the neural response provides
such an optimization principle            by examining
correlations between the output cells of the retina  recent
results have suggested that instead of trying to encode
all information present in the stimulus  neural systems

instead only seek to efficiently encode information that
is predictive about the future stimulus      
one elegant idea that emerges from this work is that
the retinas selectivity for specific features like object motion could arise not for its own sake  but rather because
encoding object motion provides an efficient way of representing the future in a world with inertia       in other
words  maximizing predictive information in a neural network may be sufficient for capturing important information from the environment  rather than resorting to handcrafted feature detectors  this is important not only for
a low dimensional understanding of how the brain processes sensory information  but also for establishing the
computations artificial systems need to implement in order to perform intelligent tasks like computer vision 
here we derive the filters that maximally predict
the systems future input in the context of gaussian
distributed stimuli with temporal correlations  we then
compare these optimal filters to measured filters in tiger
salamander retinas  and conclude that the retina is
indeed optimally capturing information from the past
that is predictive of the future 

predictive information bottleneck
problem statement

since the information processing inequality states that
we will maximize the amount of predictive information
by just keeping all information about the past  we avoid
this trivial solution and incorporate a cost to encoding
more bits by penalizing information about the past  a
model agnostic objective function that selectively keeps
information about the future while penalizing information about the past is the information bottleneck problem

 


 

min i  x   y    i  y   x   
   




where x and x are the past and future inputs to our


system  y is the past output of the system  and  pa 

fi 
rameterizes how much information we are willing to keep
about the past 

analytical derivation



since y is simply a linear combination of gaussians 
it must be gaussian distributed as well  using this observation  the definition of mutual information in terms
of differential entropy  the formula for the entropy of a
gaussian random variable  h x       log  e d  x    and
dropping irrelevant constants  the solution to our predictive information bottleneck problem is

 


 

t       arg min i  x   y    i  y   x  
h  









  arg min      h  y    h  y   x     h  y   x  
h  
 h  t       log    
  arg min       log  h  
x
h  


 h  t      
   log  h  
x  x

fig     an example information curve given a stimulus with
exponentially decaying temporal correlations  the dotted line
represents the maximal number of predictive bits that can

 

be extracted given i  x   y   stored bits about the past  the
solution to the information bottleneck problem is one point
along this curve that depends on the particular value of  

since the schur complement gives us
 

   
  

   



y  x
y
y x 
xy
x

 
 h  t     h  
   
 h  t
  h  
x
x 
x
x

  
and

system definition

 

   
  

  



y  x
y
y x 
xy
x
t

 
 h       h  

  

 h  t
  h  
x
xx 
xx
x

we assume that our stimuli x  n       where  has
arbitrary correlations  and that our system linearly combines past stimuli values to form its output  in particular 
we have






xt
yt
xt  
 


 

 
 xt   


 yt  
x         x           and y        
    
    
xt k
xtk  
ytk  
where
xt     axt   
yt     c xt     d yt   
and   n           n         since xt evolves linearly
with the productpof a  we can express e xt       at x 
t
i
ti
and e yt      
x    if we consider the
i   d c a
input fixed  our system responds linearly to its input via
c and d   our predictive information bottleneck will
optimize over these filters to maximally extract predictive


bits from x  
 
noting then that the expected value of x    at  xt  




we can write y   h   x where
 pt 


di c ati

 
  t   
  
tk  
h     
 
  a          a  
 
k ptk
i
ti
i   d c a
i  


 h  t     
  h  
x  x

by lemma a  from      we can set    i without loss
of generality  and so our solution becomes
 h  t   i 
t       arg min       log  h  
x
h  


 h  t   i  
   log  h  
x  x

note that our optimization depends on the noncausal co
   which the system must learn over
variance matrix 
x  x
time  taking the derivative of the above minimization
and setting it to zero we obtain the following equality 
 

 h  t   i  h  
 h  t   i   h  
 h  
x  x
x

 

   
  h  
x  x 
x

 

     taking the singular value deby diagonalizing 
x  x 
x
composition of h    and performing a few substitutions 
we obtain the optimal solution          


  v t
    
   


k v t 
k
h       
   


   
    

 

fi 
where
s
i  

    i     
 
i  vit x vi  

and i   vi are the eigenvalues and eigenvectors  respec 

     intuitively  we
tively   in ascending order  of 
x  x 
x

 

   as killing off the varican think of this matrix 
x  x 
x


ance in x and then slowly adding back in the bits about
the past with the least variance given the future  as 
increases  our filter h     loses its   rows and keeps more
 

   
of the higher variance components of 
x  x 
x

simulations of analytical results

 

   for our example probfig     a visualization of 
x  x 
x
lem with progressively noisier dimensions running from left
to right and top to bottom  since xti  xt j is large when
xti s variance is unexplained by xt j   the dimensions in the


top left corner represent the subspace of x correlated most
with the future 

to visualize these analytical results we drew a   q
from the qr decomposition of random matrices  which
effectively renders a a generalized rotation matrix and


gives x sinusoidal structure  figure     we let each xt
be    dimensional  and added progressively more noise
to each dimension such that dimension   was noiseless
and dimension    was nearly independent 

fig     on the left are the entries of h   corresponding
to how the system optimally combines past time points to
preserve predictive information for an intermediate value of
  the right time series is the optimal response given  
where left time points are less delayed  the drop to baseline

 

indicates that responding longer does not increase i  y   x  

 

enough to offset i  x   y   

learning predictive filters for
naturalistic images


fig       of x s    dimensions  from top to bottom the additive noise increases and the maximum predictive information
from that time series decreases 

 

    figover hundreds of iterations we estimated 
x  x 
x
ure     performed the eigenvalue decomposition of this




matrix  and used it to compute h   and y   h   x
 figure    for an intermediate value of  



since x is mk    dimensional  where m is the
dimension of a single time point and k is the number
of time points  intuitively it should skip over the m
dimensions that are very noisy and have little predictive
information to offer  indeed  we can see this periodic
nature in the rows of h    figure    

curious about what the predictive filter would look like
for natural image processing  we generated a naturalistic
  d stimulus from a database of natural images  after
picking a pixel location randomly  we would traverse the
image via a random walk for five seconds before switching
to a new image  figure     although this stimulus is only
locally gaussian  the optimal predictive filter adapted to
the timescale of the saccades   no matter how high 


was tuned  y never allocated bits beyond   seconds in
the past  figure    

predictive filters in the retina

the retina is one system where it would be highly
advantageous to selectively extract predictable infor 

fi 
convolutional filters  we modify our system slightly to be





yt
x
t  
 


 

 
   

 
 yt  
x  
   x          and y         


   
xt k
xtkn  
ytk  
xt
xt 
  
 





where
yt  

n 
x

wi  xti   

i  



  w x t tn     

fig     our stimulus of natural images  we switch images
every   seconds  mimicking saccades  and perform a random
walk over the image mimicking fixational eye movements 

and w    w     w             wn       we specifically


let each wi  be scalar  such that we can write y  


t    x where

w
   w

t      

 



 
 
  
 



 

w





is toeplitz and y is simply the convolution w  x  



fig     the optimal filter h   and system output y for the
natural image stimulus 

although weve already reduced the dimensionality
from k   qm to n where n    k  we further reduce the
dimensionality of finding w by parameterizing the
filter h   as the product of sinusoids  with parameters
frequency and phase  and a gaussian filter  with variable
width  

stimulus

mation while minimizing the total number of bits
transmitted  the retina comprises three primary layers
of cells   photoreceptors  bipolar cells  and retinal
ganglion cells  since photoreceptors and bipolar cells
roughly serve to linearly filter the raw rhodopsin
activity while retinal ganglion cells have a threshold
that generates the first action potentials in the visual
pathway  researchers typically model the retina with
linear nonlinear models  one particularly well known
fact of these models is that the filters measured via
reverse correlation are strongly biphasic  figure            
for several decades the dominant explanation of
these biphasic filters is that they serve to reduce redundancy in visual information that is highly correlated
      however  could we derive these filters from first
principles  in particular  if the retina is indeed trying to only keep predictive information  the solution to
our optimization problem should be these biphasic filters 
to directly compare our solution with measured

temporal correlations in natural images are typically
described as having a k f power spectrum  where k
is some positive constant  we generated such a dominantly low frequency stimulus by transforming white
noise in the frequency domain and then performing an
inverse fourier transform to return it to the temporal
domain  figure     after convolving the stimulus with
a given filter  we numerically compute the average

 


 

i  x   y    i  y   x   over tens of iterations for    
frames of stimuli each 

 


 

we minimized this estimate of i  x   y    i  y   x  
as a function of w using both the broyden fletchergoldfarb shanno algorithm  which has good performance
even when the error landscape is not smooth  and simulated annealing  while we initially anticipated that
simulated annealing would better find the global minimum since the information bottleneck value as a function
of the filters parameters was non convex  simulated
annealing would consistently cool at non optimal values 
the figures here are all results from the bfgs runs 

fi 

fig     the   f power spectrum  left  and time series  right 
of our  d naturalistic stimuli  the power spectrum on the
left is a log log plot with the theoretical value in red and the
simulated value in black 

  whether it be photons hitting the eye or a data matrix 
under certain assumptions  we now know that the problem of extracting the most predictive information from
 

 
the past boils down to learning the structure of 
x  x 
x
over time  lastly  the similarity of our optimally predictive filters and measured retinal filters suggests that biological organisms may already be exploiting this design
principle 
i would like to thank stephen baccus  surya ganguli 
jascha sohl dickstein  and niru maheswaranathan for
their guidance and suggestions  i would also like to thank
the tas and professor ng for their support 

results

to some degree  this optimization routine is doomed
to succeed   since optimality depends on the exact value
of  we choose  there is a large space of optimal filters 
nonetheless  the choice of  determines how highly a
system values compression versus prediction  in our
optimization  we found that high values of  resulted
in non biological filters  inset  figure     whereas low
values of  yielded optimal filters that were nearly identical to biphasic filters measured in real retinas  figure    
not only is this result consistent with the hypothesis

fig     on the left are real filters measured in tiger salamander      while the bottom right are the optimal filters
for         bottom  and         top  obtained after
two
initializations
of the bfgs minimization of

 different



 

i  x   y    i  y   x   

that the retina is selectively extracting predictable
information  it also suggests that the retina is in a
regime where the transmission of information is highly
costly  and must throw away almost all information
except that small amount which is highly indicative of
the future 

conclusions

the predictive information bottleneck provides a
model free way of quantifying the ability of a system to
extract predictable information from any arbitrary input


electronic address  lmcintosh stanford edu
    jeremy e niven and simon b laughlin  energy limitation as a selective pressure on the evolution of sensory
systems  journal of experimental biology              
           
    tim gollisch and markus meister  eye smarter than
scientists believed  neural computations in circuits of the
retina  neuron                     
    stephen a baccus  bence p olveczky  mihai manu  and
markus meister  a retinal circuit that computes object
motion  the journal of neuroscience                  
     
    toshihiko hosoya  stephen a baccus  and markus meister  dynamic predictive coding by the retina  nature 
                     
    mandyam v srinivasan  simon b laughlin  and andreas
dubs  predictive coding  a fresh view of inhibition in
the retina  proceedings of the royal society of london 
series b  biological sciences                         
    joseph j atick  could information theory provide an
ecological theory of sensory processing  network  computation in neural systems                    
    simon b laughlin  a simple coding procedure enhances a
neuronas information capacity  z  naturforsch                      
    vijay balasubramanian and michael j berry  a test of
metabolically efficient coding in the retina  network 
computation in neural systems                     
    horace b barlow  possible principles underlying the
transformation of sensory messages  sensory communication  pages              
     stephanie e palmer  olivier marre  ii berry  j michael 
and william bialek  predictive information in a sensory
population  arxiv preprint arxiv                 
     gal chechik  amir globerson  naftali tishby  and yair
weiss  information bottleneck for gaussian variables  in
journal of machine learning research  pages        
     
     felix creutzig  amir globerson  and naftali tishby 
past future information bottleneck in dynamical systems 
physical review e                     
     stephen a baccus and markus meister  fast and
slow contrast adaptation in retinal circuitry  neuron 
                   
     joseph j atick and a norman redlich  towards a theory
of early visual processing  neural computation          
          

fi