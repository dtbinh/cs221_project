cs     project report
machine assisted performance optimization of legion applications
sean treichler
december         

 

background

in our research group  were working on legion  a new parallel programming model that targets large 
distributed memory systems  e g  large hpc clusters   it is not uncommon for such a machine to have
thousands of nodes  multiple thousands of memories  and not just support  but require for performance  the
use of millions of threads of execution  the many challenges that exist when programming for these systems
can be lumped into three general categories 
   writing a functionally correct program
   achieving good performance  ideally on a variety of different machines
   writing the program in a reasonable amount of time
in most parallel programming models  the program text has to simultaneously express both what computation is being performed and how that computation should be distributed around the machine  this entangles
the first two goals  often to the detriment of the third  the legion separates these concerns by providing a
separate mapping step that determines how the logical tasks and data structures should be placed in the
system  this provides a guarantee that changes made to the mapping in an effort to improve performance
cannot introduce new functional bugs  legions operational semantics are also designed such that this invariant functional behavior happens to be equivalent to sequential execution of the programs tasks  further
simplifying the functional debugging effort 
unfortunately  debugging of performance issues remains a hard problem  the profiling of a sequential
execution can help identify some particularly gross performance problems  but many of the performance
bottlenecks in parallel systems only show up when multiple threads  cores  or nodes are contending for the
same resources  it is straight forward to gather profiling data from a large run  but it is nearly impossible
for a human to look over the reams of raw data to first determine where in time and space the performance
bottlenecks are  and then determine the likely cause of the bottleneck  however  both of these tasks seem
well suited to assistance from machine learning algorithms 
in particular  this project attempts take raw profiling data from one or more runs of an application
and categorize individual instances of tasks into groups with similar execution environments and runtime
behavior and then characterize the performance of the instances in each group with respect to properties
of the execution environment  although the categorization can help identify representative instances for
debugging  e g  the medians of each group  and instances that dont fit any of the groups  i e  outliers   the
primary output of the algorithm should be human readable descriptions of the groups and their performance
characteristics that help the programmer understand the different operating modes for a given task and what
factors have the strongest influence on those modes performance 
the description of the algorithm below consists of the following parts 
 section   gives a brief description of the raw data that is generated by the legion runtimes profiling
code 
 section   list the features that are extracted from this raw data 
 the choice of model and rationale behind the choice are presented in section   
 the algorithm for optimization of the model is covered in section   
 finally  the conversion of the models parameters to human readable output is described in section   

 

fi an evaluation of the algorithms performance on both a synthetic example and a real world example
is provided in section   
 section   concludes with a brief discussion of future work 

 

raw data

the legion runtime has existing code to generate a log of the tasks that are run as part of an application  an
entry in the log is generated for each instance of the task that is run  and contains the following information 
inst id
task id
proc id
start
finish

  a unique identifier for this task instance
  a numeric identifier     t   corresponding to which task this was an instance of
  the identifier     p   of the processor on which this task was run
  the time at which this task started executing
  the time at which this task finished executing

in addition to the execution log  the legion runtime provides information about the topology of the
machine  for each processor  the information includes the type of processor  e g  cpu or gpu  and which
node and memory domain the processor resides in  this information will be used to estimate which other
task instances may have contended for resources with a given instance 

 

feature definition

the profiling data is not useful in its raw form  however  many potentially useful features can be extracted
from the data   the future work discussion in section   will list additional features that are desirable  but
require additional profiling output   the first value extracted for each task is the runtime 
y  i    finishi  starti
 i 

the remaining features are grouped into two vectors  xd captures the discrete  i e  qualitative  features
while x i 
c captures the continuous  i e  quantitative ones   discrete features are extracted for each processor 
memory domain  node  and processor type  in each case  a feature is   for a task instance if that instance
ran on the particular processor domain node type and   otherwise  continuous features are used to extract
the runtime overlap between an instance and other tasks  for example  the average number of task instances
of a given type that are contending for execution resources with a given task can be calculated as 
 i 

proc overlapj  

  x
y  i 

  task id k    j  proc id k    proc id i   overlap i  k 

k

where overlap i  k  gives the absolute amount of time that task instances i and k were both running  a
 i 
corresponding domain overlapj estimates the amount of contention for memory system resources by tasks
 i 

running in the same memory domain  finally  node overlapj tries to estimate the contention for the network
resources by instances running on the same node 

 

model selection

the first major decision in the model selection process was that the performance of each type of task would
be modeled separately  legion tasks come in all shapes and sizes  and without any common metrics to
compare them  see section     there is no reason to believe that a model for one type of task would have any
predictive power for another  however  even after splitting instances by their task type  this is why task id
isnt listed as a feature above   observed data  see figure    indicated that a simple per task model would
not suffice either 
the next most obvious choice  a more complicated model that tries to characterize all the instances of
a given task type  was rejected for two reasons  first  a more complicated model introduces the risk of
 

fidistribution of runtimes for stencil task instances
   

number of tasks

  

  

  

  

 
   

   

   

   

   

execution time  microseconds 

figure    distribution of runtime for stencil task instances
overfitting  especially when the number of samples has been reduced to those of a single task type  second 
and more importantly  the final goal of this algorithm is a model that can be succinctly described to the
programmer  and a model in a very high dimensional space is unlikely to satisfy that requirement  even if it
fits the data well 
instead of trying to fit the data with one complicated model  we hypothesized that the data could be fit by
a relatively small number of simple models  conceptually this amounts to a piecewise linear approximation
of a curve rather than the use of higher order polynomials 
our model therefore consists of a variable number of submodels  each submodel has a gaussian distribution in the feature space and will use a simple linear estimator for the runtime of instances as a function
of the continuous features  we introduce a latent feature z  i  and then describe each submodel as 
 i 

 i 
x i 
 j
c   xd   z

y

 i 

 

 i 
x i 
c  z

 j



n  mj   j  

 j   tj xc i    n     j  

we further chose to constraint the variance in the feature space j to be a diagonal matrix  this was again
driven by the desire to avoid overfitting and to yield models that can be easily described to the programmer 

 

optimization

for a chosen number of submodels  the algorithm initializes the parameters of each submodel to exactly fit a
randomly chosen sample   when the desired number of clusters is not known  the algorithm uses the common
approach of trying increasing numbers of clusters until a knee in the curve is found   the optimization of
the model uses a minor variation of the em algorithm to maximize the log likelihood of the training data 
we start with an equation for the log likelihood that incorporates the latent feature z  i  and then select a
probability distribution qi  z  i    for each sample  allowing us to write 
l       m   

 

log

 

x

 i 

y

 i 
p x i 
c   xd   y        m   

i

log

i

x

 i 

 i   i 
p z  i    pc  x i 
c   xd   mz  i    z  i    pl  y  xc   z  i    z  i    z  i   

z  i 
 i 

 i 

 i 

pc  xc   xd   mz i    z i    pl  y  i   xc   z i    z i    z i   
qi  z  i   
i z  i 
 
 
 
 
x x
x x
 i 
 
qi  z  i    log pc  x i 
qi  z  i    log pl  y  i   x i 
c   xd    
c     c


xx

qi  z  i    log

i

z  i 

 

i

z  i 

 z

mixture of gaussians

 

 

 

 z

weighted least squares

 

fithe e step uses the normal posterior distribution of the latent feature with respect to other features and
model parameters  for the m step  the optimization of the submodels clustering parameters uses the standard mixture of gaussians approach  ignoring the off diagonal terms of each j    similarly  the optimization
of the linear performance estimate can be done with a standard weighted least squares algorithm 
in a minor deviation from the stock em algorithm  our implementation does not optimize both of these
terms at the same time  instead  an additional e step is done after optimizing the linear fit and before
the optimization of the clustering parameters  this asynchronous update seemed to result in more stable
behavior of the optimization process 

 

model description

the optimization process is run many times with random starting points  and the best result is chosen 
the algorithm then attempts to describe each of the submodels  in addition to simple statistics such as
the percentage of samples covered and the mean performance of the samples in the cluster  an attempt is
made to describe both the clustering and performance estimations  for clustering  a listing of the mean
and variance for every single feature is not likely to be helpful  instead  an analysis is done to see which of
the features is the most discriminative  i e  is the best at excluding the samples that are not in the cluster 
the top   features are listed  along with the percentages of samples they exclude  both individually and
cumulatively  similarly  a full dump of j for each model is eschewed in favor of a list of the   features with
the strongest correlation coefficients  i e  r   a sample output can be seen in listing   
listing    model description for synthetic data
model      
   s a m p l e s                  u                           s i g m a u                      
clustering  
     
     
c o n t i n u o u s v a r i a b l e                        v a r                    
     
     
d i s c r e t e v a r i a b l e  a                      v a r                    
performance factors  
     
continuous variable
model      
   s a m p l e s
clustering  
     
     
discrete

 

 

  d   dx                      

          u                        

v a r i a b l e  a                   

performance factors  
     
continuous variable

 

sigma u             

  var            

  d   dx                    

model      
   s a m p l e s                  u                         s i g m a u                        
clustering  
     
     
c o n t i n u o u s v a r i a b l e                        v a r                    
     
     
d i s c r e t e v a r i a b l e  a                      v a r                    
performance factors  
     
continuous variable

 

 

  d   dx                      

evaluation

to evaluate the effectiveness of this algorithm  we first analyzed its behavior on a synthetic data set  by
choosing a low dimensional feature space and having known clusters is it much easier to visualize both the
intended and actual behavior of the algorithm  our synthetic data used two continuous features  plotted
along the two spatial dimensions  and a single discrete feature  shown as xs vs  os in the scatter plot   the
xs form a single cluster  while the os form two distinct clusters  each with different performance behavior 
the top left pane of figure   shows the distribution of runtimes in the synthetic data while the bottom left
pane shows the intended cluster labels  the corresponding figures on the right hand side of figure   show
the output of the learning algorithm  we see that it reliably learns the importance of separating os from
xs  but it is not able to do a perfect job of separating the two clusters of os  this is because the final
clustering decision is based only on the clustering parameters  and the mixture of gaussians model cannot
describe the actual boundary between the two clusters   its likely that if the final clustering choice were
allowed to consider the goodness of the linear fit  it would get the right labels  but we chose not to allow
clustering based on the y  i  s as a performance model that gets a good fit by simply excluding the points
that dont fit isnt useful  

 

fitarget distribution by cluster

clustering quality

actual distribution by cluster

  

  

    

  

  

    

  

  

    

 

 

 

 

 

 

 

 

 

 
 

   

   

   

   

    

    

    

    

    

    

    

    

 

   

   

   

   

    

    

 

    

 

 

 

 

 

number of clusters

distribution of execution time by cluster

target clustering

actual clustering

  

  

 

 

 

 

 

 

 

 

  

number of tasks

  

  

  

  

 

 
 
  

 
 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

  

 

  

 

  

 

  

execution time  microseconds 

figure    results  synthetic data

figure    results  real data

figure   shows the algorithms results on a real legion application example  the same one that was shown
in figure     in this case  we dont have the correct labels for each individual instance  nor do we even know
the right number of clusters  the top half of figure   shows the best fit found for various cluster sizes  with
the knee in the curve being at   or    the bottom half shows the separation in the performance histogram
for the   cluster case  in this case  the algorithm has managed to separate the tasks that were run on gpus
 in red  from those run on cpus but also managed to highlight two slightly different operating modes for
the tasks on the cpus  possibly related to numa effects in the memory system  however  the algorithm
was unable to find a way to separate the spike of tasks with runtimes around    milliseconds   more analysis
is needed of the actual application to determine what features  if any  could be used to distinguish those
cases 

 

future work

we are optimistic that this algorithm will assist us in our performance optimization of other legion applications  but there are a number of additional improvements that are likely needed before we get there  first 
there are a number of additional features wed like to extract from the execution of an application  the most
obvious is counts of the number of operations performed by each task  e g  instructions  memory references 
cache misses   these will help in two ways  first  they may allow the creation of models that cover more
than one kind of task at a time  and second  its likely that the actual runtime of a task is correlated with
some of these features  although possibly a different set for different operating modes  
second  we want to explore whether its better to estimate runtime in linear space or logarithmic space 
a logarithmic space is probably better able to cover the wide range of runtimes we observe  and many
interactions are likely to have multiplicative effects on the runtime  which are easier to capture in a logarithmic
space 
third  the model descriptions for our real data sets arent as useful as wed like  these data sets have
many more features than the synthetic model shown above  and it appears that many of these features are
strongly correlated with each other  as a result  attempts to select the most discriminative or correlated
features often choose one of these correlated dependent features rather than the real feature  as a simple
improvement  wed like to weight the features by intuitiveness so that we favor descriptions based on those
over descriptions that use less comprehensible features 
finally  as with any clustering algorithm that uses a small number of clusters  the results of the optimization process are highly dependent on the initial seeds  its worth spending some time to see if an
agglomerative clustering algorithm is better able to fit submodels to smaller clusters  e g  the   ms spike in
figure    without resulting in so many clusters that the programmer doesnt have time to look at them all 

 

 

  

fi