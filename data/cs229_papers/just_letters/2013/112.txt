detection of insults in social commentary
cs      machine learning
kevin heh
december         

   introduction
the abundance of public discussion spaces on the internet has in many ways changed how we
communicate with others  whether it is a comments section for a controversial news article or a
forum for discussing a particular video game  these online spaces allow us to easily share our
own opinions and findings  as well as hear about the thoughts of others  though these
discussions can often be productive  the relative anonymity that comes with hiding behind a
username has allowed people to post insulting or inappropriate comments  these posts can
often create a hostile or uncomfortable environment for other users  one that may even
discourage them from visiting the site  this problem is a serious one that website owners
commonly face 
one potential way to mitigate this problem is to build a system that can detect whether or not
any given comment is insulting  with such a system  website owners would have a lot of
flexibility in dealing with this problem  for instance  the owner could choose to automatically
block or hide these insulting comments  or flag them so that they can more easily be found by
site moderators  the objective of this project is to do just this  build a machine learning system
that can accurately classify online posts and comments as insulting or not 

   methods
data  i obtained data for training and testing from kaggle  which is a popular website that
hosts machine learning competitions  the data set contains      examples  each of which
consists of the text of a particular post and its desired label  a label of   represents an insulting
post  while a label of   represents a non insulting post  for instance  two examples from the
data set are 



text  youre a moron  truth is beyond your reach  label   
text  ill take that tempi really hate the heat  label   

 

fiin total       of the examples are labelled as insulting  while the remaining      examples are
labelled as not insulting 
evaluation  the primary evaluation metrics that i used on my system were training accuracy
and cross validation with    folds  i used the training accuracy to determine how well the model
is optimizing with respect to the training set and to what degree it is overfitting to the training
set  the cross validation accuracy was more significant because it determined how well the
model generalizes to unseen examples  this value is a more accurate representation of how it
would perform in a realistic situation 
development  the system was developed in python       on the stanford university corn
cluster machines  it was also done with the help of pythons natural language toolkit  nltk  
as well as the pyenchant open source spellchecking library 

   classification model
model selection
i started out by trying to determine what kind of machine learning model to implement for this
task  i found that nave bayes  svms  and logistic regression are very common models for text
classification  so i decided to compare how they would perform on this task  as a result  i
implemented each of these with only unigrams counts as features  combined with some basic
preprocessing  such as lowercasing all letters and removing punctuation   their performance on
training accuracy and cross validation accuracy is shown in figure   
    
   
   

      

      
      

accuracy

   

      

   
   

      

training

      

cross validation

   
   
   
   
   
nave bayes

svm

logistic regression

figure    comparison between models using basic features
 

fiit is clear that logistic regression performs significantly better than the other two models in
terms of cross validation accuracy  about    higher   so i decided to implement the
classification system with this model as a base  i also decided to train my parameter vector using
stochastic gradient ascent since it is very easy to implement and since my data set is relatively
large 
improving the model and feature engineering
in order to improve the accuracy of the model  i decided to add many more features to my
classifier rather than just unigram counts  the additional features that actually helped to
increase the cross validation accuracy included 
 number of punctuation characters within a text


ratio of words and characters that are capitalized

 ratio of words that are bad  based on googles list of bad words 
 character   gram and   gram counts
several preprocessing techniques  besides simply lowercasing characters  also helped to improve
accuracy  including 
 tokenizing with nltks tokenizer
 stemming with nltks lancaster stemmer
the improvement produced by each feature and technique  in the order in which they were
added  is shown in figure   

cross validation accuracy

   
   
   
   
   
   
   
unigrams   of punc 

ratio
capital

nltk ratio  bad  char  tokenizer
grams

char  grams

stemming

figure    model improvement from implemented features
on the other hand  many features that i attempted to use did not improve my system  such as 



bigram counts  trigram counts  and character n gram counts besides n   and n  
part of speech tagging  provided by nltk 



autocorrection of unigrams  provided by pyenchant 
 

fireducing overfitting
with the addition of all of these features to the logistic regression model  the system achieved
     training accuracy on the full data set  while achieving only about     on cross validation
accuracy  hence  it is clear that the model is overfitting significantly in training  the two
techniques that i used to deal with this problem are as follows 




bayesian prior  instead of using maximum likelihood estimate  i used the bayesian
maximum a posteriori estimate to determine the weights  this technique allows my
parameter vector to have a smaller norm  and thus be less susceptible to overfitting 
filter feature selection  for each feature  i assign it a score which is equal to the
absolute value of the weight determined by stochastic gradient ascent  these features
will tend to be the most influential during classification  by keeping only a fraction of all
extracted features  the dimension of my feature vector will be reduced  which will also
helped to reduce overfitting 

after implementing these techniques  the training accuracy dropped to about        while the
cross validation accuracy increased  indicating that the system was generalizing better than
before 

   results and discussion
with all of the above features and techniques implemented in the system  as well as some
improvement from parameter tuning  it achieves a cross validation accuracy of       on the full
data set  figure   shows the breakdown of the results on true positive examples  insulting   and
true negative examples  not insulting  
true negative examples       

true positive examples       

     
     

correct

     

correct

incorrect

incorrect

      

figure    comparison of accuracy on true positive and true negative examples

 

fiits easy to see that though the system performs very well on negative examples          it
performs quite poorly in comparison on positive examples          hence  it tends to classify
quite conservatively as is  one way of mitigating this issue is to move the decision boundary for
classification 
normally  logistic regression classifies using zero as its decision boundary  referring to the dot
product between the parameter vector and the feature vector   in order to make the system
classify positive more easily  i moved the boundary in the negative direction from   to      
after this modification  the accuracy on true positive examples increased to        an
improvement of about     while the accuracy on true negative examples decreased to        a
drop of about       the overall accuracy  though  dropped slightly to        this kind of
decision boundary modification can be done to a lesser or greater degree as necessary 

   future work
though the initial results from this system are promising  there are a number of ways in which i
could proceed and improve on this project  one would be to use additional features  the current
system only examines the texts in isolation  though it may be useful to consider the relationships
between posts  such as the number of insulting posts within the same thread  in a more realistic
application  it would also potentially help to include a users past history  such as the number of
insulting posts that a user has written before  it may also be worth trying other nlp techniques 
such as chunking to extract phrases from texts  the current system uses stochastic gradient
ascent to compute weights because of its simplicity and convergence speed  but i would also like
to see how well another optimization algorithm would perform  such as newtons method 

references


dubs  jamie   google s official list of bad words   free art   technology  n p      july
      web 



elkan  charles  maximum likelihood  logistic regression  and stochastic gradient
training  tech  n p      jan        web 



forman  george  feature selection for text classification  rep  ibm    may       web 

acknowledgements
i would like to gratefully thank professor ng and all of the cs     teaching staff for providing
support and advice on this project  i would also like to thank kuan peng  a stanford student  for
his contributions toward finding and planning this project 

 

fi