applying machine learning to stock market trading
bryce taylor
abstract  in an effort to emulate human investors who read publicly available materials in order to
make decisions about their investments  i write a machine learning algorithm to read headlines from
financial news magazines and make predictions on the directional change of stock prices after a
moderate length time interval  using techniques that do not attempt to parse actual meaning from
headlines  i am able to approximate the overall market trends to a reasonable degree  but not well
enough to make money in times of small growth   decline  analysis indicates that features are present
in the data to make use of headlines for an algorithm  but due to an abundance of extra noisy features
we have not yet been able to determine precisely what these features are  i tried using natural language
processing to produce better features  but the best processors available were not able to interpret the
densely worded headlines well enough to be of use 
introduction   motivation 
twitter provides a standard source of data
to analyze the sentiment of the public and has
been used by many authors to attempt to predict
stock market changes  i believe that a down fall
of these methods is that they do not directly
attempt to evaluate the value of a stock and can
only be applied to large  well known companies
and wish to instead develop an algorithm that can
be used to determine how well a company will do
based on what the company actually does 
motivating examples of this are stock price
reactions to events such as changes in
management and major acquisitions  this would
allow our algorithm to mimic the thought process
of successful investors  i thus analyze news
headlines related to the company to determine
whether the headlines indicate positive news for
the company  using news sources from more
informed writers should provide much more
dense information than sources that read from
public opinion like twitter  since news articles
are released intermittently  i wish to design an
algorithm that can make a prediction based on a
single headline since if it has to wait for multiple
headlines  the market will have already reacted to
the first headline 

data sources 
i used two types of data to implement this
algorithm  headlines from financial analysts and
historic stock prices  headlines were manually
collected from the seeking alpha website by
performing a search for the stock symbols of each
of the companies  then parsing the results with a
custom java program  historic stock prices were
taken from the official nasdaq website  in
total  over       headlines were available taken
from the past   years  the furthest back the
seeking alpha website provided articles  from  
different companies  the companies were 
  ibm    nflx    goog    anf    mcd  
 shld   aapl 

the headlines were then tokenized using
the porter stemming process  with the exception
of having a token reserved for the stock ticker
symbol of each company used  to produce a set of
features corresponding to the list of symbols that
appeared in the headline and the company about
which the headline was written  feature vectors
for headlines were condensed into matrix form
and written to matlab readable files for portions
of the project involving svms and pca 
in order to simulate having to make
decisions on a real life stock market  the data was
divided         into training and testing data 

fisorted by the time at which the articles were
published so that all testing data occurred
chronologically after all training data  i initially
used randomized selection of the testing   training
data  but found that this was an unrealistic model
because any algorithm would then know how well
each company did on average over the testing
period since it was the same as the training
period  giving it unreasonable hindsight 
research questions 
for each of the algorithms  the goal was to
answer a question of the form 
given a headline released today about some
company x  will the stock price of x rise by more
than p percent over the next time period t 
as baselines  we compared our algorithm
to two simple algorithms  always responding yes
and always responding no  a third algorithm was
also used for comparison that took the best of
these two results  retrospectively choosing the
better of those two algorithms as its algorithm
 note that this algorithm can never have an error

rate above       this algorithm represents
following the general market trend  buying in an
up time and shorting in a down time 
based on research by eugene f  fama 
lars peter hansen and robert j  shiller for their
nobel prize winning paper in economics  i chose
t to be   months  in particular much longer than a
week  information taken from nobel prize press
release http   www nobelprize org nobel prizes ec
onomic sciences laureates      press html   this
is because their paper indicates it is impossible to
predict prices based on public information in the
short term  longer time periods were considered 
but due to only having access to   years of data
and making predictions based on a single headline
at a time  i decided to use t  months  p was
varied over different trials  a large value of p
indicates looking for the most extreme increases
in price  i e  a chance to make the most money  
bayesian classifier 
for my initial tests  i chose to use a simple
multinomial bayesian classifier that analyzed

figure  

fiheadlines based on the presence of each token in
the headline  since there were a total of       
tokens  this resulted in a lot of tokens being used
infrequently  and thus having inaccurate
measurements of the probabilities used in the
bayesian model   even after simple laplace
smoothing  as a second test  i removed any token
that did not occur at least    times in the
headlines to create a second set of     features
for use in the algorithm  the results of running
naive baye s are shown as p ranges from   to    
in figure   on the previous page  using all of the
features  the error was sometimes above     
which is unacceptable for most trading algorithms
and was generally outperformed by the algorithm
with reduced features since that algorithm never
had testing error above      neither algorithm
was able to beat the hardest baseline  best of
two algorithms  but the reduced feature
algorithm followed it reasonably well  possibly
well enough to use on real markets 
using data collected from the first run of
my bayesian classifier  table   shows the top  
most indicative symbols  that occurred at least   
times in the headlines  on both ends of the
spectrum for classifications run with p    some
of the symbols are not surprising  discussion of a
split likely means the stock price will drop in
half if the split happens which would be marked
negative by our algorithm  even though it is not
actually bad for investors   similarly tokens like
tough and event are logical since they indicate
difficult times for the company and the company
being involved with events  which is usually
deemed positive  this bodes well for the general
problem of extracting investing advice from
headlines 
in an attempt to improve accuracy even
further  i tried selecting only the top    most

indicative tokens on both ends of the spectrum
and rerunning the classifier  however  it
symbol

positive probability to
negative probability
ratio

buzz

       

split

      

mini

      

gross

      

tough

      

carrier

     

touch

     

effort

     

event

     

affect

     
table  

performed significantly worse than using    
tokens or using all of the available tokens  so we
likely removed important features from the
headlines 
precision   recall analysis
as a quick check  i computed the error on
positive headlines and the error on negative
headlines as p varied  varying p varies the
proportion of headlines that are actually positive
or negative   the positive error was strictly
increasing       to       while the negative error
was strictly decreasing       to        graphs
omitted to save space   these numbers are better
than the precision   recall which would be given
by the best of two algorithm  always resulting
in one of the errors being     but since they follow
the proportion of headlines marked positive  it
means that is one of the most indicative features
in my model  as opposed to the headlines
themselves  

fisupport vector machines 
after converting the data to matlab
format  i trained support vector machines
 svms  on the data for all of the same trials as
the bayesian classifier  unfortunately  matlab
was unable to process the full data set    k
headlines with   k features each   so i only
tested it on the reduced feature data set and the
minimal feature data set  the results were almost
identical to the bayesian classifier regardless as to
which type of svm was used  polynomial  linear 
etc  and roughly approximated the best baseline
solution  but did not match or beat it 
principal component analysis 
in order to determine how useful the
features i had actually were  i ran principal
component analysis on the data and then tested
linear svms on several of the top principal
components 
matlab s built in principal
component function was used to identify the
principal components and the data with all    
features was then projected onto this space to
create new training and testing data for the svm 
table   on this page shows the error rates as a
function of how many of the top principal
components were used 
there are several things to take away from
these results  first  using a small number of the
top principal components performs nearly as well
as the baseline and slightly better than naive
baye s  given a testing size of        these are
likely a statistically significant differences  
thus  some portion of the data does hold
information significant to predicting stock prices 
furthermore  adding more features does not
reduce the error rate further and in fact increases
the error rate by a significant amount  this means
that many of the features  those that contribute

to the non top principal components  are
effectively noise for the purposes of satisfying our
hypotheses 
number components error
 

      

 

      

 

      

 

      

 

      

 

      

 

      

 

      

 

      

  

      

  

      

   

      

     max 

      

bayes

      

baseline

      
table  

manual keyword selection
another method i tried for feature
selection was to manually select features human
insight indicates should be indicative  the key
words corresponding to these tokens are shown
below in table   with their frequency in
positively and negatively classified headlines  few
of these stand out as strong indicators  not
stronger than for example the average number of
times stocks rose on the training period  
training on these symbols alone meant that few
of the headlines could be processed total        
and resulted in an error of      when classifying
with p    notably worse than the baseline error of
     on the same set 

fisymbol

positive
negative ratio
frequency frequency

buy

     

     

     

sell

     

     

     

good

     

     

     

growth

     

     

     

beat

     

     

     

new

     

     

     

stop

     

     

     

disappoint

     

     

     

win

     

     

     

bullish

     

     

     

risk

     

     

     

bad

     

     

     

table  
natural language processing 
after failing to beat our baseline algorithm
using any of the standard features selection
techniques  i decided to try using natural
language processing to provide better features
for the learning algorithms  in particular  the best
feature would be an indication of whether or not
the article headline spoke of the company
positively or negatively  this is known as
sentiment analysis in natural language processing 
stanford has a publicly available natural
language processing toolkit that provides
sentiment analysis to sentences with high
accuracy         unfortunately  when tested on
the headlines  re parsed into sentence like format 
  the processor was unable to achieve high success
rates for non neutral sentences when compared to
my own reading of the headline  in a random

sample of    headlines the processor deemed
non neutral  i e  positive or negative sentiment   i
agreed with exactly half      of the assessments 
this is likely due to the fact that headlines are
very short  often requiring complex parsing in
order to fit them into a small space and thus do
not really resemble the sentences the processor
was trained in  which used normal  full text
sentences   natural language processors would
need to be specifically tailored to processing
headline like data to be able to make a
meaningful contribution towards answering my
research questions 
conclusion 
overall  my work indicates that a
sophisticated model able to beat overall market
trends by reading financial news headlines cannot
be easily found without fairly sophisticated
human like processing of the headlines 
however  using my work  one would be able to
run an algorithm that does notably better than
random chance and can do better than not
investing at all by roughly following both positive
and negative market trends 
future work 
after working on this project all quarter 
there are a couple of ways that it might be
extended to better answer the question 


customize natural language processing
tools to work well with headlines



make predictions based on more than one
headline at a time  this may allow for
better long term analysis of a company s
potential

fi