using undirected graphs to guide mutli label text classification
john cardente
          

 

introduction

use of directed acyclic and edge based graphs to perform multi label classification 

the importance of multi label classification has increased with the growth of online collaboration fongnb
rums containing large amounts of text on diverse top   
ics  user generated annotations  called tags      are
commonly used to help discover relevant information  many multi label prediction algorithms  in particular
these tags are often inconsistently applied which re  mixture models  explicitly consider tag co occurrence
duces their effectiveness 
relationships during training but not prediction  the
conditional probabilities generated by these algosupervised machine learning techniques can be used
rithms inherently confound the relationships between
to identify the topics within a body of text and pretags  while these techniques are effective  improved
dict the appropriate tags  using these techniques
accuracy and efficiency may be obtained through exmay substantially improve the consistency of text anplicitly considering tag relationships during predicnotation and make finding relevant information eastion  the network guided naive bayes  ngnb 
ier  this requires machine learning algorithms capamodel presented in this paper explores this potential 
ble of accurately modeling many tags and processing
large data sets in reasonable amounts of time 
during ngnb training  a binary multinomial naive
bayes classifier is learned for each tag  an undirected
this paper presents and evaluates a new network
graph is also created from the tag co occurrences in
guided naive bayes  ngnb  classifier that uses
the training examples  each graph node represents a
undirected graphs to accurately and quickly predict
single tag  an edge in the graph indicates that the
labels for multi topic text  the ngnb classifier
two associated tags appeared together in a training
is compared to binary relevance multinomial and
sample label  self loops are included in the graph to
parametric mixture naive bayes models to determine
represent the case when a tag appears alone  counts
its relative effectiveness 
for each edge are recorded to represent the strength
of the relationships 
during prediction  ngnb first identifies the most
likely tag using the per tag multinomial naive bayes
models  this step ignores any tag relationships and
multi label text classification is a well established evaluates each in isolation  for very large datasets  a
field  tsoumakas et al     and puurala     provide graph search starting from highly central nodes can
a good overview  madjarov et al     compare various be used to find the most likely tag without having to
multi label prediction algorithms using a variety of evaluate all the tags  once identified  the log odds
data sets  mccallum     and ueda et al     present of the most likely node is scaled by the ratio of the
mixture models based on naive bayes classifiers that self loop edge count to the number of times the tag
attempt to learn the correlations between tags  wang appeared in the training set  the resulting weighted
et al     and zhang et al      respectively describe the log odds is used to estimate the likelihood that the
most likely tag should be predicted alone 

 

related work

next  the undirected graph is used to determine the
set of potentially relevant additional tags  unlikely

 john cardente emc com

 

fitags are removed from this set using the per tag
multinomial naive bayes predictions computed in the
first step  the power set of the remaining nodes  up
to a configurable size limit  is computed  each powerset combination is evaluated by inducing a sub graph
for the associated tags and propagating the per tag
log odds values from the edge nodes to the node representing the most likely tag  a scaling factor  based
on the smallest edge count in the graph  is used to reduce the contribution of the propagated log odd values as they flow through the network  using the
smallest edge count introduces a penalty for large
tag sets and prevents their over prediction  after
the propagation phase is completed  the accumulated
log odds value at the node for the most likely tag is
used to estimate the likelihood of the combination 
after all the power set combinations are evaluated 
the set with the highest accumulated log odds value
is chosen as the final prediction 

 
   



















































































































































































































figure    tag co occurrence undirected
graph for training set 

evaluation

set to evaluate the effect of noisy data 

data
   

to evaluate the ngnb approach  a corpus of posts
to the stack exchange website across a wide variety of topics     was utilized  the data set  provided
by stackexchange  consisted of   million posts each
containing a title  body  and one or more human generated labels representing the associated topics  the
full data set contained       unique tags with an average of     tags per post 

models

to evaluate ngnb  two baseline models were implemented for comparison  a binary relevance multinomial naive bayes  brnb  model was used to establish the effectiveness of predicting each tag in isolation  in this model  a separate multinomial naive
bayes binary classifier was built for each tag  during
prediction  a tag was selected if the positive outcome
probability was greater than that of the negative outto reduce the time and resources required to evalucome  no size limit was placed on the final predicted
ate the ngnb and other models  a smaller training
set of tags 
data set was created from        posts containing
    commonly occurring tags  figure   illustrates a parametric mixture model  pmm   was used
the tag co occurrence undirected graph for this train  to represent the effectiveness of considering tag ocing set  the network consists of a single connected currence relationships while calculating the wordcomponent containing     edges between the     tag tag conditional probabilities  during training  the
nodes  the ngnb approach can also be applied to pmm  algorithm uses the tags associated with each
networks containing multiple connected components  sample to adjust the conditional word probabilities 
after applying an expectation maximization process 
to further reduce the evaluation time and resources 
the resulting conditional word tag probabilities imuninformative words were removed from the training
plicitly reflect the influence of the tag co occurrence
set using within class popularity and gini coefficient
relationships  during prediction  a greedy approach
metrics based on     
is used to iteratively select the set of tags that most
an independent set of       posts were selected to increase the likelihood until it cannot be increased
create a test data set  the uninformative words iden  further  see     for further details of the pmm  algotified in the training were not removed from the test rithm 
 

fithe ngnb implementation follows the description
provided in section    all of the models were implemented from scratch in the python programming
language 

train
   

test




   


   

in all three cases  separate classifiers were built for
the title and body features of the sampled posts  the
union of the tags predicted by the title and body
classifiers were used for the overall prediction 

   

  




brnb

pmm 

ngnb

brnb

pmm 

ngnb

figure    cross validation per fold train
and test execution times  values are in seconds  dots represent the mean  and error
bars reflect the     confidence interval 

cross validation

figure   provides the recall  precision  and f  score
results from using    fold cross validation to evaluated the three models  the brnb model achieved
the highest recall but lowest precision yielding the
overall lowest f  score  both pmm  and ngnb
performed better with ngnb achieving the best results across all three measures  these results indicate
that the naive approach of ignoring tag dependencies
substantially reduces performance  they also show
that considering these dependencies during the testing phase  ngnb  can be as effective as in the training phase  pmm   
recall



precision

although the performance of all three models decrease as the number of tags increase  ngnb exhibits
the slowest rate of decay 
figure   illustrates the average per fold training times
for the models  the data shows that the pmm 
models training time dramatically increases with the
number of tags  brnb and ngnb similarly exhibited a substantially smaller increase in training time
as the number of tags increased suggesting that these
algorithms are more scalable 

f 



   

f 




   










   







   








   








   
brnb pmm  ngnb

brnb pmm  ngnb

brnb pmm  ngnb



   
  

figure    recall  precision  and f  score
results from    fold cross validation testing 

  

brnb

figure   illustrates the average per fold train and test
execution times for the models  the data shows that
the pmm  model required substantially more time
to train than the simpler brnb and ngnb models  pmm  also took longer to classify the test folds 
in the both the training and testing phases  ngnb
performed similarly to brnb 

  

pmm 

  

  

ngnb

figure    f  score    fold cross validation
results for increasing number of tags 

   

test set

figure   provides the recall  predicision  and f  results from classifying the test data set using the three
models  as in the case of cross validation  ngnb profigure   provides the f  score results for the three vided the best results indicating that the algorithm
models while increasing the number of possible tags  generalizes well even in the presence of noisy samples 

   

tag scaling

 

fi

may be worthy of further investigation and development to enable multi label classification of large online datasets 



references

train
  
  
  


  
 










  

  

  

  

  

    http   en wikipedia org wiki tag  metadata 
brnb

pmm 

ngnb

    http   blog stackoverflow com         stackoverflow creative commons data dump 
figure    average cross validation perfold train execution times  y axis is time
in seconds  x axis is the number of tags 

recall

precision

    s  singh  h  murthy  and t  gonsalves  feature
selection for text classification based on gini coefficient of inequality  journal of machine learning
research proceedings track           

f 



    g  tsoumakas  and i  katakis  multi label classification  an overview  international journal of
data warehousing and mining  ijdwm        
           

brnb pmm  ngnb

    a  puurula  mixture models for mutli label text
classification university of waikato 



   


   








   



   
brnb pmm  ngnb

brnb pmm  ngnb

    g  madjarov  d  kocev  d  gjorgjevikj  and
s  dzeroski  an extensive experimental comparison of methods for multi label learning  pattern
recognition                      

figure    recall  precision  and f  score
results from classifying the test data set 

 

    a  mccallum  multi label text classification with
a mixture model trained by em  aaai   workshop on text learning           

conclusions

the growth of online collaborative forums has made
multi label classification a common activity  the size
and diversity of these data sets create the need for
scalable machine learning algorithms capable of modeling many labels with varying degrees of dependency 
mixture models are a common method used in multilabel classification  this paper presented data indicating that such models may require substantial
training time to calculate the conditional probabilities representing the tag relationships  to overcome
this challenge  this paper presented a new algorithm
call network guided naive bayes  ngnb  that deferred consideration of the tag dependencies to the
prediction phase  data from cross validation  tag
scaling  and test data set testing provided evidence
that ngnb can be as effective as complex mixture
models such as pmm  while only requiring training and test times comparable to simpler models like
brnb  ngnb also degraded the least as the tag
population size increased suggesting better scalability  these findings suggest that the ngnb method

    n  ueda  k  saito  parametric mixture models for
multi labeled text  advances in neural information processing systems                
    x  wang  and g  sukthankar  multi label relational neighbor classification using social context
features  proceedings of the   th acm sigkdd
international conference on knowledge discovery
and data mining acm                
     m  zhang and k  zhang  multi label learning
by exploiting label dependency  proceedings of the
  th acm sigkdd international conference on
knowledge discovery and data mining acm  new
york  ny  usa           
     c  manning  p  raghavan  and h  schutze  introduction to information retrieval  cambridge university press     

 

fi