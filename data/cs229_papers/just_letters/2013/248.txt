validity of user reports in a chat network
alan zhao
december         

 

introduction

chat networks hold users to certain standards of behavior  users who consistently exhibit
poor behavior can be filtered away from well behaved users  finding toxic users is a challenge 
when the number of users are small  a few human moderators may be sufficient  but as the
userbase grows  automated user filtering becomes a worthy goal 
a user report system alerts administrators to potentially toxic behavior  naively  one might
declare frequently reported user as toxic  assuming that all the reports are genuine  however 
user generated data contains noise  thus  in a report system  identifying toxic behavior
requires identifying the validity of results 

 

chat data

data was used from chatous  a chat network that pairs unfamiliar users to chat with each
other  after a conversation  a user can report his chat partner for toxic behavior  the
number of chats that involve a report is very small  less than     
when reports do occur  they are originate from a small number of users  for every report
originator there are on average   accused by a report  in addition      of originators in
the dataset had also been accused  this is in part since previously accused users are more
likely to be matched up with each other  however  it does make reports rather dubious 

 

text classification and reports

one idea to use text classification to predict user reports  in doing so  user reports consistent
with previous reports can be given higher regard 
 

fithe number of report generating chats is very small compared to the total number of conversations  also  since conversations are varied and spur of the moment  therefore  the data
has inherently very high variance  to produce more meaningful results  it was necessary
to restrict the training and test data was to conversations where one conversation partner
reported the other 
both the unrestricted dataset and the restricted dataset were divided into a training set and
a test set  for the unrestricted dataset  which is very large  smaller subsets was used  each
conversation was further divided into two halves  one for each conversation partner  the
goal was to predict  given a conversation half  whether than person had been reported as a
result 
two models were implemented  linear svm and multinomial naive bayes  the most reliable set of features is the text by itself  other potential features  such as profile information 
did not significantly improve the model 
linear svm
accuracy
dataset
unrestricted
      
      
restricted

when report predicted when report made
      
      
      
      

multinomial naive bayes
dataset
accuracy
unrestricted
      
restricted
      

when report predicted when report made
 
 
      
      

 

fiboth the linear svm and the multinomial naive bayes tend to produce false negatives 
the accuracy is artificially high for the unrestricted dataset because of the extremely
low number of actual reports  because of this  the data points where a report is either
predicted or actually occurs are more indicative of performance  training and testing on the
restricted dataset generates more useful results  although it still generates false negatives 
the predictions are more accurate 
each report also fell under one of three categories  so using the same techniques categories
were also predicted  these predictions  however  are fairly poor  this indicates that the
challenges of small text samples is exacerbated when looking for more fine grained results 
linear svm  restricted dataset
report category
spam
filth
harassment

accuracy
      
      
      

when report predicted when report made
      
      
      
      
      
      

multinomial naive bayes  restricted dataset
report category
spam
filth
harassment

accuracy
      
      
      

when report predicted when report made
      
      
      
      
      
      

 

fi 

report reliability

even if individual reports can be verified  the reported user must eventually be judged as
toxic or non toxic  to properly learn  we need a training set of good bad designations  but
at the current time the set of true training examples was too small to use directly  instead 
we used a report frequency heuristic  where often reported users were considered to be toxic 
and infrequently reported users non toxic  this heuristic has good       correspondence
with the true sample from server data  especially considering users may be labeled toxic with
very few  e g    in     reported conversations in the given chat dataset 
the main idea is to use the past successes failures of a report originator to evaluate their
new reports  an svm model was used  trained on user id  along with basic chat information
such as word count  text classification is not used here 
svm model
frequency modifier accuracy
 
      
 
      
      
 
  
      
      
  
  
      

bad user predicted
      
      
      
      
      
      

bad user exists
 
 
      
      
      
      

in this constructed situation  a lower frequency modifier indicates that fewer reports are
needed to mark a user as toxic  when the frequency modifier is    nearly every report
indicates a toxic user  so svm easily performs well  for higher frequency modifiers  the bad
user prediction rate remains high  but the total number of bad users identified drops  much
as in the text classification above 
 

fi 

conclusion

analyzing and evaluating user reports is a difficult task to automate  particularly since the
standard is subjective and human  applying such a standard is problematic for exclusively
machine based report judging systems  however  if report data follows certain assumptions 
machine learning techniques can aid in filtering reports to facilitate a final human review 

 

acknowledgments

i am particularly grateful to kevin guo for providing the chat datasets  and for helping me
move forward when i got stuck  i also thank everyone else at chatous for developing a cool
service  and im excited for its future 

 

fi