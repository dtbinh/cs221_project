astronomical point source classification through machine
learning
idel r  waisberg
december         
abstract
we investigate the performance of a variety of classification algorithms to astronomical point source
classification  specifically  to classify objects from the sdss survey into main sequence red giant stars 
white dwarfs or quasars using photometric data across   bands  we found that k nearest neighbors
and a multinomial svm using a gaussian kernel were the algorithms with the best performance  and
dependence on training size data and feature importance were further investigated 

introduction and objective
with the sloan digital sky survey  sdss   which provides a catalog with around     million objects in
the visible band and covering     of the celestial sphere  the amount of data available for astronomers has
become  if one can dare say  astronomical  a particular problem is the classification of point like sources 
due to the lack of visible structure of these objects  these include different types of stars  main sequence 
red giants  white dwarfs   but also more complex structures  such as quasars   supermassive black holes
surrounded by an accretion disk  
the most accurate way to classify point like sources is through high resolution spectroscopy i e  by
analyzing atomic transition lines  however  this somewhat complex process can easily take a large amount of
time given the great number of sources provided by the survey  an interesting question is whether an accurate
classification of point like sources can be made through lower resolution photometry i e  measurement of
light intensities for a few different bands that each span a large wavelength range  this is a much less
complex and robust measurement than spectroscopy  and could therefore provide a desired alternative to
the latter       
the aim of this project is to investigate different machine learning algorithms to classify astronomical
point like sources into   possible classes  quasars  main sequence  red giant stars and white dwarfs  by
using photometric data in   different bands  u  ultraviolet   g  green   r  red   i and z  very near infrared   
available from the sdss survey 

data structure and features selection
both training and test data were obtained from data release    of the sdss survey         test
data consists of      quasars  qsos        main sequence and red giant stars  m r stars  and     white
dwarfs  wds  obtained from a region spanning    in declination and  h in right ascension from the
authors zodiac sign virgo constellation    ra  dec       h        the training data consists of      qsos 
     m r stars and      wds from sky regions excluding the test data range  also  only sources for which
     mag      and for which the uncertainty in the measurement is      mag in all the photometric
bands considered are obtained  in order to exclude too bright or too faint sources  and to ensure that the
measurement is reasonably accurate  the classes of all the objects in the test and training data have been
previosly confirmed by spectroscopy 
because the brightness of an object varies with the distance from earth  the magnitudes  roughly the
log of luminosity  provided by the photometry measurements are not themselves good indicators of the class
of a point like source  rather  it is the ratio of luminosities  difference of magnitudes   called color indices
 between adjacent photometric bands that matter  since they indicate how one color predominates over
the other      in this case  we are provided with   photometry measurements in the near visible range  hence 
there are   color indices  u g  g r  r i  i z  that can be constructed  the most straightforward input vector of
features in this case is therefore   dimensional  figure   shows the three bivariate color color plots for the
test data  using the known classification solution 
 

fifigure    test data represented in bivariate color color diagrams 

classification algorithms   results and analysis
the aim of the machine learning classification algorithms is to explore complex or unknown relations
between features and labels to classify new data  without a complete understanding of the physical laws that
might deterministically connect them  below  we discuss different classification algorithms and investigate
their classfication performance 
unsupervised learning  k means clustering
we start with one of the simplest models for classification  k means clustering  with   centroids in
this   dimensional space  the result  shown in figure    shows that this is a poor classification algorithm
 accuracy           for instance  it divides the main sequence structure in the leftmost bivariate plot into
the   classes and is unable to distinguish between qsos and wds 

figure    bivariate color color plots for test sample resulting from k means clustering with   centroids 

given it is an unsupervised learning algorithm  kmeans can only take into account the spatial
distribution of the data  and ignores the different relations between features that vary depending on the
label  supervised learning tries to explore such relations through the training examples  figure   below is a
parallel coordinates plot for a section of the training data     randomly selected objects of each class   and
clearly shows different trends in feature relations depending on the class label  e g  g  r and r  i tend to
 

fibe directly proportional for m r stars and inversely proportional for qsos   therefore  it is expected that
supervised learning algorithms would have a much better classification performance 

figure    parallel coordinates plot for training data 

supervised learning  mlr  gda  knn and svm
here  we investigate the performance of supervised learning algorithms for classification  multinomial
logistic regression  mlr   gaussian discriminant analysis  gda   k nearest neighbors  knn  and support
vector machines  svm  
gda was applied assuming the   covariance matrices are the same  linear decision boundaries   knn
was applied with k   neighbors  svm was applied with a cost c       and three types of kernels  gaussian
 
 k x  z    exp  xz      linear   k x  z    xt z  and a second degree polynomial  k x  z     xt z      in this
case  because there are three classes  a multinomial svm algorithm is needed  this algorithm is based on
the regularized svm algorithm presented in     using a one versus one approach i e  given k classes  c k    
binary classifiers are constructed    if k      and the class selected by the most classifiers is chosen 
table   shows the performance of each model on both the training and test data 
table    supervised learning algorithms accuracies     on training  tr  and test  ts  data 
mlr
gda
knn
svm  gaussian 
svm  linear 
svm  polyn  

ta  m r stars 
     
     
    
     
     
     

ta  qsos 
    
     
     
    
     
     

ta  wds 
    
     
     
     
     
     

ta  overall 
     
     
     
     
     
     

ts  m r stass 
     
     
     
     
     
     

ts  qsos 
     
     
     
     
     
     

ts  wds 
     
     
     
     
     
     

ts  overall 
     
     
     
     
     
     

the table shows that all the algorithms achieved a good performance in the classification of m r stars 
overall  the accuracy in classifying qsos and wds was much lower  this is expected given the considerable
overlap between these two classes in the bivariate color color diagrams  indeed  the confusion matrix for the
algorithms show a high degree of confusion between these two classes 
the two best overall performances were given by knn and svm  gaussian kernel  algorithms  at
first  this might seem curious given the simplicity of the former algorithm and the complexity of the latter 
however  this might be expected  since the models constructed by both algorithms are heavily determined
by the euclidean distances between the training examples  changing the cost value c of the svm algorithm
and increasing the number of neighbors k of the knn algorithm did not alter the performance significantly 
table   shows the confusion matrix for knn and svm  gaussian kernel  when the model is applied to

 

fitable    confusion matrices for training  ta  and test  ts  data for knn and svm  gaussian kernel   lines refer
to predicted classification  whereas columns refer to actual  real  classes 
knn  ta
m r star
m r star
    
qso
 
wd
 
svm  gaussian   ts
m r star
qso
wd

qso
wd
knn  ts
  
  
m r star
    
  
qso
  
    
wd
m r star
qso
wd
    
  
 
   
    
  
  
  
   

m r star
    
  
  

qso
  
    
   

wd
 
  
   

svm  gaussian   ta
m r star
qso
wd

m r star
    
 
 

both the training and test data  the two algorithms performed very similar when the model was applied
to the training data  comparing the performance on the test data  they perform quite similarly for the
classication of wds  but the svm  gaussian kernel  performs superiorly for the classification of qsos and
inferiorly for the classification of m r stars 
further investigations
cross validation studies were done to further assess the performance of knn and svm  gaussian kernel  
a leave one out cross validation on the training set for knn resulted in an accuracy of        given the
higher complexity of the svm algorithm  a    fold cross validation was performed in this case  resulting in an
accuracy of         the svm  gaussian kernel  was therefore chosen to be used for further investigations
of the data  figure   shows the bivariate color color plots of the classification result of the svm  gaussian
kernel  on the test set  it is very close to the true classification shown in figure   

figure    classification result of svm  gaussian kernel  on the test set 

figure   shows how the training and test errors change as a function of the training size  m  when
svm  gaussian kernel  is applied  the training error increases slightly as m is increased  whereas the test
error decreases until approximately m         and then further increases  the large gap between training
and test errors suggests the algorithm does not suffer from high bias  so that an increase in the number
of features is not likely to increase the performance  the fact that the test error increases with m soon
after the operating point suggests that the algorithm does not suffer significantly from high variance  and
therefore decreasing the number of features is also not likely to increase the performance of the algorithm 
therefore  we conclude we are close to the optimal performance of the algorithm at the operating point 
given the information and physical knowledge available 
although machine learning algorithms can function quite well without the knowledge of the underlying mechanisms or physical laws concerning the data  their results can provide insight into the data that
might initially not have been obvious  table   shows the performance of the svm  gaussian kernel  algo 

 

qso
  
    
  

wd
  
  
    

fifigure    training and test error dependencies on the training data size for svm  gaussian kernel  algorithm  the
black dots refer to the point where the previous results in this paper were obtained 
table    left  performance of svm  gaussian kernel  given each feature used alone  right  ablasive analysis of the
same algorithm and features 
ug
gr
ri
iz

training accuracy
     
     
     
     

test accuracy
     
     
     
     

add i  z
add r  i
add g  r
u  g  baseline 

training accuracy
     
     
     
     

test accuracy
     
     
     
     

rithm using only one of the   features for classification  whereas table   shows the result of ablasive analysis
performed on this algorithm  i e  the features are added one by one from bottom to top and the increase
in the performance assessed   from the tables  it is possible to see that g  r  u  g  r  i and i  z are 
in decreasing order  the most important features for the classification of astronomical point sources in main
sequence red giant stars  white dwarfs and quasars  which might provide previously unavailable insights into
the different physical structures and characteristics of these astronomical bodies 

further work
there are several possibilites for future work in this topic  among which 
  use more complex classification algorithms such as neural networks and classification trees 
  include more object types in the dataset  more high redshift quasars  brown dwarfs  they appear in redder
bands of the sdss survey   and spatially extended galaxies 
  use physical information to add additional features and or weights and prior probabilites to the algorithms 
and check if performance is improved 

references
    http   cas sdss org astro en tools search sql asp
    http   vizier u strasbg fr  vizier catalogue  sdss dr  quasar catalog  schneider         and sdss
dr  white dwarf catalog  kleinman         
    feigelson  eric d  modern statistical methods for astronomy with r applications cambridge      
    advances in machine learning and data mining for astronomy chapman   hall
    phillips  a c  physics of stars wiley      
    ng  a  cs     lecture notes  part v   support vector machines

 

fi