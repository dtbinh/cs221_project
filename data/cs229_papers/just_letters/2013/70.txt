predict foreground object in still image
stanford univeresity
cs    fall     
amer hammudi
ahammudi stanford edu

darren koh
dtkoh stanford edu

abstract

 

in this paper  we are interested in the detection of foreground objects in a single still image  using the collections of outdoor scene
rgb images  we applied supervised and unsupervised machine
learning techniques to predict the foreground objects  the main
challenge of foreground object detection  is that single pixel from
a given image is not sufficient to distinguish between foreground
or background  and thus  combination of features needs to be considered 

we use the stanford background dataset          images  and
semantically augmented make d dataset                images 
as our training data  this dataset contains labeled regions  surfaces and layers such as foreground object  sky  tree  road  sky 
building  etc  we mainly focused on the labeled foreground object and treat other labeled regions as background  we filtered out
    images  retained     images  that do not contains foreground
object label  furthermore  to make our data easier to work with 
we resized all images to    x     figure   is the raw image and
figure   is the labeled foreground pixels 
our approach in using the dataset is to leave out     of the data
    images  for testing and the remaining          images  for
training 

 

introduction

foreground object detection has been successful using multi color
background model per pixel  gaussian mixture model  proposed
by grimson and stauffer            however it required sequence
of same scene images  we are interested in detecting foreground
object with single still image 
the ability of finding foreground objects has many useful real life
applications  for example  an autonomous self driving vehicle
can utilize foreground object detection and pairing depth map information  such as stat of the art technique done by andrew ng
and his team                and pre apply braking as object getting
closer to the vehicle  alternatively  the autonomous vehicle can
turn on warning brake lights as object is approaching and passing
over safety distance from the rear  this can replace the expensive
and bulky equipment such as radar or lidar system being adapted
in the auto industry 

 

approach

our approach is to collect outdoor scene images as our training
set  then extract features that we consider relevant for foreground
object classification  we will use k mean to cluster scenes  train
logistic regression hypothesis and employ neural network to
get the final hypothesis that allows us to predict foreground pixel
within a single still image 

data set

figure    raw image

 

figure    labeled foreground
 white   background  black 

feature extractions

the features that we think are relevant to foreground object detection are pixel location  rgb color  texture segmentation 
and pixel intensity 

   

pixels location

we use the x and y coordinate of a pixel as the pixel location feature  the intuition of using pixel location is  we believe with in
an outdoor scene image  a foreground object is most likely distributed around the horizontal axis 

fi   

 

rgb color

training phase   initialization step

we think the rgb color of an individual pixel is a relevant feature
to distinguish foreground objects in an outdoor scene image  for
example  the green color  such as tree  grass  and the blue color
 such as sky  are more likely distributed in the background than
foreground in an outdoor scene 

in the first step of training phase  in order to reduce the training
time with     images that each contain        pixels  we are
utilizing the parallel and gpu computing toolbox in matlab and
as a result  we cut down the time exponentially in comparison  in
particular  we employed parfor and gpuarray 

   

the resulting test error of each of the hypothesis function
are

texture segmentation

using each pixel entropy value of the   by   neighborhood around
the corresponding pixel in an image as texture  we want to see
if it provides any relevancy to the foreground information  it is
generated with the matlab function entropyfilt 

   






pixel intensity

figure   below is the general area where location hyphothesis
function classifies pixel as foreground 

we believe object that is closer should have brighter color intensity in the outdoor scene image  to extract the pixels intensity
feature  we convert each pixel to gray scale color         using
the matlab function rgb gray 

 

location error         
rgb error         
texture error         
intensity error         

evaluation result

to evaluate the accuracy of our foreground object detection algorithm  we use the equation
lb 
   lf
 
accuratecy  
  tf tb

figure    location   predicted foreground

   

figures      below are the original images  actual foreground pixels  and their corresponding predicted foreground pixels resulting
from the trained rgb  texture  and intensity hypothesis 

where lf is labeled foreground  t f is true foreground  lb is
labeled background and t b is true background 

 

logistic regression

in our first step to classify pixels as foreground we will train four
 
different sigmoid functions
and to classify each pixel as
 t x
  exp

foreground based on the features mentioned above  since the data
is not linearly separable we define the feature vectors
  vloc       i  j  i    j    i  j 
figure    raw image

  vrgb       r  g  b  r    g    b    r  g  r  b  g  b 

figure    actual foreground
pixels

  vtxt       r  g  b  r    g    b    r  g  r  b  g  b 
  vint       s  s   
where  i  j  in  vloc represent the location of the pixel in the image 
 r  g  b  in the  vrgb and  vtxt represent the rgb color and texture 
finally s in  vint represent the intensity 
in this classification we are assuming that the foreground pixels
are generated through randomized process bernoulli   where
 

 foreground pixel
total number of pixel

figure    rgb   predicted foreground

   

 

figure    texture predicted foreground

figure    intensity predicted foreground

fix 
location

   

location

   

rgb

x 
x 
rgb
x 

figure    raw image

h x 

figure     actual foreground
pixels

x 
texture

   

texture

x 
x 
intensity

   

intensity

x 

figure     rgb   predicted foreground

figure     texture predicted foreground

figure     intensity predicted foreground

  
figure     neural network with    input units and   hidden units

clearly we have improved our result significantly from the
independent hypothesis  location  rgb  texture  and intensity
logistic functions 

figure     raw image

figure     rgb   predicted foreground

 

figure     actual foreground
pixels

figure     texture predicted foreground

figure     raw image

figure     predicted foreground using neural network

figure     raw image

figure     predicted foreground using neural network

figure     intensity predicted foreground

neural network model

our next step is to integrate the information produced by the four
hypothesis  location  rgb  texture  intensity  and produce our
final output  to do that  we created a neural network shown in
figure    
the activation for the final neuron is the result of the four
logistic functions mentioned above  in addition  we included the
quadratic of the results from the four hypothesis due to the linear
separability issue 
the test error from the output hypothesis is        and the
images produced on the test images are shown in figure       
 

fifigure     raw image

 

figure     predicted foreground using neural network

figure     before scene classification figure     after scene classification

k mean scene classifier

to further improve our hypothesis of image foreground pixels
classification  we believe clustering the outdoor images in
the training set and then create the hypothesis function for
each cluster should improve our prediction  the intuition is that 
similar scenes should have similar foreground objects distribution 

figure     before scene classification figure     after scene classification

we used the scale invariant feature transform  sift  to
produce a feature vector for each image and then use k mean
to create clusters from the training data set  we then used sift
method to produce a feature vector for the test image to decide on
the relevant hypothesis function in order to classify the test image
pixels 
we selected sift for clustering because sift is invariant
to image scaling and rotation and partially invariant to illumination and viewpoint changes  furthermore  the sift method
has been widely used and popular in scene recognition      
we tested different values for k             for the k mean
algorithms  for k       the number of images in certain clusters
is much less than others  for our purposes we selected k     due
to the density of the three clusters and is sufficient to show that
clustering produces better results  however we think that larger
k would improve our result and further research is needed to
decide on the number of clusters  we believe that the number of
clusters should be a function of the number of different objects
identified by the sift method  and the hypothesis function for
the test images would then be selected based on the cluster with
the max number of objects matches 

figure     before scene classification figure     after scene classification

  

cross validation check

to verify that we do not have high variance due to the number
of features that we are using  we ran    rounds of hold out
cross validation  hocv  on the neural network  each round
split the data in strain       and scv       randomly  the
result shows  our generalization error is very close to our training
error  therefore we believe we do not have high variance in our
algorithm 
figure    below is the result of    rounds hocv and their
corresponding test error  the average test error of   clusters   the
doted line is the test error on cross validation  scv    while the
solid line is the test error on the    images      of     images
that were left out for testing  as mentioned in last part of section
    data set above  

after applying scene classification and train each hypothesis and neural network  the test error has improved slightly by
      from        to         figure       shows the same
images with improved predicted foreground pixels after applying
scene classification 

 

fi  

acknowledgements

we would like to thank brody huval  cs    ta  for his advice
and input on computer vision 

references
    chris stauffer and w  e l grimson  adaptive background
mixture models for real time tracking       
    w  e  l  grimson  c  stauffer  r  romano  and l  lee  using adaptive tracking to classify and monitor activities in a
site       
    chris stauffer and w  eric l  grimson  learning patterns of
activity using real time tracking  ieee trans  pattern anal 
mach  intell                 august      

figure     before to scene classification

  

future work

    a  saxena  s  h  chung  and a  y  ng  learning depth from
single monocular images  proceedings of advances in neural information processing systems  nips        

figure    below is the learning curve and it does seems to suggest that as we increase the data set  the test error is stabilizing
and no longer dropping  hence  we have a bias problem with our
algorithms  accordingly we think adding other image features is
necessary to improve our result  specifically we think that integrating histogram oriented gradient  hog  in our neuron network
should improve our result  to integrate hog  we think it is best to
train an svm and use the output as another input to the last neuron or add one more layer to the neuron network  furthermore 
we have shown that using only three clusters in scene classification  we believe having more clusters should improve the result 

    a  saxena  m  sun  and a  y  ng  make d  learning  d
scene structure from a single still image  ieee trans  of
pattern analysis and machine intelligence  pami        
    a  saxena  s  h  chung  and a  y  ng    d depth reconstruction from a single still image  international journal of
computer vision  ijcv        
    a  saxena  j  schullte  and a  y  ng  depth estimation using
monicular and stereo cues  in international joint conference
on artificial intelligence  ijcai        
    s  gould  r  fulton  and d  koller  decomposing a scene
into geometric and semantically consistent regions  proceedings of international conference on computer vision
 iccv        
    b  liu  s gould  and d  koller  single image depth estimation from predicted semantic labels  proceedings of conference on computer vision and pattern recognition  cvpr  
     
     guoshen yu and jean michel morel  asift  an algorithm for
fully affine invariant comparison  image processing online 
     

figure     before to scene classification

 

fi