learning to identify antonyms
natalia silveira
cs     final project  fall     

   introduction
antonymy is a common lexical relation that is intuitively clear  if not always easy
to define  for humans  but challenging for machines  in natural language processing 
antonymy detection has applications in tasks of understanding language  such as
paraphrase detection  question answering  and textual inference  for that reason 
wordnet  fellbaum        includes some antonymy annotation  but the relation is
relatively rare  and a quick manual inspection reveals that there are many more antonym
pairs  including very common ones  than those shown in wordnet  it also becomes clear
that the relation is somewhat vague  masculine and neuter  for example  are listed as
antonyms in wordnet  but many native speakers of english would not intuitively
consider these words antonyms 
the presence of these antonyms is wordnet makes an interesting resource for
supervised learning  however  which opens the possibility of trying to automatically
extend the annotation  identifying pairs of antonyms in corpora is the task i propose in
this paper  the approach is to train a classifier to distinguish between sets of sentences
that contain pairs of antonyms from sets of sentences that do not  the intuition behind it
is that antonyms are often used contrastively in the same sentence  the highestperforming classifier obtains     accuracy  because the literature on this task is limited 
it is hard to rigorously compare the performance of the classifier with existing published
results  however  it does seem to in line with  and perhaps above  results reported from
other research 
an important aspect of this approach is that knowledge rich feature engineering
was deliberately avoided  the reason for this is that an approach to detecting antonymy is
much more productive if it fits into a general framework for learning other lexical
relations  such as synonymy  hypernymy  entailment etc   therefore  whereas linguistic
knowledge about the antonymy relation can be useful for the task  for example  features
indicating the presence of morphemes such as un  or dis  would clearly be informative   i
instead opted for a distributional approach  where the features are
   literature review
there has been some previous work in learning lexical relations in general  and
antonymy in particular  mohammad et al         present the task of detecting antonymy
degree  and bring in insights from the study of antonyms in corpora  although their work
is of limited methodological interest for me  because the goal of the authors was different
than mine   the theoretical insights inform my approach  they point out that antonym
pairs are formed in part by collocation  speakers think of words as antonyms not by
reasoning exclusively about the semantics  but also by observing that they occur in
similar contexts  and are used contrastively  furthermore  antonyms are  similar  but
different   antonyms are not words with meanings that are as different as possible  but
words with meanings that are very similar  but different in some respect  such as referring
to opposite ends of the same scale for measuring the same property  mohammad et al 

fi       also present evidence about the behavior of antonyms in corpora that will be
important for my approach  antonym adjectives occur in the same sentence more often
than expected by chance  in fact  mohammad and hirst        show that they tend to cooccur in a five word window  this seems to happen because antonyms are often used
contrastively  for example  in a phrase like  not just not cold  but quite hot   they also
occur in similar syntactic contexts  that is  the syntactic structures that allow the
occurrence of one word will also allow for its antonym  the main insight here is that
cooccurrence data can be informative
distributional methods  based on bag of words vectors  have been criticized for
not being able to distinguish between different types of similarity  word vectors of
antonyms will look similar  because the meanings of antonyms are closely related  and
they can usually modify the same types of nouns  and be modified by the same types of
adverbs  therefore  a naive approach might not be able to distinguish pairs of synonyms
from pairs of antonyms  however  being able to use distributional information in refined
ways might be a path for a general framework for detecting lexical relations 
turney        presents a unified framework for detecting lexical relations with a
distributional approach  by introducing features that refer to the syntactic and lexical
patterns that connect words when they occur together  rather than simply looking at the
contexts of each of the word individually  an svm classifier was then trained on these
features  this seems particularly promising for antonymy  because  as mentioned above 
it is clear from corpus research that antonyms occur together in contrastive patterns  in a
task of classifying pairs of words as antonyms or synonyms  turney s approach had    
accuracy 
baroni et al         presents an interesting distributional take on detecting lexical
entailment  another relation  this means identifying that  for instance  all dogs entails
some dogs  or that bright student entails student  they create a number of pairs of
entailing and non entailing phrases  and train a polynomial kernel svm to work on the
concatenation of the word vectors of each phrase in the pair  for a large training corpus 
this method obtained     accuracy  the evaluation is less rigorous than that of turney
        because baroni et al  they report results only for distinguishing antonyms from
random phrase pairs  but not from  for example  synonyms  still  this is an interesting
result for an innovative task  and it raises a question of how far distributional methods
can be taken in the discovery of fine lexical relations 
   data
several different parts of speech can enter antonymy relations  and wordnet has
the annotation for nouns  verbs  adverbs and adjectives  in this paper  i focus on
adjectives  the reason is for this choice is that the annotations in wordnet are more
numerous and higher quality for adjectives  also intuitions about adjective antonymy
seem crisper to me  which is relevant because the approach relies on antonym pairs being
seen as such by speakers  which would be reflected in their use of antonyms in the
corpus  there are      adjectives in wordnet that are annotated for antonymy 
the text comes from the ldc annotated gigaword  ldc release ldc    t    
this choice of corpus is essentially motivated by the size of this resource  and the
availability of an annotated version  i worked with the new york times section  which
contains thousands of stories published in the new york times from july      to

fidecember       the total number of tokens in this section of the corpus is approximately
   m words 
my data consists of a pair to word matrix  indicating the frequency with which
word w cooccurs in a sentence with both adjectives in the pair of adjectives  x  y  
essentially  the vector for the pair  x  y 
the pairs were formed only adjectives occurring at least     times total  if this
threshold seems high  note that the corpus has almost   billion words   since many of the
adjectives that are annotated with the antonymy relation on wordnet are somewhat rare 
this threshold was meant to guarantee that we have enough information about the data
points to meaningfully evaluate the classifier s performance  a total of     pairs of
antonyms were found with this methodology  to create the negative examples  i
randomly drew     pairs of synonym adjectives  each occurring at least     times  from
wordnet  and then created another     by randomly pairing up the adjectives already
harvested for the synonym and antonym pairs  and ensuring that no new pairs of
synonyms or antonyms were created  the pairs were split       for training and testing 
note that there is no word sense disambiguation in the corpus  so there is some
noise in the data  for example  hot is the antonym of cold only when it refers to
temperature sense  not when it refers to spiciness  but in this methodology  sentences
such as the curry wasn t good  it was too hot for my taste  and cold by the time it reached
the table would contribute to the vector for the pair  hot  cold  
   experiments and discussion
the type of classifier chosen for this task was svm  which has often been shown
high performance in nlp tasks 
preprocessing  i experimented with different transformations commonly used in
the nlp literature on bag of word vectors  pointwise mutual information  length
normalization  and tfidf  term frequency inverse document frequency   these
transformations were not helpful in preliminary experiments with a linear kernel  my
intuition is that in this case the length of the documents  which is roughly the number of
sentence in which the adjectives cooccur  is particularly important  because for this
reason  transformations designed to  factor out  document length are not appropriate  the
only transformation on the design matrix that improved results was standardizing the
feature values to have unit feature variance  this is widely reported to be helpful for
classification with svms 
i also experimented with chi square based feature selection  it is often reported
for text classification tasks that using only a subset of features does not improve
performance  in this case  intuitively it seems like feature selection may be more helpful 
in classification with richer labels  based on genre or topic   it seems that having a richer
representation of the text is intuitively important  but since in the current task the key
features are simply signals that a contrast is being expressed  feature selection seemed
like a good idea  the results for subsets of features of various sizes were slightly worse in
preliminary experiments with the training set  since the literature points in the same
direction  i opted to use the full set of features  this being a very large corpus  there were
around         features total 
choice of kernel  i chose a linear kernel for the svm  based on performance on a
held out set  for this  i ran a very coarse grid search to optimize an rbf kernel and a

fidegree   polynomial kernel  both performed consistently worse than the linear kernel  for
all parameter settings i tried  the results were below the lowest results obtained with the
linear kernel  additionally  the accuracy of the best linear kernel classifier on the training
set was      another signal that it was appropriate for this data  for these reasons  i
chose the linear kernel for further experiments 
parameter estimation  with this choice of kernel  i performed a grid search on
the parameters c and   optimizing for cross validation accuracy on the training set
within the values                    for c and                          for   the best
results were obtained with c   and        
results  the results obtained are described in the table below  as a reminder  the
test set contains     pairs of antonyms      pairs of synonyms  and     random pairs 

precision
recall
f score

antonyms
    
    
    

non antonyms
    
    
    

total
    
    
    

the total accuracy is      these are better results than i was able to find in the
literature 
   conclusion and future work
the results obtained here are encouraging  although it is hard to make an exact
comparison  because of distinctions in how different authors define the task  these results
are clearly not worse than previously achieved results for learning antonymy  and other
lexical relations  with distributional methods  they may in fact be better  as we see almost
    error reduction with relation to the accuracy reported in turney        for
identifying antonym pairs  excluding the insight that antonyms tend to occur in the same
sentence  no knowledge about how antonymy behaves specifically was used  which
opens an avenue for extending the approach to other relations 
a clear next step in this work would be to incorporate syntactic features that
might help characterize the constructions in which the pairs of adjectives occur together 

fi