predicting corporate   k content using machine
learning techniques
min ji lee

hyungjun lee

graduate school of business
stanford university
stanford  california      
e mail  minjilee stanford edu

department of statistics
stanford university
stanford  california      
e mail  hyungjun stanford edu

abstractthis project tries to predict the contents
of corporate   k filings using naive bayes assumption
and support vector machine  we manually classify the
contents of     corporate   k filings as financial  operational  legal  administrative or hr  train the naive bayes
classifier and multi class svm based on this classification 
and test their performances  both techniques yield error
rates as high as one third due to the limited sample size 
when we test binary classification  financial and nonfinancial   accuracy improves as more data points are fed
into the classification process 

i  i ntroduction
companies are required to file form   k to the securities and exchange commission  sec  to notify its
investors of any material events  examples of such material events include changes in management  departure
of directors  bankruptcy  and layoffs  in presence of a
material event  it is likely that the market will react or has
reacted to the event  thus  we predict that there will be
an association between the content of the   k filing and
stock returns  in this project  we study the contents of  k filings using methods of machine learning  however 
the scope of this project does not extend to analyzing
their relationship with stock reactions  this project is
primarily inspired by li             which investigates
the information content of the management discussion
and analysis  md a  section in various firms annual
reports  where management discusses various aspects of
the company  as with li             the purpose of this
project is to examine the qualitative information that can
potentially be useful to investors in addition to hard data 
specifically  we try to predict the topics covered by
item      other events in the corporate   k filings 
unlike the others sections and items of   k filings 
which all have their respective topics defined in the filing
requirements  item      can cover a range of topics not

covered by the other sections  thus  predicting the topics
of the   k sections filed under item      could add
valuable insight into understanding the content of those
filings 
the project comprises three parts  the first part is
to download and extract   k filings and to split the
filings text into words using perl  then  we screen
      filings that contain item      from the total of
       corporate   k filings filed in       from the
      filings that contain item       we randomly select
    and manually classified their contents as financial 
operational  legal  administrative or hr  using the vector
of words  a multi variate bernoulli event model is trained
with naive bayes assumption  and a multi class svm is
also trained  the last part is evaluating the performance
of these models using an n fold cross validation test  if
either approach performs well on this data set  it could
be used to classify tens of thousands of corporate   k
filings every year and provide valuable insight into the
operations of the filing firms  throughout the process 
we use perl and r as our primary tools for analysis 
ii  background
the section on form   k on secs website defines
the requirement to file   k reports as the following 
in addition to filing annual reports on form    k
and quarterly reports on form    q  public companies
must report certain material corporate events on a
more current basis  form   k is the current report
companies must file with the sec to announce major
events that shareholders should know about  in      
      different firms filed          k forms  this gives
us an average of     filings per reporting firm while the
median number of filings for reporting firms is    such
a positively skewed distribution is driven by a few firms
that filed a large number of filings such as   st century

fifox  inc   which filed       k reports in       due to
the massive number of filings  we limit our analysis to
form   ks filed in      and do not expand it to those
from other years 
public companies are required to file   k reports for
events including changes in management  departure of
directors  bankruptcy  and layoffs  the initial goal of the
study was to classify   k filings according to their tones positive  negative and uncertain  however  in the process
of manual classification  we come across difficulties of
losing most of the filings to the uncertain category  as
a large fraction of filings are merely factual  thus  we
changed our gear to classifying the content of a filing
and not the tone 
form   k is divided into multiple sections and items 
depending on the content 
section    registrants business and operations
item      entry into a material definitive
agreement
  
 
section   financial information
item      completion of acquisition or disposition of assets
  
 
section   other events
item      other events
the item of our interest is section   item      other
events  which covers a range of topics not covered by
the other items and hence does not have a specifically
defined topic  unlike all other sections  in       there
are         k filings with item        note that a single
filing can contain multiple items   since the topic of the
section of form   k filed under item      cannot be
predicted by its definition  we employ machine learning
techniques to classify them  we expect that filings are
associated with one of following five aspects  financial 
operational  legal  administrative  and hr  we limit the
scope of the financial category to cover only financingrelated activities such as equity issuance  stock repurchases  and dividend payouts  for instance  below is the
following   k report by omnicare  inc  filed on march
 st       
item       other events  on march   
      omnicare  inc   the company  announced its adoption of a rule   b    plan
under which the company may continue to repurchase its shares at times when the company
would not ordinarily be in the market due to

the companys trading policies or the possession of material non public information  this
plan has been established pursuant to  and
as part of  the companys share repurchase
program      
to list a few frequent topics that each category covers 
operational covers acquisition disposition of assets  legal covers lawsuits and patent issues  administrative
contains announcement of meetings conferences  and
hr has departure or election of executives 
iii  m ethodology
a  naive bayes classifier
as naive bayes classifiers are not restricted by the
number of response classes  we adopt this methodology
as our primary mode of analysis  we have five content
classes   financial  operational  legal  administrative 
and hr   in this project  whereas li            uses
   categories for the content classification  thus naive
bayes classifier provides greater flexibility to future
extensions of this project  to prevent any arithmetic
underflow problem  we take logarithms when computing
naive bayes labels 
 i 
 i    b 
j     xj   k  y
pm
 i    b n    v  
i
i     y

pm pni

k y b  

i  

 

   

we start off with the laplace smoothing       in
equation      of the estimated word probabilities  then
we adjust  in equation     to see whether a specific
degree of smoothing yields a better performance than
others given the number of training data points  if  is
too big  that is  when we use more aggressive smoothing 
all words end up with roughly identical conditional
probabilities for all categories  thus  the conditional
probabilities contributions to log likelihood are roughly
identical for all categories and the examples end up
being classified according to the prior probabilities  as
the size of the training set increases  the smoothing
effect of fixed alpha decreases  as more word counts
are registered  if the training set size is too small and
 also too small  the few words that had appeared
in the training set will have unduly high conditional
probabilities  introducing high variance to the resulting
model and thus poor performance on the test set  thus 
picking the right smoothing parameter  given the
training set size is very important to the performance of
nb models 

fih
table i
t hree m ost f requently e ncountered w ords in e ach
c ategory
category
adm
fin
hr
leg
opr

word  
park
share
director
trust
statement

word  
standard
note
annual
report
energy

word  
annual
security
grant
quarterly
press  release

b  multi class svm
svms are natural binary classifiers and does not have
an obvious extension to the multi class cases  there are
two ways that are widely used to apply svm to multiclass problems in practice  one versus all  ova  svm
and pairwise svm     
in ova svm  for each class of observations  an svm
separating the given class from all other classes is built 
with the convention of labeling observations in the given
class with    and those in all other classes with     the
most widely used approach for making predictions based
on ova svm is to choose the class whose svm assigns
the highest margin to the point in question  with ova
svm  non linear kernels may be very useful  for even
if the decision boundary between each pair of classes
are linear  the ova svm for a class that is sandwiched
between two different classes will not perform well
under linear kernels  hence  we will try several forms
of non linear kernels to address this issue 
in pairwise svm  given each pair of classes i and j  
an svm separating the two classes is built  thus  for
an n class problem  a total of n n       svms are
constructed  to make a prediction  the class to which
the point in question was most frequently assigned by
the n n       pairwise svms is chosen  more efficient
algorithms for aggregating the results of the n n      
pairwise svms have been studied in the field  although it
would not be particularly relevant for our problem since
we only have five classes and thus only ten pairwise
comparisons to make  which  given the size of our
example  is not computationally challenging 
we will train the data using both the ova svm and
pairwise svm and compare their performances to that
of the naive bayes classifier 
iv  data p rocessing
we download all          k filings made in     
from the secs edgar website  for each   k filing  we
download what is indicated as complete submission text

file  which includes not only form   k but also attached
documents  then we sort out       filings that contain
item       out of these       filings  we randomly pick
      ks and manually classify them into each of five
aforementioned categories  as part of our data cleaning
efforts  we remove all html tags and tables to extract
only the text language  once we extract the   k text  we
further examine the text as it may contain information
irrelevant to the content analysis  for instance  in accordance with the private securities litigation reform act
of       some filings include a disclaimer that it is a
forward looking statement along with the definition of
forward looking statements as below 
forward looking statements are made
based upon managements good faith expectations and beliefs concerning future developments and their potential effect upon the
company and can be identified by the use
of words such as anticipated  believe  expect 
plans  strategy  estimate  project  and other
words of similar meaning in connection with
a discussion of future operating or financial
performance      
this will effectively increase the counts of words such
as expect and violate the naive bayes assumption 
thus  we exclude the paragraph containing the definition
and disclaimer  we are not aware of any other common
disclaimer definition at this point  then the truncated text
is tokenized into a set of words  as a next step  we use
perl module lingua  stopwords to remove stop words
such as the  and  and of  which appear frequently
in most documents but provide a little information about
the content of filings  lastly  before moving onto the
textual analysis  we use perl module lingua  stem to
reduce related words to a common root form  for instance  name  names  and named are replaced with
name 
once this process of data cleaning and parsing is
completely done  we move onto to the textual analysis
using machine learning techniques 
v  t extual a nalyses
table   shows three most frequently appeared words in
each category when the filing category is predicted using
the naive bayes method  as we have gone through    
filings manually and know the content  keywords in the
table give us confidence that the method is picking up
the right words  for instance  director can be about
director appointment while grant in hr relates to
compensation grants such as stock options  fin mostly

fih
table ii
c onfusion m atrix
actual  prediction
adm
fin
hr
leg
opr

adm
  
 
 
 
 

fin
  
  
 
 
 

hr
 
 
  
 
 

leg
 
 
 
 
 

opr
 
 
 
 
  

relate to firms financing activities  and share note 
and security are the words you would expect to see
filings about equity debt issuance  share repurchases and
such  however  most terms in the table are fairly generic
that there is a risk of misclassification 

fig     naive bayes     fold cv with different smoothing factors

smaller   less than or equal to    performs better  while
run on the full set  larger ones   equal to   or    
outperform the smaller ones 

fig     naive bayes    class vs  binary

figure   shows when the data is classified into five
categories  nearly one third of filings are misclassified 
we believe that two factors contribute to such high error
rates  first  our dataset is not large enough to produce a
statistically stable model  second  almost half of filings
belong to fin  which leads to its prior probability being
exceptionally high  thus  if conditional probability for
a given category is not particularly high  an observation
is classified as fin by default  the confusion matrix
in table   confirms this  to resolve this issue  we
examine a binary classification into financial and nonfinancial topics  in which setting each category contains
comparable number of data points 
in case of binary classification  we test various
smoothing factors as mentioned in the previous section 
for a given training set size  smoothing factors can yield
a difference as large as   percentage point in test error
rate  as expected  when the training set size is small 

fig     naive bayes vs  svm     fold cv

we then conduct    fold cross validation with both
naive bayes and svm with different kernels   naive
bayes in figure   indicates the one with         it
turns out that svm with linear kernel outperforms all
other methods in most cases  although naive bayes with
the biggest smoothing factor perform the best on the full
set  we can infer that the decision boundaries supported
by linear svms best suits our data  although differences
are within   percentage point in all cases  when we test
various cost values c for the objective function to make
it work for non linearly separable datasets  we get same
error rates which shows that our data is not sensitive to
the choice of the regularization parameter  this indicates
that our data is reasonably well behaved and does not

ficontain a disproportionate number of outliers 

fig     various methods

we return to the   category classification problem 
as there is no solid svm method that works best for
all multi class problems  we conduct both one versusall  ova  svm and pairwise svm  indicated as psvm in figure     the results tell us that there is no
significanct difference between the two svm methods
and they perform similarly to the naive bayes classifier 
vi  c onclusion
we use the machine learning techniques  specifically
naive bayes and svm  to predict the content of corporate   k filings  when introduce more than two categories  both methods yield error rates as high as onethird  for the purpose of this project  we reduce the
number of category to two  and both techniques perform
better with error rates around       percent  naive bayes
perform slightly better in binary classification and the
two methods perform very similarly in the multi category
problem  what we would be eventually interested in
is multi category classification and we are positive that
once we significantly increase the size of the training
set  that is  manually classify more filings  then more
information would be fed into the our models and we
will be able to predict the filing contents more accurately 
r eferences
    li  f          the information content of forward looking statements in corporate filings  a naive bayesian machine learning
approach  journal of accounting research                
    janes  g   et al          an introduction to statistical learning 
springer  new york 

fi