probabilistic token selection via fishers method in text
classification
authors  anqi fu  yiming sun  katherine yu
 anqif  sunstat  yukather  stanford edu
abstract  in this project we consider a multiclass text classification problem on three newsgroups with
      entries each with a feature class consisting of over        tokens  our baseline naive bayes method
gives a misclassification error rate of        and we focus on variable selection methods to improve upon this
error  we compare a token selection method using naive bayes to one using the related fishers method and
a threshold  we find that token selection with fishers method does significantly better using two effective
choices for this threshold  first  one that controls the number of tokens selected to be     of the original
        second  a threshold we compute based on a probabilistic estimate of the distribution of the test
statistic  the final misclassification errors are       and        respectively 

   introduction
in this project  we explore the well known multiclass text classification using entries from three newsgroups
titled comp os ms windows misc  soc religion christian  and talk politics guns taken from the    newsgroups
dataset      our data consists of       entries for each of the three groups  with a feature set of frequencies
of        distinct total tokens  naturally  our primary objective is to reduce the dimension through variable
selection  we observe first that token selection through naive bayes does much better than variable selection
through pca  as is often the case with sparse  discrete data  we then improve upon this baseline naive
bayes by introducing a related technique  fishers method  which estimates probabilities of categories given
tokens  and which allows us more flexibility and control over the number of selected tokens 
using fishers method  we implement a threshold to select most distinguishing tokens and de noise the
non important tokens  we first try cross validation to choose this threshold  but ultimately  we find two
more effective methods to choose the threshold  one  by selecting the threshold that reduces the number
of tokens by a factor of      and two  by computing a threshold based on a probabilistic estimate of the
distribution of our test statistic  using fishers method with these two thresholds  we reduce our baseline
naive bayes misclassification error of       to       and        respectively 
we will first present the theory of fishers method and then show how we used it to improve upon naive
bayes 
   fishers method
we understand fishers method in relation to naive bayes  in naive bayes  we model the frequency
of each token given the category as a multinomial distribution  and under this model  we estimate the
probability of the instance of token j given category k using laplace smoothing as
pm pni
 i 
 i 
  k     
i  
j     xj   k  y
 
     
j y k  
 i 
  y   k ni    v  
where m is the number of documents  ni is the number of tokens in document i  and  v   is number of tokens
in our feature set  in fishers method  we instead look at p y   k   x   j   which is the probability that the
document is in category k given that it contains token j  our test statistic is then
n
x
     
tk    
log py  k x j  
j  

this statistic comes from the chi square statistic of the standard statistical fishers method  which is
based on the fact that if we simultaneously test n hypotheses  their p values under the null hypothesis will
be distributed as u nif         so under this assumption  the statistic tk will be distributed as    n    lower
p values  which indicate the null is unlikely  lead to higher values of tk  
 

fi 

probabilistic token selection via fishers method in text classification

the test statistic in our model has a slightly different null distribution  under our setting  our null
hypothesis for each token is that the token occurrence is independent of categories  i e  given that we have
vj total occurrences of token j over all categories  these occurrences are equally distributed across categories 
our estimate of this probability is
wk j
 
py  k x j   pg
k   wk j
where g is the number of categories  wi k   j y  k   and j y  k comes from our naive bayes computation 
under the null hypothesis for token j 
 
g
g
x
x
wk j  
wk j  mult
wk j     g   k           g 
k  

k  

pvj

pg

in other words  wk j   k   wk j   i   xi   where xi  m ult      g  and vj  
large  we can use the central limit theorem to approximate this distribution

      
 
 
 
p j
g    g  
g
g 

d
  
  
  
vj
 n        
     
 
 
 
 
g 

 
g

pgj

pg

k  

wk j   so when vj is

 
 
g    g  

as vj   
the assumption of vj large is reasonable for the high frequency tokens we will be selecting  thus we assume
the test statistic in       will behave as a non central log normal distribution under the null hypothesis  the
statistic   log py  k x j represents a propensity score toward categories for a specific feature  the more
likely the category k  the smaller this statistic  the statistic tk averages these propensity scores over
the tokens in our feature set  from this null distribution        we derive an approximate quantile for tk  
which is the probability that tk would take on such a high value if instances of tokens were independent of
categories  and we classify the document in the category k which maximizes this quantile 
   variable selection methods







   


















   



  

  

   


















  

  

  



   



   

   



error rate

   


training
testing

   

training
testing



error rate

   




  



  





   

number of singular value

   

   

our baseline naive bayes algorithm on the entire dataset  with all tokens  gives a classification error of
       we aim to reduce this error with effective variable selection methods 
we use variable selection through pca as a reference point  figure     shows the results of the test
and train error of multinomial logistic regression and knn using variable selection through pca  the best
achievable test error rate in logistic regression is       which is even bigger than our baseline naive bayes 

  

  

number of singular value

figure    multinomial logistic regression and knn with variable selection through pca 
thus  we dispense with pca  pca is a very general method which distorts the specific nature of our
data  our frequency table is discrete  while pca forces us to treat the data as continuous  also  finding the
svd is unnecessarily time consuming  especially for a large  sparse matrix  we find token selection much
more appropriate and effective on our data 

fiprobabilistic token selection via fishers method in text classification

 

     selecting most distinguishing tokens  for comparison with fishers method  in naive bayes  we
select the tokens j for which the variance
g

s    j    

  x
 j y  k  j     
g 
k  

 
g

pg

is maximal  where j  
k   j y  k   this is just one measure of how much token j distinguishes
categories  choosing up to        tokens in this way  the best achievable test error with naive bayes is
      
in fishers method  for each token j  we use the statistic
     

mj  

max wj k 

 
g

wj k 

 
g

k     g

min

k     g

to determine most distinguishing tokens  note that we must have mj      given that we will later consider
the distribution of this statistic under the null hypothesis  instead of taking the n tokens with the lowest
mj  
we choose all tokens j such that mj  r   
where r  is a chosen threshold 
in the same feature set size range  the best achievable test error is        we want to find an automated
method to select a threshold which gives an error close to this optimal error  in the graph below  note that
we are not claiming fishers method works better than naive bayes as a classification algorithm  but instead
that fishers method allows us to use a better token selection method  selection through the threshold  

    
fisher
nb
     

testing error

    

     

    

     

    
    

    

    

    

    
    
number of tokens

    

    

    

     

figure    naive bayes vs  fishers method test error with token selection  threshold
selection  respectively 

student version of matlab
     choosing r  such that  v   is small  to select a threshold 
we use    fold cross validation  but we
do not simply take the threshold which gives minimal average cross validation error  with our data  many
thresholds corresponding to different sizes give about the same error  and we would like to select the smallest
one  given the high danger of overfitting  if we simply take the best threshold by cross validation without
restricting size  we get a threshold r           corresponding to         tokens        of the total   and a

fi 

probabilistic token selection via fishers method in text classification

test error of        we find it is very effective to choose the best r  that selects at most a fixed fraction of
the number of tokens  we restrict to     of the total number of tokens  this method gives r          and
a test error of      
     

    

average cv testing error

     

     

     

     

    

     
 

   

 

   

r 

figure       fold cross validation on threshold in fishers method 

     choosing r  based on clt  although restricting size improves on the results of cross validation 
it is still better in some ways to use statistical insights instead 
there are many well known issues with
student version of matlab
cv  in some sense  it is finding the expected minimum value of error  not the parameter which minimizes
the expected value of error  instead  we try to derive a threshold for the statistic mj in       based on the
asymptotic distribution of wj k under the null hypothesis  under the null hypothesis  we have that mj is
asymptotically the ratio of a maximum over a minimum of a g variate normal distribution with mean   and
covariance matrix given in       
we set the threshold r  as a quantile for this distribution p mj   r          where  is our approximate
size  we use         the method computes this quantile by simulating the statistic mj under the null
hypothesis with the multivariate normal distribution given in       
the resulting choice of r  selects a feature set of        tokens         of the total  and achieves a final
test error of       

   conclusion and future work
though the feature set selected in       is very large compared to the size in        we believe the choice
of threshold used in       is more stable and robust  both choices are viable for this problem  the choice
in       emphasizes the effectiveness of putting strict limits on the number of tokens  variables  selected to
protect against overfitting  but more importantly  we hope the choice of threshold based on the clt in      
illustrates the value of probabilistic based methodshow probabilistic interpretations can inform and guide
our method choices 
one related idea that we have not explored is that the estimation of quantiles in fishers method gives
us the flexibility to consider types of classification other than one to one classification  for example  we
could classify documents into all categories k with quantile for tk less than       this method would
give classifications with a certain degree of confidence  and would leave others unclassified that we are
not confident about  it would also classify documents into multiple categories  which is applicable in many
situations  since sometimes  texts or articles can truly belong to more than one topic  also  in many archiving
tasks  it is better to overclassify to make sure each reference text will be located 

fiprobabilistic token selection via fishers method in text classification

references
    lang  ken             newsgroups data set  data files   retrieved from http   http   qwone com  jason   newsgroups 
    segaran  toby          programming collective intelligence  kindle dx version   retrieved from amazon com

 

fi