keystroke rhythm and intensity as biometrics for user id
lucas hansen and lindsay willmore
project checkpoint  cs   
december         

abstract keystroke dynamics is a biometric measure of an individuals typing pattern  with applications
in user authentication and password hardening  past work has explored identifying users based upon
detailed keystroke timing information  including the delay between and duration of each tap of a key 
here we explore an additional factor  keystroke intensity  without imposing the need for additional
infrastructure  e g  pressure sensitive keyboards   we use the laptops built in microphone to obtain an
audio recording of the users typing  using timing information  we extract the volume intensity at the
time of each keystroke  our final algorithm uses both keystroke latency and intensity as features to
distinguish between valid users and impostors  we are able to show that the additional intensity information greatly improves user classification by achieving final far and frr of below    and     respectively 

  introduction
we will focus on the recognition of a particular user
out of a group of imposters  based on keystroke
dynamics information gathered while the user or imposter is typing a fixed password  in order to model
this situation  we collected training examples of the
legitimate user typing in the password  as well as a
large number of examples of imposter password
attempts  given this data we built a model which is
able to reliably distinguish between login attempts
by the user and by an imposter  the rate at which
our model incorrectly identifies an imposter as the
legitimate user is defined as the false acceptance
rate  far   and the rate at which the valid user
is rejected is known as the false rejection rate
 frr  
our algorithm uses both the timing between keypresses and the physical intensity of each stroke 
this particular method of biometric identification is
attractive because of the low resource requirements
that it imposes  while more robust authentication
schemes exist  e g  retinal or fingerprint scanning  
they require significant infrastructure  biometric
identification based o of keystroke dynamics requires nothing that is not already built into a normal
personal computer  for this reason  over the past   
years there has been a lot of published work on the
use of keystroke dynamics for authentication and enhanced security  nearly all of this work has focused
on data that can be harvested from the keyboard 
however  we propose using an equally elegant and

convenient method to collect additional meaningful
biometric data  which will improve authentication
accuracy and robustness 
almost every modern laptop includes a built in
microphone  this microphone is fixed in place  easily accessible  and located very close to the laptops
keyboard  these properties make it ideal for measuring the intensity with which a key is pressed 
we extend on existing authentication techniques
based o keystroke dynamics by using the intensity
information harvested from the microphone 

  procedure
    data collection
we developed a simple matlab program to extract
data from a user typing a password of a set length 
the experiment is set up in a semi controlled environment  where the user sits alone in a quiet room
and proceeds to type the password andrew ng    
times into one of two laptops of the same make and
model  there is a small gap in time between each
password entry  the user is given a visual signal to
begin typing the password  but the typed password
characters do not appear on the screen  these precautions were taken to minimize distractions and
optimize consistency between typing trials  however 
we do recognize that severely restricting the environment of the user reduces the generalizability of
our conclusion 
we begin our audio recording just prior to begin 

 

fifigure      plots of raw latency and intensity data values for   users  displaying distinct keystroke patterns
of dierent users  top  user    bottom  user    
ning of the first keypress of each separate password
trial  then we record the latencies between keypresses  which indicate the times in the audio data
where peak amplitudes should be  the volume spike
closest to the time of the keypress is taken as a
measure of keystroke force 

   users  where each user had anywhere from    to
   successful trials  this data was composed of   
features     latencies and   intensities from each of
the   characters in the password 
some users intensities were recorded using an
earlier version of our data extraction protocol  resulting in absolute values approximately    times
lower than those of users sampled using the updated
extraction protocol  we therefore multiplied those
low intensities by a correction factor of    

figure      intensity extraction  audio recorded during duration of typing  red points indicate identified keystroke intensities

    preprocessing  normalization  and
visualization
the data that we collected required relatively little
processing  the primary preprocessing step is the
extraction of the keystroke intensity estimate from
the audio data collected by each trial 
after extracting the intensity information from
the audio data generated by each trial all of our
data is entirely digital  in the process of recording 
we also maintained a record of the characters typed 
after recording a users     trials  we removed trials with misspellings  i e  the charachter sequence
typed did not match andrew ng   this resulted in
a final data set of      total examples taken from

figure      heat maps of full data set before and
after normalization  consecutive trials
grouped by user 
figure     reveals the distinct typing patterns of
each user represented by a solid band of consecutive
password trials  of note are those features which
show little variation  such as feature     representing
the latency between d and r  and those features
which vary significantly between users  such as feature     representing the latency between e and
w 
to confirm our features ability to distinguish
between individual users we performed principal
component analysis  pca  and plotted the first

 

fifigure      pca  using subset or full set of keystroke features  colors representing distinct users
principal component against the second  for this
visualization only  we performed additional normalization on the data  ensuring that each feature had
zero mean and unit variance  this normalization
drastically enhanced the explanatory power of our
pca plots 
we performed this procedure using our full set
of features as well as subsets  just latencies  the
first   features  and just intensities  final   features  
these plots are displayed in figure      in each of
these plots  points of the same color come from the
same user 
the dierences between the pca visualization
resulting from the entire set of features and the
visualization resulting from the subsets of features
are striking  while visible clusters are evident in
the subset plots  the clusters are much more distinct in the plot of the combined set of features 
even before we had found a successful classification
model these visualizations provided support for the
hypothesis that the keystroke intensity data we collected provided information that was both a  useful
for distinguishing between users and b  not already
present in the latency data 

  classification by keystroke
patterns
we evaluated the performance of several schemes
that have been previously tested in keystroke dynamics publications  these models include euclidian
distance  non weighted probability  and weighted

probability measures  as well as support vector machines  svm  and multilayer perceptrons  mlp   
the success of each method seems to vary across
publications  depending on the specific nature of
each problem and approach  with minimal guidance on where to begin our analysis  we performed a
barrage of tests using modified versions of matlab
built in tools 
clustering by k means produced results with relatively high error rates  steering our approach away
from euclidian distance measures 
we then moved on probabilistic models and explored the use of a naive bayes classifier  we
obtained the most satisfying results with a manually
adjusted prior favoring the likelihood of seeing an
imposter as opposed to the actual user 
additional tests with svms  using a linear kernel 
were performed with little to no improvement over
the results given by naive bayes 
our most successful scheme used a mlp  preprocessing and training were done with the aid of the
matlab neural network pattern recognition tool 
preprocessing steps were limited to scaling the data
within the range of          the resulting model included one hidden layer of    neurons  it was trained
with scaled conjugate gradient backpropogation
using the mean squared error performance function 
we split our data into   pieces      training     
validating      testing 
most of the techniques we used are standard in
the use of mlps  but we did have to deal with some
peculiar convergence issues  fairly frequently while

 

authentication via keystroke dynamics  fabian monrose and aviel rubin        in proceedings of the  th acm conference
on computer and communications security  ccs      acm  new york  ny  usa        
 
user authentication through typing biometrics features  l c f  araujo  l h r  sucupira jr  m g  lizarraga  l l  ling 
j b t  yabu uti sch  of electr    comput  eng   state univ  of campinas  brazil ieee transactions on signal
processing 

 

fitraining our network pathological convergence behavior would occur  and our network would either a 
fail to converge or b  converge way too soon and perform terribly on the training data  we deal with a 
by capping the number of iterations that a network
may take while training to     iterations  we deal
with b  by performing a degeneracy test after the
network converges  after convergence we calculate
the far and frr error rates using the training and
validation data  and if these rates are too high then
we retrain the network  nowhere in this degeneracy
test is the testing data used  eventually  sometimes
after a large number of iterations  the networks performance meets our minimal criteria  and only then
do we consider the network properly trained  the
addition of this degeneracy test radically improves
the performance of our classification model 

  results
we evaluated our success based upon the minimization of the following error rates  with the specific
aim to minimize far below     
general error rate  overall frequency of misclassification
false acceptance rate  far   frequency of classifying a false user as the valid user  high far
implies that many illegitimate users could access
password protected imformation 
false rejection rate  frr   frequency of the legitimate user being identified as an imposter  high
frr implies that the valid user may need to repeatedly type the password before being verified 
latency only

general
far
frr
general
far
frr

naive
bayes
     
     
      

neural
network
     
     
      
best worst
         
         
         

latency and
intensity
naive
neural
bayes
network
     
     
     
     
             
best worst
         
         
          

in the worst case  we also found       worst case
frrs for naive bayes trained on latencies only as
well as on our full feature set  also of note is the
ability of our best model to identify left out users
 those not included in the training set  as imposters
with a similar error rate  data not shown  
ultimately  we found that the mlp described
in the previous section performed the best and offered the most flexibility in trading o between far
and frr  this flexibility is important in the context of security and authentication problems  a
low far corresponds to strong security and a low
frr corresponds to convenience  depending on
the context  convenience may be more important in
an authentication scheme than extremely stringent
security  in other cases the exact opposite could
be true  ideally both of these needs could be met
simultaneously  but unfortunately  in any classification scheme there is uncertainty at play  due to
noise in the data  changing behavior of the user  or
inadequacy in the model  so it may be impossible
to maintain both a sufficiently low far and frr 
but it is often possible to trade o between the
two  the two classifiers that we investigated the
most  naive bayes and mlp  oer simple ways to
make this trade o  for naive bayes we can simply
manually adjust the class prior for imposters  for
mlps we can change the threshold at which the
networks output is considered to classify the input
as coming from the legitimate user 
the neural network whose performance is depicted
in the table used a threshold of      where for each
password attempt  the attempt is classified as originating from the legitimate user if the network outputs a value at least     and from an imposter if the
networks output is less than      this particular
threshold oers error rates which are favorable in a
general security setting  but  as discussed earlier 
dierent thresholds may be superior in specific situations  the performance of dierent thresholds are
depicted in figure     

table      summary of final error rates
we trained and tested our model    times  each
time choosing a dierent user in our dataset to represent the valid user while the other    were seen
figure      far and frr plotted against classificaas imposters  overall error rates were calculated as
tion threshold
the average over all    users  best and worst error
rates for individual users are shown for our neural
this plot was generated by varying the mlps
network results  of note is the       failure of a threshold from   to   and calculating the far
latency only model to confirm the identity of a user and frr at each level  the point at which far

 

fiand frr are equal defines the equal error rate
 eer  which we found to be approximately     
at a threshold of      this data is consistent with
previously published work on the use of keystroke
latency and intensity  as measured through pressure
sensitive keyboards  for user authenticaiton  which
has reported eers between    and        

  conclusion
our investigation has shown that intensity is highly
informative as an additional feature in user authentication through keystroke biometrics  both the
seperation between users obtained in pca analysis
as well as errors rates in valid user detection were
vastly improved by the addition of intensity to the
traditionally used latency measures  previous work
has corroborated the efficacy of pressure through
dierent methods  these studies utilized pressure
sensitive keyboards and extracted features such as
total harmonic distortion  kurtosis  and energy   audio signals also carry these characteristics  however 
within an audio signal they are often obfuscated
by extraneous noise from the users environment 
peak extraction corresponding to the keystroke intensity presents itself as a simple method to avoid
the noise problem and preserve valuable user specific
information 
this paper introduces the use of a laptops builtin microphone in keystroke intensity estimation  for
the purposes of user authentication  further investigation into this idea should prove the robustness

of the model in several areas  first  data should be
collected while the user is typing in environments
with varying noise levels  it should be proved that
signal isolation or noise reduction methods can be
employed to ensure that the extracted peak volumes
correspond to the sound of the keyboard  second 
users should be recorded across a number of different sessions to confirm that typing patterns are
retained by a single user across time   third  further
exploration of password length and content should
be conducted  we have seen that certain character
combinations vary more significantly between users
and that classification accuracy improves with the
length of the password  abbreviated data only from
andrew produced much higher error rates than the
full password  
to conclude  biometric user authentication is of
concern in a variety of areas from password hardening to confirming student id for the distribution of
verified certificates of completion from online course
providers   our results oer a method of improving
the quality and ease of using keystroke dynamics for
these purposes 
in other words  intensity is key    

  acknowledgments
we would like to thank the    users who gave their
time and intensity for the advancement of this important area of research 
one more thing   
we love you andrew 

 

pressure based typing biometrics user authentication using the fuzzy artmap neural network c  c  loy  c  p  lim 
and w  k  lai international conference on neural information processing  taiwan      
 
keystroke patterns classification using the artmap fd neural network c  c  loy  w  k  lai  and c  p  lim international
conference on intelligent information hiding and multimedia signal processing  taiwan      
 
http   edf stanford edu readings coursera announces details selling certificates and verifying identities

 

fi