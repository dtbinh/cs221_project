auto associative memory  the first step in
solving cocktail party problem
introduction 
one of the most interesting and challenging problems in the area of artificial intelligence is solving the
cocktail party problem  this is the task of attending to one speaker among several competing speakers and being
able to switch the attention from one speaker to another at any given time  human brain is remarkably efficient in
solving this problem  there have been numerous attempts to emulating this ability in machines  independent
component analysis  ica  and blind source separation  bss  are two of the most popular solutions to this problem
but they both fail to generate similar results as human brain does  they are also very computationally expensive
which makes them incapable of producing results in real time  moreover  they generally require at least two
microphones to converge but as we know human brain can also work with one ear covered  this is evident from
the fact that covering one ear does not make attending to one speaker in a cocktail party any harder 
i believe the reason that these methods fail to generate similar results is that they fail to capture the
fundamental functionality of separation of sound sources in the brain  ica and bbs both assume no prior
knowledge about the sounds that they are receiving  this is not generally true about human brain  in the course of
maturation of auditory system  brain is exposed and attuned to many sounds and learns when two or more tones
happen together and when they are not  so when a new sound signal is presented to the brain it automatically
groups the tones in the sound based on the likelihood that they are happening together 
i propose a new solution  one based on separation of sound sources through learning from past experience 
in order to accomplish this  one needs to design a system that can regenerate sounds that it has become
accustomed to when it is exposed to a noisy or distorted version of them  this task is best accomplished by an
auto associative memory 

auto associative memory 
auto associative memories are content based memories which can recall a stored sequence when they are
presented with a fragment or a noisy version of it  they are very effective in de noising the input or removing
interference from the input which makes them a promising first step in solving the cocktail party problem  the
simplest version of auto associative memory is linear associator which is a   layer feed forward fully connected
neural network where the output is constructed in a single feed forward computation  the figure below illustrates
its basic connectivity 

all inputs are connected to all outputs via the connection weight matrix where
denotes the strength of
unidirectional connection from the
input to the
output  since
in auto associative memories  we have
  therefore  all stored sequences must be eigenvectors of matrix   assuming all stored sequences are
orthogonal to each other  we can represent the weight matrix as
where is orthonormal

fimatrix of all stored sequences  also  we are enforcing the weight matrix to be symmetric so
assuming we have sequences to store in our memory we can rewrite weight matrix as 

  so 


initial testing 
to verify functionality of the linear associator  i started by training the auto associative network using
segments of sleep away song which is in sample music folder of all pcs  since the default sampling rate of
      samples second is too much to process the data efficiently  i compressed the data to       samples second
and partitioned it to segments of      samples per segment  which corresponds to one tenth of a second of the
original song  noting that we can have perfect reconstruction if we store sequences that are orthogonal to each
other  i used inner product as a measure of how orthogonal segments are with respect to each other  i calculated a
score of orthogonality for each segment according to the sum of their inner products with all other segments 


 

 

the weight matrix is then calculated using     segments with the lowest score  according to the formula
above  then  i ran several tests in order to assess reconstruction of the stored patterns  in the first test  i tried to
find out how the original signals compare with the reconstructed signals  the plots below show the reconstructed
signal versus the original signal for two stored segments with the best and worst o scores 

in order to get a better sense of the performance of the linear associator  i ran another test where i
consecutively increased the number of stored segments from   to     and measured the reconstruction error  in
one setting  i applied the original signals without any noise and measured the average error made per
reconstruction and in another setting  i applied noise with the same power as the original signals  i e   db snr   to
the original signals and again measured the average error made per reconstruction  since the original signals and
the reconstructed signals are forced to have the same length  the reconstruction error can also be thought of as
the average angle by which the reconstructed signals deviate from the original signals  the plots below show the
results for the average reconstruction error and average angle of deviation from the original signal as a function of
number of stored patterns 

fias it can be seen from the plots  the reconstruction error rises very rapidly as the number of stored patterns
reaches    but pretty much flattens out afterwards  this is expected because there is a high correlation between
segments of a song as there are same instruments being played during all segments  therefore  there are very few
segments that are orthogonal to others which makes the reconstruction imperfect  the flattening out of
reconstruction error for high number of stored patterns can also be explained by high correlation between the
segments because if a pattern to be stored can be expressed by previously stored patterns the weight matrix wont
change after learning the new pattern thus average reconstruction error remains the same 

autoencoder as autoassociative memory 
initial testing of linear associator indicates that it does quite a poor job storing patterns that are linearly
dependent  in order to better distinguish between linearly dependent patterns  nonlinearity of some sort has to be
included in the system  since the input domain is         hyperbolic tangent was chosen as the activation function
for each neuron to incorporate nonlinearity into the system 
next step was to choose the topology of neural network  feed forward networks and networks with feedback
like hopfield networks were considered for implementation of autoassociative memory but feed forward networks
were chosen because of their relative simplicity and feasibility to train  after a successful implementation of a
feed forward autoassociative memory  impact of feedback on noise reduction was also studied 
among feed forward networks  autoencoder was finally chosen for several reasons  first  the simplicity of
these networks makes them easy to work with  also  the smaller number of hidden neurons makes training these
networks much faster and at the same time enables them to store larger size patterns  perhaps the most
important reason for choosing autoencoder is based on the observation that by limiting the number of neurons in
the hidden layer  each pattern will get compressed in the hidden layer to be reconstructed again at the output so
any noise in the input should get averaged out and rejected at the hidden layer  to assess this theory  noise
performances of two autoencoders with different number of hidden neurons were tested  the results are
discussed in the following section  the downside of choosing autoencoder  however  is the lower number of
patterns that can be stored because of lower number of neurons in the hidden layer 

fithe procedure for training and testing the network was similar to the one for linear associator with an
exception on the number of samples per segment  to make training time reasonable  number of samples per
segment was reduced to      then  the reconstruction error was measured as a function of number of stored
patterns for two different autoencoders  one with    hidden neurons and another with     results were compared
with reconstruction error of a linear associator also trained with     samples per segment to make the comparison
fair  the plot below shows the results 

as it can be seen from the plot  incorporating nonlinearity into memory can significantly reduce
reconstruction error  moreover  the higher the number of neurons in the hidden layer the better the network can
fit more patterns and therefore the lower the reconstruction error will be  as it is evident from the plots  the
maximum number of stored patterns in an autoencoder that can be perfectly reconstructed is as high as the
number of hidden neurons  as the number of stored patterns increases beyond that  the average reconstruction
error rises gradually and then flattens out  this makes autoencoders also a pretty decent candidate for data
compression for applications that can tolerate a small loss of information 

noise performance 
in order for an autoassociative memory to work well in a cocktail party problem application  it has to be able
to reject most of the noise and interference  so  in final evaluation of autoencoder autoassociative memory  i
tested noise performance of autoencoders with different number of hidden neurons  to get the best noise
performance  i trained each network with four noisy versions of each patterns while forcing the output to be the
ideal noiseless pattern  to reduce over fitting while maintaining a low reconstruction error  i enabled
regularization with a small regularization coefficient              then  i applied noisy inputs with snr   db to
each network and measured average increase in reconstruction error  the plot below shows the results for a linear
associator  an autoencoder with    hidden neurons and an autoencoder with    hidden neurons 

fia surprising result from this experiment is how much better the noise performance of a linear associator gets
when the size of each segment is reduced from      to      however  since the reconstruction error remains high
it is still not a suitable topology for implementing an autoassociative memory  another interesting result is the
superiority of autoencoder with    hidden neurons in noise performance compared with the one with    hidden
neurons  this is in line with the hypothesis that noise gets averaged out at the hidden layer and thus  the lower the
number of neurons in the hidden layer the higher the averaging and the higher the rejection of noise  since noise
power at the output of this network is less than the noise power at the input  we can get an even better noise
performance if we place a feedback loop around the system and recursively apply the output to the input  the plot
below shows log of reconstruction error of the two autoencoders with noisy inputs as a function of number of
recursive iterations 

conclusion 
this project illustrates that autoencoders can be used to store linearly dependent patterns with negligible
reconstruction error  results  however  suggest that there is a trade off between minimizing the reconstruction
error and maximizing noise rejection  while networks with higher number of hidden neurons can store more
patterns and with less reconstruction error  they will have lower noise rejection and vice versa  moreover  if an
autoencoder with an optimum number of hidden neurons  is placed in a feedback loop  saved patterns will act as
attractors  so  with each recursive iteration  more noise is rejected until a noiseless pattern is reconstructed at the
output  this makes autoencoders very attractive candidates for implementation of an autoassociative memory in
solving the cocktail party problem  further research needs to be completed to find the optimum number of hidden
neurons for a given input size  minimum noise rejection or maximum reconstruction error 

references 
http   ufldl stanford edu wiki index php ufldl tutorial

fi