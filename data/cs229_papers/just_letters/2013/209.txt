identify  a  vehicle  and  its  driver  from  the  vehicles  maneuver  data  
mehrdad salehi and ding zhao

abstract
in this project  we applied supervised learning algorithms to detect drivers based on their driving behavior  using
labeled driving data  time  speed  acceleration  heading and gas usage   we created a set of features such as maximum
of speed  standard deviation of acceleration as well as additional complex features like variation of heading and
variation of mpg over time  in addition  we applied pca to convert a set of possibly correlated features into a set of
values of linearly uncorrelated variables and optimize our algorithm performance  after applying the pca  the
accuracy results were improved from       to       and the running time was also decreased  using svm model  
we applied svm  random forest  decision tree  nave bayes  and nearest neighbor algorithms to the data and
compared the accuracy and running time of the results for different features sets  adding complex features greatly
improved the classification results for all algorithms  among the above mentioned algorithms  svm optimized for its
parameters based on a grid search provided the best accuracy of      however  the running time of svm was the
longest    
  
acceleration and gas mpg  the data for these sensors
  
is collected every second  the data is collected from   
  introduction
cars with duration that vary from one week to several
lots of identification methods have been developed
months  the datasets are labeled  i e   we know the
and widely used in security agencies and business
vehicle for which each data file belongs to  the
companies  most of existing methods are based on
program is supposed to learn enough information from
biomedical identification  such as fingerprint  face and
the data and when given a piece of unknown driving
retina  however  behavior signatures can be more
data  it is able to output the ownership of the data 
valuable because they contain much more information
about a person  we hypothesize that an individual s
  model
driving behavior is a unique signature that can be used
in identification  huihuan et al          we collect the
we treat driver identification as a classification
driving data from sensors that measure the
problem  first  we process the raw data to let it be
instantaneous speed  heading  acceleration and gas
more appropriate for feature engineering  then  we
mpg  the objective of this project is to create a
extract features from the processed data  and at the end
machine learning algorithm to identify a car and its
we apply supervised learning  below  we will describe
driver from detailed driving data  below we describe
the model for the problem 
how we model this problem and the algorithms we use
to tackle it 
    data processing

  task definition
the task is to build a system that can identify a driver
from detailed driving data  here we describe the
mechanics  training and test process of the program 

    problem mechanics
we hypothesize that everyone has her own unique
driving habit  we may be able to extract enough
information from her driving data to create a unique
signature that can be used as an identification method 
more specifically  we want to build a program that is
able to learn an individuals driving signature and
correctly find the ownership of a given driving data 

    inputs   outputs
the input for the program is the driving data from
sensors that measure the instantaneous speed  heading 

      trip extraction
the raw data includes instantaneous speed  heading 
acceleration along the three dimensions x  y  and z and
gas mpg that are measured every second for each
vehicle  if we treat data as it is provided  i e   consider
data only for    vehicles   we have so few data and any
model we create will have a high chance of overfitting  in order to avoid such a problem  we create
trips for each vehicle  a trip is defined as a set of
vehicle data that starts from one zero speed and ends to
the next zero speed  in some cases  the vehicle is
stationary in one location for several seconds  in order
to avoid adding noise to trips in such cases  we
removed multiple stationary points from the input data 
using this pre processing approach  we avoid
overfitting by creating tens of thousands of trips  from
the raw data  we were able to generate around       
trips 

fi      bad reads recovery
occasionally sensors can generate invalid reads  and
these reads are discarded in the raw data set  in order to
feed the program with consistent and continuous data 
we need to recover these bad reads  speed  acceleration 
and heading are all continuous properties  and
therefore we can use neighboring reads to recover
these bad reads  suppose that the last valid read is at
time        and the next valid read is at time       
since sensors generate read every second  if
                 we know there are some bad
reads in between  let            be the bad reads
between      and        also let      and      be the
last valid read and the next valid read  we approximate
the change of read as linear and recover bad reads
using the following formula 

                  
       

    feature engineering
once the trips are created  we can attribute features to
them  feature engineering is a critical part of this
project as the more relevant features are selected the
better the algorithm learns and hence will provide more
accurate classification results 
      feature extraction
speed
for each trip  we use the max  the mean  and standard
deviation of speed as our features because they imply
the type of road  city or highway  the driver usually
drives in  also  they can reflect whether the driver
tends to overspeed 
acceleration
acceleration read depends on how the accelerometers
are mounted on a car  since the accelerometers are
mounted differently in different cars  i e   the x
direction is not consistently defined for all the cars  we
will have totally different acceleration vectors for
different cars  if we try to align the axis of acceleration
vector with respect to the car  forward  side  and
upward   we will lose a large amount of precision 
because acceleration is the first derivative of speed
with respect to time  we use the difference of
consecutive speed reads over time as the acceleration 
we use the average and standard deviation of the
acceleration as our features 
heading
the heading of a vehicle is the direction where it is
traveling  the heading data is in the range of   degree
to     degree  with   degree denoting the north  since
the absolute value of the heading just mean the
direction the driver is facing  it makes no sense for us

to use mean or maximum values of heading  however 
some drivers may tend to change the cars heading left
and right while going in a straight line  therefore  we
use standard deviation of heading as an estimate 
miles per gallon
miles per gallon is a metric to evaluate the energy
efficiency of a vehicle  we could not use the absolute
value of mpg because absolute value of mpg depends
only on the make of the car  however  we could take
the standard deviation of the mpg to see how drivers
behavior change fuel efficiency   
complex features
in addition to applying pure statistical methods to our
data  we extracted some additional features and were
able to achieve much better results  we call these
features that are not directly measured as complex
features  below is the list of our complex features 
   more from heading changes  how fast a driver
change cars heading indicates how wide he makes
turns  we need to use the first derivative of heading to
represent such feature  however  one thing to note that
the heading changes over time actually is related to the
acceleration of the car  as the centripetal acceleration
has a component in the forward direction  to remove
such relationship  we extract the sideward component
of the heading changes using speed and heading
changes  after applying the knowledge of centripetal
acceleration and newtons law  we found that
        
   more mpg measurement  we found that only taking
the standard deviation of mpg does not fully represent
the fuel efficiency changes  we further make use of the
mpg changes by taking the derivative of mpg values 
      scaling
we scale each column of the  feature matrix x to
have zero mean and unit variance by the formula 
                      
    
                  
where
 
 
                  
    

                
 

 
   

   

 

                      
  


fi      principal component analysis and feature
space reduction
principal component analysis  pca  is a statistical
procedure that utilizes orthogonal transformation to
convert a set of observations of possibly correlated
variables into a set of values of linearly uncorrelated
variables  principal components   pca was leveraged
to reduce the dimension of features  automatically
detect and eliminate noise in the feature set  minimize
the complexity of the hypothesis class considered 
avoid overfitting  and optimize our algorithms
performance  in conjunction with pca  we applied a
whitening transformation to ensure that our feature
vector contained unit component wise variances 

    supervised learning
after generating features  the next step is to apply a
supervised learning algorithm  for this project  we
tried the algorithms below and compared their
accuracies and run time 
      support vector machine
a support vector machine  svm  constructs a hyperplane or set of hyper planes in a high or infinite
dimensional space to maximize the distance to the
nearest training data points of any class  since an svm
is only capable of binary classification  the oneagainst one approach will be applied toward multiclass classification 
as we have five classes 
   
 
       classifiers were constructed and each
 
trained data from two classes  each svm is assigned a
weight of    and predictions are made based on the
largest number of votes the svms collectively assign
to a particular class 
grid search and k fold cross validation
the kernel type k  the error term c and kernel
coefficient    were three important svm input
parameters requiring adjustment  we selected a
gaussian kernel as our kernel function  since it
constructs a hyper plane in an infinite dimensional
space and is reputed to be among the most powerful
kernel functions available  c balances between the dual
goals of minimizing      and of ensuring that most
examples possess a functional margin of at least     
larger values of  correspond to worse balanced
classification  which suggests that the deflective
classifier model is causing lower accuracy on one side
of the classification destination and higher accuracy on
the opposite side  we applied a two step grid search
method that tests every combination of possible c and

values
 
                             
   
  
 
                        to find the optimal
combination  after conducting a search with a coarse

grid to locate a better region  the method constructs a
finer grid within that region 
to evaluate the accuracy of each combination  we used
a   fold cross validation on the data  we found that
                         gives us optimal solution 
      decision tree and some other machine
learning methods
in addition to svm  we also apply other machine
learning methods to solve our problem 
 decision tree  a non parametric supervised
learning method that constructs a decision tree
that maps features vectors to labels 
 random forests  fits a number of decision
tree classifiers on various sub samples of the
dataset and use averaging to improve the
predictive accuracy and control over fitting
 nave bayes  a simple probabilistic classifier
based on applying bayes  theorem with strong
 naive  independence assumptions 
 nearest neighbor  a classification model that
assigns to observations the label of the class of
training samples whose mean  centroid  is
closest to the observation 

  test results and analysis
    summary
we tested our models using the driving data from  
drivers  therefore  it is a   class classification
problem  i e  the model predicts driver s identity given
any piece of driving data from   drivers  in order to
validate and improve the results  we did a   fold cross
validation on the      data points       for each
driver   we measured the accuracy of our classifier as
number of correct classifications divided by total
number of test data  we achieved       accuracy
using parameter optimized svm with   class
classification  in next sections  we will include an
analysis of the effects of each method we used and also
a comparison of different machine learning algorithms 

    the effect of scaling features
enormous improvement was achieved by scaling
features  from       accuracy in      seconds to
      accuracy in      seconds if we use svm with
default parameters  this was due to the fact that
scaling disallows attributes in greater numeric ranges
 i e  maximum speed  from dominating values within a
smaller numeric range  i e  standard deviation of
acceleration   in fact  the introduction of scaling
resulted in accelerated calculations and the avoidance
of
computational
difficulties 
such
as
overflow underflow caused by the kernel values
dependence on the inner products of feature vectors 

fimpg  complicated acceleration  the cross validation
accuracy of our svm model increases  this result is
illustrated in the figures below 

principal component analysis was one technique
applied to reduce the feature space  to identify the
optimal number of features to incorporate  we
evaluated our models performance while varying the
number of components retained  our results appear
below 

        
       
       

   

   

   

   

   

number  of  features  included  

figure   cross validation accuracy vs  number of features

       
   

   

    

    

    

running time vs number of features

    

number  of  components  

figure   cross validaction accuracy vs number of
components

    

running time vs number of components
running  time   s   

       
       
       
       
       
       
       
       

       

running  time   s   

cross validacation accuracy

cross validaction accuracy vs number of
components

cross validation accuracy vs  number of
features
cross  validation  accuracy  

    principal component analysis and feature
space reduction

     
     
     
     
     
   

   
   
   
   
   
   
   

   

   

   

   

number  of  features  included  

figure   running time vs number of features

   

   

    

    

    

    

number  of  components  

figure   running time vs number of components

initially  as the number of components increases 
predictive accuracy dramatically improves  from      
to        this result can be explained by the increased
preservation of useful features  however  when the
number of features retained exceeds    noise is
subsequently introduced and we begin incorporating
irrelevant features or those that overlap with existing
features  therefore  we think that   is the best number
of components to keep with pca transformation 
compared to the accuracy of       without pca  we
get an accuracy of       with pca optimization using
  components 

    the effect of individual features
as we gradually include the features of speed 
heading  mpg  acceleration  time  complicated

from the graph  we can see that every feature we
introduce is useful for our learning algorithm  the two
last features from complex features give us the greatest
improvement  the reason is that they capture very
realistic driver behaviors  it is a little surprising that as
we add the last two features  the running time also
decreases  this is mainly due to faster convergence in
svm algorithms when we introduce important features 

    comparison of different machine learning
algorithms
we measured the accuracy and performance of
different machine learning algorithms  and the results
are shown below 

ficross  validaction  acuracy  

cross validaction accuracy of different
machine learning methods
        
       
       
       
       
       
       

optimized
svm

svm

random decision tree nave bayes
forest

nearest
neighbor

machine  learning  methods  

figure   cross validation accuracy of different machine
learning algorithms

running time  s 

running time of different machine
learning methods
   
   
   
   
   
   

optimized
svm

svm

random
forest

decision nave bayes nearest
tree
neighbor

machine learning algorithm
figure   running time of different machine learning
algorithms

we can see that there is actually a trade off between
performance and accuracy  nearest neighbor and
nave bayes are very simple algorithms  they run
really fast and almost finish instantaneously  however 
the accuracies they produce are not so good  these
methods are very useful if we want to get a sense of
how our model works and get results very quickly 
svm and random forest  on the other hand  give us
very high accuracy but take a lot of time  they are
more suitable if we have big computational power and
memory space 

  conclusions
in this project  we proved that an individual s driving
behavior is a unique signature that can be used in
identification  we collected the driving data from
sensors that measure the instantaneous speed  heading 
acceleration and gas mpg  we successfully created a
machine learning algorithm to identify a car and its
driver from detailed driving data    
  

we first processed the data to make it appropriate for
supervised learning  we avoided overfitting by
creating trips from each vehicle data  a trip is defined
as a set of vehicle data that starts from one zero speed
and ends to the next zero speed  also  we removed
multiple stationary points from the input data to avoid
adding noise to the data  the second step was feature
engineering where we created different features for the
learning algorithms  since the acceleration data was
collected in an inconsistent and noisy way  instead of
using the collected acceleration  we derived it from
speed and time  also  we extracted a number of
complex features such as the variation of heading over
time  variation of mpg over time  and ratio of mpg to
speed   
  
in addition  we applied pca to convert a set of
possibly correlated features into a set of values of
linearly uncorrelated variables and optimize our
algorithm performance  after applying the pca  the
accuracy results were improved from       to      
and running time was decreased   
  
at last we applied supervised learning algorithms to
detect drivers based on the driving behavior  we
applied svm  random forest  decision tree  nave
bayes  and nearest neighbor algorithms to the data
and compared the accuracy and running time of the
results for different features sets  adding complex
features improved the classification results for all
algorithms dramatically  among the above mentioned
algorithms  svm optimized for its parameters based on
a grid search provided the best accuracy of     for  class classification  however  the svm running time
was the longest       seconds compared to     
seconds using nearest neighbor     

references
q  huihuan  o yongsheng  w  xinyu  m  xiaoning 
and x  yangsheng         support vector machine for
behavior based driver identification system  journal
of robotics  volume      
h  chih wei  c  chih chung  and l  chih jen        
practical guide to support vector classification   
national taiwan university  taipei      taiwan 
  

fi