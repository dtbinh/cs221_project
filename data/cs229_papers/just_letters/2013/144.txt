sms spam detection
using machine learning approach
houshmand shirani mehr  hshirani stanford edu

abstractover recent years  as the popularity of mobile phone
devices has increased  short message service  sms  has grown
into a multi billion dollars industry  at the same time  reduction
in the cost of messaging services has resulted in growth in
unsolicited commercial advertisements  spams  being sent to
mobile phones  in parts of asia  up to     of text messages
were spam in       lack of real databases for sms spams 
short length of messages and limited features  and their informal
language are the factors that may cause the established email
filtering algorithms to underperform in their classification  in
this project  a database of real sms spams from uci machine
learning repository is used  and after preprocessing and feature
extraction  different machine learning techniques are applied to
the database  finally  the results are compared and the best
algorithm for spam filtering for text messaging is introduced 
final simulation results using    fold cross validation shows the
best classifier in this work reduces the overall error rate of best
model in original paper citing this dataset by more than half 

i  introduction
the mobile phone market has experienced a substantial
growth over recent years  in second quarter of       a total
of       million mobile phones have shipped  which shows a
     year over year increase      as the utilization of mobile
phone devices has become commonplace  short message service  sms  has grown into a multi billion dollars commercial
industry      sms is a text communication platform that allows
mobile phone users to exchange short text messages  usually
less than     seven bit characters   it is the most widely used
data application with an estimated     billion active users 
or about     of all mobile phone subscribers at the end of
          as the popularity of the platform has increased  we
have seen a surge in the number of unsolicited commercial
advertisements sent to mobile phones using text messaging 
sms spam is still not as common as email spam  where in
     around     of emails was spam  and in north america
it is still not a major problem  contributing to less than    of
text messages exchanged as of december           however 
due to increased popularity in young demographics and the
decrease in text messaging charges over the years  in china
it now costs less than        to send a text message   sms
spam is showing growth  and in      in parts of asia up to
    of text messages was spam  in middle east  some of the
carriers themselves are responsible for sending out marketing
text messages  additionally  sms spam is particularly more
irritating than email spams  since in some countries they
contribute to a cost for the receiver as well  these factors
along with limited availability of mobile phone spam filtering
software makes spam detection for text messages an interesting
problem to look into 

a number of major differences exist between spam filtering
in text messages and emails  unlike emails  which have a
variety of large datasets available  real databases for sms
spams are very limited  additionally  due to the small length of
text messages  the number of features that can be used for their
classification is far smaller than the corresponding number
in emails  here  no header exists as well  additionally  text
messages are full of abbreviations and have much less formal
language that what one would expect from emails  all of these
factors may result in serious degradation in performance of
major email spam filtering algorithms applied to short text
messages 
in this project  the goal is to apply different machine learning algorithms to sms spam classification problem  compare
their performance to gain insight and further explore the
problem  and design an application based on one of these
algorithms that can filter sms spams with high accuracy  we
use a database of      text messages from uci machine
learning repository gathered in               it contains a
collection of     sms spam messages manually extracted
from the grumbletext web site  a uk forum in which cell
phone users make public claims about sms spam   a subset
of       sms randomly chosen non spam  ham  messages of
the nus sms corpus  nsc   a list of     sms non spam
messages collected from caroline tags phd thesis  and the
sms spam corpus v     big         sms non spam and    
spam messages publicly available   the dataset is a large text
file  in which each line starts with the label of the message 
followed by the text message string  after preprocessing of the
data and extraction of features  machine learning techniques
such as naive bayes  svm  and other methods are applied
to the samples  and their performances are compared  finally 
the performance of best classifier from the project is compared
against the performance of classifiers applied in the original
paper citing this dataset      feature extraction and initial
analysis of data is done in matlab  then applying different
machine learning algorithms is done in python using scikitlearn library 
the project report is organized as follows  section   explains the preprocessing of the data and extraction of features
from the main dataset  and explores the result of initial analysis
to gain insight  section   explores the application of naive
bayes algorithm to the problem  in section    application
of support vector machine algorithm to the classification
problem is studied  section   shows the performance of knearest neighbor classifier for the data  section   explores
application of two ensemble methods  random forests and

filabel percentage in dataset
spams
     
hams
     

   

  

table i
the distribution of spams and non spams in the dataset

  

accuracy

adaboost  finally  section   concludes the report 
ii  feature extraction   initial analysis
as mentioned earlier  our dataset consists of one large
text file in which each line corresponds to a text message 
therefore  preprocessing of the data  extraction of features 
and tokenization of each message is required  after the feature
extraction  an initial analysis on the data is done using naive
bayes  nb  algorithm with multinomial event model and
laplace smoothing  and based on the results  next steps are
determined 
for the initial analysis of the data  each message in dataset is
split into tokens of alphabetic characters  any space  comma 
dot  or any special characters are removed from feature space
for now  and alphabetic strings are stored as a token as
long as they do not have any non alphabetic characters in
between  the effect of abbreviations and misspellings in the
messages are ignored  and no word stemming algorithm is
used  additionally  three more tokens are generated based on
the number of dollar signs      the number of numeric strings 
and the overall number of characters in the message  the
intuition behind entering the length of message as a feature
is that the cost of sending a text message is the same as
long as it is contained below     characters  so marketers
would prefer to use most of the space available to them as
long as it doesnt exceed the limit  for the initial analysis of
data  we have used the multinomial event model with laplace
smoothing  extracting tokens for all messages in the dataset
will result in       features  however  not all of these features
are useful in the classification  going through the extracted
tokens  we removed the ones with less than five and more
than     times frequency in the dataset  since those tokens
are either too rare or too common  and do not contribute to
the content of the messages  these two thresholds are set by
exploring different values and checking the performance of
nb classification algorithm on results  finally  the remaining
tokens result in       features 
figure   shows the result of applying nb algorithm to the
dataset using extracted features with different training set sizes 
the performance in learning curve is evaluated by splitting the
dataset into     training set and     test set  as shown in the
figure  the nb algorithm shows good overall accuracy  the   fold cross validation for this algorithm on current data shows
     overall error      of spams caught  sc   and       of
blocked hams  bh  
sc  

false negative cases
number of spams

   

bh  

false positive cases
number of hams

   

  

  
test set classification
training set classification
test set ham classification
training set ham classification
test set spam classification
training set spam classification

  

  

  

 

   

    

    
    
    
training set size

    

    

    

fig     learning curve for naive bayes algorithm applied to the dataset and
evaluated using cross validation      of initial dataset is our test set

from the analysis of results  we notice that the length of
the text message  number of characters used  is a very good
feature for the classification of spams  sorting features based
on their mutual information  mi  criteria shows that this feature has the highest mi with target labels  additionally  going
through the misclassified samples  we notice that text messages
with length below a certain threshold are usually hams  yet
because of the tokens corresponding to the alphabetic words
or numeric strings in the message they might be classified as
spams 
by looking at the learning curve  we see that once the nb
is trained on features extracted  the training set error and test
set error are close to each other  therefore  we do not have
a problem of high variance  and gathering more data may
not result in much improvement in the performance of the
learning algorithm  as the result  we should try reducing bias
to improve this classifier  this means adding more meaningful
features to the list of tokens can decrease the error rate  and
is the option that is explored next 
we update the features list by adding five different flags
to gathered tokens  these flags determine if the length of
the message in characters is                     
and       additionally  we add the string of non alphabetic
characters and symbols excluding dot  comma  question mark 
and exclamation mark to our tokens  for instance  a string
of characters such as     can imply the presence of a web
address  or a character such as   can imply the presence
of an email address in the message  the resulted features are
again filtered if they are too rare or too common in the dataset 
finally  we end up with a list of      features 
iii  naive bayes
in this section  nb algorithm is applied to the final extracted
features  the speed and simplicity along with high accuracy
of this algorithm makes it a desirable classifier for spam
detection problems  in the context of naive bayes algorithm

fikernel
overall
spams
blocked
function
error   caught  sc    hams  bh   
linear
    
    
    
degree   polynomial
    
    
    
degree   polynomial
    
    
    
degree   polynomial
    
    
    
radial basis function
    
    
    
sigmoid
    
 
 

   

  

accuracy

  

  

  

test set classification
training set classification
test set ham classification
training set ham classification
test set spam classification
training set spam classification

  

  

  

table ii
   fold cross validation error of svm with different kernel functions on
dataset

 

   

    

    
    
    
training set size

    

    

   
  

    
  
test set classification
training set classification
test set ham classification
training set ham classification
test set spam classification
training set spam classification

fig     learning curve for multinomial nb algorithm applied to final features

with multinomial event model  entering the feature of length of
the message corresponds to assuming an independent bernouli
variable for writing each character in the text message in spam
or ham messages 
applying naive bayes with multinomial event model and
laplace smoothing to the dataset and using    fold cross
validation results in       overall error        of sc  and
      of bh  using the data priors and applying bayesian
naive bayes with same event model will decrease sc        
and bh         by a small margin  but overall error will stay
the same  this is what we would expect  since bayesian model
improves the algorithm in case of high variance  figure  
shows the learning curve for mutinomial nb applied on the
final features extracted from dataset  the errors for different
datasets in this plot are produced using cross validation with
    of the samples as the training set  as it is shown in the
figure  the test set error and training set error are close to each
other and in the acceptable range  and it implies no overfitting
in the model  to reduce the bias and improve the accuracy of
algorithm  we can explore other more sophisticated models in
following sections 
iv  support vector machines
in this section  support vector machine is applied to the
dataset 
table ii shows the    fold cross validation results of svm
with different kernels applied to the dataset with extracted
features  as it is shown in the table  linear kernel gains
better performance compared to other mappings  using the
polynomial kernel and increasing the degree of the polynomial
from two to three shows improvement in error rates  however
the error rate does not improve when the degree is increased
further  radial basis function  rbf  is another kernel applied
here to the dataset  rbf kernel on two samples x  and x  is
expressed by following equation 
k x    x      exp 

kx   x  k  
 
  

   

accuracy

  
  
  
  
  
  

fig    

 

   

    

    
    
    
training set size

    

    

    

learning curve for svm algorithm applied to final features

finally  applying the sigmoid kernel results in all messages
being classified as hams 
the learning curve for svm with linear kernel validated
using cross validation is shown in figure    from this figure 
there is a meaningful distance between accuracy of trained
model on training set and test set  while the overall training
set error of the model is far less than error rate for naive bayes 
the test set error is well above that rate  this characteristic
shows the model might be suffering from high variance or
overfitting on the data  one option we can explore in this case
is reducing the number of features  however  the simulation
results show degradation in performance after this reduction 
for instance  choosing     best features based on mi with the
labels and training svm with linear kernel on the result yields
to       overall error        sc  and       bh 
while applying svm with different kernels increases the
complexity of the model and subsequently the running time
of training the model on data  the results show no benefit
compared to the multinomial naive bayes algorithm in terms
of accuracy 
v  k nearest neighbor
k nearest neighbor can be applied to the classification problems as a simple instance based learning algorithm  in this
method  the label for a test sample is predicted based on the
majority vote of its k nearest neighbors 

fioverall
spams
blocked
k error   caught  sc    hams  bh   
 
    
    
    
  
    
    
    
  
    
    
    
  
   
    
    
   
    
    
    
table iii
   fold cross validation error of k nearest neighbor classifier

table iii shows the    fold cross validation results of knearest neighbor classifier applied to the dataset 
vi  ensemble methods
in this section  two ensemble learning algorithms named
random forests and adaboost are applied to data  ensemble
learning methods combine several models trained with a given
learning algorithm to improve robustness and generalization
compared to single models      they can be separated into
two subcategories  averaging methods and boosting methods 
averaging methods build multiple models independently  but
the overall prediction is the average of single models trained 
this helps in reducing the variance term in error  on the
other hand  boosting methods build models sequentially and
generate a powerful ensemble  which is the combination of
several weak models 
a  random forests
random forests is an averaging ensemble method for classification  the ensemble is a combination of decision trees
built from a bootstrap sample from training set  additionally 
in building the decision tree  the split which is chosen when
splitting a node is the best split only among a random set of
features  this will increase the bias of a single model  but
the averaging reduces the variance and can compensate for
increase in bias too  consequently  a better model is built 
in this work  the implementation of random forests in scikitlearn python library is used  which averages the probabilistic
predictions  two number of estimators are simulated for this
method  with    estimators  the overall error is        sc is
        and bh is        using     estimators will result in
overall error of         sc of         and bh of         we
observe that comparing to the naive bayes algorithm  although
the complexity of the model is increased  yet the performance
does not show any improvement 
b  adaboost
adaboost is a boosting ensemble method which sequentially
builds classifiers that are modified in favor of misclassified
instances by previous classifiers      the classifiers it uses can
be as weak as only slightly better than random guessing  and
they will still improve the final model  this method can be
used in conjunction with other methods to improve the final
ensemble model 
in each iteration of adaboost  certain weights are applied
to training samples  these weights are distributed uniformly
before first iteration  then after each iteration  weights for misclassified labels by current model are increased  and weights

model
multinomial nb
svm
k nearest neighbor
random forests
adaboost with decision trees

sc   bh   accuracy  
          
     
          
     
          
     
          
     
          
     

table iv
final results of different classifiers applied to sms spam dataset

for correctly classified samples are decreased  this means the
new predictor focuses on weaknesses of previous classifier 
we tried the implementation of adaboost with decision trees
using scikit learn library  using    estimators  the simulation
shows      overall error rate        sc  and       bh 
increasing the number of estimators to     will change these
values to               and       respectively  like random
forests  although the complexity is much higher  naive bayes
algorithm still beats aadaboost with decision trees in terms of
performance 
vii  conclusion
the results of multiple classification models applied to the
sms spam dataset are shown in table iv  from simulation
results  multinomial naive bayes with laplace smoothing and
svm with linear kernel are among the best classifiers for sms
spam detection  the best classifier in the original paper citing
this dataset is the one utilizing svm as the learning algorithm 
which yields overall accuracy of          next best classifier
in their work is boosted naive bayes with overall accuracy
of         comparing to the result of previous work  our
classifier reduces the overall error by more than half  adding
meaningful features such as the length of messages in number
of characters  adding certain thresholds for the length  and
analyzing the learning curves and misclassified data have been
the factors that contributed to this improvement in results 
viii  acknowledgments
the author gratefully thanks tiago a  almeida and jose
maria gomez hidalgo for making sms spam collection v  
available 
references
    press release  growth accelerates in the worldwide mobile phone and
smartphone markets in the second quarter  according to idc  http 
  www idc com getdoc jsp containerid prus        
    tiago a  almeida  jos mara g  hidalgo  and akebo yamakami 
      contributions to the study of sms spam filtering 
new collection and results  in proceedings of the   th acm
symposium on document engineering  doceng      acm 
new york  ny  usa           doi                        
http   doi acm org                        
    http   en wikipedia org wiki short message service
    http   en wikipedia org wiki mobile phone spam
    adaboost  http   en wikipedia org wiki adaboost
    sms spam collection data set from uci machine learning repository 
http   archive ics uci edu ml datasets sms spam collection
    scikit learn ensemble documentation  http   scikit learn org stable 
modules ensemble html
    t  g  dietterich  ensemble methods in machine learning  in j  kittler
and f  roli  editors  multiple classifier systems  pages       lncs vol 
      springer       
    sms spam collection v    http   www dt fee unicamp br tiago 
smsspamcollection

fi