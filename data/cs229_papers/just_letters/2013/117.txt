how to prevent another financial crisis on wall street
helin gao
helingao stanford edu

qianying lin
qlin  stanford edu

kaidi yan
kaidi stanford edu

abstract
riskiness of a particular loan can be estimated based on several of its economic indicators  although the actual
relationship is very complicated  it is possible to construct a simplified model which can roughly classify a loan into one
of the three categories   high risk loan  medium risk loan and low risk loan   based on certain criteria  this paper seeks
to build such a model based on publicly available information as well as various machine learning techniques 

 

introduction

after eliminating those with missing data  we are left
with     training examples from california and    
training examples from new york  all of our data
are available in the company filings of us securities
and exchange commission  sec  website

during the           financial crisis  one of the principal culprits that led to the later domino effect of
financial meltdown was collateral debt obligations
 cdo s   a type of structured asset based security 
which allows allocation of interest and principal payment based on seniority  what happened during financial crisis was that many cdo s were overrated
by rating agencies due to asymmetric information 
this misleading rating was aggravated when a subset of cdo s pool the junior tranches of collateral
mortgage backed securities  cmbs s  together and
thereby creating systemic risk for security holders 
from      to      alone  the issuance of cmbs increased from    billion to     billion  eventually 
cmbs delinquency rate skyrocketed  paralyzing the
entire economy  curiously  many factors may influence loan default rate  including loan type  ltv ratio
 loan to value ratio   benchmark and average spread
of interest rate  this project aims to fix the systematic
risk created by cdo s by applying machine learning
algorithm to derive a method in classifying risky assets  by comparing the accuracy and f scores of different machine learning algorithms  we managed to
derive an optimal method in classifying loan risk and
therefore contributing to improved accuracy of risk
classification of cmbs bond 

 

in particular  each loan has its associated estimated average spread of interest rate  in general 
the higher the average spread of a loan is  the more
risky the loan will be  in other words  it will be more
likely the loan will default   based on the average
spread  we are able to categorize loan riskiness into
low        bps   medium          bps   and high
 above     bps  risk assets   hong  tang         and
our objective is to correctly classify each loan into
one of these categories based on its various economic
indicators 
we have    potential variables for each loan 
some of which  such as property name  trustee loan
id  are clearly irrelevant to the project  based on
economic theory  we decide to select the following
seven variables for our preliminary analysis of the
data 
   debt service coverage ratio under net operating
income  noi   this feature captures the ratio of
noi available for debt servicing to interest  principal and lease payments  based on the general
trend of market  there is likely to be a negative
correlation between noi and loan default rate 
our dscr noi ranges from          

data processing and intepretation

   debt service coverage ratio under net cash
flow  ncf   this feature captures the ratio of
ncf available for debt servicing to interest  principal and lease payments  similar to noi  a
higher ncf probably indicates stronger financial

we have     data points collected from      cmbs
transactions in california and     data points in
new york state  each of which detailing the characteristics of loaner as well as current loan status 
 

fichoose in the preliminary analysis  whose correlations
with the average spread are all greater than   

strength of the loaner based on economic theory 
our dscr ncf ranges from      to      
   debt yield  dy   defined as the ncf divided by
first mortgage debt expressed as a percentage  it
describes the relative size of the loan compared
to the companys total cash flow  in this data set 
the dy ranges from      to    

feature
net operating income  noi 
loan to value  ltv 
debt yield  dy 
major type  mt 
net cash flow  ncf 
time to maturity  tm 
bench mark  bm 

   major type  mt   describing the commercial
purpose of cmbs bonds  there are   main categories in this case  namely  in  industrial  lo
 lodging  mf  multi family  mu  mixed use 
of  office  rt  retail  ss  self storage  

table    m i scoring

   benchmark  bm   long term debt obligation issued by the government  we use   types of
benchmark  namely           treasury bill rate
and libor   month borrowing rate 

   

   time to maturity  tm   measuring the time
between when the bond is issued and when
it matures  higher maturity usually indicates
greater default rate risk subject to the fluctuation of macroeconomic condition as well as
higher bond durationsensitivity of the price to a
change in interest rate  therefore  higher time
to maturity usually corresponds to higher spread 

xi

and the conditional entropy of x based on y as 
x
x
h x y     
p  yj  
p  xi  yj   log p  xi  yj  
yj

ig x y     h x   h x y  

mi scoring

xx
xi

y

p xi   y  log

p xi   y 
p xi  p y 

   

finally  we can calculate su using the following formula 
 ig x y  
   
su  x  y    
h x    h y  

for feature selection  we first use mutual information
 mi  to examine the correlation between each feature
and the outcome  based on class  we have the following formula for calculating mi score 
m i xi   y   

xi

   
next  we define the information gain of x provided
by y  quinlan        as 

feature selection

   

symmetric uncertainty

next  we try to reduce certain features which are potentially repetitive  in order to do so we calcualte the
symmetric uncertainty  su    press et al         between each pair of distinct features from the seven
features above  we choose to calculate su instead
of the linear correlation coefficient since major type
and benchmark have no numerical values  we first
define the entropy of a variable x as 
x
h x    
p  xi   log p  xi  
   

   loan to value  ltv   loan to value ratio used
by lenders to describe the ratio of loans to the
value of the asset purchased  the higher the ltv
ratio  the higher the risk as perceived by lender 
in this project  ltv ratio ranges from     

 

mi score
      
      
      
      
      
      
      

one can easily check that the range of su value is
        intuitively  the larger su  x  y   is  the more
likely knowledge of one variable can predict the other
variable  hence the more closely two variables are correlated 
based on the formula given  we are able to calculate the su score between each pair of features  the
results are listed in the following table 

   

since some of the features  such as loan to value 
are continuous variables  we use the most straightforward discretizing technique  butte and kohane       
michaels et al        in order to apply equation     
below are the mi scores of the seven features we
 

filtv
mt
bm
ncf
noi
dy
tm

ltv
    
    
    
    
    
    
    

mt
    
    
    
    
    
    
    

bm
    
    
    
    
    
    
    

ncf
    
    
    
    
    
    
    

noi
    
    
    
    
    
    
    

dy
    
    
    
    
    
    
    

tm
    
    
    
    
    
    
    

table    su scoring
based on the table  we can see that bench mark  bm 
and time to maturity  tm  have a relatively high
correlation  su  bm  t m           considering
the fact that tm has a higher m i scores than bm
does  we decide to eliminate bm from our feature
list and use tm instead  notice also that this result
makes economic sense  in defining benchmark  we
have already incorported the factor of time    month 
       years   thus it is no surprise that bm will
have a high correlation with time to maturity  which
is also an indicator of time 

 

figure    problem of similar data    listed here 
in total  there are    training examples in our
california loan data with the same type  lo   very
similar ltv values and the same average spread
    bps  as shown in figure    due to space limit 
the ltv values are not displayed in figure    
therefore our training samples are far from being
uniformly distributed  also  we have anomalous data
points with unusually high ltv values  these factors
contribute to the low f score of our lr and svm
models  and affect their accuracy rates negatively 

model selection

in the beginning  we consider three different models 
naive bayes  nb   logistic regression  lr  and
support vector machine  svm  using linear kernels 
our svm model is solved by sequential minimal
optimization due to its efficiency  and below are the
accuracy rate  precision  recall and f score for each
model  with    fold cross validation  
model
nb
lr
svm

accuracy
      
      
      

precision
     
     
     

recall
     
     
     

each other 
logistic regression and svm  on the other hand 
give higher accuracy rate on the training examples 
however  these two models are still unsatisfying
because their f scores are low  in other words 
our lr and svm models are not robust enough to
handle with skewed class cases   the problem with
skewed class is present since many of the loans fall
into the medium risk category  besides  by manually checking the data  we see the following problems 

in considering the disadvantages of all the models presented above  we decide to try decision tree
as our new model  the reasons for choosing decision
tree are 

f score
     
     
     

   this model is time efficient since we only need
to build the tree once 
   unlike nb  lr or svm  decision tree is nonparametric  this allows us to deal with outliers  data which has anomalous behaviour  with
higher confidence  thus boosting the accuracy of
our model 

table    performance of different models

we first try nb because it gives an easy way to
see the trend between outcome and different features 
yet based on our result  nb gives a fairly low
accuracy rate of less than      this implies that the
nb assumption that different features are conditional
independent given the outcome is probably not true
in our situation  in fact it corresponds to the acutal
economic situations in the real world   different
features affect each other in a complicated way  thus
they can never be conditionally independent with

   based on certain splitting criteria  decision tree
can be relatively insensitive to an unbalanced
data distribution   drummond  holte       
in particular  we use c    algorithm  quinlan        
it makes use of the information gain which we have
calculated above  and splitting criteria based on information gain has relatively good performances in
 

fi   

dealing with skewed class and unbalanced data distribution  monard  batista         the outlined c   
algorithm for building a decision tree is 

finally  we examine the generalization error of our
decision tree model on the new york data  and below is the accuracy rate  precision  recall and f score
when the model is run against new york data 

   for each feature x in the list  find the normalized information gain ratio from splitting on x
   choose xoptimal which gives the largest normalized information gain ratio

accuracy
      

   create the decision node which split the training
data based on xoptimal

   

training result

 

the decision tree built based on c    algorithm
has the following accuracy rate  precision  recall and
f score  in reality  we use an open source package
which allows us to run the algorithm with    fold
cross validation  
precision
     

recall
     

recall
     

f score
     

in total    out of     examples are correctly classified  the generalization error of our decision tree
model on new york data is slightly higher than our
training error  this is an acceptable result  and furthermore it also produces a high f score on the new
york data 

result and analysis

accuracy
      

precision
     

table    generalizatoin error of decision tree

   for each subset of the training data  repeat step
  until the base case is reached  meaning that all
the training data belong to the same class 

 

generalization error on new york
data

conclusion

in conclusion  we have built a model based on decision tree which has reasonable accuracy and f score 
the model is by no means sophisticated  since we
are only concerned with less than    features here  in
reality there can be many more features which play a
part on the riskiness of a loan  but that require a much
more complicated model and also much longer time
for training and lerning   nevertheless  this model
is still useful to a certain extent  as it provides a
simplified perspective on the type of loans and range
of ncf and nois which will likely give a higher risk
loan  and cross reference with new york data shows
that this model has a relatively small generalization
error and thus is less likely to suffer from the problem
of over fit 

f score
     

table    performance of decision tree
as we can see from the table  decision tree yields
the highest accuracy among the four models  nb 
lr  svm and decision tree   this is expected since
decision tree  as argued in the previous section 
deals with outliers with highest confidence  decision tree also gives the highest f score  indicating
that our decisoin tree does not produce too many
true negative or false positive predictions  it shows
that our decision tree is relatively insensitive to the
skewed class as well as the uneven distribution of
data 
a graphical illustration of the decision tree we
build in shown in figure    figure on next page   an
importance observation is that in this decision tree 
only four features are concerned in the end   noi 
ncf  tm and mt  one possible reason is that the rest
two features are not able to split the data as much as
the first four features do  thus absent in the decision
tree model 

 

future work

despite the simplicity and relatively high accuracy 
there are still some problems associated with our decision tree model  motivating us to work further on
this project 
   the new york and california data turns out to
suffer from the problem of skewed class  if the
data are more evenly distributed among different
classes  svm and lr might give us appropriate
models   the advantages of decision tree might
not manifest themselves in those circumstance 
 

fifigure    decision tree trained using california data  self drawn 

 

   the riskiness of loans are dicretized into three
categories   in reality  this might be oversimplified  thus we want to see if we can
quantify the riskiness through other parameters
and construct a quantitative prediction algorithm
based on that 

 

reference

l yu and h liu  feature selection for highdimensional data  a fast correlation based filter
solution  icml      
r steuer and j selbig  the mutual information  detecting and evaluating dependencies between variables  bioinformatics  vol      pp         
d tang and h yan  market conditions  default risk
and credit spreads  discussion paper  banking and
financial studies       
andrew ng  cs    lecture notes       

acknowledgement

we would like to express our gratitude to keith silates
 icme  computer engineering  stanford university 
who accepted to mentor our project 
 

fi