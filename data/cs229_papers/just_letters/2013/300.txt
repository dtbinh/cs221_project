maximizing precision of hit predictions in baseball
jason clavelli

joel gottsegen

clavelli stanford edu

joeligy stanford edu

december         

data collection

introduction
in recent years  there has been increasing
interest in quantitative analysis of baseball 
this field of study  known as sabermetrics 
has been embraced by baseball managers as
an effective means of predicting how players
and teams will perform  and has gained
mainstream popularity as a result of films
like moneyball and celebrity statisticians like
nate silver 

one positive aspect of working on a baseball
prediction problem is that baseball data is
plentiful and easily accessible    the first step
of our data collection process was to write a
python script  that gathered all of the box
score data from a given season into a single
file  we then processed the raw box score
data into a set of training examples  with a
players state before a game as the input  and
a binary indicator of whether or not that
in this project we apply sabermetrics  player had a hit in that game as output 
in the form of machine learning algorithms 
to a difficult baseball prediction problem 
we attempt to build a model that  given
the set of all baseball players playing on a feature selection
given day  chooses the player most likely to
get a hit  in order to get a sense for the because of the easily quantifiable nature
difficulty of the problem  we implemented a of baseball and the recent trendiness of
few trivial models  a model that chooses a sabermetrics  there are countless baseball
player at random is correct approximately statistics measuring everything from how
    of the time  a model that chooses a many runs a player has produced  rp  to
player randomly from the subset containing how many runs a pitcher allows in   innings
only the best hitters  with batting average  era  to the estimated contribution of a
greater than       is correct approximately player to his teams wins  win shares   since
    of the time  our aim in this project
  all of our data was taken from http   www baseballis to find a more sophisticated model that reference com
 a
portion of our data scraper was copied
increases this number as much as possible 
from
benjamin
newhouses
scpd
scraper 
https   github com newhouseb scpd scraper

 

fiour problem focuses only on a player getting
a hit and not on the game as a whole  we
were able to eliminate the vast majority of
metrics as possible features 
we considered as possible features all
measurements that in some way quantified
information about the batter  the pitcher 
or the position of the fielders  as these are
the factors that relate to the probability an
at bat results in a hit  by experimenting
with different features  we determined that
the following are good predictors 
 average hits by the batter per game  last figure    relative weights assigned to each of the
features by logistic regression
n games 
this measures how a batter has perballpark factor    which approximates
formed in the past  average hits per
the advantage that a park gives to hitgame is more relevant to our problem
ters  relative to other ballparks  a simple
than batting average  since it penalizes
batting average over all of the at bats in
players for bunting  sacrifice flying  getthe park would not work  because that is
ting walked  hit by a pitch  etc 
highly influenced by the hitting ability of
 average at bats for the batter per game 
the home team  using
last n games 
pm
 
total hits in home game i
this measures how many opportunities
m
pni  
 
the batter has to get a hit  this feature
j   total hits in away game j
n
is extremely important  a mediocre hit mostly  removes dependence on the
ter with   at bats in a night is generally
home team by dividing by the teams permore likely to get a hit than an excepformance in other ballparks 
tional hitter with   at bats 
 average hits given up by the pitcher per
game  last n games 
this measures the past performance of
the pitcher  this feature was very predictive and  perhaps more importantly 
independent of all of the batting features 

for the measurements that consist of an
average over the last n games  we created
two features  one with large n  one with
small  the large n feature represents how
that player performs in general  while the
small n feature measures whether the player
is on a hot or cold streak going into a given

 hitter friendliness of the ballpark 
for this feature we used a statistic called

  http   espn go com mlb stats parkfactor

 

figame  as the figure   indicates  the general
performance of a player is a better predictor
than streakiness 

performance metrics
the first performance metric that we used
fp fn
was general error   fp fn tp tn
   however 
since our data is unbalanced and our goal
is more interested in avoiding false positives
than false negatives  we quickly realized that
precision is a better measure of our models
success 
another useful measurement looked at how
well the model was able to separate positive
and negative samples  using the difference between the average confidence assigned to the
two classes  this was given by

figure    the distribution of confidence  t x   separated by positive and negative samples  the stars
represent the mean confidence for positive and negative samples  respectively 

logistic regression
the primary model that we used was logistic
regression  logistic regression appealed to
us for several reasons  the most important of
which were 

a     a   
where a k  is the average confidence assigned
to samples with output k  given by

i  simplicity of implementation 
pm t  i 
 x   y  i    k 
i  
pm
 i    k 
i     y

ii  as discussed earlier  logistic regression
uses  to assign a measure of confidence
in each sample  this creates an easy way
to choose a small subset of most confident
samples 

the difference in mean confidence is often
quite small  see figure     but improvements
with this metric were strongly correlated with
our final and most important metric  precision in the most confident subset  see figure
    this metric looked at the error rate over
subset of the samples in which our model was
most confident  which directly corresponds
with the goal of this project 

after implementing a version of logistic
regression that worked using any one feature 
we ran into a major roadblock  our algorithm
broke down when the training examples were
higher dimensional  the gradient ascent took
more than    passes over the design matrix
 

fisvm
another model that we used was a support
vector machine with a radial basis function
kernel  which we hoped would address the
apparent nonlinear decision boundary better
than logistic regression did  using the same
features as with logistic regression  we were
able to reach     training precision on a
training set with        samples  which indicates that some form of separation is possible  however  this model generalized very
poorly  with only     precision when it came
to testing error 
figure    the precision over subsets of different sizes 
where a subset of size n has the n samples in which
the model is most confident  note the sharp increase
in precision for the subsets with fewer than     samples 

conclusions
our results were very encouraging  overall 
the testing precision achieved by logistic
regression        was considerably better
than random guessing        or trivial selection algorithms such as choosing the best
hitters         additionally  the fact that
we were able to achieve very high training
precision with the svm indicates that  given
the right tweaks to reduce over fitting  the
svm has the potential to outperform logistic
regression 

 which had       samples  to converge and
resulted in nonsensical values of theta  this
issue resolved entirely once we normalized
the design matrix  gradient ascent began
converging within one pass over the data 
our logistic regression results were encouraging 
training precision       
testing precision       
training and testing precision are roughly
the same  indicating that the model is not
over fitting  however  we were never able to
reach a training precision greater than    
with logistic regression  indicating that the
model has high bias and that the optimal
decision boundary may be highly nonlinear 

as an alternative to logistic regression
and svm  we have begun experimenting
with neural networks  initial attempts have
resulted in approximately the same testing
precision achieved with logistic regression 
in addition to being able to predict hitters with reasonable accuracy  we also know
which factors contribute the most to this
predcition  see figure   again   we can see
 

fithat the number of at bats is far more correlated with hits than anything else is  and
that overall performance of a player is more
correlated with hits than recent perfance is 
both of which are very non intuitive results 
we are also planning to add additional 
more nuanced features  some of these include the handedness of the pitcher and
batter  because the batter has a significant
advantage when he and the pitcher have
opposite dominant hands   the time of day
that the game takes place  and the favored
pitch types of the batter and the pitcher 

references
 scpd scraper  benjamin newhouse 
https   github com newhouseb scpdscraper
 chih chung chang and chih jen lin 
libsvm   a library for support vector machines  acm transactions on
intelligent systems and technology 
                   software available at
http   www csie ntu edu tw  cjlin libsvm
 mlb park factors 
http   espn go com mlb stats parkfactor

 

fi