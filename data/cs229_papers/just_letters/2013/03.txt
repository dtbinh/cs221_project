literature search made easy
bo wang  min liu  bowang  liumin  stanford edu

any of us have been faced with this situation 
we found a certain field of research intriguing  went to an online archive to search for
related papers  and the results were just less than
satisfactory  they are too unstructured to obtain a
big picture  such as how this field evolves over time 
what sub fields it is composed of  instead of relying on human instinct for such insights  our project
provides an alternative  using machine learning algorithms to peruse thru online archives  generate
the information of interest  and present them in a
human friendly way 
in this project  we used hierarchical latent dirichlet allocation  hlda  to find the topics and their
hierarchy and dynamic topic model  dtm  to find
the time evolution of topics  in addition to using
these two models  we also made an important extension to them   constructing the corpus with key
phrases instead of keywords  furthermore  we devised
an algorithm to remove the duplication in word and
phrase count  which enabled us to generate a corpus composed of both keywords and key phrases 
this approach allowed us to greatly improve the interpretability of the result since a large number of
concepts in science and technology are formulated
as phrases 
in our experiments with       condensed matter
physics papers from arxiv  the above methods effectively found the trend and hierarchy of research
topics and revealed some very interesting insights 
in our report  the trends in research topics section demonstrates the analysis of topic trends  advantage of using key phrases versus keywords and a
caveat in word and phrase count when constructing
the corpus  the topic hierarchy section presents
the generation of a hierarchy of topics and the solution to the caveat aforementioned 

m

latent dirichlet allocation  lda 
latent dirichlet allocation  lda      is a bayesian generative probabilistic model of a corpus  it assumes that
documents are random mixtures of latent topics  where each
topic is characterized by a distribution over words  here we
describe the smoothed lda since this is the version of lda
that serves as the basis of the other models used in this
paper  smoothed lda assumes the following generative
process  figure    for a document in a corpus d 
   choose n  p oisson   
   choose   dir   
   choose   dir   
   for each of the n words wn  
a  choose a topic zn  m ultinomial  
b  choose a word wn from p wn  zn      a multinomial
probability conditioned on the topic zn  
the key computational problem of lda is to infer the
hidden variables  i e  finding what hidden topic structure
is most likely to generate the observed corpus with the
above generative process  once the posterior of the hidden
variables are computed  it can be used to output topics
words that appear most often in each topic  details of the
algorithm can be found in     





background





z

k

w

n

m

figure    illustration of generative process of lda    

page   of  

fihierarchical latent dirichlet allocation
 hlda 
hierarchical latent dirichlet allocation  hlda      is an
extension of lda which analyzes hierarchies of topics in a
corpus  instead of constraining each document to have a
fixed finite number of k topics as lda does  it allows an
infinite number of topics  which are organized in a hierarchy 
with the most abstract topics near the root of the hierarchy
and more concrete topics near the leaves  in hlda  a word
is generated not based on the topic but a path of topics
 from the root topic to the leaf topic  it belongs to 
the generative process  figure   can be illustrated as 
   let c  be the root topic 
   for each level l             l  
a  draw a sub topic from topic cl   
b  set cl to be that sub topic 

   for each document 
a  draw   n  t   a  i 
b  for each word 
i  draw z  m ultinomial     
ii  draw wt d n  m ultinomial  t z    with
exp k t w  
 k t  w   p exp 
k t w  
w

here   and  are not sampled from dirichlet distributions
as in smoothed lda  instead  t k is a random walk with
gaussian steps  while  is drawn from a logistic normal with
mean t   which undergoes similar dynamics as t k   the
choice of logistic normal is due to its amenability with time
series modeling 
dtm computes the posterior of hidden variables of all the
time slices  which is used to output the evolution of topics
and keywords 






   draw an l dimensional topic proportion vector  from
dir   







   for each word n              n   

z

z

z

w

w

w

n

n

a  draw z              l  from multinomial   
b  draw wn from the topic associated with cz  
where the root topic is the topic on level    its sub topics
are those on level    etc 

a



a



n

a

 k

figure    illustration of generative process of dtm    






c 

feature extraction
c 



data cleanse
c 
z



w

n m

 

cl

the collection of documents have to be converted to a
term frequency matrix as required by all the topic models
described above  instead of generating a term matrix with
raw data  we pre process the data as follows to remove noise 

figure    illustration of generative process of hlda    

   separate an abstract into sentences 
   for each sentence 
a  remove stop words  e g  the  and  of  

dynamic topic model  dtm 

b  remove tokens containing no letters 

dynamic topic models  dtm      can be seen as a combination of lda and time series  it is a generative model
that analyzes the time evolution of topics of a collection of
documents over time 
the generative process of dtm  figure    is as follows 
where time t is discretized in to time slices 

c  remove common verbs  e g  get  take  think  

   draw topics t  t   n  t       i  
   draw t  t   n  t       i  

d  remove tokens shorter than minimum term length
of   characters 
e  remove latex format tokens 
f  split hyphen connected words if both words are
longer than the minimum term length 
g  convert nouns to sigular form and verbs to infinitive form 

page   of  

fih  stem tokens 
then we generate a dictionary containing all terms that
appeared in at least   documents and a term matrix whose  i 
j  th element n denotes that the j th word in the dictionary
occurred n times in the i th document 

phrase generation
while doing data pre processing  we observed that new
concepts in science and technology are usually expressed in
the form of phrases  such as computer science  machine
learning and topic modeling  which indicates a topic
might be better characterized by its key phrases instead
of keywords  furthermore  phrases also offer better
interpretability of results  e g  a topic with keywords
dimension  principal  analysis may greatly perplex us 
while one with key phrases principal component analysis 
dimension reduction is quite informative 
we generated phrases before the second step in the
previous section by 
   separate words into blocks by punctuation marks and
stop words 

experiment setup
data is crawled from the online archive website arxiv  in
fields like mathematics and physics  almost all scientific
papers are self archived on the arxiv  despite not peer
reviewed  arxiv has its submissions reviewed  or even recategorized if off topic  by moderators that are experts in
their fields of research  in addition  submission dates on
arxiv more accurately reflect when the research project
was completed and paper drafted  since usually papers are
posted on arxiv months before they are finally published on
a peer reviewed journal or conference  based on these facts 
we believe arxiv is a better collection of research works than
any journal or conference alone 
the titles  authors  abstracts and citation of papers are
scraped from api of http   export arxiv org api  using python and stored in separate files according to the
publish year  we used only papers in condensed matter
physics since one of us is familiar to the field  the methods
demonstrated in this report however can be applied to the
analysis any field  the corpus contains       abstracts from
year      to      and      unique words and       unique
phrases after our data processing procedure 

results

   within each block  generate phrases made up of consecutive words while restricting the maximum length trends in research topics
of phrases to   words since usually phrases longer than
in our dtm experiment  the number of topics is set to    a
  words are represented by their acronyms 
priori  results are summarized in figure   and figure   

duplicate counts removal
using the phrase generation method described above can
lead to duplicate counts in the resulting corpus  for example  phrase stanford university will generate stanford
once  university once and stanford university once 
while these two words should only be counted as a phrase
once 

in figure   we demonstrate   out of the    topics
for purpose of figure size control  the   tables respectively
are topics and keywords generated from only words  words
plus phrases and only phrases  in these tables  the topics
are written in bold and keywords in regular font style 
we see the in all cases  the dtm algorithm very nicely
identifies major research topics  however  the classification
with phrases only clearly gives much better interpretability 
note that the first two tables look quite similar  in fact  in
the second table  even though the corpus contains both
words and phrases  we can hardly see any phrases among
the important keywords  this is quite intuitive since in
the corpus  there are        words       of them unique
and        phrases with       of them unique  so on
average  a word appears     times in the corpus while a
phrase appears only    times  resulting in a term matrix
dominated by words  this is what has motivated to remove
the duplicated counts in words 

to eliminate this inaccuracy  we implement a twophase algorithm  the first phase is the same as the
approach described above  extract words and enumerate
phrases in each document  then filter the words and phrases
whose occurrences are below the minimum threshold  in
the second phase  documents are scanned through again 
for each word in a document  we check whether it can
be combined with its previous or next word to generate a
phrase appearing in the filtered phrase dictionary  if not 
this word is counted as one individual occurrence  otherwise
it is skipped  this method basically recounts all terms and
in figure    we present the plot of one specific
checks if their appearance contribute to the generation of a
topic phase transition  which is typical of all the plots 
phrase  only those terms not as part of a phrase is counted 
from figure    despite the frequencies of keywords do
by using this procedure  we were able to produce a corpus
rise and fall  there is hardly any obvious trend  this is
which nicely combines both phrases and words 
not unexpected since its natural that a paper on phase
transition should always involve keywords such as phase
transition and phase diagram regardless of when it is
evaluation
published 

page   of  

fisuperconductivity
field
magnet
quantum
electron
effect
current
tunnel

phase
transition
model
critic
transit
scale
temperatur
expon

ultracold
atoms
condens
bose
atom
trap
einstein
state
superfluid
mode

computation
model
structur
method
cluster
neural
algorithm
energi

statistical
mechanics
quantum
time
random
distribut
correl
system
function

supercondphase
uctivity
transition
temperatur
transit
fermi
phase
liquid
model
superconduct critic
superconductor temperatur
phase transition
scale

ultracold
atoms
condens
bose
atom
trap
einstein
vortex
superfluid
ga

computati
on
model
surfac
structur
dynam
observ
energi
network
simul

ultracold atoms
bose einstein
condensate bose einstein
bethe ansatz
bound state
ground state
calogero sutherland

computation simulation
monte carlo
monte carlo simulation
quantum monte
quantum monte carlo
low temperature

statistical mechanics
random matrix
correlation function
matrix theory
probability distribution
distribution function
matrix model

statistical
mechanics
time
distribut
dynam
random
correl
model
function

the case her thesis project fails  she can turn to the side
project and produce some results quickly without having
to acquire a new set of skills 

b  in certain field  it is easy to publish  but difficult to
discover something of importance  while its the opposite case in some other fields  quantum hall effect 
although being quite famous even outside the field of
physics has actually very few publications  which is probably due to the requirement of ultra high magnetic field in
figure    dtm topics and keywords  corpura corresponding
quantum hall experiments  on the other hand  there are
to the   tables are respectively composed of a  words
surprisingly large number of papers on computations
b  words plus phrases without removing duplicate
and simulations despite the small number of researchers
counts c  phrases
in this field  e g  one out of about    physics faculty
members at stanford   this implies that if ones priority
time evolution of topic phase transition  words phrase 
is to have as much publication as possible  then he should
choose to do simulations 
phase transition
phase transition
renormalization group
field theory
mean field
phase diagram
order parameter

phase transition
renormalization group
field theory
mean field
phase diagram
order parameter

frequency

    

    

    

superconductivity
external magnetic field
low temperature
high temperature
order parameter
josephson junction
fermi surface
temperature dependence

 

  

  

topic hierarchy
we produced four depth   trees generated by corpora
composed of a  words alone b  phrases alone c  words plus
phrases without duplicate count removal d  words plus
phrases with duplicate count removal  

year

as expected  results from the first and third corpus
composition look almost identical  as in the dtm case  so
in the report we show only one of them  in figure     
and    the other three trees are presented  where each node
one very informative plot we can make is the evolution of represents a topic  since these trees have a total of   levels
topic popularity  in figure    we show a plot of frequencies and tens of leaves  we include in the plots only   or   topics
for   topics over time 
on the second level  and expand further one out of those  
or    the remaining topics are represented by             
figure    keywords trend plot of topic phase transition

topic popularity trend

quantum hall effect
hall  fraction  edg  composit  fill

    

 

  

  

figure    plot of frequencies of different topics over time

this plot has some really interesting implications 

electron transport
electron  conduct  quantum 
tunnel  liquid

statistical mechanics
distribut  time  scale  statist 
chaotic

root topic
model  field  state  quantum 
energi

    

computation simulation
quantum hall effect
superconductivity
ultracold atoms

    

topic frequency

josephson junction
current  ring  junction  mesoscop  persist
magnetism
spin  model  magnet  phase 
antiferromagnet

low dimension systems
wire  phonon  drag  plasmon  rate
transport
pseudoparticl  chaotic  billiard  nonloc 
unitari

material growth
surfac  phase  growth  diffus 
deposit  kinet  site
interact  fermi  fermion  state 
temperatur
computation
network  algorithm  method 
problem  random

ultracold atom experiments
trap  ga  gas  harmon  confin
ultracold atoms
condens  bose  einstein  atom 
superfluid

quantum entanglement
entangl  ion  glas    he  efimov
skyrmion state
matter  neutron  skyrmion  nucleon  nuclear

band structure
electron  structur  band  metal 
atom
       

kondo effect
kondo  impur  channel  anderson  heavi

soliton
soliton  anyon  kink  statist  dual
field theory
gaug  chiral  invari  chern 
fermion

random  dirac  qcd  eigenvalu

a  superconductivity and ultracold atoms are clearly
negatively correlated  which indicates it is possible that
for some reason  e g  similarities in research techniques  figure    hierarchical classification of a corpus composed of
words alone
research works in these two fields are almost done by the
same group of researchers  the negative correlation is
created by the shift of their interest and time allocation
between the two fields  so for a phd student whose
thesis project is in superconductivity  it may be wise to
choose a backup side project in ultracold atoms  in
disordered and nonlinearity
breather  nonlinear  discret  guid  schroeding

page   of  

fibec experiments
bec

root topic
magnetic field  ground state  phase transition  monte
carlo  correlation function

spin systems
spin chain  heisenberg chain  quantum spin 
quantum spin chain  nonlinear sigma model

bose ga  gros pitaevskii  scattering
length  trapped condensate  dilute bose
gas

quantum hall effect
landau level  lowest landau level 
lowest landau  strong magnetic field 
filling factor

fractional quantum hall effect
exclusion statistic  fractional exclusion statistic 
fractional exclusion  virial coefficient  second
virial coefficient

chacterized by keyword superconductor instead of the
phrase superconducting state  combining words and
phrases and implementing duplication removal allows
the algorithm to choose the most important expressions
from both of two sets instead of one  resulting in better
performance and interpretability  hence we think this is
the best way to do topic modeling 

landau level
extended state  lowest landau level approximation 
wigner solid  soliton lattice  conjugated polymer

statistical mechanics
random matrix  random matrix theory  matrix
theory  quantum chao  level spacing
superconductivity
order parameter  normal state  fermi surface 
high temperature superconductor 
superconducting state

bec with interactions
interaction bose einstein  critical velocity 
attractive interaction bose einstein  nonlinear
schrodinger equation  interaction bose
quantum hall effect
composite fermion  quantum hall system  hall
system  quantum hall regime  hall regime

phase transition
renormalization group  fixed point  scalar
field theory  scalar field  replica symmetry
ultracold atoms   quantum hall effect
bose einstein  quantum hall  quantum hall
effect  condensate bose einstein  fractional
quantum hall

optical lattice  matter wave  dark soliton  optical
lattice bose einstein  state bose einstein

quantum hall fliud
quantum hall fluid  hall fluid 
incompressible quantum 
incompressible quantum hall  minimal
model

conformal filed theory
conformal field theory  conformal field 
logarithmic conformal field theory  quantum hall
transition  integer quantum hall transition

conclusion

in this work  we proposed a phrase based extension to topic
modeling which improves the clustering performance and
enables effective interpretation of the document context 
figure    hierarchical classification of a corpus composed of phrases contain important information on the correlation
phrases alone
between documents and can help human understand since
many words do appear in combinations  the two phase
phrase extraction mechanism first enumerates word combinations and then filters out significant phrases based on
cross document validation  this approach implicitly selects
the key phrases in the corpus since it leverages the information hidden in multiple documents  in the evaluation  we
compared three scenarios  i e  word only  phrase only  and
word phrase  in both hlda and dtm modeling  we can
clearly see that with phrase information the result is much
more accurate and easy to interpret 
       

spin scattering
spin dephasing  spin dephasing time  spin
polarization  quantum well  dephasing time

phase transition behavior
thermal conductivity  time reversal  gap anisotropy 
pairing state  pair tunneling

root topic
model  state  field  quantum  phase

ultracold atoms
condens  bose einstein  bose  trap 
einstein

superconducting phase transition
impur  order parameter  superconductor 
scatter  penetration depth

collective behavior
drag  coulomb drag  collective mode  band gap 
carrier density

statistical mechanics
random matrix  random  matrix 
random matrix theory  distribut

ground state phases
ground state phase diagram  nonmagnetic impurity 
ground state phase  state phase diagram  ladder

quantum hall effect
quantum hall  quantum hall effect 
hall  hall effect  quantum

pairing mechanism
polaron  hole  holstein  single hole  interaction
electron phonon

superconductivity
superconduct  superconductor  dope 
gap  electron

magnetism
spin  spin wave  antiferromagnet 
magnet  hubbard model

model   computation
hubbard model  quantum monte carlo 
monte carlo  hubbard  quantum monte

phase transition
renormalization group  field theory 
renorm  critic  fixed point

       

magnetism in superconductors
density wave  in plane magnetic field  magnetic
field  charge density wave  spin density wave

slave boson
slave boson  quantum monte carlo technique  slave 
bose hubbard  staggered flux

computational methods
quantum monte carlo method  monte carlo method 
fixed node  wave function  diffusion monte carlo

structural phase transition
multi band  model electron phonon 
jahn teller  low temperature regime

structural phase transition
structural phase  structural phase transition 
condensation energy  feedback  rich phase diagram

references
    blei  d   ng  a  and jordan m  latent dirichlet allocation  the journal of machine learning research 
volume               mar     

figure    hierarchical classification of a corpus composed of
words plus phrases with removal of duplication

    blei  d   griffiths  t  and jordan m  hierarchical topic
models and the nested chinese restaurant process 
nips       
all of these three plots look similar on the second level 
and the classifications very accurately capture the sub
    blei  d   griffiths  t   jordan m   and tenenbaum j 
fields in condensed matter physics  this is quite surprising
dynamic topic models  proceedings of the   rd interespecially considering that hlda a completely unsupervised
national conference of machine learning   pittsburgh 
learning algorithm 
pa      
here again we see that using phrases results in much better
interpretability  e g  in the second tree  phrases in every
rectangle look like something meaningful  and indeed they
are  even to those who are not experts in physics  while
in the first tree the topic with keywords random  dirac 
qcd  eigenvalue is very bewildering  hence not labeled with
any topic  however  there is a draw back of only including
phrases in the corpus  words such as superconductor and
antiferromaget  though being key concepts in condensed
matter physics cannot be used in the classification because
they are single words  so in the third tree  we made use
of a combination of words and phrases  after removing
the duplicated counts in words  the keywords generated
by the hlda algorithm have a well proportioned mixture
of words and phrases instead of solely dominated by
words  in addition  the topic superconductivity is now

page   of  

fi