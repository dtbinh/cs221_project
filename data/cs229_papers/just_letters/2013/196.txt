extracting emotions from news headlines
raghavan  bharad
bharadr stanford edu

samar  anshul
asamar stanford edu

december         
abstract
determining emotion in a piece of text is a difficult subset of sentiment analysis  a field which has largely
focused on binary classification of text as positive or negative  classifying sentences in one of six major emotion
categories  anger  disgust  fear  joy  sadness  and surprise  can make significant contributions to political polling 
marketing  and stock market predictions  in this study  we focus on the first of these six emotions  anger  and
develop a model to predict whether or not a reader will feel anger after reading a headline  our model involves
nlp tools   stemming  named entity recognition  and part of speech tagging   and a bag of words model run on
a svm with a linear kernel and a naive bayes classifier with laplace smoothing  we developed experimental
models to test on smaller subsets of data including  pca  to reduce dimensionality of data  a synonym algorithm 
to expand each headline allowing our model to classify with respect to more words  and a co occurrence model 
to score each headline based on co occurrences of words with the word anger in the new york times archives 
we also made an emotion headline generation model with markov chains to see if we can automatically create
angry headlines  with these models  we hope to gain greater insight into methods that one can use to detect
emotion within text 

 

overview

tions  we hoped that in doing so  our model would pick
up on words associated with anger and non anger 

in this paper  we discuss our original model  provide
error diagnostics  and describe our final model built
based on the lessons we learned  we also discuss our
experimental additions and possible next steps 

 

we developed a parser based on nlp tools to sanitize and normalize headlines  among other features 
we removed headlines that werent properly encoded
in utf   and stripped them of hyphens punctuation
marks  we used the stanford named entity tagger
to recognize locations and ran them across a lexicon
of us states and added a feature as to whether or not
the headline concerned domestic or international affairs  we theorized that headlines that included non
us states like foreign countries were related to negative news   we used the stanford part of speech  pos 
tagger to remove prepositions  we detected numbers
using pos and added features for that as well  we theorized that numbers are correlated with emotion   we
removed articles and also resolved possessions   the
stanford tagger split possessions into the original word
and the apostrophe ss were removed by our parser 

data set and pre processing

initially  we used semevals data set   of      news
headlines as our testing set  from now on  referred to
as semeval   semeval contains headlines scored from
  to     for six different emotions  we provided a
threshold on which to classify each headline  headlines
that had been scored by semeval as greater than   
were interpreted by our system     i e  containing a
non negligible amount of anger  
however  we had no training data  we created a
lexicon we believed instigated anger  based loosely on a
wordnet anger lexicon     our lexicon included words
like anger  kill  accused  rape  etc  we wrote a
script to extract headlines from the new york times
article search v  api that included these words  we
similarly extracted clearly non angry headlines with
another lexicon of words associated with positive emo 

to normalize the headlines  we un capitalized
words and ran all of our headlines through nltks
snowball stemmer  this can be tested online   thankfully  thankful  thanking for example are turned into
thank   we then parsed each headline and returned
a text file of unigrams and bigrams  we did this as

  http   www cse unt edu rada affectivetext 
  http   www cse unt edu rada affectivetext 

 

fihappy and not happy reflect two different sentiments
and the latter sentiment can be more easily captured
through bigrams 
note that the parser does not work with full accuracy as many of the tools it relies on are also based on
machine learning methods 

 

diagnostic on         headlines  we found that our
svm had     recall on angry headlines but classified
      of non angry headlines as angry 
analysis for naive bayes naive bayes worked best
with          distribution  as the ratio of angry to
non angry headlines increased  recall increased sharply
at the expense of precision and accuracy  our naive
bayes on          distribution classified     of angry headlines as angry  but also classified       of
non angry headlines as angry  the naive bayes has
higher accuracy and recall than svm  however levels
of precision are relatively similar 
conclusions 

initial model

we wrote scripts to extract xml data from semeval
for our test set as well as create a training set in matlab from our accumulated nytimes headlines  we
created a lexicon and ran a bow  bag of words 
model  because this is a text classification problem 
we used a support vector with a linear kernel  referred
to as svm  and a naive bayes classifier with laplace
smoothing  referred to in this paper as naive bayes 
based on code from cs    exercises  the svm code
came from liblinear 

 

   distribution  testing results depend significantly on distribution  an excess of angry headlines leads to more words being indicative of
anger which leads to higher recall   our results are low simply because the training and test
data are drawn from different distributions  one
is created automatically from our api and the
other from semeval  our dataset does not capture the true distribution of anger in news headlines 

initial results

here were our results for the svm  testing set is
semeval   n m refers to n angry headlines and m
non angry headlines used for training 
angry nonangry
        
         
         
         

precision
     
     
    
     

recall
     
     
     
    

   bag of words  we theorize that because bow
does not take into account ordering semantic
meaning  we classify a lot of non angry headlines
as angry because they contain the same words
as angry headlines  for example  one might be
angry at the headline taliban kills american
troops but not angry at american troops kill
taliban 

accuracy
   
     
     
     

here were our results with naive bayes  testing set
is semeval  
angry nonangry
        
         
         
         

 

precision
     
    
     
     

recall
     
     
    
     

   data  even though our initial results might
make it seem that having smaller data sizes is
better  we know that having more data is preferable as its leads to a larger lexicon and thus
more words in the testing data are being captured  furthermore we found that in the svm
on         only        of words in the test lexicon appeared in the training lexicon  meanwhile 
in naive bayes on          only        of words
in the test lexicon appeared in the training lexicon  after discussions with a ta  we learned
that most text classifications are done with massive corpuses that are greater than the number
of words in the english lexicon  this clearly is
a problem for us and is something to consider in
the future 

accuracy
     
     
     
     

error diagnostic and analysis

analysis for svm we experimented with data distributions for our training set  the svm worked best
with a           ratio  as ratios of angry to nonangry headlines increased  accuracy and recall went
up  while precision and recall went down  we performed further tests  not shown above  in which we
increased the size of our dataset while keeping to a
    distribution   but the precision remained relatively
consistent  hovering between            while recall
and accuracy decreased  when we performed an error

   naive bayes vs  svm  our naive bayes does
better than svm on accuracy and recall and
has comparable results on precision  the reason for this is because the angry headlines in our
dataset contain key words from our lexicon  because naive bayes assumes independence on all
 

fi 

features it found a strong correlation between appearances of these angry words and having anger
in the headline  in the svm however  these features are not considered independently 

with our distribution problem solved  we fed into our
algorithms a     ratio of angry to non angry headlines 
we randomly permuted our data set and ran cross validation   training on    percent of our set and testing on
   percent  we then averaged the results of    trials 
here were our results with svm doing cross validation 

   recall  naive bayes classifies less non angry
headlines as angry  we gave naive bayes a
dataset of          and it classified       of
non angry headlines as angry  we gave svm a
dataset of         and it classified       percent
of non angry headlines as angry  we performed
this error diagnostic on the distributions in which
each algorithm did best  we think these results
are due to our flawed distributions   the naive
bayes was given less angry headlines to begin
with and thus did not have a high tendency to
label non angry headlines as angry  the svm
had a flipped distribution and thus labeled more
non angry headlines as angry 

 

new results

angry nonangry
       
         
         
         

precision
     
     
     
     

recall
     
     
     
     

accuracy
    
  
     
     

here were our results with naive bayes doing cross
validation 
angry nonangry
       
         
         
         

precision
     
     
     
     

recall
     
     
     
     

accuracy
    
     
     
     

new dataset
 

we decided to create a new dataset from which we
would do cross validation  with this  we could ensure
that both our testing and training data came from the
same distribution 

analysis

after choosing to run cross validated data with the
same distribution  our numbers for the both classifiers
increased dramatically across the board  accuracy 
precision  and recall were above     in both cases 
and the performance of the both algorithms remained
consistent even as the size of the dataset was increased 
the recall of nave bayes was slightly higher than that
of the svm  by approximately        but in all other
aspects both approaches yielded comparable results 

we self labeled new york times headlines from
december  st      to december   th      for
anger nonanger  we also included headlines from
our api calls  but made sure to remove words from
our lexicon which we had originally used to query 
we did this because those words were obvious indicators of an angry headline  for example  we turned
our automatically generated headline berlusconi accused of bribing witnesses    into berlusconi of bribing
witnesses     lastly  we included semevals dataset
within this training set  our data pool thus had     
angry headlines and      non angry headlines 

analysis  we trained and tested on the same data
pool   this by itself improved the performance of our
algorithms as both training vectors and testing vectors
are drawn from the same distribution  also note that
when compared to our old data set  the recall for nave
bayes and svm was not as high  this is partly due to
our new dataset having an equal amount of angry and
non angry headlines  as opposed to having an excess of
one over the other  leading to over classification on one
of the two outcomes   we think this was also because
the automatically extracted portion of our data set
was stripped of angry keywords we used to query the
nytimes api  this prevented the nave bayes classifier from identifying the obvious correlations between
the appearance of angry keywords in headlines and the
headline being classified as angry  and thus made for a
less skewed training set  and leads to a drop in recall   

we conducted a random sampling of     headlines
from our automatic nytimes collection to see how
many were actually anger related  our parser removes
some headlines  but this is still a good estimate of the
error   from non angry headlines     were indicative
of anger  from our angry headlines      were indicative of non anger  while there is some degree of corruption in our automatically generated training set 
the majority are still fairly well labeled  note that
these corrupted headlines are supplemented with human annotated headlines from semeval and headlines
from the month of december 

  note that with api calls and parsing there may be some inaccuracies  jumbled words  missed headlines  etc   some headlines still
contained lexicon words 

 

fi 

experimental models

applied this model to a small data set   specifically
the first     angry and non angry headlines from semeval  
we made very significant improvements with synonyms  with a        increase in accuracy for naive
bayes  we had    increase in accuracy for svm 
co occurrence model one of the techniques we
saw in research papers looked at the correlation of
words with words representing an emotion  we wrote
a script to remove uninteresting words  prepositions 
the  be  etc  from headlines and then queried the
nytimes api for all the headlines between jan  st 
     and december   th       that contained a word
in the headline and the word anger  we scored each
word with the count of headlines from the api call and
summed the counts together  dividing it by the number
of words in the headline  for normalization   we added
this to our training matrix and ran svm and nb on
it  due to limited time  we only applied this model    
angry and non angry headlines from semeval 
we made moderately significant improvements
with the co occurrence model  with a       increase
in accuracy in naive bayes and    accuracy in svm 

we decided to experiment with models to fix some of
our recurring problems  lack of a large lexicon  limited
semantic analysis  and large dimensionality 
here were our results on svm with         from
our data set with pca and cross validation 
model
no pca
pca

precision
     
     

recall
     
     

accuracy
    
    

pca   we had more features than data points and
thus decided to run pca to see if it could determine
common subspaces which held training points  we
performed pca on         training set  and ran the
svm on the reduced training matrix  the number of
features we had in our feature vector had an avg  of
     features and after pca this number was reduced
to an average of     features  due to the sparseness of
our training matrix and incompleteness of our lexicon 
finding common subspaces for all the data points was
likely difficult  but the slight increase in recall and accuracy might be attributed to the elimination of noisy
data when making the reduced matrix 

  

here were our results on svm with         from
semeval with experimental models  described below 
  cross validation 
model
normal
synonym
co occurrence
synonym co o

precision
     
     
     
    

recall
     
     
     
     

we attempted to generate angry headlines by supplying a seed word to our program based on a markov
chain model  we took in a corpus of angry and nonangry headlines and kept track of all words that could
succeed or precede any given word  our output headlines were   words long and the seed word appeared
anywhere in the headline  the words preceding and
succeeding the seed word were randomly picked from
a list of generated successors predecessors 
one would think this could be modeled as a chain
factor graph where we assign words to maximize the
potentials   i e  the transition probability between  
words   but since the domain of the nodes  possible
words one could place in one of the   places on the
headline  is completely dependent on the value of the
node beside it  domain would be the list of possible successors predecessors of a given word   we cannot use a
constraint satisfaction problem  csp  solver to maximize the potential  instead we randomly generated
hundreds of headlines with the given seed word and
rank them in descending order by coherency  which is
the product of all the transition probabilities between
the words in the headlines   these headlines are then
classified as angry or not angry and returned for us to
judge  in this case  we used our naive bayes classifier
 trained on           from our data set  to determine
whether or not the headlines were angry 

accuracy
  
  
  
  

here were our results on naive bayes with        
from semeval with experimental models  described
below    cross validation 
model
normal
synonym
co occurrence
synonym coo

precision
     
     
     
    

recall
     
     
     
     

accuracy
  
     
     
    

synonym model with limited data  we barely cover
all similar words in the english lexicon  hence  we
decided to expand headlines with synonyms to be able
to capture more words across fewer headlines  unfortunately we were only able to expand     angry
headlines because of api call limits  but even with
this we showed significant improvements   note that
due to api call limits for synonym expansion  we only
  code

intelligent headline generation with markov chains

based on tutorial from  http   matlabdatamining blogspot com         principal components analysis html

 

fihere are some examples of generated headlines for
anger  seed words  rape or kill   prudery blasts kill
dozens beginnings asia have  temple terry blasts
kill dozens deception nelson  it simple in rape of
genocide scenes  rape problem solving fruit fleiss
theft of 
while the classifier is able to pinpoint headlines
containing words commonly found among angry headlines  its inability to look at word ordering or semantics
makes it classify nonsensical headlines as angry  another flaw is how the markov chain program measures
coherency between every pair of words instead of the
overall sentence 

  

of words as well as entire emotion synsets   kozareva
and navarro     also worked with semeval and used
mutual information across webpages on www to determine emotion   in general terms  they determined
whether particular words in headlines were found in
pages with particular words in a known emotion lexicon  this was similar to our co occurrence model 

  

please note that both of us are new to nlp and have
not taken a language processing class before  bharad
raghavan is in both cs    and cs    and is using this
for his final project for both classes  anshul samar is
in cs    
acknowledgements include  cs    class  cs   
notes  tas from cs    cs     stanford parsing
tools  classmates for debugging  matlabdatamining blogspot com their tutorial on pca  callback js
javascript class for making api calls  thesaurus
service was provided by words bighugelabs com  and
many online forums for resources 
lastly  note that originally we were going to classify
for all six emotions  and we did that for our milestone  
however  we decided that it would be best to focus on
anger as we spent time creating our own data set  and
doing so manually for all six would have been too much
given the time we had  
we
have
made
all
code
public
at
github com anshulsamar pathos 

conclusions and next steps

in conclusion  we found that our training set  distribution  and experimental models significantly contributed to our results and the success of this project 
whether our pre processing step and development of
a parser to stem and identify parts of headlines  collection of a data set through automatic nytimes api
scripts  usage of smv and naive bayes  and experimental models with synonyms  co occurrence  and
pca  we learned that many parts of the machine learning pipeline contribute to the final result   not just the
algorithm itself 
due to time and resources  we were not able to implement some of our ideas  here are some of them 
   run the experimental models on more data 
   get more features from nytimes including lead
paragraphs

references

   sort the words by importance  for example in
girl killed in egypt the most important word is
killed   we experimented with stanfords online
root word finder  however for the headline  rise
in deadly attacks on shiites in iraq stirs anger
at government it gave us rise as the root word 
we would need to find a better way of finding
critical words in headlines 

  

acknowledgements  notes to
staff

    zornitsa kozareva and borja navarro  ua zbsa 
a headline emotion classification through web information  proceedings of the  th          june         
     
    carlo strapparava and rada mihalcea  learning to
identify emotions in text  proceedings of the     
acm symposium on applied computing   sac    
page            

research

    wenbo wang 
lu chen 
krishnaprasad
thirunarayan  and amit p  sheth 
harnessing twitter big data for automatic emotion
identification 
     international conference
on privacy  security  risk and trust and     
international confernece on social computing 
pages         september      

we read quite a few papers  here were some of the ones
we read that were useful  yang et a      discussed crf
 conditional random field  to assist classification  it
locally searches for emotion in the vicinity of the sentence it is classifying  wang et al      automatically
collected data through twitter hashtags like  annoying or  nervous and applied pos and n grams  this
made us think about increasing our data set and using pos  strapparava and mihalceas paper     uses
semeval and applies latent semantic analysis which
determines similarities  and can also use synonyms

    changhua yang  kevin hsin yih lin  and hsinhsi chen  emotion classification using web blog
corpora  ieee wic acm international conference on web intelligence  wi     pages        
november      
 

fi