a case for ipca in financial forecasting
christopher fougner  ruoxi wang  michael dangelo

abstract
principle component analysis  pca  is used to reduce dimensionality and noise  while still preserving the majority of the variance
in the data  it however gives little guarantee on the predictive value
of the remaining data  this paper proposes an inverted principle
component analysis  ipca   to achieve dimensionality and noise
reduction  by removal of the largest principle components  in addition a three part decomposition of the financial markets is proposed and its validity is tested  by applying ipca we were able
remove the market component that is less predictable  while at the
same time keeping the components that carry most of the predictive
information  our method is applied to various regression models
including recurrent neural networks and an improvement of     is
observed 

 

refer to this procedure as inverted pca  ipca   we seek to verify this hypothesis through application to high frequency data of
the     largest  stocks by market capitalization  listed on the new
york stock exchange  nyse  

 

model of financial markets

we first introduce some notation  let x  rmn be the data
matrix  of returns  r t     where each column represents one stock 
and each row is the return of each individual stock at a specific time
t  where the row number corresponds to t 


t
 r   

t



 r   

x 
 


  



time series prediction in finance



a time series is a sequence of observations at successive points in
time  time series prediction consists of predicting future values of
a time series  given past history of the same series  in finance  the
 t 
 t  
return r t    p pp
is typical to viewed as the variable  where
 t  
 t 

time series prediction in finance is a constantly evolving field  the
proliferation of electronic trading means that an individual can invest in more instruments and markets than ever before  it is not
uncommon for an investor to maintain a portfolio of derivatives 
etfs  and equities in markets all over the world  this behavior
has made individual stocks and markets as a whole significantly
more correlated  the additional advancement of algorithmic trading has removed many of the inefficiencies in the markets  making
stock prices look increasingly noisy  whereas simple trend following strategies worked well in the   th century  increasingly sophisticated algorithms and trading systems are required to maintain an
edge over the market 
to deal with the large number of financial instruments and the high
degree of what can be classified as noise  both dimensionality reduction and filtering are necessary preprocessing steps  principle
component analysis  pca  has a long history of being used for
both these purposes in many fields of statistics including time series prediction  pca projects the data into the d dimensional subspace  that accounts for the most variability in the data  among all
spaces of dimension d  doing so  implicitly assumes that any noise
in the data either has low variance or is uncorrelated among the
different variables  in finance  neither of these assumptions can be
made  data can be extremely noisy and unpredictable events can
have highly correlated effects on the stock market  we hypothesize
that the directions of largest variation in the data in fact correspond
to directions of most noise and greatest unpredictability  if this is
true  then one would be better off removing the largest principle
components and keeping the smaller principle components  this
is opposite to the way pca is typically employed  accordingly we
 e mail ruoxi stanford edu
 e mail mdangelo stanford edu



next  we propose the following decomposition of the return of a
stock i 
 t 

 t 

mi

mi

   

p is the stock price at time t  the objective is to predict r    
t   given r          r t    frequently one has multiple stocks  in which
case each stock is viewed as a variable 

 e mail fougner stanford edu

t

r t  

 t 

 

i

 t 

 t 

ri



 t 

i

 

 t 

ri

 t 

si

si

figure    left  decomposition of one stock  right  our stock
model after market component removed by ipca
 t 

 t 

where ri is the return of the ith stock at time t  mi represents
 t 
the influence of the market  si represents a signal component and
 t 
i is noise  we make the following five assumptions
   s t  shows at least some correlation in time 
    t   n     d   where d is diagonal  we assume at each
point of time  the noise for each stock has zero mean and are
independent of each other  furthermore  we assume that the
noise is uncorrelated in time 
   m t   d    m    where d is some unknown distribution 
and the individual components are highly correlated at each
point in time  this leads to a near low rank assumption of covariance matrix m   in addition  we assume that m t  shows
little correlation in time and is very difficult  if not impossible to predict  we formalize this notion as being equivalent to
 m t     r        r t         this directly leads to the assumption that the market component has zero mean 

e

     cov s t    m t         cov s t    s t       in other words  the
leading direction  the main subspace  of the market must be
sufficiently different from any leading direction that the signal
may have  as shown in figure    
  seven

of these had to be removed due to missing data 
consistency with machine learning notation  we use x  rather
than r  to represent the data matrix 
  for

fi     cov s t    s t         cov m t    m t      

turns  the sharpe ratio  defined as

intuitively  this decomposition represents the notion that the movement of a single stock is mostly influenced by the movement of the
market as a whole  yet each stock has nuances that makes it somewhat predictable  a pictorial description of our model is shown in
figure   

sr  

rp  rf
p

where rp is the portfolio return  rf is the risk free rate and p is
the portfolio volatility  in more detail 
 rp is the percentage return of your portfolio if you follow a
policy   t         n   a policy simply states how heavily
your portfolio is invested in each stock at time t  it is frequently enforced that     t          which ensures that the
portfolio cannot be leveraged  mathematically 

since the market contributes most of the variance in our data  the
largest principle components of the data will primarily consist of
the subspace spanned by the eigenvectors of m   therefore by
removing the largest principle components we are able to remove
the market noise and restrict ourselves to a subspace consisting
of mainly signal and uncorrelated noise  the proposed model is
shown in figure    this is promising since the market is not easily
predictable while the signal is predictable if only marginally 

rp  

y
       t   r t      
t

 rf is the return your assets could achieve if they were invested in a risk free asset  this risk free asset is generally
taken to be the   month t bill   
sres   
sp

 p is the standard deviation of the returns that your policy
achieves 
  
m
m
  x  t     t   
  x  t   t 
 
 r 
p  

r
m t  
m  

mp
mres   

t   

the sharpe ratio quantifies the notion that increased performance
should come from smarter investments  rather than riskier investments 

figure    the crosses represent data points  pink vectors represents
the market movement  and the blue vectors represents the noise and
signals  subscript p indicates that the subscripted variable moves
principally in the indicated subspace  subscript res indicates residual movement in the indicated direction

 

the space of possible policies   t  is large  portfolio optimization
is even a field in itself  since  the focus of this report is not on optimizing returns  but rather on evaluating ipca  we choose a simple
policy 
 
  t    sign r t   
n
where n is the number of instruments in your portfolio and r t  is
the predicted return at time t  recall that r t  can be a function of
r           r t    

metrics
 

to evaluate whether financial forecasting benefits from ipca  we
must formalize a metric  by which performance can be measured 
rmse is typically used in machine learning  however it is poorly
suited for financial forecasting  typically predicting the exact return of a stock is extremely difficult  and in most cases it is sufficient that the sign of the return is predicted correctly  in other
words  an algorithm  which can correctly predict whether a stock
will go up or down  is very valuable  likewise  an algorithm which
correctly predicts when there will be large swings in the market  but
generates wildly incorrect predictions the rest of the time  can still
be a very useful algorithm  in lieu of reporting high rmse  many
research papers resort to reporting accuracy metrics that are a function of the predicted price p t        r t   p t     such metrics are
extremely misleading  because the price tends to show small fluctuations  which leads to very low reported errors  without actually
being indicative of performance  in many cases  simply predicting
p t    p t   will result in low reported errors 
rather than focusing on error metrics  we instead direct our attention to how well a model would perform if implemented in practice 
to do so  we use the industry standard metric for risk adjusted re 

regression models

since ipca is a pre processing step  we also need a regression
model to be able to make any observations about performance improvements  consider the general form of linear regression 
y   g x    
where y is the response variable  g is an arbitrary function   is an
unknown parameter  and  is a noise with zero mean  finding the 
can be posed as an optimization problem 
min   y  g x   pp       qq


where     q is a penalization term to avoid over fitting  the degree of regularization is governed by   r  different choices of
g  p and q lead to different regression models  we will focus on the
case when p         regression   for the following reasons 
  rather than using the actual risk free rate  which was near zero at the
time of writing this report   we use the market return over the period   th of
april to november   th  which was close to     

fi optimization is computationally inexpensive  which means
the model can be retrained frequently and it can be applied
to many datasets  this is important when dealing with large
financial datasets consisting of large numbers of instruments
and time series that can span decades 
 decay can easily be applied to the training data by a weighted
least squares extension  this allows developments of newer
market dynamics to be more prominent 
 the least squares objective  not only attempts to find an r 
r  but also the r that minimizes the variance of the errors 
this follows because e   r  r        p    under the standard
statistical assumption that errors follow a normal distribution
with zero mean  we see that least squares attempts to decrease
p and increase rp   and by implication increases the sharpe
ratio 
the regularization term prevents over fitting by forcing the q norm
of  to be small  the most common choices are q     and q     
choosing q     encourages  to be sparse  which can be viewed
as a method to perform feature selection  on the other hand  p    
is unlikely to set any element of  to zero  but it is the optimal
choice assuming a bayesian prior on   the case p      q     is
commonly known as lasso and the case p      q     is known as
ridge regression 
various functions g can be chosen  but we have opted to look at
two specific ones in this report  the first is simply a linear function
g x    x  the second function is a randomized approach to
recurrent neural networks  rnns   known as the echo state network
 esn   esns relies on sparsely connected neurons  where sparsity
and edge weights are chosen at random  in essence it is a method
to generate a highly non linear function with time dynamics from a
time series matrix x  a diagram of an esn is shown in figure   

row independently  instead the dependence of g
follows 
    
g r  
g r      r     

gesn  x    g r      r      r     

  
 

can be viewed as






the advantage of such a recurrent function is that if the stock returns show a non linear dependence on previous returns  then such
a function may be able to capture part of the dynamics 
to summarize  we compare the ipca applied to the following
 g x    x with    and    regularization 
 g x    gesn  x  with    and    regularization 
we also use a re training method  whereby the x is initially split
into three component xtrain   xtest and xrest   time wise  all observations in xtrain strictly precede those in xtest   which strictly
precede those in xrest  order of the data points is maintained   the
model is then trained on xtrain and tested on xtest   thereafter
xtest and xtrain are combined to form a new xtrain   lastly
xrest is split into xtest and xrest   this process is continued until
xrest is empty 

 

data

for the empirical evaluation of ipca we chose to analyze the    
largest companies by market capitalization on the nyse  stock
prices at one minute intervals were obtained from bloomberg for
the period april   th to november   th       which corresponds
to approximately       data points  we chose to use data at one
minute intervals because there is surprisingly little academic literature which mentions it and much of algorithmic trading is done at
high frequency  bloomberg is the industry standard data provider
for financial data  so we can be confident that the data we are using
is clean 
as an input to the various regression models  we simply used the
first m    time points of the data matrix  and as the output we used
the data matrix shifted by one  using m atlab notation 
x   bb returns   end     
y   bb returns   end   

figure    echo state network architecture

mathematically the esn function  gesn   can be defined recursively
as 
x n        f  w in u n        w x n    w f b y n  
y n        w out     u n       x n      
  u    x    
 
gesn  x    

 

u   
  
 
u n 

x    


x n 

where u n      is the input  and hence synonymous with r t    
x n  is the reservoir neuron activations  y n  is the network output
and f is a non linear activation function  win   w  wout and wf b
are weight matrices for input  internal connections  output and feedback respectively  f is typically chosen to be a sigmoid function 
such as the logistic function  we notice that g does not act on each

in section    we made the assumption      that part of our data
shows correlation in time  to understand the degree to which this
assumption can be justified  the autocorrelation of each stock return was computed and indeed  the   lag autocorrelation was statistically significantly different from zero  an excerpt of these correlations can be seen in figure   
we also made the assumptions     and     that the the largest principle component s  consist of market movements m t  and that the
market movement has significantly larger variance than either the
signal s t  or noise  t    two very interesting observations were
made  which both greatly support this claim 
 the principle direction v    n    where   is a vector consisting of all ones  the projection of x into the subspace
spanned by v    simply averages the columns  stocks  of x 
this averaging can naturally be viewed as extracting the underling market direction  additionally we see that the corresponding eigenvalue is significantly larger than the rest  in
fact accounting for     of the variance in the data  figure    
both these observations support our model 

fige us equity

autocorrelation

autocorrelation

   

    
 
    
   
 

 

  

  

    
 
    
   
 

  

 

lag
intc us equity

  

  

regression model

   
autocorrelation

autocorrelation

  
lag
t us equity

   
    
 
    
   
 

 v satisfies x t x   v v t    various values for b were tested 
and the optimal value b was reported  the results can be found in
table     the baseline performance column shows how well the
model performed when simply predicting y using x and the ipca
performance shows the results after applying ipca 

goog us equity

   

 

  

  

  

    
 
    
   
 

 

  

lag

  

  

ridge
lasso
esn w  ridge
esn w  lasso

baseline
 sr 
     
     
     
     

ipca performance
 b   sr 
           
          
           
          

improvement
   
   
  
   

lag

figure    autocorrelation of stock return for a selection of   companies  each lag is equal to one minute 

 the contribution of each stock to the second principle direction v  were ordered from most negative  to most positive  of
these  the top and bottom    stocks were extracted  the results
can be seen in table    we see that the subspace spanned by
v  is essentially the difference between the average movement
of the oil  gas   coal industry and the average movement of
the consumer products industry  such movements can clearly
be classified as market movements  further justifying our assumptions 

table    comparisons of best performances among different models 
performance improved across the board when applying ipca  most
notably when training with lasso and applying ipca  the sharpe
ratio shot up by      to achieve this performance improvement 
only the very first principle component had to be removed  the
cumulative portfolio return can be seen in figure    

eigenvalue decay of stock return covariance matrix
 

normalized eigenvalue

  

 

  

 

  

 

 

  

  

  

  

  

index

figure    eigenvalue decay of x t x  the eigenvalues are normalized using the largest eigenvalue 
positive contrib 
chevron
occidental petroleum
freeport mcmoran
conocophillips
schlumberger
apache
anadarko petroleum
eog resources
halliburton
national oilwell varco

industry
oil  gas   coal
oil  gas   coal
metals   mining
oil  gas   coal
oil  gas   coal
oil  gas   coal
oil  gas   coal
oil  gas   coal
oil  gas   coal
oil  gas   coal

negative contrib 
pepsico
altria group
philip morris
coca cola
procter   gamble
colgate palmolive
eli lilly   co
wal mart
merck   co
abbott lab 

industry
consumer prod 
consumer prod 
consumer prod 
consumer prod 
consumer prod 
consumer prod 
biotech   pharma 
retail staples
biotech   pharma 
medical equip 

table    stocks contributing most positively and most negatively to
the second principle component of x 

 

results

to test the performance of ipca  we implemented the four regression models outlined in section    these models were then tested
on the whole dataset  x  y   and the dataset  x  y      x v   y v   
where v is obtained by removing the b largest eigenvectors from v

figure    cumulative market adjusted return when applying ipca
to minute data and training using lasso  notice that no returns
are available for the period   th of may to   th of june  since this
period was used for training 
the effect of annealing eigenvalues can be seen in figure     we
make two additional observations
 when removing smaller principle components  the performance immediately worsens  implying that small principle
components do not correspond to noise  this even suggests
that adding more stocks to the model could improve performance 
 removing principle components   through    have negligible
impact on the sharpe ratio  suggesting that these components
dont add much in terms of predictive value 
intuitively  since removing components   through    doesnt impact performance significantly  one would expect that the corresponding rows in  are reasonably close to zero  when  x  y   are
obtained by removing the single largest principle component from
  these

account 

results do not take trading costs  bid ask spread or slippage into

firegression model

influence of annealing smallest largest eigenvalues on sharpe ratio

  
anneal smallest
anneal largest
complete spectrum

  
  

sharpe ratio

  

lasso ridge
esn w  ridge
esn w  lasso

ipca column wise regression
 sr 
     
     
     

improvement
  
    
    

  

table    columnwise regression of x onto y   note that when
performing column wise regression lasso and ridge regression are
the same 

  
  
  
 
 

 

  

  

  

  

  

  

  

  

  

number of smallest largest eigenvalues annealed

figure    effect on sharpe ratio of annealing largest and smallest
principle components of x 

improvement   but we also showed that it was possible to train each
column of x independently  which reduces the computational cost
and yields even better performance   furthermore  we find that using ipca as a pre processing step improves the performance of all
the models we tested  among which lasso performed the best 

 
 x  y     this hypothesis was justified by looking at the magnitude of the entries in the  matrix  figure     we quite clearly
see that the rows corresponding to the largest principle components
 the last rows  contain only very few elements that are different
from zero  surprisingly  we also see that  is nearly diagonal  this
indicates that the cross correlation matrix of x and y is nearly
diagonal  in other words  the correlation between two columns
xi  col x   yj  col y   where i    j  is zero  if this is the case 
then regressing each column of y onto each column of x independently should yield better performance  since there is less noise  
additionally it is much less computationally intensive to perform
linear regression of   variable onto   variable  n times  than it is to
perform linear regression once of n variables onto n variables 

future work

we would like to see how well this model generalizes to time series
at lower frequencies  such as hourly or daily data 

references
jaeger   h        the echo state approach to analysing and
training recurrent neural networks with an erratum note  bonn 
germany  german national research center for information
technology gmd technical report     
m edsker   l   and jain   l  c        recurrent neural networks 
design and applications  crc press 
pascanu   r   m ikolov  t   and b engio   y        on the difficulty of training recurrent neural networks  tech  rep   technical
report 

figure    maginutde of lasso coefficients when regressing the
columns of y onto the columns of x  ipca was used to remove the
largest principle component  higher indices correspond to larger
principle components 
by performing column wise regression of x onto y   we were able
to increase the sharpe ratio by at least        we also see that
esn with lasso overtook plain lasso  indicating that there are nonlinearities in the data that lasso could not account for 

 

conclusion

we see that by applying ipca to financial forecasting  we are not
only able to increase the sharpe ratio from       to        a    

fi