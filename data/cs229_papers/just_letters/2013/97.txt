string regularization
tamara andrade
december         

 

introduction

patterns in textual data are easy for humans to spot  humans are good at ignoring small permutations to focus
on larger similarities  for example  the phrases example   and exampl e   are easily recognized as being
approximately the same by humans  however  computers struggle to see these larger patterns  example   and
exampl e   read quite differently to a computer  just as example   and example   would 
this is a big problem when it comes to analyzing databases  a researcher would want the computer to know
example   and exampl e   are the same entity  but the computer treats them as distinct  researchers have
two choices  either they can ignore these problems and essentially throw out some information  or  if their data is
not large enough to safely discard this information  they can manually clean the data  a lot of researchers spend
a lot of time and money cleaning  or hiring someone to clean  their data 
my project aims to help researchers spend more time analyzing their data and less time cleaning it  to
this end  it aims to automatically identify typos and suggest corrections when it has been given a list of strings 
this project is aimed at medium sized databases  those that are too large to easily clean by hand but too small to
analyze properly without cleaning 

 

choosing the algorithm

at first brush  this would seem like an unsupervised learning problem because there is no training data  in that
case  the problem is one of clustering  the goal is to group words together and then extract the right meaning  if
this were the case  k means and expectation maximization  em  would be the right algorithms to choose 
however  these approaches have a major flaw for this application  while clustering is easy based on numerical
value    and   are both closer to   than they are to      the approach is much slower when it comes to text 
measures of textual distance like levenshtein distance  exist  but they are valid largely only when comparing two
strings  this means that clustering strings requires an enormous number of calculations between all the strings in
the database 
i chose to go with a different approach  instead of using an unsupervised algorithm  i tweaked an algorithm
from supervised learning  naive bayes  so that it would work in an unsupervised setting  the main advantage
to this approach was processing time  naive bayes still requires the comparison of a large number of strings  but
it can be adjusted to make fewer comparisons without throwing out any relevant information 
naive bayes as a choice makes sense if you think of the problem as not one of clustering  but fundamentally one of spelling correction  most commercial spell correctors use some kind of naive bayes  they solve the
following problem 


pr  candidate word   what typed     argmax pr  what typed  candidate word    pr  candidate word  
  levenshtein distance is a count of the number of transformations  either addition  subtraction  or transposition  needed to change
one word into another  this knowledge comes from cs    

 

fithe difference is that most spellcheckers have to be trained on a large collection of text  a corpora  they derive
some model of english  from which they know which words are more likely to appear  and a model of common
typos  from which they get the probability that you would have typed what you did given a candidate word  

 

my algorithm

i apply naive bayes to an unsupervised setting by developing a model of these probabilities based on the data  i
assigned the probabilities as follows 
count of times word observed
size of the data set
 
pr  what typed   candidate word    
    levenshteindistance  what typed   candidate word  
pr  candidate word    

the chief advantage to this approach is that the comparison in terms of levenshtein distance only needs to be done
between unique words  no information is being lost because word counts is being stored separately and entering
into the equation through the probability put on different candidate words   in contrast  you would not want to
take the unique instances of the data in clustering because it would increase error  the difference in runtime on my
machine is startling  on one dataset  k means was taking    minutes to run  implementing this version of naive
bayes reduced it to less than   minutes 
there are some weaknesses to this approach  one might think there are some times when word count will
be less informative  for example  if the data was naturally skewed so that the true observations appeared at
different rates  you should rely less on word counts  for this reason  i added a weight to these two probabilities
 set by the user   so that they can decide for themselves how much they want to rely on one kind of probability
versus another 
another way to think through this would be to add some kind of laplace smoothing parameter  initially 
i tested a few instances of laplace smoothing and found that weights did a better job on my two datasets  however  with more time  i will be testing these different parameters more rigorously on a wider variety of datasets
than i was able to do during this quarter 
the other design choice was how to penalize edit distance  conventional spellcheckers use information about
common typos  for example  replacing m with n would intuitively seem to occur more often than replacing
m with q  this occurs both because n is closer to m on the keyboard  looks more alike  and makes a sound
closer to m than q  however  i was reluctant to take this approach  i wanted to be more agnostic about the
sources of errors  most common spellcheckers encounter errors of only one type  people mean to type one thing
and they type another  that same error generating process might not be true for many datasets  however  for
example  someone could alternate between writing mr  fox and mister fox  this is wrong in the sense that
the machine cannot recognize these two entities as the same  but it does not follow the same pattern as typos  i
therefore settled upon the above formula as something that would penalize a smaller number of mistakes more  but
then taper off as the number got larger  the difference between   and   typos is not big  but the difference between
  and   is  
in deriving this probability  i thought about it in terms of a penalty function  a function that would assign less probability to candidate words that required more edits to get to the word  however  as was pointed out
to me in the poster presentation  this is not actually a true probability in that the total does not sum to one  in
the future  i will think about different ways to conceptualize this variable and see the effect it has on performance 

 

the data

i wanted to ensure that my algorithm was checked on data that had very different distributions in terms of the true
number of types  to see how the data performed there  as well as a number of different error generating processes 
  once

again  thanks to cs    for this information about spellcheckers 
candidate words are all unique words in the dataset 
  the weights are complementary  the user sets a number between   and   and this is subtracted from   to get the other weight 
  these

 

fii created data sets to test both of these kinds of models  the first was a list of the states in the united
states  each typed    times  typos were not corrected  therefore  odd spellings like alalmba are present in the
data  this dataset allows me to see how the algorithm works when the errors are at random and when the true
distribution of types is roughly equivalent 
the second was a dataset generated from historical transcripts of the british parliament  these records were
taken from online records  and the errors represent the inconsistent spellings present in any historical records 
chancellor of the exchequer gets spelled chanceller of the exchequer  chancellor of exchequer  etc  since
these data were based on the number of times individuals spoke in parliament  the distribution is highly skewed 
the chancellor of the exchequer speaks a lot  some quieter members appear only once or twice in the dataset 

 

performance

it is not exactly surprising  given the more challenging nature of the parliamentary data  that it was easier to
generate higher correction rates for the state data 

   
   
   

   

   

fraction correct

   
   
   

   

fraction correct

   

   

parliament data

   

states data

   

   

   

   

   

   

   

weight on number of times a word occurs

   

   

   

   

   

weight on number of times a word occurs

it was reassuring  however  to see both very high rates of success in the states data and to still see some gains
in performance using this approach even on the challenging data 
i was expecting the alpha to skew closer to     for optimizing performance on the state data  but it was actually significantly to the left  this would seem to imply that a better model is needed  as the best performance still
needs to rely more on one part of the probability 

 

limits of this project

when designing this approach  i was cognizant of some limitations  first  this algorithm can only generate corrections that are actually present in the data  for example  the user might desire mr  fox  to be changed to mr 
fox  however  if the only options in the data are mr  fox  or mr  fox   the algorithm should standardize
both cases to whichever occurs more often 
the other major limitation is that the program will not work if there are no correct spellings and every
misspelling occurs at the same rate  this  however  seems like an acceptable problem  since this is clearly a case
where some outside knowledge of the data would be needed regardless  for example  if someone had to choose the
correct spelling and only saw shanghay  shunghai  and shenghai one time each  the best value could only
be chosen by someone with knowledge of major cities in china 

 

future work

there are a few important areas to work on in future work  i will focus on three 
 

fi testing on a wider variety of data  i spent most of the time this quarter collecting and cleaning the
data and working through which algorithm to select  the result was that i did not get to test on as much as
data as i would have liked  one way to see how robust these specifications i have chosen are is to bring in a
wider variety of data 
 make changes to improve accuracy  once i have a wider variety of data  i can begin to analyze different
ways to improve the accuracy  for example  i can get a better sense of how much laplace smoothing will
help me in different contexts 
 make a user interface for this data  after i have a sense for how well the algorithm works  i want to
distribute this to a larger public  i think it has a lot of applications in my field of interest  political science 
where people have a lot of messy data and sometimes lack the programming skills to clean their data in the
most efficient way possible  therefore  i want to build a website where data can be uploaded  analyzed  and
then the guesses will be returned for the user to approve or decline  this will result in helping avoid errors
where the machine makes a wrong guess  i am envisioning an interface where the machine gives a summary
of its suggested changes  ranked from least sure to most sure  the user would accept changes up to a point
where the algorithm has done so well that she is confident in the remaining changes 

 

fi