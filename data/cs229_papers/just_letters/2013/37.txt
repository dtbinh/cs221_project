cs    project report
polyphonic piano transcription

mohammad sadegh ebrahimi
stanford university

sadegh stanford edu

jean baptiste boin
stanford university

jbboin stanford edu

   introduction

as our ground truth 

in this project we want to employ machine learning
algorithms to extract the notes that are played in a
polyphonic piano song  there has been a lot of research on music transcription recently  but most of
them are aimed at monophonic identification  in this
project  we looked at the problem in a more general way and tried to improve the performance using different techniques     one of the significant differences between using a monophonic and polyphonic
song is that in polyphonic identification we cannot use
the information that at most one note is playing  so
techniques using multiclass classifiers are not applicable  depending of how we apply our algorithm  we
found that there was a trade off between sensitivity
and specificity as we will cover in this paper  eventually we tested our system by playing back those extracted notes by piano and then recognize that music
with human ear  although there is still a lot to do for
this subject  the primary results were quite impressive
and promising 

one famous dataset of polyphonic piano songs is
maps     so we also decided to use it in this project 
from the maps package we chose    songs in the
maps enstdkam   and maps sptkbgam   folders 

   dataset and preprocessing
first we needed some polyphonic piano songs in a
sound file  wav  along with the corresponding list of
notes so that we could use supervised learning algorithms for classification  one good option is to use
midi files which contain the information about all the
notes played in a song  their pitch  the exact timing
when they are played  their duration and even their
velocity  although we did not use this last item  using a soundfont associated to an instrument  in the
rest of the text we used the same piano soundfont  
it is then easy to produce the wav file corresponding
to these notes and to train the algorithm based on
that  this rendered wav file is used as our observations  and the information contained in the midi file

we also used ken schuttes matlab package to work
with midi files      this package takes a midi file and
parses it so that we can have access to the notes in
a more friendly way  we produce the feature vectors
by slicing the song wave into     ms intervals and for
each interval we take the fft  this makes sense because the pitch of a note is highly correlated with its
frequency  so we would expect the fft to give us more
information on the pitch than the actual signal  for
our learning algorithm we actually do not care how
loud or low is the note played  so we can normalize
the energy spectrum      for that matter we take the
fft of each     ms section and then calculate the
norm of the fft vector  for normalizing we need to
divide this value by the sum of the vector elements 
but there are many small elements in the energy spectrum that actually do not matter and only clutter the
data  so we apply a threshold equal to     of the
maximum value of energy spectrum and then we normalize it to one  the figure   shows the raw feature
vector and the processed one  the sampling rate is
equal to      khz 

figure    thresholding applied to the feature vectors

fipolyphonic piano transcription

   problems of dimensionality   pca
now that we have a full dataset  we can start processing the data  the basic idea to solve our problem is
to view it as a classification problem  each segment of
    ms has a certain number of notes that are playing
during that interval  thus  we can see the output as
binary with respect to one note   either this note is
played  or this note is not played  if we train a binary
classifier for each note  we should be able to tell for
each segment if this note is played or not  putting
the information for each of the classifiers together  we
could tell the list of notes that are played during that
interval 
the first problem that we have here is that we have a
really huge amount of data  we use half of the total
data as our training dataset and it already includes
more than       intervals of     ms  moreover  our
feature space is also very large  given that we had
     coefficients after applying the fft to each segment  our space is r       it means that we have to
deal with very large matrices and that most of the
classification methods that we know will not work as
such  this is why the first step that we will apply to
our data is a pca  so that we can drastically decrease
the dimensionality of our feature space 
we run that pca on a randomly sampled subset of
      examples  this number was chosen because it
makes the pca run quite quickly  in just a couple of
minutes  while still being representative of the whole
subspace  the justification behind the pca is that
since there are only a limited number of notes  we could
expect the data to lie on a much smaller dimensional
space than the initial one 

figure    singular values of the data matrix

algorithm  the next steps are described later  for the
   notes that are played the most  as the number
of features grows  the sensitivity increases too  which
makes sense  but we can see that we get diminishing
results and after around         features  there is not
much improvement anymore  we get exactly the same
kind of plots when we look at the specificity  we fixed
our number of features used to      the dimensions
associated to the     largest singular values   which
still gave a good increase in speed while training our
algorithm without decreasing the performance 

figure   shows the log magnitude of the singular values of the data matrix used for the pca  sorted decreasingly  as we can see  there is a jump after    
singular values  which verifies our assumption that the
feature vectors lie on a smaller subspace 
we restrict our space to no more than a few hundred
dimensions  which allows us to run logistic regression 
for the logistic regression  we can then apply newtons
method  which reaches convergence after very few iterations  the reason behind that is that because of
the pca  the hessian matrix is still small enough to
be inverted fast  and running a gradient descent was
much slower 
in later versions of our algorithm  we noticed that all
    dimensions were not needed and we settled for a
choice of     features kept after the pca  this is an
empirically supported choice as we can see in figure
   where we plotted the sensitivity of a run our full

figure    sensitivity of the    most played notes with different dimensions of feature space

   dealing with unbalanced sets
if we look at figure    which shows the appearance frequency of the notes in the data that we set aside for
training  we can see that all the notes are not equiprobable  and the notes in the medium range appear more
often than the lower and higher notes  some of the
lowest and highest of the midi range did not even appear in our data  or very rarely  this fact may seem
obvious but it means that we cannot deal with the

fipolyphonic piano transcription

notes the same way if we want accurate predictions 
even the most frequent note only occurs in     of the
samples  so if we do not try to correct the balance  our
classification may be biased towards negative examples  and this may considerably decrease our sensitivity  since many positive examples will be misclassified 

figure    the boundary will be shifted depending if we use
all the negative examples or only a fraction of them  subsampling 

figure    appearance frequency of the notes in the half of
the dataset used for training

the problem of unbalanced datasets has many applications and has been studied many times in the litterature  there are two basic ways to deal with this problem   sub sampling and over sampling     if we have
many more negative examples than positive examples 
sub sampling implies that we construct our training
set by taking all the positive examples  but by sampling only a fraction of negative examples  in oversampling  we take all our negative examples and we
add several instances of the positive examples to balance the training set  these two methods have been
shown to be asymptotically equivalent  in our case 
sub sampling  illustrated figure    looks more appealing because we already have a large amount of data 
so it is not a problem to reduce it by sub sampling the
negative examples 
using this technique  we can construct a different
training set for each note  in which that note will
be present in a fixed ratio of training examples  this
is very useful for getting comparable performances for
different notes  which was not the case before   our
performances dropped as the note became less frequent
in the training data  by adjusting the ratio of positive examples  we will see in our next section that our
algorithm can perform differently  also  we chose to
address only the notes that appeared more than     
times in the totality of our training data  because we
do not have enough information about the other notes 
this only rules out    of the notes that appear in our
dataset  in terms of number of intervals   which we decided was negligible in our application  it is important

to note that this limitation could easily be avoided if
we had more data for these notes that appear less frequently 

   two different approaches in
sub sampling
     standard method
the first approach that we decided to take was to use
sub sampling at a low effect setting   we just used
it to equalize the ratio of positive examples in each
of the training sets corresponding to the notes  more
explicitly  since the most frequent note appears in    
of the intervals  we can use this ratio for the other notes
so that they also appear in     of the intervals of their
training sets  the expected advantages of this method
is that we still keep many negative examples so we still
expect to have a high specificity  the inconvenient is
that     is still quite low  and our classifiers may be
biased towards negative examples  which will decrease
the sensitivity 
in practice  we observe exactly these effects on our
testing set  as we can see qualitatively on the pianoroll corresponding to this method applied to a small
part of our testing set  figure  b   to assess the performance quantitatively  we use specificity and sensitivity because they are good quantities for understanding
how well we classify negative examples  specificity  or
positive examples  sensitivity   this will be used consistently for the rest of the report  for this standard
method  we get a specificity of        and a sensitivity
of        

fipolyphonic piano transcription

that minimizes the number of transitions  this corresponds to the multi objective problem of minimizing
j   kx  x  k     kdxk  
where d is the square matrix with   on its diagonal
and   on its upper second diagonal so that dx returns
the difference of consecutive elements of x  and  is
a parameter that chooses the relative weight between
our two objectives 

figure    data put in piano roll shape   each line corresponds to the timeline of one note and each pixel on the
horizontal axis corresponds to a different interval of    
ms  a white pixel symbolizes the presence of that note
in the interval while a black pixel symbolizes its absence 
these piano rolls correspond to the same part of a sound
file and are respectively  from top to bottom    a  reference data  ground truth     b  output of the standard
method    c  output of the conservative method without
post processing    d  output of the conservative method
with post processing 

     conservative method
a second approach is to use sub sampling at a higher
effect setting   we push the ratio of positive examples
higher  at     instead of      this means that we will
tend to label more intervals as positive  so we will get a
higher number of false positives  lower specificity   but
also a lower number of false negatives  which means
the sensitivity will be improved  if we look at the
piano roll of this method  figure  c   we can see that
this conservative approach makes the output very cluttered  which confirms our intuition 
up to now  we have only treated the feature vectors
corresponding to each interval as independent  and we
can expect that adding a constraint on consecutive intervals may give us better results  this is what we
attempt with the output of this method  as a postprocessing step  we call x   rn the binary vector
corresponding to the intermediate  cluttered  result
for one note  one line of the piano roll   we want to
find the binary vector x that is close enough to x  but

the norm should be the l  norm but it is equivalent to
the squared l  norm since our vectors only have values
in            this problem is not easy to solve if we
constrain x to a binary vector but we can solve this
problem easily by relaxation   we solve in rn   and then
we threshold to get a binary result  we can even solve
it very fast if we consider x as a circular vector because
in that case we can express our problem as h  x   x 
where h is a n dimensional vector depending on 
and  is the circular convolution  and then finding x is
easy by taking the fft of the expression above 
figure  d shows the result of this post processing step
using  c as the input  for          we can see that
we can get rid of many of the isolated false positives 
while still keeping the true positives  on figure   we
show the mean specificity and sensitivity associated to
different values of   for very low values of   the postprocessing step has no action  for very high values
of   transitions are strongly penalized and the best
move is to take a zero x  in between  there seems to
be an optimum for log                where the sensitivity slightly decreases but the specificity decreases
considerably  this is the zone where we remove the
outliers but not too many of the true positives  and so
we take a value in this interval for our postprocessing
step  after confirmation by testing and verifying we
chose log            

figure    sensitivity  left  and specificity  right  for different values of   logarithmic scale  

for this method  we get a specificity of         comparable to the standard method  but an improved sensitivity of         in the end  this method seems to

fipolyphonic piano transcription

be more promising if we want to emphasize on having
a higher sensitivity whithout sacrificing specificity 

   generalized method and final results
in fact  by choosing a different value for the ratio of
true positives when we sub sample  we can get different performances  if we prefer to sacrifice sensitivity
in order to have fewer false positives  another option is
to use a lower ratio  like the one we used in the standard method and to apply the post processing  this
gives a specificity as high as         but a sensitivity
of        only  in fact  by tuning this ratio as a parameter  we can span a whole trade off curve given in
figure   
we can see here that the post processing gives in fact
better results in both sensitivity and specificity  if we
adjust the ratio of sub sampling accordingly  the
points corresponding to the two methods discussed in
the previous part are circled 

tion to classification or error measurement  because
of our time limitations  we could not try as many of
our ideas as we would have wanted for each section
of the algorithm and we focused on having a full algorithm working  different ideas that we could have
added to make our algorithm even better included  
trying a different classification algorithm  like a svm
  improving our time analysis based on the fact that
notes are usually played on a certain tempo   using a
prior probability on the appearance of the notes using
the key used in the song   etc  however  on the whole 
the output of our algorithm sounded very similar to
the original music  and we were quite happy with the
results that we got 

references
    n  boulanger lewandowski  y  bengio and p 
vincent  modeling temporal dependencies in
high dimensional sequences 
application to
polyphonic music generation and transcription 
icml       
    v  emiya  r  badeau and b  david  multipitch
estimation of piano sounds using a new probabilistic spectral smoothness principle  ieee
transactions on audio  speech  and language
processing  vol      no     pp                  
    http   www kenschutte com midi

figure    trade off curve of our two performance measures
by taking different ratios in the sub sampling step  from
     point on the right of each curve  to      point on
the left 

qualitatively  when we listened to our actual results 
we found that it is better for the ear to be on the
higher specificity   lower sensitivity side of the tradeoff curve   getting rid of false positives is much more
important than recovering all the notes in their full
length because our brain can easily reconstruct the
missing parts  while the outliers can be heard very
easily 

   conclusion
it was interesting to develop a full processing pipeline
for this algorithm  because we could deal with many
different aspects of machine learning  from data selec 

    j  nam  j  ngiam and h  lee  a classificationbased polyphonic piano transcription approach
using learned feature representations  ismir 
pp                
    h  he and e  garcia  learning from imbalanced
data  ieee transactions on knowledge and
data engineering  pp                  

fi