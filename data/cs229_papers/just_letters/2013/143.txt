   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

parity and predictability in the national football league
final write up
siddharth patel

abstract
predicting the outcome of national football
league games is a staple of american sports
culture and a significant component of the
multi billion dollar sports betting industry 
the nfl prides itself on the perception of
parity  the idea that any team can beat any
other on any given sunday  in this paper 
i apply machine learning techniques to team
performance statistics in order to predict the
outcome of games  i seek to determine with
what accuracy the outcome can be predicted
and how those predictions would perform
against sports book betting odds  after tuning a random forest classification tree on the
last ten years worth of nfl games  i find
that i can predict the outcomes of games in
the      season with     accuracy  i cannot
consistently make money against las vegas
sports book odds 

   introduction
a sports league is said to have parity when most teams
in the league have similar levels of talent and are
equally likely to do well in a season or to beat each
other in head to head matchups  parity is important
to the reputation of the nfl  and this commitment
manifests itself where it counts   in the money  the
nfl has a hard salary cap  so no one team can just buy
up a bunch of talented players  in addition  the nfl
pools and redistributes the massive television revenue 
ticket sales  and merchandise sales relatively evenly to
all    teams  this shared revenue system is meant to
hinder any one team from establishing a wealthy and
winning dynasty  the nfl does not want the new
york yankees model of sports teams  the perception
of parity has significant advantages for the nfl  fans
pay attention for most of the regular season because
preliminary work  under review by the international conference on machine learning  icml   do not distribute 

they believe that their team  with the right breaks of
luck here and there  may win enough games to get into
the playoffs  or may defeat the best team in the league
in a key matchup  this sense of unpredictability  the
belief that one standout performance from one player
or one big break on one key play can make all the difference  is the bedrock of the excitement generated by
nfl games 
that said  some teams are obviously better than others  as demonstrated through their win loss records 
team statistics  and playoff performance  this information is readily available and remarkably complete 
my hypothesis is that machine learning techniques can
be applied to this dataset to accurately predict the
winner of a given football game  other cs    students have already given this a try   hamadani  sierra 
specifically  to predict the outcome of a regular season
game  i will use the team performance statistics from
all of the games in the prior weeks as a training set  as
the season progresses  the machine learning algorithm
should be able to more accurately predict who will win
any particular game  i do not believe that any given
game is truly a       coin toss  so i want to compare
my prediction accuracy to some other measure   and
sports betting odds provide a great reference point 
the national gambling impact study commission
estimates that total annual sports wagering reaches
into the hundreds of billions of dollars internationally 
legally operated sports books in nevada handle over
   billion annually on football   karp        given
the large sums involved  one would expect that these
sports books define betting odds that are very hard to
beat consistently  therefore  i will consider my machine learning algorithm accurate if it can consistently
make money against las vegas sports book odds  even
if its just for some period of the regular season  one
important note   sports books will adjust the odds they
offer based on the volume of wagers coming in on either
side of the bet  sports books want to keep even money
on both sides of the bet so that once the game finishes 
the losers money will pay the winners  and the sports
book keeps a cut  this method has an important im 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fiparity and predictability in the national football league

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

plication   the way that human fans bet will influence
the money line odds faced by my machine learning algorithm  human bettors may be influenced by many
factors  but they probably pay some attention to what
sports broadcasters and analysts say 

manner  scraping from footballlocks com  i use the
          seasons to compare and train the machine
learning algorithm  and i will test the algorithm on
the      season 

if my algorithm can predict the outcome of games accurately  it undermines to an extent the claim of parity in the nfl  in particular  i intend to examine how
often the algorithm gets very confident predictions
wrong  furthermore  if my algorithm can consistently
win against vegas sports book odds  then perhaps it
will have found the true signal underneath the noise
of sports analysts  hunches  and loyalties 

     feature definition

   data
the nfl keeps track of an enormous volume of team
and individual performance statistics  i decided to focus on team game statistics alone because using individual player statistics would overcomplicate this
project  in addition  i believe that football is fundamentally a team sport  moreso than baseball or basketball 
i use historical data from the      through      regular seasons because the set of teams has remained the
same over that period  i e   no team moved  no expansion team was created   i focus exclusively on regular
season games because playoff games are played very
differently  in terms of strategies and schemes  so i
dont want to mix the two up when trying to learn
game outcome prediction 
at the end of each game  the team statistics for each
team are tallied up and reported  here are some examples of team statistics 
 yards gained per rushing play
 total passing yards
 number of interceptions thrown
 total touchdowns
the nfls website reports approximately    such
statistics per team per game 
i learned how to web scrape using pythons beautifulsoup package  and i am very glad to have acquired
that skill  i wrote a script that extracts team game
statistics for each game of the      to      seasons
from the nfls website   to be frank  for me the
most exciting part of this project was watching my web
scraping script start executing successfully   i created
a dataset of money line odds for each game in a similar

for each game  i construct a feature vector that can
be used to predict that games outcome  the basic
idea is that the average performance of a given team
over the course of the season to date should be useful
in predicting whether or not they will win the current
game  let pw
i be a vector of the    game statistics
achieved by team i in its game during week w of the
current season  suppose we are predicting the outcome
of game g in season s  week w   between team t  
and t    where t   is always set as the home team
in order to consistently capture home field advantage 
the feature vector for this game is defined as x  
 pt   pt      where 
pt i  

w
 
x
 
pw
w    w   t i

   

for example  if the san francisco   ers are playing
at home against the seattle seahawks  then to compute pt     i average together all of the seahawks game
statistics from their prior games of the current season 
do the same thing for   esr to compute pt     and then
concatenate the two vectors to form the feature vector
for the current game  note that i do not include performance in games from prior seasons when constructing this feature vector because i have a sense that team
performance can change quite a bit from season to season  due to player trades  injuries  coaching changes 
and intangibles like morale and momentum 
the feature vector as defined above is rather naive 
with two major deficiencies  for one  the nfl game
statistics for a particular team focus almost entirely on
offensive performance  but defense is definitely important to game outcomes  in addition  taking the simple mean of the last several weeks doesnt take into
account that more recent performances probably have
more information than more distant ones  so some type
of time weighting would probably be useful  complicating the matter  the feature vector has dimension
n      which is relatively large compared to the number of games in each season       for example  when
making predictions in the early weeks of the      season  the fifth season i am considering   the training
data set would have dimension            a setting
that may be susceptible to high variance and overfitting  i did intitially reduce the dimension of the variable space by weeding out a handful of statistics that

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fiparity and predictability in the national football league

were linearly dependent on others  but before investing
much more time into fine tuning the feature vector  i
started by testing a few different classification methods
against each other 

   initial trials
in order to get some sense of where to put my time  i
ran initial trials comparing a naive bayes classifier  a
random forest  logistic regression  and an svm  each
of these classifiers was run using the simple feature vector x as defined above  for predicting whether team
t   would win a game in week w of a given season 
the classifiers used the games from prior seasons and
from the past w    weeks of the current season as a
training set  thus  with each passing season  the size
of the training set grows  so the classifiers should get
better at prediction with time 
     naive bayes classifier

   

   

i took professor ngs suggestion to start with a simpleto implement naive bayes classifier in order to build
an end to end system quickly  the initial results using naive bayes were promising from the standpoint
of predicting wins and losses   for simplicity  i disregard the possibility of ties  though there have been
four over the last decade   figure   plots the average
classification error      loss  for each season for each
of the classifiers  the naive bayes classifier predicts
the outcome of games correctly more often than not 
a misclassification rate of     is meaningfully better
than random guessing 

   

nb
rf
lr
svm

   

error rate

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

    

    

    

    

    

season

figure    classification error rates      loss  by season for
each of the classifiers initially tested  they are all comparable  and all are better than random guessing  the
random forest is the most accurate in the last few seasons 
but only by a thin margin 

for each prediction  the naive bayes classifier outputs

the posterior probability of each class   the probability
that team   loses  which is the same as the probability that team   wins   and the probability that team  
wins  i used the maximum of the two probabilities in
order to predict the winner of the game  i kept track
of this maximum posterior probability as a measure of
the classifiers confidence in its prediction  figure  
is a histogram   it shows what percent of the predictions were made for a given prediction probability  in
short  more than     of the time  the classifier made
its prediction with almost      confidence  it would
be useful if the classifiers most confident predictions
were also consistently more correct  but this is not
the case  there was no decrease in the error rate with
higher confidence predictions 
taken together  these two conditions are serious limitations for use in betting  the naive bayes classifier
makes most of its predictions with probability close to
   so this makes it impossible to compute a meaningful expected payoff of a bet  in addition  the highconfidence predictions are no more accurate than the
low confidence predictions  so that makes it difficult
to set some threshold on confidence above which bets
should be placed  these deficiencies drove me to look
for other methods 
     random forest
i learned about random forests in statistics     this
quarter  i varied the main tuning parameter  the number of trees  to find the best performer with reasonable
computation time  settling on     trees  the random
forest generates probability estimates for its predictions  based on the fraction of votes that each class
received from the     trees  for example  if     of the
trees predicted that t   would win  then the prediction probability would be      figures   and   show
that the random forest has nice behavior in terms of
its prediction probabilities  it makes predictions over
all ranges of confidence  and the higher confidence predictions tend to be more correct 
     logistic regression and svm
i implemented logistic regression and got very similar
results to the naive bayes classifier  so i will not describe them in detail for the sake of concision  as for
the svm  i spent a lot of time trying to tune the svm 
eventually settling on the radial basis function kernel
and a very low penalty for margin violations  as seen
in fig     the svm performed well in terms of classification error  in addition  figures   and   demonstrate
that its prediction probabilities behaved nicely  similar
to the random forest  im not certain how the e    

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fiparity and predictability in the national football league

   
   

   

error rate
  

rf
svm

  

   

   

   

   

   

   

prediction probability
 

  of predictions

package in r generates probabilities for the svm predictions  but i took them as a black box output that i
used for evaluating the algorithm and placing bets 

   

   

   

   

   

   

prediction probability
figure    the naive bayes classifier made most of its predictions with extremely high confidence  that is  the posterior probability for the predicted winner was almost always
near    this undermines the ability to compute a meaningful expected value of betting on the predicted winner 

 

 

rf
svm

 

  of predictions

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   

   

   

   

prediction probability
figure    both the random forest and the svm make predictions over a spectrum of confidence levels  this suggests
that the probabilities output by the classifiers may contain
useful information that can be used in evaluating whether
to place a bet or not  e g   expected payoff  

     choosing just one
i decided to choose the random forest for four main
reasons  the first is that it was one of the most accurate classifiers  and it seemed to be getting better
in the later seasons  the second is that its prediction probabilities behaved in a way that could make
them useful for evaluating bets  the third is that the
random forest can evaluate the importance of the various variables it uses for prediction  producing a sum 

figure    the more confident the random forest or the
svm is in its prediction  the more likely the prediction is
to be correct  the spikes on the right hand side are likely
noisy side effects of the fact that very few predictions were
made with probability greater than      as seen in fig    

mary of which variables contributed the most to the
accuracy of the classification trees  this interpretability advantage could be useful in variable selection or
in evaluating the stability of the prediction algorithm 
lastly  random forests tend to have less problems with
high variance and overfitting because each of the trees
created has a certain randomization in which variables
it chooses when growing leaves  therefore  the trees
tend to be weak learners that are not strongly correlated with each other 
i also evaluated the amount of money that i would
have won or lost if i had made bets based upon the
classifiers predictions  the betting strategy was to
place a bet if the classifer made a prediction with confidence greater than     and if the expected payoff
of the bet was positive  this strategy led to the following over the last three seasons of the training data 
     to the              lost using naive bayes        
won using random forest  on a total wager of       
       lost using logistic regression  and      lost using the svm  these results were mostly in line with
the observations about the probability outputs from
each classifier  the naive bayes and logistic regression
classifiers were too confident and encouraged too many
bets  whereas the random forest and svm were more
tentative about their predictions and therefore placed
fewer bets 

   improving the random forest
my efforts to improve the random forest focused on the
feature vector itself  the first thing i did was to add
defensive statistics to the feature vector   for exam 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fi   validation and conclusions
for validation testing  i ran the random forest classifer
and betting algorithm on data and odds from the     
season through week     none of this data was used
to tune the random forest a priori  figure   is a visualization of the prediction accuracy for the random
forest with defensive statistics added  the map shows
that in later seasons and later weeks the algorithm
tends to be right more often  which is a good indication
that its learning something meaningful  with a relatively simple approach  i was able to predict games in
     with an error rate of       which is just mediocre 
for comparison  i looked at the prediction accuracy of
thirteen experts designated by espn com for the     
season  the most accurate expert has a      error rate
to date  and the least accurate        my algorithm did
a little better than the worst expert 
placing bets with this algorithm was surprising   it
decided to bet on just one game in       it won that
bet  so the net balance for      is a gain of       the
accuracy for      is noticeably worse than what it was
for            so perhaps the fact that the random
forest was not predicting well was reflected in lower
prediction probabilities  meaning that the algorithm
almost never thought it was confident enough to bet 
upon reflection  i believe that tuning for prediction accuracy implies a different objective function that tuning for winning bets  during the training on the          data  i focused on minimizing the     loss on
the assumption that this would also minimize betting
losses  for raw prediction accuracy  it only matters
that the probability be over     for the team most
likely to win  for betting  in particular for evaluat 

    
    

ple  how many net yards of offense did the other team
achieve  how many rushing touchdowns  etc  now the
random forest could take into account defensive performance  this generated a slight improvement the
prediction accuracy  though increasing the dimension
of the feature vector increases the risk of overfitting 
next  i implemented a very simple weighted mean 
applying a geometrically decaying weight for performance statistics from more distant weeks  but that did
not improve accuracy  lastly  i aggressively stripped
down the feature vector by removing variables that i
did not think were the most important to game outcomes  this variable selection did not noticeably improve prediction accuracy  but it didnt make it all
that worse either   and for the purposes of predicting
accurately on       a more parsimonious model may
face less risk of overfitting on the           training
set  and therefore may be more accurate 

season

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

    

parity and predictability in the national football league

 

 

 

  

  

  

  

week

figure    color map of prediction accuracy for the final
random forest algorithm  green hues indicate that for a
given week in a given season  most of the predictions were
correct  the darker the better   yellow to red hues indicate
that most of the predictions were wrong  the darker the
worse   the map is greener in the upper right half 

ing the expected payoff of a wager  it matters that the
probability actually be close to the true probability
of the team winning  therefore  if my priority had
been to choose and tune algorithms to perform well
in the gambling setting  i would have needed to use
the monetary winnings and losses as the key criterion
instead of the     classification loss 
its not terribly surprising that my project didnt uncover the secrets of the nfl and las vegas simultaneously  but it is surprising that the algorithm didnt do
a little better than     accuracy  in my opinion  this
reinforces the notion that the outcome of nfl games
is pretty hard to predict  based on the results of this
project  i cannot substantively argue against the claim
of parity in the nfl  and thats fine by me 

references
hamadani  babak 
predicting the outcome of
nfl games using machine learning 
url
http   cs    stanford edu proj     
babakhamadani predictingnflgames pdf 
karp  h  the nfl doesnt want your bets  the wall
street journal        url http   online wsj 
com news articles sb                   
sierra  a  et  al 
football futures 
url
http   cs    stanford edu proj     
sierrafoscofierro footballfutures pdf 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fi