    predictions on kaggle  austin lee
project description
this project is an entry into the seeclickfix contest on kaggle  seeclickfix is a system for reporting local
civic issues on open     each issue has an associated number of views  comments  and votes  the goal
of the contest is to minimize the root mean square log error  rmsle  of predictions of views  votes  and
comments based on the provided data including latitude longitude  summary description text  source
category  created time  and tag category 
for complete control of the algorithms  as well as self education  i wrote many of the routines used in
this project myself in matlab instead of relying on libraries such as scikit learn  the final code base was
around     total lines in    functions  the most significant built in function calls are to matlabs left
divide operator and kmeans algorithm 
minimizing rmsle
the contest is evaluated by taking the rmsle of all predictions together  since this metric isnt
normalized  the views and votes predictions are given somewhat more importance than the
comments prediction  which are often   

  
 

 

 

 

  

 

i use a simple linear least squares model to minimize rmsle by treating log y    as the variable that we
wish to predict  and then converting the predictions back to linear space by reporting exp pred     this
gives the model the freedom to incorporate many different features at the top level while still running
very quickly  all of the features that are discussed in this report are combined in this linear model 
validation
there are two data sets for the competition  the training data set contains         issues and includes
the actual number of views  votes  and comments  the testing data set contains         issues with the
prediction variables stripped 

fikaggle judges entries using contestants predictions on     of the testing data  the remaining     is
used to generate public leaderboard positions  two entries can be submitted per day  and the
leaderboard position for each entry provides valuable validation information  however  since the
submission rate is limited  performing validation on the training data is also necessary 
i used simple       cross validation to determine the generalization error of the model  and took the
mean error of    cv runs to improve the stability of the calculated error  i found this method to give
good agreement with the rmsle values that kaggle reported on my submissions  in addition  i found it
useful to have a backward search algorithm to check for any over fitting in my model 

data processing
i found matlabs data loading was slow and didnt parse this data set well  so i wrote my own  my data
loader performs a number of different parsing functions on the various fields  the data is sourced from  
cities  so the loader runs kmeans with k   and an initial mean located in each city  this quickly creates
the city categories  which the data does not provide  the text fields  summary and description  are
parsed by converting to lower case  removing any non alphanumeric characters  and tokenizing on
spaces  each token is run through a non cryptographic hash function  sourced from an implementation
of the djb  function written by d  kroon university of twente   which greatly speeds up the vocabulary
generation  categorical variables  source and tag  are analyzed without hashing  since their
vocabularies each only contain about    possible values  dates are parsed with the datenum function in
matlab  which i found was one of the slower functions calls in my load routine 
this parsing process takes about    minutes for the kaggle data  so it is loaded into a struct in matlab so
that it is easy to maintain and manipulate in
memory from a single load operation 
time features
plotting the mean of the data by day  fig      we see
temporal patterns in the data that we can use as
indicators in the linear model 

figure    mean values per day for all data

fithe most obvious pattern is a large decrease in the number of
comments and votes around day     through day      looking
more closely  we see that this only affects chicago  fig      and
that the impact on the overall results is largely due to a marked
increase in the number of submissions  fig      which skews the
average  in this case  it is sufficient to simply make independent
figure    mean values per day for chicago

models for the   cities and break chicago down into two
categories so that we have   effective city categories 
once this is accounted for  we can properly categorize the
temporal data and attempt to model it  in general  the cities have
a linear overall trend  fig     as well as a clear weekly pattern  fig 
    to account for the linear decreasing trend  we can add the
timestamp value itself as a feature for each category  for the
cyclic effect  we can use an indicator that breaks down the data
figure    number of issues per day for all data

by day of the week  this gives us   features in each of  
categories  for    total features to use in the linear model 
i found that these features provide the greatest benefit in my
model  going from about      rmsle to      alone  this is hardly
surprising given the large variation that is clearly explained by
these features in the figures shown 
text features
as mentioned earlier  the text fields are sanitized  tokenized  and

figure    mean values per day for richmond  detail

hashed into numerical values  the vocabulary for the summary
and the description are independently generated for the most
common tokens 
for each vocabulary set  we can then generate a feature for each
vocab entry as an indicator of the presence or absence of that
word in its associated field  i found a     word summary

figure    mean values per day for richmond

fivocabulary and a     word description vocabulary provided good results  this includes tokens that are
represented in about    issues at a minimum 
in addition  a length feature was generated for both the summary and the description fields based on
the number of tokens for each  this is somewhat redundant with the token features  but does provide
additional information when a token is used more than once  i found including this feature did improve
generalization error slightly 
categorical features
the source and tag fields are trivial to implement as binary indicators in my model  they ended up
only having a minor effect on the result compared to the time and text features described so far 
location features
city categories are captured in the time analysis discussed earlier  but that doesnt make full use of the
 potentially valuable  location information  i opted for a simple  somewhat brute force model to try to
capture these features 
i ran k means on the entire data set  the test set as well as the training set together  for a large k  over
      and used those cluster assignments as indicators in my model  i found this worked best around
k      although it only reduced my rmsle from       to       
it may be possible to improve this part of the algorithm in particular  a soft clustering algorithm may
provide better results  or using an svm to categorize high view and low view locations  unfortunately 
the short time span of the project precluded investigating these options 
attempted tweaks
in the course of this project i tried a number of tweaks to the algorithm to improve it  none of them
proved effective  likely because the algorithm i arrived at by the project milestone was already quite
competitive  using all of the features described above except for the fine grain location features  
a few notable attempted changes that increased rmsle were  removing any of the features i had
already generated  increasing the size of the vocabulary to     words  combining the description and
summary vocabularies  and any attempt to use a different overall model  my linear model fit to the
log of the data proved quite effective  

firesults
i placed   th out of     teams with a final rmsle of         the winner had an rmsle of         the
median score was         the baseline  all zero  prediction had an rmsle of       
considering that the entire leaderboard is largely populated with phd students and professionals  i am
happy with this result and believe that it confirms that my algorithm works reasonably well  with more
time to improve the algorithm i think the most promising features would be census data  population
density and household income in particular   and figuring out whether any of my other features interact
the same way i found city and time interact 
the most important result  i got to get my hands dirty with real world machine learning and learned a
lot in the process  thanks for reading 

fi