mapping text phrases to complex logical forms for semantic parsing 
exploring named entity recognition
ashish gupta
ashgup stanford edu

siddharth jain
sjain  stanford edu

sushobhan nayak
nayaks stanford edu

cs     machine learning project
computer science department  stanford university

 

introduction

the above observations motivated us to work on improving named
entity recognition specifically for the task of questions and answers 
the main contributions of this paper are 

semantic parsing is the task of matching natural language utterances
to formal language representations  annotated logical forms are usually expensive to acquire and semantic parse  as shown in sempre
     which learns through question answer pairs  question answering systems build a coarse mapping from phrases to predicates using
a knowledge base and a large text corpus  they use a bridging operation to generate additional predicates based on neighboring predicates  an example of this is quora  which is a popular question
answering website  questions asked on the website often lack structure and recommending similar questions requires identifying named
entities in agreement with the semantic category expected by a given
question 
ner is the task of locating and classifying atomic elements in text
into predefined categories such as the names of persons  organizations  locations  expressions of times  quantities  monetary values 
percentages  etc  in sempre      ner forms a large part of the
algorithm to align the question  named entity recognition has been
studied mostly for information extraction and its use case has not
been assessed in a question answering environment  questions have
different inherent structures than free text  which makes the task nontrivial  we performed some initial experimentation and made the following observations 

   manually annotated a web questions dataset 
   scraped imdb dataset to improve ner for movies 
   analyzed and improved ner for qa by varying training sets
and models 
the structure of the report is as follows  section   discusses related
works  in relation to our paper  section   explains the datasets used
to explore ner  in section    we list the the models and tools used
in classification of named entities  in section    we provide the result
and a detailed error analysis of the different models and training sets
used  we finally conclude and discuss future work in section   

 

related work

sempre     uses lambda dependency based compositional semantics to represent latent logical forms  if the knowledge base k
is treated as a directed graph in which entities are nodes and properties are labels on the edges  then simple lambda dcs unary logical forms are tree like graph patterns with a subset of the tree nodes 
sempre recursively constructs distributions over all possible derivations by using i  lexicon mapping from natural language phrases to
knowledge base predicates and ii  a set of composition rules 
we focused on the lexicon construction phase where a lexicon is constructed by aligning a large text corpus to a knowledge base  eg  a phrase and predicate aligns if they co occur with
large number of same entities   we evaluated the current candidate phrase generation techniques and performed experiments with
the source code of sempre     available publicly at http   wwwnlp stanford edu software sempre 
    works on named entity recognition in a query setting i e detection of a named entity in any given query and its corresponding
classification  the paper uses latent drichlet allocation by considering contexts of named entity as words of a document and classes of
the entity as topics  experimental results indicate that their method
can accurately perform named entity recognition in queries 
    talks about ner probabilistically by converting multiple entity
labels to strings  they compare their qa system with probabilistic
ner and traditional qa systems with single entity labels  the paper
shows that the added noise introduced by the additional labels is offset
by the higher recall gained 
    target the identification and classification of named entities in
natural language questions  they play with balancing the amount of

 we began our analysis by testing questions on sempre     
our experiments with the qa system showed that due to poor
ner  a lot of candidate phrases were missed which led to a drop
in performance 
 the system failed to recognize some popular entities  recognized in statements  when presented in the form of questions 
upon looking at the ner model used by the system independently  we found that the ner model performed poorly on questions since it had been trained on statements rather than questions 
 we found the system also performed poorly on unstructured
questions  we claim this is attributed to the ner model being
trained on well formed sentences  typically news datasets like
reuters 
 questions involving certain entities like movies  books  tv
shows failed to return answers despite such entities being
present in the rules and answer database  on analysis  we found
that the ner employed by the system failed to recognize these
entities which resulted in the answer not being found 
 

fi org   denotes the name of an organization  eg  who founded
horgi google h orgi

free text and questions to optimize the training set for questions     
is most closely related to our work  however  one underlying emphasis is the use of unstructured questions in our setting  when querying  users generally tend to forget capitalization and ignore grammar 
which we attempt to handle 

 

 misc  denotes the name of movies  shows  books etc  eg  who
directed hmisci the    steps h misci
for each question and text sentence  we annotate the named entities
by marking them between two equal xml tags  only one category is
attributed to each named entity and no nesting of entities as allowed 

dataset

our main aim was to gather data that was a list of questions  this
would allow us to see how named entity recognition worked on single questions  questions are usually short sentences  that may not be
well formed  lack punctuation and may or may not be interrogative in
nature  for example  who is the president of usa can also be expressed as president of usa is when being searched in a knowledge
base 
as we were working with the stanford ner     system  we first
gathered the datasets used in this system  yates compiled a list of    
questions taken from    domains of the freebase database  the stanford ner system also referred to another dataset compiled by berant
et al     which consisted of     questions  these two datasets were
not annotated with any entities  we essentially divided this amongst
the team members and manually annotated the datasets  the paper
discusses the annotations in this section below 
mendez et al      performed a similar analysis of named entity
recognition on questions  to perform their experiments they had created a dataset of       annotated questions that was freely available
for research  we took their dataset as well and broke it into a training
and test set for our experiments 
given that a supervised dataset is not readily available  we figured
that a semi supervised dataset would be good enough for our experiments  as we were trying to also address entity recognition in cultural
references like movies  shows and books  we concluded that imdb
would be a good source  imdb is a web database of movies  the
website also publishes a daily webpage which contains news articles
on movies  this news webpage has links to other movies shows  actors actresses directors and production houses  we could  therefore 
scrape these webpages to generate a dataset of text and use the links
to annotate entities  using a python script we made html requests
to these webpages over a time period ensuring that the imdb server
does not block our requests  categorizing them as spam   once we
got back the html webpage  we massaged the ha hrefi tags into the
appropriate entity tags to create a semi supervised dataset  as we
were relying on the links for tagging entities  not every named entity
could be tagged as there were a few actors etc who didnt have links
associated with them  we wanted to see what effect this might have
on our experiment of annotating web questions 
finally  we took the annotated reuters dataset used by bengio et al 
    to generate another annotated dataset of around         words 
this dataset was not in the form of questions  but statements  we
wanted to show that using statements along with questions as a training set would help induce better performance on a test set of just
questions 
all our data is annotated using four named entity tags 

dataset
yates train
yates test
berant et al  test
mendez et al  train
mendez et al  test
imdb train
reuters train

tokens
    
    
   
     
    
     
      

per
   
  
   
    
  
    
    

loc
  
  
  
    
   
 
 

org
   
  
  
   
  
   
   

misc
   
   
  
 
 
    
    

table    dataset size and annotations

 

models and tools

we used the stanford ner  metaoptimize and etxt db software
tools available from the web to train our models and evaluate our test
dataset  the following are the algorithms used by these tools 
 hidden markov model  lingpipe    find parameters to maximize p x y   assumes features are independent  when labeling xi future observations are taken into account  forwardbackward 
 conditional random field  stanford ner    based on conditional random fields  it tries to model the factor graph to generate a discriminative distribution over labels using appropriate
features 
 support vector machine  minorthird    decision boundaries
using features such as lexical information  unigram and bigram   affix      suffix and prefix letters   previous named entities  possible named entity class  token feature  dictionaries
 company  person and location names   svms main ability is
in the inclusion of overlapping features while that of crf is to
include various unrelated features 
 perceptron ner  metaoptimize    augmenting supervised
learning with unsupervised word representations as extra features  also implementing brown clustering and gazetteers using
known lists of named entities 

 
   

experiments and results
preliminary experiments

we drew attention to the notion that the available ner models are
usually trained on statements  particularly reuters dataset which consist of newspaper articles  which have a very higher propertion of
statements to questions  to see if it adversely affects the ner task 
we ran the stanford ner      which has been known to outperform
most ner models       and various models augmented with gazzeters
and word clustering representations from      for stanford ner 

 per   denotes the name of a person  eg  who is hperi george
washington h peri
 loc   denotes the name of a location  eg  what is the capital
of hloci egypt h loci
 

fialgorithm
hmm
hmm
svm
svm
crf
crf
stanford ner
stanford ner
stanford ner
stanford ner
stanford ner
stanford ner
perceptron ner
perceptron ner
perceptron ner
perceptron ner
perceptron ner
perceptron ner
perceptron ner

train set
mendez et al 
yates and mendez et al 
mendez et al 
yates and mendez et al 
mendez et al 
yates and mendez et al 
default model
default model  with capitalization 
mendez et al 
yates and mendez et al 
imdb and yates and mendez et al 
reuters and imdb and yates and mendez et al 
default model
default model  with capitalization 
default model with brown clusters
default model with brown clusters  gazetteers
mendez et al 
yates and mendez et al 
imdb and yates and mendez et al 

yates test set
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

berant et al  test set
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

mendez et al  test set
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    f  scores of named entity recognition using different algorithms
we used an already trained caseless model available from their website  which has been trained on the massive reuters  muc and ace
datasets  which gives it robustness against only american or british
english  for the other algorithms  we trained the models on a subset
of tagged reuters dataset available from      to investigate the effect
of capitalization  we use both caseless and caseful models available
for stanfordner and train the     models with capitalization feature
switched on and off  the results are shown in table    the f  scores
are phrase f  scores  i e   we consider an assignment correct if all the
words of the named entity phrase have been assigned a correct tag  
from the table  its evident that capitalization has a remarkable effect  the trained caseful models perform awfully on our test data 
primarily because our test data consists of real questions asked on the
web  which are asked with minimal capitalization  e g  google search
who is the author of x instead of who is the author of x   this effect is universal  we also tested it with senna      a state of the art
deep learning model  and our f  score was      we dont report
it in detail here because senna in its readily available form cant
be trained  reducing its usefulness in the current investigation where
one of our significant contributions is the creation of new datasets  
using a caseless approach takes care of it  but the f  score is still
in the   s at the most  which is pretty low than the usual numbers
in the   s and   s for statements         the model fails primarily on org and misc tags  the f  score for misc is   for both
the datasets  denoting a complete failure  some examples of entities they mispredict are starry night    juno asteroid  ron glass  the
philosophers stone  six feet under  x men  batman  the dark knight
returns etc   which confirms to our conjecture of the model mispredicting names of movies  tv series  books etc  the failure with organization and location detection is attributable primarily to different
sentential structure in questions  for example  when was interstate
    formed   loc wrongly tagged as o   what is yahoo   org
tagged as o  are wrongly tagged due to lack of precedent structure 
while what area did the meiji constitution govern  and what was
procter   gambles net profit in       are tagged with the correct

org tag due to familiar structures like org govern and orgs
profit in year 

   

augmenting with questions

to combat the lack of questions in the training data  we train our
models on the new training set of questions we have prepared   on
mendez and yates datasets  this is in stark contrast to our previous
models  where they were trained on massive news data  our dataset
consists of       tagged questions  but even with such minimal data 
the resulting f  scores are quite close to previous values  and in the
case of yates and mendez test sets  actually exceeding that of previous models  we have significant improvements in f s for tags misc
 most of the previously wrongly tagged phrases like   juno asteroid 
the philosophers stone are correctly predicted   loc and per when
we use both mendez and yates datasets for training  see fig   and
    however  at the same time  we see a dip in the f  for org 
interestingly  the errors are different from the ones we got in the previous run  we make correct predictions for what school was delta
delta delta founded in  and when was the order of saint michael
founded  showing we have begun learning structures particular to
questions  at the same time  we make mistakes on what area did the
meiji constitution govern  and what was procter gambles net profit
in        pointing to a lack of learning in other sentential structures 
it seems that org as a tag is more influenced by words around it 
most of the sentences with org or per have similar structures and
org requires specific data points to learn the difference  while it
learns question specific structures from this dataset  it loses the statement specific structures it learned from reuters 

   
     

including imdb dataset
effect of inclusion

we tried nullifying the effect due to lack of questions in the training set in the previous section  we next turn to tackling the issue of
mistagging movie and tv series names  to combat that  we have created a huge dataset of imdb news as described before  and we train
our models with a combined dataset of yates  mendez and imdb 

 
the results here might differ from our results in the milestone  we have made significant changes to datasets since then  the starkest of which is including a misc label
along with per  loc and org we considered in the milestone 

 

fitraining set
imdb  x and yates and mendez et al 
imdb  x and yates and mendez et al 
imdb whole and yates and mendez et al 

yates test set
      
     
      

berant et al  test set
      
      
      

mendez et al  test set
      
      
      

table    effect on f  score when increasing imdb training set size
almost the same as we increase the number of examples of the imdb
statements  this is natural since as we keep on increasing the number
of statements  the effect of structures in the questions are mitigated 
in our case  equal number of both seems to be the ideal combination 
though a detailed investigation would involve observations at more
finer steps  like         times the size of the question bank  for our
purposes  this small investigation is sufficient 

the f  results dont change much for the yates test set         compared to          but we see significant improvement in the berant
et  al  test data  where we achieve the best result of         table
    in both cases  we see a significant improvement in tag f  for the
per and org tags  see fig   and     however  at the same time 
performance for misc tag suffers  drops to       from     for yates
test set  while remains   for berant   f  for loc tag remains more
or less the same since the imdb dataset has no loc tagged entities 
the increase in per f  is primarily due to prediction of person entities like justin beiber  keyshia cole etc  while the increase in org
f  is due to more examples of sentence structures involving org in
statements  e g  paramount pictures is       which were missing from
the question only data bank 
the drop in misc predictions can primarily be attributed to lack of
tags for all the occurrences of movie tv series names  imdb dataset
is not self sufficient  the news items that occur in the page have links
to corresponding movies  people and studios mentioned in the article 
however  not all occurrences are properly linked and as such  a typical example in our dataset looks like this   link  the hobbit  the desolation of smaug   link  opened at no    in all nine overseas markets
in which it debuted on wednesday   link  warner bros   link  reported
thursday  it took in      million  three percent better than the first
film in  link  peter jackson   link  s trilogy  an unexpected journey  did in its first rollouts last year  france was the top market  as
smaug brought in      million  topping the debut of unexpected
journey by eight percent  also read  hobbit sequel will outpace
the first at overseas box office  and thats saying plenty the
desolation of smaug rolls out friday in       u s  theaters  with
midnight screenings in      a typical news article repeats the name
of a movie multiple times  or mentions related movies  but linking is
not done on every instance  on an average  its the movies that are
primarily not linked compared to actors and studios  so while some
are tagged as misc in the training set  a lot of similar structures remain untagged  creating a bias towards un tagging such occurrences 
there are two possible ways to counter this  leverage other specialities of the news articles to improve the tagging  for example  most
movies are presented inside brackets  or use a knowledge base of
known movies  a gazetteer in particular  to improve the f  score  we
follow the later approach in a future section 
     

   

introduction of unsupervised information

if we take an existing supervised nlp system  a simple and general
way to improve accuracy is to use unsupervised word representations
as extra word features     to improve upon our current results  we
follow this approach of augmenting word feature vectors with unsupervised feature models  in      word feature representations involving brown clusters work the best  so we incorporate that here  the
brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams  brown et
al         so it is a class based bigram language model  the hierarchical nature of the clustering means that we can choose the word
class at several levels in the hierarchy  which can compensate for poor
clusters of a small number of words  one downside of brown clustering is that it is based solely on bigram statistics  and does not consider
word usage in a wider context  now  stanford ner is not easily extendable  so we use    s perceptron ner implementation for further
experiments 
another improvement that is usually done in literature is to include prior knowledge of named entities through a gazetteer  particularly  we leverage a list of entities extracted from wikipedia and
made available by      the list contains names for currencies  places 
states  art work  films etc  we augment all of our models with these
two features  the final results are shown in fig   and    while the
default models trained on the reuters dataset fail miserably  even a
small set of questions augmented with brown clusters and gazetteers
outperform them on questions  we have an absolute increase of    
in f  scores for both of our test datasets  the final f  score for the
berant dataset reaches      comparable to results usually achievable
on statements  which we set out to achieve 
another important trend is that inclusion of imdb dataset improves the f  scores for the berant dataset while it brings down the
f  of yates  a trend seen both when we use stanford ner and the
perceptron ner with augmented weights  this is because the berant
dataset has more occurrences of questions that involve trivia about
hollywood actors  while yates doesnt focus too much on them  this
is apparent from the individual tag f  scores  the f  score for per
remains exactly the same after inclusion of imdb data  where as it
goes up by    in case of berant dataset 

effect of size

    deals with an investigation of a healthy combination of questions
and statements for optimal performance  we have already seen that
lack of statements leads to poor performance of primarily org tags 
but lack of enough questions also leads to poor results for f  on questions  to see if a similar effect is visible in our data  we mixed parts
of tagged imdb data with our tagged question bank  particularly  we
ran three experiments     with equal number of sentences from both
imdb and question bank     with five times the number of sentences
from question bank  and    the whole of imdb dateset  the results
are in table    we see that the f  score either decreases or remains

 

conclusion and future work

in this work  we have tried to analyse and improve the existing ner
techniques specifically for question answering systems  we began
 

fiwith the task of analyzing such systems and quickly realized that
ner forms an integral component which was acting as a bottleneck
in the performance  specifically  we looked into issues of the overdependence of existing models on capitalization features  the lack of
questions in training sets for ner models and the effect of external knowledge and domain specific data  like documents related to
movies  on the detection of such entities  owing to a want of an exhaustive tagged question bank  we created hand annotated tagged corpora  we also used a semi supervised approach to create a partially
tagged domain specific corpus  a dataset of imdb news with ner
tags  which helped improve our results  going further  we used unsupervised word cluster features and entity lists to increase the baseline
of a state of the art ner system from     to     on a test set of
unstructured and ungrammatical questions typical of a web platform 
in future  we would like to integrate the modified named entity recognition models to the question answering system  sempre      and
study the performance improvement of the system in detecting answers 

figure    stanford ner tested on yates test set   f  scores

references
    samet atdag and vincent labatut  a comparison of named
entity recognition tools applied to biographical texts  corr 
abs                 
    jonathan berant  andrew chou  roy frostig  and percy liang 
semantic parsing on freebase from question answer pairs  in
emnlp  pages           acl       

figure    stanford ner tested on berant test set   f  scores

    r  collobert  j  weston  l  bottou  m  karlen  k  kavukcuoglu 
and p  kuksa  natural language processing  almost  from scratch 
journal of machine learning research                    
    jiafeng guo  gu xu  xueqi cheng  and hang li  named entity
recognition in query  in proceedings of the   nd international
acm sigir conference on research and development in information retrieval  sigir     pages         new york  ny 
usa        acm 
    ana cristina mendes  lusa coheur  and paula vaz lobo  named
entity recognition in questions  towards a golden collection  in
nicoletta calzolari  khalid choukri  bente maegaard  joseph
mariani  jan odijk  stelios piperidis  mike rosner  and daniel
tapias  editors  lrec  european language resources association       

figure    perceptron ner tested on yates test set   f  scores

    diego molla  menno zaanen  steve cassidy  and north ryde 
named entity recognition for question answering  in in lawrence
cavedon and ingrid zukerman  editors  proceedings of the
     australasian language technology workshop  pages   
         
    joseph turian  lev ratinov  and yoshua bengio  word representations  a simple and general method for semi supervised
learning  in proceedings of the   th annual meeting of the association for computational linguistics  acl     pages    
     stroudsburg  pa  usa        association for computational linguistics 

figure    perceptron ner tested on berant test set   f  scores

 

fi