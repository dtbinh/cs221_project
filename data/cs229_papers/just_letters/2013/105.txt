auto tagging piazza posts
cs     autumn       prof  andrew ng
ai jiang hajiang
kathy sun kathysun
december         

 

problem introduction

our project is to make an automatic tag recommender for piazza posts  since each post
could have multiple tags  the problem can be viewed as a multilabel classification problem 
where each post and possible tag combination is a binary classification  this could be used
to improve the user experience for students using piazza as well as increase the accuracy
and relevancy of tags  for the project  the data we used is the set of piazza posts from the
cs    class  but the methods we describe generalize to piazza data from any class 

 

method approach

each post on piazza corresponds to a training example and the text content of the post is
processed and converted into vector format  where each feature corresponds to some statistic
about a word  we then run various classification learning algorithms to predict the tags of
each post of a held out test set and calculate our desired metrics  from there we perform
analysis of our initial results and implement further improvements to our algorithms 

 

data collection and preprocessing

from the raw data  we applied stop word removal  stemming and case conversion  then  in
order to reduce the feature set  we computed the term frequency of each word and filtered
out terms below a minimum cut off value of    which gave us a dictionary of      words 
lastly  we converted each post to vector format using the multinomial event model where
each element corresponded to the term frequency of that word  and later the tf idf score 

 

fifigure    system pipeline

 

learning algorithms and initial results

we decided to use naive bayes and svm to run supervised learning and for a baseline  we
used a predictor that predicts   for everything  for a given tag  however  the large majority
of posts has class    and thus our simple baseline predictor scored just as well as naive
bayes and svm in terms of accuracy  because of this  we decided to use a knn predictor as
our baseline as well as to look at precision and recall in addition to accuracy  naive bayes
seemed to do better for recall on most tags and knn for precision 
figure    initial tag metrics of training set split      
 a  accuracies

 

 b  recalls

 c  precisions

analysis and improvements

from our initial results  we plotted the bias variance graph and saw that our test error generally decreased as we added more training examples  suggesting we had high variance  as
 

fifigure    bias and variance diagnosis

figure    select     best features

a result  we decided to decrease the amount of features by selecting features based on the
weights of the initial learning to reduce our feature set down to            the downward
trend also suggests more training examples may help  another optimization we did was use
tf idf scores instead of pure term frequencies  the last improvement that we made was to
use course handouts from the class and convert them into extra features  we read in the
documents from the class including homeworks  lecture notes  section notes and logistics
documents and then converted them to the same vector representation as our training examples and used the cosine distance between each post and each document as a new feature 
this improved all of our metrics for naive bayes classifier but didnt have a large effect on
the svm classifier 
figure    changing feature representation and size
 a  tf vs tf idf representation

 b  course documents as extra features with filtered dictionary

 

fi 

results
table    top five most useful words used as features

lecture

naive bayes
svm w  linear
ps 
naive bayes
svm w  linear
ps 
naive bayes
svm w  linear
project
naive bayes
svm w  linear
other
naive bayes
svm w  linear
ps 
naive bayes
svm w  linear
logistics naive bayes
svm w  linear
midterm naive bayes
svm w  linear
ps 
naive bayes
svm w  linear
section
naive bayes
svm w  linear
polls
naive bayes
svm w  linear

kernel
kernel
kernel
kernel
kernel
kernel
kernel
kernel
kernel
kernel
kernel

note
lectur
function
 
 
lectur
read
page
typo
 
set
point
word
label
 
ps 
empti
liblinear
wo
shown
question
problem
function
theta
 
 a
sheet
quick
anyth
assumpt
project
featur
data
learn
propos
project
propos multiclass
svms
normal
learn
featur
question
data
answer
scipi
associ
collabor unobserv
link
state
valu
 
iter
problem
norm
ps 
 
definit
assum
scpd
student
exam
correct
submit
mandatori
calendar
find
huang
writeup
midterm
exam
scpd
 
time
midterm
abov
cover
doe
practic
problem
centroid
imag
bound
ps 
ps 
werent
distinct
markov
q b
featur
section
hour
offic
class
section
friday
attach
quick
code
hard
easi
latex bayesian
midterm
bayesian frequentist
item
vote submityou

table    best algorithm per tag

tag
lecture
ps 
ps 
ps 
ps 
midterm
logistics
project
other

accuracy
svm    
svm    
svm    
svm    
svm    
svm    
svm    
svm    
svm    

recall
nb    
nb    
nb    
nb    
nb    
nb    
nb    
nb    
nb    

 

precision
svm    
svm    
svm    
svm    
nb    
svm    
nb    
svm    
nb    

fi 

conclusion

we were able to build a successful system to automatically tag piazza posts with high accuracy  depending on the application requirements and what metric should be prioritized 
different learning algorithms seem to work better than others  i e  naive bayes for recall 
knn for precision   since we treated each tag as an independent binary classification problem  our training set was heavily skewed with more untagged than tagged labels for each
tag  this made it difficult to learn for some tags  trying various ideas such as tf idf and
supplementary document features had different effects on different classifiers but generally
improved performance metrics  in the future  it would be interesting to try to treat each
field of the post separately  similar to the bm   f algorithm from information retrieval  the
techniques and ideas presented here would also be useful in the application of determining
post similarity  which could be used to detect whether a similar post has been made in order
to reduce duplicate posts 

references
    corinna cortes and vladimir vapnik  support vector networks  mach  learn            
     september      
    j  l  dawson  suffix removal for word conflation  in bulletin of the association for
literary and linguistic computing  pages            
    c  d  manning  p  raghavan  and h  schutze  scoring  term weighting  and the vector
space model  page           
    andrew mccallum and kamal nigam  a comparison of event models for naive bayes
text classification  in aaai    workshop on learning for text categorization  volume
     pages       aaai press       
    jose r  perez aguera  javier arroyo  jane greenberg  joaquin perez iglesias  and victor
fresno  using bm  f for semantic search  in proceedings of the  rd international semantic
search workshop  semsearch     pages         new york  ny  usa        acm 
    david m  w  powers  evaluation  from precision  recall and f factor to roc  informedness  markedness   correlation  technical report sie         school of informatics and engineering  flinders university  adelaide  australia       
    a  rajaraman and j  d  ullman  data mining  pages      cambridge university press 
     

 

fi