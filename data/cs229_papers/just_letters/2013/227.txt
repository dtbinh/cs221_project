prediction of cell line sensitivity to cancer drugs
peyton greenside and winston haynes

   background
  a  project goal
the goal of our project is to find a machine learning model that will predict how sensitive a given cell line
will be to a certain drug  a common problem in biological research that extends into clinical research is
how therapeutic treatments vary across tissue type  disease state  and other variations across biological cells 
for example  one drug might be very successful at inhibiting the growth of a certain type of breast cancer
tumor  however  there are many different types of breast cancer that are differentiated by mutation status
and tissue type and these may not respond equally to the same drug  if a particular drug works in one type
of breast cancer  it would be highly useful to know if that drug would perform similarly in other types of
breast cancer  researchers and clinicians would both want to know what characteristics of a cell line may
enable or prevent a therapeutic from working in a particular tissue type 

  b  data
in order to address this question  we are using
a data set from the cancer cell line encytable      data properties
clopedia  this data set provides the sensitivdata type
  points
ity of     different human cancer cell lines to
copy number variants
     
   different drugs  for each cell line there is
gene expression
     
comprehensive information detailing the base measurments
oncogene mutations
    
expression level for each gene in that cell line 
effect
sensitivity to anti cancer drugs
  
copy number variations known for that cell
line  as well as known oncogenic mutations 
this data set is part of a larger effort to conduct a detailed genomic characterization of human cancers and
make the data publicly available  when these three feature data sets are aggregated  we have    unique
drugs on which to build a prediction model for     cell lines with        features 

   dimensionality reduction
the ccle data exists in the mn space  where the number of measurements is significantly larger than the
number of samples  we realized early in the project that the two order of magnitude difference necessitated
careful prevention of model over fitting  further  we wanted to enable lower dimensional visualization of the
data to identify meaningful cluster patterns 

  a  principal component analysis
principal component analysis  pca  is a commonly utilized algorithm for reducing the dimensionality by
selecting orthogonal combinations of data points which minimize the variance of the underlying dataset 
pca was implemented using the prcomp function in r  pca reduced the data to a     dimensional feature
space  equivalent to the number of samples   as part of the tuning process  we needed to identify the optimal
number of principle components to use in our analysis 

  b  stochastic neighbor embedding
stochastic neighbor embedding  sne  is a relatively new algorithm for dimensionality reduction  which is
noted for maintaining local structure while also revealing the global structure of the data set  sne calculates
a probability distribution that any two points are in the neighborhood of each other  which is then utilized
to form a lower dimensional representation  in particular  we utilized implementations of t distributed sne
 tsne  
tsne calculations can be computationally intensive for high dimensionality datasets  the first implementation we utilized ran on quad core server with   gb of ram for over a week of constant resource consumption
without producing any results  fortunately  we were able to use the barnes hut sne algorithm which completed calculations within one minute 

fi  c  visualizing dimensionality reduction
we want to use our reduced dimensionality to visualize the data in a two dimensional space  in particular 
we would like a  d visualization which exhibits clustering of similar outcomes measures to gauge how well
sensitive and insensitive cancer cell lines can be distinguished  to test this  we plotted our first and second
ranked principal components on the x and y axis  then  we generated    graphs  one for each drug  where
we colored each point according to its sensitivity to the drug 
we have displayed two cases in figure      l        left  exhibits the strongest clustering patterns of the   
drugs  erlotinib  right  is more typical of our results  with clustering patterns bordering on random noise 
unfortunately  even in the case of l        we do not see any cases where the two dimensions alone provide
enough information to segregate the data into meaningful clusters 





  



 
















 



pca 






 



 

   


 




 



 



 
    
 

 


 

   


 




   


 


 






  



 

 

 





  






 







 












 









  


 




 






 

 
 
     


  

 


  



 
  

 
 








   



    



  



      


  
 


 


 
     
 












 


   




 




  
 
 





 

  

  
   

  

 


 



   




pca 

erlotinib
   

   

l      











 



 

   


 




 



 



 
    
 

 


 

   


 




   


 


 






  



 

 

 





  






 







 












 









  


 




 






 

 
 
     


  

 


  



 
  

 
 








   



    



  



      


  
 


 


 
     
 












 


   




 




  
 
 





 



  

   

  

 

  
pca 

l      

erlotinib


 






  


  



 





 


 


 







 






  


  



 





 


 


 






  

  

pca 

   

  
  

  

  

 

  

  

sne 

  
 



 


   

  



 


   






 



 

 



 

 


 











 

    

     
  
 







  
  















 







 

 
   
 
 

 




 

  

 
 







  





 

  




    
   
  



 

  
  





  

 



 

 






 

 




   
   


  









 




    
 
     

    



 

  









  






 



 

 



 

 


 











 

    

     
  
 







  
  















 







 

 
   
 
 

 




 

  

 
 







  





 

  




    
   
  



 

  
  





  

 



 

 






 

 




   
   


  









 




    
 
     

    



 

  






sne 

  

  

  

 

  
  

sne 










  

 










  






  

  

  

 

  

  

sne 

figure       d visualization results  the first and second columns are l       and erlotinib  respectively 
purple  sensitive to drug  blue  insensitive to drug   top  pca component clusters 
the x axis and y axis represent the first and second principal components   bottom  sne
component clusters  x and y axis represent the first and second components of the sne
dimensionality reduction 

  d  optimal dimensionality reduction
with pca  it is necessary to select the optimal number of principal components to include in calculations 
tsne requires tuning of both the perplexity  a measure controlling the number of nearest neighbors  and
the ratio of points to be used as landmarks  in order to determine the optimal dimensionality reduction
methodology for our data  we examined a broad range of parameterizations of both pca and tsne  we
looked at the performance of the different parameterizations using simple classification  k nearest neighbors 
and regression  linear models  algorithms  figure     visualizes the results for each drug 

   classification methods
instead of using continuous sensitivity data as the response  we divided the drug cell line pairs into sensitive
and not sensitive classes using a sensitivity value threshold of    we randomly held out     of the data
for testing and examined the effect of each algorithm on our classification accuracy 

fi  

count

lm

 

  
   

   

   

 

   

 

   

value

raf   
tki   
pd       
azd    
tae   
aew   
zd    
azd    
irinotecan
pf       
lapatinib
lbw   
erlotinib
pha      
nilotinib
pd       
panobinostat
paclitaxel
  aag
topotecan
plx    
l      
nutlin 
sorafenib

value
tsne       

tsne       

tsne        

tsne        

tsne        

tsne      

tsne       

tsne       

pca      

tsne        

tsne      

pca      

tsne       

tsne         

tsne        

tsne        

tsne       

tsne        

pca     

tsne       

pca      

tsne       

tsne        

tsne       

tsne      

tsne      

pca      

tsne       

tsne       

pca    

pca    

tsne       

pca    

pca    

tsne         

tsne        

tsne       

pca      

tsne        

pca      

tsne       

pca     

lbw   
  aag
pd       
azd    
plx    
aew   
raf   
pha      
tae   
azd    
zd    
irinotecan
tki   
erlotinib
lapatinib
pf       
pd       
l      
nutlin 
sorafenib
paclitaxel
nilotinib
topotecan
panobinostat

  

color key
and histogram

knn

 

count

color key
and histogram

figure      optimal dimensionality reduction columns are drugs and rows are parameterization of pca
   principle components  and tsne  perplexity  landmark ratio   lighter coloring indicates a
higher percent accuracy and correlation for discrete and continuous data  respectively  rows
sorted top to bottom are from worst to best performance 

  a  k nearest neighbors
as a first  and most simple  classification algorithm  we analyzed our data using the k nearest neighbors
 knn  implementation from the r package fnn  knn has a convenient intuition for follow up to dimensionality reduction as  in the ideal case  we are forming clusters which will bring similar data labels to a
similar geometric location 

  b  support vector machines
maintaining a desire to work with classifiers that have intuitive interpretations  we examined the ccle
data using support vector machines  svms   we hoped that the hyperplane decision boundaries formed by
support vectors would nicely capture some of the visual cluster patterns  we used the svm implementation
from the r package e     

  c  naive bayes
we implemented naive bayes also using the r package e       while the independence assumptions in our
data may not necessarily hold true  we found that this simple method was comparable in its performance to
the other classifiers 

  d  neural networks
we used the r package nnet to implement a discrete neural network  for this implementation we used the
softmax version of the nnet package to use maximum conditional fitting  this binary implementation is a
special case of the multinomial implementation of softmax  we scaled the input data such that each column
has mean   and standard deviation   

  e  comparison
results are shown in table      while the svms performed slightly better than all other algorithms  the
accuracy of the various approaches were surprisingly similar  even at the single drug level  classifiers perform
with similar levels of accuracy  where some drugs are easily classified and others confound every methodology
we implemented 

fi 

 

discrete

 

count

  

color key
and histogram

   

   

   

value

k neares
svms

naive ba

table      comparison of classification algorithms  color represents the percent accuracy of the predictions 

   regression methods
we found that despite implementing and tuning four classification methods  we seemed to reach a limit in our
predictive power  as a result  we then tried to see if we could predict sensitivity values on a continuous scale 
this implementation is likely more biologically useful  because in reality therapeutics are often designed to
have certain dose response curves and the most strongly inhibiting state is not always the most useful to
predict  we decided to assess the success of our regression algorithms by looking at the correlation between
the real and predicted values  we implemented two continuous methods  a continuous version of neural
networks and linear models 

  a  linear models
as a simple regression model  we experimented with implementing linear models using the lm function in
r  due to their simplicity  we also utilized the linear models in determining the optimal dimensionality
reduction approach 

  b  neural networks
we used the nnet package in r  which uses one hidden layer  we scaled
each column of our input data to fit a distribution with mean   and standard deviation    the parameter we adjusted the most to find the optimal
prediction was the size of the single layer or how many units were included
in the prediction  we found that a smaller layer size was more successful at
prediction  but the margin between that improvement and the other implementations was minimal  as a result  we used in our final model a hidden
layer with   units 

  c  comparison
results are shown in table      the neural networks slightly outperformed
the linear model implementation  as with the classification algorithms 
certain drugs seem to be either inherently easier or more challenging to
predict 

   next steps
one aspect of our model that may interfere with successful prediction is the decision to interpret missing
data as sensitivity values of    the way our model is designed  this implies that missing values become cases
where there is absolutely no growth inhibition  these values should either be transformed to be closer to
the boundary between sensitive and non sensitive states or dealt with in a more sophisticated way  we also

l      
nutlin 
sorafenib

neural ne

lbw   
erlotinib
pf       
nilotinib
pha      
pd       
panobinostat
paclitaxel
x  aag
plx    
topotecan

percent correctly classified
     
     
     
     

aew   
azd    
zd    
azd    
tae   
pd       
irinotecan
raf   
tki   
lapatinib

approach
k nearest neighbors
support vector machines
naive bayes
neural networks

fi 

 

continuous

 

count

 

color key
and histogram

 

           

value

linear models

avg  correlation
     
     

sorafenib
nilotinib
tki   
paclitaxel
erlotinib
irinotecan
pf       
l      

approach
linear models
neural networks

lbw   
  aag
azd    
pd       
plx    
nutlin 
pha      
aew   
tae   
raf   
zd    
azd    
panobinostat
pd       
lapatinib
topotecan

neural nets

table      comparison of regression algorithms  color represents the correlation of predicted with the actual
data 
discovered that some drugs were more easily predictable than others  from just looking at how the sensitive
and non sensitive cell lines separate when plotted with the first two pca components  we can already begin
to see that some data are more easily separable  these drugs also often had more successful prediction rates
for the models we implemented  thus  we can focus follow up experimental efforts on studying the predictive
ability of our model on these well characterized drugs 

   conclusions
the challenge of predicting cell line sensitivity to drugs has been tried many times on many similar data
sets with varying features  no one has been able to find a good prediction model  we experimented with
  different models  attempting to frame the problem both as a discrete and a continuous model  we found
that the majority of methods had comparable predictive power  the accuracy of these models may help to
gauge which therapeutics can be looked at more closely in given cell lines  but are by no means sufficient to
address this clinical and biological research question  the predictive power of our model  as well as other
similar therapeutic efficacy prediction problems  are also quite limited by what we know about genomics
at this stage  we have three types of input data   oncogenic mutations  gene expression  and copy number
variation   but there are many more parameters characterizing cells  and particularly cancer cells  that may
be more strongly related to a therapeutics inhibiting potential  as with most current biological models  we
can work well with what data we have  but imminent biological advances will likely improve this prediction
problem  however  the models we have implemented provide a success rate substantially above random
that provide useful context to interpreting classes of therapeutics and gauging the general behavior of each
therapeutic in a given cellular context  predicting therapeutic efficacy is an important biological and clinical
research challenge  and our results take a strong first step toward a practical research model for cancer cell
sensitivity 

   implementation notes
a majority of data analysis was performed using the r programming language  due to the high availability of
machine learning packages  sne calculations utilized a python interface on top of a binary implementation
of the barnes hut sne algorithm 

   acknowledgements
we would like to thank david knowles  who motivated our investigation of the ccle dataset and provided
us with the processed data for analysis 

fi