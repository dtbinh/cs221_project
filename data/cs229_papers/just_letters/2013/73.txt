 

identifying phishing attacks
brandon azad
bazad stanford edu

abstractphishing emails attempt to steal a users personal
information by masquerading as legitimate transactional email 
recently  stanford has been the victim of a large number
of phishing attacks  which caused the author to wonder
about the difficulty of classifying phishing emails  the author
examined the accuracy of several existing algorithms  including
naive bayes  logistic regression  and support vector machine
 svm  classifiers  on the bag of words and augmented bag of
words models of phishing and non phishing emails  of these 
the linear svm under the augmented bag of words model
performed best  additionally  the author attempted to use an
svm with a nonlinear word subsequence kernel  but this kernel
was determined to be too computationally expensive to deploy
in a real world setting 

i  i ntroduction
phishing attacks impersonate legitimate transactional
electronic communications from trustworthy entities in order to steal a users personal information  often targeting
usernames and passwords  credit card numbers  or social
security numbers  phishing emails are designed to resemble
transactional email from the impersonated institution closely
so that the user believes she is interacting with the legitimate
institution  these attacks have become a serious problem
with the rise of digital commerce  since both money and
information access are put at risk     
flagging suspicious messages before the user opens them
is one way to mitigate the threat posed by phishing email
attacks  while the task of identifying phishing emails is similar to that of identifying spam emails  less research has been
conducted into the former      additionally  the difficulty of
classifying phishing emails is compounded since the emails
are designed to look like legitimate transactional email 
recently  i have received a surge of phishing attempts
to my cs stanford edu inbox  which made me curious  how
difficult is it to identify an email as phishing  to narrow the
scope of the question  i only considered fully offline analysis
of the email features  while current research looks at both
offline features  such as whether the email contains words
such as verify your account  and online features  such
as whether remote javascript loaded from a link attempts
to modify the status bar       offline analysis is appealing
since no network access is necessary to classify the email 
this can be advantageous when the additional processing
incurred by classification is costly  eliminating factors such
as network access can significantly reduce classification
time 
this paper aims to examine techniques that can be used
to identify phishing emails with high accuracy  the emails
are first examined under the bag of words model  a multinomial event model used for natural language processing 

the emails are analyzed using a naive bayesian classifier
to establish a baseline classification accuracy  the bag of
words model of the email is then extended to include   
additional features extracted from the emails in the dataset
and reclassified with naive bayes  many of these features
have already been examined by the literature on phishing
email classification      next  logistic regression and support
vector machines  svms  are examined as replacements for
naive bayes  finally  i attempt to use a nonlinear word
subsequence kernel based on          to capture the meaning
inherent in the emails better than the bag of words model 
ii  data c ollection
i used a set of public corpora of phishing emails available at      these corpora contain various hand selected
phishing emails  i used two of these corpora  the first of
which contains      emails collected between november
         and august          and the second of which
contains      emails collected between august          and
august          it was discovered during the processing and
feature extraction phases that some of these emails are best
described as traditional spam rather than phishing emails  so
such emails were removed from the data sets when found 
i also used two corpora of non phishing emails  the
first is the publicly available spamassassin easy ham ham
corpus available at      the easy ham corpus consists of
     emails and was published in       the second corpus
was collected from my personal stanford email account and
consists of      non phishing emails i received in the past
year 
this dataset is likely not representative of phishing and
non phishing emails received at a typical inbox since it was
compiled from multiple sources at widely different times 
however  there are very few publicly available phishing
corpora  and  at the time of this writing  none that i could
find with both legitimate transactional email and phishing
email  it would have been better to have a single corpus
containing phishing  legitimate transactional  and ham email 
but since no such corpus could be located  this compilation is
the best i could achieve  furthermore  my personal account
collected only    phishing emails during the course of
this project  which was not sufficiently diverse to train a
classifier  subsequent research could be improved by using
a unified data set  however  for the purposes of this paper
the dataset used here is likely sufficient 
iii  f eatures
the dataset was preprocessed to extract the relevant
features  each email was split into a header and body  and

fi 

if the email was a multipart mime message only the part
containing textual html content was used  if the body was
html  it was parsed and decoded to extract the textual
content  once the text had been extracted  it was scanned to
identify the following common formats 
address an address  e g  postal 
url a textual  non anchor  url 
email an email address 
telephone a telephone number 
ipaddr an internet protocol  ip  address 
date a gregorian calendar date 
time a time  and
number a number 
these generic tokens replaced the original text of the
message when they were identified  all non alphabetic
characters were then removed to yield the text of each
email  finally  each word in the text was stemmed using
the snowball stemmer     available through the natural
language toolkit     to produce the stemmed text 
the stemmed text was further processed in order to create
the bag of words representation  the frequency of each word
in the stemmed text for each email was determined  and
then for each corpus the most frequently occurring words
and the words appearing in the most number of emails were
recorded  this resulted in a vocabulary of      words for
the bag of words model  all other words were discarded
to form the word frequencies for each email  this list of
     word frequencies formed the feature vector for the bag
of words representation 
during subsequent analysis  this feature vector was augmented with    additional features extracted from the email
headers  the original html  and the text 
charsperword the average number of characters per
word in the email text 
difffromreplyto whether
the
from 
and
reply to  header fields are different 
domainnumdots the maximum number of dots in the
domain of any url 
inreplyto whether the email header contains the
in reply to  field 
ipinurl the number of urls containing ip addresses 
javascript whether the email contains javascript 
numscripts the total number of script environments in
the email 
numurls the total number of urls in the email 
uniquewordcount the number of unique words in
the stemmed text 
urlhtmlentities how much the urls shorten when
html entities are decoded 
urllinkmismatch how many link target domains do
not match link text domains 
urlnumtlds the number of different top level domains
in the urls of the email 
urlport the number of explicit port references in urls 
and
wordcount the total number words in the text 

fig     the learning curve for naive bayes using the bag of words model 
although the learning curve was generated with    monte carlo repetitions
per training set size  significant irregularities remain 

many of these features have been used before in phishing
classification  while some i have not found in previous
literature 
iv  r esults
using the bag of words model  a naive bayesian classifier
was trained with laplace priors on a stratified random
sample of    percent of the data  when classifying the
remaining    percent  it achieved an accuracy of      
percent  establishing a baseline empirical error of     
percent  the training error of the classifier was      percent 
these high classification accuracies indicate that the data is
readily separable based on the most frequent words observed
in each corpus  using the bag of words model augmented
with the additional features  the naive bayesian classifier
was again trained on a stratified random sample of   
percent of the data  empirical error on the remaining   
percent of the data was      percent  thus  the addition of
the extra features improved classification accuracy by     
percent  this improvement is considerable given the high
classification accuracy without the additional features 
learning curves for all models and classifiers were generated using    monte carlo repetitions per training set size 
the learning curve for the naive bayesian classifier using the
augmented bag of words model  figure    suggests that the
classifier is encountering an issue with bias  unfortunately 
the confusion matrix  table i  reveals that the classifier
missed an unacceptably high       percent of phishing
emails  even though it only incorrectly labelled      percent
of non phishing emails 
to address the bias issue  a logistic regression classifier
was trained on the dataset using the augmented bag of words
model  however  the logistic regression classifier did not
reach convergence within    hours  the maximum time a
job is allowed to run on the computational clusters used

fi 

fig     the learning curve for naive bayes using the augmented bag of
words model 

fig     the learning curve for an svm using a linear kernel and the
augmented bag of words model 

table i
c onfusion m atrix for naive bayes

phishing
not phishing

as phishing
            
          

as not phishing
            
             

fig     the learning curve for logistic regression using a subset of just
   features  the features chosen were identified during forward feature
selection using an svm with a linear kernel 

for this project  thus  it was decided that logistic regression
would be run on a smaller feature set  using just    features
identified during forward feature selection on the linear
svm  see table ii   the logistic regression classifier reached
convergence with a test error of      percent  which is higher
than that of either naive bayesian classifier  however  the
reduction of the feature space reintroduced the bias problem
observed in the naive bayes classifiers  figure    
using an svm with a linear kernel produced a classifier
with a test error of      percent and a training error of
     percent  the learning curve for this classifier  figure   

suggests that there is now a problem with variance  although
classification accuracy is still very high at      percent  in
order to narrow down the features used  forward feature
selection was used to identify the top    features  the first
   of which are listed in table ii  however  training a linear
svm on just these    features identified by forward feature
selection produced a classifier with      percent test error
and      percent training error  experimenting with using a
quadratic kernel as opposed to a linear kernel produced a
classifier with significant overfitting  although training error
was just      percent  the test error was     percent  this
high variance is also visible in the learning curve  figure    
the svm with the linear kernel was the best classifier of the data  interestingly  backward feature selection
revealed that the augmenting features wordcount and
uniquewordcount actually were the least helpful in
classifying emails  and removing them improved training
error slightly  the confusion matrix is given in table iii 
the classifier missed      percent of the phishing emails
and incorrectly flagged      percent of the non phishing
emails  this is a significant improvement on the false
negative rate of naive bayes  additionally  if we remove
the augmenting features from the svm  the test error rises
to      percent and the classifier misses      percent of
the phishing emails  indicating that the augmenting features
are extremely important in correctly classifying emails as
phishing 
examining the decision boundary for the first two features
identified during forward feature selection  figure     it is
apparent that the svm is simply drawing a line separating
words that are common to each type of email  this hypothesis is also supported by the most important features found
during forward feature selection in table ii  words such
as account  ebay  and bank are found almost exclusively in phishing emails while words such as stanford 
science  and linux are found almost exclusively in non 

fi 

table ii
f orward f eature s election on augmented bag of w ords
m odel
feature
account
urllinkmismatch
stanford
ebay
time
anni
into
der
del
ipinurl
some
 x 
inreplyto
paul
attend
untitl
scienc
bank
linux
submit

cumulative error
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

fig     the svm decision boundary plot for the first two features found
during forward feature selection  most negative  non phishing  training
examples reside at         and the decision boundary simply separates that
point from the rest of the graph 

into and some  this motivates the search for a model
that better captures the intrinsic meaning of the contents of
an email than the  augmented  bag of words model 
v  w ord s ubsequence k ernel

fig     the learning curve for an svm using a quadratic kernel and the
augmented bag of words model 
table iii
c onfusion m atrix for l inear svm

phishing
not phishing

as phishing
             
          

as not phishing
          
             

table iv
c onfusion m atrix for l inear svm without augmenting
f eatsures

phishing
not phishing

as phishing
             
          

as not phishing
          
             

the bag of words model assumes that the class of each
email is drawn independently and identically from some
distribution of email classes  and then for a fixed class the
words of each email are drawn independently and identically
from a distribution that depends on the email class  while
this model performs well in many practical applications  it
does not account for word ordering or other indicators of
meaning  thus  it is possible that using a nonlinear kernel
that can better measure the similarity in meaning between
two emails might produce better results 
one such kernel k is the word subsequence kernel
described in           this kernel treats each input s and
t as a sequence of words  and finds the number of word
subsequences of length p common to both s and t  a
subsequence can be penalized for gaps by a factor  to prefer
subsequences that are close together  this kernel could better
capture the inherent degree of similarity in meaning between
two emails since similar communications are more likely to
share words and phrases than dissimilar communications 
an efficient solution can compute kp  s  t  in time
o p s  t   and can be used as a nonlinear subsequence
kernel  to correct for the fact that longer sequences have
more potential subsequences  the kernel was normalized 
kpnorm  s  t    p

phishing emails  features such as urllinkmismatch
and inreplyto play a similar role  additionally  many of
the words identified in forward feature selection are common
words that carry little content or meaning  words such as

kp  s  t 
 
kp  s  s   kp  t  t 

however  training an svm classifier with this kernel did
not complete within the    hour time limit for any value
of p greater than    and subsequently it was discovered
that computing the unnormalized kernel function between

fi 

emails took an average of   seconds  thus  this kernel is
too computationally expensive to be feasible for real world
use under current technology 
vi  c onclusions
while it was discovered that the word subsequence kernel
was too expensive to be a viable real world solution to phishing classification  other classifiers performed adequately for
this task  all the classifiers examined achieved over   
percent accuracy  although naive bayes and the svm with
the linear kernel performed best  naive bayes misclassified
over    percent of phishing emails  however  while the linear
svm misclassified only      percent of phishing emails 
without augmenting the bag of words model with additional features  the linear svm misclassifies      percent
of phishing emails  indicating that the additional features
significantly improve classification accuracy  performance
of the svm was on par with naive bayes and logistic
regression operating on the reduced feature set  making
the linear svm a practical real world method of flagging
potential phishing emails before they reach the user 

r eferences
    p  p  stavroulakis and m  stamp  eds   handbook of information and
communication security  springer       
    w  gansterer and d  polz  e mail classification for phishing defense 
in advances in information retrieval  ser  lecture notes in computer
science  m  boughanem  c  berrut  j  mothe  and c  soule dupuy 
eds  springer berlin heidelberg        vol        pp         
 online   available  http   dx doi org                             
    n  cancedda  e  gaussier  c  goutte  and j  m  renders  wordsequence kernels  journal of machine learning research 
vol     p             
    j  rousu and j  shawe taylor  efficient computation of gapped
substring kernels on large alphabets  j  mach  learn  res   vol    
pp            dec       
    j  nazario  phishing corpus  published online         online  
available  http   monkey org jose wiki doku php
    spamassassin  published online         online   available  http 
  spamassassin apache org publiccorpus 
    m  f  porter  snowball  a language for stemming algorithms 
published online  october        online   available  http   snowball 
tartarus org texts introduction html
    s  bird  e  klein  and e  loper  natural language processing with
python  analyzing text with the natural language toolkit  beijing 
oreilly         online   available  http   www nltk org book

fi