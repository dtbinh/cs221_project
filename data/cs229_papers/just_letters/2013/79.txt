validating user spam reports in chat networks
arjun gopalan  ashish mathew  prachetaa raghavan  arjung  amathew   pracheta   stanford edu

   abstract
in this paper  we consider the problem of validating user reports and detecting malicious users in chat networks  bad
behavior in chat networks is usually handled by a rather tedious system where the victim generates a report for the
administrators to act on  however  since most of these reports are spurious  administrators have to manually validate each
report before taking action  we analyze the effectiveness of naive bayes and support vector machines using a couple of
different kernels  our experiments indicate that a svm based algorithm using a polynomial kernel with the right features
achieves an overall accuracy of            the recall for the positive and the negative classes are also over     

   introduction
this problem falls under the broad domain of moderating users in online chat rooms networks  this is an active area of
development and there is an ever increasing need for better solutions  the particular problem of focus here is validating
user spam reports and detecting malicious users in the context of  chatous   a specific online chat network  the dataset was
provided by the company  the current state of the banning system in chatous involves administrators having to manually
view the conversations corresponding to the reports and then decide whether or not to ban a given user  this is rather
cumbersome as the number of reports keeps increasing over time  this necessitates a good machine learning algorithm that
would automatically validate user generated reports as well as predict probable spammers  we describe our analysis using
naive bayes as well as svms  the code was written in python and most of the standard algorithms used were from the
sklearn package      section   describes the framework for the system including details about the dataset and definition of
positive and negative classes  section   motivates feature selection and briefly describes the chi   statistic  section  
describes our implementation and analyses the accuracy of naive bayes and svm  we conclude in section   and mention
some future directions in section   

   framework
    dataset
there are two data sets   the profiles dataset that contains the age  sex  location  screen name and about me section for
each user and the conversations data set that consists of over    million conversation logs  a conversation is a persistent
entity that corresponds to the entire history of communication between a pair of user profiles  there are over        
registered user profiles  this conversation log includes the time when they first talked to each other  time when they last
talked to each other  whether the conversation is still alive and if not  who deactivated it  whether either of the two was
reported by the other and finally the bag of all words used in the conversation 

      preliminary approach
our preliminary approach was to assume that the reported field in conversation logs is accurate  conversations where
someone was reported are labeled    and    otherwise  this is a nave way to define class labels and unsurprisingly  it
resulted in a prediction accuracy of around    with naive bayes  thus validating the premise of our project that most
reports are spurious 

    class definition
since we did not have official data that tagged users as clean and malicious  we had to select a reasonable way to define
our class labels  the training set consisted of two types of conversation logs chosen as follows  for each user profile we
keep a count of reports against him  the top k profiles with the most reports against them are assumed to belong to
malicious users  profiles that have never been reported are assumed to belong to legitimate users  a conversation where at
least one participant is malicious is labeled    and a conversation where both participants are legitimate is labeled as    or
   

fi   feature selection
it is not surprising that the number of words in the dictionary is close to   million  in chat networks  one can expect to
have many typographical errors  the motivation for feature selection for words is primarily driven by the following
reasons 
i  commonly occurring words in the english dictionary were part of the dictionary in our dataset 
ii  too many features usually results in over fitting  this means that the training error will be small whereas the
testing generalization error will be significantly larger 
iii  irrelevant features may negatively impact the performance of the learning algorithm 
one can also imagine pruning the features to remove commonly occurring words prior to feature selection  a brief
literature survey suggested that the chi   statistic is a good approach for feature selection for text classification problems 
fortunately  this was also implemented in the sklearn package  we now briefly describe the chi   feature selection
algorithm  we assume that the readers are aware of the chi   probability distribution 
chi   is a statistical feature selection approach that captures the divergence from the expected distribution  if one assumes
that the occurrence of a feature is independent of the class value      in effect  it measures the lack of independence
between a class label  c and a term  t  
the   statistic  chi    as defined by yang and pedersen in     is given by the following expression
  t  c     n   ad  cb        a   c    b   d    a   b    c   d  
where n is the number of documents  a is the number of documents of class c containing the term t  b is the number of
documents of other class  not c  containing t  c is the number of documents of class c not containing the term t and d is
the number of documents of other class not containing t  this is comparable to a chi   probability distribution with one
degree of freedom 
chi   was performant enough for bringing down the number of features  words in this case   hence  we did not
experiment with different feature selection algorithms  one experiment we however did regarding this was to vary the
number of chosen features and measure its impact on the overall accuracy of the learning algorithm  figure   in section
      plots the results of this experiment  
additionally  we added a few extra features that significantly aid the learning algorithm  these include  the presence of a
report in a conversation  the number of reports against a user  the number of disconnects against a user  age  sex  the  about
me  section  some of these features like the number of reports disconnected against a user had to be made binary values
because absolute count values were large and the algorithm  svm  did not converge  we do this by considering a relative
threshold  for example  if a user has been reported in     of all the conversations he was involved in  the feature value
would be     and    otherwise  there was other information that we expected to be useful but actually degraded the
accuracy of prediction  a few of these are  the location of both the users in a given conversation  whether or not both
users were from the same region  whether or not the conversation ended in a friendship and whether or not the
conversation is ongoing  in chatous  there is exactly one conversation thread between a pair of users 

   implementation and analysis
we started out with the basic evaluation using nave bayes so as to get a general idea of classification and then proceeded
onto different svm kernels for classification 

     nave bayes
we implemented nave bayes using multinomial distribution by using all features  which include word vectors of around
         sex of the user  user location  duration of the conversation and a few other features   this resulted in a very bad
recall percentage for spam   around      on an average as shown in fig    when we performed feature selection using

fichi   and included relevant features as mentioned in section    we included      features for the run in fig     we
observed a significant drop in the spam recall accuracy as shown in fig    this is due to the fact that nave bayes assumes
independence of features and we observed after pre processing of the data that the features are in fact not independent and
therefore  with feature selection  we lose some features leading to a loss in the recall percentage for spam  the x axes in
both graphs below represents the training set size and the y axis represents accuracy 

fig   nave bayes without any feature selection

fig   nave bayes after feature selection using chi square

    svm
from feature selection  we observed that we did not have independence of features and therefore  mapping the given set of
data points to a higher dimension would definitely help  however  we needed to choose the right features and the right
kernel for good results  we explain two kernels that gave us good results  one is the gaussian kernel and the other is a
polynomial kernel with degree    all the experiments involving svms were regularized  the reason is that even after
feature selection  we are still susceptible to over fitting  regularization of svms would smoothen the final learning curve 
this was particularly the case when we experimented with polynomial kernels with high degrees 

      gaussian kernel

fig    plot showing accuracy vs  number of true
spam reports using a gaussian kernel

fig    plot showing accuracy vs  number of features
using chi   feature selection and gaussian kernel

fig   is a plot of accuracy vs  number of true spam reports that were considered during training  we observe that using a
gaussian kernel  our accuracy of prediction is not satisfactory  we observe that our recall for spam is around      for
values of    to     taking    to    top reported users as the true spam reports   however  our non spam accuracy is pretty
low at around       due to the fact that our data set is mainly biased towards non spam reports  which include no reports
too   we have a really low total accuracy  this showed that mapping to an infinite dimensional space is not helping us
much 

fione other heuristic that we considered was aimed at removing words occurring frequently in both classes of
conversations  the threshold for the calculation was varied and we observed that the accuracy improved but was still
performing lower than feature selection using chi   

      polynomial kernel
we now consider a polynomial kernel of degree   for the support vector machine  as before  we use the chi   test for
feature selection  fig    shows our variation of accuracy vs  number of features included using the chi   test  with the
additional features included again  and we observe that with      features included from the chi   test  we have a very
good recall for spam   around      and a total accuracy of about       fig    shows an increasing trend for accuracy as
the training set size increases and we observe that at a training set size of       examples  we have a significantly high
accuracy of     and a recall of around       the goal is to find the minimal training set size that would result in a high
prediction accuracy  fig    shows that as we increase the number of true spam reports for our training  the recall for spam
slightly reduces and therefore  we use    as our optimal value for all experiments 

fig    accuracy vs  training set size

fig    accuracy vs  number of true spam reports

fig    shows that as the testing set size increases from       to        we observe that our recall for spam is always high
at around       furthermore  our non spam accuracy and the total accuracy are constantly high  this shows the
predictions are accurate and are independent of the test size  this experiment was performed on a random test data set  we
tested the same on a balanced data set where      of the samples were labeled    and the other     labeled as     we
observed similar results 
confusion matrix  test size of
spam
non  spam
      
   
 
spam
   
     
non  spam
table i  confusion matrix for a polynomial kernel       features after chi   feature selection 
fig    shows the f  score  also f score or f measure  which is a measure of a test s accuracy  it considers both
the precision p and the recall r of the test to compute the score  p is the number of correct results divided by the number of
all returned results and r is the number of correct results divided by the number of results that should have been returned 
the f  score can be interpreted as a weighted average of the precision and recall  where an f  score reaches its best value
at   and worst at    we observe that for the polynomial kernel  our f score is pretty high  around    to       whereas for
a gaussian kernel  we observe an f score of      showing that our svm using polynomial kernel performs well 
table i shows the confusion matrix for a test size of        note that most of the spam users are caught with only  
spammers being wrongly classified as non spammers  this suggests that we do not miss out on many malicious users in
the chat network  however  we do have a significant number of predictions of non spam as spam and would like to bring
that down to a nominal number as a part of our future work 

fifig    accuracy vs  testing set size

fig    f score vs  number of true spam reports

   conclusion
anonymous chat networks are now an important form of anonymous conversations  however  there exist malicious users
who cause disruptions in these networks  to validate user spam reports  manual inspection is not the feasible solution  our
work shows that feature selection is extremely important and in particular  including the right set of features in the learning
algorithm is paramount to achieving good prediction accuracy  we used the chi square feature selection to reduce the
number of features from more than         to      features and manually add a few important features  we have shown
that using an svm with a polynomial kernel of degree   and with regularization  we can achieve a recall spam accuracy of
     and a non spam accuracy of around      these results are promising and we believe our system can be used
effectively validate user spam reports and thereby ban users automatically without manual inspection 

   future work
we recently received a gold dataset tagging users as malicious or clean  we can use that data set to improve on the
implementation described in this paper  one interesting extension would be to build an online learning system that
proactively predicts users as malicious  which will help in immediately banning malicious users in chat networks  one of
the features we might consider here is the number of reports received by each user over a period of time  this can be used
to assign weights goodness scores to users which will then be used in the online algorithm  finally  we will push for the
adoption of our system in chatous  that will be the ultimate real time test of our learning system 

   acknowledgements
we would like to thank kevin guo  co founder of chatous for guiding us through the project and providing us with the
data set without which none of this would have been possible 

   references
    http   www scikit learn org 
    z  zheng  x  wu  r  srihari  feature selection for text categorization on imbalanced data  sigkdd explorations 
      pp       
    y  yang and j  o  pedersen  a comparative study on feature selection in text categorization  in proc  the fourteenth
international conference on machine learning  icml      pages              

fi