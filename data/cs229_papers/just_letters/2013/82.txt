articles  tagging  
tuan  nguyen   rohan  maheshwari   xuesen  li  

i   introduction  
extracting topics from texts have wide applications on many fields  in user experience
design  we can propose a set of tags for user to decide rather than make them type and
choose among hundreds of tags  also  target marketing can make more relevant
advertisement if we understand which topics are more relevant to a user  we try to solve
this problem by working with the new york times database 

i   data  
    data  source    
newspapers from the historical archive of the new york times are collected as our
source data  the corpus includes metadata provided by the new york times newsroom 
the new york times indexing service and the online production staff at nytimes com 
this corpus contains nearly every article published in the new york times between
january          and june           after preprocessing by the new york times
company  research and development  these newspapers are in xml format and contain
different fields including  titles  article abstract  headline  and body etc  these files
also contain news tags which were either produced by indexing service  human labeling 
or by online producer  computer labeling 

    xml  parsing  methodology  
we used lxml library in python as our main tool for xml parsing  the lxml xml toolkit
is a pythonic binding for the c libraries  it is unique in that it combines the speed and
xml feature completeness of these libraries with the simplicity of a native python api 
mostly compatible but superior to the well known elementtree api 
two groups of texts are extracted from each xml articles 
 content  the main content of the article  containing the titles  byline  article
abstract and body 
 tags  only human labellings are selected since human labellings are the target that
we are going to compare with  a label is considered as human labelling only
when classifier class is indexing service and classifier type is descriptor 
 date  date of the newspapers are extracted for organizing purpose

    result  
in total         xml files are parsed  in which        files in              files in     
and        files in      

fiii   parsing  
    motivation  
before proceeding the learning process  we need a good algorithm on breaking down an
article into a set of significant words  these words are then used as inputs for deciding
tags  significant words would usually be object  people or places  therefore  we want to
extract nouns and names from an article 

    pos   part  of  speech   tagging  
the objective for this module was to read text in some language and assigns parts of
speech to each word  and other token   such as noun  verb  adjective  etc   although
generally computational applications use more fine grained pos tags like  noun plural  
for this  we first explored and benchmarked certain existing standard implementations 
like those available with nltk  but  as we started implementing our own pos tagger 
we realized that perfecting this module is itself more than just a quarter long project  so 
after analyzing performance of several existing tools  we picked stanford nlp group s
log linear pos tagger  for analyzing performance we tagged a set of randomly selected
articles from our database  and observed the errors in tagged results  after comparing the
results  stanford nlp s tagger was clearly the best choice  but  since most of our existing
code was in python  we chose the forked  https   bitbucket org torotoki corenlp python 
an already forked  https   github com dasmith stanford corenlp python  wrapper for the
stanford s core nlp tools which are coded in java 
we referred to several papers to get a better understanding of natural language parsing in
general  of which most of them were published from the works of stanford s nlp group 
some of the interesting ones were 






christopher d  manning        part of speech tagging from     to       is it
time for some linguistics  in alexander gelbukh  ed    computational
linguistics and intelligent text processing    th international conference 
cicling       proceedings  part i  lecture notes in computer science       pp 
          springer
kristina toutanova  dan klein  christopher d  manning  and yoram singer 
      feature rich part of speech tagging with a cyclic dependency network 
hlt naacl     
kristina toutanova and christopher d  manning        enriching the knowledge
sources used in a maximum entropy part of speech tagger  proceedings of the
joint sigdat

    frequency  distributions  
another part of parsing articles was to figure out how often do some words occur in
article writing  and in the process prune them our for better tagging  this also enables us
to figure out trending words at different periods of time  for this  we created a redis
database with frequency distribution of all words in all the articles in our collected
database  the choice of redis was preferred over any sql based alternative  as key value
stores are way faster than any relational database alternatives  this will help in

fisubstantially increasing our performance in the future  and present   when we want to
quickly retrieve update the frequency distributions of millions  soon billions   of words 

    word  to  vector  representations  
this is ongoing work where we want to explore different ways to represent words as
vectors in   d space  so as to easily compute differences or similarities between different
words  many current nlp systems and techniques treat words as atomic units   there is
no notion of similarity between words  as these are represented as indices in a
vocabulary  this choice has several good reasons like simplicity  robustness and the
observation that simple models trained on huge amounts of data outperform complex
systems trained on less data  but  in our case the similarities or differences between
words  also  at multiple degrees  cam prove to be really beneficial when tagging the
entire article with a tiny subset of related words  for this we referred to the paper on
efficient estimation of word representations in vector space by tomas mikolov  greg
corrado  kai chen and jeffery dean  http   arxiv org pdf           pdf   we are still
trying to develop an efficient implementation of their algorithm in python for our project 

iii   learning  
    evaluation  
before implementing an algorithm is to propose a way of measuring error for the purpose
of model evaluation and comparison  given a set of tags from a single article  we want
our predicted set to be close to that set  here  there are two types of error  one is
predicting tags that are not in the articles set of tag  the other is not predicting tags that
are actually in the articles set of tag  we define the score function between predicted tags
and data tags as 
  

       
 
 

 

where p is precision and r is recall  this score is derived from              for small
 

 

precision    is large  and for small recall    is large  a small precision means that our
algorithms overshoot with tags prediction  it predicts a lot of tags that are not in the data
outputs  while a small recall shows that our algorithm are too conservative  we miss a
lot of tags that are in the data set outputs  by increasing the score  we are trying to play
the algorithm so that its prediction is accurate but not too conservative so that it doesnt
miss so many tags in the data set 
also  we would like to claim that our actual performance of an algorithm is always better
than its maximal scores  since there are tags that are similar in meaning but we dis allow
it in the measurement  for example  the articles tag is theatre  but our prediction is
motion pictures  in reality  we would consider this as reliable tagging  but our
measurement excludes this circumstance for precise measurement 

fi    algorithm   tag switch  learning  
our learning algorithm is simply generating a sequence of individual tag switching 
given an article  we put it against each tag in our universe set of tags  for each tag  we
predict a   or   depend on the content of the article and our algorithm  after running
through the universe of tags  we obtain our predicted set of tags with tags that appeared  
after the process 
for every tag  we use the same learning algorithm for prediction  our first three
candidates are nave bayes  lasso logistic and svm  an article will be decomposed
and filtered into a set of words  this parsing algorithm is used as discussed previously in
this report  before any training process  we would like to build a universe set of tags and
set of words  for tags  we only select tags that appear more than a certain threshold  for
words  we want to keep words that are not too rare and not usual  on top of that  we only
want to exclude words that dont have good correlation with any other tag  these are
considered non significant words  after building the universe sets  we are ready for
training and testing 
before running machine learning for each tag  we also need to decompose an article into
set of words  and the dictionary of words is our feature  we use   types of parsing for this
process 
 word by word  we take each word after a space
 compound parsing  we take meaningful   grams as additional feature  for
example  film producer will be taken as feature instead of film and
producer
 nouns  we takes nouns only from an article

    results  
we trained on      articles within the year       we keep tags that appear at least   
times throughout the training set  words that appear less than    times and more than
     times are excluded from the set  we train on      articles and test on     others 
here is the f score reported on the test set 
nave bayes
svm
lasso logistic

simple parsing
      
      
      

compound parsing
      
      
      

nouns only
      
      
      

iv   conclusion  
by  observing  the  result   we  come  to  have  certain  conclusions   
 robust  learning  method  such  as  svm  and  l  regularization  are  powerful  
 knowing  nouns  is  reasonably  enough  for  predicting  content  of  an  article   
this  relates  to  questions  of  who   where  and  what  in  journalism   
 however   for  industry  use  where  speed  is  necessary   we  dont  need  advanced  
parsing   with  simple  parsing   we  can  have  reasonable  performance   

fi