bilingual continuous space language model
hieu h  pham  hyhieu cs stanford edu
hung tan tran  htran  stanford edu

abstract
we attempt using simple feedforward neural networks to learn a continuous space language model
 cslm  and evaluate its performance on french and english  the training of the model is performed
bilingually  in which we try to capture not only different features from one language  but also the
multilingual relation demonstrated in the parallel training data  we achieved the perplexity of      
on french and       on english  and visualized some interesting monolingual and bilingual relations 

 

introduction

we can compute these probabilities  a popular idea
is proposed in bengio  y  et al         called the
continuous space language model  cslm   compared to n  gram models  cslms are not only more
accurate on unseen context in training corpus but
also more memory efficient  the cslms of bengio  y  et al        uses a neural network to learn
a map from the vocabulary of the language into
a high dimensional vector space  and then based
on this representation of words to compute the n gram probabilities 
since bengio  y  et al        proposed cslm 
the model has been very popular and successful 
such as le h s  et al        or mikolov  t  et al  
      we are particular motivated by the work of
mikolov et al         in which they built a recurrent neural network  rnn  to exploit similarities
among different languages  even ones with dissimilar structures  although their work shows very
promising results of rnn lms  the model is slow
to train  in this paper  we attempt a different approach to a subset of their tasks  specifically  we
focus on the task of french english translation  using a very simplified  and hence efficient  structure
of feed forward neural network  fnn   we achieved
similar results as in mikolov et al        

language model  lm  is a tool that scores the fluency of sentences in a specific language  lms arise
in many different problems of natural language processing  such as speech recognition  handwriting
recognition  or machine translation  goodman j  
       a popular approach for lms is n  gram 
in which the model makes the assumption that a
word wi in a sentence depends only on its n   
precedents  that is
y
p w  w     wn    
p wi  wi     win     
i

n  gram lms estimates from a training data set the
probabilities p wi  wi     win       and then based
on these learned probabilities to score the new sentences  this method suffers many shortcomings 
first  it does not generalize well  if we have a combination of words that has not yet appeared in the
training data set  which very likely happens   the
sentence will be scored    many smoothing techniques have been developed to overcome this issue  such as kneser ney smoothing  ney et al  
       however  a bigger issue is that for an n gram model  we would have to store  v  n   where v
is the vocabulary of a language  whose reasonable
size is about    k  with this requirement  even
tri gram model is a memory challenge 
an alternative to this approach is that instead
of explicitly storing all p wi  wi     win       we
store parameters of another model  based on which

 

bilingualized neural network

our model is based on the cslm described in
schwenk h  et al         we use an fnn with
 

fihieu h  pham  hyhieu cs stanford edu
hung tan tran  htran  stanford edu

bilingual continuous space language model

two layers  which we will call the monolingual fnn 
supposed that we have a language with n words in
the vocabulary  we will learn a matrix w  rn p  
whose row ith corresponds to the representation of
word ith in the vocabulary  when we have an ngram w  w     wn   at the first layer  called the shared
projection layer of the nn  we pick out the corresponding word vectors wwi and concatenate the
first n    of them in to a  n    p  dimensional
vector  which is given to the neural network as the
input  let this vector be c  r n  p  
o 

o 

the work of evaluating gradient at the nns output
layer  the second term  with weight decay factor
 is there to prevent overfitting issues  training is
done by stochastic gradient descent  that is  for
each training example  we do a standard feed forward and then back propagation to compute the
corresponding gradients  e  and finally update 
via the equation
       e
where  is the learning rate  the parameters  and
 are determined at the construction of our nn 
and hence could only be tuned after a few experiments  in our final experiments  we set          
and        

   on   on

hidden layer  h 

   
w   p



wn   p

w 



wn 

adapting the bunch mode idea from schwenk  h  
      we propose a bilingual setup for the training
phase of our nn  instead of having one matrix w
as described above  we used french english parallel
training data  we doubled the size of our matrix
w   to simultaneously hold the vocabularies of both
languages  doing so drastically increases the size of
our nn  in the sense that the sizes of the matrices
m and v exploded by  h  however  as a tradeoff 
our nn has more synapses  and thus  is capable of
capturing similar structures between the two languages via parallel linguistic data 
bilingual training also leads to a change in our
objective function  we have to capture the pair of
next words in our n gram  namely the next english
word and the next french word  our new objective
function is therefore

figure    structure of a monolingual fnn
at the second layer  called the hidden layer 
we used hyperbolic tangent function to compute
d   tanh  mc   b   where m  r n  p h   with
h is the size of our the hidden layer  and tanh is
applied component wise to the vector  the output
layer of the nn has size n   equals to the size of
the languages vocabulary  at this layer  we first
compute o   vd   k  and then we add a softmax
exp  oj  
layer p  where pj   pn
represents the
j      exp  oj    
conditional distribution
pj   p wn   j w  w     wn   

e    

jl

n
x
i  

the objective that the nn tries to minimize is


n
x
 x   x   
e    
yi log pi  
mjl  
vij
 
i  

bilingualization

yi log pi  

n
x

yi log pi

i  



x
x

 
  
m jl  
vij
 
jl

ij

ij

where the terms y log pi correspond to the cross entropy of the model for french  as for the case of
yi log pi   only one yi is one  and all the others are
zeros 
last but not least  in order to keep the size
of the vocabulary  and hence the size of our nn 

where y is the output vector  for which ywn    
and all other components are    thus  the first
summation in this objective function is the crossentropy of the model on it training data  only one
term in this summation is non zero  and this eases
 

fihieu h  pham  hyhieu cs stanford edu
hung tan tran  htran  stanford edu

bilingual continuous space language model
english sentences in the test corpus  we then use
the geometric mean of them as the final perplexity
of our model 

reasonable and manageable  we restrict the size of
vocabulary of both languages into n     k  the
word in the vocabularies of french and english that
we chose are the words which appear most in the
training data set  for each language  we also reserved a special token  called  unk   which generally represents the words that are not in the vocabularies we are considering  linguistically  this corresponds to the action that we treat the words we
are less likely to see the same  in our experiments 
the full size of the english and french vocabulary is
   m   and a simple count through out the training
dataset yields that     of them appears less than
   times in the data 

   

experiment settings

our implementation of the nn is array based  in
our code  we explicitly used the arrays of doubles
w   c  b  m   v and k  the sizes of these arrays
are specified at the construction of the nn  making
the nn scalable  however  we restricted the nn
into a three layer nn as described above  so it is
not adaptable for generic uses of nn  as a tradeoff 
we achieved faster training time  for the avoidance
of creating many classes for different nn routines 
also  we use a   gram model  in which each word
is mapped into the space with dimension p      
and we set h       neurons in the hidden layer 

we extracted our training and testing data from
the europarl french english bilingual corpus provided by koehn  p         the training data set
consists of    k parallel sentences  and the testing
corpus consists of     parallel sentences  to extract our training data  we randomly picked    k
parallel sentences from the corpus  then preprocess
them into lowercase words and separate punctuations from words  such as parlement  debout 
into parlement   debout     we then train our
nn on the extracted data set for   epochs  after each epochs  we compute the perplexity of the
model  if the perplexity does not decrease after an
epoch  we stop the training process  otherwise  we
save our model into a file  and continue with the
next epoch 
all training and testing are performed on our
personal laptop computer  macbook pro running
mac os x maverick  with the processor     ghz
intel core i  and     gb of ram  in our experiments  each epoch runs in about     hours  and all
  epochs are carried out 

 

 

   

   

implementation notes

experiments
perplexities

   

we evaluate our model by measuring its perplexity
on an unseen data set dev  let a          a              
an       be the   grams in our data set  then the perplexity of our model on this data set is computed
via the geometric mean
   n
n
y
ppl  
p ai    ai   ai   ai   ai  

results
perplexities

we observed decrease in perplexities after each
epochs  the result is shown in the table below
epochs
 
 
 
 
 
 
 

i  

due to the structure of our nn  we can only compute parallel testing data  that is  when evaluating
perplexity of the model  require two parallel pairs
of   gram  from french and english  then compute
p and p at the same time  this allows us to simultaneously compute the perplexity of french and

french
    e  
       
      
      
     
     
     

perplexities
english
    e  
       
      
      
     
     
     

total
    e  
       
      
      
     
     
     

table    perplexities of the model after each epoch
 

fihieu h  pham  hyhieu cs stanford edu
hung tan tran  htran  stanford edu

   

bilingual continuous space language model
figure    the same set of countries in french produces roughly the same figure with that in english 
still with europe closed to their centroid 

visualization

further than achieving the perplexities above  our
model also captured many linguistic interpretations
of both french and english  following the idea of
mikolov  t   et al         we implemented the principle component analysis  pca  to project the
word vectors from the    dimension space into the
plane  this shows not only interesting relations in
french and english themselves  but also between
the two languages  following  we plot some interesting relations that we have found  the plot was
carried out by first doing pca on specific words to
produce their corresponding   dimensional vectors 
and then using geogebra to plot those points onto
a plane 

perhaps one of the most amazing achievements of
our model is that similar set of countries in french
produces roughly the same figure  actually  two
figure differs by roughly a rotation  and most significantly  the polygon formed by those countries
have the same counter clockwise order of vertices 
since we projected all words from the two vocabularies into the same linguistic space  there are
also many relevance between two those vectors  for
example  in the    dimensional space  we found out
that the vectors one un  two deux and three trois
are roughly parallel  their component wise difference roughly align into the same ratio  

 

conclusion

compared to the perplexities of less     of state ofthe art language models  ours are considered modest  further than that  we required bilingual training and testing data  which are generally harder to
get than monolingual data  more problematic is the
decoding phase of the neural network  our nn can
only evaluate the n gram probabilities if it is given
parallel input  finally  the worst issue is that although our model is intentional programmed based
on array to speed up the training time  its training
phase turned out to be prohibitively slow  one way
to speed up the training phase is to use the bunchmode optimization of mentioned in schwenk  h  
      but that would require a lot of modifications
in our code to allow cache friendly matrix multiplication  as well as other caching techniques  thus 
there are still a lot of possible improvements needed
for our model 
however  we have shown that the model proposed by schwenk  h         can be adapted to
train on bilingual data and could indeed capture
interesting bilingual features  perhaps after adding
powerful optimization techniques that allow more
efficient training  we can train our model on klingual data  and promisingly  can achieve better
results 

figure    the vector europe appears to be closed
to the centroid of the vectors england  france 
germany  ireland  greece     

 

fihieu h  pham  hyhieu cs stanford edu
hung tan tran  htran  stanford edu

 

bilingual continuous space language model

acknowledgement

in this last section  we want to deliver our grateful
words to professor christopher manning and professor andrew ng  who have given us two interesting and helpful courses  namely cs   n and cs    
without them  this work would have no reason to
be done  and hence could never be done 
we also thank the tas of the two classes  who
have sent out a lot of emails to remind us to stay
on track  among these awesome graduate students 
we thank minh thang luong for giving us many
helpful advice and references 

references
    joshua goodman  a bit of progress in language modeling  microsoft research one microsoft way redmond  wa            
    holger schwenk  continuous space language
models  spoken language processing group 
limsi cnrs  bp            orsay cedex 
france      
    philipp koehn  europarl  a parallel corpus for
statistical machine translation  mt summit 
    
    yoshua bengio  rejean ducharme  pascal vincent  christian jauvin  a neural probabilistic
language model  journal of machine learning
research   pages               
    tomas mikolov  quoc v  le  ilya sutskever  exploiting similarities among languages for machine translation  arxiv      

 

fi