  
  
  
  
  
  
  
  
  
  
  

classifying  news  for  topic  and  sentiment  
thomas  atwood   miguel  de  lascurain   jack  smith  
final  paper  
cs       
dr   andrew  ng  
december             
  
  

  

ficlassifying  news  for  topic  and  sentiment  
  

   

classifying  news  for  topic  and  sentiment  

  
around  the  globe   there  are  more  than         daily  newspapers  selling  close  to       
million   copies   every   day       additionally    there   are   blogs    micro   blogs    periodicals   
magazines   fanzines   etc   how  can  we  make  sense  of  all  this  information   how  can  
we  classify  it  and  aggregate  it  so  that  we  can  perform  quantitative  analysis   
  
this   project   explores   one   possible   answer   to   these   questions    automating   the  
classification   of   news   articles   by   sentiment   and   topic    our   vision   is   to   create   the  
capability   to   track   how   sentiment   on   a   topic   has   evolved   over   time    how   different  
news   outlets   cover   the   same   topic    and    in   the   limit    to   be   able   to   predict   future  
behavior  through  sentiment  trends   

data  
  
for  our  project   we  have  considered  two  databases   an  economic  news  database  for  
training   and   the   new   york   times               dataset    rebecca   weiss    ph d   
candidate  in  communication  at  stanford   provided  us  with  this  data   
  
economic   news   database    our   initial   dataset   is   comprised   of            sentences  
selected  by  rebecca  weiss  and  richard  socher   ph d   candidate  in  computer  science  
at   stanford    from   economic financial   news   in   american   newspapers    weiss   and  
socher   submitted   the   data   to   amazons   mechanical   turk    where   workers   labeled  
each  word   phrase   and  sentence  for  sentiment  on  a     point  scale  from      very  bad   
to      very  good    at  least  three  workers  labeled  each  token   and  the  dataset  stored  
the  average  of  the  three  workers   
  
new  york  times             dataset    the   second   dataset   consists   of   every   article  
published  by  the  new  york  times  between        and          approximately           
articles    the   dataset   is   well   structured      each   article   was   hand labeled   for  
classification  by  topic   however   the  data  does  not  have  labels  for  sentiment   

approach  
  
we   approached   the   problem   using   incrementally   powerful   algorithms    this   allowed  
us   to   compare   the   performance   as   well   as   to   comment   on   the   advantages   and  
disadvantages  of  each  algorithm   
  
nave   bayes    we   applied   nave   bayes   by   creating   a   word   vocabulary   using   the  
median   words   in   our   training   set    that   is    we   chose   an   upper   and   a   lower   bound   for  
the  number  of  appearances  of  a  word  in  our  training  set  and  created  the  vocabulary  
with   the   words   between  those  bounds    we   optimized   the   word   bounds   by   choosing  
the  bounds  that  minimized  the  testing  error   
  

ficlassifying  news  for  topic  and  sentiment  
  

   

additionally    we   assumed   that   the   sentiment   could   be   divided   into   two  
classifications    good   sentiment   and   bad   sentiment    for   this    we   considered   all   the  
articles   in   our   news   article   database   that   had   a   sentiment   score    determined   by   the  
mechanical   turk   workers    less   than   or   equal   to         to   have   a   bad   sentiment   and   the  
rest  to  have  a  good  sentiment   
  
given   this   vocabulary   and   possible   categories    we   applied   a   straightforward  
multinomial  event  model  under  the  nave  bayes  assumption   
  
bag   of   words   classifier   we  recognized  that  by  creating  the  vocabulary  the  way  we  
were   doing   for   nave   bayes    we   were   considering   words   that   didnt   have   any  
emotional   charge   a   priori    for   example    in   our   vocabulary   we   included   the   word  
business   which   is   a   neutral   word   by   itself    it   takes   its   emotional   charge   from  
adjectives  and  context   
  
we  tried  a  bag  of  words  classifier  for  our  second  approach   we  selected  the  words  
for  our  new  vocabulary  by  identifying  the  words  with  the  highest  emotional  charge  
in   our   economic   news   database    once   this   vocabulary   was   created    we   applied   a  
multinomial  event  model  under  the  nave  bayes  assumption   additionally   we  scaled  
the   probabilities   to   reflect   the   magnitudes   of   the   sentiment   of   each   word    for  
example   the  word  rat infested  had  a  higher  weight  than  the  word  inappropriate     
  
support   vector   machine    svm    with   principal   component   analysis    pca    for  
context    as   mentioned   in   socher    et   al              classifiers   that   only   take   single  
words   into   consideration   are   generally   bounded   in   terms   of   prediction   accuracy   
with  this  in  mind   we  sought  a  method  to  represent  words  mathematically  in  order  
to   access   other   machine   learning   algorithms   for   classification    we   chose   to   use  
windowing    which   builds   upon   the   theory   that   a   word   is   given   meaning   by   its  
context   i e   the  words  that  surround  it   to  accomplish  this   we  ran  a  parser  on  the  
new  york  times  database  that  counted  the  number  of  appearances  of  each  word  in  
our         word   vocabulary   in   a   given   words   window    for   the   most   part    each  
window  was  comprised  of  the  five  words  on  either  side  of  the  given  word   we  added  
padding  to  windows  that  fell  at  the  beginning  or  end  of  sentences   
  
we  now  had  an         by         
matrix  containing  word  counts   
and   sought   to   reduce   the  
dimensions   in   order   to   use   a  
svm    to   accomplish   this    we  
ran   pca   on   our   matrix    and  
found   that   using   the   eight  
principal   components    we  
captured         of   the   variance  
in   our   matrix    by   analyzing   a  
few   clusters   of   words    we  
found   that   pca   was   very  

ficlassifying  news  for  topic  and  sentiment  
  

   

successful  at  locating  synonyms  and  closely  related  words   for  instance   the  closest  
words   in   terms   of   euclidean   distance   to   friday   were   monday    tuesday   
wednesday   and  thursday   figure     illustrates  other  representative  clusters   
  
finally    we   trained   an   svm   on   our   labeled   dataset   by   representing   each   article   as   an  
array  of  words   where  words  were  represented  by  their  four  principal  components     

results  

  
by  running  a  nave  bayes  algorithm  
on  the  economic  news  database   we  
achieved           prediction   accuracy  
         above   baseline     by  
examining  how  the  test  error  related  
to   the   training   error   using   different  
training   set   sizes    we   found   that  
there   was   a   bias   issue   since   both  
errors  were  above  our  desired  error  
threshold  of        figure       
  
we   also   looked   at   accuracy   by  
sentiment   score    figure         as   we  
expected   we  found  that  using  nave  
bayes    we   are   very   accurate   at  
predicting   very   bad    scored       to  
       and   very   good    scored         to  
     news   articles   but   our   predictions  
are   not   accurate   for   neutral  
articles    this   result   also   holds   true  
for  bag  of  words   
  
paradoxically    as   we   used   more   powerful   models    our   accuracy   decreased    we  
achieved   an   accuracy   of                above   baseline    using   the   bag   of   words  
approach    we   found   that   this   algorithm    despite   being   in   theory   more   defensible   
performs   worse   than   nave   bayes    we   found   two   reasons   that   could   explain   this  
performance   the  first  is  that  the  words  with  higher  sentiment  charge  do  not  appear  
as   frequently   in   our   training   set   as   we   would   have   liked    hence    some   of   the  
predictions  were  based  on  very  
few   data   points    this   contrasts  
greatly   with   our   nave   bayes  
approach  since  we  had  a  lower  
bound   constraint   on   the  
number   of   appearances   that   a  
word   should   have   for   it   to   be  
considered    the   second   reason  
why   our   algorithm   performed  

ficlassifying  news  for  topic  and  sentiment  
  

   

badly  is  that  in  order  to  determine  the  sentiment  of  a  sentence   it  is  not  enough  to  
consider  the  sentiment  of  the  words   as  exemplified  in  figure      the  context  in  which  
the  words  are  used  can  greatly  affect  the  sentiment  of  the  sentence  as  a  whole     
  
finally   we  achieved  an  accuracy  of        equal  to  baseline   using  the  svm  with  pca  
approach   by  using  pca   we  were  very  successful  at  giving  a  measure  to  the  words  
and   thus   determining   which   words   were   similar   to   other   words   in   terms   of   their  
context    however    by   running   these   contextualized   words   through   our   svm    we  
did   not   achieve   a   high   degree   of  
accuracy   because   knowing   the   context  
in   which   each   word   appears   is   not   the  
same   as   knowing   how   one   word  
changes  the  context  of  the  rest  within  a  
sentence    in   other   words    we   were  
successful   at   encoding   words   but   we  
were   unsuccessful   at   encoding  
sentences   figure     shows  a  summary  of  
the  results   
  

conclusions  
in   summary    we   approached   a   very   well   known   problem   with   known   high   difficulty  
without  prior  knowledge  of  the  more  sophisticated  tools  that  have  been  developed  
to  deal  with  problems  in  this  space   and  were  unsuccessful  at  predicting  sentiment  
on   topics   in   news   articles   on   a   large   scale    however    we   were   successful   at  
measuring   words   in   an   abstract   space   in   such   a   way   that   we   could   determine   which  
words   are   clustered   together    additionally    we   achieved   an        increase   in   accuracy  
of   predicting   the   sentiment   of   a   news   article    finally    through   research   
brainstorming    trial and error    and   exploration   of   natural   language   processing  
 nlp     we   learned   an   enormous   amount   both   about   nlp   and   about   machine  
learning   
  

references  

  
socher   r    perelygin   a    wu   j   y    chuang   j    manning   c   d    ng   a   y       potts   c   
         recursive  deep  models  for  semantic  compositionality  over  a  sentiment  
treebank   stanford   ca   

fi