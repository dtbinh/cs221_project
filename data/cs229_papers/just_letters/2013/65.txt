 

predicting property loan spread using
segmented linear model
final report
sheng zou             jack huang            and yu zhu           
abstractthis project attempts to predict the interest spread
of a property loan based on the borrower and property related
attributes  each attribute can be regarded as a potential feature 
the problem is how to predict the spread accurately based on
those features  this report describes our approaches of using
linear and segmented linear models as well as other clustering
methods  the comparative results and some analysis are also
provided 
keywordsloan spread  ltv  dscr  segmented linear
regression  mars  k means clustering  pca

i 

introduction

in debt and mortgage market  there are a number of pricing
schemes available for determining the loan interest rate  in
particular  many loan originators use risk based pricing
methods  risks are generally composed of systematic risk 
which is the economy wide risk  and unsystematic risk  which
is the company and investment specific risk  in this project  we
attempt to design a model that can effectively capture how the
unsystematic portion of the risk in a loan is related to some key
attributes of the borrower and the property itself 
the loan originators tend to be risk averse  especially post
the global financial crisis  and it is expected that for higher risk
classes  the penalty on the interest rate should be higher than
the lower risk classes  we use the loan spread as the measure of
unsystematic risk  the loan spread is the difference between
loan rate and treasury rate  therefore  by subtracting treasury
rate out we can eliminate the macro economic factors that
affect the nominal loan interest rate  which is the systematic
risk 
since we do not use any specific non linear economic
model as our prior knowledge  it seems natural to fit those
attributes via some simple and extended linear models 
ii 

datasets

our dataset consists of the fixed rate property loan records
in new york from oct      to jun       with benchmark at
treasury   year  treasury   year and treasury    year rate 
table i provides a snapshot of a subset of the records contained
in the dataset  and only some attributes of the records are
shown  all the property records are obtained from the u s 
security and exchange commission website 
as seen in table i  the dataset has both numerical and
categorical attributes 

table i 

a snapshot of new york proeprty loan records

spread

dscr

ltv

year
built

debt
yield

   

    

    

    

   

   

    

    

    

    

property type
rt single
tenant
rt unanchored

    

    

     

    

     

ch  all 

     

    

     

    

    

mu  all 

   

    

     

    

    

of urban

     

    

    

    

   

ss  all 

     

    

     

    

     

lo full service

iii 

features

historically  the most important fields in pricing the loan
spread are ltv  loan to value ratio  and dscr  debt service
coverage ratio   ltv is the ratio of a loan to the value of an
asset purchased  representing the fraction of money borrowed 
dscr is the ratio of cash available for debt servicing to
interest  principal and lease payments  denoting the ability of
the borrower to pay his her debt  in addition  we will also
examine a broad range of numerical features that could
potentially affect the loan spread  including borrower as well as
property related features 


yield  rate of return  of the debt



current year expenses per unit



current year revenue per unit



remaining terms of the loan



allocated balance



built year of the property



occupancy of the property 



number of units of the property



remaining terms of the loan 

in addition  we also use the qualitative features to
categorize the model  one such attribute is the property type 
which potentially influences loan originators perspective on
the risk profile of investing on such property  for example 
hotels are generally more risky than multi family housing  as
the returns of hotels are more difficult to predict  we can thus

fi 

divide the training examples into different categories and
derive a separate regression model on each category 

new york loan  dscr vs average spread  normalized rmse         
   
   

iv 

data preprosessing
   

to account for the difference in benchmark  we need to
adjust the loan spreads by subtracting    bps from the
treasury   year loans and   bps from the treasury   year loans 
as the average gap of treasury rate between treasury   year and
treasury    year are around    bps from oct      to jun      
and   bps between treasury   year and treasury    year  this
procedure further eliminates the systematic risk 

spread

   
   
   
   
   

before training the model  we selected the training samples
with ltv between    to    and dscr between   to   as the
data points with values outside the above range are extremely
rare and considered as abnormal and thus should be excluded
from the pricing analysis 
models

a  linear regression  spread vs  ltv and spread vs  dscr
as our first attempt  we have constructed linear models for
loan spread versus ltv and dscr respectively  we use
rmse  normalized root mean square error  to evaluate the
effectiveness of the regression model  the rms error is
normalized by the mean of the spread  a constant for all models 
which gives us a sense of how big the error is  fig    shows the
  d linear regression of loan spread vs  ltv  the positive
correlation observed is reasonable  as the higher the ltv value 
the higher the risk and thus the higher the loan spread 
new york loan  ltv vs average spread  normalized rmse         

 

   

 

   

 
dscr

   

 

   

 

fig     linear regression of spread vs  dscr

b    d linear regression  spread vs  ltv   dscr
after observing the correlations of the two features
separately  we then combined the two features to construct a  d linear model  it is observed in fig    that the normalized
rmse value only improved slightly from the   d model 
one piece  normalized rmse           

   
   

spread

v 

  

   
   

   

   
   

 
 

   

 
spread

   

   
 

   
 

   

dscr

  
 

 

ltv

   

fig     linear regression of spread vs   ltv  dscr 

   
   
  
  

  

  

  

  

  
ltv

  

  

  

   

   

fig     linear regression of spread vs  ltv

fig    shows the   d linear regression of loan spread vs 
dscr value  the negative correlation observed is reasonable 
as the higher the dscr value  the lower the risk and thus the
lower the spread 

c    d segmented linear regression
we attempt to improve our regression model by
segmentation  for the purpose of prediction  the model needs
to be continuous at the boundaries  the breakpoints are
manually set at midpoint  ltv      and dscr       to divide
the data into four regions  cvx is used for minimizing the least
square error given the above constraints  as shown in fig    
the rmse value further improved from the previous   d linear
model 

fi 

higher variance  the clustered data points and centroids
according to ltv and dscr values are shown in fig     it can
be seen that the rmse do not improve from using mars 

two piece  normalized rmse           

   

spread

k means  k    normalized rmse           
   

 

   

   

   

group 
group 
group 
group 
means

 

   

   

 

dscr

 
 
   
 

   
 

 
   

  
 

dscr

 

 

ltv

   

fig      x  piecewise  d linear regression of spread vs   ltv  dscr 

d  multivaraite adaptive regression spline  mars 
in order to do more segments  we look at the multivariate
adaptive regression splines  mars  algorithm         mars
has the ability to find the optimal placement of breakpoints
using heuristics and perform piecewise linear regression  it also
implements regularization to help avoid overfitting  fig   
shows the result of using mars  which further improves from
linear regression and   by   segmented regression model  the
equation for the model is given as 
 

 

 
 
 

 

 
 
 

 
 

 

 
  

  

  

  

  

  
ltv

  

  

  

   

fig      ltv  dscr  clustered using k mean clustering  k   

f  property type clustering
we suspect that the risk profile for different property types
will be different  therefore  it comes naturally to generate
different regression models for different property types  we
first pick out some major types  which has a record size greater
than    and then categorize the loans according to the types  a
category other is created for those records which belong to
minor types  this prevents us from having a category with too
few records  fig    shows the categorized data points with their
ltv dscr values  ch  coop housing  lo  lodging
 hotels   mf  multi family housing  of  office  rt 
retails  
cateogrizing by property type  rmse           

mars
rms error           

 
ch
lo
mf
of
rt
others

   

   

 

   

   
dscr

spread

   

   
   

 
   

   

 

 
 

   

 

   
 

   
 

dscr

  
 

 

ltv

fig     mars of spread vs   ltv  dscr 

e  k means clustering
a cluster of data with similar feature values should have
similar risk profile  therefore  we also attempt to cluster the
data using k means clustering algorithm and do a linear
regression for each cluster  however  due to the limiting size of
our dataset  which has     records  the smallest cluster for k  
has only less than    data points  which potentially results in

 
  

  

  

  

  

  
ltv

  

  

  

   

   

fig      ltv  dscr  clustered using property type

fig    compares the linear regression models for lo and
mf respectively  it is clearly shown that for similar values of
ltv and dscr  lo loans generally have higher spread  the
two models are therefore very distinct from one another 

fi 
property type   lo

  rmse          

pca  normalized rmse           

   
   

   

   
   

spread

spread

   
   

   
   
   

   

   

   
 

  
  

  

 

  
  

  
 
dscr

 

  
 

 

  

property type   mf

 

ltv

   
spread

dimension  

fig     pca linear regression of spread vs   ltv  dscr 

  rmse          

   

   
   
 
   

   
   

 
  

h  mixture of models
finally  we combine the method of k mean clustering 
property type clustering  and pca with mars to test their
performance  these more complex models can reduce the
training rmse value significantly  as shown in section vi 
however  due to the substantial increase in fitting parameter
numbers  we expect the variance of our model to be very high 
which is illustrated in the large gap between training error and
generalization error in cross validation analysis described in
the next section 

  

   
dscr

  

dimension  

  

 

 

  

  
 

  

vi 

ltv

fig     lo property type regression model  top  and mf property type
regression model  bottom 

g  principal component analysis  pca 
so far only dscr and ltv are considered as the input
numerical features because of their conceived strong
correlation with the spread  however  the data file has many
more numerical features  such as year built and debt
yield  as shown in table i  we would like to use these
features as well  and doing a pca is a natural way to visualize
the result 
first  we examined the data file and picked out nine other
numerical features which might be useful  this prevents us
from including some obviously improper features  such as zip
code  while spread might be correlated with geographical
location  it is unlikely that the relationship between spread and
zip code is linear   now  the standard pca procedure is
performed  the mean value is removed from each feature  and
each feature is then normalized  the features are compressed
down to two dimensions  and used for linear regression 

results and analysis

we used ten fold cross validation to evaluate each of our
models  for the cases of two input features  ltv  dscr  and
eleven input features  including other nine numerical features  
the results are shown in table ii 
table ii 

normalized
rmse
mean
 baseline 
 d linear
 x  linear
mars
pca
k means
type
pca mars
k means  
mars
type   mars

cv results for different models

two input features
 ltv  dscr 
training prediction
error
error

eleven input features
training
error

prediction
error

     

     

     

     

     
     
     
na
     
     
na

     
     
     
na
     
     
na

     
    
     
     
     
     
     

     
     
      
     
     
      
     

     

     

    

     

     

     

    

     

fig    shows the linear regression result 
the first row  mean  means we just predict the spread to
be the average of the spread in the training data and use it as
the baseline to evaluate the performance of other models 
for two input features  we see that the training error is
generally decreased by using more complex models  in contrast 
the gap between training error and generalization error are

fi 

wider when more parameters are introduced  indicating higher
variance and the training data is overfitted  this is reasonable
because those models contain higher vc dimensions 
when we increase the input features to eleven  the problem
of overfitting becomes more obvious  as all models except
linear suffer from huge variance  one exception is pca and
pca   mars  since pca reduces the number of features
back down to    its variance is significantly smaller than the
rest 
however  an interesting observation is that the
generalization error of using pca is still worse than simply
using ltv and dscr directly for linear regression  this
implies that pca should be used for a good reason and it might
not outperform the result of simply picking the best features 
the best performing model is mars with   input features 
this hints that a linear model cannot capture very well the
relationship between the spread and the features  and there
might be some deep nonlinearity inside which is beyond the
goal of our project 
compared with using mars alone  the inferior
performance of k means and type in both training and
generalization error is beyond our expectation  however 
considering the result given by mars  this is not impossible 
because of the inherent nonlinearity  putting our data into
different types does not help overcome this problem  on the
other hand  clustering and categorizing might lead to highly
uneven splitting of the data and some groups might have very
few points  this will cause potential high variance  generally
speaking there should be a tradeoff between bias and variance 
however  in the case of strong nonlinearity and if our model
does not take that into account  both bias and variance can be
high 
another thing that is worth mentioning is that our data is
very noisy  despite the nonlinear structure it has  this is
because the loan originators decision might also be influenced
by subjective factors  and cannot be simply predicted by
limited open objective observation  such large noise will tend
to make our models have large variance given few data 
finally  the loan records have the following characteristic 
many records have missing fields  this implies a large portion
of the data is given only incomplete set of features  our
approach is to substitute the missing fields by the average value
of those records in the training set which have valid feature
values  however  the actual unobserved fields might be far
from the given average 
vii  conclusion
give a data file of loan records  we tried to predict the loan
spread  which is a measure of the unsystematic risk  by using
some attributes from the borrower and the property  we tried
various methods and found that using an adaptive segmented
linear model called mars with two most important features 
i e   ltv and dscr can give the best performance  this is
because the adaptive segmented linear model can partially
describe the non linear structure in the data  and using only two
features minimizes the risk of overfitting 

clustering and categorizing are still potentially promising 
and in high dimensional case combined with mars the
training error is dramatically reduced  however  because of
increased variance  the ultimate performance is not so
satisfying  it still needs to be examined whether increasing data
points can improve their generalization error 
the large noise observed in the loan spread is an indication
that the spread is not well modelled by the given features 
looking for more relevant features is a crucial task to address
this problem 
the last challenge is how to manipulate partially observed
data in our training model  there might be better method than
replacing missing fields with observed mean values 
viii  acknowledgement
we would like to thank keith siilats for his help in getting
us started  offering data  and providing suggestions along the
way 
ix 

reference

    friedman  j  h           multivariate adaptive regression
splines   the annals of statistics       
    open source regression software for matlab octave  url 
http   www cs rtu lv jekabsons regression html
    andrew ng  cs    
http   cs    stanford edu materials html

course

notes 

url 

fi