this version  december         

applying deep learning to enhance
momentum trading strategies in stocks

lawrence takeuchi  
yu ying  albert  lee

ltakeuch stanford edu
yy albert lee gmail com

abstract

and natural language processing  bengio et al         

we use an autoencoder composed of stacked
restricted boltzmann machines to extract
features from the history of individual stock
prices  our model is able to discover an enhanced version of the momentum effect in
stocks without extensive hand engineering of
input features and deliver an annualized return of        over the           test period
versus        for basic momentum 

in this paper we examine whether deep learning techniques can discover features in the time series of stock
prices that can successfully predict future returns  the
objective is a challenging one  while most research in
deep learning considers tasks that are easy for humans
to accomplish  predicting stock returns using publicly
available information is notoriously difficult even for
professional investors given the high level of noise in
stock price movements  furthermore  any patterns
that exist are subject to change as investors themselves
learn over time and compete for trading profits 

   introduction
price momentum is the empirical finding  jegadeesh  
titman        that stocks with high past returns over
  to    months  winners  continue to perform well over
the next few months relative to stocks with low past
returns  losers   subsequent studies find that this momentum effect continues to remain robust in the us
after becoming widely known and applies to international stocks as well as other asset classes including foreign exchange  commodities  and bonds  asness et al  
       for finance academics the fact that a simple
strategy of buying winners and selling losers can apparently be profitable challenges the notion that markets quickly incorporate available information into asset prices  indeed  fama and french        describe
momentum as the premier anomaly in stock returns 
the momentum trading strategy  along with its many
refinements  is largely the product of a vast  ongoing
effort by finance academics and practitioners to handengineer features from historical stock prices  recent
advances in deep learning hold the promise of allowing
machine learning algorithms to extract discriminative
information from data without such labor intensive
feature engineering and have been successfully applied
to fields such as speech recognition  image recognition 

 corresponding author 

considering the pervasive use of historical price charts
by investors and noting that the primary mode of analysis is visual  we take an approach similar to that used
by hinton and salakhutdinov        to classify handwritten digits  in particular  we use an autoencoder
composed of stacked restricted boltzmann machines
 rbms  to extract features from stock prices  which
we then pass to a feedforward neural network  ffnn 
classifier 

   data
we obtain data on individual us stocks from the center for research in security prices 
     sample selection
we restrict our analysis to ordinary shares trading on
nyse  amex  or nasdaq  to mitigate the impact of
any market microstructure related noise  we exclude
stocks with monthly closing prices below    per share
at the time of portfolio formation  this step also reduces the number of examples with extreme returns 
the training set covers the period from january     
to december       which coincides with the period examined by jegadeesh and titman         and contains
        stock month examples  the test set covers
the period from january      to december      and
contains         stock month examples  on average

fiapplying deep learning to enhance momentum trading strategies in stocks

there are       stocks in the sample each month 
     input variables and preprocessing
we want to provide our model with information that
would be available from the historical price chart for
each stock and let it extract useful features without
the need for extensive feature engineering  for every
month t  we use the    monthly returns for month t 
through t     and the    daily returns approximately
corresponding to month t   we also use an indicator
variable if the holding period  month t      falls in
january   thus we have a total of    input variables
for each stock month example 
next we compute a series of    cumulative returns
using the monthly returns and    cumulative returns
using the the daily returns  we note that price momentum is a cross sectional phenomenon with winners
having high past returns and losers having low past returns relative to other stocks  thus we normalize each
of the cumulative returns by calculating the z score relative to the cross section of all stocks for each month
or day  figure   illustrates preprocessing pipeline using the    monthly returns for one example in the test
set 
finally we use returns over the subsequent month  t  
   to label the examples with returns below the median
as belonging to class   and those with returns above
the median to class   

   deep learning model
     model and learning algorithm
we follow an approach similar to that introduced by
hinton and salakhutdinov        to train networks
with multiple hidden layers  our model consists of a
stack of rbms  which unlike full boltzmann machines
have no intra layer connections  each rbm consists
of one layer of visible units  the inputs  and one layer
of hidden units connected by symmetric links  the
output of each rbm serves as the input to the next
rbm in the stack 
we train the encoder network  see figure    layer by
layer in a pretraining step  following hinton        
we split the dataset into smaller  non overlapping
mini batches  the rbms in the encoder are then unrolled to form an encoder decoder  which is fine tuned
using backpropagation  in our implementation  the
number of hidden units in the final layer of the encoder
is sharply reduced  which forces a reduction in dimensionality  as described below  however  the size of this
bottleneck layer is an outcome of the optimization procedure we use to specify the network dimensions rather
than an explicit design choice  
at this stage  the encoder outputs a low dimensional
representation of the inputs  the intention is that it
retains interesting features from the historical stock
chart that are useful for forecasting returns  but eliminates irrelevant noise  we use the weights estimated
thus far to initialize the corresponding weights in the
full network  which is composed of the encoder and a
ffnn classifier  the final step is to train the entire
network using the labeled examples via backpropagation  
     network specification

figure    preprocessing of inputs

 

we choose these inputs given the casual observation
that investors tend to use higher frequency prices when
viewing charts covering shorter windows of time 
 
turn of the year patterns in stock returns were wellknown prior to jegadeesh and titman         in any case 
using dummy variables for each calendar month gives similar results 

we use hold out cross validation to determine the
number of layers and number of units per layer in our
network  in particular  we further divide the training set into two subsets covering           and           respectively  each model specification is trained
on the first subset and then tested on the second  we
choose this approach over k fold cross validation since
we have a sufficiently large dataset and more importantly want to avoid the look ahead bias that could
arise from training the model with data not available
at a given historical date 
 

hinton and salakhutdinov         by contrast  omit an
explicit bottleneck layer in their digit classification example
and use a                     network 
 
see ng et al         for a tutorial on training deep
networks 

fiapplying deep learning to enhance momentum trading strategies in stocks
table    confusion matrix
predicted

actual

 
 

 

 

      
      

      
      

     classification performance
table   summarizes the results in a confusion matrix
with the entries scaled by the total number of test
examples  the model achieves an overall accuracy rate
of         the model is correct        of the time
when it predicts class   and a somewhat lower       
of the time when it predicts class   
figure    network architecture

to keep the task manageable  we fix the number of
units in the penultimate hidden layer at      thus 
our first set of candidate specifications is   s     
and the second is     s   s         where sl denotes the number of units in layer l  a grid search
over the number of units using our hold out cross validation scheme finds that s       and s      give
the lowest classification error  the next set of candidate specifications is     s   s   s          given
the large number of dimensions  we use s       and
s      and only search over s    we find that adding
another layer does not reduce the classification error  
thus our final specification is the five layer network
                 consisting of an encoder that takes
   inputs and reduces them to a   dimensional code
and a classifier that takes these   inputs and outputs
the probabilities for the two classes  while the approach taken is admittedly heuristic  we believe that
it provides a disciplined method to specify a base case
model 

   results
we train our model using the examples from          
and test it using those from            in this section
we keep both the network configuration and weights
fixed  but discuss later how these could be updated
over time 

these probabilities alone  however  do not provide a
complete picture of the models performance since investors actually care more about returns in their objective function  in particular  we would like to compare
the realized returns for the stocks predicted to be in
class   against those for stocks predicted to be in class
   pooling all months in the test set  the average onemonth holding period return is       for stocks predicted to be in class   and       for those predicted to
be in class    or a difference of        alternatively 
we can use past    month returns  from month t    
to t     and predict that examples with past returns
below the median will be in class   and those above
the median in class    this basic momentum strategy
produces an average holding period return of      
for stocks in class   and       for those in class    or
a difference of       
     information content of class probabilities
while these results appear promising  we have used
only a portion of the information produced by the
model  figure   shows the relation between the estimated probability of being in class   according to
the model versus the holding period return that is actually realized for the test set  where we use a gaussian kernel regression to smooth the curve  we see
an increasing relation indicating that a higher class  
probability leads to higher realized returns on average 
since there is no requirement to hold every stock  an
investor could clearly do better if he bought and sold
stocks in the tails of the distribution rather than using
the     threshold to form long and short portfolios 

 

however  once we determine the specification of the
stacked autoencoders we verify that this is a reasonable
choice 
 
our ongoing work suggests that additional layers may
be useful when the number of features are increased 

we rank all stocks each month by their class   probabilities and buy those in the top decile and sell those in
the bottom decile  next month we close out these positions and form new long and short portfolios  repeat 

fiapplying deep learning to enhance momentum trading strategies in stocks

 

     

  

     

enhanced strategy
basic strategy

     

 

cumulative wealth

holding period return

  

     
 
     

 

  

     
 

  

     
     
     
    

    

    

    

    

    

    

    

    

    

class   probability

figure    holding period returns by class   probability

 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

figure    log growth in cumulative wealth

table    average monthly momentum returns            
strategy

decile  

decile   

      

enhanced

      
     
     
    

     
    
     
    

     
    
     
    

t statistic

basic
t statistic

ing this process over the test set generates a monthly
time series of investment returns  for comparison we
also perform this analysis using past    month returns
to form deciles  table   presents the average returns
for these two strategies  which we call enhanced and
basic momentum  respectively  the enhanced strategy
generates an average monthly return of       with a
t statistic of       in contrast  the basic strategy produces a more modest  but still statistically significant 
      per month  
on an annualized basis  the returns from our model
are a very impressive        versus        for basic
momentum  figure   shows the growth  on a logarithmic scale  of    invested in each of the enhanced and
basic strategies from      to       ending wealth  before considering implementation costs  is       for the
basic strategy and         or     times higher  for the
enhanced strategy 
     source of investment returns
we now show that the enhanced strategy is  in fact 
a momentum strategy in the sense that the decile   
stocks have higher past    month returns than the
decile   stocks  table   shows the past    month
 month t     to t     and past    day  month t  returns expressed as z scores for the enhanced and basic
 
these returns are calculated as the arithmetic mean of
the time series of monthly returns 

strategies  we focus on these two features since they
are highlighted in the finance literature  jegadeesh  
titman        as being potential predictors of stock returns  by construction  the basic momentum strategy
takes very extreme positions based on past    month
returns with a     spread in this feature of more than
  standard deviations  the enhanced strategy has a
      spread of      showing that it is also making a
bet based on past    month returns  but in a less extreme way than basic strategy  the lower half of the
table shows that the enhanced strategy buys stocks
that have had poor recent returns and sells those with
high recent returns  consistent with the short term reversal effect  jegadeesh         the basic momentum
strategy  however  has similar past    day returns in
deciles   and    
while we highlight these two features  we emphasize
that the model is likely picking up other more subtle
patterns in the historical price chart that are less intuitive to interpret  table   shows that the enhanced
strategy does not take overly extreme positions based
on past    month and past    day returns as would
be the case if we double sort on these features to form
portfolios as is common practice in finance studies  we
regard this finding  together with the size of the investment returns  as encouraging for deep learning as
it suggests that our model is not merely rediscovering
known patterns in stock prices  but going beyond what
humans have been able to achieve 

   discussion and further work
this study represents one of the first  as far as we are
aware  applications of deep learning to stock trading
and makes two main contributions to the applied machine learning literature  first  we show that stacked
autoencoders constructed from rbms can extract useful features even from low signal to noise time series

fiapplying deep learning to enhance momentum trading strategies in stocks
table    stock characteristics by strategy 

references
asness  clifford  moskowitz  tobias  and pedersen 
lasse  value and momentum everywhere  journal
of finance                  

decile  

decile   

      

past   m ret
  enhanced
  basic

     
     

    
    

    
    

bengio  yoshua  courville  aaron  and vincent  pascal  representation learning  a review and new perspectives  arxiv e prints       

past   d ret
  enhanced
  basic

    
     

     
    

     
    

fama  eugene and french  kenneth  dissecting
anomalies  journal of finance                    
hinton  geoffrey  a practical guide to training restricted boltzmann machines  technical report
mtml tr           university of toronto       

data such as financial asset prices if the inputs are
appropriately preprocessed  second  we illustrate the
potential for deep learning to reduce the need for extensive feature engineering in an application area  financial markets  that has long been of interest to machine learning researchers  our model easily accommodates returns of different frequencies as well as nonreturn data and produces investment results that exceed those of most strategies in the vast finance literature on momentum strategies 
in ongoing work  we are considering additional features such as industry and aggregate market returns
as well as non return data such as firm characteristics
and macroeconomic indicators  an open question as
we expand the number of inputs is whether separate
autoencoders for various categories of features would
perform better than combining all features in a single
autoencoder 
we are also examining the impact of updating the
weights in our network over time  our current methodology  in which we train the model once and then
hold parameters fixed for the entire test period  is
unlikely to be optimal as investor behavior as well
as the institutional framework of the market change
over time  furthermore  even if the data were stationary  we could improve performance by training with
all the available data at each date  an important implementation issue is computational cost  especially as
the number of features and depth of the network increase  we are exploring a parallel implementation of
the learning algorithm that could be run on gpus 
this approach should lead to a substantial decrease
in training time as the algorithm can take advantage
of parallelization at the data level  since it uses minibatches  as well as at the network layer level  alternatively  a more straightforward approach would be
to retrain the classifier each month  but update the
autoencoder less frequently in order to limit computational costs 

hinton  geoffrey and salakhutdinov  ruslan  reducing the dimensionality of data with neural networks 
science                   
jegadeesh  narasimhan  evidence of predictable behavior of security returns  journal of finance     
             
jegadeesh  narasimhan and titman  sheridan  returns to buying winners and selling losers  implications for stock market efficiency  journal of finance 
              
ng  andrew  ngiam  jiquan  foo  chuan yu  mai 
yifan  and suen  caroline 
ufldl tutorial 
      url http   ufldl stanford edu wiki 
index php ufldl tutorial 

fi