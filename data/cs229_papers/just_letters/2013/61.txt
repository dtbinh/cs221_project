predicting x   program runtime for intel processor
behram mistree  hamidreza hakim javadi  omid mashayekhi
stanford university
bmistree hrhakim omidm stanford edu

introduction

absolute value normalized error

 

programs can be equivalent  given the same set of inputs 
they always produce the same set of outputs  superoptimizing compilers leverage this notion of equivalence  substituting one program  or section of program  with an equivalent program that runs faster  recent work by schkufza attempts to build a superoptimizing compiler that intelligently
searches a program space composed of all equivalent sets of
instructions      this work relies on being able to predict a
programs runtime given its program text 
this report explores several runtime prediction models 
specifically  given a set of loop free x   input instructions 
our project predicts how long a processor takes to execute
those instructions 
section   discusses predicting runtimes for fixed length
programs and shows that kernelized regression models can
predict program runtime with     average error compared
to a gold standard tool      following these results  section   proposes and evaluates a generative model for predicting runtimes for programs with arbitrary numbers of instructions  finally  section   provides a performance analysis of
the proposed predicter 

 

figure   shows the results of applying linear regression on
randomly generated programs of length             and       
each program feature set is a vector containing the amount of
time that each instruction would otherwise take to run by itself  these times range from   to    cycles  feature weights
are learned from a training set of        programs  each  and
results are plotted against a validation set of       programs
each  normalized error is calculated following 

  section

   
   
   

  

  
  
  
program length  instructions 

where iaca is the number of cycles a gold standard  proprietary tool     predicts as program runtime 
by itself  linear regression performs well  across all program lengths tested  linear regression produces an average
normalized error of only       
the results in figure   suggest an interesting trend  as
programs increase in length  a basic linear regression works
better and better  in some ways  this is surprising  althgouh
the training set for each experiment shown in figure   is the
same size  the size of the space of potential programs that
each model tries to learn is very different  the space of programs of length    is roughly        larger than the space
of programs of length     there are approximately      separate x   instructions  
this means that for increasing program lengths  each regression learns over a training set that is more likely to be
less and less representative of what it is trying to predict 
i e   for increasing program lengths  we should expect more
error from variance 
table   plots normalized average error for programs of
length             and    as training set size increases  note
that after      samples  increases in training set size have
almost no impact on test set error  this suggests that the
generalization error shown in figure   is less a matter of
under representativeness of the training set and potentially
more a matter of bias in the underlying regression 

linear regression

 pred  iaca 
iaca

   

figure    normalized error for randomly generated programs using linear regression across programs of different lengths 

this section builds models to predict runtimes for programs
composed of a fixed numbers of instructions  section    
shows the performance of a simple linear regression and    
expand this model using gaussian and polynomial kernels 

normalizederror  

   

   

fixed length programs

   

   

   

    extends this to real programs disassembled from objdump 

 

fi str        

   

 k

 k

 k

  k

  k

  
  
  
  

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

absolute value normalized error

prog
len

   

table    normalized error as training set size increases 

   

kernelized regression

the previous subsection argued that error from linear regression was likely attributable to model bias  kernelizing the
regression can reduce model bias  whereas linear regression
only captures linear dependencies between the features and
the output of the model  kernelizing can capture more complex relationships 
we use the kernel trick      replacing each original feature vector  x i    with a  potentially  high dimensional feature vector   x i    containing nonlinear functions of the input features  and then we solve linear regression using these
high dimensional vectors as our new feature vectors  therefore  we solve 

absolute value normalized error

i  

one issue in doing regression  especially nonlinear regressions  is that the solution to the regression problem can be a
very complex and non smoothe function  which can cause
high variance  in order to overcome this problem  we use
tikhonov regularization      specically  we add          to
our cost function  which penalizes non smoothe predicted
models   the intuition behind this is that if our predicted
model has smaller coefficients then the model will be more
smooth and we will have less variance   therefore  we solve
the following problem 



   
   

  

  
  
  
program length  instructions 

   

   
   
   
   

  

  
  
  
program length  instructions 

 b  polynomial kernel regression

figure    normalized error for gaussian and polynomial
kernels on fixed size programs 
figure   presents the results of the gaussian          and
polynomial kernelized regresssion  d    c     both approaches showed only mild improvements compared to linear regression  on average     for the gaussian kernel and
   for the polynomial 
there are several possible reasons that these kernels only
show modest improvements  section     explores one of the
most basic of these  changing the mapping from x   program text to features 

m

min 

   

   

      x i     y i    



   

 a  gaussian kernel regression

m

min 

   

      x i     y i               

i  

using the kernel trick and the representer theorem      we
can show that the
m

y predicted       x i       ci k x i    x new   

 

i  

where 

the previous section discussed predicting runtimes for a set
of programs of fixed length  however  programs of interest
can have varying numbers of instructions  a superoptimizing compiler is primarily concerned with the question what
is the fastest equivalent program of any number of instructions  not what is the fastest equivalent program of ten instructions  a patchwork approach that trains separate models over the entire space of program lengths following the
methodology outlined in section     is feasible for our target
application  however  from a machine learning standpoint 

c    k    im    y 
using these results  we expanded our linear regression to
use 
   a gaussian kernel with parameter   bandwidth  in


which k x i    x  j      e

  x i  x  j      
   

variable length programs

and

   a polynomial kernel with parameters d degree  and c in
which k x i    x  j        x i   t x  j    c d  
 

fisub program vec      
runtime estimate    
instruction inst 

programs  fewer than      points are distinguishable because many points overlap  this is particularly true for programs composed of single instructions  which can achieve
fewer predicted outcomes than larger programs composed of
many instructions  a least squares line  plotted in green 
gives a sense of data spread for overlapping data 
gold standard prediction

for inst   program first ins until
inst   prgram last inst 
runtime estimate   
increment time sub program vec  inst 
sub program vec append inst 
end

figure    pseudo code for generative runtime prediction
algorithm 
it is relatively uninteresting 
this section instead explores a different approach  it attempts to build a generative model of program runtime  instead of predicting the runtime of a fixed length program 
the generative model estimates the length of a subset of a
program and iteratively updates this estimate as additional
instructions are added to the end of the program  figure  
shows high level pseudocode for this algorithm 
in essence  a generative model predicts the incremental
change a single additional instruction makes to program runtime  a good generative model therefore addresses the challenge of programs of different lengths  one can predict the
runtime of an arbitrarily length program by progressively
updating a runtime estimate for each instruction in the program 
section     presents a naive generative model  section    
analyzes the deficiencies of this naive model and uses them
to motivate more complicated feature selection discussed 
section     uses these features to build more complicated
generative models and discusses their performance 

   

  

prog size    

  

optimal predictor

 

  

optimal predictor

slope       
 
 

  

  

naive linear prediction

  

 

slope       
 
 

  

optimal predictor

slope       
 
 

  

  

  

optimal predictor

slope       
 
 

  

figure    naive generative prediction error  the horizontal axis shows the number of cycles that the naive generative model predicts  the vertical axis shows the number
of cycles predicted using the gold standard iaca tool     
an optimal predictor would  predicted cycles equals gold
standard cycles  is labeled on each graph as a black line 
the green line shows the results of fitting a generative regression through the data 
as can be seen in figure    as program size increases  the
naive generative model performs poorly  average error for
single instruction programs is almost zero  as indicated by
how closely the generative regression line matches the optimal line   while the naive predictor for sixteen instruction
programs  on average  has a misprediction error of almost
      this trend is to be expected  implicilty  the naive
generative model essentially trains across the space of single
instruction programs  therefore  when it is tested on a single instruction program  it predicts well  however  when it
is tested on programs with more instructions  that therefore
were not in its training set   it performs poorly 
the likely cause for this error is that the naive generative model ignores processors potential parallelization and
pipelining  training a model to independently predict runtime of individual instructions and summing the prediction
for each does not capture this feature of processors  the
following section explores mappings that preserve register
read write conflicts 

naive generative model

this section describes and shows results on programs from
implementing the simplest possible generative model  which
we call the naive generative model  the naive generative
model decomposes a program into its individual component
instructions  it then assigns each instruction a weight that
is the  precomputed  average of the time it takes to run that
instruction on all sets of valid inputs  e g   registers and immediates   finally  the naive generative model calculates the
overall program runtime as the sum of each of its instructions
weights 
figure   shows the performance of this model for     
programs composed of a single instruction  four instructions 
eight instructions  and sixteen instructions  each  the vertical axis of each subfigure shows each programs runtime  in
cycles  predicted from a gold standard proprietary tool    
and the horizontal axis shows the runtime predicted by the
naive generative model  each subfigure shows the   o line
corresponding to optimal predictions  any point on this line
is a correct prediction by the naive generative model of program runtime 
note that although each subfigure presents data over     

   

feature mapping

the degree that a program can be parallelized partially depends on read write dependencies on registers between instructions  for instance  consider the two  short programs
shown in figure    the semantics of each are simple  in the
 

fiprogram   
movl   x     rax
addl   x     rbx   rbx

we have built a feature mapping function     parameterized by a bandwidth parameter   that models these readwrite dependencies  more concretely  define a program  p 
as the ordered set i    i    i         im   where ii is the ith instruction in p  then the feature vector associated with appending
the instruction im   to this program is

program   
movl   x     rax
addl   x     rax   rax

figure    read write dependencies


c im        im    im    
c im        im    im    




  
  im       p    

 




c im      ii  im    
c im    

first  the movl instruction sets the value of register rax to a
constant   x     the addl instruction adds a constant   x   
to the contents of register rbx and puts this result in rbx 
the second program is similar  except the addl instruction
is over rax  in the first program  the processor can run both
the movl and addl instructions in parallel  however  in the
second  the addl instruction explicitly reads from a register  rax  the movl instruction writes to  and therefore the
processor cannot run both in parallel  therefore  although
composed of identical instructions  program   takes longer
than program   to run 
both of the feature vectors described in the experiments
of sections     and     did not track register dependencies 
for both programs   and    they produce identical feature
vectors 
to improve results for the generative algorithm  we updated our feature mapping to include read write dependencies  for each instruction in a program  we parse the instructions name as well as each of the instructions register arguments  importantly  registers with different names can refer
to parts of the same physical register      for instance  the
name al refers to the lower order bits of the same physical
register that the name eax refers to  we track this aliasing
as well as which portions of a register can be operated on
concurrently 
given an instructions name  we index into a pre compiled
database that returns

where  c    is the number of cycles it takes to run a program  and i j  ik is true if the jth instruction of p writes to a
register that the kth instruction of p reads or writes to  in this
model  the bandwidth parameter    specifies how many instructions to look back from the current instruction for readwrite conflicts  section     discusses the performance of this
feature creation approach  as any proposed solution should
be fast enough to be useful in practice 

   

generative model

in order to predict the incremental runtime that each instruction adds to the total program runtime  we have devised the
following learning process  for programs of length     take
each instruction  say i  and collect the features as explained
in section      then run the program with and without i  and
use linear regression to learn the incremental runtime that i
adds to the program from the collected features 
for a given program with arbitrary length l  we add the
predicted incremental runtime of each instruction based on
the collected features to predict the total runtime  figure  
shows the results  as the bandwidth increases the predictions
becomes more accurate  however  the difference for bandwidths greater than   is insignificant  note that as we saw
in section      the gaussian or polynomial kernel are almost
the same as linear regression so we only show the result for
linear regression  here 

   a list of implicit registers  registers not specified by
the arguments to the instruction  that the instruction
reads from and writes to   
   a list of which of its arguments it reads from and writes
to 
for instance  the database may specify that the instruction

 

movx a b
reads from its first argument  a  and writes to its second argument  b   but does not operate on any implicit registers 
following parsing and this database lookup  for every line
in a program  we can define its full register read and write set 
from these read and write sets  we can easily infer register
conflicts between a programs instructions 

conclusions and extensions

this report examined using linear and kernelized regressions
to predict program runtime for x   programs  for programs
of fixed length we were able to predict program runtime with
    average error compared to a gold standard tool  for
variable lengthed programs  our generative model performs
similarly  and is flexible enough to predict programs of arbitrary size  the following subsections discuss additional
experiments we ran using our model 

  instructions can also zero extend registers  which writes zeros across
the higher order bits of a physical register 

 

fi    

bw  
bw  
bw  
bw  
bw  

    

absolute value normalized error

absolute value normalized error

    

    
    
       

  

  
program length

    
    
    
    
    
       

  

figure    normalized error of generative model with different bandwidth parameter  training set size is composed of     programs with length     but  each test set
has     programs of length             and    

   

  

  
program length

  

figure    normalized error of generative model for disassembly of gcc using linear regression with a training set
size     of and test set size of     

real programs
instr sec

the dataset introduced in section     and used throughout
this report was composed of random sequences of instructions  this random dataset matched the project sponsors
goals  to intelligently search the space of all x   programs 
however  we were curious how well our model predicted
runtimes of compiler generated programs  which have more
regular structure 
to explore this question  we ran a disassembler
 objdump  across programs compiled using gcc  because
the model produced in the previous sections was for loopfree programs  we filtered jump instructions from the disassembled code  this filtering limits our ability to draw strong
conclusions about the models accuracy for real programs 
but likely still produces more realistic code segments than
the previous randomly generated dataset 
figure   shows the normalized error from running the generative model with linear regression on a disassembly of the
gcc binary  running the same regressions on other program
types  e g   programs that make heavy use of io  gui based
programs  etc   produced similar results  average normalized error is approximately    higher for this real data 

   

bw  
bw  
bw  
bw  
bw  

bw  

 

 

 

 

       

       

      

       

      

table    feature vector construction time versus bandwidth 
model described in section     produces predictions orders
of magnitude faster than issuing a system call to execute the
gold standard tool       predictions s for generative linear
regression model on programs of    instructions vs    predictions s for the gold standard tool  

references
    intel architecture code analzyer 
http   
software intel com en us articles 
intel architecture code analyzer 
    x   structure 
http   en wikipedia org wiki 
x   structure 
    e  schkufza  r  sharma  and a  aiken  stochastic superoptimization  in proceedings of the eighteenth international conference on architectural support for programming languages
and operating systems  pages         acm       
    b  scholkopf  r  herbrich  and a  j  smola  a generalized
representer theorem  in computational learning theory  pages
        springer       
    a  n  tikhonov and v  y  arsenin  solutions of ill posed problems       
    v  vapnik  the nature of statistical learning theory  springer 
     

feature collection runtime

our collaborators target application requires not only predicting runtime accurately  but quickly as well  to ensure
that the feature collection described in section     does not
become a prediction bottleneck  we benchmarked the performance of our unoptimized python feature collector  table  shows these numbers collected on a     ghz processor 
with a bandwidth parameter of    our unoptimized python
feature collector is able to produce features at a rate of approximately    k second  coupling these numbers with the
time required to perform linear regression  the generative
 

fi