above the din  identification of human voice in noisy
signals
a  fried   d  stonestrom   m  stauber 
 
department of physics  stanford university  stanford  ca             usa
 
department of mechanical engineering  stanford university  stanford  ca             usa
abstract  a large amount of machine learning research has been dedicated to understanding
human speech  but the ability to identify human speech in the first place can be useful as well  in this
paper  an algorithm is described that can identify the presence of a human voice in an audio signal 
an investigation into the most relevant features for this process is included in the description  the
resultant algorithm applies a support vector machine to the audio features to classify the signal with
      accuracy  the algorithm then filters the classifications to successfully classify the entire audio
file correctly 
  introduction
the human auditory system is particularly attuned to differentiating human speech from other sounds  the goal
of this project is to write an algorithm that can perform live detection of human speech even when partially
masked by environmental noise  one envisioned application for such a method is in disaster situations that span
a large area or an area that human rescuers cannot enter  robotic listeners can be deployed instead and can
report if they pick up the voice of survivors in their area  another application would be for mobile phones that
can change their behavior  for example if they ring or vibrate  depending on if they detect the user to be in the
middle of a conversation or in a lecture 
in the past  machine learning research has been focused on speech recognition  defined as the ability to
interpret human speech  ironically  not as much attention has been paid to the recognition of speech itself versus
other forms of noise  for classic speech recognition there are a variety of features that are used to remove
differences between an individuals tone  accent  and other vocal traits  these features effectively produce a
signature of human voice regardless of the speaker  the approach of this paper is to utilize a combination of the
features developed for classic speech recognition algorithms for the more fundamental identification of the
presence of speech problem 
  dataset
for this project we compiled a unique dataset  we used single voice  single source recordings of people reading
poetry found on www librivox org as our human voice training and testing data      for our noise data we
selected an array of noise recordings found in everyday environments from www soundjay com  the types of
environmental noise include industrial noise  nature sounds  and household noises  in total  the dataset
contained about   hours of recorded sounds  approximately split between human and environmental recordings 

fifigure    sample short time power spectra for different audio signals  there is a
clear difference between the spectra of a single human voice and that of freezer
hum

  methods
the development of the algorithm progressed on two main fronts  the features particular to human speech and a
classification algorithm that can identify the audio stream in real time with reasonably high accuracy  the
dataset audio files were divided up between those that contained a single human voice and those that contained
environmental noise  a matlab script was created to automatically process the  wav files  record the files apriori classification  and extract the features 
    features
the algorithm employs two classes of spectrum derived features of the audio file  to extract these two sets of
features  we used algorithms available in reference    the first class of features is compiled by comparing the
short time spectra after compressing them in a manner that re bins and rescales the spectrogram to better mimic
how humans perceive sound  the mel scale is one such scale  integrating the power spectrum against the bank
of filters generates these features  the filters used in the mel scale are illustrated below and were designed so
that they are perceived as equally spaced in frequency based on human testing  taking the inverse fourier
transform after this mapping picks out harmonics of voiced signals and generates the mel frequency cepstral
coefficients  mfcc   as noticeable in figure    the covarying harmonics clearly are a unique feature of the
spectra  in taking the fourier transform of the power spectra in this fashion it is hoped that the fundamental
frequency of voice box or sound source might be identified and used as a feature  however  this technique
clearly cannot capture all harmonics  as only those that are equally spaced in the mel scale will be detected  and
most sound sources only follow that pattern approximately  furthermore  harmonics are not always resolvable
within the spectrograms 
voice signals are temporally correlated over short times  voice recognition algorithms tend to approach the
problem by using the source filter model this assumes that voiced sound is comprised of a signal that
contains the content of the speech  but then it is filtered as it passes through the vocal tract of the speaker 
while the source signal for any given word should be identical regardless of who is saying it  within the model 
it is the filter that is unique to every speaker  these filters result in different spectrograms for different people
saying the same thing  most voice recognition algorithms use a variety of heuristics to re bin  filter  smooth or
rescale any given spectra  so that the effects of different filters are not present in the processed signal  rasta
filtering is another common processing step that smooths noisy spectral features  machine learning performed
on the processes signal will consequently only be sensitive to the information present in the source  and will
be able to distinguish between spoken words and other sounds 

fiperceptual linear prediction  plp  is one technique for extracting features from this processed signal and
constitutes the second class of features we examined  in the time domain  the linear model below is fitted to the
processed signal and the a coefficients are extracted 

the fit to the model that optimizes the squared error between x and the signal  yields the equation above for a 
expressed in terms of autocorrelation functions  c  which are easily recovered from the fourier transform of
the processed spectrum 
    classification
to classify our data we sought an algorithm that will run fast enough to allow for live classification of audio
signals  we found that a support vector machine  svm  with a linear kernel achieved high accuracy as well
speed  our svm is optimized by l  regularized norm  in addition  we apply a low pass filter to prevent the
output from tracking the misclassification error 

figure    diagram of algorithm scheme

  results
a program was developed in matlab to apply the algorithm described above to our dataset  the program
allows the user to select the percentage of the audio files devoted to training the algorithm and uses the
remaining audio to test the classification accuracy of the algorithm 
a

b

figure    a  confusion matrix of the algorithm when using all of the features of the dataset  note that the
algorithm does about as well for classifying both positive and negative data  b  classification accuracy of the
algorithm using various subsets of features  values were taken when svm was trained on     of our positive and
negative data and tested on the remaining portion of data 

    svm classification
the svm step was quite successful at classifying the files in our dataset  as seen in figure  a  the percentage
of false negatives and false positives are about the same  the different feature sets that were employed each
produced different levels of accuracy when the svm was trained off only that feature  this was likely due to
the fact that each of the features is designed to pick up on a different aspect of human speech  for example  the
rasta plp is very good at picking up the onset of syllables and words  but the mfcc is about the same

fiacross a whole syllable  the combination of these features was able to produce a much higher accuracy than
their individual applications  figure  b  
    filtering
the high accuracy of the svm was over the entire file  but there is still an error of      that causes the
classification of a live audio signal to appear choppy  these misclassifications happen most often by the natural
breaks in human speech  between syllables and words  by applying a filter  the svm removes these brief
misclassifications and achieves a much higher accuracy for the incoming audio stream as seen in figure   
a

b

c

figure    classification of streaming audio signal with filtering  a  the incoming audio signal starts with a single
person talking and transitions to the sound of freezer hum at      seconds  b  decision values of algorithm after
filtering  c  raw svm classifications before filtering 

    classification in noisy signals
after training the algorithm on clean recordings of human speakers and environmental sounds  the algorithm
was tested on a mixture of these signals  to accomplish this  the volume of a particular environmental noise was
held constant and the volume of a speaker was adjusted from zero to one hundred percent of their full volume 
when the speaker is at one hundred percent  the two audio signals are about the same volume  the algorithm s
accuracy at identifying the speaker s presence was recorded  figure    
when the speaker represents over     of the sound in the audio file  the algorithm classifies a majority of the
signal as containing a human voice  this result suggests that the algorithm should prove useful to identify
human voices in the noisy environments of disaster situations or our everyday lives 

fifigure    example relationship between the amount of environmental noise in an audio file and the ability to
identify human voice  when the voice is at      the algorithm successfully picks up the voice as the dominant
classification 

  conclusion and future work
this paper presented a strong start to the development of an algorithm to identify human voice in noise
environments  the algorithm presented here is currently fast enough to process an incoming signal  but it may
prove useful to develop a less computationally intensive set of features  in addition there may be certain sources
of environmental noise that were not considered but could be sufficiently similar to the human voice to cause
misclassification such as animal noises  this possibility can be tested by compiling an even larger dataset of
environmental noise for training and testing 
another possible extension of the work we have done is counting the number of unique speakers in an audio
sample  the features we used  in particular the plp  are explicitly designed to remove essential differences of
the signals of two people saying the same thing  it is not surprising  that when training and testing the svm off
sets of multiple people talking and one person talking  it has a hard time distinguishing between the two
categories  likewise  the svm cannot distinguish between a crowd of people and a single speaker  simple
tests of nonlinear svms have not been effective  though further exploration might prove fruitful  this suggests
that the features we chose are not suited for this application  finer details of the spectrogram might be used
instead to count the number of people talking  in particular the correlations between the time dependence of the
harmonics of the voice spectrograms might be a particularly useful set of features 
  acknowledgements
thank you to david held and andrew maas for assisting us during the development of our project  thank you
to professor andrew ng and the wonderful teaching staff for the course  thanks to our fellow classmates and
piazza contributors 
  references
    abercrombie  lascelles  short poetry collection      librivox  http   librivox org
    ellis  daniel p  w  plp rasta and mfcc inversion in matlab  http   www ee columbia edu  dpwe 
resources matlab rastamat 
    fan et al liblinear  a library for large linear classification journal of machine learning research
                   
    h  hermansky   perceptual linear predictive  plp  analysis of speech   j  acoust  soc  am   vol      no    
pp             apr       
    h  hermansky and n  morgan   rasta processing of speech   ieee trans  on speech and audio proc  
vol     no     pp           oct       

fi