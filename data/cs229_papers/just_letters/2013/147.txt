music genre classification

 

john cast  chris schulze  ali fauci
cs    autumn          

introduction
music genres allow the categorization of music into broad groups of related songs for use in applications such as
song recommender systems based on genre  many existing songs are unlabeled  and new songs are constantly
being released  while hand labeling them all would be too tedious  classifying them by genre can fit them into
categories for use in recommendation  also  even if the artist or producer labels a song as a specific genre 
finding the songs that are most similar in terms of frequency structure rather than subjective labeling might
provide recommendation information that the user would enjoy more 
in this project  we attempt to classify songs into a set of genre classes using mel frequency cepstral coefficients
 mfccs  from the songs  because of the high dimensionality that results from using mfccs  our main focus
was to find appropriate methods that could    best classify data with such a high dimensionality and    reduce
this high dimensionality and still retain the meaningful components of the data  we use both unsupervised and
supervised algorithms  to cluster songs by genre and predict the genre of a given song  respectively 
data set and features
data set  we used the marsyas      song data set      an openly distributed and widely used data set for song
analysis  it consists of ten genres each with     songs  each    seconds in length  each song is a       hz
mono    bit audio file 
features  our features consisted of the mfccs  which are commonly used in music analysis  the human ears
response is approximately linear below  khz and logarithmic above  khz  thus  the mel scale provides a
useful metric to discern the human ears response to the hz scale 
feature retrieval
we analyzed the data set for   and    second intervals taken from the middle of each song and segmented them
into   ms chunks  which helps in analyzing statistically stationary segments to build up an effective fingerprint
for a song  we smooth the edges of these chunks with a hanning window and take the periodogram  which
gives us their power spectral components  we multiply by overlapping triangles spaced according to the mel
scale and take the log  which tells us effectively how much energy  i e  filter bank energy  is within a given
frequency range because the human ear tends to be able to discriminate less accurately between closely spaced
frequencies as frequency increases  we then take the discrete cosine transform  dct  to decorrelate the
originally overlapping filter bank energies  choose the lower    dct coefficients  for each frame   and ignore
the rest  which gives a slight performance gain since higher coefficients represent fast changes in filter bank
energies and often degrade performance 
data conditioning
in addition to the  sec and   sec data parsing  we applied two pca conditioning algorithms to the data sets in
an attempt to reduce the dimensionality of each song while retaining the meaningful information 
pca basic  this representation interpreted the entire collection of songs as a single data set  normalized  the
data  stacked the frames of all songs  and applied the standard pca algorithm as described in class  and
normalized again  this approach effectively reduced the number of mfccs per frame from fifteen to ten 

                                                                                                                
 

normalization for a given data set  i e  xi s  is subtracting off the mean of the entire data set in question and dividing by
the component variances of the set 

fipca map  this conditioning interpreted each song as an individual data set  normalized each data set  and
found the top ten principal eigenvectors for a given song by the methods of pca discussed in class  we then
replaced the frames for each song with that songs eigenvectors and normalized  this perspective effectively
viewed the eigenvectors of each song as a mapping that preserved information about the ordering and relative
 not absolute  values of the mfccs of a given song 
data used
each algorithm was run on the following data sets  original mels  before normalization   normalized mels  and
pca basic mels and pca map eigenevectors  our feature vector for each song was the mels for each frame
concatenated together  or in the case of pca map the eigenvectors concatenated together   for the original and
pca basic mels  we also ran the algorithms on the mean mel vectors over all frames of a song  for pca map 
the means lose their meaning  so results were very poor  
all of these feature vectors were retrieved from both   second and    second segments of the song  after
experimenting with multiple combinations of the ten genres with varying success  we focused on a set of three
and a set of four genres that gave us the best results   classical  metal  pop   and  classical  country  metal 
pop   this choice is consistent with haggblade et al       who asserted that accuracy of genre classification falls
off dramatically for subsets of size greater than four 
distances
euclidean distance  the default distance used for k means and k medoids by the matlab packages  as well as
knn in our implementation is euclidean  where the distance between two vectors is the norm   squared 
mfccs of each frame of a song were concatenated  normalized  and taken as the songs feature vector 
kullback lieber  kl  divergence  for knn  we also used kl divergence as a distance metric between songs 
consider p x  and q x  to be the two multivariate gaussian distributions with mean and covariance
corresponding to those derived from the mfcc matrix for each song  then  we compute the following     

 det    
  
kl  p    q     tr                  t              k  ln

  
 det   
the kl divergence is not symmetric  so we took the following average 

dkl   p    q    kl  p    q    kl q    p 
while this divergence is not usually used as a distance metric for music classification  a prior project in cs   

    reports success with it  under this representation  we used the mean mel vectors described above and the
covariance matrix of the mels for each song 
figure     



algorithms
naive bayes  since our features are continuous valued mel coefficients 
we used the gaussian naive bayes model  where the probability of a
specific feature given a class is given by 

p x i   v   c   

  v  c     
exp


 
   c 
  c 
 

we are able to use this model because the features are normally
distributed  which we determined by graphing the frequency
distributions of the mean mel values for all songs  figure   shows one
suchgraph for the first mel  

fiwe used the matlab statistics toolbox naivebayes function     to fit the model and predict classes  unlike with
k means  naive bayes trains a model and then tests on separate data to predict genres for the test songs  we took
    of the data as our training set and left     for testing  the accuracy is the number of correctly assigned
songs over the total number of songs tested 
k means  the k means algorithm is an unsupervised learning algorithm that clusters data based on feature
similarities  for our selected genres  we ran the matlab kmeans algorithm in the statistics toolbox     on all of
the songs for the selected genres to cluster them  we ranged the number of clusters  the k of k means  from  
to    times the number of genres  i e  for   genres  we tried                 clusters  
in this algorithm  we wanted to look at whether  using our features  songs could be clustered correctly into their
genres  i e  whether each cluster represented a specific genre rather than a mix of genres   instead of actually
predicting genre based on a given song  this approach can be used to find songs that are similar to each other for
applications such as music recommendation  to measure accuracy  we first assigned a label to each cluster by
choosing the majority vote   which genre was the most common in that cluster  our total accuracy for the
model was given by the total number of songs labeled as the genre  label  of their cluster divided by the total
number of songs used 
k medoids  k medoids is a very similar algorithm to k means  except that the cluster means are assigned not to
the average of the points in the cluster at that time but to the closest actual data point to this average      we
used an open source package in matlab     to do the clustering itself  and our genre assignment and calculation
of the total accuracy was the same for k medoids as that for k means 
k nearest neighbor  the k nearest neighbor  knn  classifier is a non parametric classifier that polls the  k 
nearest training points to a given test point and classifies that test point based on the majority vote of these  k 
nearest neighbors  knn relies on a local prominence of data with a certain label  and works well with data that is
described by a number of highly dense clusters  with each cluster representing a given label  while this
algorithm is simple to implement  it is vulnerable to high dimensional inputs  it can be shown that increasing the
dimensionality of the data corresponds to decreasing the effectiveness of knn   with increasing dimensionality 
the  k  nearest neighbors to a given test point become increasingly less local to that test point      therefore  we
implemented the kullback leibler divergence as described in the distances section to represent distance
between two songs for knn in addition to the standard euclidean distance  we took     of the data as our
training set and left     for testing 
results
discussion  figure   shows the peak performance of each algorithm on each of the datasets discussed above 
below we discuss notable results and overall trends for the four algorithms 
naive bayes  accuracies for naive bayes were consistently among the highest across both the three and four
genre testing and the   sec song and    sec song datasets  with above     for all of the three genre tests and
most of the four genre ones as well  while results for the original mels vs  the pca features were different  we
found that normalization and pca mapping did not affect results  for the original and normalized mels of three
genres  we got     accuracy 

fifigure     

k means  for k means clustering  we reached     accuracy with the    second song data using the three genre
set  all accuracies  for each form of the mel vector  for this feature set were     or above  four genres gave
accuracies in the low     range for this data set as well  overall  three genres always performed better than
four  it is important to note that there is some variation between runs of k means due to the random initialization
of clusters and the possibility of local optima 
figure     

as we increased k  starting with the number of genres  we
found that the accuracy increased  see figure    
however  after a factor of about   on the number of
genres  the results plateaued and stayed generally level as
we increase k  we believe this occurs because  while not
all songs of a genre are very similar in structure  groups of
songs within each genre may be  taking notice of these
effective subgenres for an overarching genre thus helps
improve recommendation accuracy  we capped it at a
factor of    since too many clusters would give too few
songs per cluster to maintain significance 
on a related note  we tried spectral clustering  using the
meila and shi algorithm and the algorithm detailed by ng
et al       on the data  but obtained poor results  with       accuracy using   genres and the mean mel vectors
 using the original data set   given our results for k means and knn  and the fact that k means clusters data
based on compactness while spectral clustering uses connectivity  we hypothesize that our data is quite
compact and can thus be effectively clustered using algorithms which rely on compactness 
k medoids  the results for k medoids followed the same patterns as for k means  but were generally about   
less accurate than the k means results for each feature set 
knn  accuracy for this algorithm was seen to increase with k until about k     and then subsequently
decrease  this agrees well with expectation  as polling too few neighbors would not give a proper picture of the

fienvironment surrounding the test point  polling too many neighbors  on the other hand  would increase the
probability that a significant number of neighbors would not be local to the test point  accuracy for knn peaked
      for   genre classification  when using the mean mel vectors  using the original data set   this agrees with
expectation when considering dimensionality 
future work
pca k means  another data conditioning scheme that could be tried is to first reduce the number of frames per
song and then effectively reduce the number of mfccs per frame  in this approach we would first normalize
the data set  and then run the standard k means clustering algorithm on the song for k centroids where k is less
than the number of frames for a song  then pca would be applied to this resulting set of frames for a given
song using only the first ten principal eigenvectors 
pca map  by inspection  the eigenvectors produced by pca for the songs seem like a reasonable set of
attributes to classify the songs  our methods used unitary weighting of each eigenvector retrieved by pca 
however  this may be a naive approach  it would be interesting to try different weightings for each of the
eigenvectors to better reflect their order of importance as determined by pca  possible weightings to consider
are one over their respective ordering from pca or respective eigenvalues 
kl and jensen shannon divergences on all algorithms  we wrote code to evaluate the kl divergence as well
as the jensen shannon  js  divergence as defined in      we also had custom implementations of k means  kmedoids and nave bayes using the kl divergence as well as k means using js divergence  however  due to
poor results and problems with invertibility issues surrounding clustering of covariance matrices when using
these divergences as distance metrics  we abandoned our custom implementations of these algorithms and hence
abandoned using the kl and js divergences for these algorithms  further investigation should be performed to
find the root causes of these issues  our attempts to use these divergences showed that there was little
difference in using the kl or js divergences but further work should be performed to provide a better analysis 
features  more analysis should be performed using other features coupled with mfccs  for example  sources
such as       suggest that the use of delta and acceleration coefficients in conjunction with mfccs improves
classification accuracies 
references
    marsyas  data sets  web   nov       http   marsysas info download data  sets 
    haggblade  m   hong  y   kao  k   music genre classification  cs    course website  web   nov
      http   cs    stanford edu proj     haggbladehongkao musicgenreclassification pdf 
    k means clustering  matlab r    a statistics toolbox  web   dec      
http   www mathworks com help stats kmeans html 
    murphy  kevin  machine learning  a probabilistic perspective  cambridge  mit press        print
    chen  mo  k medoids  matlab file exchange     sept       web   dec      
http   www mathworks com matlabcentral fileexchange       k medoids 
    naive bayes  matlab     a statistics toolbox  web   dec      
http   www mathworks com help stats naive bayes classification   html 
    wikipedia  kullback leiber divergence http   en wikipedia org wiki kullbackleibler divergence
    ng  andrew et al   on spectral clustering  analysis and algorithm  nips       
http   ai stanford edu  ang papers nips   spectral pdf
    wikipedia  jensen shannon divergence http   en wikipedia org wiki jensenshannon divergence
     karpov  i   hidden markov classification for musical genres http   www cs utexas edu 
 ikarpov rice comp    finalreport pdf

fi