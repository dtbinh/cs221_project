 jazz   automatic music genre detection
tom camenzind

shubham goel

tcamenzi stanford edu
department of computer science
stanford university  ca

gshubham stanford edu
department of computer science
stanford university  ca

abstract
automatic genre classification of music is an
important topic in music information retrieval
with many interesting applications  a solution to
genre classification would allow for machine tagging
of songs  which could serve as metadata for building
song recommenders  in this paper  we investigate
the following question 
given a song  can we automatically detect
its genre 
we look at three characteristics of a song to
determine its genre  timbre  chord transitions  and
lyrics  for each method  we develop multiple
data models and apply supervised machine learning
algorithms including k means  k nn  multi class
svm and naive bayes  we are able to accurately
classify         of the songs from each genre in a
  genre classification problem between rock  jazz 
pop  hip hop  and metal music 

   introduction
     motivation
there has been an explosion of musical content available
on the internet  some sites  such as spotify and pandora 
carefully curate and manually tag the songs on their sites
     other sources  such as youtube  have a wider variety
of music  but many songs lack the metadata needed to be
searched and accessed by users  one of the most important
features of a song is its genre  automatic genre classification
would make hundreds of thousands of songs by local artists
available to users  and improve the quality of existing music
recommenders on the web 

intermediate level features  we cannot approach the genre
classification problem in the same way a trained musician
would  previous work has focused on applying existing signal
processing techniques to find low level features that correlate
with musical genre  in particular  mel frequency cepstrum
 mfc  coefficients  originally used in voice recognition tasks
     have proved particularly useful in describing a songs
timbre or tone quality     
     our work
we use three techniques to determine a songs genre 
 we analyze a songs timbre using mfc coefficients 
mfc coefficients represent the power spectrum of a
short duration sound wave  and are scaled to more
closely match a humans perception of sound      each
of the    coefficients corresponds to some quality of the
sound its loudness  tone brightness  sharpness of the
soundwave  and so forth      we investigate three ways
of modeling a song as a series of mfc coefficients  first 
we use the method described in        and model our
data using a multivariate gaussian distribution  we
also explore techniques we call timbre vector voting
and time window gaussians 
 we analyze the chord transitions within a song  because
reliably detecting chords within a song is still an open
research question  we focus on modeling only the root
of the chord  we formulate the chord transitions as a
markov process  and calculate the mle markov process
used to generate our song 
 we analyze a songs lyrics  although lyrics do not define
the music itself  in practice there is a strong correlation
between a songs lyrics and its genre     

   dataset
     past work
detecting a songs genre from its raw waveform is difficult 
and has been studied in a number of previous papers  many
characteristics of music that humans recognize in music 
beat  chord progressions  and distinct instruments  cannot
be reliably detected from audio files      without these

we used a subset of       songs from the million songs
dataset       a freely available collection of audio features
and metadata for a million contemporary popular music
tracks  the dataset provided features describing the songs
timbre at     millisecond intervals  specifically  each interval
had    mfc coefficients calculated  the dataset also lists

fi jazz   automatic music genre detection

the dominant chord being played at every     millisecond
interval  each song also has associated tags  which we
analyzed to determine the genre 
the million songs database did not contain complete song
lyrics  and so we gathered our own data  we wrote a crawler
to download song lyrics from songlyrics com       which
has a top     category for country  hip hop rap  r b 
rock and pop  we parsed the lyrics of     songs to construct
our vocabulary 

       multivariate gaussian model
here  we assume that the timbral coefficients of the song at
each time interval are drawn from a multivariate gaussian
distribution 
we calculate the maximum likelihood
estimation gaussian for each song  and represent the song
using the mean  and covariance  of this distribution
        this reduces the number of features from         to
                  features 

   methodology
     timbral analysis
here  we analyze a songs timbre to determine its genre  at
every     millisecond interval in the song  we have    mfc
coefficients  calculated as follows 
we begin by taking the fourier transform of the song
waveform  
x      


x

x n ein

n 

we scale the result using the mel scale  which models the
sensitivity of the human ear to sound frequencies  we take
the discrete cosine transform of the result  which is defined
as follows  for k              n    
n
 
x

 
xn cos 
nk 
xk    x       k xn      
 
n  
n  

there are      intervals in a typical song  for a total of    
             features describing a song  this is too many
features for our training set  in practice  we can hold only
     songs in ram at once  so we analyze     songs per
genre  in all analyses of timbre and chords  we train on    
of songs and test on      we experimented with several
models to reduce the number of features per song 

figure    mfcc representation of a song  the x axis shows each
of the    mfc coefficients  the y axis shows each time interval 

figure    mfc coefficient extraction and the gaussian model 

multivariate gaussian distance metric  symmetrized kl
divergence
consider two multivariate gaussian distributions p x  and
q x  with mean p   q and covariance matrices p   q
respectively  the kl divergence        is then given by
 k p  q    log

 q  
t  
 
 p    tr q p    p q   q  p q  x

where x denotes the dimension of the feature vector  since
the kl divergence is asymmetric with respect to the
distributions         our distance metric is given as
d p  q    k p  q    k q  p 
multivariate gaussian classification  k nearest neighbors
as a simple test of our models effectiveness  we implement
k nearest neighbors  to classify a new song  we calculate
its mle gaussian  and compare that to the songs in our
training set using the kl divergence distance metric 
multivariate gaussian classification  k means with kl
divergence
we expect that the gaussians for a single genre will be
clustered together  or that there may be multiple clusters
representing subgenres  for jazz  we may have one cluster
associated with swing songs  others with jazz ballads  etc 
to identify these clusters  we group all songs from a genre 
calculate the mle gaussians  and run k means using the kl
divergence as our distance metric  we do this for all genres 
and store all the centroids calculated  to classify a new song 
we find the nearest centroid  and assign the song to the genre
of that centroid 
interestingly  we obtained the highest classification accuracy
using k      meaning each genre was best modeled by a
single cluster  this suggests that songs of the same genre
have tightly related timbres  as opposed to being split into
several subgenres with distinct timbres 

fi jazz   automatic music genre detection

       timbre vector voting
by modeling the entire song as a single gaussian  we lose
information about the individual feature vectors  we expect
that some timbral vectors are highly indicative of genre  for
example  those extracted during an electric guitar solo will
be characteristic of metal  while others will be common to
many genres  we would like to automatically distinguish
between these types of feature vectors during classification 
instead of training a classifier to distinguish between songs 
we train our classifier to detect which genre a timbre vector
most likely comes from  our classifier will also produce
a confidence value  indicating how strongly that timbre is
associated with the predicted genre  timbres common to
many genres will have confidence values near zero 
once we can predict where individual timbre vectors come
from  we can make predictions for entire songs  for a song we
want to classify  we calculate how strongly its timbre vectors
are associated with a genre  and make predictions using the
following 
consider the set of genres g and the set of timbre vectors
 vs   for a song s  our classifier has a confidence function 
that returns a value indicating how strongly a timbre vector
is associated with the given genre  the predicted genre  ps  
for that song is given by 
x
ps   arg max
confidence v g 
gg

vvs

each timbre vector votes for what genre vector it is
associated with  more weight is given to high confidence
associations  and the genre with the most votes across the
songs timbre vectors wins 
we now describe which classifiers we used to predict genre
associations 

figure    timbre vector voting  we predict the genre of each
timbre vector  and take the confidence weighted average to predict
the genre of a song 

timbre vector svm
here  we train a multi class svm on the timbre vectors  each
labeled with its genre  we can make genre predictions on
timbre vectors as usual  for binary classification problems 
we use the distance from the decision boundary to estimate
confidence values  for multiclass problems  the svm we used
only provided     predictions  so we set the confidence to be
  for the predicted genre  and   for all other genres 
k means classification
for each genre  we gather all timbre vectors occurring in
songs associated with that genre  and run k means on the

timbre vectors  we store the centroids calculated for all
the genres  to classify a new timbre vector  we find the
nearest centroid in each genre  and calculate the distance to
that centroid  the negative of this distance is the confidence
measure for each genre  so the song will be assigned to the
genre whose centroids best fit the songs timbre vectors 
       time window gaussians
with timbre vector voting  every timbre vector votes
independently  in practice  we expect that neighboring
timbre vectors will be closely related  here  we propose a
technique that captures the dependencies between timbre
vectors  the benefits of our multivariate gaussian model  
while providing the finer time resolution of timbre vector
voting 
here  we group neighboring timbre vectors  groups of    
to form time windows  we calculate the mle multivariate
gaussian distribution for each time window  to avoid
overfitting  we restrict the covariance matrix to be diagonal 
now  we can represent each time window as a vector
containing the mean and  diagonal  covariance matrix  which
we call the time window gaussian 
we now use the same methods of timbre vector voting 
but replacing the timbre vectors with the time window
gaussians  the mean vector provides the fine time resolution
we had with timbre vector voting  while the covariance
matrix also captures the structure of the timbre changes over
the time interval 

figure    the time windows gaussian model 
we group
neighboring timbre vectors  and calculate the mle gaussian for
each group 

     chord transitions markov model
for trained musicians  chord progressions are one of
the defining features of musical genre  we investigate
whether they can help us with our automatic classification
task  previous research has successfully detected chord
progressions from individual instruments       however 
detecting chords from ensemble music is a difficult  open
research problem       in particular  background noise 
especially from percussion  makes it impossible to reliably
detect chords using current signal processing techniques 
therefore  we restrict our efforts to looking at the dominant

fi jazz   automatic music genre detection

note  the chords root  at each time interval  and finding
transition probabilities between time intervals  this can
provide a variety of information about a song 
this
can correlate with tempo  slow songs will have   chroma
transitions more often   major vs  minor    chroma vs
  chroma transitions   and so forth  we look at transitions
between     ms time intervals 
we assume that each song is generated according to a markov
process  and that different genres will have different markov
transition probabilities  all transitions are in the range
        corresponding to the    chromatics in an octave 
define procedure
gettransition s  t   t    
 dominantchroma s  t    dominantchroma s  t    mod   
where s is a song  and t  and t  are time intervals 
the function dominantchroma returns a number              
corresponding to c  c          b and mod    maps this change
into our octave of interest 
consider the number of transitions for a song s  ns   the ith
transition probability  ti   is then given by 
pns
  gettransition s  t  t        i 
ti   n s t  

representation that consists of the indices of words in the
vocabulary and frequencies of each index 
       multinomial naive bayes
we implemented multinomial naive bayes  we calculate the
probability of a training example occurring given each genre 
the parameters for each class are given as      l                  
pm pni
k y l  

i  

pj  
m

i  

 i 

  xj   k  y  i    l     
  y  i    l ni    v  

pm
y l  

i  

  y  i    l 
m

once the probabilities and parameters are trained  the
probability for test data is calculated and decisions are
made based on max likelihood estimates of the training data
belonging to the genre  for lyrics  we train on     of songs
and test on     

   results

we calculate this for every possible interval to get a chord
transitions vector representing the song 
once we have modeled the songs  we train an svm on our
chord transitions vectors with genre labeled  and classify
new songs by calculating their chord transitions vector and
making predictions using this svm 
figure     a 

figure     b 

figure     c 

figure     d 

 a  k means single gaussian  k      b  k means gaussian
window  k    
 c  svm gaussian window  d  chord markov model svm

figure    the transition probabilities calculated for a song 

     lyrics
       model
all song lyrics are parsed to produce a vocabulary  each
songs lyrics are converted into a multinomial model

each bar in the chart gives the percent of songs of that genre
that were classified as being that genre  that is  these graphs
represent the diagonal of each confusion matrix 

timbral analysis using mfc coefficients provided the best
method for predicting musical genre  where we achieved
a     accuracy on a   way classification problem  this
supports the results of previous work on this subject 
classification by lyrics also performed well  examining chord
transitions worked well for binary classification problems 

fi jazz   automatic music genre detection

but for the   way classification problem posed here  there
was too little separation among the data to obtain accurate
predictions 

   references
   panagakis y  and kotropoulos c  music classification
by low rank semantic mappings  eurasip journal
on audio  speech and music processing       
   weller a   ellis d  and jebara t  structured prediction
models for chord transcription of music audio 
columbia university       
   milner b  and shao x  speech reconstruction
from mel frequency cepstral coefficients using a
source filter model  university of east anglia  uk
      

figure      e 

figure      f 

   silla c   koerich a  and kaestner c  a machine
learning approach to automatic music genre
classification  journal of the brazilian computer
society       
   tzanetakis g  and cook p  musical genre
classification of audio signals  ieee transactions on
speech and audio processing       
figure      g 

figure      h 

 e  single feature vectors svm  f  k nn single gaussian 
k     
 g  lyrics  h  k means single feature  k    

   challenges and future work
the field of music information retrieval is growing and
has huge potential  while text based information is readily
queryable  music is a major source of online content that
has not been unlocked 
currently  genre detection is
difficult because many of the high level ideas that humans
use to determine genre  such as chord progressions  cannot
be determined reliably using existing signal processing
techniques  advances in signal processing techniques  in
particular  ones for reliable chord  beat  and instrument
detection  will allow us to better analyze and organize
musical content 
our work presents several successful methods for genre
classification  by analyzing a songs timbre  chord roots 
and lyrics  we obtain accuracies of up to     on   way
classification problems  this could prove commercially
valuable for song databases too large to manually tag all
songs  our predictions could be used as a feature in song
database relevance rankings  to give higher weight to songs
that are likely a genre of interest 
we have several ideas to improve our classifier in future work 
other features used in voice recognition  for example  the
zero crossing rate and the short term sound power spectrum
 may also correlate with musical genre and enhance our
classification accuracy 

   jahan  t and desroches  d  the echonest analyzer
documentation
http   docs echonest com 
s  website us east   amazonaws com  static 
analyzedocumentation pdf       
   rajani  m  and ekkizogloy l  supervised learning
in genre classification  department of computer
science  stanford university       
   haggblade  m   hong y  and kao k  music genre
classification 
department of computer science 
stanford university       
   rabee a   go k  and mohan k  classifying the
subjective  determining genre of music from lyrics 
department of computer science  stanford university
      
    mahieux t   ellis d   whitman b   and lamere p 
the million song dataset  in proceedings of the   th
international society for music information retrieval
conference       
    lyrics dataset for songs  www songlyrics com
    arndt c  and li l  automated transcription of guitar
music  stanford university       
    jiang n   grosche p   konz v  and muller m 
analyzing chroma feature types for automated
chord recognition aes international conference 
germany       

fi