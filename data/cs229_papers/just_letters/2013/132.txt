discovery of the source of contaminant release
devina sanjaya

 

henry qin

introduction

computer ability to model contaminant release events and predict the source of release in real time is crucial in various
applications  especially in environmental safety monitoring and homeland security  in the event of unintentional industrial
accidents or biological attacks in urban environments  immediate  accurate response is required  a real time computer
program that provides information about the identification of the contaminant  the source of the release  and the prediction of the subsequent path of contamination can be used to assist the decision making process for evacuation and
countermeasures 
the contaminant source inversion problem involves intricate geometry  uncertain flow conditions  and limited  noisy
sensor readings  moreover  the problem is generally ill conditioned in the sense that small changes in the sensor readings
can cause large changes in the calculated source of release      this makes single point deterministic calculations not
robust  the lack of robustness is due to the fact that some inputs might produce nearly the same outputs  especially when
measurement error is taken into account  to increase robustness  statistical approaches are used  but it often requires
large sampling and numerous forward simulations which can quickly become computationally expensive 
multiple previous studies have been conducted to reduce the computational cost  such as grid coarsening  reduced order
modeling  and stochastic expansions  previous studies have also considered applying uncertainty quantification methods
to analyze the propagation of input uncertainties  in this paper  we combine machine learning models and computational
fluid dynamics to discover the source of release for large scale problems in real time  various machine learning models
are evaluated for robustness to noisy sensor readings and limited training data  we will also compare our results with
statistical results from markov chain monte carlo  mcmc  with single walker     and ensemble walkers     

 

data format

our training and test data are obtained using the computational fluid dynamics software  xflow  developed by dr 
fidkowski at the university of michigan  ann arbor  for the  d case  we simulate a contaminant release around cross
sections of buildings  see figure    left    five sensors are placed around the buildings in a pseudorandom fashion without
iteration or tuning  each sensor takes three readings spaced equally in time  for a total of    sensor readings  a spatial
approximation order of p     is used and the peclet number for the simulations  based on the mean velocity and domain
size in the x direction  is p e        we use the    sensor readings as input features to our machine learning models  and
x and y as the output features  each forward simulation used to obtain sensor readings was completed in less than  
minute when parallelized with   processors 

figure    mesh and setup used during cfd simulation for two  left  and  d case  right   sensor readings from both cases
are used as our training and test sets 
 

fifor the  d case  we simulate contaminant release in a realistic urban area  see figure    right    this domain is the same
as in lieberman et al      there are    sensors placed around the buildings chosen in a pseudorandom fashion without
iteration or tuning  each sensor provides   readings for a total of    sensor readings  a spatial approximation order of
p     was used and the peclet number for the simulations  based on the mean velocity and the domain extent in the
direction of velocity  was pe       our input features are the    sensor readings  and our outputs are the x  y   z  and
amplitude  each forward simulation takes about   minutes on     processors 

 

implementation   discussion

in this section  we will discuss how we implement machine learning models to our problem  all of our models are trained
using the statistical programming language r or matlab  for the purpose of the discussion below  we assume that we
are only attempting to predict x  since predicting other variables  y in  d case and y  z  and amplitude in  d case  is
symmetric  moreover  to create realistic test cases  sensor errors are considered  we consider both uniform and gaussian
error distributions  due to our time constraint  we will mainly discuss the results from the  d case 

   

test error definition

all reported test errors are defined as a percentage of the interval over the parameter which we are making predictions 
equation    shows how we compute the test error when predicting x 
 test error  

 x  x 
 max x   min x  

   

where x is the true value of x  x is the predicted value of x  max x  is the maximum of the interval of x  and min x 
is the minimum of the interval of x in the  d case  the x          and in the  d case  x             we believe this
definition of test error percentage  rather than the standard definition of  expected   actual    actual  allows us to
fairly compare predictions across examples with different true locations of the source of release  intuitively  since we are
trying to predict a location rather than a quantity  an error of      model units should be interpreted the same way
whether it is an error from of a ground truth of     or a ground truth of      and our definition of error reflects this 
furthermore  this percentage error deifinition enables us to compare the test errors between  d and  d cases 

   

perfect sensor readings

first  we consider the case where all sensor readings are perfect  for our  d test case  we have     examples in total 
   for training and    for testing  we found that ordinary linear regression  which directly models the output values as
a linear combination of the raw input feature values  does not perform well  with a mean error of      however  linear
regression with logarithmic feature mapping  see equation    performs quite well  with a mean error of     this suggests
that there is a log relationship between the sensor readings and the location of contaminant release 
x         log r      log r             log r  

   

to use log transformed model  we first replace any readings that are less than equal to zero with the fixed constant
          take the log of each sensor reading  and then apply multiple linear regression  figure    left  shows the residuals
plot from our testing  this figure shows the mean error of    and standard deviation of    in predicting the source of
release 
next  we consider the  d case  here  we have     examples in total      examples for training and    examples
for test  as with the  d case  linear regression does not perform well  while linear regression with logarithmic feature
mapping works well  with a mean error of        performing the same steps as before  we found a mean error of      
and standard deviation of       as shown in figure    right   we acknowledge that these errors are unusually low  but
do not claim that these errors will generalize to other  d simulations  even assuming perfect sensors 

 

fi   
residuals

   

   

   
   



   

residuals



 

 





 


  
 
  
    
 

   
  


 



 





   

   









 

      
      



 



 



 




 

  

  

  

  

  

  

  

 

test examples

  

  

  

  

   

   

test examples

figure    test residuals of linear regression with logarithmic feature mapping from the  d  left  and  d  right  cases with
perfect sensor readings

   

sensor readings with uniform error distribution

having successfully modeled the simple case  we moved onto a more complex case  uniform  substantial sensor error 
based on knowledge of the field         is a reasonable sensor error  to apply the uniform error  we add the same fixed
constant to each of our sensor readings  and treat these perturbed sensor readings as the new raw features  during
training  we naturally assume that the constant is unknown  as it would be in practice  our training and test sets are the
same as in the previous case 
unfortunately  this more complex case clearly demonstrated that our previous method was not robust against uniform
sensor error  as our test errors increased by an order of magnitude  this behavior is consistent with the ill conditioned
nature of the inverse problem  in retrospect  we could have anticipated this  when systematic sensor error is added  the
true model starts to look like the function in equation    while we were still trying to model it using equation   
x         log  r          log  r                 log  r      

   

we tried to model equation   by using rs nonlinear least squares nls package  but we ran into singularity problem 
since direct model fitting did not pan out  we implemented a hill climbing algorithm in an attempt to greedily discover
the value of the hidden constant   more specifically  our hill climbing algorithm varies the value of  to find the maximum
r  statistic for a least squares fit against log  ri     the procedure is as follows 
   initialize a step size s to a constant       
   choose a random starting value                       
   fit a least squares model using the features log  ri      
   fit two more least squares models using       s and        s 
   set   to be equal the  in the model above which produced the highest r  statistic 
   half the step size s  so s  s   
   if r          terminate the algorithm and report     otherwise  return to step   
the above algorithm is able to pinpoint  in the  d case well  and thus  we can substitute  to   before modeling the data 
however  this hill climbing algorithm does not work well in  d case due to multiple local maxima 

   

mixed sensor readings

now  we will consider the case where some sensor readings might happen to contain no errors while others have uniform or
gaussian distributed errors  to simulate these errors  we first replicate the original set of examples three times  creating
a new data set with three times the number of examples  next  we add a constant error term to one full replica  add
gaussian distributed errors  mean    stddev  e     to the second full replica  and then randomize the order of the data
 

fiset  from this mixed data set  we randomly select half the examples for use in training  and hold out the other half
as a test set  multiple machine learning models are trained  such as linear regression  linear regression with logarithmic
feature mapping  locally weighted linear regression with logarithmic feature mapping  decision tree  boosting  random
forest  and k nearest neighbors  figure   compares the various models based on different testing error metrics  mean 
standard deviation  median  and   th percentile  for our  d case  here  we can see random forest gives us the lowest
mean testing error  which is about    and the lowest   th percentile error  which is about      figure   shows the test
residuals of random forest for our  d and  d cases 
    
linear
linear with log
loc  weighted linear with log
decision tree
boosting
random forest
knearest neighbor

   

    

testing errors

   

    

   

    

   

    

 

mean

stddev

median

   

error metrics

   

figure    testing error metrics of all machine learnings method applied to the  d case with mixed sensor readings

   





   





















 


 
 






























































































































































































































































































  


   






  
  
  













   

















   





 





residuals

   

residuals
















 





 






 
 
  
 












 

 

 

      
 












 
 
 
   
  
 

 


 
 
  
 
   




 
    

 
  

  
    


  
   






 

 
















   

   



   





   



  

   

   



   

 

test examples

   

   

   

test examples

figure    test residuals of random forest applied on the  d  left  and  d  right  cases with mixed sensor readings  note
that these figures are not on the same scale because the  d and  d cases have different ranges for their dimensions 

 

fi   

comparison with mcmc

we compare our results with statistical results from mcmc with single and ensemble walkers presented in     using mcmc
and noisy sensor readings of          we obtained less than    in predicting x for both  d and  d cases  although the
results from mcmc are far more accurate  it takes substantial time to obtain a single prediction from mcmc because
generating a set of sensor readings at multiple proposed location of the mcmc walker s  is time consuming  for instance 
the  d case converges in about   minutes using    processors and the  d case converges in about   minutes using    
processors  on the other hand  we can train a random forest and evaluate hundreds of examples in less than     seconds
on a single processor 

 

conclusion

to summarize  we make the following contributions in this paper 
 with perfect sensor data  the relationship between sensor readings and the contaminant source is a simple log linear
one 
 out of all the models we experimented with  random forest proved to be the most robust against noisy data 
 comparing to mcmc  supervised learning requires far less computational power  but is less accurate and less robust
to noisy data 
 to increase the robustness of supervised learning to noisy data  more research is required 
to the best of our knowledge  predicting the location of contaminant release in a realistic setting remains an open problem 

 

acknowledgement

we gratefully acknowledge dr  fidkowski at the university of michigan  ann arbor for the use of his computing resources
and simulation software  xflow  in generating our training and test data 

references
    j  goodman and j  weare  ensemble samplers with affine invariance  communications in applied mathematics and
computational science                  
    j  hadamard  lectures on the cauchy problem in linear partial differential equations  yale university press       
    w k  hastings  monte carlo sampling methods using markov chains and their applications  biometrika              
     
    c  lieberman  k  fidkowski  k  willcox  and b  van bloemen waanders  hessian based model reduction  large scale
inversion and prediction  international journal for numerical methods in fluids       
    d  sanjaya  i  tobasco  and k  fidkowski  adjoint accelerated statistical and deterministic inversion of atmospheric
contaminant transport  unpublished 

 

fi