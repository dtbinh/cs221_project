biometric id  based on phone accelerometer usage
govardana sachithanandam ramachandran

introduction
two level authentications are slowly getting adapted in
mainstream  since password based authentication are
vulnerable  prime example would be sunet
authentication at stanford  since smart phones are
getting very pervasive and since everyone moves
differently when they use their phone  this project
aims to use phone accelerometer usage data for
validating
a
user 
this type of authentication is very subtle  compare to
finger print   facial recognition since finger print or
face can be captured or forged easily by different
means 
this project is an adaptation of the kaggle com
competition    accelerometer biometric competition 
 http   www kaggle com c accelerometer biometriccompetition    this project investigates the feasibility
of using accelerometer data as a biometric for
identifying
users
of
mobile
devices 

data
seal the kaggle sponsor  has collected accelerometer
data from several hundred users over a period of
several months during normal device usage  to collect
the data  seal has published an app on googles
android playstore that samples accelerometer data in
the background and posts it to a central database for
analysis 
they have uploaded approximately    million unique
samples of accelerometer data collected from    
different devices  these are split into equal sets for
training and test  samples in the training set are
labeled with the unique device from which the data
was collected  the test set is demarcated into   k
sequences of consecutive samples from one device 
the training data is of the form

field name
t
x
y
z
deviceid

description
unix time  milliseconds since          
acceleration measured in g on x co ordinate
acceleration measured in g on y co ordinate
acceleration measured in g on z co ordinate
unique id of the device that generated the
samples

the test set has t x  y  z   sequence id  the objective
was to determine if the sequence id is that of a device
id 

leaks
   it was apparent from the start there were
potential leaks in the test   train data  the
most apparent ones being the samples from a
device were equally divided equally into test  
train set  just the numbers of sample alone
were enough to predict the results with very
high accuracy 
   the sampling on a device was done almost at
the same time of the day  hence time had
high correlation 
i decided not to use these features   others which
i believed that had leaks 

feature selection
one of the challenges faced  was to convert the
time series data into feature vectors  it was
observed that there were series of dense period of
activities  it was decided to break the time series
data into segments  start of session is defined by
time between activity is more than a second 
below is an example of breaking series into
segments

fit

x

y

z

device

segment

             

 

        

        

 

 

             

        

        

        

 









 

             

        

        

        

 

             

 

       

        

 

             

        

        

        

 

             

        

        

        

 

  

  

  

  

 

             

        

        

        

 

             

        

        

       

 

             

        

        

        

 

             

        

       

        

 

model selection
model position 

 

fig  phone positions   their corresponding accelerometer values
 

position with which a user holds the phone was found
to be strongest compared to any other feature  this
was very apparent when running mutual information  
pca of the positions against the labels

tab  segmenting sequence for aggregative features  for better
prediction

some measures to capture the characteristic of each
segment were devised  these were 
type

features

distributio
n   position

mean values of x y   z
variance of values of x  y   z
magnitude  x    y    z     

distributio mean   rate of change in acceleration on
n   rate of  x y   z axis 
change
variance   rate of change in acceleration
on  x y   z axis 
temporal

duration of the session
  of activities in a session
mean time between   activities
variance in time between   activities
tab  features that were considered

fig  x y z segment mean correlation between themselves

fiother features though sounded promising  later found
to be noisy   didnt have signal in them to be selected
as features  these includes variance  rate of changes of
g  time variance etc  

since it was obviously clear there could be no linear
boundaries between the above selected features and
clearly there were correlation between axes  svr was
used to model the problem  the reason to use svr
instead of svm svc  is that it provide probabilistic
interpretation  since each test sequence has almost
    segments  the probabilities of each of these
segments are calculated separately  the probability for
the entire sequence is determined by taking the log
likelihood of each of the segment classes predicted for
the sequence 
monte carlo by grid search was used on    fold cross
validation set to determine the parameters for the
model while maximizing the accuracy of the results 

fig  x axis variance distribution

argmax accuracy 
c    p
s t fp     
below are the results for individual segment

model
kernel
c


svr
rfb
 
   
 e  

tab  model parameters
fig  time mean vs variance distribution

measures
accuracy
f measure

value
       
      

tab  results for individual segments

measures
accuracy

value
     

tab  results for entire sequence  for    classes 

fig  normalized rate of change

as first cut  the problem was reduced to multi class
multivariate binomial classifier  as it made more
engineering sense  to have each device store their
respective model s  locally on the phone  but it soon
became apparent that the sample from other class
had very high density   started to skew the results 

fimodel harmonics 

from analyzing the input signals  it was apparent that
there might be some harmonics either voluntary or
involuntary emitted  which might be unique to the
user  it seems body has natural frequency of       hz  
to capture this the sample rate as to at least    ms 
due to loss of data sample rate was set to    ms 

fig  correlation of   classes on   frequency spectrums

parameters
kernel
c


since the signals seems to have been sampled at
varying sample rate  non uniform discrete fourier
transformation was used to transpose to frequency
domain 
as promising as it had appeared  the result were not
that hopeful  this was due to the fact that they are very
low variance  after z score normalizing most of the
frequencies amplitude were the constant  a better
approach would have been to get the first   order of
harmonics as features 
it was observed that here few correlation though on
two frequency spectrums

values
sigmoid
    
    
 e  

tab  parameters of the model

measures
f measure
accuracy

values
         
        

tab  the segment level measures where

model by relative state change 
it might be argued that one might have quirks that
would make the mobile move in a pattern between the
axes  this would require pattern recognition 
for this ive hypothesized the model as viterbi
formulation of hidden markov model
given a series of observed series of vectors  x y z 

the above formulation would provide the most likely
state at t  again here the segmented sequence of
events are used  while training  since the states are
know each of the observed vector  instead of using just
the believes the actual states are use for training 

fibaum welch algorithm is used to train the hidden
markov model 
parameters
p si    n
p aij    i j
p aij    i  j
p st    s  o   ot   

comments
where n is   of classes

this is done while
training  as these are
labeled set

tab  model parameters

in order to avoid exuberant event   state transition
matrix  the  x y z  are pruned to first   significant digits

ensemble

fig  the ensemble models

the result of these models where then fed to an
ensemble classifier  sdg logistic regression was use to
determine the final results 

conclusion
though there were some obvious leaks in the dataset 
with accuracy       clearly shows the possibility of
further research   adaptation into real world  the one
obvious thing about the dataset is there were almost
       sample for a device  in real world application
this might not be possible  further advancement could
be done to help reduce the sample required to detect
fraud 
it might also be noted the sample doesnt fully profiles
the user as it was taken at one instance in time  the
user might as well use the device on a table top  the
device position will be fixed and there would be very

few signals to go with  all the above models will fail 
harmonics analysis showed the importance of accuracy
and sample rate to capture human subtle signal as
noise from device overwhelms it  one thing this
competition has showed is the importance of accurate
sampling of data set 

reference
   the viterbi algorithm by g  david forney 
jr 
http   cbio ensmp fr  jvert svn bibli local for
ney    viterbi pdf
   whole body vibration building awareness in
sh e by helmut w  paschold and alan g 
mayton
http   www asse org professionalsafety pastis
sues                f paschold     z pdf
   machine learning paradigms for speech
recognition   an overview by li deng  fellow 
i and xiao li
http   research microsoft com pubs        t
asl deng         x   pdf
   an introduction to hidden markov models
by l  r   rabiner and b  h  juang
http   blog digitalagua com            accel
erometer xyz based on iphone position 
   accelerometer biometric competition by
seal mobile id
http   www kaggle com c accelerometerbiometric competition
   baumwelch algorithm 
http   en wikipedia org wiki baum e      
 welch algorithm

fi