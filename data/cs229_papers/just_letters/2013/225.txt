predicting conversational likability on
anonymous chat networks
arpad kovacs  aditya somani  daniel velkov
abstract
we implemented a learning system that is able to predict if two users on the chatous anonymous
chat platform are likely to have a meaningful conversation  this has the practical application that
by examining the past conversations and activity of existing users  we can better match together
new conversations and generate more personalized suggestions of which other members a user would
be interested in connecting with 

 

introduction

chatous is a   on   anonymous online chat network which matches users from around the world with
one another in text based conversations  however  the anonymous nature of the chatous platform can
easily be exploited by users who engage in antisocial behavior  eg  taunting or threatening the other
party  which results in an undesirable user experience  another issue is that some users might start a
chat  only to end it immediately upon determining that they are not interested in the other member 
user acquisition in a social network is a costly investment  a new user is much more likely to be
retained if her initial conversations turn out to be highly satisfying  our project is focused on using a
users profile information and past conversations to predict which users tend to be good conversationalists 
we use conversation length as a proxy for the quality of conversation  based on the observation that a
long chat which holds the users attention is more likely to be meaningful and engaging than a short or
zero length conversation 

 

model

in this section we will describe the different approaches and sets of features that we use to model
the conversation likability problem  for a given pair of users  we want to predict the length of their
conversation and whether they will establish a friendship connection  we do this by looking at pairs
which have actually engaged in a conversation and trying to predict the outcome by only looking at the
information we have at the time when the conversation starts 
to establish a baseline  we started with the following user and profile features  profile information
for each user  the absolute value of their age differences  the xor of their declared sex    if identical    if
not   and whether one of them specified their location  profile information for each user consists of  a
boolean indicating if the user specified a location  the users age and sex and the length of their about
me description 
we trained logistic regression over   million chat records with a     train      test split  since the
percentage of long conversations in the original dataset was too low  we boosted it to ensure a       split
between examples of each category  initially  we utilized the labels short  s   long l  and finished f 
provided by chatous as our output class  we soon determined that many of the training examples were
labelled incorrectly with regard to the problem we were trying to solve  e g  a chat might be labelled
as finished even if the users engaged in a long conversation  similarly  we found cases in which a chat
with little content was marked as long  therefore we switched to a binary class output which indicates
whether the chat was long or not based on whether the conversation contains at least    words 
we also looked at predicting if the conversation will be reported by one of the users due to several
predefined reasons  spam  harassment  filth  we evaluated several algorithms and different sets of features
for this task  our conjecture is that knowing the probability that a conversation will be reported is useful
signal for the general conversation likability task  we performed analysis on the frequency of reports
and the possible reasons for reporting a conversation 

 

fi 
   

results
predicting conversation likeability

our classifier was able to attain       accuracy using a dataset of   million chat records  the following
table shows the results we obtained 
class
tp rate fp rate precision recall f measure
short conversations
     
     
     
     
     
long conversations
     
     
     
     
     
in order to determine which features are most relevant to predicting conversation length  we utilized
the j   tree implementation of c    from the weka machine learning package      the c    algorithm    
builds a decision tree from training data consisting of already classified examples  by recursively splitting
branches based on the attribute which most effectively distinguishes between the available classes  we
determined the most relevant decision features  in order of importance  to be whether users are of
opposite sexes  whether the difference between the users ages is low  and whether both users specified
their locations in their profiles  and if yes then whether they are from the same country 

   

predicting user quality

we performed preliminary analysis of the chatous data by sampling         conversations out of the  
million available  this sample was joined with the other provided dataset which has labels for a set of
users  the labels are  clean  dirty and bot  we chose to look at the problem as a binary classification
task  predicting if the user is clean or not  after joining the sample conversations with the user quality
dataset  we had        conversations with a quality label  the number of clean and not clean labels
was almost evenly split        clean        not clean 
as a baseline we looked at the number of lines that each user has sent per conversation  the
breakdown depending on the user quality is as follows 
quality
numlines  numlines 
not clean         
        
clean
        
        
this suggests that we can use conversation lengths as a baseline for modeling the user quality problem 
using the number of lines that each of the users wrote as our features and running a    fold cross
validation gives an f  score of      for the user quality task  we found that when optimizing other
scores like accuracy  precision or recall it was easy to get high values but it didnt seem like those results
would be applicable in a real world scenario 
another feature that we considered is the word vectors for each user in a conversation  the raw
counts were taken without applying any transformations  for optimizing the cost function  we used a
stochastic gradient descent algorithm from the scikit learn machine learning package for python      we
explored different parameter values for the sgdclassifier  regularization terms  penalties  l   l   elastic
net   loss functions  hinge  logistic  perceptron   number of iterations  we choose the optimal parameters
by running a grid search over the parameter space and performing a    fold cross validation for each
combination  using those optimal values  we obtained a f  score of      using the word vectors as a
feature matrix 
we implemented a user proximity feature by extracting the countryname from each user profiles
declared location  and checking whether the country names of two users engaged in a chat matched
exactly  this feature yielded an f  score improvement of approximately      
surprisingly the best performance was achieved by just using the word vectors from the about field
of the user profile and the country extracted from the users location  the best cross validated f 
score was       at       iterations of sgd  this might be due to the fact that the about fields for some
problematic users are actually the same due to creating multiple accounts 
other features which were considered but did not result in higher performance are the ages and
genders of the two users  and boolean values indicating if they are the same gender and age group 
other features that might be worth exploring are the geographic distance between users  as well as the
time elapsed between when a user created their account and a particular chat occurred  however we
predict these will probably only result in small marginal performance improvements over our alreadyexisting features 

 

fifigure    model performance as a function of sgd
iterations

figure    error as function of sample size

we experimented with running the sgd algorithm for increasing numbers of iterations  since we
used regularization  increasing the number of iterations led to improvements in the f  score  those
improvements started diminishing above     iterations and thats why we didnt continue experimenting
with much higher values  the results are summarized in figure   

   

taking past behaviour of users into account

we added features modeling users past behaviour  the premise was that low quality users are likely
to exhibit previous antisocial and disruptive behaviour  while high quality users may have a history of
congenial  friendly conversations  for each user  we created the following features 
   enough cov  an indicator function indicating if we have enough conversations      from a user to
make a meaningful prediction from history
   perc ended self  percentage of conversations which were ended by the user herself
   perc non zero ended other  percentage of non zero length conversations which were ended by other
users
   perc long  percentage of long conversations that the user has had
   perc zero ended self   zero length conversations ended by the user herself
now it is logical to assume that the outcome of a conversation will be dependent upon the user with
the lower quality amoung the participants  so for each conversation  we generate the following features
based on the features of the two partcipating users  u   u   
   u  enough cov    u  enough cov
   min u  perc ended self  u  perc ended self 
   max u  perc non zero ended other  u  perc non zero ended other 
   min u  perc long  u  perc long 
   max u  perc zero ended self  u  perc zero ended self 
we tried various classifiers  logistic  neural networks  j   trees  using these features in conjunction
with existing features  surprisingly  these features didnt result in any improvement in performance 

 

error analysis

after executing the logistic regression classifier to predict conversation length  we used wekas visualize
classifier errors feature to determine which classes of examples are being misclassified  and how correlated
various features are to the predicted and actual labels  the charts below can be interpreted as follows 
 

fifigure    age difference between users  x axis  vs
predicted conversation length  y axis  

figure    sex difference between users  x axis  vs
predicted conversation length  y axis  

   correct prediction  predicted matches actual longchat label 
   incorrect prediction  predicted does not match actual longchat label 
 blue  actual longchat label    this was a short conversation 
 red  actual longchat label    this was a long conversation 
examples for which the feature matches the predictedlongchat label are shown in the bottom left
 both predicted and actual labels are    and top right  both predicted and actual labels are    of each
figure 
in the age difference plot  most of the examples are being clustered on the left  eg  the absolutevalue difference in ages between most users is small   the classifier does not lend much weight to the
age difference feature  the classifiers predictions for examples with a given age are almost evenly split
between   and     the error rate for this feature is also high  for instance  we observed that examples
in the top left  where age difference is low and the classifier predicted longchat    have lots of blue 
indicating actual longchat   label does not match the prediction  while there are lots of red  examples
in the bottom left  however  notice that the vast majority of examples with age difference more than
   appear to have short conversations  as indicated by dominance of blue markers on the right   this
suggests that turning age difference into a quantized or binary feature  agediff      might improve its
predictive power 
in contrast  it seems like the sex difference feature    if users participating in a conversation have
the same sex    if users are of opposite sex  is a better predictor of the length of the chat  notice the
abundance of blue s in the lower left corner  this indicates that when two users have the same sex 
the classifier correctly predicts that their conversation will be short  the classifier has more difficulty
predicting when opposite sex partners will have a long conversation  as evidenced by the large number
of false positive blue   classifier predicted long conversation  but actual conversation was short  in the
top right and false negative red   classifier predicted short conversation  but actual conversation was
long  examples in the bottom right  to make conversation length prediction of opposite sex users more
effective  we may add features that capture sentiment as a proxy for the chemistry between users  eg 
positive sentiment may be linked to longer romantic conversations  while negative sentiment may be
associated with offensiveness and aggression  another hypothesis is that some troll users lie when
declaring their sex  which we will be able to confirm or refute upon combining the conversation length
model together with the abusive flagged users model 

 

diagnostics

in order to determine the sources of error in our sgd algorithm and how we could improve it  we
implemented several of the recommendations from the advice for applying machine learning lecture 
 

fifirst  we plotted the learning curves of the training error and the test error as seen in figure   
as the size of the training set increases  we observe that the training error continues to decrease 
while the training error increases  the gap between the training error and the test error  as well as
the shape of the train and test curves indicate that we are experiencing high variance  in response  we
pruned the number of features we use down to the users about descriptions and countries  which yielded
the highest f  score of      and confirmed our hypothesis 
since we are in a high variance regime  a larger training set should also improve our classifiers
performance  but unfortunately we began to run into out of memory errors at around         chats 
we believe that we could achieve better results with improved hardware that could fit more data into
memory  alternatively  we could implement a streaming solution that loads subsets of the data into
memory  batch processes each subset  and then combine the results  however this would be significantly
slower due to disk access becoming the bottleneck 
we also plotted the f  performance as a function of the number of iterations of stochastic gradient
descent  as you can see from figure    there are improvements when increasing the number of sgd
iterations  but we encounter diminishing marginal returns  we elected not to try running more than
      iterations of sgd in due to computation time limits 

 

future work

we developed two separate models  one for predicting conversation length and the other for predicting
user quality  there could be a potential benefit from combining those two models  for example knowing
the predicted label for user quality could be useful when predicting the conversation length and vice
versa  this could be achieved by stacking the predictions of the two models and then training a simple
regression model over the resulting two column matrix 
the largest sample size we processed was     of the entire chatous dataset due to computational
constraints  mostly memory limitations   that sample size was the upper limit for tuning our models
since we are running a grid search over a large parameter space which requires several hundred cross
validation runs  given enough computaing resources it would be interesting to see how will the model
performance change when the complete dataset is taken in consideration  also more computational
resources would allow us to explore a larger parameter space which will likely further improve our
models performance 
we also observed from the dataset that some low quality users often create multiple profiles with
contradictory information about their age  sex and location  we believe that such features would be very
useful in filtering out low quality users 
one area of possible further improvements is feature engineering  the country based proximity
feature is very coarse  and could be improved by computing geographical distances between the actual
users locations  another possibility is to refine the user profile features  for example by computing the
time between when a user created an account and a particular chat as a proxy for the users experience
level on chatous  although to accurately measure engagement  we would need login times which we did
not have access to   finally  since we found that since the sex difference is the greatest predictor of
whether users will have a long chat  it would be worthwhile to perform semantic or tone analysis on
the conversation to determine whether users actual sex and interests match those which are declared in
their profile  however  we were unable to do this since we did not have access to the actual conversation
text  

references
    m  hall  e  frank  g  holmes  b  pfahringer  p  reutemann  and i  h  witten  the weka data
mining software  an update  sigkdd explorations  vol      no     pp             
    j  r  quinlan  c     programs for machine learning  san mateo  ca  morgan kaufmann       
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion  o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and
e  duchesnay  scikit learn  machine learning in python  journal of machine learning research 
vol      pp                 
 

fi