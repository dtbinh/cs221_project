classifying relationships between nouns
jun araki

heeyoung lee

erik kuefler

abstract
in this paper we develop a system for classifying
relationships between noun pairs  this is accomplished by finding the dependency paths between the
nouns in the reuters corpus and training a multiclass classifier based on the counts of the paths 
we experiment with several classification algorithms
 nb  knn  svm   feature selection techniques  and
additional features  the resulting system achieves
up to a     classification accuracy over four classes
of word relationships using an svm 

figure    the data collection and classification process 

 

introduction
 

as research in natural language processing shifts towards
semantic understanding  it is becoming increasingly important to understand the relationships between words 
one source of semantic information is wordnet  a lexical database which aggregates semantic relations over a
huge number of words  however  constructing and maintaining such large scale databases are extremely laborious  therefore  there is much interest in the automatic
extraction of semantic relations from text corpora  such
information is useful in topics including automatic thesaurus creation and question answering systems  in this
paper  we develop a system for automatically classifying relationships between nouns based on their usage in a
large corpus of text 

 

related work

hearst developed a hypernymy detection method based
on lexico syntactic patterns  but did not apply it to other
relations  hearst         girju et al  proposed a patternbased meronym classifier that showed some improvement  but they focused on only a few hand written patterns such as np  of np   girju et al          snow et
al  built an automatic classifier for hypernymy relations
that extracted lexico syntactic patterns based on known
hypernyms in wordnet  snow et al          using dependency paths  they generated a set of patterns to find
novel hypernyms  we build off this work by generalizing
from hypernym classifiers to multi class classifiers capable of identifying several noun relationships 

approach

our approach consists of two phases  data collection and
classification  in the data collection phase  we start by
extracting nouns from a corpus and retrieving antonyms 
meronyms  hyponyms  and hypernyms from wordnet for
each noun  section       for each pair retrieved  we find
sentences in another corpus in which both words appear
 section         we then extract the dependency path 
between the words in that sentence  section         for
each pair  we then count the number of times each type
of dependency path occurs to generate a feature vector 
in the classification phase  we experiment with several
classification algorithms and feature selection methods
 section      on the feature vectors resulting from the
data collection phase  we consider a subset of the extracted vectors balanced by word class and use    fold
cross validation to train and evaluate a multi class classifier  the resulting system is capable of classifying the
relationship between an arbitrary word pair by examining
its relationships in a corpus and running a classification
algorithm on the results 
   

word relationship extraction

in order to extract word relationships  we start with a seed
set of nouns  to obtain this set  we use nltk  loper and
bird        to extract all nouns from the trec   corpus  voorhees and harman        and retain the      
most frequent nouns  we use a different corpus for this
  see

 snow et al         for a full description of dependency paths 

fistep so as to avoid over fitting in our data  though we expect the results to be comparable 
for each candidate word  we then extract from wordnet the top antonym  meronym  hyponym  and hypernym
for that word  this results in a large set of pairs of related words  labeled according to the type of their relationships 
   

nsubj president    obama   
cop president    is   
det usa    the   
prep of president    usa   
figure    output of the stanford parser for the sentence
obama is president of the usa 

feature extraction

once a set of word pairs has been generated  the next step
is to extract the features that will be used directly by the
machine learning algorithm for each pair  this requires
extracting sentences for the corpus  we use the reuters
rcv  corpus  lewis et al         for the remainder of
this paper  and parsing and counting their dependency
paths 
      sentence extraction
the first step is to locate in the corpus every sentence
that contains both words in the word pair  the large size
of the reuters corpus  nearly  gb comprised of over
        articles  makes this a non trivial task  as performing a linear search for each word pair is prohibitively
expensive  to alleviate this problem  we first construct an
index that gives  for each word in the word pair list  the
documents in which that word occurs  since each article
is short and many words are rare  this leads to an efficient
representation  the resulting index is approximately    
mb  
given this index  we can efficiently extract sentences
as follows  first  we fetch the document list for each word
in the pair and take the intersection of those lists  we
then perform a linear scan over the remaining documents 
which can be done fairly efficiently as each individual
document is short  usually under   kb   whenever we
find a sentence containing both words  we add it to the
list that will be processed in the next phase 
      sentence parsing
we have so far gathered a list of sentences for
each word pair 
next  we must parse each sentence to extract the dependency path between the
word pair 
to do this  we use the stanford
parser  klein and manning         which  given
a sentence  extracts a list of dependencies in that
sentence in the form dep name word  index  
word  index    figure    
given this output  we wish to find the dependency path
between the word pair  when the two words are directly
dependent on each other  such as president and usa
in figure     we return this dependency directly  otherwise  we perform a breadth first search to find the shortest path between the two words  for example  in figure  
we would output nsubj prep of as the dependency
path between obama and usa 

this process is repeated for each extracted sentence 
yielding a list of all dependency paths between the two
words  all that remains in generating the feature vector is to assign a unique index to each path and to
count the number of times that the path was encountered  we would expect to see many occurrences of
short paths like prep of and few of longer paths  e g 
dep advmod dep pobj 
   

feature selection and expansion

the process so far results in a set of feature vectors suitable for training and testing a classifier  however  we
would not necessarily expect all of the extracted features to be useful  and there may be relevant factors of
words not captured by dependency paths  accordingly 
we experiment with several feature selection and expansion techniques  we hope that feature selection will be
able to remove irrelevant or noisy features  thereby improving our classifiers by reducing their variance  conversely  adding new features should improve the flexibility of our classifiers  thereby decreasing their bias 
     

feature selection

one easy way to reduce the number of features is
path shortening  there are many noun pairs having long
and complex dependency paths  we examine three simple path shortening methods  taking only the first dependency  only the last dependency  and only the first
and the last dependencies  for example  the dependency
path prep by dep prep to shortens to prep by 
prep to  or prep by prep to respectively 
another simple technique is removing the features that
appears only once through the data  this technique could
potentially be generalized to removing any feature that
occurs fewer than n times  we experiment only with removing single count features 
finally  we use a   analysis to compute a rank for
each feature for each class  we then retain a feature only
if it appears in the top n features for any class  with n
between    and        
     

feature expansion

in addition to experimenting with techniques for removing features  we also examine the effects of adding
new features  the two additional features with which

fiwe experiment are shared prefix and suffix length  which
specify the number of characters that the words share at
the beginning and end of the words  respectively  for example  the words cooperation and competition have
a shared prefix length of   and a shared suffix length of
   these features were motivated by the observation that
antonyms in particular tend to share long common suffixes  e g  stability vs  instability   so such features
may be useful for discriminating antonyms in particular 
   

classification

once features have been chosen  we train and test on
the resulting vectors using a variety of classification algorithms  the three we consider here are multinomial
naives bayes  using laplace smoothing and including
variants such as complement naive bayes  rennie et al  
        k nearest neighbor  and support vector machines
as implemented by libsvm  chang and lin         each
classifier is trained and evaluated using    fold crossvalidation over a balanced subset of the extracted data 

 

results

in this section we examine the results of our data collection and classification process  section     examines the
counts of each sort of relationship extracted at each step
in the data collection process  section     examines the
accuracy of several classification algorithms using several
feature selection techniques 
we focus on accuracy as our means of evaluating classifiers  as it is the most intuitive way to summarize a
multi class classification problem  here  accuracy is defined as the number of correctly classified examples divided by the total number of examples  this gives us an
aggregate score rather than a per class score  we would
expect an accuracy of     for purely random classification 
   

data collection

the number of features remaining after each step of the
data collection process is shown in table    the smallest class was antonyms with    examples and the largest
was hypernyms with     examples  from this data  we
created a balanced subset of     examples     per class 
by taking the longest  and so least likely to be effected by
noise  feature vectors from each class 
   

classification and feature selection

results for each classification and feature selection
method are shown in table    we found support vector
machines with shared affix length features and no feature
selection to be the most accurate classification method 
we report first on the baseline classification results for
each algorithm in sections             and then examine

antonyms
meronyms
hypernyms
hyponyms

pairs
   
   
     
     

sentences
   
   
     
     

paths
  
  
   
   

table    for each relationship  respectively  the total
number of word pairs drawn from wordnet  the number
of pairs which appear together in the reuters corpus  and
the number of words for which at least one dependency
path could be extracted 
the results of adding feature selection and expansion techniques in sections       and       
      naive bayes
transformation and weight normalization  rennie et
al         were found to hurt performance  so we report only the results of standard multinomial naive bayes
 nb  and complement naive bayes  cnb  in table   
we achieved a baseline accuracy of       using nb and
a slight improvement of       using cnb 

base
thresholding
shortening
 
expansion

nb
     
     
     
     
     

cnb
     
     
     
     
     

knn
     
     
     
     
     

svm
     
     
     
     
     

 expansion for nb and cnb includes   feature selection 

table    accuracy of different classification techniques in
the baseline case as well as after applying feature selection and expansion techniques 
      k nearest neighbor
we experimented with knn using k     and k  
   we observed that  nn outperformed  nn by a few
percentages under every condition  for instance  we
achieved a baseline accuracy of       for  nn and
      for  nn  thus  we report only the results of  nn
in table   
      support vector machines
our most accurate baseline results of        were
achieved using a multi class support vector machine  using the one against one method  with features scaled in
the range         a gaussian kernel  a c value of         
and a    value of     
we found       scaling to outperform both         scaling  which was better than no scaling   and c        
 

is a parameter to the gaussian kernel  which is computed as
k x  z    exp  x  z     

fiand      were found via grid search to outperform all
other c and  combinations  polynomial kernels with degrees between   and   did not perform significantly better
than chance  a linear kernel achieved an accuracy of only
     a sigmoid kernel performed relatively well       
but was still inferior to the gaussian kernel       
      feature selection
for non svm classifiers  feature selection methods
were useful in proportion to their complexity  simple thresholding slightly harmed accuracy in all cases 
whereas   gave a significant      boost in performance 
path shortening fell between these two methods with a
minor      improvement in performance 
table   shows a few of the highest ranked features
for each class according to     we saw that the most
important paths tend to only contain one or two steps 
particularly important relationships were prepositional
 prep for and prep of   conjunctional  conj and
and conj or   and adjectival  amod  

 
 
 

anto 
conj and
conj or
nn

mero 
prep for amod
nsubjpass
prep of amod

hypo 
amod
nn
infmod

hyper 
amod
conj and
prep of

table    the highest ranked features for each class  according to    
though feature selection improves accuracy for other
classification algorithms  it harms accuracy for svms 
for each selection method examined  accuracy dropped
from     to around         
      feature expansion
feature expansion via shared affix lengths was useful
for all classifiers  note that feature selection harms the
performance of svm   leading to an improvement between    and     even svms are improved by feature
expansion   the svm classifier resulting from feature
expansion achieved an accuracy of        which was the
highest of any classifier examined 
interestingly  feature expansion also greatly changed
the optimal value of c in the svm classifier  reducing it
from         to    this implies that the optimal svm
without feature expansion approximates a hard margin
classifier  but that the optimal classifier with feature expansion is much more tolerant of noisy data 

 

discussion

the results yield several interesting patterns  in section     we examine the raw data that we collected  section     looks at the results of feature selection and expansion techniques  finally  section     considers individ 

ual classification algorithms and insights gathered from
them 
   

data collection

even though the data is of reasonably high quality  the
amount of data  table    is limited by our smallest class
 antonyms  due to the desire to train over a balanced data
set  after balancing our data  we are left with     total
training examples     per class   this seemed to be sufficient data to get useful results  but gathering more would
be likely to improve classification accuracy  based on our
results  the best way to do so would appear to be finding
a more comprehensive list of antonyms  increasing the
number of frequent words  currently       or introducing some heuristics such as using antonym dictionaries
into out system would likely improve its performance because it would have a balanced set of more features 
   

feature selection and expansion

the results of feature selection as displayed in table   are
interesting for several reasons  we see that different techniques lead to different degrees of success  simple thresholding based on the number of times each feature occurs harms accuracy in all cases  whereas techniques that
combine features by shortening paths are slightly helpful
and   is very helpful  the first two results are interesting  it is not obvious that features occurring only once
would have much explanatory power  nor is it clear that
the first and last parts of the path are the most important 
but focusing on them does appear to improve accuracy
slightly    is a standard technique known to be successful in many contexts  so its good performance is unsurprising    generally did best when selecting around   
features for each class  out of        
another interesting fact is that these feature selection
techniques only appear effective with less powerful classification algorithms  naive bayes and k nearest neighbor   for support vector machines  performing any sort
of feature selection harms performance  though the resulting classifier still outperforms the other techniques 
svms seem powerful enough to select their own features
without using a separate technique 
though results from feature selection were mixed  the
effect of adding a new feature  shared affix length  was
unambiguously positive  accuracy was improved in all
cases  including svms  the fact that adding features
works better than removing features likely implies that
classification errors are a result of high bias rather than
high variance  meaning that the classification algorithms
do not have enough information available to adequately
distinguish among classes  it seems likely that finding additional useful features would further improve accuracy 
finally  the results of the   analysis in table   are
interesting on their own  in that they reveal how word

fipairs of a given class tend to be connected  for example  antonyms tend to be connected with the conjunctions and and or  this matches our intuitions of how
antonyms appear in text 
   

classification

support vector machines proved to be the best classification algorithm by a wide margin  outperforming naive
bayes and knn regardless of the feature selection and
expansion techniques used  naive bayes outperformed
knn  and was improved in most contexts by using the
complement heuristic  these results are all consistent
with those reported elsewhere 
another interesting way to look at the results is via the
confusion matrix  which is shown in table    these results suggest that hypernyms can be identified with high
accuracy compared to other classes  this may be due to
having significantly more data available for hypernyms 
which allows us to train and test over less noisy samples
for that class 
act   pred
antonym
meronym
hyponym
hypernym

anto 
 
 
 
 

mero 
 
 
 
 

hypo 
 
 
 
 

hyper 
 
 
 
  

table    confusion matrix for the base svm classifier
over one fold of the data  actual values are shown in
rows and predicted values are shown in columns 
one final way to examine performance on individual
classes is to train one vs rest classifiers for each class of
the data  to do so  we trained a classifier for each class by
drawing    examples from that class and    from the remaining classes  the results are shown in table    these
results suggest that meronyms are the easiest to identify 
achieving nearly     accuracy  interestingly  these numbers dont correspond to the confusion matrix in table   
this may be because libsvm trains one vs one rather
one vs rest classifiers 
class
accuracy

anto 
     

mero 
     

hypo 
     

hyper 
     

table    per class accuracy for one vs rest svms 

 

conclusion

we were able to extract dependency paths for many
words pairs by examining the reuters corpus  using a
support vector machine trained over these dependency
paths with additional features based on affix length  we
were able to achieve     accuracy classifying over four

relationships  random classification would be       feature selection methods were useful for non svm classifiers  but did not improve svm performance  these results suggest that our technique is useful for distinguishing among related words  and could likely be improved
via the identification and addition of other useful features 

 

future work

though our results are promising  there is still room for
improvement  our data set is fairly small  and so gathering more data from a larger corpus or a more comprehensive word list would likely improve results  our experiments with affix lengths as a feature suggest that we could
further improve accuracy by identifying relevant features
apart from dependency paths  finally  in order for this
system to be useful in practice  it will likely have to be
able to identify more than just the four relations we examined as well as explicitly indicate when no relation exists
between a given word pair  we note that our multi class
classification method can be easily extended to identify
more relationships  even between other parts of speech
 e g  verb noun relationship  

references
chih chung chang and chih jen lin        libsvm  a library for support vector machines  software available at
http   www csie ntu edu tw  cjlin libsvm 
r  girju  a  badulescu  and d  moldovan        learning
semantic constraints for the automatic discovery of partwhole relations  in proceedings of the human language
technology conference  hlt  
marti hearst        automated discovery of wordnet relations  in an electronic lexical database and some of its
applications  mit press 
dan klein and christopher d  manning        accurate unlexicalized parsing  in proceedings of the   st annual meeting on association for computational linguistics volume   
pages        
david d  lewis  yiming yang  tony g  rose  g  dietterich  fan
li  and fan li        rcv   a new benchmark collection for
text categorization research  journal of machine learning
research           
edward loper and steven bird        nltk  the natural language toolkit 
j d  rennie  l  shih  j  teevan  and d  karger        tackling the poor assumptions of naive bayes text classifiers  in
machine learning international workshop and conference 
volume     page     
rion snow  daniel jurafsky  and andrew y  ng        learning syntactic patterns for automatic hypernym discovery  in
proceedings of nips     mit press 
ellen voorhees and donna harman        overview of the
fifth text retrieval conference  trec     in proceedings of
the sixth text retrieval conference  trec      nist special
publication         

fi