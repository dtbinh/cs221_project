adaptive execution with online price impact learning
beomsoo park
electrical engineering  stanford university
beomsoo stanford edu
december         

  

introduction

buying or selling a large block of security is often followed by unfavorable movement of price which is called
price impact  one reason for the impact is that the block execution causes abrupt imbalance between supply
and demand and the other is that it might convey to other investors information about fundamental value of
the security that will be reflected on their future investment decisions  thus  when submitting large orders 
it is important to take into account the price impact in order to minimize the amount of value lost by it 
we propose an efficient multi period execution algorithm to minimize transaction cost incurred by the
price impact when buying or selling a block order  we assume that we have incomplete knowledge about
the price impact which is indeed the case in practice  a good execution algorithm strikes a balance between
exploration and exploitation  that is  on one hand  it needs to learn unknown characteristics of the
price impact for better future trading decisions in exchange of losing optimality at present  on the other
hand  it should make the best of current knowledge about the price impact for making an optimal decision
that is not necessarily effective for identifying remaining uncertainty about the impact 
more precisely  this problem can be formulated as a dynamic program that has random variables representing uncertainty on the price impact  but it is quite challenging to solve the corresponding bellman
equation due mainly to need for incorporating probability distributions on the random variables into a state
space  also  difficulty comes from the fact that both state and action spaces are continuous and that in most
practical cases a trading horizon is finite  say a few days or a week  therefore  we seek to propose a reasonably good  simple heuristic strategy that captures a good balance between exploration and exploitation
and compare its performance with that of a naive baseline strategy to be defined later and an upper bound
derived through information relaxation  to this end  we propose a linearized least squares with regularization
that is a modified version of least squares with regularization dealing efficiently with nonlinear relationship
between observations and model parameters 

  

problem formulation

consinder a trader who wants to liquidate a large long position or to recover a large short position of a
stock over a finite time horizon t   let xt denote the size of his ex trade position at period t with an initial
position x  such that a positive value implies a long position  he requires that his final position xt be zero 
he is assumed to be risk averse such that he seeks to maximize the objective function of the form
  t
 
x
 fi
 
e
pt xt    xt  fi x 
t  

where pt  pt  pt  is defined as the increment of a per share transaction price  which will be defined
later  the first term in the sum represents the change in book value that can be viewed as a per stage
 

firevenue  the second term reflects a holding cost  with  expressing the extent to which the trader would
execute sooner rather than later  note that the risk aversion coefficient is proportional to the volatility of
the stock price   
we model a natural price movement  defined as evolution over time of the price without the traders
transactions  as a gaussian random walk  i e  pt   pt    t   t  n          in practice  due to presence
of the price impact is the trader faced with a less favorable per share transaction price at which he sells or
buys a share  in order to capture this unfavorable movement of the price  we propose the following price
impact model with n time constants 




    
 
 

                             i
  
pt   pt      yt   yt   ayt    ut  rn   a       
   
i
i
 
  
 



n

n

where ut represents the size of market order submitted by the trader at period t and the pair  i   i  
characterizes one market impact component  note that the per share transaction price pt is a function of
ut   for i      trading ut affects the transaction price only at stage t and we call this impact an immediate
impact  for     i      the excitation triggered by ut dies away as time goes by and we call this impact a
temporary impact  for i      the effect of ut on the transaction price persists and we call this impact a
permanent impact  in this paper  we consider the case in which the coefficients i s and i s are unknown to
the trader  instead  we assume that he has  usually inaccurate  estimates for the parameters through data
analysis on historic transaction records 
to sum up  the trader solves the following finite horizon control problem 
  t
 
x
  fifi
 
max
e
pt xt    xt  fix 


subject to

t  

pt   pt    t   t  n          pt   pt      yt  
yt   ayt    ut   y        rn   xt   xt    ut   xt     

where the policy                    is a collection of functions
ptt each of which maps history up to time t to a
trading decision ut   for notational simplicity  define zti   j   itj uj and z i     for all i  then  it is easy
i
  ut and yti   i zti for all i 
to see that zti satisfies the recursion zti   i zt 

  
    

analysis
bellman equation with augmented state

let ht be history up to period t  from bayesian perspective  together with some prior distributions on 
and   bellman equation for this problem can be written as
fi i
h
fi
vt  xt    ht     max e pt xt    x t    vt    xt   ht    fiht  
ut

but it is quite challenging to get a closed form solution for vt   therefore  we aim to find an approximate
solution via policy parameterization  one might want to think of value function approximation as an
alternative solving methodology  in this particular problem  we prefer the former to the latter by the
following reasons  first  a class of well structured policies are readily available from analysis of clairvoyant
case which will be done in the subsequent section  moreover  it is impossible to simulate price impact without
actually trading and to get samples   finally  we need to learn the model parameters in an online fashion
over a relatively short time horizon and thus we should take advantage of special structural properties of
this execution model that the clairvoyant policy can capture effectively 
  some

people call this heisenberg uncertainty principle of price impact

 

fi    

policy parameterization

in order to derive an upper bound for the traders profit and a class of policies parameterized by  and  
consider the case in which both  and  are known  we seek to find reward to go functions vt  xt    zt       
that satisfy the following bellman equation 

 
vt  xt    zt          max      ut   i  a zt   xt    x t    vt    xt    ut   azt     ut      
ut r

with the terminal condition vt  xt     zt                   x t       i a zt   xt     it is natural to
conjecture that vt  xt    zt        is quadratic in the two arguments xt  and zt    that is  vt  xt    zt         
 
bt x t    ct  xt  zt    zt 
dt zt    in order to determine the coefficients bt  r  ct  rn   and dt  rnn  
we need to solve the following equation for them 
n
 
dt zt    max      ut   i  a zt   xt    x t 
bt x t    ct  zt  xt    zt 
ut
o
 
  bt    xt    ut      ct  
 azt     ut   xt    ut      azt     ut    dt    azt     ut  
 
the second order optimality condition is bt     ct  
       dt          comparing the coefficients on both
sides  we obtain the following set of three recursive equations 

bt   bt     

 
  bt     ct  
          
 
 
  bt     ct          dt     

 
 
       dt     a
          ct  
  bt     ct  
 
         d
  bt     ct  
t     


 
       dt     
 ct       dt       ct  
dt   a  dt   
a 
         d
  bt     ct  
t     

 
ct    ct  
a      i  a  

with terminal condition bt               ct        i  a  and dt       note that each bt   ct and
dt are functions of  and   the upper bound for the profit is given by v   x    z        

    

parameter estimation

two primary difficulties are present for estimating the price impact parameters  and   one is the absence
of offline training examples and the other is highly nonlinear  nonconvex relationship between observations
and the model parameters  especially  due to the second reason we cannot directly apply ordinary least
squares into the estimation problem of interest  in light of this  we propose linearized least squares with
regularization to address the technical difficulties  let us highlight two important features of this algorithm 
one is that this algorithm converts the nonlinear observation process into a linear one using iterates in the
previous stage so that it preserves efficiency of ordinary least squares  the other is that it deals effectively
with the constraints for  and   i e         and       using regularization while the solution of
ordinary least squares usually violates the range constraints  algorithm   gives the details of the linearized
least squares with regularization 
let us finish this section with comparison of algorithm   with two possible alternatives  extended kalman
filtering and nonlinear least squares  the extended kalman filtering suffers from the out of range problem
related to the range constraints for  and  and requires a strong probabilistic assumption that the prior
distributions on  and  be gaussian  meanwhile  nonlinear least squares can avoid these problems but it
turns out that it converges much more slowly than algorithm   

  

numerical experimentation

for numerical experimentation  we use the following setting for the model parameters 

 

fi t       for   trading day      hours   the lentgh of one trading interval    mins

 daily volatility          two ticks   i e            t
 risk aversion coefficient           
 initial position x             shares
 true parameter values                                                        
 initial estimates                                                               
the choice of         and               represents the case where todays market is more liquid than we
expected yesterday  for the purpose of comparison  we define a naive baseline policy as the one using the
same initial estimates for  and  over the entire time horizon  i e  no adaptation for  and   it is based
on the hypothetical notion that daily change of  and  is so small that it can be ignored 
we evaluate performances of our adaptive execution policy ae  and the baseline policy bl  by two
performance measures  percentage performance loss relative to the clairvoyant case cl  and percentage
performance gain relative to the baseline policy  we carry out    simulation runs for averaging  the
following summerizes the results 
 average performance percentage loss of bl relative to cl       
 average performance percentage loss of ae relative to cl       
 average performance percentage gain of ae over bl       
 percentage sample deviation from the cl value function        
estimate of  

estimate of 

estimate of  

 

   

   

   

estimate of  

 

 

 
   

   
 

 

  

   

   

estimate of  

 

 

x   

  

   

estimate of  

 

x   

  

 

   

  

   

 

estimate of  

x   

  

 

   

 

  

   

estimate of  

 

   

   

 

   

 

   

 

 

 

   
 

 

x   

 

 

  

   

   

 

  

   

figure    evolution over time of estimates for   above  and   below 

cl trades over time

ae trades over time

bl trades over time
 

    

    

    

    
    
    

size of sell order

 

size of sell order

 

    
    
    

 

  
t

   

    
    
    

 

  
t

   

 

  

   

figure    evolution over time of size of order   i  clairvoyant case left    ii  t adaptive execution middle    iii 
baseline policy  right 

 

fi  

conclusion

we quantify contribution of the adaptive execution policy adopting online learning of the unknown price
impact coefficients relative to the naive baseline policy with no adaptation and we show that the performance
gain from adaptation is significant  this execution model fits for intraday execution in the situation where
price impact remains unchanged within a single trading day but changes daily  one might want to argue the
presence of intraday change of price impact but it is quite challenging to track the time varying aspect because
of lack of data set for training  hopefully  the price impact model adopted here can capture practical intraday
price impact pattern reasonably well on average and our adaptive execution policy can make a contribution
to reduce transaction costs incurred by price impact  future work includes  i  theoretical justification for
choice of regularization coefficient and  ii  exploration for a better estimate over an uncertainty ellipsoid
centered at the linearized least squares estimate that has been studied in multi armed bandit literature 

references
   d  bertsimas and a  w  lo        optimal control of execution costs  journal of financial markets
   a  obizhaeva and j  wang        optimal trading strategy and supply demand dynamics  revised
and resubmitted  journal of financial markets
   c  c  moallemi  b  park  and b  van roy        strategic execution in the presence of an uninformed
arbitrageur  submitted for publication 
   d  bertsekas        dynamic programming and optimal control 
   r  sutton and a  barto        reinforcement learning  an introduction 
algorithm   linearized least squares with regularization
initialize x    y    z              
   wait until coefficient matrices for ols are full rank
for t       n    do
choose greedy ut w r t  vt  xt    zt     t       t    
xt    xt    ut   zt    a t   zt     ut    t      t       t       t  
end for
   once the matrices are full rank  perform ordinary least squares with regularization on the linearized model
for t   n   t do
initialize  t      t       t       t  
while  t  and   t  do not converge yet do
 t 
 t 
compute
          yt 
 y 
 using      

y  
 
 
 
yt 

p        y   u   
 
     

   
 
 
pt       yt   ut  
  

 t    
 t     argmin   g
   
t   ht       t     
 t 
compute
z
 
 
 
 
 
z
using

 
 
t 


  p   
u      z    i  a 
 
 


  ht   
gt    
 
 
 
 
 
pt
ut     zt 
 i  a 
  t     argmin   gt   ht        t       t       
g
t

h
t

end while
choose greedy ut   w r t  vt  xt    zt     t      t     xt      xt   ut     zt      a t  zt    ut  
end for

 

fi