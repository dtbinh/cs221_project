cs    project  a machine learning approach to stroke risk
prediction
yu cao
hsu kuang chiu
aditya khosla
cliff chiung yu lin

abstract
in this paper  we consider the prediction
of stroke using the cardiovascular health
study  chs  dataset  missing data imputation  feature selection and feature aggregation were conducted before training and
making prediction with the dataset  then
we used a mixture of support vector machine  svm  and stratified cox proportional
hazards model to predict the occurrence of
stroke  different methods and variations of
models were evaluated and compared to find
the best algorithm 

   introduction
currently home monitored chronic health condition
data is used in health risk assessment systems  according to the data  the systems make predictions on
the possibility that a a patient might need medical
help in the next few years due to the onset of disease
or other conditions  the prediction result is helpful
for prevention or early treatment  however  many current systems use relatively simple hand coded rules to
build the prediction models  applying machine learning techniques to the health risk assessment problem
will be a possible approach to have more accurate
predictions  in this project  our objective is to improve the accuracy of stroke prediction using the chs
dataset  it is a challenging task for three main reasons 
    the chs dataset suffers from problems such as a
large fraction of missing data         sets of correlated
features  while some of the features are not highly related to stroke  and     the stroke prediction problem
itself involves the survival time of individuals that are
not captured well by typical machine learning meth 

yufcao stanford edu
hkchiu stanford edu
aditya   stanford edu
chiungyu stanford edu
ods     the dataset is extremely skewed as only   
of the patients had a stroke over the period of consideration  therefore  missing data imputation  feature
selection and aggregation  and the cox proportional
hazard models are integrated to overcome problems    
and      to overcome problem      we use the area under the receiver operating characteristic  roc  curve
instead of the usual measures of accuracy as we can
achieve     accuracy simply by classifying all the patients as not having a stroke  which is clearly not a
useful prediction 
     the cardiovascular health study  chs 
dataset
the chs     is a study of risk factors for cardiovascular diseases in people above    years old  more than
      patients were examined yearly from      to      
with about       attributes collected annually through
medical tests and a set of questionnaires  events such
as stroke and hospitalization were also recorded for
each patient  however  in a longitudinal study like
chs  it is unlikely that all patients return each year
and provide data for all the required attributes  in
the chs dataset  some attributes are missing intermittently for certain years  while some attributes have
missing data for a contiguous series of years  furthermore  the attributes collected per patient change from
year to year and there is no easy way to determine the
change over time for a single feature 
     our contributions
we compared more than   different data imputation
methods to find the best method for the given dataset 
we also used forward search feature selection to pick a
smaller set with     features  a prediction model using svm and cox is then built to determine whether a
patient has a high risk of stroke in the next   years  we
tried a variety of other methods including em based
algorithms and gaussian process regression  but we
have not described those as they did not produce sat 

fia machine learning approach to stroke risk prediction

isfactory results  using data imputation  combined
with feature selection  we achieved      for area under the roc curve  we also tried to incorporate the
data from multiple years to make a better prediction 
the rest of the paper is organized as follows  section  
provides an overview of the problems that we consider 
and reviews the related previous works in the literature  section   describes the approaches to tackle the
issues and improve the results  then  experimental
results are shown and discussed in section    finally 
conclusions are in section   and future research directions are given in section   

   problem overview and
previous work
the problem we consider can be roughly divided into
three parts      imputation of the missing entries in
the dataset      selection of strong features and aggregation of weak features  and     stroke prediction with
the selected filled in features 
     missing data imputation
the first part of the problem is to fill in the missing
entries in the dataset  in     several methods were discussed  including filling missing features with column
mean  column median and hot deck imputation  they
are commonly used in statistics and serve as the baseline methods against which we would like to test the
other algorithms that we develop or use 
we evaluated the missing data imputation results with
the following two sets of metrics 
   data imputation accuracy  the primary target
of missing data imputation is to achieve accurate
imputation of the missing entries  evaluated by
the following   metrics as described in     
 a  root mean square deviation  rmsd 
 b  mean absolute deviation  mad 
 c  bias  the difference between mean of the imputed results and mean of the ground truth
values 
 d  proportionate variance  pv   the proportion
of variance of the imputed results to variance
of the ground truth values 
   overall stroke prediction quality  the ultimate
goal of the missing data imputation is to collaborate with stroke prediction algorithm  imputation results were fed to the prediction methods to
evaluate the overall stroke prediction quality 

     feature selection and aggregation
the chs dataset has a large number of attributes
ranging from demographic information  clinical history  to biomedical and physical measurements      
however  only a small subset of attributes is highly
relevant to stroke prediction  in addition  some individual attributes can be weak but correlated  and the
aggregated feature may serve as a good indicator to
stroke occurrence      has shown that svm is one of
the best methods for feature selection  other papers
such as      also use manually selected features according to risk factors analyzed by medical and clinical
study  the subset of features selected can be combined with stroke prediction models to evaluate the
performance of feature selection and aggregation 
     stroke prediction
after filling the missing data entries and selecting the
most representative features  we can use those preprocessed data to build the stroke prediction model  in     
several machine learning algorithms were applied in a
stroke risk assessment problem  support vector machines  svm   decision trees  nearest neighbors  and
multilayer perception  according to      svm is the
most promising algorithm with high sensitivity and
specificity  therefore svm was chosen to build our
stroke risk prediction model  the evaluation metric in
medical diagnosis is better chosen as the area under
roc curve in order to assess both the sensitivity and
specificity performance of the model 
the cox proportional hazards model is one of the
most important statistical models used in medical
research     this model has been extensively studied   
    and has been applied in various medical applications for the prediction of various diseases        and
analysis of medical data     the paper by lumley et
al        which makes use of the cox proportional hazards model is the paper that we used as our baseline
result as it contained the best stroke prediction scores
as compared to the other papers relating to this application  we followed the method described in      to
attempt to achieve the same results and to use that
as our comparison metric to gauge the success of our
models 

   algorithms
     missing data imputation methods
for missing data imputation  two main observations
on the chs dataset affect our strategies      only a
small subset of the features is highly related to the
occurrence of stroke  and     many of the features are

fia machine learning approach to stroke risk prediction

discrete instead of continuous  due to observation     
we focus our prediction and evaluation on a set of    
features selected by forward search using svm  and
due to observation      for most algorithms that we
implement  we also evaluate extra versions that align
each imputed value to its closest discrete label 
besides the baseline methods  the following imputation algorithms were implemented and evaluated 
   column mean with alignment to the closest discrete value
   linear regression
   linear regression with alignment to the closest
discrete value
   singular value decomposition  svd 
   singular value thresholding  svt  from    
     feature aggregation
in the set of questionnaires answered by each patient 
there are several groups of contiguous questions designed to evaluate a similar quality  for example  how
often the patient does various sports or whether the
patient can spell some words correctly  the answer
options also have the same pattern for these questions 
these features are highly correlated  therefore we
looked through the descriptions for each feature and
decided to aggregate a group of contiguous features if
they are targeted at the same type of assessment and
share the same set of values in the choice of answer 
in the end     groups were selected  since the features within a group have the same answer values  we
could use the mean of these values as an indicator for
the aggregated feature  however there can be missing
value in any of the features for a patient  therefore we
should check whether missing values of features exist
for each patient  and exclude the missing values when
computing the mean for that patient  in addition  the
aggregated features should be computed before standardizing all the feature values and removal of features
with lots of missing data to ensure that the aggregated
features capture the property of the original data 
     feature selection

so it will be computationally expensive to complete
the entire forward search process with all the features 
thus  l  regularized logistic regression was executed
first to choose     features with highest significance
ranking as the domain of our forward search process 
the forward search was performed with linear kernel
svm and   fold cross validation  the value of parameter c was also tuned to achieve the best results 
     stroke prediction with support vector
machine
our stroke prediction model has five steps 
   feature aggregation  average weak but correlated
features
   data preprocessing  remove features and training examples with missing data rate higher than
threshold values 
   features selection  select the best subset of features 
   missing data imputation  fill in values of missing
entries 
   svm training and testing  with the feature set
obtained from previous step  train an svm model
with linear kernel due to computation efficiency 
in testing  we used    fold cross validation to obtain an average generalization performance 
     cox proportional hazards model
the proportional hazards regression model is given by
h t x    h   t exp 

n
x

i xi  

   

i  

where h t x  is the hazard value at time t given the
feature set x for an individual  x            xn are the
features  h   t  is an arbitrary baseline hazard function 
and             n are the parameters that we are trying
to estimate for the model  this model is known as
a semiparametric model because the baseline hazard
function is treated nonparametrically  thus  we can
see that the parameters have a multiplicative effect
on the hazard value which makes it different from the
linear regression models and these models have been
shown to correspond better to biological data    

firstly  features with missing data rate higher than a
threshold value were removed  even though we have
missing data imputation algorithms  features with too
many missing entries still may not give us accurate information after imputation  therefore  those features
were filtered out 

given the attributes of two individuals  x    and x     
we can obtain their hazards ratio as
n
x
h t x     
   
   
 
exp 
i  xi  xi   
   
h t x     
i  

then forward search technique was applied to select
the subset of most representative features  however 
the number of features was     after preprocessing 

thus  we observe that the hazard ratio is independent
of the time t  and by comparing these hazard ratios 
we can find a threshold to classify the individuals 

fia machine learning approach to stroke risk prediction

the same steps as described in the previous section
were used with this model to make predictions to be
able to compare the two models 
     cox proportional hazards model with
svm
the svm and cox models described in the previous
sections only allowed us to make predictions using the
baseline data  or data from a single year  to incorporate the effect of the data from multiple years  the two
models were combined using the following algorithm 
for each of the years that we want to consider 

ble    here column mean was used for missing data
imputation  we chose a linear kernel with a fixed c
value equal to        for svm model  we can see
that using only the feature set by forward search has
better performance than using manually selected features      and using all chs features  forward search
technique has successfully selected the most representative features in the stroke risk prediction problem 
moreover  we also tried the feature set obtained by
forward search beginning with lumleys features      
the result is slightly lower than pure forward search 
feature aggregation combined with the     selected
features yields similar performance as well 

   experimental estimation

the various methods of data imputation were combined with the svm model for verifying the importance of good data imputation  the list of data imputation methods  and their results are listed in table
   we found that the different methods of data imputation did not affect the outcome of the model significantly  furthermore  the rmsd values obtained for
the various data imputation methods were not exactly
correlated to the stroke prediction outcome  instead 
we found that the data imputation models that gave us
the lowest rmsd values such as linear regression and
linear regression with discretization produced worse results for area under the roc curve than simple imputation methods such as discretized column mean and
column median  the reason for this may be the fact
that the initial feature selection algorithm was run using data that used column mean as the method for
data imputation 

     missing data imputation quality

     cox proportional hazards model

the missing data imputation results with baseline
methods and our proposed algorithms are shown in
table      and   respectively  in general  linear regression achieves the least rmsd and mad values
with reasonable bias and pv values  and alignment
to discrete values further lowers the mad value but
increases rmsd and bias values 

we implemented the cox proportional hazards regression model using the same set of features as described
in       and found the coefficients by fitting the data
using matlab in a similar fashion as described in the
paper  however  we were not able to obtain the same
coefficients as described in the paper  and found that
the area under the roc curve was only        using
the features given in the paper  and        using the
    features found using forward search 

   perform feature selection using forward search
   get a hazards value by applying the cox proportional hazards model
   combine the hazards value by using them as features for a svm
this model allowed us to incorporate the fact that the
features from each year affected the prediction score
in a multiplicative way  and the relative importance of
each year could be estimated by finding the parameters corresponding to the hazards ratio from each year 
the regular model that allows for time dependence in
the cox model could not be used as features do not
remain consistent from year to year  this model allowed us to incorporate any set of features from multiple years 

     feature selection and feature
aggregation results
using forward search and   fold cross validation with
linear kernel svm  the area under roc curve achieves
the largest value       when     features are selected 
the properties of the    aggregated features are summarized in table   
     stroke prediction precision with svm
the average performance of stroke prediction models
with svm and different feature sets is shown in ta 

furthermore  to try to emulate the       we followed
the method for processing the data exactly as described in the paper and tried to use the same coefficients for the generated data  and found a an area
under the roc curve of         this was very low as
compared to the reported area of      in the paper 
we were not able to verify this value despite following through the details provided in the paper multiple
times  also  we requested professor lumley for his
data set to be able to verify his result but we have not

fia machine learning approach to stroke risk prediction

heard back from him for the last   weeks 
     svm combined with the cox model
the results of the model combined for two years is
given in table    the results from using data for multiple years proved to be the best as we managed to
achieve area under the roc curve values close to      
the area under the roc curve was averaged over   
independent training and test data sets  this model
was only tried for data from two years due to the lack
of time and is possibly a promising direction to achieve
better overall results by combining the data in this way
for more years 

   conclusions
this project has shown that machine learning algorithms can be a powerful tool to achieve good results
even in difficult data sets like the one presented in
this paper  we managed to match the values achieved
by hand selected features through automatic feature
selection  furthermore  we found that the machine
learning tools provide a strong mechanism to handle a
variety of tasks involving both imputation of missing
data and stroke classification 

   future work
due to the shortage of time  we were unable to implement some of the suggestions that were brought up
during the course of the project  we would like to
suggest these as possible directions for future work 
firstly  for data imputation  an ensemble of methods
could be tried that encompass various machine learning methods that are selected from a variety of methods such as svm  linear regression  logistic regression 
em based methods  etc  each of the methods could be
tested for each feature  and depending on the results 
we could use the best method found for each of the
features 
furthermore  upon looking more closely at the data 
we found that the data contained values for certain
features that could lead to poor classification as well
as data imputation  for some of the features that
were yes no questions  these answers were assigned
a value of      but sometimes there was another value
unknown which was written in the data as    removing these values and applying better data imputation techniques could lead to an overall increase in the
stroke prediction quality although the initial tests did
not suggest that data imputation improved prediction
scores 

secondly  for stroke prediction  we could try increasing the number of years considered for the cox svm
model to potentially increase the prediction score 
also  we could attempt using other methods for exploiting the time series property of the data which was
not completely used in our current project  this timeseries property could also be used to improve data imputation  but this problem is a difficult one given the
nature of the data  the features in the data vary from
year to year and even the same features do not have
the same names  thus  it would be a cumbersome
task to find commonality between features from multiple years 

acknowledgements
we thank honglak lee for his guidance and support
throughout the project  he provided us with direction
and various useful tools to carry out the data analysis
that we needed  also  we thank professor ng for giving
us an opportunity to work on such interesting topics
by offering this class and for his invaluable teachings
throughout the quarter  lastly  we thank anand iyer
for partaking in discussions about the project during
its inception 

references
    j  m  engels and p  diehr  imputation of missing longitudinal data  a comparison of methods 
journal of clinical epidemiology                 
     
    klein j  and moeschberger m  survival analysis  techniques for censored and truncated data 
springer       
    j c  sanchez j  prados  a  kalousis  mining
mass spectra for diagnosis and biomarker discovery of cerebral accidents  proteomics        
           
    e  j  candes j f  cai and z  shen  a singular
value thresholding algorithm for matrix completion  arxiv       
    satoshi saitoh kenji ikeda  hiromitsu kumada 
effect of repeated transcatheter arterial embolization on the survival time in patients with hepatocellular carcinoma  cancer       
    tsuyoshi nakamura kouhei akazawa  simulation
program for estimating statistical power of coxs
proportional hazards model assuming no specific
distribution for the survival time  elseview ireland       
    steven g  self kung yee liang and xinhua liu 
the cox proportional hazards model with change
point  an epidemiologic application  biometrics 

fia machine learning approach to stroke risk prediction

                
    p  enright l p  fried  n o  borhani  the cardiovascular health study  design and rationale  ann
epidemiol                    
    thomas augustin ralf bender and maria blettner 
generating survival times to simulate
cox proportional hazards models  statistics in
medicine                    
     r  a  kronmal t  lumley  a stroke prediction
score in the elderly  validation and web based
application  journal of clinical epidemiology 
                   

training rmsd
test rmsd
training mad
test mad
training bias
test bias
training pv
test pv

column mean
max 
avg 
      
      
      
      
      
      
      
      
      
 
      
       
      
      
      
      

min 
    
      
      
      
       
       
 
 

model
cox model year   with feature selection 
cox model year   without feature selection 
cox svm model on above data sets
cox svm model with feature selection for
both years

auc roc
      
      
      
      

table    cox proportional hazards model with svm

column median
max 
avg 
     
      
     
      
     
      
     
      
     
      
      
      
      
      
 
      

min 
      
 
     
 
      
      
 
 

hot deck
max 
      
      
      
      
      
     
     
    

avg 
      
      
      
      
 
       
      
    

min 
      
      
      
      
       
       
      
      

table    data imputation quality with baseline methods

algorithm
hot deck
column mean
column mean discretized
column media
linear regression
linear regression discretized
svd

auc roc
      
      
      
      
      
      
      

table    stroke prediction quality for a variety of data imputation methods 

training accuracy
testing accuracy
auc roc

all
      
      

lumleys     
      

forward
search
      

forward search begin
with lumleys     
      

forward search with aggregated features
      

      

      

      

      

      

      

      

      

             

table    results of stroke prediction models with svm using different feature sets
 the number in parenthesis is the size of feature set 
auc roc  area under roc curve

fia machine learning approach to stroke risk prediction

training rmsd
test rmsd
training mad
test mad
training bias
test bias
training pv
test pv

column mean and alignment
max 
avg 
min 
     
      
    
     
      
 
     
      
     
     
      
 
     
      
      
      
      
      
     
      
 
 
      
 

linear regression
max 
      
      
     
      
      
      
 
    

avg 
      
      
      
      
       
       
      
      

min 
 
 
    e   
    e   
       
       
      
      

linear regression and
alignment
max 
avg 
min 
      
      
 
      
      
 
      
      
 
      
      
 
      
      
       
      
      
       
      
      
 
      
      
      

table    data imputation quality with non baseline algorithms

training rmsd
test rmsd
training mad
test mad
training bias
test bias
training pv
test pv

fill in with
max 
      
      
      
      
      
     
       
    e   

svd
avg 
      
      
      
      
       
     
      
    

fill in with svt
max 
avg 
      
      
 
      
      
      
 
      
      
      
 
      
 
 
 
 

min 
 
 
 
 
       
       
 
      

min 
      
 
     
 
     
 
 
 

table    data imputation quality with other non baseline
algorithms

 
 
 
 
 
  
  

ability to walk without difficulty
recent exercise or physical work
general trend of mood
awareness of time and venue
ability to spell simple words
perform simple tasks with hands
frequency of eating various beans
table    properties of aggregated features

 
 
 
 
  
  

optimistic or pessimistic
difference in physical stamina
sleep problems
simple mathematical ability
ability to repeat words
frequency of taking fruits fruit juices

fi