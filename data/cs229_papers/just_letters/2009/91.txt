 

cs    final project  autumn     

automatic beat alignment of rap lyrics
sang won lee   jieun oh 
department of management science and engineering
 
center for computer research in music and acoustics
 sangwlee  jieun   stanford edu
 

abstract rap is characterized by highly rhythmic
delivery of words  the subtle ways in which
syllables in the lyrics fit into the beats in the music
lead to expressivity in rap music  unfortunately 
given a lack of notated score for the genre of rap 
the task of determining the rhythm to existing rap
performances must be carried out manually  in this
paper  we explore a method of automatically
aligning rap lyrics to beats using logistic
regression  specifically  we select top linguistic
and audio features and compare the two resulting
models  the audio model yielded about    higher
accuracy than the linguistic model but at the cost
of heavy computations in the data preparation
stage 
potential applications to our technique
include automatic rap rhythm transcription and
style characterization and imitation of rap artists 
   introduction
rapping is a primary ingredient of hip hop
music that is characterized by highly rhythmic
delivery of words with relatively small variation in
pitch  the alignment of the syllables in the lyrics
to the beats in the music thus defines the sound
and feel of a particular rap song 
unfortunately  the rhythm of rap is rarely
notated  and the task of determining how the words
in the lyrics fit wit the musical beats must be
carried out manually through careful listening 
a technique using dynamic programming    
tries to align lyrics and audio of popular music at a
paragraph to segment level using hand labeled
lyrics  this algorithm finds the minimum cost
alignment path between lyrics and audio and
further adjusts the results using vocal and nonvocal classifiers  this approach works on a higher
structural level  so it has an average alignment
error of      seconds and a standard deviation of
     seconds  which is too coarse for our purpose
of aligning words to music at the syllabic level
 lasting only a fraction of a second  
one approach to this problem would be to take
the audio signal of the entire song  perform source 

separation between vocals and instrumental parts 
andassuming that the vocal part can be cleanly
extractedsegment the audio at every syllabic
onsets to perform analysis in the time  and
frequency domain  unfortunately  extracting only
the vocal part     and automatically segmenting
the resulting speech audio at every syllable onsets
      are challenging signal processing maneuvers 
even using state of the art techniques  so  we have
manually created a data set resembling what would
likely result from successful source separation and
syllable segmentation  to serve as an input to our
learning algorithm  audio model  and to compare
the resulting alignment prediction against that of
an easier alternative method 
the alternative approach under consideration is
based on taking linguistic features in the symbolic
domain  i e  not relying on the audio signal   the
merits of using features in the symbolic domain
and in particular linguistic features taken from the
lyricshave previously been explored to perform
music genre classification      taking this idea  we
also create a linguistic model with which to train
parameters and calculate the probability of a given
syllable in the lyrics falling on the musical beats 
we compare this result with the audio model 
   method
    data
we chose ill be missing you by puff daddy
 featuring faith evans  as the music for training
and testing  
since our goal is to determine the syllables to
which the musical beats fall  we broke down each
word in the lyrics  into their constituent syllables 


 

ideally  parameters should be trained and or tested on
multiple songs to check robustness of our method  the
extent to which parameters found on puff daddys ill be
missing you applies to  i  other renditions of the same song
by a different artist   ii  other songs by puff daddy  and
 iii  a completely different style of rap  would be an
interesting study to conduct as a next step 
 
ill be missing you comprised of   verses  each having   
measures of     meter 

fi 

cs    final project  autumn     

for example  the word notorious was broken down
into four syllables  no to ri ous 
    response variable y i 
the song has a     time signature  so a phrase of
text is spread across four major beats  thus  we
design a response variable yj i  with four
components  j   indicating whether syllable i
falls on beat j  optionally  syllables that do not fall
on any of the four beats can be categorized as j    
    feature vector x

 i 

for each syllable i  we prepare features in the
following three categories   see section   for a
description of our feature selection method to
choose the most informative features to make up
our linguistic model and audio model  
      relative position in phrase  a 
we created four features to convey the temporal
order of syllables in a given phrase  to do so  we
first calculated rpij          the relative position of
syllable i in phrase 
index of syllable i in phrase
rpi  
  of syllables in phrase
from this  we calculated four features 


x i       rpi
x i       rpi
x i       rpi
x i       rpi

        
        
        
        

annotated  but could be automated by
performing string matching on phonetic
transcription  and through other sophisticated
techniques for detecting internal and imperfect
rhymes     
   parts of speech  nine features indicating
whether the word to which the syllable
belongs is a noun  pronoun  verb  preposition 
article  adjective  adverb  or conjunction 
      audio features  c 
finally  we added three audio features  ideally 
these features would be calculated using the vocal
part that has been extracted from the original
recording of the song  but because audio source
separation is a difficult problem   we created a data
set resembling what would likely result from
successful source separation  in order to get a
rough idea of the prediction accuracy that can be
achieved by these means 
specifically  we created a recording of the rap
part as closely as possible to puff daddys
performance  then we segmented this audio at
every syllable boundaries  and calculated the
following features for each syllable 
   duration  length of the syllable in seconds
   power  average power of signal  the mean
square of a real signal 
   pitch  average frequency of the syllable in
hertz  normalized to      

      linguistic features  b 
we also annotated the following eleven
linguistic features  note that these features are
relatively easy to create  based on a simple
dictionary and phonetic transcription lookups 
thus  it would be quite feasible to automate this
process to annotate a large volume of data 
   number of syllables  denotes the number of
syllables in the word that the syllable came
from   i e  assign   to each of the syllables in
no to ri ous  
   lexical accent  encoded as   if the syllable is
accented  and   otherwise 
   rhyme  encoded as   if the syllable functions
as a rhyme  and   otherwise   this was hand 


 

this design would assign a class to every syllable  and
thus would also allow the use of softmax regression as an
alternative machine learning algorithm 
 
based on the work of      we could also consider
encoding rhyme as a continuous value between   and  

figure    syllable beat mapping and feature vector


such that half rhymes or internal rhymes are assigned a
smaller number compared to perfect rhymes  
 
source separation on multi channel audio using ica
works well if each channel represents input from a
microphone placed in distinct locations  but the problem
becomes very difficult when the different microphone
inputs have been combined  flattened  and digitally
manipulated to create effects 

fi 

cs    final project  autumn     

    learning algorithm
for each output component j  representing
musical beats  and feature k  we performed logistic
regression using batch gradient ascent  we updated
j k until convergence              iterations  
 j k     j k     y  i j k  h  x  i     x  i j k  
we then made our hypothesis 
 
t
h   j  x  i      g  j x   
 t x
   e j

train error and test error calculations were made
by summing up the correct predictions  that is 
instances in which  hj x i       and y i      or

 hj x i       and y i       but we used a separate
heuristics for making the final syllables to beat
alignment prediction  the details to which are
presented in section   
   feature selections
to determine the extent to which each feature
improves our hypothesis  we carried out feature
selection using a hybrid method of filter selection
and forward search  based on the features average
correlation score across output components 
    feature selection score
for each output component j  we calculated the
feature selection score sj k  to be the correlation
between xk and yj to measure how informative
each feature xk is about the class label yj  we then
sorted features in decreasing order of
 
s k      s j  k   
j
 



s  k 
s  k 
s  k 
s  k 
s  k 
s k 

rhyme

pronoun

verb

article

noun

prep 

      
      
     
     
      
     

      
      
      
      
     
     

      
      
     
     
      
     

      
      
      
      
     
     

      
     
      
     
      
     

     
      
      
      
     
     

accent

adjective

adverb

conjunction

 syllables

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
     

figure   and figure   show the selection scores
for linguistic features and audio features 
respectively  in decreasing order of s k  
    feature selection method
we use a hybrid of filter feature selection and
forward search to pick features to insert in our
linguistic model and audio model 
specifically  having ranked our n features
 n    for linguistic  n   for audio  in decreasing
order of s k   we construct a setl consisting of top l
features  for  ln  in other words  for each
linguistic and audio domain  the lth most
informative feature gets added to setl  we desire to
determine the optimal l  for the linguistic model
 ll   and audio model  la   based on the training
error and test error computed 
the rationale for using this hybrid model is that
it involves comparing error rate across just n
models  as opposed to o n   required by forward
search  but because we choose l  based on our
calculation of training and test errors  rather than
arbitrarily deciding on it beforehand  as is done in
filter feature selection   we are able to make a
computationally efficient decision about the
number of features that will our models will
comprise of   
we use k fold cross validation  k     we  i 
train on verse   and test on verse    and  ii  train
on verse   and test on verse    and sum the two
results  figure   and figure   show comparisons
between training and error rates as we append lth
feature in our models 

figure    linguistic features  selection scores
s  k 
s  k 
s  k 
s  k 
s  k 
s k 

duration

pitch

power

     
     
     
     
      
     

     
     
      
      
      
     

     
     
      
      
     
     

figure    audio features  selection scores

figure    linguistic features
training and test error comparisons


 

we confirmed using the audio set  n    that our hybrid
selection method  involving comparisons of   different
sets  yielded the same feature selection order as forward
search  involving comparisons of   different sets  

fi 

cs    final project  autumn     

   discussion

figure    audio features
training and test error comparisons

based on test and training error comparisons 
we chose ll     rhyme  pronoun  verb  and la   
 duration  pitch   thus  the features that make up
our models are as follows 
linguistic model 
 relative position in phrase     rhyme  pronoun  verb 
audio model 
 relative position in phrase     duration  pitch 

   alignment result
given a phrase of lyrics to be spread across four
beats of a measure  we assigned syllables to beats
using the following heuristics 
for each measure  m   
for each beat  j  
assign to beat j the syllable i with highest h x i   
t

h   j  x  i      g  j x   

 


t

x

   e j
consequently  we are guaranteed to have exactly
one syllable assigned to beat j  even if h j x i  
happens to be less than     for all syllables i in the

measure  figure   compares the accuracy of
alignments attained from our models  see
appendix for a comparison between the linguistic
and audio models alignment result for verse   and
verse   

our audio model yielded higher alignment
accuracy than our linguistic model  the train error
difference was       and the test error difference
was       however  the difference in accuracy
may not be worth the technical trouble of creating
the audio model  audio featuressuch as power
and pitchrequire heavier computations in the
signal processing domain  but even a greater
challenge with preparing the audio features lies in
obtaining a clean source separated vocal part that
has been segmented at every syllable onsets  these
are time consuming processes to carry out
manually  and have dissatisfying results when
performed automatically using even state of the art
techniques 
in contrast  linguistic features are relatively
easy to prepare using a dictionary and phonetic
transcriptions  given that rhyme was the most
informative linguistic feature in predicting beat
alignment  we may be able to improve our
accuracy significantly by using automatic rhymedetection techniques that can spot even the subtle
internal rhymes and imperfect rhymes      this
would be a good next step for improving our
current design 
additionally  a separate model on anacrusis
prediction  i e  determining whether the first
syllable of phrase falls on the downbeat or not 
should be integrated into our design to improve
alignment of beat   and beat   
finally  future studies should evaluate the
extent to which our model can be applied to songs
by different rap artists  or even to different schools
of rap  this will open up the possibilities of
characterizing styles of rap artists or performances
based on the temporal rhythmic flow of rap 
potential applications for our model include
automatic creation of extended lrc files  which
contain song lyrics that are time stamped at the
word level  for the genre of rap  these files are
currently manually created for use in karaoke  but
with the ability to align rap lyrics to musical beats
based on the characteristic style  creation of these
files can be largely automated  online music
streaming services can also use our technique to
display lyrics right as they are being played 
   acknowlegment

figure    accuracy comparisons

the authors wish to thank professor andrew ng
and the teaching assistants of cs    for their
feedback and guidance 

fi 

cs    final project  autumn     

   references
    lee  kyogu  and markus cremer 
segmentation based lyrics audio alignment
using dynamic programming  proceedings of the
 th international conference for music information
retrieval  ismir       
    vincent  emmanuel  hiroshi sawada  pau
bofill  shoji makino  and justinian rosca  first
stereo audio source separation evaluation
campaign  data  algorithms and results  in
proc  int  conf  independent compon  anal  signal
separation  ica           cd rom 
    shastri  l  chang  s  and greenberg  s  
syllable detection and segmentation using
temporal flow model neural networks   proc 
xivth int  cong  phon  sci   pp                 
    ying  d w   w  gao  and w q  wang  a new
approach to segment and detect syllables from
high speed speech  in eurospeech              
    mayer  rudolf  robert neumayer  and andreas
rauber  rhyme and style features for musical
genre classification by song lyrics  proceedings
of the  th international conference for music
information retrieval  ismir       
    hirjee  hussein and daniel g  brown 
automatic detection of internal and imperfect
rhymes in rap lyrics  proceedings of the   th
international society for music information
retrieval conference  ismir       

   appendix

fi