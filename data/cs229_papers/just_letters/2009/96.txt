cs     project
opponent modelling in heads up nl holdem
jon harris
vianney hocquet
pierre thomas
december         

introduction
 heads up no limit holdem  refers to the most successful poker game  with only two players
and no limit in the bets  without limit  computer bots are not able to beat a good human player 
we focused our work on the opponent modelling  which is a promising field to improve the results
of poker bots  we had on average    hands per player  our algorithms must be able to identify
quickly our opponent strategy and predict from this inference what the opponent will play  this
suggests that we train generic specific models that we will use when we identify the style of play
of our opponent  so we must work on two different parts  classification algorithms and generic
predictive algorithms  among the many ways to deal with the second part of this problem are
bayesian algorithms and neural networks  we used a database of hands played for real money on
various websites ad at various limits 

  probabilistic model
in this approach action probabilities are calculated based on the previous two actions in the hand
as well as the current pot odds  the size of the pot of bets already made relative to the required bet
size   a naive bayes type of assumption of the independence of the features is used  we use the
following notation  a action  h hand history  po pot odds r raise  or bet   c call  ch check 
f fold
we first calculate the conditional probability of a particular hand history h given action a based
on the raw counts of these events in the data 
n  h a 
n  a 
our model for the action probabilities based on the pot odds is basically a two step logit model
and has form 
 
p  r po   
    exp r   r   po 
 
p  c po   
     p  r po  
    exp c   c   po 
p  f po       p  r po   p  c po 
p  h a   

with the above two models we can now calculate the probability of action a given h and po as

p  h a  p  a po 
i p  h ai   p  ai  po 
at any time the predicted action is the action with the maximum probability 
p  a h  po     

 

fifigure    left the figure shows how the different action probabilities depend on po 
right  this figure shows how the  error   actually the percent of correctly predicted actions  depends
on the size of the training set used 

  identifying strategies
in poker a first distinction is whether a player is loose or tight  does she he play a lot of hands
or not   then  depending on her his play after the flop  is she he aggressive or weak  one can
imagine more classes but for the time being we will focus on the way to differentiate between
those  we chose to use as a training set     of the hands available  from the start we faced some
problems  most classification algorithms necessitate that the training examples are labeled  but
even if we went through all the data  by hand  we could not certify that one player belongs to
this specific type for sure  so we tried k means clustering  used package stats  clust and fpc in r 
with   clusters hoping that it would result in the above mentioned classification  but in heads up 
because of the blinds paid at each game  players have to be much looser  so the numbers significant
for a game of   players or more dont have any meaning in this case  so we used the classification
supplied by clustering  we used the kmeans package 
won   when seeing flop
betting frequency at each point  preflop and post flop 
aggressiveness
but this classification is based on statistics which need quite a few hands played to converge 
when we will play they wont be available or at least significant  but a betting pattern will often
tell us more consequently we used other algorithms such as lda  linear discriminant analysis 
library mass in r  or k nearest neighbors  library class  or svm  library e      to attribute to
each player his type of play while playing  the rates for the prediction were quite good  our error
function was built as such   we compared the labeling given to the training set after    and   
hands played with the labeling given   hands before the end of the game to avoid that the statistics
lda   
are skewed by excessive risk taking  and we got those results   knn   
svm   

  tailored predictions
we used the classification to train four different svm on four training sets corresponding to the
different types of players  for each svm  the type of inputs were the same  we used a combination
of those classifiers on the basis that ensemble prediction will yield better results  which in our case
 

fiwas marginally true  then we used those classification not as a direct input for our svm prediction
algorithm but to choose the one which correspond to the type of player identified  that yielded
the best results among the three techniques used  however  we trained the svm on much larger
datasets than the other algorithms  so the comparison is not entirely valid  we used the package
error rate svm average player    
svm in r 
error rate svm tailored
   
 with features used  

figure    clustering results

  neural network
neural networks are a common tool in machine learning  and have already been applied to
opponent modelling  see       we try to prove again the efficiency of neural networks in opponent
modelling  to study the important inputs and parameters  and to adapt the opponent classifiaction
that we made  however  we used r packages to compute neral networks and did not sepdn time
on arificial neural network theory 

    neural network modelisation
we used r and the package neural to train  test and setup the parameters of our neural networks 
the neural networks we used are mlp neural networks  which are setuped by a back propagation
algorithm  the initialisation of the nodes is made randomly  the outputs in our situation were the
three boolean values   did the player fold   did he raise bet   did he call check   we chose to limit
our study to very simple neural network  made of one hidden layer  this choice was motivated by
      therefore  we had to optimise our results with respect to the number of neurons  the number
of iterations to setup the neural network  the choice of the features  and the size of the training set 

    parameters of the neural network
we tried to optmise over the number of neurons  nbneurons   the number of iterations to setup
the neural network  it   and the size of the training set  trainsize   after some blind trials  we
decided to take as standard values nbneurons      it        and trainsize        we trained our
neural networks and then found the error on a testing set for several values of these parameters
 see figure      

 

fifigure    parameters selection in the neural network
we can conclude that the best parameters are nbneurons      it        and trainsize       
as expected  the more iteration and the bigger training set  the better results we get  but we need
to choose reasonable values to limit the computation time  error with the number of neurons is
harder to interpret 

  action prediction
    action prediction using standard features
neural networks output the conditional probability of the outputs given the inputs  so  the ouputs
of a neural network are the same as the quantity computed by our bayesian algorithm  our main
concern in action prediction was to choose the best features to predict the opponents next move 
from the experience we had from previous experiments  we tried the following features     action
player    immed  pot odds    stage flop    stage turn    stage river    last act  bet raise   flush poss 
  a on board   k on board     akq on board   cards      previous p  action player    p action    p p
action player    previous previous action    pot total before action     calls bets 
with these features  the mean error of the algorithm was       we compared it to the error the
algorithm made if one of the features was taken off  from figure    we can see that the most usefull
parameter is the pot total before action  it is always interesting to know that most of this value
carry a lot of informations on the game  the rest of the parameters contribute to approximatively
  percent of the error and are equivament to our neural network 

    action prediction using enhanced features
finally  we used new indicators to try to improve our results  the training of a neural network is
long enough to discourage us from using four differents networks for the four categories of player
as done in part tailored predictions  in addition  results were not much better with this technic 
however  we added four features to our neural network   those which had been developped
to enhance the strategy identification process     the pre flop odds    a picture of hands played    the
pre flop aggression    the post flop aggression  the error of the neural network with these four new
features was         which is a significant improvment  more precisely  we can see in the following
tabular that the pre flop odds and the post flop aggression are the key parameters to reduce
error  the tabular presents what the errors would be if we added three instead of four features 
feature deleted
 
 
 
 
new error
                       

 

fifigure    using the neural network without one of the feature 

conclusion
the standard bayesian approach is part of a very interesting framework and could be improved
by mixing two distributions of action   one trained on a set of hands played by a lot of players 
the other one trained on a set of hands played by a lot of players        however  the algorithm is
complicated to adapt to every situation and does not predict actions as well as the neural networks 
another way to approach opponent modeling is to classify the other player  we have seen that
these results can be used to improve action prediction using neural networks or standard machine
learning algorithm  but the main direction for our work and challenge for potential improvements
is how to adapt the algorithms for a real time utilisation as well as how to handle the vast amount
of data that we have  more than     millions of hands   we would like to thank stanislas marion
and the website pokerai org for providing the data 

references
    m  ponsen   j  ramon  t  croonenborghs  k  driessens and k  tuyls  bayes relational learning
of opponent models from incomplete information in no limit poker  proceedings of the twentythird aaai conference on artificial intelligence      
    a  davidson  using artificial neural networks to model opponents in texas holdem  alberta
university      
    d  billings  a  davidson  j  schaeffer   d szafron  the challenge of poker  artificial intelligence
                  

 

fi