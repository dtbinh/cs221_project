chinese sentence tokenization using a word classifier
benjamin bercovitz
stanford university
cs   

berco cs stanford edu
abstract
in this paper  we explore a chinese sentence tokenizer built using
a word classifier  in contrast to the state of the art conditional
random field approaches  this one is simple to implement and easy
to train  the work is broken down into two pieces  the sentence
maximizer makes guesses over a large number of sentence
tokenization candidates and scores each one  the highest scored
sentence tokenization is returned by the algorithm  the word
classifier tells the sentence maximizer about whether a particular
character sequence is a word or not  the obvious word classifier
for words that were seen in the training set is a simple dictionary 
we have gone further by building a new word classifier out of
character  and word level features using logistic regression  the
performance of the new word classifier is promising  but the
sentence maximization objective will have to be constrained more
to get meaningful practical results by reducing the impact of false
positives 

   introduction
a first step in applying information retrieval techniques to a
corpus of text is tokenization  the process of breaking down
blocks of text in the form of documents into single words that
span the vocabulary of the search terms that users may wish to
query for  in many languages  tokenization is trivial or fairly
straightforward  for example in english  simply removing
punctuation and splitting by whitespace does a fairly good job 
some european languages benefit from a form of processing
called stemming that groups related words together into a single
index term  however  tokenizing chinese is hard because it is
written without whitespace between the words  typically  a
chinese word may contain from one to four characters  and
sometimes contains eight or more when transliterating foreign
words and names  human chinese readers  of course  can easily
distinguish the word boundaries anyways based on context  but
basic tokenization schemes do not comprehend these semantic
boundaries 
there are a variety of approaches that have been taken  with
everything from part of speech trees to conditional random fields
and markov chains involved  in contrast to implementing
something complicated with a rich theoretical model  the goal of
this work is to build a good performing tokenizer based on a
simple model that uses a binary word classifier 

   problem
given a chinese sentence  the ideal tokenization algorithm will be
able to produce the same tokenization as a human chinese reader 
formally  we want
      where
                    and              
to simplify our problem to dealing with character sequences only 
we will tokenize the sentence into phrases by punctuation before

starting  this is a trivial tokenization  searching for the set of
predefined unicode symbols that are defined as punctuation for
the chinese code page  thus evaluation will be done phrase by
phrase  even though we will loosely refer to these phrases as
sentences  the distinction is not very important in our context 
because sentences are just strings of words in our definition 

   models
    sentence likelihood maximization
the intuition behind this maximization objective is very simple 
true sentences are made up of words  each token has some
probability of being a word  the probability of being a sentence is
thus proportional to the product of the each token being a word 
we will first introduce a naive bayes assumption for sentences 
the sentence likelihood will not capture whether a sentence
makes sense  nor even if it is grammatically correct  in the
linguistic sense   rather  we will assume that a sentence is any
string of valid words  and that each word is independent  this
implies 
                

 
  

we already know the classification result  all the sentences we are
provided in the testing set are true sentences  we are looking for
the sets of words that gives a positive sentence classification 
     arg max


   
  

    word maximization as an estimate
the intuition behind word maximization is very similar to
sentence likelihood maximization  but it is reformulated as
features that are word based instead of sentence based  each word
is classified by the algorithm and if the length normalized sum of
classifications is maximized  then so will the likelihood that the
sentence is properly tokenized  under the naive bayes assumption
for sentences  this is proposed because it is not clear how to find
many features suitable for linear classifiers at the sentence level 
formally 
  





    arg max


  

each of the hypothesis functions is a binary classifier that takes
features that can be extracted at the word level  the length
 is simply the length of the word raised to  
normalizer 

without this factor  the maximization would prefer short words
because each hypothesis function outputs either   or    a neutral

fi a  word length distribution
 is    whereas      gives preference to longer words  which is
desirable for chinese   
training the classifier is nonobvious  since the corpus of training
data is all positive examples  we need to generate incorrect tokens
in the same fashion that incorrect tokens will be encountered
during classification  training all possible tokenizations of the
training set will involve a massive amount of data  we will return
to this topic in section   

   related work

 b    max word length in sentence distribution

recent work done in this highly specialized area is found in the
acl sighan workshops and the hlt naacl conference 
sighan has sponsored a number of  bake offs   or short
competitions in recent years to advance the state of the art for
asian language problems  the data for the      sighan bakeoff  which was a chinese tokenization competiion  was used for
this work    
in particular  one theoretical model has taken over the field 
finding its way into all of the top performing contestants  the
work is based on peng  feng  and mccallum      in which they
describe how to use conditional random fields for chinese
tokenization  in contrast to this work where sentences are
considered as a whole chunk  the work with conditional random
fields is stream based  working with just a few characters in a
buffer at a time  the test performance of some contestants
exceeded      which is more than we can possibly hope for with
this basic approach 
the theoretical underpinnings of using conditional random fields
are completely opposite of the stated strategy we have taken here 
instead of assuming independence between words  the conditional
random fields approach indeed relies on that dependence  in truth 
some of the features used in the word classifier actually depend on
character ordering  so it uses a similar idea 

   sentence maximizer
before worrying about how to classify candidate character
sequences as words or not words  it is necessary to verify that
doing so will be useful  to test the usefulness of the sentence
maximization technique  several experiments were run where the
word classifier had perfect forward knowledge of the words  the
classifier consisted of a dictionary of all the words in the training
set  and it returned   if the candidate sequence was in the
dictionary and   otherwise  thus it can be thought of as an ideal
classifier  so long as the dictionary is pre populated with the
words from the test set  this allowed the isolation of the sentence
maximization step and allowed comparison of the various
methods  we built three different sentence maximizers based on
the model described in section     

    brute force
initially  we thought that it would be possible to use a brute force
search to find the tokenization of words that has the highest score 
although a brute force list of permutations of word breaks would
be guaranteed to include the optimal solution  and indeed  every
other solution  providing a perfect test case for our scoring
function   it was impractical to use 

 

it turns out that using    will result in many ties where two
shorter words must be combined to make the correct word 

figure    word length distributions
initially it was expected that phrases would be at most around   
characters  but it was found that this was an unreasonable
assumption  so the exponential number of permutations to test was
wholly impossible on available hardware  it was necessary to
innovate a method for generating possibilities that would include
the optimal solution without exponentially many non optimal
solutions 
the word length distributions were analyzed  as seen in figure   
this information proved very useful  as it confirmed that words
longer than   characters were exceedingly rare  and     of the
sentences did not contain any word longer than     the first pass
at using this information was to improve the brute force search by
adding filtering on any partition that would leave a subsequence
of more than   characters  unfortunately  filtering alone this did
not make the problem tractable       sequences take a very long
time to enumerate  and it seems that even with the word length
distribution the permutations list will be exponential  but the
insight of the distribution helped drive the next attempt 

    random draws from pdf
armed with the discrete probability distribution function for the
word length  the next try at enumerating possible sentences was to
repeatedly draw word lengths from the distribution and use them
to form the word breaks  with        draws  it was found that the
real sentence was included in the draws over     of the time
 averaged over the whole training set   shorter sentences needed
fewer draws  so a reasonable improvement was to use     times
length as the number of draws  however      candidate error on
top of significant baseline scoring error obtained from limited
experiments with the brute force method  of about     was still
unsatisfactory 

fi a  pseudocode for dynamic algorithm

 b  training error versus k for dynamic algorithm

table    list of features in new word classifier

figure    dynamic optimizer
figure    character positions matrix for  abcd 

    dynamic programming algorithm
the short maximum word length created an opportunity for using
a dynamic programming algorithm  since the max word length is
limited  intuitively finding the best first word among the first  
characters would be just as good as looking for it among the first
    all permutations of the spaces within the   characters are
made  once the first word is found  it can be dropped from the
remaining characters array and progress has been made 
repeating this process gives a solution in o n     k  time  where
k is a small constant  the max number of characters to consider  
the algorithm is presented as figure   a  
the solution is not necessarily the same as the correct one  but in
practice it frequently is  as is seen in figure   b   this algorithm 
using the ideal classifier and a length biased scoring function 
approached only     scoring error   candidate error  they cannot
be separated because they are intertwined in the dynamic
programming approach   which was comparable to the brute force
method  this implies that most of the remaining problem is
scoring error  for which we need to evolve the maximization
objective in section     to further improve performance 

    scoring error
reducing the scoring error toward zero presented additional
challenges  since the focus of the project was intended to be on
machine learning techniques and the most promising
improvements here did not involve those techniques  they were
not explored to a great extent  it seemed to be a battle of
diminishing returns because of two facts  first  the dynamic
programming method did very well  besides from getting nearly
    of the sentences perfect  it got overall about     of the
actual word breaks correct  an analysis of the    which were
mistakes showed that in a number of these cases  there was an
ambiguity in the dataset  it can be argued that the computed
answer was also valid under the tokenization rules   indeed  for
the intended applications  ir like systems that required
tokenization  the mistakes were often harmless  an equivalent in

english might be can not versus cannot   instead of spending
much time working out how to squeeze away the last few bad
mistakes  a baseline of performance was established and efforts
moved on to the evaluation of different word classifiers beyond
the simple dictionary 

    basic word classifier performace
using a dictionary alone did quite well  on    fold cross
validation  using just the dictionary yielded     accuracy  out of
an ideal of      at the whole sentence level  looking at
individual word breaks  it got     correct  compared to a    
ideal  thus  the focus of the machine learned word classifiers
studied here is to bring that closer to the ideal 

   new word classifier
once the sentence maximizer strategy was verified  we focused
efforts on building a binary classifier for words using logistic
regression  the parameters were optimized using parallel batch
gradient descent  we tried a stochastic gradient descent approach 
but it found it hard to parallelize and so it ran slower even though
it came close to convergence in fewer iterations  we noticed that
it was more sensitive to local optima as well 
the success of the basic dictionary classifier meant that the main
utility of a learned word classifier would be for identifying new
words  since known words could simply be read from the
dictionary 
we used a test set of        sentences  a statistics training set of
       sentences  and a classifier training set of       sentences 
instead of incorporating the dictionary into the learned classifier 
it was found that performance was improved dramatically by
keeping it as a separate  perfect precision classifier that would
defer to the learned new word classifier when it did not contain
the candidate word 

fi a  avg  jaccard similarity with actual sentence

 a  all features

 b  without g features
 b  training testing error  word candidates 

figure    performance optimization using training bias
figure    feature selection

    features
several classes of features were chosen to aid in the identification
of new words  because the gap between training and test error
remained reasonable  we realized we were underfitting the data
and spent a lot of time trying to develop new features  the list of
features that were actually closely evaluated in the end are listed
in table   
one important feature is the character positions matrix  which is
also show in figure    in the statistical training step  we collect
character statistics about how often characters start a word  appear
in the middle of a word  and end a word  it proves indeed unlikely
that a two character combination that starts with an  ender  and
ends with a  starter  would be a word 
other features we tried included individual character frequency 
word length  and absolute and relative positioning within the
sentence  in addition  two domain specific features for handling
written numbers and interspersed ascii words were evaluated 

    feature selection
we performed an evaluation on the different feature sets using
various metrics  the two metrics we found most helpful were
test training error over word candidates and a jaccard similarity of
the chosen words in the sentence between the sentence guessed by
the sentence maximizer and the real sentence  this similarity can
be thought of as a percentage of the word gaps that were guessed
correctly at the sentence level using the information from the
word classifiers 
the results of the feature selection are shown in figure   

generally  adding features helped performance  the feature sets
that seemed to boost performance the most were a  b  and d 

    optimizing performance
we found that this particular sentence scoring is very sensitive to
false positives  as we will discuss in the next section  even though
the classifier did fairly well when it came to distinguishing words
from candidate character combinations  disappointingly  it did not
translate into improved performance at the sentence level because
every additional word that it got right was counterbalanced by a
non word that it got wrong and caused the sentence optimizer to
choose it 
in hopes of obtaining at least some benefit from the new word
classifier  we biased the training set by including many times
more negative examples  ideally  the new learned parameters
would have a lower false positive rate  at the expense of a higher
false negative rate 
the results of this attempt are shown in figure    figure   a 
shows the bizarre results when all the features were used  we
attributed this to an overfitting caused by some of our sentence
position features  g   once the g features were removed  the
expected trade off was observed  as seen in figure   b  
ultimately  we could not get the learned new word classifier to
improve performance on sentences over just using the dictionary
as a classifier 

fi   discussion
    issues with false positives
the word classifier actually worked well  as we can see from the
relatively low test and training error  however  the sentence
optimizer was underconstrained and thus very sensitive to false
positives  while the dynamic programming approach clearly is
obviously very sensitive because it assumes an optimal
substructure  the problem actually lies with the maximization
equation  it is underconstrained because nearly any false positive
will result in an incorrect sentence  this was not detected in the
early stages because the basic dictionary classifier had no false
positives  thus it can be thought of as having good training error
and poor generalization error 
conceivably two approaches could improve this  one would be a
learned sentence model that follows common word length
patterns  the other is adding constraints that exclude more
character combinations  for instance  some simple grammar rules
might substantially reduce the number of false positives 

    conclusion
it is possible to build a fairly accurate word classifier using
logistic regression  in combination with a dictionary  it performs
even better 

the sentence optimizer also did well at turning word
classifications into accurate sentence scores  but it had a dismal
tolerance for false positives  which made using the learned word
classifier difficult  adding constraints or changing the objective
will likely allow the classifier to actually help without hurting too
much 

   acknowledgments
thanks to all the course staff and prof  ng for your help and
encouragement 

   references
    r  sproat and t  emerson  the first international chiense
word segmentation bakeoff  proc  of second sighan
workshop on chinese language processing 
    f  peng  f  feng and a  mccallum  chinese segmentation
and new word detection using conditional random fields 
international conference on computational linguistics 
     

fi