classification of amazon reviews
mayank agarwal  maneesh bhand
december         

 

introduction

product reviews are a valuable source of information when it comes to making online purchases 
however  some reviews are more helpful than others  the goal of this project is to predict the
usefulness of a product review  our corpus is a set of online reviews taken from amazon com
 http   www amazon com   given the title of the review  the authors name and location  the
authors rating of the product on a scale of        and the text of the review  we want to predict
the percentage of users who find the review helpful 

 

problem statement

this problem is a supervised learning classification problem  we are given an input vector x  rn
which can consist of various things  such as the summary text  review text  etc  and our output
vector is the percentage of people who find the review helpful  y          thus  the goal is to learn
the mapping h   rn        
there are two ways of approaching this problem  one way is to treat it as a classification problem by discretizing the percentage we wish to predict into bins  this allows us to use classification
algorithms like naive bayes and svm  the other strategy is to treat the percentage as a continuous value and use regression algorithms to predict its value  we explored both strategies and
implemented various algorithms in order to find a good model for the data set 

 
   

methods and results
multinomial model

the first model we implemented was the multinomial event model  as described in the lecture notes
     if we divide the interval        in k bins  then the rth interval corresponds to
n r 
ro
ir   y  
y
k
k
the parameters of our model are r  y    prob y  ir   and i yir   prob xj   i y  ir    the
goal is to choose these parameters so as to maximize the log likelihood estimates of the data 
we applied laplace smoothing to the parameters so that the final expression is
pm
  y  i   ir  
r  y    i  
   
m

 

fi i 
 i   i  
r
j     xj   k  y
pm
 i 
 ir  ni    v  
i     y

pm pni
k yir  

i  

 

   

here  is the laplace coefficient which we choose as   for our first run   v   denotes the length
of our dictionary which was constructed by accruding all the distinct words in the training reviews 
m denotes the total number of reviews  and ni denotes the length of the text of the review  we
divided the corpus into roughly     training data and     test data  we removed all the reviews
where the number of people who voted whether they found the review helpful was less than    
the results are tabulated below 
number of bins
 
 
 

train error  
     
      
     

test error  
     
     
     

next  we removed all the low frequency words and extreme high frequency words such as of
and the  since we believed these words to be content free and not indicative of the quality of
the review  originally  we had        words in our dictionary  which was truncated to        
unfortunately  however  as the results below depict  the error actually worsened 
number of bins
 
 
 

   

train error  
     
     
     

test error  
     
     
     

stemming and removal of stop words

we realized that removing high frequency words is not necessarily the right thing to do because
it removes words such as good and worst which are very relevant to our classification  so we
got a list of stop words from the course taught by prof  chris manning and removed only those
words  this list of stop words had around     words and consisted of content free words such as
and  of etc  next  we applied a standard stemming algorithm  due to porter     to our dataset 
the length of the dictionary after the above two steps reduced to about       words  we achieved
significant improvement in the results  the table below summarizes the result achieved 
number of bins
 
 
 

   

train error  
     
     
     

test error  
     
     
     

changing the smoothing parameter

we tried changing the smoothing parameter in the laplace smoothing equation     above  however 
it did not make a noticable difference to the output error  by decreasing the laplace coefficient 
the training error increased while the testing error decreased 

 

fi   

bigrams and trigrams

next  we included all the bigrams and trigrams of the words in our dataset to our model  the size
of our dictionary increased to about   million words  we then applied all the previous methods
on this extended model and multinomial naive bayes stood out  the results below indicate that
the train error was close to    which means that the model was able to learn the training set
with remarkable accuracy  the impact on testing error was significant but not as dramatic as the
training error 
number of bins
 
 
 

train error  
    
    
    

test error  
     
     
     

the very low training error indicates that some overfitting may have occurred  a sample of
n grams the model found most indicative of a helpful or unhelpful review is below  looking at the
n grams which were most indicative of helpfulness  some phrases  like arrived in and shipment 
make sense as useful features  as some customers complain about shipping problems they experienced instead of reviewing the product   other phrases  like man of babylon and benedict
xvi  seem to be specific to certain products or reviews and may not generalize well  nevertheless 
perhaps because the corpus is so large  all these review specific features  when aggregated  produce
a model that does generalize to other  similar review data 
indicative of helpfulness
benedict xvi
peter schiff
liesels
a lifestyle
hard things
days of
perfume
rate of

   

indicative of unhelpfulness
man of babylon
money of this
her kids
condition
with oprah
zombies
ready the
in great condition

looking at other features

there is other information contained in the reviews besides the words in the text  the length of
the review and usage of capitalization punctuation can also be used as features in predicting the
usefulness of a review  longer reviews might be more likely to be helpful  for example  while
reviews written entirely in upper case might not be as helpful 
we used a set of features that included the word count  average word length  count of words
beginning with a capital letter  count of words entirely in uppercase  and counts of various punctuation marks 
in order to use these features in the context of a multivariate bernoulli model  the feature
vectors  which contained continuous values  were discretized by assignment into bins  then  each
feature in the vector was treated as a word in the review  and the modeled probability of a review
was adjusted accordingly  the results are below 

 

finumber of bins
 
 
 

train error  
     
     
     

test error  
     
     
     

although some of the features selected  such as review length  did provide information on the
distributions of reviews  the performance of the classifier did not improve when given these extra
features  training error decreased  while testing error increased  indicating that the inclusion of
extra features led to overfitting 
one possible explanation for the lack of improvement is the discretization procedure  if values
are assigned to a small number of bins  a lot of detail is lost  whereas if values are assigned to a large
number of bins  the bins are sparsely populated and not representative of the true distribution 
another explanation is that simply treating the features as additional words is not an effective
way of integrating them into a naive bayes model  a different approach to combining feature
probabilities with word probabilities is needed 

   

logistic regression

in order to explore predicting the percentage as a continuous value  we implemented a logistic
regression model based on just the non word features  the predicted values were placed into bins
and evaluated  so as to make the results comparable to those of the discrete classifiers 
number of bins
 
 
 

train error  
     
     
     

test error  
     
     
     

from the results  it is clear that the non word features are not nearly as effective as n grams
of the review text in predicting usefulness  this is a somewhat surprising result  as one might
expect features such as capitalization and punctuation to impact the legibility of a review and 
consequently  its perceived usefulness 

   

locally weighted linear regression

we also attempted locally weighted linear regression      however  in a large corpus such as this one 
the non parametric nature of the algorithm makes computation prohibitively expensive  therefore 
we used k  means clustering on the training data in order to produce a smaller  representative set
of examples  and then performed locally weighted linear regression on these clusters  in order to
obtain a reasonable measure of the distance between two feature vectors  we mapped each feature
to a normal distribution and measured the distance between feature z values 
however  even at low bandwidths  predicted values almost always fell into the        range 
which is close to the average helpfulness of a review  this suggests that the non word features
alone were not sufficient for describing the helpfulness of a review  or that the clustering process
removed too much information from the training data 

   

other methods

we tried various other classification methods on our dataset as well  we used the weka software
package to try svm  max ent  and the k star clustering algorithm  we also implemented the
 

ficomplementary naive bayes model  as described in      however  none of these classifiers produced
significant improvements in the classifier accuracy  the multinomial naive bayes model  using ngrams of the review text  remains  by far  the most simple and accurate model for our problem 

 

conclusions

various algorithms as described above were tried on the dataset  it is surprising that the multinomial model performed the best among the lot of models that we tried  the performance of the
naive bayes model improved by applying porters stemming algorithm  removing stop words  and
including bigrams and trigrams of the review text  we looked at some non word features  and found
that  while a few features  such as review length  did correlate with review quality  most non word
features were not effective in predicting review usefulness  another observation is that the classifier
is relatively good at identifying the very helpful and very unhelpful reviews  the confusion matrix
indicates that the majority of the error is in the middle   with the classifier sometimes mistakenly
predicting a neighboring bin 
the problem of predicting review usefulness is somewhat different from more standard text
classification problems like spam filtering  because the helpfulness of a review is a very subjective
quantity  conventional models such as the ones described in this paper were unable to capture all
of the variance in the data  for example  in the case of two bins  it is very difficult to get the
classifier accuracy more than     without overfitting the model 
overall  the model is reasonably able to predict the usefulness of a review  this work is
useful because it can be directly applied in a variety of ways  for example  online markets like
www amazon com can use it to automatically serve better reviews to its customers  or to flag
potentially unhelpful reviews for moderation 

 

acknowledgements

this project is the idea of prof  chris potts  we are grateful to him for providing us with the
corpus and some code to process it 

 

references

    c  j  van rijsbergen  s e  robertson and m f  porter        new models in probabilistic
information retrieval  london  british library
    tackling the poor assumptions of naive bayes text classifiers  jason rennie et al  proceedings
of the twentieth international conference on machine learning  icml        washington dc 
    
    andrew ng  cs     lecture notes  supervised learning  discriminative algorithms

 

fi