bot detection via mouse mapping
evan winslow
stanford university
stanford  ca
ewinslow cs stanford edu
abstract
bot detection has become a key need on the internet for a variety of reasons  take as one motivating example online registrations  if bots are not prevented from registering automatically 
then spam invariably becomes a serious problem  also consider polling data  if bots are allowed
to participate in polls  then poll data can be easily and dramatically skewed by bot participation 
the first step to preventing their participation is detecting their presence  in this paper  i present
a method for detecting bots called mouse mapping  which relies on ui based features to distinguish between human and bot activity 
introduction
in this paper i present a technique called mouse mapping  which relies on user interface based
features of web activity to distinguish between bot and human traffic  some examples of ui features that could be extracted are clicks  mouse movements  typing  and screen size  i derived the
name mouse mapping from a related practice called click mapping  in which a website administrator records the timing and location of clicks on various pages  the difference between that
and mouse mapping is that i have intended mouse mapping to be broader  mouse mapping
records all mousemove events as well as clicks 
motivation
many current defenses against spam rely on detecting the presence of spam itself  however  if
the creators of the spam can be detected before the spam is ever created  then the would be spam
becomes a moot issue  this is part of the usefulness of captcha  for example  captcha
aims to prevent spam from ever being created by presenting the machine with a problem it cannot solve  but humans can  however  the weakness of captcha is that it adds a slight barrier
to the genuine user  i propose mouse mapping a complementary method to captcha for detecting unusual activity on the web  mouse mapping has the same advantage of preventing spam
before it happens  but the advantage that mouse mapping has over captcha is that the presence of a mouse mapping defense is invisible to the genuine user  that is  a mouse mapping defense can be in place on a website  but the everyday user would never know it because data collection happens behind the scenes  with no appreciable effect on web application responsiveness
and therefore no effect on user experience 
problem description
the goal of mouse mapping is to determine whether a user is behaving correctly  in this case a
user can be one of two classes  either bot or human  if the user is behaving correctly  then they
would be allowed to continue using the website as normal  but in the event they are not behaving
correctly  some action may be taken  however  we run into the problem of defining what
correct behavior is for users on a website  we already have an intuition that human like
should be more correct than bot like  but then what is human like ui activity  this is where machine learning techniques come in  instead of setting arbitrary thresholds and requirements for

fiui activity  we allow real users to determine for us what correct behavior looks like  and then
build a classifier based on that input data 
features
there are several aspects of gui interaction from which features can be extracted to distinguish
between bots and humans  these include but are not limited to  mouse clicks  position and frequency   mouse movements  time spent on a page  screen resolution  and keystroke activity 
since clicking is commonly tracked already  i decided to focus mainly on extracting mouse
movement features  initially i started with a proliferation of features  too many to keep track of
and too many to be useful  many of the features were obviously linearly dependent or deterministic  not varying significantly at all even among human users  so i dropped a significant number of features before settling on the final seven  the final set of features i used was 
   the number of move segments generated on a page  a move segment is the line between
two successive records of a mousemove event generated by the browser which occur less
than     milliseconds apart  this seemed useful to try because computers  assuming they
used the mouse at all  would most likely take the straightest and shortest path and therefore have the least number of segments per page view on average 
   the number of distinct mouse motions  a motion is defined as any string of successive
segments that had no more than     milliseconds between points  this essentially captures the number of times the mouse is rested while the user was viewing the page 
   the average length of these mouse motions  this is a measurement of how far the users
mouse travels before resting 
   the average time of these mouse motions  this is a measurement of how long the users
mousemoves consistently in between rests of     milliseconds or more 
   the average speed of mouse movement  one of the primary reasons bots are used is because they are so much faster than humans at accomplishing repetitive tasks  speed is
therefore likely to be a distinguishing factor between humans and bots 
   the variance in speed of mouse movement  humans are not completely consistent like
computers are  therefore we would expect there to be some variance in human activity
while there is little to none in computer activity 
   the variance in acceleration of mouse movement  supposing a bot incorporated variant
speed into its activity  it may still not be able to mimic or predict the variant acceleration
that humans are sure to express 
experiment setup
to make the necessary measurements  i leveraged courseware   a social network and course
management website  courseware provided an interesting variety of genuine user data  since it
has many different kinds of web environments such as a calendar  forum  faq  dashboard  wiki 
and more  these web environments provided many varieties of mouse maps that i tested
against  courseware also has a constant stream of genuine users  so supply of genuine user data
was not a problem 
in order to gather the necessary features from courseware users  i extended an open source
project called clickheat  which only records user clicks to record mouse positions and keystrokes as well  every time a users browser fires a mousedown  mousemove  keydown  or keyup
event  javascript records a timestamp of the event  as well as the x  y coordinates  and other event

fimetadata where applicable  e g   which button was being pressed at the time of the event   the
users browser  operating system  and screen resolution were also recorded  but only once per
page load  in order to prevent damage to the user experience  the javascript performing the recording accumulates the data in browser memory instead of sending the data to the server as
soon as possible  it does not send its current bundle of data to the server until it accumulates data
for      events  the user clicks one of their mouse buttons  or the user leaves the page  i also
adapted the backend of clickheat to store data in a database  rather than in files  so that the analysis of the data would be much easier  by the end of the project  courseware users had generated over    million data points from which to extract and analyze features 
visualization
in order to confirm my intuitions about the distinctness of human activity on a website  it was
helpful to have a visual representation of the data
early on  figure   to the right depicts a map of users
clicks for a particular web page on courseware  it is
immediately obvious that there are certain hotspots
on the page which are clicked more often than other
spots on the page  links are the main attractions
causing these clicks to be grouped as they are  however  clicks are not evenly distributed on a link 
meaning that mimicking human activity is not as
trivial as evenly distributing click activity across the figure    an example map of human click patterns
visual space of a link or other clickable entity  i had on a web page
aimed to do the same kind of visualization for mouse movements and keystroke events  but time
constraints prevented me from accomplishing that goal 
model
i decided to use a nave bayes model for the classifier  however  since i only gathered data for
one of the two relevant classes  i had to use an unsupervised version of nave bayes  the algorithm constructed based its classifications solely on the distribution of observed features generated by humans  and the goal became to detect anomalies in the distribution  the anomalies would
be considered bot activity  each feature in a test observation was compared to a discretized distribution of features in the training set  based on which of the     buckets the measured feature
fell into  the feature was assigned a probability  this was repeated for all features in the test observation to form a vector of probabilities corresponding to each of the features in the test observation  all of the values in this resulting vector were multiplied together to give an overall probability of the observation  if this overall probability was below a certain threshold  the observation would be marked as bot generated activity  otherwise it would pass and be considered
human activity 
results of classifier
in order to test my classifier  i was going to use a few black box security scanners that the cs
security lab had access to  however  upon inspection it turned out that these scanners dont
generate mouse events  this means that it is trivial to detect their activity  in another attempt to
test the classifier i tried using a macro generator for the chrome browser  but this suffered from
the same kinds of problems as the scanners  it generated events that were impossible to create

fithrough normal use by human users  e g  click events had no x or y coordinates  and was therefore trivial to detect 
conclusions
this research has been a promising first step in the direction of detecting bot and abnormal behaviors by using gui features  my research demonstrated that current bots and scanners are not
prepared to cope with this detection mechanism  on the one hand  this is good news because it
can give those on the defense the upper hand  on the other hand  this makes the effectiveness of
the classifier hard to assess  because there is so much variance in human gui activity  it is not
clear how difficult it would be to create a bot that could beat the classifier  in sum  i have only
scratched the surface of the problem  and it seems clear that more in depth research and rigorous
analysis to determine the robustness of this approach will be needed before mouse mapping can
become an effective tool in real world defense 
acknowledgements
specials thanks go to elie burzstein for the initial inspiration  constant support  and helpful advice throughout this project 
 
 

https   courseware stanford edu 
http   www labsmedia com clickheat index html

fi