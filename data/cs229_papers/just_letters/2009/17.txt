identifying hand configurations with low resolution depth sensor data
sameer shariff and ashish kulkarni
december         

motivation
in recent years  there have been many advances in the technology used to interact with computers  video
game consoles  etc  including touch screens and input through use of accelerometers  nintendo wii   an
natural extension to this trend is to enable human computer interaction though simple hand gestures and
body movements  we set out to tackle a sub problem of this task  detecting and classifying different hand
configurations from low resolution depth sensor data  naturally  with a good a hand configuration
classifier  sequences of these configurations can be detected to identify full hand gestures  applications of
such a technology are widespread  including automatic sign language interpretation and computer and
video game interaction 

data source
as our source of data  we re using the swissranger       the  th generation of series of time of flight
cameras put out by mesa imaging  the camera uses infared light and measures phase differences in order
to get an accurate measure of depth  its output is a stream of frames  where each frames consists of
a   x    pixel image  and each pixel has an associated intensity and depth 

problem
the problem we set out to solve was to detect and classifier different hand configurations from still frames
of this data source  to simplify the problem  we focused on classifying two hand poses     opened  and
 closed  hands    but we allowed for a lot of variations in terms of the distance from the sensor  the
orientation  and the angle from which we viewed the hand  the goal was to build a relatively robust
classifier to differentiate between opened and closed hands in a variety of environments 

the end to end system
in the end to end system  we envision there being three major components 
   localize the hand  or hands  in the image  identifying the  sub image  containing the hand
   recognize the configuration of the hand in the sub image
   use sequences of hand configurations over time to recognize and classify full hand gestures
as mentioned above  for this project  we primarily focused on     manually labeling hand locations to feed
into the recognition algorithm 

data set
our data set consisted of     still frames taken from our sensor  with the hand appearing at varying
distances and orientations  all images contained a hand in either a closed or opened configuration 

data pre processing
in a pre processing step  we chose the region of interest from the full image based on a location which has
manually input and a window size which was chosen based on the depth in the region around the hand 
during this stage  we also labeled the hand configurations  all the data outside the region of interest was
discarded  and all the data within the region of interest  including the intensity and depth at each pixel  was
passed onto the next stage 

fifeature extraction
there were several types of features we developed  a few of which are described below 

 d shape based features
these features typically used the depth information primarily to do segmentation  and then looked at the
resulting contours in  d space  in a few cases  we computed a mask binary image indicating which pixels
fell within a certain bounding box of in the z coordinate of the hand  and then used this mask image to
derive features  two of our more valuable features that came out of this process are described in more
below 

size based features
one advantage of using a depth sensor is that we know an absolute measure of the scale of objects  since
distances are no longer ambiguous  as they would be for a regular image   one idea was to use this as
feature for classification  leveraging properties such as an open hand having more surface area exposed
than a closed hand  although these size based features did add some value to our model  we found that in
many cases  if the arm leading up to the hand was at a similar depth to the rest of the hand  this would get
included in our segmentation and would shift this surface area measurement substantially  from looking
through some of our failure cases  we found that these size based features were often too sensitive to what
exactly was included in our region of interest 

frequency based features
another observation we had is that an open hand  with fingers spread  has much higher frequency content
than a closed hand  we developed a few different features to capture this effect  first  we computed the  d
discrete fourier transforms of our images and used the fourier transform weights as features  on their
own  these features did add signal towards classifying open vs  closed hands  but we found that adding
these features in addition to the  d shape based features described above added only a marginal gain  from
this we concluded that our  d shaped based features were  in effect  capturing some of this frequency
content indirectly 

specific shape features
the starting point for feature extraction for these features was the images depicted in sections  d  and  e  of
figure   below  after segmenting based on depth info and removing some of the noise in the image  some
of our priorities in our search for features that were scale and rotation invariant  and also resistant to small
changes in the location and shape of the hand  we suspected that this would make it easier to properly
detect open and closed hand configurations in different orientations  below we describe two major features
we have used so far which meet the above criteria 

centroid distance histogram
we first computed the centroid of the white pixels  and then collected the distance between the centroid and
each of the edge pixel in the image  to make this process scale invariant  the distance was normalized by
diving each distance with the max distance  these distances were then bucketed and used as input features 
note that this process is invariant to scale and rotational changes 
edge pixel max distance histogram
similarly  this feature calculated the distance of an edge pixel with each other edge pixel in the image  and
took the max across all these distances  this information captures some aspects of the shape of the hand 
the resulting information about each pixel was again normalized using the max distance  and bucketed as
our feature values  similarly  this feature is also scale and rotationally invariant 

filearning algorithms tested
although we tried a few different learning algorithms  we seemed to be getting the best results with svms
so we focused our energy primarily on them  we trained our svm using a linear kernel 

figure     mask images

 a  original image

 b  after using
depth information  c  after running
density mask

 d  remove small
blobs

 e  after canny edge
detection

figure     histograms

feature  

feature  

evaluation
to evaluate each of our models  we used k fold cross validation with k       we computed the test error
for each of these    trials  and then computed the overall test error and test error variance as the mean and
variance of these values  respectively  we chose our features and our parameters to maximize this overall
test error 

firesults
the table below summarizes our results  we ve included the test accuracy of our model as each different
feature type was added  that is  the test accuracy in the first row indicates that only discrete fourier
transform feature was used  the second row indicates that the first two features were used  etc  
feature added

test accuracy

discrete fourier transform

      

      

edge distance of each pixel from       
other pixels

      

distance of centroid from edge
pixels

      

      

measuring size of hand

      

      

variance of test accuracy

error analysis

figure   the graph on the right is a plot of
the training and the test error  as
the number of training examples is
increased  the green line indicates
the test error  and the blue line
indicates the training error  the
graph starts from plotting the
training error when    examples
were taken for training  and ends
when     examples were taken for
training  it can be seen that the
test error decreases as the number
of examples used for training
increases 
a few examples of images which we classified incorrectly are included below 
wrong mask detection

error due to wrong
depth estimation

noise outside the hand

figure  
features couldn t
differentiate

fiwe analyzed the data which we got incorrect  and classified the reason of error into four parts  the four
parts are
   wrong mask detection    this is the case when there is error in the way mask is calculated  as it can be
seen  in this image  the fingers got eliminated from the open hand during mask generation  and thus  this
image looks more like a closed hand to the svm classifier 
   error due to wrong depth estimation    this is the case when the sensor gave incorrect information about
the depth in a particular region  this lead to classification of the hand as the background  and the
background as the hand  as a result  the classifier was trying to classify incorrect data 
   noise outside hand    this type of image image might be resulting due to one of out strong features  one
of the features calculates the distance of each edge pixel from other edge pixels  the seems to be a lot of
noise below the hand  moreover  a part of the wrist is not detected  and thus giving it a weird shape below
the hand  this results in the wrong classification by one of the features  which might be making the
classifier have a wrong output 
   svm failing due to more than one of the above problems    this last error type is when the classification
fails due to multiple reasons  for example  this image has noise around it  after noise reduction  and there
are holes in the middle of the wrist  which might mislead the svm classifier 

conclusions and future work
our analysis above has shown that with a few carefully chosen features  we can derive a model of high
accuracy to predict between two different hand configurations  the majority of our features above used an
image representation as a starting point  after using depth data for segmentation  but several other features
can be derived from the  d representation of the data directly    and this is an area we d like to further
explore  there are also several other directions we d like to take this in terms of future work  first 
naturally we d like to extend this work to classify a larger number of hand configurations  additionally 
although we built some of our features into a real time demo in c    most of our features above are only
implemented in matlab  and we d like to move these features over so we can do more accurate
classification real time  finally  we d like to work more on the hand localization tasks and gesture
recognition in order to develop the full end to end real time system 

acknowledgements
we d like to acknowledge varun ganapathi and christian plagemann for their guidance and input
throughout this project  varun and christian provided the initial codebase through which we collected and
began to visualize data from the sensor  and on many occasions they provided valuable feedback and
helpful ideas on how to proceed  varun and christian have also provided several suggestions that we plan
to plan to pursue beyond this project in order to build a more reliable and robust model 

references
    andrew ng  support vector machines  cs    class notes       
    andrew ng  advice on applying machine learning  cs    class notes      
    canny  j           a computational approach to edge detection   ieee trans  pattern analysis and
machine intelligence            doi         tpami             

fi