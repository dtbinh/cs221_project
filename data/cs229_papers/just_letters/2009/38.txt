multi touch multi robot interface
david millman
millman cs stanford edu

higher levels of interactivity than a mouse  they
have a combined interface and display  and are
ideal for showing broad views of a scenario 

abstract
the overarching theme of this project is to
explore ways for a single user to control
multiple robots using a multi touch interface 
in particular  how does increasing the number
of robots change both the interaction methods
and the control methods  this project aims to
the answer the question of what types of
problems occur when controlling real robots 
and how do you solve them 

the multi touch multi robot project is an initial
attempt at creating an integrated multi robot
solution 
we expand on an existing early
prototype from stanford ai lab 

  system infrastructure

  introduction
much like cell phones have become an everyday
item in the last decade  robots are increasingly
coming into mainstream use  despite there being
no standard interface for controlling a single robot 
we are exploring the idea of how a single person
can control multiple robots 
imagine driving a car  your hands on the wheel 
now imagine driving two cars at the same time 
with two steering wheels  two gas pedals  and two
brakes  this is a difficult task  much like driving
two cars at the same time is tough  trying to drive
two robots would be just as hard  now what
happens if you have three  four  or ten robots  it
gets even worse if you consider that the robots
may each have their own arms  too 

figure    multi touch multi robot interface sample display

    previous work
the multi touch multi robot interface extends
work done previously by people at the stanford ai
lab  ricciardi         the prototype could accept
raw touch input and do simple processing on that
input  it provided the foundation for displaying a
map of the environment and the locations of the
robots  there was also support for having a robot
follow a user drawn path 

to remedy this single operator multiple robots
control problem  we must give the robots some
level of autonomy  once you decide that a robot
will have some self control  the next question is
how much  current thinking suggests that the
level of autonomy should be able to slide from
tele operated  precisely remote controlled  to
completely autonomous  e g  joebot  get me some
coffee   baker   yanco       

    configuration changes
the first step in advancing the state of the project
was to upgrade the project to work with the latest
version of ros  it is important to leverage all of
the fixes and functionality that have been and will

multi touch displays provide a natural way for one
or more people to control a group of robots at
 

fibe added to ros  being tied to an older version
would require implementing code that might
already exist 

       for example  tapping on a touch screen can
be used for selection  without a mouse  it is
natural to tap with one finger  or with a few
fingers  to handle this behavior correctly  the
touch processor has built in support for multifinger gestures  note that this is different from
gesture recognition with multiple strokes  

launch files were added to the project to better
compartmentalize setup and testing  now  the
ros
core
 including
map
server  
input visualization  and robot controllers are all
separate components  the overall system design is
now as follows 

multi finger gestures are handled by using a
preprocessing step to join fingers into a group  a
fingerset  and then allow fingers to join or leave
the fingerset  conceptually  a fingerset most
closely represents a few fingers on a persons
hand 
each fingerset emits events based on the state
transitions 

figure    multi touch multi robot interface overview

    input processing
the input system for the project has been updated
to 





improve the responsiveness of the input
add more types of touch inputs
allow for complex input controls
allow for input controls that are
dependent on the state of the interface

figure    touch processing state machine

the following assumptions are being made in with
the current implementation 

the input now follows a staged pipeline approach 







figure    input system overview

based on work done in user studies it is common
for users to use multiple fingers when gesturing
 micire  desai  courtemanche  tsui    yanco 
 

once a finger is determined to be in a
fingerset  it should not leave that set or join
a different set  the thinking behind this is
that a user will move all fingers in a gesture
in the same manner 
association with a fingerset can be
determined from the initial placement of the
finger  in practice  this means that a finger
identified near another set of fingers  based
on some distance threshold  will join the
fingerset  a finger far from another set of
fingers will not join the fingerset  even if it
later moves closer  even within the distance
threshold 
association with a fingerset can be
determined by distance  even for people
with very large hands  we assume that when
fingers are intended to be used as a set  the

ficurrent location of the robot to the goal  a high
autonomy situation 
at the other extreme 
inputting many waypoints  at the scale of one per
each cell in the map  would effectively cause the
planner to follow your exact path 

distance between each finger is relatively
small 
notice in the fingerset diagram  that state
transitions are mostly unaffected by single finger
changes 

the current implementation of the robot controller
supports   waypoint following behaviors 

to improve responsiveness of the touch interface 
the touch processor emits events at several states 
not just terminal states  for example  as soon as a
finger is pressed  an event is emitted  at the point
of the finger press  its impossible to know if the
user intends to press and hold or press and release
 i e  tap   but  if the user presses in a location
where the interface would respond the same to a
press and hold or a tap  the interface can respond
as soon as the press is detected instead of waiting
for the entire tap  press and release  







gesture recognition is handled as a final stage 
after a fingerset terminates with a drag event  the
dragged path is used as an input to the gesture
recognizer  by the time the gesture recognizer
processes the event  the touch processor will have
already emitted press and move events
corresponding to the dragged path  in the case
where the gesture was the intended input  the other
events will be ignored  and in the case where the
other events were the intended input  there is no
need to wait for gesture recognizer 

standard  the robot follows the planned
path to the first waypoint  then it follows
the path through each waypoint until it
reaches the goal  then it stops 
repeat  once the robot reaches the goal 
it finds a path to the first waypoint and
repeats  this can be used for having a
robot move in a loop 
repeat in reverse  the robot follows the
planned path through each waypoint and
to the goal  once the robot reaches the
goal  it follows the same path in reverse 
this can be used for having a robot patrol
up and down a hallway 

  continuous area sweeping
high level tasking for robots can be thought of in
terms of having a playbook  that is  a predefined set of rules that a team of robots should
follow  in this vein  we look at implementing
continuous area sweeping 
previous work
 ahmadi   stone        has used the assumption
that the robots will divide the map into regions of
responsibility  since this will eventually be used
in conjunction with human operators  this
constraint is relaxed for more flexibility 

for testing purposes  the mouse can simulate a
single finger input  pressing the mouse left button
represents a finger press  and releasing the button
represents releasing the finger 

    robot control
the robot controller has been expanded to
incorporate the idea of waypoints  waypoints give
a method for allowing a user to control the robots 
but still have the robots act autonomously 

in continuous area sweeping  the goal is to find a
path through an environment such each area is
touched  this might be touched in the literal
sense if you were physically sweeping  or e g 
areas that we have observed when surveying  
continuous area sweeping assumes a dynamic
environment  so the best path through an
environment may be constantly changing 

the implementation for waypoints is a plug in for
the ros move base module  the waypoints are
treated as intermediate goals  with the last
waypoint being the final goal  the ros navfn
module is given a costmap and is used to plan a
path between each pair of waypoints 

continuous area sweeping has connections to
several complex behaviors including surveillance 
exploration  search   rescue  and cleaning  we
use area patrolling as a reference case for
implementation  there is a team of robots that
must organize themselves to continually inspect
each area of a map 

the granularity of waypoints can be used to adjust
the desired autonomy of the robot  for example 
giving only a single waypoint  i e  goal  will cause
the planner to find a best cost path from the
 

fiour initial implementation makes some
simplifying assumptions regarding the problem 





the robot team is homogenous  all robots
are identical
each robot has a     degree field of view 
i e  can see in all directions
the map can be discretized into a small
set of cells
the robot can move up  down  left  or
right  and the movement is deterministic

figure    the top row shows the obstacle map and initial
reward function for the square and cave maps  the
bottom row shows the value function for the red and green
robot respectively 
since the value function is  
dimensional  the display shows all states for the red green
given a fixed position for the other robot 

    value iteration
patrolling is implemented using value iteration 

    fitted value iteration

the reward value is based on the time since a
location was visited and the other visible cells
from that location  more precisely  we use the
sum of the difference in last visit times 

when adding more robots to patrol a map  the
computational complexity increases dramatically 
in our case  each additional robot increases the
state space by two dimensions  x and y
coordinate   even in very small maps  this causes
a problem  in light of this  we explored the use of
fitted value iteration  ng        to sample the
state space 

   

    


where  is a cell visible from  

in evaluating fitted value iteration  we visually
inspected the result of using a number of different
terrain features  figure     since the goal of fitted
value iteration is to approximate the value
function  we used the discrete value iteration result
of the cave map as a reference  an infinitely
sampled fitted value iteration function should
match exactly  due to the non linearity of the
data  our best results came from using a locally
weighted linear regression function to fit the
parameters  the only features ultimately used
were the x and y coordinates 

for example  say we have a map and initialize all
cells in the map to have the same last visit time 
this would be analogous to a robot having just
entered the map  no cells have been visited yet 
then  we place a robot at cell        the reward
function would be updated based on the robots
line of sight  figure    

figure     cave  map obstacles  reward function  value
function  with robot line of sight   ligher color indicates
areas of higher value 

running the algorithm on a series of maps
produces intuitive behaviors  for the square
corridor map the robots correctly find an optimal
configuration  they can see all cells  and thus stop
moving  in the cave map  one robot will patrol the
bottom of the map  while the other patrols the top
and occasionally peeks inside the cave 

 

ficore modules  in addition  we showed initial work
towards incorporating continuous area sweeping
as a high level feature 

acknowledgements
thanks to morgan quigley for the fruitful
discussion about the project  and for assistance
with ros 

references
figure     cave  map feature functons  from left to right 
 distance transform  x distance transform  y distance
transform  gradient  x gradient  y gradient

ahmadi  m     stone  p          a multi robot
system for continuous area sweeping tasks 
ieee international conference on robotics and
automation   pp            

we found it was particularly important that
obstacle cells were always included in the sample
set for locally weighted linear regression 
otherwise  there is the very undesirable effect of
having functions that would take the robot through
walls  figure     this is because the walls are a
definite source of discontinuity in the value
function  in theory  the value function could be
discontinuous anywhere  based on the last visits
between cells  in practice though  the value
function is near continuous in all areas except
obstacles  this is because robots trying to sweep
an area are constantly moving around  leaving a
trail of continuity 

baker  m     yanco  h  a          autonomy
mode suggestions for improving human robot
interaction  ieee conference on systems  man 
and cybernetics 
micire  m   desai  m   courtemanche  a   tsui  k 
m     yanco  h  a          analysis of natural
gestures for controlling robot teams on multitouch tabletop surfaces  retrieved       from
http   robotics cs uml edu fileadmin content public
ations                 tabletop     pdf
ng  a  y         
http   www stanford edu class cs      retrieved
      from
http   www stanford edu class cs    notes cs   notes   pdf
quigley  m   gerkey  b   conley  k   faust  j  
foote  t   leibs  j   et al          ros  an opensource robot operating system  open source
software workshop at the international
conference on robotics and automation 
ricciardi  t          multitouch nav  retrieved
      from ros 
http   www ros org browse details php name mult
itouch nav

figure     cave map  value function approximation 
green dots indicate sampled points 

  conclusion
we presented the current state of the multi touch
multi robot interface along with an explanation its
 

fi