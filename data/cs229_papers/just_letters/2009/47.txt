deep networks on the gpu
michael harris  jae hyun park  and jeffrey wang
december         

 

project goals

 

features

ages matrix implementation  in order to get our
code to compile  link  and work well with the sdn
the overall goal of this project was to imple  library  we had to implement many of eigens mament various components of the stanford deep net  trix features as well as some others  here is a list
work library on graphics processing units  gpus   of major features implemented in cudamatrix 
training neural networks is usually a long and tedious task  especially when there are many layers
 all matrix matrix and matrix scalar arithmetic
of weights  on a single cpu  training a convoluoperations
tional neural network to get state of the art results
for a task such as mnist can take hours or even
 component wise square  exponentiation  abdays  fortunately  most of the computations insolute value  hyperbolic tangent  and inverse
volved in training can be easily parallelized  which
functions
make gpus the perfect candidate to speed them up 
 component wise arithmetic operations and
recent research has shown that the use of gpus can
row wise and column wise sum
immensely reduce the overhead of training a large
neural network           we are using gpus to ac static initializers for matrices of all zeros  all
celerate the matrix operations used in both forward
ones  or all of a certain scalar value  and ranand back propagation so that it is possible to train
dom matrices
deep networks much more efficiently  the implementation uses nvidias compute unified device
 block  submatrix  abstraction for operating on
architecture  cuda   which supports fast parallel
rectangular blocks of a matrix without operatexecution on gpu processors  and cublas  a liing on the whole matrix  blocks support all of
brary written over cuda that implements many
the operations that matrices as a whole supmatrix computations           we implemented this
port except for row wise and column wise rein a way so as to hide as much of the complexities
ductions 
of using the gpu as possible  e g  by providing a
clean interface that can seamlessly go from running
 ability to initialize a matrix via a commaon cpu to gpu as appropriate  finally  we tested
separated list of values via the    operator
and benchmarked our results on image datasets like
 gpu memory manager for improved allocamnist  and saw significant  up to   x  speedup
tion deallocation speed and reduced fragmenin training time  recently  we integrated our work
tation
into the sdn library 
 lazy transpose and lazy transfer between cpu
and gpu

we implemented a wrapper around cuda and
cublas that provides a clean interface and integrates directly into the sdn library  a class called
cudamatrix  and the corresponding cudavector  
sdn currently uses the eigen linear algebra pack 

 padding of matrix dimensions to multiples of
    which significantly improves performance
 custom kernel for memset  as built in memset
is not parallelized
 

fi   

most of the arithmetic operators use either a
cublas cuda kernel  or a cuda kernel of our
own design derived from cublas kernel code  the
end result is an interface that is almost the same
as eigens matrix interface  which is very matlablike  while still getting massive performance gains 
this is great because our code plugs directly into
the sdn library with literally a single change of
a  define  it will be very convenient to continue
writing code for the library using cudamatrix in
the current style and have the freedom to go back
to eigen if desired 
we discuss in more detail several of the more complex and essential features of the class 

   

we found that memory management in cuda is
somewhat of a mess  cudamalloc  the gpu equivalent of malloc  returns pointers drawn primarily
from a pool of   kb pages  which it can only allocate discretely  we observed that if we allocate a
large number of  kb arrays  the memory used will
be approximately    times as much as we expect 
as   kb instead of  kb are being used for each
allocation  this is a big problem because the training data consists primarily of small matrices so this
wasteful allocation often results in running out of
gpu memory  which is a very limited resource 
to resolve this issue  we wrote our own memory
manager for gpu memory  all of our allocations
pass through the memory manager  which implements two major pieces of functionality  firstly  it
caches allocated arrays  because our matrix implementation generates a temporary result for essentially every operation  arrays are allocated and freed
very often  this is expensive  but since the arrays
are almost always the same size  caching them works
very well  secondly  the memory manager treats
small allocations specially  it uses cudamalloc to
allocate   kb pages  but returns pointers within
those pages to represent smaller arrays  so that in
a single   kb page we can hold sixteen  kb arrays
instead of just one  the memory manager greatly
improved the memory performance of the matrix
class 

block operations

eigen implements an abstraction for operating on
blocks  submatrices  of a matrix  since this feature
is used quite extensively in the sdn library  we also
needed to implement this using cuda kernels on
the gpu  an important aspect of block operations
is that the blocks are virtual  that is they are
never explicitly constructed until necessary  their
representation is four integers specifying the block
within a matrix   this makes operations such as
copying from one block to another or adding blocks
bypass matrix construction  instead just calling a
kernel 

   

gpu memory manager

lazy transfer

since gpu memory is not directly accessible from
the cpu  it is necessary to maintain copies of the
matrix both on the gpu and in cpu memory 
keeping both copies always synchronized  however 
would incur a huge overhead as memory transfers
from gpu to cpu are extremely expensive  thus
we use lazy transfers  i e  only synchronizing when
the copy being used is known to be dirty  while
training a neural network  the computations are all
done on the gpu while the copy of the matrix on
the cpu is never updated  only when the weights
need to be read  e g  to evaluate the network  does figure    cudamatrix reconstructions  top  and
the synchronization occur  the latter occurs rarely eigen reconstructions  bottom 
during the training process  only to produce some
intermediate results 
 

fi   

correctness

in each of our experiments  we found the reconstructions and training error to be almost identical
every      iterations  figure   shows the reconstructions of a   layer autoencoder on mnist data
with      hidden units for both cudamatrix and
eigen  figure   shows the training error for the
same experiment  we verified that the results are
the same for cudamatrix and eigen in each of the
other experiments as well 

figure    cudamatrix training error  top  and
eigen training error  bottom 

 

experiments

we ran three different sets of experiments using the
sdn library in order to test both the correctness
and performance of our matrix class  in each of
these we output both reconstructions and training
error every      iterations  the goal is to see how
cudamatrix performs against eigen in a variety of
different settings 

 training a   layer autoencoder on mnist
   x    data using       iterations with a variable number of hidden units
 training a   layer autoencoder on mnist
   x    data using       iterations for each
figure    computation times for cudamatrix and
layer with a variable number of hidden units
gpumat up to     x     matrices  top  and up to
    x     matrices  middle   and speedup factor
 training a   layer autoencoder on ng video  bottom 
data using       iterations       hidden units 
and variable input size
 

fi   

benchmarks results

all of our benchmarks were performed on michaels
desktop  which has a quad core     ghz intel xeon
e     cpu and an nvidia gtx     gpu  gpu
performance should be much higher still when moving to the stanford lab computers  which have dual
nvidia gtx     gpus 
our first step was to benchmark cudamatrix
against gpumat  which is a very nice cuda library
for matlab that mostly provides the interface of
our cudamatrix  we found that cudamatrix outperforms gpumat in doing matrix multiplies and
adds by a factor of  x for large matrices and much
larger factors for matrices smaller than     x     
graphs of this performance increase are shown in
figure    the graphs show some noise  but the
trends are clear 
we also noticed while benchmarking the performance of our cudamatrix class that matrix operations are much faster   x  when matrices are
padded to dimensions that are multiples of    floats 
the overhead of computations  especially memory
transfer  is too large for this to be effective for very
small matrices  but we do pad all matrix dimensions
of size at least    to the next multiple of     and we
have noticed significant performance improvements
 again   x  from doing so  this has complicated
the internals of cudamatrix significantly  but the
performance gain is worth it  and the complications
are hidden by our interface 

figure    training autoencoders on mnist

figure   shows the speedup of cudamatrix over
eigen for the   layer and   layer autoencoders using mnist data as a function of the number of hidden units  the speedup improves dramatically for
larger networks  reaching about   x for networks
with      hidden units  figure   shows the speedup
for the ng video data as a function of input size 
showing speedups similar to the mnist data  figure
  shows the raw speedup of cudamatrix arithmetic
operations over eigen arithmetic operations  the
speedup again improves as a function of the matrix dimensions  with a speedup of over    x for
    x     matrices 
figure    training autoencoders on ng video

clearly this is a huge win for the library  as training of large networks that used to take days will now
only take a few hours 
 

fi 

acknowledgements

thanks to ian and quoc for advising us on the
project and to paul for providing us with a great
profiler for cuda kernels that allowed us to very
quickly identify performance issues in our code 
thanks also to professor ng for teaching cs    
and cs     and for the opportunity to work on
such an awesome project 

references
figure    raw speedup of cudamatrix over eigen

    d  l  ly  v  paprotski  and d  yen  neural networks on
gpus  restricted boltzmann machines  department of electrical and computer engineering  university of toronto 
     

 

    r  raina  a  madhavan  and a  y  ng  large scale deep unsupervised learning using graphics processors  proceedings
of the   th international conference on machine learning 
     

future work and conclusions

    cublas library  nvidia corporation  available at
http   www nvidia com object cuda develop html 

as we integrated cudamatrix into the sdn library 
we moved some of the computations done in the library into cudamatrix  for example  in one part
of the library  tanh is called on each element of a
matrix in turn in a tight loop  which is extremely
slow because it is not parallelized  this could be implemented very efficiently in a cuda kernel  however  so we moved that code into cudamatrix with
a component wise tanh function  this helped improve the speed of training significantly  but there
is certainly more of this kind of work to be done 
with a raw speedup over eigen of    x  we would
expect to get much better speedup while training
a network  currently  the sdn codebase uses a lot
of eigen functions and is not optimized for cudamatrix  if we spent some more time on this  we
expect that we could improve performance significantly over the current speedup 
we found in this project that  as research has
indicated  moving matrix computations from the
cpu to the gpu greatly speeds up the training and
testing of deep networks  our cudamatrix class
gets up to a   x performance improvement in training networks over eigen and still has room for more
improvements  it should also be noted that cudamatrix is a general purpose matrix class that could
be used for many other applications both in machine
learning and elsewhere  any application that makes
heavy use of matrix computations and requires a
clean matrix abstraction could benefit from using
cudamatrix 

    cuda programming guide  nvidia corporation  available
at http   www nvidia com object cuda develop html

 

fi