cs    project report
document retrieval by similarity  an application of
probabilistic latent semantic analysis  plsa 
firouzeh jalilian

haiyan liu

jia pu

december         

 

introduction

number of documents  a user not only expects a
good precision recall rate  but also expects to see
in this project  we explored using probabilistic most relevant results first  secondly  a keyword
latent semantic analysis  plsa  technique to based search system requires a user to know the
model a large collection of documents  based on exact term used in the document he wants to resuch a model  we also investigated the possibility trieve  the value of a similarity based search sysof an interface that allows a user to    observe tem is to retrieve other related documents based
and explore the document collection on macro  on what the user has found using the keyword
scopic level     conduct specific search based method 
on document similarity  plsas performance
on standard information retrieval  ir  tasks has
plsa
been well documented      the emphasis of this  
project is to build an application based on plsa
model to help end users to explore and search in detail of plsa can be found in      plsa model
large collection of documents  many practical can be viewed as unsupervised variation of naive
issues arise during this attempt  such as compu  bayes model  in plsa  the topic  i e  class latational efficiency and interpretation of learned bel  of each document example is unknown  so
model parameters  this report summarizes ex  it is necessary to learn the topics from trainperience gained and lessons learned from build  ing data  since the number of topics is given
 often determined empirically   plsa model asing such an application 
signs probability distributions over the topics to
documents  intuitively  such a soft classification agrees with the fact that a document often
  motivations
contains more than one topic 
formally  given a set of documents d with
there are two main motivations for providing terms from a vocabulary w  we assume there
similarity based document search facility to end are a set of latent topics  z  so that 
users  first  as storage capacity keeps growing 
x
large repository of documents exists not only on
p  d  w    p  d 
p  w z p  z d 
enterprise databases or internet  but also on perzz
sonal computers  for instance  its quite common to have tens of thousands of email messages let f  d  w  denote the number of occurrence of
on a personal email account  given such large word w in document d  the parameters  p  w z  
 

fiand p  z d   are learned by maximizing the log  following probabilities on the two topics 
likelihood 
p  z d                
xx
p  z d                
l 
f  d  w logp  d  
dd wd
p
 z d                
xx
f  d  w logp  w d 
p  z d                
dd wd

in this case  deuclidean  d    d    is same as
which can be maximized with em algorithm as deuclidean  d    d     although from information
following 
theory perspective  the difference between having probability   and     is much more significant
e step 
than the difference between     and     
kullback leibler  kl  divergence is another
p  z d p  w z 
measure
of the difference between two probabilp  z d  w    p
   d p  w z    
p
 z
ity
distributions 
it has been used to measure
 
z z
difference between two multinomial distributions
m step 
in similar context     as document similarity in
this report  using kl divergence as document
p
similarity  it is defined as 
dd f  d  w p  z d  w 
p
p  w z    p
   p  z d  w    
f
 d 
w
 
k
pdd w w
x
p  zi  d   
ww f  d  w p  z d  w 
dkl  d    d     
p  zi  d    log
p
p  z d   
p  zi  d   
f  d  w 
i  
ww

 

but one issue with kl divergence is that it is
non symmetric  hence difficult to interpret in
this application  after all  what do we mean
when we say that a is similar to b  but b is
not similar to a  also  this definition of similarity does not utilize p  z   the probability of each
topic on the whole training corpus  intuitively
the fact that d  and d  both have high probability on topic z is more significant if z has overall
low probability 
so we need a definition of similarity that is
based on sound statistic theory  and has the
properties of well defined metric  in      the authors proposed a fisher kernel for generative statistical model  for two examples x  and x  generated by a model which is parameterized by  
fisher kernel is defined as 

definition of similarity

in order to put the learned plsa model into
use in our final application  we need to obtain a
definition of similarity 
assume we have a model using k latent topics  i e   z    k  we use p  z d   a multinomial distribution  to denote the probabilities of
document
d being on each topic of z  since
p
p
 z d 
     document d can be reprezz
sented as a point in the  k    simplex defined
pk
by i   xi      a possible geometric interpretation of similarity is euclidean distance in this
simplex 
deuclidean  d    d     k p  z d     p  z d    k 

k x    x     uxt  i   uxi

however  a major drawback of this definition of
similarity is that it does not take any advantage
of information theory  for example  if we have
a two topic plsa model  and four documents
d    d    d  and d    also  if these documents have

where ux    log p  x    the value of fisher
kernel is that it defines a metric relationship directly on generative model  in      the authors
derived the fisher kernel for plsa model  in
 

fitheir derivation  the kernel consists of two components 

and up to n most similar documents  d  
 d         dn    are returned  for each di in d  if
it shares at least one tag with q  i e  tdi tq     
x p  zk  d   p  zk  d   
then di is a successful retrieval  our dataset conk   d    d     
 
p  zk  
tains     unique tags 
k
although this type of evaluation is standard
and
practice in ir research  we need to view it with
a grain of salt  because matching tags is rather
x
k   d    d     
tf  wj  d   tf  wj  d   
a narrow view of document similarity  for exj
ample  in reuter corpus  document       is
x p  zk  d    wj  p  zk  d    wj   
on the subject of chinas winter crop production  this document is tagged with grain 
p  wj  zk  
k
wheat and rice  when using this document
where tfp
 w d  is the term frequency defined as as query  one of the returned similar documents 
f  d  w   w f  d  w   the two components em         is tagged with soybean  red bean and
phasize two aspects of plsa model  k  com  oilseed  for some users  this would be appropares two documents based on their topics  while priate search result in that they are both clearly
k  compares two documents based their shared on the topic of agriculture  but if we only look
words  since the computation of k   d    d    is at the tags  this is considered a false positive 
for text processing  we only used rather trivexpensive  due to the limited time  we only use
k   d    d    as similarity measure in our imple  ial methods  each document is tokenized using
mentation  which alone shows improved preci  white spaces  then tokens that do not contain
sion in certain scenario  detail in section     to any alphabet  and tokens that are only one charsummarize  in our final application  similarity acter long are removed  after that  all tokens 
except acronyms that consist of only upper case
between two documents is defined as  
letters  are converted to lower case  and a porter
x p  zk  d   p  zk  d    stemmer is applied  finally  we removed top
 
d d    d     
    
k   d    d   
p  zk  
words that are identified using normalized enk
tropy     
the inversion is just to make smaller distance to
using evaluation described above  fig   shows
indicate higher degree of similarity  so that it is the precision recall rate of using three different
consistent with other definitions of similarity we definitions of similarity  as it is shown  fisher
studied in this project 
kernel generally achieves better precision recall
accuracy than other two similarity measures 
also  empirically we determined to use    la  experiments
tent topics in our final model  fig   shows the
precision recall curves using different number of
people have done many precision recall evaluatopics 
tions on various corpus using plsa model  we
repeated some of the experiments during our
project to    find better parameters     verify  
spotlight
that our plsa implementation is correct 
in this project  we used a subset of      doc  the second component of this project is a gui
uments from reuters       dataset  in this based application that allows one to explore a
dataset  each document d has been given a set collection of documents  it also helped us to subof tags td by human annotators  to evalu  jectively evaluate plsa model  this application
ate  a random document q is chosen as a query  is designed with following goals 
 

fiwords from the two documents that contribute most to their common topic  however  sometimes these words are not really
essential to the subject of the documents 
for instance  for documents about grain
production forecast  word such as december  or february often show up 

 
fisher kernel
kl divergence
euclidean

   
   

precision

   
   
   

 by expanding nodes that are connected  it
allows a user to identify a set of documents
that are similar to each other  but are not
similar to any other documents  in fact 
these documents form a connected component in the graph built on the whole corpus 

   
   
   
   
 

 

   

   

   

   

 

recall

we set off on this project to investigate
the possibility of using plsa on personal
computers  from implementing it  we realized that it is computationally very expensive when the number of latent topics is
high  due to the iterative method of learning plsa model  in our implementation 
we processed up to eight data partitions simultaneously  even with this optimization 
it takes more than   hours to run em training for     iterations on     topics  the demand on computation resource makes it a
less attractive solution on todays personal
computer  but it still could be useful for
moderate dataset 

figure    precision recall with    latent topics

   
   topics
   topics
   topics
    topics

   
   

precision

   
   
   
   
   
   
 

 
 

   

   

   

   

conclusion

 

recall

this project demonstrated that plsa is a powerful technique for modeling document similarfigure    precision recall of different number of ity  combined with keyword search  it provides
topics
more flexibility on searching in large collection
of documents  in our implementation  we didnt
utilize p  w z   it deserves further investigation
 it shows similar documents given a query on how we can take advantage of p  w z  to prodocument  this is done by showing the vide more useful information 
nodes connected by edge  from subjective
evaluation  it seems plsa is quite accurate
on identifying similar documents  at least on references
this dataset 
    j r  bellegarda  latent semantic mapping 
 given a pair of similar documents  it shows
dimensionality reduction via globally opti 

fifigure    screen capture of spotlight application

mal continuous parameter modeling  in automatic speech recognition and understanding       ieee workshop on  pages    
     nov       

ference on advances in neural information
processing systems ii  pages         cambridge  ma  usa        mit press 

    dong zhang  daniel g  perez  samy bengio  and deb roy  learning influence among
    thorsten brants  test data likelihood for
interacting markov chains  idiap rr    
plsa models  inf  retr                     
idiap  martigny  switzerland       
    thomas hofmann  learning the similarity
of documents  an information geometric approach to document retrieval and categorization       
    tommi s  jaakkola and david haussler  exploiting generative models in discriminative
classifiers  in proceedings of the      con 

fi