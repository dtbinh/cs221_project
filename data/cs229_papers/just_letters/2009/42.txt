value iteration and ddp for an inverted pendulum
by 
gregory m  horn

cs    final project
professor  andrew ng
december         

fiabstract
my intention in this project was to learn about nonlinear control from a reinforcement learning standpoint  i got value iteration working on a simple gravity pendulum with torque controller by discretizing
the state space and quadratically interpolating in between grid points  i couldnt apply this technique
to an inverted cart pole because of the curse of dimensionality so i explored differential dynamic
programming  ddp  as an alternative  i built a cart pole swing up controller with receding horizon
ddp 

discretized value iteration
a nonlinear dynamical system can be described as

xk  

  f  xk   uk  
 ak xk   bk uk  

the value function is the total cost to get from a state to a final state assuming optimal control policies
are used at each time step 

v  xk  

  c xk   uk     c xk     uk       c xk     uk         
  c xk   uk     v  xk    

where c x  u  is the cost associated with a given state and action  assuming that the value function is
known the optimal control policy at state xk maximizes v  xk   

uk

  arg max v  xk  
u

  arg max  c xk   uk     v  xk       
u

a quadric cost function in u was adopted in order to derive a simple optimal policy 


 
arg max c xk    utk r uk   v  xk    
u
 


  t
uk c xk    uk r uk   v  xk    
 

uk

 

 

 

 

 

r uk    x v   uk  xk    

r uk

 

 x v   uk  ak xk   bk uk  

t

t

 

fir uk

 

 x v  t bk

uk

 

r   x v   bk  

t

the technique for finding the optimal policy was taken from     
to find the value function the state space is discretized into states s and value iteration is applied 
the value function is quadratically interpolated in between states when needed  when the optimal value
field is found  a real time controller would also interpolate the value function in deriving its policy 
v  x       
for n iterations
vold    v  
for  xk  s 
aij   

f  xk  u i
 
xj

bij   

f  xk  u i
 
uj

  linearize dynamics
  linearize dynamics
t

u    r   x vold  xk    b 

  compute policy at xj

x     a xk   b u 

  propagate state with simulator

v  x       interpolate x    vold   


v  xk    c xk   u    v  x    

  interpolate value f unction at propagated state
  update value f unction

end
end

mass on spring
the algorithm was debugged on the simple mass on spring system  the dynamical equation and cost
function are 

x
c x  u 

  k x  b x   u
 

  t
 
x q x   ut r u
 
 

the algorithm worked and the learned controller found the exact same path as an lqr controller designed
with matlab  see figure    

 

fifigure    mass on spring value function and learned controller

gravity pendulum
the algorithm was then applied to controlling a simple gravity pendulum with saturating torque control 
the pendulum dynamics are 
  

k
g
u
sin   
 
  
l
ml 
ml 

the algorithm converged and the value function was learned  see figure    

figure    value function for limited torque pendulum  streamlines show the phase portrait of the
optimally controlled system 

differential dynamic programming
i was unsuccessful in applying value iteration to a cart pole system  largely because of the curse of dimensionality  so i tried differential dynamic programming  ddp   in ddp a path   x    u      x    u         xn   un   
 

fiis iteratively improved upon in two steps  in the backward sweep the value function is quadratically approximated about each state in the sequence and the policy u is improved  in the forward sweep the
new policy is simulated and a more optimal path x is found  this iterates until convergence 
to quadratically expand the value function  first quadratically expand the q function  the unoptimized value function 

q xk   uk  

  c xk   uk     v k    f  xk   uk   

   


 
 q    qtx x   qtu u     x
 

qxx
t 
u   
qtux





qux   x 


quu
u

   

where subscripts denote derivatives  the q function is maximized with respect to u

 

  u q x  u    qu   qux x   quu u

u   q 
uu  qu   qux x 

plugging the optimal u back into q x  u  yields the value function

v  xk    

 
q   qtu q 
uu qu
 





  t
qxx  qtux q 
  qtx  qtu q 
uu qux x
uu qux x   x
 

the coefficients of     are found by quadratically expanding     

c xk   uk   
v k    f  xk   uk    

 
 
c xk   uk     cxt x   cut u   xtk cxx xk   utk cuu uk   utk cux xk
 
 

v k      vxk    t  ak xk   bk uk    

 



 k    ak x 
 
 ak x t  bk u t vxx


 
bk u



  x 
 
xt ut h 

 
u

where elements of h are given by

hij  

x v k      f  xk  l
l

xl

xi xj

 

ddp was run on the gravity pendulum with an array of starting points  from the resulting array of
optimal paths it seems that ddp is more or less finding the same controller as in value iteration  figure
    there are some noticeable discrepancies which i currently believe are results of buggy code 

 

fifigure    ddp results overlaid on value iteration results  left  ddp  red  traces out paths similar to
those found with value iteration  black   right  value functions found with ddp  black  hug the value
function found with value iteration  colored  

finally  ddp was run on a standard cart pole system  using a receding horizon a nice swing up
controller was obtained  figure    

figure    cart pole swing up controller found with ddp   left  strobelight style transient plot  

references
    kenji doya  reinforcement learning in continuous time and space  neural computation           
          

 

fi