lifetracker
carlin eng  tim su  vala dormiani
cs      professor andrew ng
autumn     
introduction
in the internet age  worker productivity is plagued by the myriad of distractions available to anyone with
a computer and an internet connection  for those amongst us obsessed with maintaining reasonable levels of
productivity  it might be useful to have a program running in your computers background to monitor system
usage and prod you whenever it detects that you might be slacking  for our project  the lifetracker application 
we attempted to create a framework to collect data on computer usage behavior and classify that behavior into
one of several categories 
data collection framework
lifetracker relies on a modular data collection framework written in python to collect data about user
activity in a linux based operating system  the following diagram explains the architecture of this system 
training label
x   events
python r bridge

active processes

os

classifier

collector
tcpdump
resource usage

data modules collect information from the operating system  the collector component aggregates all data 
solicits a training label from the user  if provided   and feeds it to our machine learning algorithms written in r
through an interface library  each data module subclasses the recorder class  built atop python s threading
library 
class recorder threading thread  
   generic class for collecting and returning data  to create a
recorder  create a class that subclasses recorder and override the
methods below   
def   init   self  
threading thread   init   self 
self finished   threading event  
self daemon   true
def collectandclear self  
   method to return collected data and clear internal state  this
method is called by the master recording thread to collect data from
individual recorders 
data should be returned as a dictionary of feature name   value
pairs  a feature value can be either a boolean  a single number  or
a list of strings    
raise exception  unimplemented method  
def run self  
   start collecting data   
raise exception  unimplemented method  

fidef stop self  
   main thread wants to stop collection   
raise exception  unimplemented method  

example data
following are examples of data collected by the framework  attached to illustrate the types of data collected
and give the reader an intuition about how different features are related to user activity 
programming

label
words typed

e mail

ac  cl  xb  cd  coll  exit  and  right  just  house 
sleep  xs  provided  no 
is  hard  am  idea  at 
py  start  kill  xwcls  ls 
yes  next  really  are 
answer  import  run  stop
little  thank  things

internet

gaming

delic  digg  popcorn 
mmmm  com

rroo  zzz  zzx  zzzz 
zz

www      imaps     
domain    

domain      imaps     
www     

https       www       
domain     

https      www    
imaps      domain 
 

packet destinations

stanford edu 
mozilla com 
google com

google com 
stanford edu 
moody edu

google com 
facebook com 
akamaitechnologies com

google com 
              
             

mouse travel pixels

    

     

     

    

 

  

  

 

   

   

   

    

   kb

   kb

    kb

   kb

received bytes

      kb

    kb

    mb

   kb

disk write bytes

     mb

     mb

     mb

    mb

disk read bytes

    mb

     mb

    mb

    mb

user cpu  

     

     

      

      

kernel cpu  

     

     

     

     

packet ports

mouse clicks
keystrokes
transmitted bytes

nave bayes classifiers
we implemented several nave bayes classifiers with the goal of using specific events as predictors for
user behavior  our data collection scripts output python dictionaries with histograms of four types of events 
words typed  outgoing tcp packet destinations  outgoing tcp packet port numbers  and processes running  the
idea is simple  certain events are more likely to occur when a user is performing certain tasks  so these events
should have some power in helping us predict current activity  for example  someone in the act of programming
in python is more likely to type the word import than someone watching online videos  similarly  an open web
browser is more indicative of internet activity than a closed web browser 
our implementation reads the dictionary outputs of the data collection scripts and generates a prior
distribution over all potential activities based on their relative frequencies  then  using a multinomial event
model  we attempted to calculate the likelihood of each of the activities based on the dictionary contents 
the events come in four different flavors  so a nave bayes classifier can be constructed using every
combination of one or more of these dictionaries  our team tried implementing classifiers using all    possible
combinations  the results were underwhelming  the best training error we achieved was      with the majority
of classifiers guessing incorrectly over     of the time 

fithe worst classifiers  using only dictionaries of words typed  or packet destinations  had astounding
training errors of over      correctly classifying only     of the time  which is only slightly better than the
performance we could expect from someone randomly guessing                these astronomical training
errors indicate our model suffers from an extremely high bias  and fails to capture the story correctly  looking at
the distribution of guesses from training set predictions reveals part of the reason why the errors are so high 

these charts show the distribution of guesses on our training set for the processes and the ports classifiers  as
well as our distribution of prior beliefs  using these nave bayes classifiers  every single one of our points is
either assigned internet  working or nothing  looking at the prior distribution  we see that internet and
working are by far the most likely  accounting for        and        of all observations respectively  with so
little data  we were not able to consistently observe specific events associated with specific behavior  so when

ficalculating the likelihood for each of the categories  these large prior values dominated the likelihood observed
from the actual events  the events contributed almost nothing to the likelihood  resulting in the majority of our
observations picking up the label of internet or working  these results suggest that the nave bayes
approach to solving this problem performs poorly when given small samples of data 
logistic regression
quantitative data such as number of keystrokes  mouse movement  bytes read written to disk  network
activity and cpu usage can all serve as predictors for user behavior  users who are idle  playing games  coding 
or watching videos are all likely to use varying degrees of system resources  we used a multinomial logistic
regression to classify user behavior into one of eight categories based on the following features collected over
five minute intervals  total mouse distance moved  total number of mouse clicks  total number of keystrokes 
ram used  bytes transferred  bytes received  bytes written to disk  bytes read from disk  last one five minutes of
system load  kernel cpu   user cpu   and io wait cpu  
we implemented our multinomial logit model in r using the vgam package developed by yee and wild
        each of our feature vectors contains    predictors  but it is very likely that several of the features are
either correlated with each other  or simply do not act as good predictors for user behavior  in fact  plotting the
test error and training error vs  training set size shows that when our model includes all    predictors  our test
error remains very high  despite very low training error 

this is a classical indication that our model suffers from high variance and we are overfitting  choosing an
appropriate model is an essential task  and there are plenty of methods available for model selection  several of
which we explored 
model selection
exhaustive search 
an exhaustive model search fits all possible models and compares their performances using a technique such as
leave one out cross validation  selecting the model with the best results  each of our observations consists of   
features  so an exhaustive search would require fitting            models and cross validating each one  this is
an extremely expensive operation  and as our training set size increases  as it inevitably will for our application 
the procedure becomes computationally infeasible 
stepwise methods 

fiakaike s information criterion  aic  calculates the log likelihood of the estimated model and penalize the model
for each additional parameter  aic        loglik    k  where k is the number of model parameters  a model with
lower aic is superior  we utilized a two way stepwise search  starting with all    parameters  at each step  we
consider either dropping or adding a parameter  making our decision based on which model has the lowest aic 
when we reach a point where a better model cannot be attained by either dropping or adding a parameter  our
model selection is complete  results can be further tested using cross validation 
our final selected model contained the following predictors  mouse distance moved  number of
keystrokes  bytes read from disk  bytes written to disk  used memory and bytes received  the following chart
plots our errors versus training set size  showing the new model is a significant improvement over the previous
approach 

conclusions
our attempts to classify behavior using a nave bayes algorithm performed poorly  the set of total
dictionary items such as words typed was simply too large for our small sample size to effectively associate
events to user behavior  perhaps if we extended our sample size to include many more data points across many
more users  our algorithms effectiveness would improve  our multinomial logistic regression model performed
reasonably well  but initially suffered from overfitting  using a stepwise model selection process  we were able
to reduce the variance of our model by a significant margin by including only the most important features in our
model  in the future  we could also utilize regularization techniques such as l  or l  penalties  combined with a
prior distribution over our parameters  the logistic regression model  with its ease of implementation  and
satisfactory results  will most likely be the algorithmic method of choice moving forward  also  our current data
collection framework works only in a linux environment  in the future  we hope to create a platform agnostic
version  suitable for use with more common operating systems such as windows 
references
ng  andrew  cs     autumn      lecture notes  available at 
 http   www stanford edu class cs    materials html  
sakamoto  m  ishiguro  g  kitagawa   akaike information criterion statistics   journal of the royal statistical
society  series d  the statistician   vol      no              pp          

fi