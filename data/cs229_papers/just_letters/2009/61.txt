learning to grasp objects  a novel approach for localizing objects using
depth based segmentation
deepak rao  arda kara  serena yeung
 under the guidance of quoc v  le 
stanford university

abstract
we consider the problem of grasping novel objects with a
robotic arm  a recent successful technique applies machine
learning to identify a point in an image corresponding to
the most likely location at which to grasp an object  another approach extends their method to accomodate grasps
with multiple contact points  this paper proposes a novel
approach that tries to find graspable points in an object after localizing it  we present a depth based segmentation
scheme for object localization and discuss how depth information can be combined with visual imagery to improve the
performance of both segmentation and object localization 

   introduction
the problems of image segmentation and object localization remain great challanges for computer vision  having received continous attention since the birth of the field 
the last few years have seen considerable progress in eigenvector based methods of image segmentation  these methods are too slow to be practical for many applications 
the state of the art approach for segmenting an image into regions uses graph based representation of the
image     although  the method works better than its counterparts  it has some limitations  since the algorithm is
based entirely on pixel intensities it works poorly in scenes
having shadows or more than one light source 
the task of object localization involves drawing a tight
bounding box around all instances of an object class within
the image  this paper aims to highlight some of the drawbacks of using color based segmentation for localizing an
object  figure   shows an example of an object composed
of multiple colors  performing color based segmentation
on the object results in different segments all belonging to
the same object  taking into account these limitations and
drawbacks we propose a segmentation framework for object localization that combines depth information with visual imagery  color intensity   the next section gives a de 

figure    example of an object composed of muliple colors

tailed comparison between the two approaches 
the intuition behind using the depth data for segmentation came from     in which quigley et al used high resolution  d data for improving object detection  we conducted
experiments to prove that localizing objects after performing our proposed model of segmentation works better than
localization performed after segmentation using      a detailed explanation of our approach along with the experimental results have been given in the following sections 

   segmentation
our system makes use of both depth and colored image
data  to capture depth  we used an active triangulation sensor      an important feature of this sensor is that it gives
a very detailed depth data  also called depth map or point
cloud   since color is an important attribute for segmentation  we added a functionality in ros     that enabled us to
capture   channel colored images  figure   gives an idea of
tha data captured by our robot 
graph based image segmentation     tries to create a
graph based representataion of an image and finds the evidence of a boundary between regions based on l   norm
between the intensity of pixels 
  i pi    i pj        

   

where i pi    r  is the intensity of the pixel pi and  is
the threshold function 
if the l  norm between the intensities is greater than

fifigure    image and depth data captured by our robot 
the threshold function then the pixels are considered to be
in different regions 
as discussed earlier  segmentating an image truly on the
basis of color is not the correct approach  the advantage of
depth data allowed us to extend the approach in      we
stick with the same scheme of representing an image as
a graph but updated the metric for finding boundaries between regions to include the depth data 
  w t   f  pi    f  pj         

 a  regular segmentation

   

where f  pi    r  is the intensity of the pixel pi having an extra dimension of depth value corresponding to that
location in  d space  w  r  is the weight vector assigning weights to different elements of f and  is the threshold
function 
the intuition behind including weights in the equation
was to rank the elements in order of their importance  this
approach allowed us to give greater weight to the depth
value and smaller but equal weights to the intensities  another approach could have been to learn the weights that
give the best segmentation but due the comparitively small
size of features  we decided to hand tune the weights to produce good results 

     comparison
figure   shows the comparison between the two segmentation approaches  it can be seen in figure  a that athough
the method proposed in     works well to segment the scene 
it is not robust to small color changes  this results in a large
number of artefacts on the table and the wall which ideally should have been a part of the single parent segment 
the limitations of segmenting an image only on the basis of
color can be cleary seen in this example  another observation that made us to use a combination of depth and color
for segmentation was the inability of the base algorithm to
handle shadows  all these factors contributed to our hypothesis that segmentation based purely on color would not
be ideal for object localization 
in order to prove our hypothesis we decided to localize
objects using both the approaches and compare their scores 
object localization involves drawing a tight bounding box

 b  proposed segmentation

figure    comparison between the two approaches 
around an object  over the years  the complexity of the
problem has led to various approaches and different algorithms to solve the same  this made it difficult to choose
a standard recognized scoring system to compare our approaches  all these constraints led us to make our own
intuitive scoring scheme that was able to compare the approaches pretty well 
we took     images from the robot having different objects at varied positions to compare our approaches  this
was followed by manually labelling the objects using the
image sequence labeler in stair vision library      the
human label for the object was considered to be the ground
truth upon which our scoring scheme is based  once the labelling was done we ran both the segmentation algorithms
on the dataset and made bounding boxes around all the obtained segments  figure   gives an illustration of the same 
the green bounding box drawn around the object was the
ground truth label 
the approach while designing the scoring framework
was based on finding the individual scores for every image
and then use those scores to determine the final score of the
algorithm 

fifigure    image obtained after drawing bounding boxes
around all the segments 

the scoring scheme can be broken into the following
parts 
ai j   f  si j   gi  
where ai j is the area of overlap between obtained segments si j and ground truth gi  
ri j   min 

ai j ai j
 
 
asi j agi

where asi j is the area of segment si j and agi is the
ground truth area of gi  
i  

ni
x

ri j exp   ri j  

j

i  

ni
x

exp   ri j  

j

where ni is the number of segments in image i and  is
some constant 
pm
score  

i

i
i

   
m
where m is the number of images in the dataset 
the scoring scheme to some extent can be considered as a weighted average of individual scores where the
weights are exponential  the intuition behind exponential
weights came from the idea that the segments similar to
the groundtruth would have a higher value of r and hence
should be given more weight 
after designing the scoring scheme we plotted a graph
showing the comparison of scores between the two approaches at various thresholds  as portrayed in figure  
our algorithm outperforms the base algorithm at almost every threshold  the scores are relatively smaller at low and

figure    performance of the two methods 

high thresholds because of over and under segmenation respectively  this proved our hypothesis that segmentation
based only on color is not ideal for object localization 
after observing the graph we decided to change equation   to include the threshold that gives the optimal performance 
o   arg max t  score    
  w t   f  pi    f  pj         o

   

   classifier design and feature selection
in this section we briefly touch upon the features that
were used by our classifier to localize objects  before designing the classifier for object localization we needed to
label segments of all the images as positive or negative 
the bounding boxes of all the segments were assigned positive or negative labels based on their overlap ratio with the
groundtruth 
in our proposed framework  we consider features from
visual and depth data  a brief description of all the features
and the intuition behind choosing them is elucidated in the
following sections

     bag of features
the features were chosen such that they give a true representation of the bounding box  the width  height  area and
aspect ratio are a measure of the geometry of the bounding
box while distance from left and right edge gives an idea
about its position  historically  researchers have avoided the
use of color based features due to computational and philosophical reasons  the computational trade off is obvious 
and the philosophical reason being that humans can perform
these task of localization without color information as well 
since  the proposed segmentation framework already uses
color data we decided to use some color features as well 

fi a  area

 b  height

 c  width

 d  aspect ratio

 e  distance from the left edge

 f  distance from the right edge

 g  variance of l  lightness 

 h  variance of a

 i  variance of b

 j  variance of depth

figure    feature vector plots for all the features 

fiin order to make the classifer robust to lighting changes we
converted the color space from rgb to lab  instead of using raw color values or mean as features we decided to go
with variance  variance of l  a and b within the bounding
box were chosen to be the color features  to complete the
feature vector we needed a depth feature and hence decided
to go with variance of depth within the bounding box  the
intuition behind chosing variance came from the fact that it
is a higher degree feature and probably a better indicator of
the state of the object within the bounding box 

rank
 
 
 
 
 
 
 

features
variance of a
variance of b
variance of depth
area
variance of l
width
height

table    feature ranks

     feature selection
after selecting the features explained in previous section  we trained a svm model to classify bounding boxes
as positive or negative  surprisingly  the svm failed to
converge  to get a better understanding of the reason we
created a feature vector plot for all the features  see figure
     there were some interesting insights after studying the
feature plots which in turn served as a motivation for our
next step  we decided to perform forward search to find the
optimal subset of features  as expected  the best feature
subset returned by forward search was 

of

 width  height  area  variance of l  variance
a  variance of b  variance of depth 

it can be seen from figure   that the feature plots of distance from both edges and aspect ratio are randomly distributed and were the main reason for making the data inseparable  hence  these features were omitted from the feature vector and svm model was trained using the remaining
features  table   shows the rank of different features 

     grasping
the localization model we proposed takes a new test
scene and finds all bounding boxes that have a high probability of containing an object  this is followed by calculating centroids of the bounding boxes in  d space by mapping them with the point cloud  to test our localization approach we considered the centroid to be best grasping point
and tried to grasp the object at that point 

   experimental results
we consider two sets of experiments  the first set of
experiments is performed offline to the test the accuracy of
the classifier for localizing objects  in the second set we
compare our method with     when grasping a novel object 
we perform all experiments with the barrett hand having
three fingers 

figure    accuracy versus number of folds of the svm
model for object localization 

     offline test
our dataset consists of     images and their correspondig depth data  the dataset consists of different objects
at varied positions  it is further divided into     positive
segments and      negative segments  this test is totally
offline  i e  without robotic execution  the goal of the test
is to determine the accuracy of the classifier for object localization  we tried to test the accuracy by performing k fold
cross validation on the dataset  figure   shows the results
of the test  the accuracy increases as we increase the number of folds  i e  training data  and becomes constant after
some time  this shows that the classifier needs only a certain amount of data for training after which the improvemet
in accuracy becomes negligible 

     grasping novel objects
figure   shows the stair  robot grasping a nerf gun  in
this section we try to compare our grasping results with     
the comparison is not a totally fair because our grasping approach remains static and is independent of the size shape
and orientation of the object  still  there were some interesting insights for objects that dont necessarily have an ideal
grasping point like nerf gun  football  foam and joystick 
surprisingly  our algorithm outperformed the algorithm in
    for those objects that are non uniform in their compos 

fifigure    results  from left to right  original image  segmentation     proposed segmentation  object localization

fifigure    an image sequence in which stair  grasps a
nerf gun 
tion and dont necessarily have an ideal grasping point  this
builds a foundation for our future work in which we will try
to find graspable points based on different objects after localizing them 

   acknowledgement
the project has been made under the guidance of quoc
v  le  it would not have been possible to implement this
idea without quocs sound mathematical ideas and direction  siddharth batras help with the stair vision library
for labelling the dataset while training is also acknowledged 

   video
we have posted a video on youtube giving a demonstration of our project 
http   www youtube com watch v  bb rsc wda
we also have a stair fetches a stapler section at the
end of the video 

references
    a  saxena  j  driemeyer and a  y  ng  robotic grasping
of novel objects using vision international conference on
robotics research                     
    p  f  felzenszwalb and d  p  huttenlocher  effecient graphbased image segmentation international journal of computer vision                     
    q  v  le  d  kamm  a  f  kara and a  y  ng  learning to
grasp objects with muliple contact points
    m  quigley  s  batra  s  gould  e  klingbeil  q  le 
a  wellman and a  y  ng high accuracy  d sensing for
mobile manipulators  improving object detection and door
opening icra      
    s  gould  o  russakovsky  i  goodfellow  p  baumstrack 
a  y  ng and d  koller the stair vision library  v    
http   ai stanford edu sgould svl       
    the ros  robot operating system  framework is an opensource  peer to peer  cross platform message passing system
being jointly developed by stanford university and willow
garage  ros is available on sourceforge  documentation is
avaiable at http   pr willowgarage com wiki ros 

fi