convolutionalrestrictedboltzmannmachinefeatures
fortdlearningingo
byyanlargmanandpeterpham
advisedbyhonglaklee


  background motivation

  algorithms

althoughrecentadvancesinaihaveallowed
go playing programs to become moderately
strong even on full   x   boards  go
programs still cannot approach the highest
levelsofplay oneofthestrongestgoplaying
programs  strongest in       utilizes the
uctalgorithm whichisamontecarlosearch
algorithmpairedwithavaluefunction    

   convolutionalrestricted
boltzmannmachines

the value function was learned through
temporal difference function approximation
withallpossible      templates onallboard
positions however theremaybefeaturesin
suchasetthatarenotverymeaningfulingo 
if more meaningful features could be found 

then the stateoftheart algorithm could be
applied on top of these features to create an
even stronger go playing agent  in this
project  we applied the convolutional
restricted boltzmann machine to go data in
anattempttolearnfeaturesbettersuitedfor
tdlearning wethenevaluatedthefeatures
on a series of tests to determine the
effectiveness of the boltzmann machine in
capturingfeaturessignificantingo 


 atemplateisanassignmentofblack white or

emptytoeverypositioninagivenarea for
examplea      area  applicationofan n  n 
templatefeaturestoa b  b goboardresultsin
 nn   b  n        features whereeachfeatureis
 ifatemplatefeatureexactlymatchesalocation


ontheboard 





our primary goal for this project was to
automatically generate translationinvariant
featuresthatareinformativeforencodingthe
state of a go board   therefore  a
convolutional restricted boltzmann machine
 crbm  seemed well suited to the task   a
crbm is a network that learns statistical
relationship between a visible layer and a
hidden layer       in particular  by using the
contrastive divergence  an rbm adjusts the
value of the weights so that the
reconstruction of the visible layer from its
hidden layer representation matches the
visible layer training data as close as
possible   in a crbm  weights are shared
betweengroupsofhiddenlayerunitssothat
signal propagation from the visible layer to
the hidden layer can be implemented as a
convolution 
ourinputlayerforthecrbmconsistedofa
threechannelrepresentationforagoboard 
where the unit in the visible layer channels
were   if the position was black  white  or
empty positions  respectively  and  
otherwise   we modeled these units in two
ways for comparison  realvalued units and
multinomial units   for crbm training with
realvalued visible units we used the mean
fieldapproximation sonosamplingofvisible
units was done during the reconstruction
phase of contrastive divergence   for crbm
training with multinomial visible units we
usedsoftmaxsamplingtodetermineinwhich

fichannel a unit would be active  according to
theformula

agent   specifically  we used the update
equations   

exp i hi c j   

  r   v  s   v  s 
e  e    v  s  
     e

 

 exp i h

c
i  j



  

c  



where hi c j isthehiddenunitatposition  i  j  
inchannelc and

c
i hi  j       w b  f h b   i  j   c 


b
istheincreaseinenergyresultingfromthe
activationofthatunit whichthetopdown 
signalpropagatedfromeachhiddenlayer h b 

throughthecorrespondingweights w b bya
fullconvolutionplusabiastermc 


because go exhibits rotational
and
 augmented an
reflectional symmetries  we
ordinary crbm by adding rotational and
reflectional invariance to the training
process  this was accomplished by adding a
layer that ties the weights of the different
orientations of the kernels together  in
practice this involved the following  we first
apply the weights to the raw board in each
possible configuration  then  instead of
sampling the hidden layer corresponding to
each configuration independently  we take
the softmax of the activations so only one of
thesehiddenunitlayersisactive finally we
sum the contrastive divergence gradient for
each configuration  rotating reflecting
appropriately to recover the original
orientation  to find the gradient for the
original shared weight that generated these
configurations 

   temporaldifference
reinforcementlearning
once our features were trained  we used
temporal difference learning with a linear
value function approximation for the
afterstates of moves to train a goplaying

where

t 
 v  s    sigmoid   s    


  istheweightvectortheagentlearns and
 s isthefeaturevectorassociatedwithstate

 s inthiscase thegradientissimply
 
  
 v  s    sigmoid  t  s     sigmoid  t  s    s 

fortherewardfunction wegavearewardof
  on a win  and all other rewards were   
therefore  the value function can be seen as
approximating the probability of winning a
game from the given state   although
intermediate rewards  such as a reward for
capturing opponent stones or a penalty for
losingstones couldbegiven thetruegoalof
the agent is only to win  and in some cases
sacrificing stones or passing up an
opportunity to capture stones could be
beneficial  furthermore  other similar
applicationsoftd  learningtogohavehad
success using only the reward for a win     
we also used     which is a onestep td
backup  as this was a parameter value that
achieved the most successful learning in
relatedapplications       

for our agents policy  we used an  greedy
policy  so the agent either picked the move
that resulted in the board position with
maximum value with probability       or a

random move with probability     the state
space of go is very large  so a softgreedy
policy was used so to ensure that the agent

would explore the state space during

training theonlyconstraintimposedonthe
policy was that suicide moves could not be
chosen   this was done to prevent games
withinfinitelength 

fi

  experimentalsetup
   featurecomparisonusingtd






we evaluated the effectiveness of the crbm
features against enumerated       template
featuresbytrainingavaluefunctionsforboth
features and then pitting the  greedy
          policies defined by these value

functions against each other   the agents
used  greedy policies so that games would

not deterministically play out the same way
eachtime 


to achieve a fair comparison  the number of
templatefeaturesandcrbmfeaturesthatwe
used was approximately the same   we used
    weights of size       for a total of     
crbm features on a       board   in our
experiments 
nonrotational reflection
invariant features performed better in td

learning play  so the features we used for

comparison are regular crbm features   for
templatefeatures weusedall  enumerated
      templates for a total of      template
featuresona      board 

thevaluefunctionsweretrainedofflineona
thousandgamesplayedbygnugo   against

itself   although td learning is not
guaranteedtoconvergeintheofflinecase we
found that in practice this training was
sufficienttoestablishabaselineatwhichwe
couldcomparethefeatures 

gameswereplayeduntilbothsidespassedor
    moves had been played  at which point
nowinnerwasdeclared  becausescoringa
boardanddecidingthewinnerofagogame
is nontrivial  we used gnugo through gtp
protocol to score the finished games   a
standard komi  score handicap to offset
blacks initial move advantage  of     was
used  and both features were given turns
playingasblack 


   selfplaybootstrappingusingtd


as an additional confirmation that crbm
features can be useful for td learning  we
tested whether the model learned from
crbm features could learn to consistently
beatitselfthroughselfplaybootstrapping 

using the value functions learned from the
offlinetrainingongnugogames twomodels
initiallyusingthesamevaluefunctionplayed
against each other   one of the models used
td learning to update itself online during
play  while the other model remained static 
we decayed   of the learning model during
training soinitiallythealgorithmgivesmore
emphasis to exploration  and gradually it
startsexploringlessandexploitingmore 


   winnerclassification
we sought to indirectly measure how much
information the crbm kernels encode about
the game by classifying the winner of
professional games given the middle or end
board state configurations  we used an svm
forclassification makingsuretofindthebest
kernels for each set of features and using k
foldcrossvalidationtofindthebestcvalue 
asabaselinefeatureset weusedallpossible
 x templatesappliedateachpositiononthe
board  the kernels that we used in this
classification problem were trained so that
the end number of features would be about
the same as the  x  features for a fair
comparison 


  results
   featurecomparison selfplay
results
we ran    games of crbm features vs       
template features   crbm won    of these
games whichisawinrateof       


furthermore  when we conducted the self

fiplay experiment with crbm features  the
agent was able to learn to consistently beat
its former instance in     iterations   this
seems to indicate that same td learning
process previously applied to enumerated
templatefeaturesinthestateoftheartcould
be applied in the same fashion to crbm
features 

unfortunately  because full convergence for
td in go can take on the order of       
games      we did not have the time or
computationalpowertotestthefullytrained
modelsforbothfeatures 

a comparison between randomly sampled
multinomial  fig     and realvalued kernels
 fig     shows that the multinomial kernels
are more sensitive to empty space on the
board thisfollowsfromthedefinitionofour
visualization   appropriately considering
empty space is important in go because the
primaryobjectiveistosurroundit 

   qualitativeanalysis



figure  randomlysampledrealvaluedbases 

c
b

d

a


figure  meaningfulshapeslearnedbycrbm 


in order to obtain some sense of what the
crbmlearnsineachofourformulations we
visualized the weights of the first layer 
visualizationwasperformedinseveralways
asthere is notsimply one configuration that
corresponds to a particular kernel  one
simple visualization we performed was
assigning the state at a particular point of a
kerneltothestatewiththehighestweightin
a given kernel  fig     qualitative analysis
suggests that the multinomial crbm learns
more meaningful features than the real
valuedcrbm 


figure  randomlysampledmultinomialbases 

from the visualization of the multinomial
crbmweights itispossibletomakeoutboth
common shapes and small  but meaningful
situationsingo forexample kernelainfig 
 showsasetofblackandwhitestonesthat
formaformationcalledapinwheel kernelsb
and c in the same figure show two
orientationsofamoveonblack spartcalleda
 peek   which is characterized by a stone
approaching a one space gap between two
stonesoftheopposingcolor finally kerneld

fiin the figure shows a partial eye shape  the
essentialshapethatcandeterminethelifeor
deathofentiregroupsofstones 


   winnerclassificationresults
theresultsofthewinnerclassificationgiven
a board are somewhat discouraging because
no feature set we learned from a crbm of
equivalent size to the baseline feature set
performed better  however  they show
definitively that training the crbm with
rotational
invariance
improves
the
generalization of the features to the task of
winnerclassification 




 conclusion futurework
although our results have not shown
definitivelythatcrbmfeaturescanbebetter
thanasetofnaivelyenumeratedfeaturesfor
high level play  there is hope that crbm
featurescanbettercovertheenormousstate
space in go  in our experiments  crbm
features were able to outperform
enumerated template features given the
same number of features and the same td
training algorithm and data  so it certainly
seemspossiblethatapplyingtheuctandtd
algorithms to crbm could improve
performance
of
the
stateoftheart
techniques 

furthermore  because expert players
evaluate the board for shape patterns which
can be decomposed into subpatterns 
application of higher layers of a deep belief

networkcouldcreatemoreeffectivefeatures 
also  instead of using the raw board as the
inputtothecrbm thevisiblelayercouldbe
augmented with easilycomputed gospecific
information at each position  such as the
numberoflibertiesorproximitytoanedge 

 sources workscited

  gelly s  andsilver d      combiningonline
andofineknowledgeinuct inghahramani z  
ed  icml volume           acm 

   lee  h   grosse  r   ranganath  r     ng  a  y 
        convolutional deep belief networks for
scalable unsupervised learning of hierarchical
representations  in bottou  l     littman  m 
 eds    proceedings of the twentysixth
international conference on machine learning
 icml    acm montreal qc  canada 

  schraudolph n n  dayan p andsejnowski t 
j  temporal difference learning of position
evaluationinthegameofgo inj d cowan etal 
eds   advances in neural information processing
systems            morgan kaufmann  san
mateo calif       

   silver  d   sutton  r     muller  m        
reinforcement learning of local shape in the
game of go    th international joint conference
onarticialintelligence pp           

   sutton  richard s   barto  andrew g  
reinforcement learning  an introduction 
cambridge ma usa mitpress      p    
http   site ebrary com lib stanford doc id    
      ppg     copyright        mit press 
allrightsreserved 

   tesauro g  tesauro  tdgammon  a self
teachingbackgammonprogram achievesmaster
levelplay neuralcomputation               

  http   www gnu org software gnugo 






fi