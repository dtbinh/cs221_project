favorites based search result ordering

ben flamm and georey schiebinger
cs     fall     

  introduction
search engine rankings can often benet from knowledge of users  interests  the query  jaguar   for example 
has dierent meaning for a british car enthusiast than for a paraguayan park ranger  here we implemented
a nave bayes classier and a cosine angle similarity metric to sort a set of documents by their relevance to
a  bookbag of user selected documents  moreover  we demonstrated that combining the outputs of these
algorithms with a support vector machine produces a better estimate of user relevance than either algorithm
does individually 
we used the ucsf legacy tobacco documents library as our source of documents    

this online

library contains more than    million documents released as a result of federal lawsuits against the tobacco
industry 

in an eort to overwhelm the library and its users  the tobacco companies ooded the library

with millions of documents  including folder covers  blank pages  and duplicate documents  researchers are
therefore presented with the problem of sifting through a vast number of  junk documents to nd references
that are suitable to their particular research topic  since this library is used by many researchers  and is
growing daily  adding relevance based search result ordering will allow the users to more easily discover useful
documents 

  data acquisition and feature selection
our rst step in acquiring data was to generate  bookbags  or lists of user selected documents with which
we will order the search results  we extracted three sample bookbags from the references of research papers
that cited documents from the legacy library           we also selected      random documents from the
library to estimate the baseline word frequencies of the library  we call the bookbags
of random documents

b    b    b 

and the set

r 

for each document  we retrieved the pdf version from the legacy website  stripped the text from the
pdf  and generated a list of tokens using the porter stemming algorithm 
we then combined the token lists for all of the documents  and found the token frequency distribution 
we selected the medium frequency tokens as our working set     eliminating tokens occurring less than   
times and more than      times total over the      documents 

we constructed a document frequency

matrix  d  whose ij th entry is the frequency of word j in document i 

  algorithms
our goal is to provide the user with documents relevant to their interests 
document

si  

from a set of search results

s 

a relevance score

d si   b  

to this end  we assign each

we compute this score with the aid

b   a set of documents that are relevant to the user  we observe  i e  train on  a portion of
b  b   and random documents  r  r  we train a nave bayes classier and vector space
these sets  and generate relevance scores dn b  si   b  and dv s  si   b  for each document si in the

of a bookbag
the bookbag 
classier on

held out documents  in our testing  we sorted the documents via an optimal linear combination of these two
metrics  which we computed with a support vector machine 

 

fi    nave bayes classier
we implemented a nave bayes classier using the multinomial event model and laplace smoothing  our
training set consisted of positive examplesb drawn from the bookbag
from a set of random documents
document

si  s

r

r

drawn

 representative of the baseline word frequency of the library  

for a

and a training collection of relevant documents

parameter as the ratio of 

b  

b

and  negative examples

we dened the nave bayes relevance

 the log probability that the document is in class    not in

probability that the document is in class    in

b    

b   

to  the log

i e 

dn b  si   b  

log p  si     
log p  si     

dn b values  this technique is called the probability
ranking principle  prp   this ranking method is optimal in the sense that it minimizes the expected loss

note that the more relevant documents have smaller

at k  where the loss at k is dened as the number of documents

bi  b

that are not ranked in the top k

positions  a proof of this fact can be found in     

    vector space classier
for our next classier  we treated each document as a vector in a high dimensional vector space and used
the cosine of the angle between two document vectors as a pair wise document similarity metric  that is 
 

p    
dt
  d 
is the euclidean norm of d 
i di
kd  kkd  k   where kdk  
for this vector space classier  unlike for the nave bayes classifer  we used a term frequency inverse

for document vectors

d    d 

we computed

document frequency  tf idf   normalization scheme for the token frequency matrix  as suggested in      the
term frequency is simply the number of times a token appears in a document  the inverse document frequency
is the log of    the fraction of documents the term appears in  

the two weightings play opposite roles 

term frequency rewards tokens that occur more often in an individual document  while inverse document
frequency down weights tokens common across many documents  which likely play little role in determining
the document s relevance  to generate the tf idf weight  we multiply the tf and idf weights for each token 
the ijth entry of

d

is then


dij   nij  log

m
mj



nij is the number ofp
times that word j appears in document i  mj is the number of documents
p
j appears in  mj   i   nij       and m is the total number of documents  m   j mj  
now  given two sets of documents b and s   for each si in s we dene the vector space relevance parameter
dv s  si   b  as the mean pair wise document similarity over the elements of b  
where

that word

  x bti si
dv s  si   b   fifi fifi
fib fi bi b kbi k ksi k

    svm combination of nave bayes and vector space scores
after computing the nave bayes  probabilistic  and vector space  non probabilistic  scores  we seek the
optimal linear combination of the two  for each document
space parameters

dn b  bi   b bi  

and

dv s  bi   b bi   

bi

in

b

we calculated the nave bayes and vector

we did the same for each document

ri

in

r 

then we

used sequential minimal optimization  smo  to nd the optimal margin classier separating the documents
in

b

from the documents in

r

in this two dimensional space  see gure    below  

one advantage of this svm combo algorithm is that it can be used to combine any number of ranking
metrics  here we demonstrated its use with two common ranking algorithms  but one could easily imagine
adding other ranking metrics such as the mean kl divergence from the documents in
vector space distance to

b

instead of the mean  etc 

 

b  

or the minimum

fifigure    above are the results of training on    documents from
from

r

 blue   we then tested on    documents from

and     from

r

b 

b 

 the green set  and     documents

 red      from

b 

 green       from

b 

 yellow 

 blue   on the left is a  d scatterplot of the documents in nb vs score space  each x

represents a document in the training set and each dot represents a document in the test set  the value on
the x axis is

dv s

and the y axis is

dn b  

if we were to draw an imaginary horizontal line at y    and ascend

vertically  picking points as we meet them  this would amount to ranking the documents by the nave bayes
parameter  similarly  if we were to draw an imaginary vertical line at x    and sweep left  this would rank
the documents by their vector space score  the dotted diagonal line shows the optimal linear combination
of the two parameters  calculated with a svm as discussed in section    the gure on the right shows the
functional margin of each point 

note the clear separation of the green documents from the rest  which

shows that we separate one set of bookbag documents from both random documents and documents from
other bookbags 

  results
we tested our nave bayes  vector space  and svm combination classiers on three bookbags of varying sizes 

b    b    b   

the rst set had    documents  the second     and the third had roughly       our methods

generated favorable results  particularly for the svm combination classier  which regularly returned more
than half of the held out bookbag documents at the top of the ordered results  below we discuss the results
for each method  examine learning curves for the various methods  and note several issues we encountered 

precision recall curve
from the ordered results returned by each algorithm  we generated precision recall curves 

 

fifigure    the precision recall plot for the test described in gure    our precision is   when our recall
is     meaning that the rst    returned documents are all true bookbag documents  similarly  since our
precision is     when our recall is    all of our    bookbag documents are in the top    search results 

learning curves
we generated learning curves by training each method on a varying number of documents  testing on a xed
number of documents  and computing the area under the precision recall curves 

figure    filearning
curves for dierent classiers  the x axis is the number of positive bookbag training
fi

examples  or

fi fi
fib fi 

the y axis is the area under the precision recall curve 

we notice several things in these learning curves  first  the svm combination is better than either the
nave bayes or vector space classiers individually  ignoring slight noise issues for the medium size bookbag 
second  the learning curve values increase only slightly as the positive training example size increases  this
is a good sign  indicating that even if users specify few documents  the classier returns relevant results 
third  the results for

b 

are much worse than the results for

b 

or

b   

this is probably because

b 

is a

much larger collection of documents and has a greater variety of topics  our algorithm does best with fewer
topics in a bookbag  see below   it also could be that the random examples that received high relevance
scores  see gure    for example  were actually relevant documents 

  conclusions and future work
we see that our svm combination classier does a good job of returning held out results for the compact 
single topic bookbags

b 

and

b   

our algorithm has the advantage of being independent of the size of the

 

fidocument library      million documents   however  the algorithm does not handle multiple bookbag topics
very well  so we naturally cast our collective eye towards more sophisticated methods 
there are a number of algorithms that address these issues that we considered but did not implement due
to time constraints  one simple solution  for example  would be to try running k means clustering  using
the cosine angle distance metric  on the document library to uncover about         clusters  one could then
assign high relevance scores to the search results that clustered with the bookbag documents 
another approach would be to try latent semantic indexing  lsi   i e  to do a singular value decomposition on the  very large  document frequency matrix 

d 

representing all the documents in the library 

to our knowledge  lsi has not been done on such a large dataset before 

however  it might be possible

dt d  r v   v    where  v   is the size of the
right singular vectors of d   and multiplication by d gives
value   we can then form a low rank approximation of d  

to compute the eigenvalues and eigenvectors of the matrix
vocabulary   the eigenvectors of

dt d

are the

the left singular vectors  scaled by the singular

this way of conducting lsi is independent of the size of the document library 
yet another approach would be to run latent dirichlet allocation  lda  on the entire library  or a large
representative subset  to represent each document as a mixture of latent topics  and then rank the search
results according to how many topics they shared with the bookbag documents 
it would also be interesting to tinker with ways to test and improve the feature vector  this might involve
more aggressively testing the eect of changing the lower and upper cutos on token frequency  we could
also use n tuples of letters or words as tokens  the problem with this is that the documents in the legacy
library have been ocred  which injects a lot of noise into the data  were we to look at these features  we
also did not include metadata such as author  publication date  or document title  this would likely be a
straightforward addition to our svm combination classier 
favorites based search result ordering has applications beyond the legacy tobacco documents library 
we believe this algorithm can be used to rank results for web search based on user preferences  in a manner
similar to amazon com s related product display 

references
    legacy tobacco document library  http   legacy library ucsf edu 
    ripley  b  d        pattern recognition and neural networks  cambridge university press 
    cortese dk  lewis mj  ling pm  tobacco industry lifestyle magazines targeted to young adults  journal of adolescent health      sep                 http   dx doi org         j jadohealth             
    campbell rb  balbach ed  building alliances in unlikely places  progressive allies and the tobacco
institute s coalition strategy on cigarette excise taxes  am j public health      jul                 
http   dx doi org         ajph            
    proctor rn  the golden holocaust  unpublished manuscript
    manning cd  raghavan  p  schtze h  introduction to information retrieval  cambridge university
press        http   nlp stanford edu ir book pdf irbookonlinereading pdf  ch     

 

fi