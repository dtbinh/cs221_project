online learning for url classification

onkar dalal
    lomita mall  palo alto  ca       usa

onkar stanford edu

devdatta gangal
yahoo       first ave  sunnyvale  ca       usa

devdatta gmail com

abstract
this work consists of a review of online algorithms for url classification followed by
some extensions and tweaking of these methods to make them efficient in terms of computational time and memory  we found out
that the trade off in error for these extensions are fairly comparable for some of these
algorithms  we also applied two more kernel
based methods namely forgetron and projectron 

   introduction
in this project  we study the online algorithms for classification of the urls as malicious or benign  we analyze and reproduce the earlier work on this and then
extend the same and analyze the change in error for
these extensions 
    outline of report
we begin with the previous work on this data in section    given the enormous size of the data and number of features  we have focused on applying feature
selection and forgetron like techniques to these methods  in section    we give our methods for restricting
the size of feature set for the   methods  it also talks
about other kernel based algorithms  forgetron and
projectron and our modification to forgetron  the
report ends with conclusion and references 
    data
the data was taken from the uc irvine ml repository  it is a multivariate  time series data consisting
of approximately     million data points with     million attributes per point  the data is spanned over
    days and the urls come with a label    or  
based on whether they are malicious or benign  one of

the important features of the data is sparseness which
compensates for its large size 

   previous work
this section is reproduction of the work in      we
studied the four algorithms suggested by them and reran them to match their results  this was essential
to understand of the algorithms and further suggest
extensions in section   
    basic algorithms
we study six algorithms for online classifications i e 
a system in which we receive pairs  xt   yt   at every
point t  such that xt is the input feature vector and
yt          is the actual classification of the data
point at time t  however  we do not get the label
yt before we have to classify it using the old data
 x    y      x    y           xt    yt     where each xt is a
feature vector and yt          is the label  the
label prediction is given by ht  xt    which for linear
classifiers is ht  x    sign wt  x   where w is the
weight function 

      online perceptron
this is a classic linear classifier that updates the
weights in following manner for every misclassified
sample 
wt    wt   yt xt

   

this is a very simple  fast and memory efficient algorithm but all misclassifications are treated similarly in
this algorithm 
      logistic regression with stochastic
gradient descent
the stochastic gradient descent  sgd  is perfectly
suited for online applications  here sgd is applied to

filogistic regression to optimize the log likelihood function with following update rule for parameters 
wt  

lt
  wt   t xt
 wt   
w

   

where lt is the log likelihood for sigmoid distribution
 and t   yt      wt  xt   is the difference between
the actual and the predicted likelihood that the label
is     the training rate  is kept constant  this
update is similar to the perceptron  but the training
is proportional to t and the model is updated even
if the prediction is correct 

    reworked results
the error percentage calculated at the end of each
day is plotted for the   methods  we see the cw is
the best algorithm among the four  pa is marginally
better than logistic regression  perceptron being the
most efficient happens to have the maximum error
among the four  the table below gives the value of

      passive aggressive  pa  algorithm
in this algorithm  the following optimization problem
is solved for each sample to minimize the change in
model     
wt    arg minw   wt  w   
   
s t 

yi  w  xt     

if the prediction is below some confidence level   i e  
yt  wt  xt       we update the parameters as follows 
wt    wt   t yt xt

figure    errors for the four algorithms

   

t  wt xt  
      the use of confidence
where t   max   y  x
 
t   
measure while classification makes this algorithm better in practice 

the error towards the end of the experiment by which
the algorithms have learned the model 
algorithm

error

      confidence weighted cw  algorithm
as given in      in this algorithm  we maintain the
weights and a confidence measure on them  as opposed
to confidence on classification in pa   the weights wi s
are modeled as gaussian random variables with mean
i and variance i   at each step we make minimal
change to the model to classify xt with probability  
this is equivalent to minimizing the kl divergence between the gaussians under the constraint of confidence
measure 

online perceptron
logistic regression sgd
passive aggresive
confidence weighted

          
          
          
          

   extensions
as extensions to the previous work  we have looked
at following algorithms and variations of earlier algorithms in this project 

 t     t      arg min    dkl  n       n  t   t   
s t 

p
yi    xt         xtt xt

   
where  is the c f d of standard normal  this corresponds to the following update rule 

 restrict the size of weight vector to consider only
the valuable features and see the changes in errors
for the four algorithms described in section  

   

 forgetron  a kernel based algorithm  which stores
the examples with a fixed budget on number of
support vectors    

since every feature here is treated differently in terms
of confidence  the features with less confidence are updated more aggressively  however  the heavy computation per update makes this slower than the rest 

 projectron  a kernel based algorithm  which
checks the projection of new data point onto support set and update only if it is sufficiently off
   

t  
t  

 

 t   t yt t xt
   
 t     t  ut
diag    xt  

fi    correct feature selection  limited w 
      motivation
since each of the url has over     million features  we
want to store and utilize only the important features 
our motivation for limiting the memory usage  by limiting size of w  comes from the applications like web
search where classification needs to be done in minuscule times  another application for quick  memory efficient online classification would be to decide whether
a user is a bot or not  it is important for the business to know which top features are shown by bots in
contrast to real people 
      growth in number of features
we also looked at growth of the feature set for each
of the four algorithms  the results are represented in
fig    as expected we can see that perceptron quickly
updates all the features as compared to linear growth
in lrsd and very sub linear growth for pa and cw 

figure    growth in size of feature set with time

      budgeting the features
for all the four algorithms  we restrict the features selected to a budget b  we implement this budgeting by
choosing only the top b features where the w vector
has changed the most  we run our budgeting after
every update  we observe that by setting the number
of features to a budget b           of the total features in the data  the accuracy is still comparable to
otherwise  the results of this experiment for perceptron  lrsd and pa are plotted in fig    the results
for cw are plotted in fig   
      observations
 although perceptron uses most of the features 
if we restrict the number of features  the error

figure    error dependence on budget b for perceptron 
logistic regression and passive aggressive

fichange for b        or      is about    
 even though pa uses less features            with
a budget of      features  the performance goes
down for pa and lrsd by about     
 restricting the features in cw decreases the performance of the algorithm by a substantial degree
          this could be because cw not only
uses the weight of the feature  but also the confidence in the feature as an important measure of
how important that feature is 

figure    comparison of actual forgetron  dotted 
and simplified forgetron  solid  for b     blue  
b     green   b      red   b      black 

    projectron

figure    error dependence on budget b for confidence
weighted

    forgetron
perceptron becomes very effective when used along
with kernels  but the kernel computations are costly
and memory intensive  forgetron is a kernel based
online learning algorithm with a budget on the kernel size  forgetron assigns heavier weight to recent
data points and lighter weight to old data  the exact
algorithm is given in      however  there are few disadvantages to the forgetron 
at every update  forgetron reduces the weight of the
previous stored examples in the kernel  slowly for
large b  after many updates  the weights of the very
old stored example vectors become negligible  this
makes the updates slower and does not add too much
improvement in the errors  re weighting process is
expensive we have tried a simplified version of forgetron which stores latest b misclassified examples
and weighs them equally  this runs     times faster
and requires less memory  and for b        forgetron stores the old samples but their weights gradually become negligible  after       days  oue simplified version performs better than forgetron  the
results for these for different values of b are given in
fig   

another kernel based algorithm we applied was projection  in this algorithm  whenever a data point is
misclassified  we check if it lies in span of existing support set and update the weights  if however it does
not lie in the support set  it gets added to the support  the details of this algorithm are in      the
main disadvantage of this was the magnitude of calculations required  this was the slowest of the six
algorithms and the errors for linear kernel were worse
than perceptron  the results are given in fig   

figure    projectron vs perceptron

   conclusion
we started with reproducing the results in      as
expected perceptron is the most efficient  but low on
accuracy and cw even if slower compared to the rest 
has the best accuracy  we observed that algorithms
like perceptron and lrsg use almost all the features

fihowever pa and cw add new features at a sub linear
rate  to exploit this fact  we restricted our feature
size to a small but significant subset        of the features  we observed that perceptron works with same
accuracy for restricted weights  however  the errors in
cw increase substantially  in the second half  we applied forgetron and projectron for classification  we
found that the errors in forgetron and projectron are
worse than perceptron  this can be attributed to the
loss of information in both the algorithms  the algorithms also turn out to be slower than the other four
because of the kernel calculations  however  the simplified forgetron works very well for moderately high
budgets  b        

references
    k  crammer  o  dekel  j  keshet  s  shalevshwartz  and y  singer  online passive aggressive
algorithms  the journal of machine learning research              
    o  dekel  s  shalev shwartz  and y  singer  the
forgetron  a kernel based perceptron on a fixed
budget       
    m  dredze  k  crammer  and f  pereira 
confidence weighted linear classification  in proceedings of the   th international conference on
machine learning  pages         acm       
    j  ma  l k  saul  s  savage  and g m  voelker 
identifying suspicious urls  an application of
large scale online learning  in proceedings of the
  th annual international conference on machine
learning  acm new york  ny  usa       
    f  orabona  j  keshet  and b  caputo  the projectron  a bounded kernel based perceptron  in
proceedings of the   th international conference on
machine learning  pages         acm       

fi