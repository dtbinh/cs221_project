an adaptive agent for no limit texas hold  em poker
david osteen
daost stanford edu

cs    project
abstract
creating a computer agent to effectively play the game of no limit texas hold em is a formidable
challenge for machine learning researchers  and to do date  existing techniques have failed to create an
agent that is competitive with skilled human opponents  an effective agent must be able to assess the
game state based upon incomplete information  model an opponents playing style  and make economic
decisions based upon these factors and random future events  some researchers have had success using
evolutionary neural networks in this setting  and i extend this approach by implementing more
specialized opponent models  rather than training one neural net to play against any random opponent
type  i train separate models that are each specialized to play against a number of basic  hard coded
playing strategies  i find that the combination of these models into a final type performs against random
opponent types as well as the average of the static types 

i  introduction
in the game of no limit texas holdem  nlh   a players hand consists of the best five cards between his
two concealed cards  hole cards   and five cards shared among all players  the board   there are four
betting rounds  an initial round when each player has seen their hole cards  another after three shared
cards have been revealed  the flop   the next after one more shared card  the turn   and the final round
after the last shared card is revealed  the river   unlike the limit variant of the game  nlh has no
restrictions on maximum bet size  so a player is always at risk of losing all of their money  stack  when
playing against an opponent with an equal or greater sized stack  this feature of the game changes the
strategy significantly  it is very difficult  and usually ill advised  to bluff in lh  but in nlh  it is a
necessary component of any good players strategy  effective play becomes critically dependent on an
assessment of the opponents playing style 
with this in mind  i extend the approach employed by nicolai and hilderman      who use evolutionary
neural networks to train a nlh agent  they model opponents with two aggressiveness features which
calculate scores based on players full or recent history of actions  fold  check  or bet   while this is a
sensible feature type  they acknowledge this component of their model could be improved 
to that end  rather than modeling opponent playing style as a feature  i train a separate neural network
for each of a number of hard coded playing styles  i then combine those models into a final model 
which uses a weighted combination of those models  with weights calculated as the relative distance
between an opponent s playing style  and that of each of the static classes 
this extension has limited direct application because it is dependent on an artificial and simple
classification of opponents  but it is my hope that this approach can be extended by training against
more complex opponents  e g   other no limit bots  with distinct playing styles  given a set of bots that
roughly simulate the archetypal playing styles of real players  i believe my approach could yield an agent
that is more likely to be successful against human players 

fiii  approach
i use a number of different modules to generate the final adaptive agent  the first is the player class 
which can be a hard coded player  or an adaptive agent  i use five hard coded types  a random player  a
player that always calls  a player that always raises  and then two more sophisticated types which are
meant to roughly simulate tight aggressive  tag  and loose aggressive  lag  players 
the primary model for the artificial agent is a fully connected feedforward neural net  with one hidden
layer  there are four features used as inputs to the neural net  shown in table   below     nodes in the
input layer     nodes in the hidden layer  and   outputs  the outputs correspond to the different actions
a player can take  and they include folding  calling  or making a bet in one of four different ranges 
relative to the pot size 
table    inputs to neural net
feature
description
win probability
  of wins  given the game state
  opponent bets
  times an opponent bet this stage
stage
pre flop  flop  turn  river
position
position relative to dealer

the parameters are learned with an evolutionary algorithm  i begin with       players and select the
parameters from a uniform random distribution  with range          each player then plays against the
selected static opponent     times  during which time data is collected about the opponent s observable
 style   and also about the player s likelihood to win  given a certain game state  table   shows the
playing style features  these become useful when trying to classify an opponent of an unknown type 
there are certainly other observable features which could be useful here  but given the simplicity and
consistency of the static player classes  these features are sufficient to obtain a decent classification 
table    features for playing style
feature
description
flop 
percent of flops seen
turn 
percent of turns seen
river 
percent of rivers seen
preraise 
percent of raises before the flop
flopraise  percent of raises on the flop
turnraise  percent of raises on the flop
riverraise  percent of raises on the flop

table   shows the representation of the game state for the opponent model  the number of
occurrences of each state is counted  as is the number of times the opponent has the better hand at that
point  this data is used later to calculate the win probability  which is in turn used as an input into the
neural net  this is a fairly coarse representation of the win probability  and it is stage specific  for
example  if the opponent bets preflop  that will have no effect on the win probability estimate on the

fiflop  turn  or river  the primary motivation for such a simple probability estimate was to keep the game
state space manageable  given the amount of time required to train a model with this evolutionary
technique     hours per     generations   calculating reliable estimates for a large state space would
have required a prohibitive amount of time 
table    opponent game state discretization
feature
description
opponent s hand strength
e g  pair  full house
  opponent bets
  times an opponent bet this stage
stage
pre flop  flop  turn  river
position
position relative to dealer

after collecting this data for the opponent model  i then play each of the       artificial agents against a
static opponent       times each  each player begins each hand with     times the big blind amount  a
fairly standard buy in for no limit   the earnings for each hand are summed  and after all agents have
finished playing  i select the top    agents with the highest earnings and discard the rest  with the   
remaining  i create    children agents for each  which are generated by mutating a fraction of the
parameters according to a n      distribution  i then generate     more opponents with randomized
parameters  and the process repeats for     generations  the best model from the last generation is
selected as the final model for that class of static player 
after models have been selected for each of the static playing classes  i combine them into a final model 
this model estimates an opponent s class of play by measuring the distance of an opponent s observable
playing style features to those recorded for each of the static classes  each of the outputs for the static
player types are weighted according to this distance and summed  the action with the highest sum is the
final output of the model 

iii  results
figure   shows the results for each of the static player classes  the results are measured as the total
amount won  divided by the total number of hands played  divided by the big blind amount  to normalize
for different stakes   unsurprisingly  the agents selected for the  dumb  models  caller  raiser  random 
performed significantly better than the agents that played opponents with basic strategy  tag  lag  
generally speaking  the results are passable  albeit against weak competition  and still not comparable to
what a human would achieve against these opponents  the returns per hand per big blind for the
overall final model were       which is a little less than the average of the static types 

fiiv  conclusion
while the final poker agent is still not competitive with a human opponent  i believe this approach could
be significantly improved  the estimates for win probability are very crude  and there are many
enhancements to the evolutionary algorithm which could improve its performance  in addition  it would
be possible to create more agents by training against the models i developed for the static classes  by
increasing the pool of player types  it is possible overall performance against more difficult opponents
could improve 

v  references
    g  nicolai and r  hilderman  no limit texas holdem poker agents created with evolutionary neural
networks  in      ieee symposium on computational intelligence and games  pages          

fi