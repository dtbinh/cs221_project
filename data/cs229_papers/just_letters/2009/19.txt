relational rss clustering techniques
richard roesler
stanford university
rroesler stanford edu
   introduction
there has been an explosion in the amount of available news and current event
information on the internet     hours news outlets and independent bloggers alike flood the wires
with a constant stream of data  though an increasing number of people rely on the internet as
their primary source of news and current events  it is becoming increasingly difficult for users to
find what they are looking for  real simple syndication  rss  provided a means to bring the
data directly to the user through easily parsable xml feeds  of course this only changed how
users looked for news  not how much news there was 
rss aggregators  likes google news  were created to learn a users preferences and only
display rss stories that a user would like  google news  in particular  has found a great deal of
success applying algorithms from the text clustering community toward rss data  users are able
to choose a story and the search engine provides a list of related stories that the user  hopefully 
finds interesting  the potential applications of rss clustering  however  go well beyond simple
aggregation 
in the future  semantic web technologies may be able to actually extract knowledge from
a stream of rss feeds  i e  find similarities  detect patterns  and infer things that the best human
analysts cannot detect  this type of application is well beyond the scope of this project  instead 
we focus on aspect of such an application  sorting large amounts of rss data  we present here
the results of applying standard clustering techniques to the rss problem  and an analysis of how
well these techniques work 
in section   we describe some of the unique features that separate rss clustering from
other text clustering applications  the methods and results of our application are presented in
section    along with an extensive analysis of these results  we finish with some suggestions to
improve future applications 
   problem setting
as a subset of the larger text clustering problem  rss clustering faces many of the same
challenges  the number of documents available to cluster and the high dimensionality of their
representation make text clustering an unwieldy problem  beil  ester  xu   feature vectors with
tens of millions of elements would not be uncommon in such situations  the ambiguity of human
language further complicates things  it is frequently difficult to separate semantic similarity with
syntactic similarity  any rss clustering application would clearly have to address these
problems  along with two further complications introduced by the nature of online news media  a
rapidly expanding dataset and real time search requests 
major news agencies are able to literally make up to the minute updates to their rss
feeds  not only do we have to deal with an extremely large dataset  but also with a rapidly
expanding dataset  any rss application needs to be frequently updated to include newly posted
stories  in addition  most rss clustering applications need to be able to handle real time user
search requests  having to run an hour long clustering program will be inadequate in most cases 

fi   methods
    data collection and storage
in order to make our experiment reflect real world conditions  we decided to collect our
data sample from real rss feeds from a wide variety of sources  over the span of three weeks 
we accumulated       rss articles on topics including sports  politics  science  entertainment 
business  and headline events  appendix i   for each article we stored the headline  the article
summary  and the full text of the article in our database 
we relied on the traditional bag of words approach to modeling our feature vectors  we
represent each article as a set of words contained in that article and the frequency that word
appeared 
the difference between any two documents can then be calculated as the ordinary euclidean
distance between those documents  the feature matrix of all documents is then constructed with
each row representing a document and each column the frequencies of word j 
    preprocessing
as is typical in text clustering problem  we needed to perform a number of preprocessing
steps on our feature vectors before we could begin  at first this was limited to the removal of all
symbols  html tags  numbers and unwanted characters  words like dont simply became
dont  and hyphenated words were separated 
initial clustering attempts turned out poorly  however  as common words  the  i  he 
etc   dominate the less frequently used  but more semantically important words  thus  we added a
preprocessing step to remove these context free stopwords  at the same time  an implementation
of porters stemming algorithm     was to reduce the remaining words to their basic stems  this
ensures words like working and work are clustered as the same word  in the end  each
document was represented as a        word vector 
    k means algorithm
    showed that k means clustering far outperforms hierarchical techniques in the realm of
text processing  while some improvements have been suggested to k means  such as bisection kmeans      we decided to use the ordinary k means on order to make use of efficient clustering
software  in order to try a variety of parameters  clustering was performed with cluster sizes of
k                and     each time being run to convergence at least    times 
we tested three different distance measures for the clustering algorithm  ordinary
euclidean distance  pearson correlation  and spearmans correlation  through experimentation
we found that the euclidean distance provided the most logical clusterings and was used for all
our testing  although not tested here      also suggests using the cosine angle to determine the
difference between stories 
    evaluating accuracy
the difficulty in characterizing the accuracy of text clustering applications is that the idea
of right and wrong is ambiguous  using a set of data that did not belong to some well known
database  like the reuters corpus  meant we had no standard with which our results could be
compared  in the face of this  we define the concept of cluster coherence to qualitatively speak
about solution quality 
we define term coherence as a measure of how tightly a specific term  or group of
terms  is clustered  regardless of context  for example  with k     over     of the articles

ficontaining the name obama appeared in one cluster  we say that this cluster has high term
coherence  for news stories  this corresponds to how well a cluster represents a theme or genre 
story coherence refers to how well a cluster represents a specific story  with k    we
found that the name obama was split into   of different clusters  but each cluster represented a
different series of events  one cluster  for example  contained     of the stories involving
president obamas policy on afghanistan  while another contained     of the stories about
president obamas visit to japan  these clusters have high story coherence at the expense of term
coherence 
we represent this visually in figures  a and  b  each color represents the number of
times obama appeared in a cluster  we see that for k    that the vast majority of occurrences
appeared in a single cluster  representing a high term coherence  likewise  for k     we see a
larger number of clusters  but looking at the headlines of each would reveal each cluster to be
more topically correlated 
in order to get a more quantitative view of accuracy we will use two more measures  the
first is ratio of the maximum cluster size to the mean cluster size  solutions which are able to
distribute data into a number of medium sized clusters is more desirable than one which creates
many tiny clusters and a few extremely large clusters  the second is the norm of the meansquared error for any story in a cluster  the further away a story is from the average story the
less likely it is to fit in that cluster 

figure  a and  b  a smaller number of clusters results in higher term coherence  while larger
numbers of clusters yields higher story coherence 

   results
after running the k means algorithm at a variety of clusters sizes we found that k    and
k    generated the results with highest coherence  from the    clusters we identified ten as
belonging to one of the categories  politics  crime  sports games  and entertainment  of those  we
noticed that three clusters corresponded strongly to a specific story  one cluster had     of the
stories about the fort hood shooting  another had      of the stories about hurricane ida  and
the third had     of the stories about the musician rihanna and her domestic abuse case  that is
to say     clusters give us a solution with high term coherence  but low story coherence  when
we increase the number of clusters to     we get a very different result  we found    of the   
clusters to have discernable categories  with    of those representing specific stories  in other
words     clusters yield a solution with high story coherence  but lower term coherence 
from this we can say that the number of clusters roughly corresponds to the resolution
of our solution  fewer clusters represent higher level groupings with high term coherence  while
more clusters result in a more story centric view of events  this fact could be leveraged in future
applications by first breaking the problem into larger themes and then breaking each theme into
individual stories  through reapplication of the clustering process  an interesting result was also
noticed in our above results  stories which had highly repeated  very unique terms  like
hurricane ida could be correctly clustered even with small numbers of clusters  so it may not
always be necessary to cluster with large k to get results about a specific story 

fiunfortunately  both cases above suffer huge polydispersity in the cluster size  in fact  for
k    the largest cluster is     elements  that is    times the average cluster size  the reason for
this is simple to understand  most stories will fall within some average range of our vector space 
features vectors that lie far outside the average are easier to separate than those within  thus  as
k means runs  each cluster centroid is pulled toward a group of stories on the extremities  all the
remaining stories in the center end up grouped into one large cluster 
the obvious next step was to improve our results by normalizing all our vector data  and
thus reducing the effect of magnitude on our clusters  again  we ran k means with k    and
k     this time on our new normalized data  immediately we saw that cluster sizes were more
consistent  in fact  the largest cluster size for k      was only       we found only    of the   
clusters had discernable themes  a decrease of        what is interesting  however  is that the
clusters without strong correlations still were better  on average  than those in the non normalized
set  what we saw is that clusters contain two or three different unrelated stories  but each story
was very strongly grouped 
in general  errors in clustering solutions can be fixed if they can be detected  for the
relatively small amount of data we used this could be performed by hand  but for any realistic
problem this needs to be done automatically  calculating the mean squared error for each story
proved to be a good way to detect how poorly a story fit in a cluster  if a story had an mse
significantly greater than the others in the cluster  we found with good probability that this story
did not belong in the cluster  iteratively removing the stories with high mses provides a simple
method for finding cluster quality 
   conclusions
there are a number of promising avenues that should improve results significantly 
particularly research focusing on comparing semantic meaning  instead of just word frequencies
has given good results      has used the wordnet ontology to find synonym and hyponym
relationships between documents  applied to rss clustering  however  this may not provide the
most significant difference  we found that most articles prefer using the same common words 
instead of using synonyms  what may prove more useful is the work by     into leveraging the
taxonomic structure of wikipedia  wikipedia has the advantage of being frequently update with
current events and people  which are likely to show up in rss feeds and links between articles
typically denote logical links between elements  thus  articles about pork barrel spending
could be related to government finance and the      us elections  instead of with stories on the
meat industry 
one problem that still needs to be addressed is what t do when new articles are added 
our tests with only       articles took up to an hour an a half for large values of k  being useful
in an internet setting would require near real time results  which cannot be achieved by reclustering every time a new article in introduced  instead  we could cluster a large set of data
once and then calculate the probability a new story fits into each of the clusters  if we treat our
cluster assignments as correct labels for a training set  this could be done with any supervised
learning technique 
we have seen that even using the most basic techniques good reasonably good results
could be achieved  defining a set of quantitative and qualitative measures to speak about
accuracy allowed us to find a number of places where improvements could be easily made  as
work proceeds in the field of text clustering we can expect even greater improvements in both
accuracy and speed 

firesources 
    beil  f   ester  m   and xu  x        frequent term based text clustering  in proc  of the eighth acm
sigkdd international conference on knowledge discovery and data mining  edmonton  alberta 
canada  july                 kdd      acm  new york  ny          
    hu  j   fang  l   cao  y   zeng  h   li  h   yang  q   and chen  z        enhancing text clustering by
leveraging wikipedia semantics  in proceedings of the   st annual international acm sigir
conference on research and development in information retrieval  singapore  singapore  july         
       sigir      acm  new york  ny          
    andreas hotho  steffen staab  gerd stumme         ontologies improve text document clustering  
icdm  pp      third ieee international conference on data mining  icdm     
    porter m       an algorithm of suffix stripping  cambridge england 
    steinbach m   karypis g   kumar v        a comparison of document clustering techniques 
proc  textmining workshop  kdd 

acknowledgements
cluster     by michael eisen of stanford university 
http   bonsai ims u tokyo ac jp  mdehoon software cluster software htm
stop word list from the journal of machine learning  mit csail 
http   jmlr csail mit edu papers volume  lewis  a a   smart stop list english stop
appendix i  rss sources
mtv entertainment and music news  http   www mtv com rss news news full html 
reuters political news  http   feeds reuters com reuters politicalnews 
reuters science news  http   feeds reuters com reuters sciencenews 
reuters technology news  http   feeds reuters com reuters technologynews 
bbc business news  http   newsrss bbc co uk rss newsonline world edition business rss xml 
espn sports news  http   sports espn go com espn rss news 
cnn top stories  http   rss cnn com rss cnn topstories rss 
cnn entertainment news  http   rss cnn com rss cnn showbiz rss 
bbc headline news  http   newsrss bbc co uk rss newsonline world edition front page rss xml 

fi