finding optimal hardware configurations for c code
john clark  ilana segall
 jpclark  isegall  stanford edu
december         

 

introduction

to look at a piece of c code   especially an uncommented one   is often not very enlightening 
even small pieces of code with one or two functions will often appear to perform the same
array initializations  loops over the data  and basic arithmetic functions  without clearly
indicating what the overall purpose of the routine is  this can be frustrating to those using
the code as part of a package who are forced to understand it as a black box  without
completely understanding its properties  more specifically  attempting to find the most
optimal way to run the code becomes a very difficult problem if there are  for example 
several different choices of harware to choose from  and the actions of the program are a
mystery 
in this problem  we choose to attack a tractable subproblem that we hope will be
helpful for deciding how to run this software for a user with several different hardware
options  specifically  we have observed that most single code files perform one of two
functions  data manipulation and computation  we hope to be able to successfully classify
c code files into these two main categories to allow a user who has not been informed of
its functionality to quickly determine its core purpose  and use this information to enhance
performance of the overall project 
we choose to look at the assembly representation of these files  whose simple structure
and more limited vocabulary we believe contain the key clues to functionality in order
to perform this classification  generally  the conversion to assembly is done behind the
scenes  and only a small percentage of programmers actually feel comfortable reading any
particular type of assembly and understanding what it does  so we believe that assembly
is underutilized in understanding codes functionality  thus  we also hope that classifying
code in this manner will provide a strong argument for using assembly to extract features of
code for use in future machine learning applications pertaining to c or other programming
languages  and eliminate the need to look at assembly directly 

 

data

the data for our learning algorithms consists of c code files from one of the two groups     
   data manipulation algorithms and         computational algorithms  we hypothesize
that a learning algorithm able to distinguish well between code files from these two canonical
 

figroups would be also able to categorize code files with respect to their expected performance
on a hardware platform  we acquired the code files used in this project from online c
repositories and the cblas library  the data manipulation code consists of searching and
sorting algorithms including algorithms such as breadth and depth first search  dijkstras
algorithm  the floyd warshall algorithm  mergesort  and bubblesort  many of which have
several samples created to perform the algorithms over different types of data structures 
the cblas library is a c interface to the blas routines  which contain many low level
linear algebra producures such as matrix matix operations and operations on sparse vectors 

 

methods

we chose to use the assembly instructions of each code file as the features for this project 
for each of the code files we produce the corresponding assembly language file by compiling
the code file with the command gcc  s  this produces a  s file which is the code compiled
but not assembled or linked  each line of the assembly file has the format 
hcommandihargumentsi  we chose to use only the assembly command  the first token
of most lines  since they carry the most information about the operation carried out by
each line 
once each code file was compiled into assembly we wrote a simple parser to extract all
of the commands and their frequency of appearance in each code file  the parser opens
and reads each file storing in the vector tokenlist commands which have not been seen
before  this gives us in tokelist a list of all commands that appear in all the files which
can be used as our dictionary 
a beneficial aspect of using assembly commands as the features of our learning algorithm
is that that they are extremely regular tokens in the code files  there is no preprocessing
necessary  each command appears exactly the same way every time it occurs in the file 
also since the commands are non ambiguous  unlike with natural language tokens  the
meaning of each command is completely contained in its name  in contrast  a negative
aspect of using assembly commands as the features of our learning algorithm is that there
are not very many of them  our token list taken from     algorithmically very different
code files contained only    tokens  this is a relatively small number of features from which
to gather information  though in our case we found this number to be sufficient 
because we are performing a binary classification  we chose to compare results from
clustering  naive bayes  and a support vector machine  we chose to use an unsupervised
learning algorithm  k means clustering  to see if the code examples divided themselves into
two groups different than the ones we had originally imagined  which could provide insight
into our categorizations 
with naive bayes  we chose to approach the problem two ways  we used the standard
naive bayes model  using a feature vector with every assembly command represented as
a   or    where   indicates a features presence in the code example  we then also used
the multinomial event model to see if the code classified better as text   specifically  if the
number of times a command appeared in code was more informative than its presence  for
both methods  we used laplace smoothing and a custom  add          smoothing to avoid
    errors but reduce the probability of an event we have not yet seen   especially in the
case of the multinomial event model  there are relatively few occurences of each command
 

fiin every document  and we want to preserve accurate multinomial probabilities as well as
we can 
finally  we implemented a support vector machine to test the linear separability of the
data  we used a simplified version of the smo algorithm  which converged quickly enough
that we did not feel the need to include all the optimizations in the full algorithm 
all of the tests described above were performed in matlab 

 
   

results
clustering

because clustering does not have an explicit error metric and it would be very difficult
to visualize our data  as the feature vector is in    dimensions   we examined both how
the code divided into the two clusters and the composition of each cluster  for k      we
found that  of the computational code  about     was in cluster    and     in cluster
   while for data manipulation code  about     was in cluster   and     in cluster   
alternatively  this means that cluster   composed of     is computational code and    
data manipulation code  while cluster   contained     computational code and     data
manipulation code  clearly  clustering was not an effective algorithm to divide the code
samples into two groups 

   

naive bayes

in the tables below we show some of the estimated generalization errors calculated by cross
validation of each naive bayes algorithm  we performed two types of cross validation 
k fold with k     or    and leave one out cross validation  we ran these cross validations
for all four of the below described versions of naive bayes  and ran each once with all of
our files and again with a subset of the computational files  because we had more than four
times as many computation files as data manipulation ones  we wanted to assure that the
algorithms performed robustly if the amount of each was even   figure   below displays
the best of the results from this analysis 
clearly  all algorithms performed extremely well in this analysis   several checks were
performed on the implementation of the algorithm to ensure that such low error was indeed
calculated correctly  interestingly  very little difference was seen between the standard naive
bayes and multinomial event model error  indicating that the presence of an instruction in
the code was as important as the frequency of its occurence  we also see that the custom
smoothing we performed did not create a better alternative than laplace smoothing  though
because all tests returned such excellent results  we would have to examine its performance
on a different data set to conclude its effectiveness  also  we found a slightly lower error
when we used our full data set  although the difference was negligable 
figure   below shows the top five tokens that strongly indicate classification into one of
the two categories  these results agree with what we would expect from such algorithms 
especially the presence of the streaming simd extensions in the computation category 
which are used for parallelization of a large number of computations 

 

fifigure    estimated generalization error for the naive bayes classification models  these
estimates are taken using both k fold cross validation and leave one out cross validation
 loocv  

figure    these are the top five assembly instructions which most strongly indicate that
a code file is either a data manipulation type algorithm  left column  or a computational
type algorithm  right column  

 

fi   

svm

using k fold cross validation with k     and an equal number of data manipulation and
computational files on our svm  we obtained an error of         which is also excellent 
this is an interesting result considering how poorly the data clustered  strongly indicating
that supervised learning is necessary for this classification  this indicates that though a
separating hyperplane does very well in separating the data  the functional margins of the
training examples are not very large relative to their distances from each other 

 

conclusion

both naive bayes and support vector machines appear to be excellent algorithms for classifying our code  and performed much better than the unsupervised k means clustering  the
presence of such low error in both trials indicates that the feature set was extremely indicative of the classification we desired  despite the fact that it is difficult to ascertain what
assembly commands will be needed by looking at the direct c code  assembly commands
appear to make very powerful features for code classification 

 

future work

the original purpose of this project was to be able to determine the optimal hardware configuration to optimize the runtime of an unidentified piece of code  due to several technical
issues  we were unable to get a hardware simulator  simplescalar  to import the code we
were interested in using as our training set  we do believe  however  that our classification
could be used to help make this determination  perhaps by using its designation as data
manipulation or computation as a feature  in the future  using simplescalar would be an
excellent way to create the training set for such a project 
though our feature set was sufficient for this problem  future work in this area should
also look into ways of encoding the location in the code file of the assembly command as well
as its frequency of occurrence  we feel that this would provide another piece of information
which would shed even more light on the algorithm behind the code  especially if a more
complicated problem  such as the one above  is being explored  many other features are
available from the assembly code  such as the number of registers used and the arguments
of the instructions 

references
    cblas library  http   www gnu org software gsl manual htmln ode gsl  cblas 
library html 
    happy codings  http   www c happycodings com  

 

fi