feature selection methods for svm classification of microarray data
mike love
december         

svms for microarray classification tasks
linear support vector machines have been used in microarray experiments to predict a certain class
of a sample tissue  e g  healthy or tumor  using the mrna expression of different genes  in measuring tens of thousands of genes  a microarray dataset includes many genes that are uninformative
with respect to the classes  finding subsets of genes for the svm to train on could help improve
the generalization of the algorithm and reveal a small set of relevant genes that could be used to
build a cheaper diagnostic test 

feature selection algorithms
i compare three methods of feature selection against running a linear svm on the full dataset  on
simulated data and an open microarray dataset 
golub        describes a weighted voting method with filter feature selection  the algorithm takes
a certain number of genes that show the most extreme measure of a weight representing correlation
between genes and the class labels  the measure of weight for gene i is 

ci  


 
i  i
i    i

 
where  
i is the mean of gene i for the positive class in the training data and i is the standard
deviation of gene i for the negative class in the training data  a certain number of genes with the
highest  ci   then contribute a vote to the total 

 
vi   ci  xi    
  
i   
  i
here vi is the vote from gene i and xi is the value of gene i for a test case  the votes are then
summed and if the total is positive then a positive class label is predicted  this method is similar
to but not the same as a svm algorithm 
guyon        and zhang        describe methods of backward search feature selection  both
methods involve starting with the full set of genes  picking a set of decreasing feature subset sizes 
and eliminating batches of genes at each iteration by ranking their contribution  the measure of
contribution in guyon is wi    the squared value of the weight vector for gene i from running svm 
 

fi
the measure in zhang is wi   
i  i    the final subset size is chosen by k fold cross validation 
which includes the feature selection steps as well as training the svm  if multiple sizes tied for
minimum cv error  i choose the one with more features  as choosing too few features can increase
the error much more steeply than choosing too many  the final set of genes is chosen by the most
frequently used genes at the step with the smallest cv error 

simulated microarray data
i construct various sets of simulated data  using methods described in zhang         a set of informative genes are sampled from n           with the sign depending on which class the observation
is from  a set of uninformative genes are sampled from n         to simulate outliers  some percent
of the expression values for each sample are drawn from a gaussian with the appropriate mean
and    times the standard deviation  a training set with     observations and a test set with     
observations were created using this same procedure     simulations were run for each setting of
the parameters to assess variance  i varied the ratio of informative to uninformative genes  and the
ratio of positive to negative classes in the training data 

simulation results
i use the smo algorithm presented in class with c    tolerance         and max passes       for
the method presented in golub  i set the number of features to filter to      for the two backward
search methods  i stepped through the subset sizes                                                   
here are results for different parameter settings  the final column indicates if the feature selection
methods had test error with mean significantly different than the mean test error of the full svm
 two tailed t test at level         

    informative genes       uninformative genes     outliers  balanced classes 
method
full svm
golub
guyon
zhang

cv error    
         
         
         
         

test error    
         
         
         
         

features kept
    
   
   
   

number of sv
  
na
  
  

significant
na
 
 

    informative genes       uninformative genes     outliers  balanced classes 
method
full svm
golub
guyon
zhang

cv error    
        
        
        
        

test error    
        
        
        
        

features kept
    
   
   
   

 

number of sv
  
na
  
  

significant
na
 
 

fi    informative genes       uninformative genes     outliers  balanced classes 
method
full svm
golub
guyon
zhang

cv error    
        
        
        
        

test error    
        
        
        
        

features kept
    
   
   
   

number of sv
  
na
  
  

significant
na

    informative genes       uninformative genes     outliers  unbalanced classes      positive 
    negative  
method
full svm
golub
guyon
zhang

cv error    
         
         
         
         

test error    
         
         
         
         

features kept
    
   
   
   

number of sv
  
na
  
  

significant
na
 
 
 

microarray data
to test the methods on real microarray data  i used an openly available dataset published by alon
        the data include      of the genes with the largest minimal intensity across    tumor
and    normal colon samples  the number of samples is not large enough to have a training set
and a separate test set  so instead i compared the methods using out of bootstrap error  for each
method  i trained the svm on a bootstrap sample from the    observations  and tested on the
observations that did not appear in the bootstrap sample  this was repeated for    bootstrap
samples  as with cross validation in the simulations  the feature selection loops were embedded
within the bootstrapping loop 
method
full svm
golub
golub
guyon
zhang

out of boot error    
         
          
          
         
         

features kept
    
   
  
    
   

number of sv
  
na
na
  
  

significant
na
   worse 

conclusions
for the simulation data  the backwards search feature selection methods have statistically significant
reduction in cross validation and test error when there are few informative genes  although the
differences are not large for the balanced class data  this agrees with the conclusion from nilsson
       that the svm performs well even with many uninformative features  making large decreases
in predictive error through feature selection hard to achieve  all three feature selection algorithms
gave significant improvements with the unbalanced simulated data  and the reduction in error was
large for backwards search methods      decrease in test error  
the two backwards search methods performed very similarly  both better than the method from
 

figolub  on the simulated datasets  the number of support vectors always decreased as the number
of features was reduced in the backwards search methods  which might imply better generalization
results 
for the actual microarray data  the two backwards search methods had similar error to the full
svm  but in this case the golub method performed worse  i set the golub method to pick only   
genes after trying on     genes and having high prediction error  the minimum number selected
for the guyon and zhang methods can be arbitrary in cases like this when the error curve is fairly
flat over the various subset sizes 
the plots of cross validation error indicate that the predictive errors often increase more steeply
with the loss of any informative features than they do with the gain of uninformative features 
these plots might provide  for empirical data  a rough sense of the number of relevant genes for
a certain contrast of sample classes 

figures
plots of cv error from the simulated data are the averages over    simulations 

plot of out of bootstrap error for the alon        data is the average over    bootstrap samples 
 

fireferences
   alon  u   barkai  n   notterman  d a   gish  k   ybarra  s   mack  d   levine  a j   broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by
oligonucleotide arrays  procedings of the national academy of sciences                    
   golub  t r   slonim  d k   tamayo  p   huard  c   gaasenbeek  m   mesirov  j   coller  h   loh 
m   downing  j   caligiuri  m   bloomfield  c   lander  e s   molecular classifiation of cancer  class
discovery and class prediction by gene expression monitoring  science                   
   guyon  i   weston  j   barnhill  s   vapnik  v   gene selection for cancer classification using support
vector machines  machine learning                  
   nilsson  r   pena  j m   bjorkegren  j   tegner  j   evaluating feature selection for svms in high
dimensions  springer berlin   heidelberg       
   zhang  x   lu  x   shi  q   xu  x   leung  h e   harris  l n   iglehart  j d   miron  a   liu  j s  
wong  w h   recursive svm feature selection and sample classification for mass spectrometry and
microarray data  bmc bioinformatics         

 

fi