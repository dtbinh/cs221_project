a framework for stock prediction
hung pham  andrew chien  youngwhan lim
december         

 

introduction

one which learns from historical data  and the other
which learns from the user data  then combine these
mechanisms into a single prediction  in particular  the
    motivation
framework needs to maintain the following natural
stock price prediction is a classic and important prob  requirements 
lem  with a successful model for stock prediction  we
can gain insight about market behavior over time 
   it must provide predictions on a daily basis and
spotting trends that would otherwise not have been
return this prediction to the users 
noticed  with the increasingly computational power
of the computer  machine learning will be an efficient
method to solve this problem  however  the public
   it must have comparable performance as the best
stock dataset is too limited for many machine learnusers 
ing algorithms to work with  while asking for more
features may cost thousands of dollars everyday 
in this paper  we will introduce a framework in
   it must be stable  in particular  there is no dicwhich we integrate user predictions into the curtator or a group of dictators 
rent machine learning algorithm using public historical data to improve our results  the motivated
from a high level viewpoint  the framework will comidea is that  if we know all information about todays
bine an objective prediction  from historical data 
stock trading  of all specific traders   the price is preand subjective prediction  from user predictions  
dictable  thus  if we can obtain just a partial informathe first requirement means that the framework must
tion  we can expect to improve the current prediction
employ online and adaptive algorithms and run as
lot 
fast as possible to respond to a large scale market 
with the growth of the internet  social networks 
the second requirement means that the best users
and online social interactions  getting daily user preshould not perform better than the algorithm  which
 
dictions is feasible job   thus  our motivation is to
uses data from these users  the last requirement
design a public service incorporating historical data
means that the predictions are consistent over time
and users predictions to make a stronger model that
even more information is discovered  on the other
will benefit everyone 
hand  it implies that there is no group of users who
can decide the prediction of the whole framework
 who we call dictators   or otherwise  the frame    framework description
work will become unstable as users predictions are
in addition to using historical data  our framework not stable at all  thus  we have to control how much
also uses daily predictions made by users of the stock we want to reply on objective predictions and submarket  the framework will build two mechanisms  jective predictions  so that the overall predictions are
good  but not too subjective 
  certainly  it may take time to build such a system  the
in the next section  we will describe in details our
hard part is obtaining serious predictions instead of random
predictions from users 
framework that can satisfy those three requirements 
 

fi 

prediction using
stock information

   

historical

and s c  d  is a distance measure between c and d  in
this paper  we will consider      x  y     t x  y  
and s c  d    kc  dk    the function s is introduced
to gain stability by making i slowly change  next 
we will apply this idea for both regression and classification models 

the dataset

to make predictions  we use standard ohlcv  open 
high  low  close  and volume  data that can be downloaded from financial websites    we focus primarily     the classification model
on technology companies  google  yahoo  microsoft 
often  we care less about the actual price than we
etc  to test our model 
do about the change in the price from one day to
the next  the algorithm above describes a regres    problem description
sion problem  but we can perform classification by
we formall formulate the stock prediction problem as comparing tomorrows predicted value with todays
price  
follows 
given n     feature vectors x    x            xn  
and observed labels y            yn for the first n
days  predict the label yn   for day n     

     

deep correlation algorithm

in order to make a prediction  we minimize the objective function with some             n   this allows us to
in our particular formulation  xi  r are the feamake the prediciton yn     nt xn   to solve this optitures on day i  and yi is the actual price  making this
mization function  we introduce a method we call the
a regression problem 
deep correlation  dc  algorithm 
experimentally  gradient descent performs too
    objective function
poorly to optimize the objective function due to slow
since we are basing our predictions off a feature convergence  instead  the following algorithm  deep
vector  linear regression is a reasonable method to correlation  uses coordinate descent to solve the opsolve this problem  using the standard linear regres  timization problem much more quickly 
sion method  we find a vector  that minimizes the
choose parameters  and  
 
pn
choose d  the depth of correlation in the data 
regret i   t xi  yi   and makes the prediction

   
set    
yn     t xn     however  this algorithm does not
repeat until convergence 
capture the temporal correlation inside the data  for
for each i from n down to n  d     
example  stock behavior one year ago is different from

set z   
xi     xi    xi yi  
stock behavior now   thus  we do not expect this alset i    m    t z  m   xi xti   ik  
gorithm to turn out well in practice  and empirical evset n     n  
idence confirms this  nevertheless  we can still apply
output               n     
the idea of empirical risk minimization while trying
to capture the temporal correlation 
the above update rule follows by setting partial
we modify the linear regression model by allowing
derivatives to be zero
the vector  to change with time  we introduce the
following objective function
xi xti i        i   i     i    xi yi  
k

fn  

n
x
i  

i   i   xi   yi     

n
x

  this

assumes a linear correlation between x and y  we can
certainly make polynomial extension on the dataset to introduce more complicated correlation 
  we expect regression methods to work better than classification  because by changing all actual prices to     labels  we
lose too much information  since any regression algorithm can
be converted into a classification algorithm  its fine to only
consider regression 

i s i   i    

i  

here   is the discount factor representing how current data relates to past data    is the loss function 
  for

example  as http   www nasdaq com 

 

fistock
decidable days
in practice  the dc algorithm is incredibly fast  it
goog
      
takes no more than one second before converging if
yhoo
      
we set d      and initialize  from previous results 
msft
      
this satisfies the first requirement of our framework 
aapl
      
moreover  after each update all i      i  n fully
amzn
      
t
satisfy the identity i xi   yi   so  the movement of
bidu
      
i does somehow represent the movement and correfigure    dc algorithm on    
lation of the data 

     

stock
goog
yhoo
msft
aapl
amzn
bidu

testing results

in testing mode  we introduce a threshold constant
and the concept of decidable day  a decidable day
is a day whose price changes significantly compared
previous days price  for example  if we set a threshold of     the decidable days are the ones where the
price changes by at least    compared to the previous days price  although a good algorithm  should
perform well on all days  the analysis of algorithms on
only decidable days is also meaningful  because these
are the days wehre being correct is more important 
we also preprocess the data by dividing it into discrete blocks of multiple days  the price of a block of
days is the average price on those days  thus  we aim
for a long term prediction  which is more important
than a short term one 
the following table shows the result of some basic
learning algorithms  in our experiments  dividing the
data into blocks does not improve these results by
very much 
stock
goog
yhoo
msft
aapl
bidu

decidable days
      
      
      
      
      

lin  reg 
      
      
      
      
      

decidable days
      
      
      
      
      
      

lin  ext
      
      
      
      
      
      

quad  ext
      
      
      
      
      
      

  days blocks  threshold   
lin  ext
      
      
      
      
      
      

quad  ext
      
      
      
      
      
      

figure    dc algorithm on       days blocks  threshold   

the dc algorithm performs well with many stock indexes  reaching nearly     when analyzing   days
blocks with google  apple  amazon  and baidu 
in contrast  it still performs poorly with microsoft 
looking at the data  this is because microsofts stock
price very stable  finally  note that using quadratic
expansion slightly improves linear expansion  especially with data where we have lower success rates 

   

the regression model

although the classificaiton model we use is based off
the regression model  the success of dc in the classification model does not carry over to the regression model  and dc performs about as well as other
methods  such as linear regression  for performing actual stock price  this is because stock prices tend to
change very little on a daily basis  so even the simplest algorithms  assuming the price does not change 
can reach     to     correctness  in this section  we
will introduce a more general version of the dc algorithm that outperforms linear regression in the long
term 

log  reg 
      
      
      
      
      

figure    classical methods on     single days  threshold
  

the dc algorithm does not work any better than
these algorithms with single day prediction  however        generalized dc algorithm
when dividing data into block  we see an incredible in the dc algorithm  we assume that yi  n  i     
improvement 
where i   it xi   although the dc algorithm captures temporal correlation  it assumes a strong rela  empirically  any algorithm consistently getting       cortion  the price of a day is a linear function of the
feature on that day  even when we use a quadratic
rectness can be considered good 
 

fimethod
tomorrow
  days
  days
   days
  months

expansion  the results are not improved significantly 
in the general model  we modify this assumption
to a more
one  yi  n  i   i    where
pn reasonable
t
i  
x

m
and
i is some determined by
ij
j   j j
some fixed function of our choice 
under this model  the price of a day depends on all
observered data  in contrast  under the standard dc
algorithm the price of each day is determined by the
features of that day   a reasonable choice for mij is
mij   ij if i  j and mij    ji if j   i  i e   and
 are respectively the backward and forward correlation factor  by maximizing the log likelihood function and adding the stability requirement  we want
to minimize the following objective function 

 
n
n
n
x
x
x
  
t

y

x

m
 
i  i  i      
i
ij
j j
 

j  
i  
i   i

lin  reg 
     
      
      
      
      

gen  dc  
     
      
      
      
      

gen  dc  
      
      
      
      
      

figure    error of long term predictions on     days
with google 
the above table suggests that generalized dc is not
better than linear regression  actually worse  when
predicting tomorrow price  however  generalized dc
performs much better in long term predictions  even
when linear regression does poorly  experiments suggest that quadratic expansion does not help generalized dc perform better at all 

as before  we find the gradient descent does not work
well for this algorithm  while coordinate descent does 
after some math  we get the update rules for this
coordinate descent 

 

prediction using user feedbacks

m
let n be a matrix such that nij   ij
  let ij     if this section will present a method of predicting based
i
on users prediction so that it can satisfy requirement
i   j and   otherwise  let s   n t n   and let d be a
   the prediction should be relatively good compared
matrix such that dij       ij  sij   then we update
to the best users  although regression is more desirt
t
able  we have to restrict our framework to classificacj    j j 
  j   j  
tion only due to time constraint  the algorithm to

rj    j ik   j   ik   xj xtj sjj
be presented is proved to be good  in the worst case 
 
without any assumption about the dataset  thus we
n 
n
x yi
x
j    cj  
nij xtj 
dpj pt xp xtj rj    can expect that it may performed better eventually

when the real dataset is fit in 
p  
i   i

it is not hard to see that this model generalizes dc 
linear regression  and weighted linear regression with
appropriate choices of      and   in contrast  linear regression can be considered as a greedy version
of both dc algorithm and generalized dc algorithm
when the s are fixed 
     

   

problem description

assume that we have a group of n people  on day i 
each person j makes a prediction pij         indicating whether the stock is up or down  we have to make
prediction each day before the actual price is released at
the end of the day  how to make predictions each day 

testing results

we test our algorithm with google  using different
 and  and compare to the standard linear regression method  we choose i    n  i   to allow more
variances on past data  the evaluation is based on
the average linear absolute error made by each algorithm  to predict the price in the next d days  we
simply shift the features back d days 

this is a typical example for boosting algorithm  we
will apply the weight majority algorithm  adaboost
is under our consideration  but we wont really employ it until a real user dataset is obtained 
  the generalized dc       correspond to different choice
of                  and            

 

fi   

the weighted majority algorithm together to satisfy the third requirement  again  we
restrict our model to the classification problem only 

weighted majority  wm  works by assigning weights
for each user wj   on day i  the prediction is made
comparing p    p    in which
x
x
p   
w j   p   
wj  

the natural idea is to find the probabilities p    p 
for the classifications made by the two algorithms 
and base our final prediction on cp        c p  for
some constant c of our choice  or we can learn it  
j pij   
j pij   
the probability obtained by the wm algorithm can
 
  the hard part is to find
be assumed to be p p p
 
at the end of the day  if person j predicts wrongly  a probability by generalized
dc  notice that in the
wj
we will update his weight  wj   c   otherwise  the generalized dc algorithm  we assume y  n        
i
i
i
weight is kept the same  it is shown that  if mi is the thus  when y
n  is observed  the probability that
total number of mistakes made by user i  and m   y   y
n
n  is given by
mini mi   then the total number of mistakes made by


z yn 
the wm algorithm is no more than
 
 
 
p
f  yn     
exp
 x  i    
 i 
 i 

log n   m log  c
 
 
log   c
unfortunately  this integral has no closed form  instead  it is represented as as
most importantly  this upper bound is true on daily



basis with any dataset  therefore  we can make sure
x  i
 

 
 
 
erf
that the requirement   is always satisfied  the predic 
i  
tion is as good as the best users prediction up to a
constant factor 
here erf  the error function  is an odd function and
approximated by
s
    test with sampled datasets


 ax    b x 
erf x       exp
although it is not a convincing evidence about the
    x 
performance of wm algorithm in practice  we want
to at least see how it works under some reasonable
where
sampled dataset  because we do not have any ac      
 
a 
 b    
tual datasets of user data  we generated our own
      

data sets from the actual prices  we created a data
set such that predictions are never more than       this last formulation explicitly show how the probacorrectness and the average correctness is       bility the price goes down is computed  consequently 
then the result made by wm algorithm is roughly the combined probability can be computed easily 
moreover  if we choose c greater than     so that
             
objective prediction contributes mainly to the final
prediction  we can assure that there is no dictator in
  combine predictions
the framework 
the previous sections present two methods of predictions  objective prediction by learning historical stock
features  and subjective prediction by learning user
predictions  both methods are online and adaptive 
thus satisfy requirement    however  requirement  
is not always satisfied with subjective prediction  as
the wm algorithm will give the best users very high
weight and making them into dictators  this section
will introduce how those methods can be combined

 

conclusion

this paper proposes a relatively good framework for
stock prediction which can satisfy three natural requirements  with any dataset   however  this may not
be the best framework in practice  such a better one
should be built when a real dataset is obtained  nevertheless  the most important idea of the paper is to
 

ficonnect classical  objective  machine learning algorithms to  subjective  user feedbacks to learn complicated social structures  in the time being  no machine can really get the intelligence level of the human  therefore it is better to ask the human to learn
about them and their game  unless there are machines who are more intelligent than them and can
simulate them  
in the context of stock prediction  we believe that
the lack of data is the major issue  while machine
learners do not spend too much effort on this subject 
hedge funds and other trading cooporations should
have much better datasets  but they never want to
disclose it in fear that it will affect their business  in
contrast  user predictions may be the best dataset for
stock prediction  and if it is publicly distributed  it
may encourage machine learners to attack this problem more in depth 

 

lecting and analysing historical stock data  detailed
explanation on the structure of the stock market and
suggestion on designing the platform since the last
summer  we also thank prof  ng and quoc le for
their help on machine learning methods and the motivation of the temporal correlation 

references
    roger lowenstein  efficient market hypothesis 
wikipedia org
    nick littlestone  manfred k  warmuth  the
weighted majority algorithm  information and
computation  volume      issue         

future works

following are a list of works that we intend to do 
   obtain the real user predictions  hung has designed an online system for that purpose  see
http   www stocknet us  
   make a better mechanism to combine two methods together  for example  if the prediction
based on historical data suggests the price goes
up  but user mechanism suggests that it goes
down  should we make update on  or the weights
of users before announcing the final prediction 
in the proposed framework  two methods do not
really interact with each other before final predictions 
   develop a regression model for user mechanism 
   develope a better regression model for historical
data prediction  generalized dc is not totally
satisfiable 

 

acknowledgment

we would like to thank prof  nguyen dinh tho  and
ms  ha viet phuong for their dedicated job of col  he is currently the director of financial department  foreign trade university  vietnam

 

fi