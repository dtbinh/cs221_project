chris meill
bharath sitaraman
cs      fall     

risk  a case study in applying
learning algorithms to strategy board games
chris meill  bharath sitaraman
cmeill  bharath stanford edu

abstract
our goal was to create a learning algorithm that can play the classic
board game  risk  a game in which players control territories that
produce armies  which the players can use to conquer the territories of
others  it offers an intriguing environment in which to deploy an ai
player  with many different decisions to be made  and a wide variety of
gameplay aspects affecting those decisions  we first implement several
deterministic ai players that will focus on maximizing separate elements
of the game  therefore responding formulaically to isolated game
situations  our learning algorithm used these ais as training partners 
however  it adapted to game situations based off of a decision tree where
we adaptively weighted our possibilities of achieving future situations
and the relevancy of distances to surrounding territories  this ai played
multiple games in order to generate more diverse data  and is optimized
to increase its effectiveness 

     motivation

   introduction

risk tends to be a very entertaining game due to the constantly evolving
situations that unfold during repeated sessions  however  from personal
experience  we have found that most ai players in computerized versions
of risk tend to fall into predictable  and thus  exploitable  patterns 
making these versions of the game rather trivial to beat  granted  the
game is somewhat cyclical in nature  spend a few turns building up an
army  go out on an attack  fix up defenses  build up another army  etc   
so even human players can tend to fall into patterns  however  many of
the best of them do not  the best strategies in risk are adaptive 
reacting to game situations as they unfold  rather than rigidly adhering
to some sort of master plan 
additionally  there are many  unwritten rules  to risk that any human
player figures out very quickly  which many ais tend to struggle with 
for example  while conquering the entire continent of asia provides an
appealingly large unit production bonus  it is virtually impossible to
maintain control of the entire continent  it simply has too many borders

fiand its centralized location frequently makes it a staging ground for
conflict  on top of this  when a player controls asia  that same appealing
unit production bonus makes him an immediate top priority threat that
other players will concentrate on dealing with  our hope is that a
learning algorithm could identify these types of counterintuitive
situations and behave more intelligently than a deterministic ai  without
being explicitly programmed to do so 

     problem definition
the problem can somewhat be boiled down to a classification task  in
which our algorithm will be choosing its set of actions depending on the
current status of the board  if its in a powerful position  it should tend
to attack more  when troops are low  it will want to defend and increase
troops at its borders in order to stave off impending attacks from
opposing players 

     dataset
we actually did not start with any sort of dataset  we gave our algorithm
arbitrary weights  which it will gradually adjust and converge using
logistic regression over time  our dataset will actually be generated
through playing numerous games  while pitting our deterministic ais
against one another  we will be generating data and storing it into a file 
it may be prohibitively slow to re train the ai every game  well probably
have it play games in clusters  and then re train to apply what it learned
after each set of games  it then starts with its optimized weights from
the previous games  and will adjust them in accordance with the current
game 

   approach
currently  we have recreated risk as a text based game played through
the consol  with a player super class that is queried throughout the game
for decisions  for example  the human player extension of this class
simply prompts the user for input   we have implemented a system of
reading preconfigured game boards from a file  so that we can easily
create desired situations  rather than hoping that they unfold as the
game progresses  this also allows us to play on various board sizes  and
scale the game up and down in complexity 

     deterministic  dummy  ais
we have produced   dummy deterministic ai players  which basically
respond formulaically to the current state of the board  each ais

fibehavior is an exaggeration of certain tactics or playing styles frequently
employed by human players  they are listed below 
  
  
  
  
  

aggressive  attack everything possible
bully  attack whenever victory is essentially guaranteed
ocd  aggressively maintain current territorial holdings
border patrol  just defend borders
wanderer  put all armies in one horde that wanders around the
board
   distributor  distribute all armies evenly across territories
originally  our dummy ais were going to help our learning algorithm
train based off of different game situations and help it act appropriately
depending on what kind of position it was in  eventually  they just
ended up acting more like sparring partners  allowing the ai to retrain
its weights to adjust to each different ai  which in turn would affect its
actions  studying their patterns and habits also enabled us to observe
and create a mix and match strategy we tried to cover as many bases as
possible in terms of playing style with each ai so that after multiple
games  the ai would be able to adapt much more quickly to future
opponents 
the aggressive ai and the bully ai definitely surprised us  we
expected in a set of trials that they would probably win the majority of
them  but the speed of the games and the adeptness with which they
defeated their opponents reinforced our belief that the ai should attack
when given the opportunity rather than wait  this actually helped
simplify our process somewhat 

     attacking strategy
our algorithm learns by optimizing a set of two different discount factors
 and   during a game when it is the ais turn to attack  it will consider
the full set of available actions a  including a   the do nothing action
that indicates the end of a turn   where each action ai is a tuple
containing the territory used to attack  the target of this attack  and the
number of attacking dice rolled  for each of these actions  there is some
uncertainty about the resulting game state due to the randomness of the
dice  so the perceived value of this action is assessed to be the sum of
each possible resulting state reward weighted by the probability of
ending up in this state 
the reward for a resulting state is calculated using several different
factors  the first is fa     the current fighting force of the attacking ai 
which is the sum of armies from all its territories weighted by relevancy
to the current action  this relevancy is determined by the distance away

fifrom the territories involved in the action using the discount factor  
for example  if there were n troop in a territory d hops away from our
action  it would contribute d   n to fa     similarly we have fd     the
current fighting force of the defending opponent  there is also pa    and
pd     the number of armies produced by each player at the beginning of
their next turn given the current board state  the resulting state will
have values fa     fd     pa     and pd     which are simply these values
recalculated for the potential resulting state  the reward is then 

fa      pa    fa      pa   

fd      pd    fd      pd   
or the difference between the ratios of how well off the attacker ai is
compared to its opponent after taking this action 

the resulting states reward  however  need be calculated only if the ais
turn were to end after that action  since an ai can take many actions
over the course of one turn  these resulting states can be further
expanded by considering the new set of actions available at that state 
this creates a tree structure terminating in a  actions nodes whose
values are calculated using the formula above  this value is passed
upwards through the tree to the state node s that was considering this
action  s determines its reward to be the maximal value of the as
available to it  weighted by the discount factor   in this sense  
determines the relative importance that our ai places on future
possibilities when considering actions 

     learning
the learning comes in the two discount factors  and   essentially  they
are found to the optimal values for the effectiveness of the ai  the
percentage of games it is observed to win   we update them alternating
between the two  for some given   the learning process plays through
many games to find the optimal value for   and then the resulting  is
then used to assess the effectiveness of the algorithm  in this way we
create a series of training examples    i    i     where  i  is the discount
factor used and  i  is the observed effectiveness of using this  i   initially
a set  i  is chose across the range        which yields a set of training
examples that we can apply a polynomial regression to  the resulting
function w can then be used to find the optimal  that maximizes
effectiveness  for this optimal  we can find an optimal   which together
can be assessed by the new effectiveness   together with   this 
creates a new training example  which when added to the training set

fiproduces a new w  which in turn can be used to find an updated optimal
  this process is repeated until the updates to  and  converge 
at each iteration of this learning  the optimal  is found for a given  in a
similar fashion  a set of    i    i    training examples is computed which
is used to train a polynomial regression function w  the value of  that
optimizes this w is then assessed to find a new   which creates a new
training example that is added to the training set 

     other decisions
defending 
this action was strictly deterministic  if the attacker were to roll one die 
we would defend with two die  if possible  no matter what  rolling
aggressively against the attacker s single die gives us a strict advantage
in that we have more chances of winning the roll 
in the case that the attacker rolled multiple die  given a choice  we would
defend with two die only when their lower dice roll was a   or less 
rolling on a   was somewhat of a judgment call that we made
deterministically since it has approximately a     probability of either
player winning the roll  otherwise  we would only defend with one dice 
fortifying 
for this algorithm  we actually would apply our own attacking learning
algorithm in terms of our opponents troops and fortify our troops
accordingly in order to minimize our expected losses 

   future considerations
   be able to read and train on game situations using somewhat of a
reinforcement learning algorithm  through playing multitudes of
games  we would be able to see favorable outcomes  conquering a
continent  eradicating an opponents strong troop base  etc   and
be able to use those in our current learning scheme 
   we made modifications in order to make the game of risk simpler 
for example  we created randomized balanced board states at the
beginning of games  we also did not introduce the concept of risk
cards in the game since that would add an extra element to the
game 
   currently  we have a deterministic defending strategy  we made a
judgment call based on the result of an attackers roll that human
players frequently use in order to simplify the problem  however 
we would like to have let the ai come to this decision itself based
on the board state 

fi