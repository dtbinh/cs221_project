investigation of error tolerant nature of machine learning algorithms

hyungmin cho
endroit stanford edu

   introduction
soft computing differs from conventional  hard  computing in that  unlike hard computing  it is
tolerant of imprecision  uncertainty  partial truth  and approximation  this definition implies two
important things  the computation result from those soft computations can be considered to be valid if
it is within acceptable range even with some inaccuracy  and those applications are less sensitive to
erroneous computations and invalid or corrupted data values  these observations made the basis of
building a computer architecture which is specially designed to such error tolerance soft computations 
in this computer architecture  we will have relaxed requirements for correct computations and by doing
that  we can expect huge savings in computation costs  many of machine learning algorithms are
consisted of soft computations  in this paper  we will explore various kinds of machine learning
algorithms to find out how much inaccuracy those algorithms can tolerate by injecting artificial
inaccuracy during the computation process  this will be used for building the application aware
computer architecture that will exploit the error tolerant nature of the machine learning algorithms 

   system architecture for application aware resilience
computing system we will use for soft computing machine learning applications is error resilient
system architecture  ersa   which is designed to execute error resilient applications with relaxed
reliability hardware  the underlying architecture of ersa is based on multi core architecture with
asymmetric reliability level  there is little number of reliable computing cores in the system  which is
responsible for executing error intolerant or critical computations  and the rest of the computing
system is consisted of inexpensive  relaxed reliability cores which will execute the error tolerant
computation  reliable computing cores  which is called super reliable core  src   has similar reliability
requirement like modern computing system  which means the target error rate is near zero  therefore 
there can be huge amount of cost overhead to ensure this level of reliability  enough time slack in
circuit level  voltage band gap or extra error detection correction mechanisms are the sources of such
overhead  in ersa  the portion of computing components which needs such a high reliability is limited
to very little  to minimize overall overhead  relaxed reliability cores  rrcs  are allowed to generate
some amount of computation errors  besides of eliminating overhead for reliability  rrcs can have
more radical approach to reduce computation costs like reduce cmos feature size or reduce the
operation voltage 

     computation model
when we execute machine learning algorithms on ersa  we will execute the main execution path on
the src  and execute individual computation tasks on rrcs  because the computations tasks on
rrcs can be crashed during execution because of the underlying hardware errors  ersa has runtime
software to manage re execution of crashed tasks 

fi   machine learning algorithms
not all machine learning applications can be executed on ersa and still have high quality acceptable
results  following is the list of characteristics of algorithms that can be executed on ersa 


parallel computations
ersa has multi core based architecture which assumes the application is highly parallelizable 
if the ml algorithm is not parallelizable  then it cannot have benefits from the ersa
architecture 



iterative computations
if the computation is done in iterative fashion  the overall convergence process can tolerate
errors in intermediate computation state  because the consecutive computations can recover
from the previous incorrect computations 



individual training data computations
if the algorithm doesnt have iterative computations  at least its computation tasks can be
divided into individual computations from different data instances  by ensuring those criteria 
erroneous computations from one computation task can be prevented to be propagated to other
computation tasks 

following is the selected machine learning applications to be investigated in this paper


least mean square
we used newtons method to optimize the parameters of lms algorithm  errors will be
injected to gradient and hessian computations in learning phase 



logistic regression
we used newtons method for logistic regression also 



support vector machine
simplified smo algorithm was used for optimization algorithm  errors will be injected to
kernel computations in learning phase 



k means clustering
we applied k means clustering algorithm for image clustering based on a pixels rgb value 
errors will be injected when finding the closest centroid  when an error is injected  the id of
closest centroid for each data point will be changed randomly 



bayesian network with loopy belief propagation
bayesian network structure was applied to learn and detect object images based on spatial
context  the core algorithm used to get posterior probabilities over the bayesian network is
loopy belief propagation  error will change the value of message between clusters 

     dataset
we used input data from the uci machine learning data base and the input data provided for cs   
and cs    homework assignments 

fi   error model
actual computation error an application will face will come from the physical characteristics of
underlying hardware  therefore  it will be very hard to build a concrete error model without the actual
implementation of the computer system  in this stage  therefore  we assumed there will be random
errors during the computation tasks on the rrcs  we will set a certain error rate  and select
individual computation tasks randomly during the computation and add disturbance value to the
original value  the disturbance value will also be chosen randomly from gaussian distribution 
      
       
if the computation result is not a real or integer value  such as category id  then we will change the
original value with a random number within possible range 
    min       max   
to find out error tolerance level  we will do the experiment for different error injection rates and will
compare the output quality 

   experiment result
according to the error model we artificially inject errors into the original program and measured how
much quality degradation happens as the error rate increases  the basic quality measurement is
percentage of correct classification  we will compare the correct classification percentage with the
original error free execution 
for k means clustering  using the number of miss clustered points as the measure of the quality is not
a good idea  because according to the initial starting point  the clustered result can be different 
instead  we measured the output quality of the clustering by the average diameter of the cluster  the
purpose of the k means clustering iteration is achieving the tightest clustering by finding best cluster
centers  thus  if the resulting cluster diameter is larger than the diameter from error free execution 
then we can use the looseness of the diameter as the measure of quality degradation 

     comparison of lms and logistic regression
lms

logistic regression
classification accuracy

classification accuracy

 
   
   
   
   
   
                                  
  injected errors   sec

 
   
   

adult

   

diabetes
gamma

   

spam
   
                                  
  injected errors   sec

films

logistic regression
classification accuracy

classification accuracy

 
   
   
   
   

 
   
   

adult

   

diabetes
gamma

   

spam
   

   
   

   

   

   

   

   

   

   

        

  injected errors   sec

   

   

   

   

   

   

   

   

   

    

  injected errors   sec

lms and logistic regression is very similar algorithm  so we could apply same dataset for two
classification algorithms  it turns out that in the original error free execution  logistic regression is
better in terms of correct classification percentage  but as we increase the error injection rate  lms is
more robust and sustained its result quality with higher error rate  possible explanation is the gradient
of lms is linear while that of logistic regression isnt  it can suppress the unstable behavior according
to the injected disturbance values  this discovery gave us an insight that we can have one more
options to choose machine learning algorithm according to the applications purpose  if the application
wants low cost classification algorithm  we can choose lms instead of logistic regression and execute
it on error prone  unreliable hardware which yields low cost computation 

     various ml algorithms
we also did same experiments on different ml algorithms  however  we failed to collect sufficient
amount of dataset that can be applied to all algorithms  following algorithms are tested with only one
dataset  it is not sufficient to characterize the details error tolerant nature of the algorithms  but we
can see certain level of error resiliency in each algorithm 
       support vector machine

classification accuracy

svm
 spam classification 
 
   
   
   
   
   
                                  
  injected errors   sec

       k means clustering

fik means clustering
   
    
 
    
   
   
   
   
   
   
   
   
   
   
    
    
    
    
    
    
    
    
    
    
    

relative cluster diameter

    

  injected errors   sec

       bayesian network inference with loopy belief propagation
bayesian network inferencing
 tas network 
detection precesion

 
   
   
   
   
   
                                        
  injected errors   sec

   conclusion
in this paper  we explored several kinds of machine learning algorithms and figured out how much
amount of errors they can tolerate with small impact on result quality  we also have concluded that
the selection of right algorithm according to the purpose of application is crucial to design a system
with desired level of reliability  this result will be used to actual implementation of ersa system to
determine which level of unreliability it should have  also  based on this result  we can extend the
research to find what we can have to increase the resiliency of given machine learning algorithm 

   references
references

    y  jin         jan   soft computing   online   http   www soft computing de def html
    uc irvine machine learning repository   online   http   archive ics uci edu ml 
    g  heitz and d  koller  learning spatial context  using stuff to find things  in european
conference on computer vision  eccv        
    a  ng  cs     notes   online   http   www stanford edu class cs    materials html

fi