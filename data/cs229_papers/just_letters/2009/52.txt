algorithm trading using q learning and recurrent reinforcement learning
xin du

jinjian zhai

koupin lv

duxin stanford edu

jameszjj stanford edu

koupinlv stanford edu

investment performance is path dependent if we use the
cumulative profit as the criterion        optimal trading
strategy and asset rebalancing decision require the
information of current position of portfolio and market
status          besides  market imperfections like
transaction costs and taxes will make the high frequency
trading overwhelmingly expensive      
our algorithm trading results indicate that rrl has more
stable performance compared to the q learning when
exposed to noisy datasets  q learning algorithm is more
sensitive to the value function selection  perhaps  due to
the recursive property of dynamic optimization  while rrl
algorithm is more flexible in choosing objective function
and saving computational time            

abstract
the reinforcement learning methods are applied to
optimize the portfolios with asset allocation between risky
and riskless instruments in this paper  we use classic
reinforcement algorithm  q learning  to evaluate the
performance in terms of cumulative profits by maximizing
different forms of value functions  interval profit  sharp
ratio  and derivative sharp ratio 
moreover  direct reinforcement algorithm  policy search 
is also introduced to adjust the trading system by seeking
the optimal allocation parameters using stochastic
gradient ascent  we find that this direct reinforcement
learning framework enables a simpler problem
representation than that in value function based search
algorithm  and thus avoiding bellmans curse of
dimensionality and offering great advantages in efficiency 

   portfolio and trading system setup and
performance criterion
     structure of portfolio

key words  value function  policy gradient  q learning 
recurrent reinforcement learning  utility  sharp ratio 
derivative sharp ratio  portfolio

the most important cornerstone in     s in the field of
finance theory is the establishment of capital asset pricing
model  capm            a very important conclusion of
capm is the holding strategy of risky riskless assets for the
investors  assuming risk free asset  like cash or t bill  is
uncorrelated with other assets  there is only one optimal
portfolio that can achieve lowest level of risk for any level
of return  which is market portfolio  market portfolio is the
weighted sum of all the risky assets within the financial
market  and it is totally diversified for the risk       
our investment account is built up following the capm
theory  which is the combination of a riskless asset  cash 
and a risky market portfolio  the relative weights of these
two assets will be rebalanced by trading within each time
step  figure   gives the intuitive way of presenting our
transaction account  which is the combination of a riskless
asset and a risky asset from the efficient frontier 
our traders are assumed to take only long  short
positions  ft          of a certain magnitude  long

   introduction
in the real world  trading activities is to optimize rational
investors relevant measure of interest  such as cumulative
profit  economic utility  or rate of return  in this paper  we
study the performance of q learning algorithm with
different value functions subject to optimize  internal profit 
sharp ratio  and derivative sharp ratio  empirical results
indicate that derivative sharp ratio outperform the other two
alternatives by accumulating higher profit in the value
iteration 
due to the property of financial decision problems 
especially when transaction costs are included  the trading
system must be recurrent and immediately assessment of
short term performance becomes essential for the gradually
investment allocations         direct reinforcement learning
approach is able to provide an immediate feedback to
optimize the strategy  in this report  we apply an adaptive
algorithm called recurrent reinforcement learning  rrl 
to achieve superior performance of collecting higher
cumulative profit compare to the case of using
q learning      

positions are initiated by purchasing some amount of assets 
while
short
positions
correspond
to
selling
accordingly     

 

fiprofits to calculate the wealth 

optimal portfolio by capm
     

t

wt   w        rt  

   

t   

    

t

  w            ft    rt f   ft  rt       ft  ft    
t   

     
return

in which wt is the accumulative wealth in our account
up to time t  and w  is the initial wealth  rt

    

f

is the risk

free rate  such as interest rate for cash or t bills  when the
f
f
interest rate rt is ignored   rt        a simplified

     

expression of cumulative wealth is obtained 
 
    

t

     

    

     

               
standard deviation

     

    

     

wt   w        rt  

    

t   

figure    construction of trading account  combination
of a riskless asset and a market portfolio
within the market portfolio  there are m risky assets  and

t

  w        ft  rt       ft  ft    
t   

their prices at time t are denoted as  zt   i             m   the
i

position for asset i  ft   is established at the end of time step

f

t  

j

   

  zt   zt    
j

j

u   wt     wt     for     
u    wt     log wt for     

   

when 

      the investors only care about the absolute
wealth or profit  and be viewed as risk neutral      
indicates that people prefer to take more risk  and described
as risk seeking       corresponds to the case of risk
averse  which means investors are more sensitive to loss
than gains  by the assumption of capm  the rational
investors are falling into this category  we take       
in our simulation 
besides cumulative wealth  risk adjusted return are more
widely used in the financial industry as the inputs for the
value function  as suggested in the modern portfolio theory 
sharp ratio is most widely accepted measure of risk
adjusted return  denote the time series returns as rt   we

i

weights of risky assets sum up to one 

  

m

traditionally  we use utility function to express peoples
welfare in the position of certain wealth     

period t is  rt     zt   zt         at any time step  the
i

ft  i   zt i   zt  i  

     performance criterion

between risky and riskless assets in the portfolio at time t 
moreover  from time t   to time t  the risky asset  which is
the market portfolio  also changes relative weights among
all the securities because of their price volume fluctuations 
we also define the market return of asset i during time

m

m
zt
i ti  
        ft i  f
i
zt  
i   

  is the effective portfolio weight of asset i before adjusting 

ft mentioned above denotes the asset allocation

f

i   

j   

i t is the
information filtration including security price series z and
other market information y over the time up to
t  i t     zt   zt         yt   yt          

i

t   

i ti  
f

weights between risky and riskless assets  and

i

m

  where

t  and will be rebalanced at the end of period t    market
imperfections like transaction costs and taxes are denoted
as  to incorporate into the decision function iteratively 
in a word  our decision function at time step t is defined
as ft   g  t   ft     i t     in which  t is the learned

the

t

  w     ft  i  

i

   

i

   

i   

     profit and wealth for the portfolios
a natural selection of performance criterion is the
cumulative profits of the investment accounts  in discrete
time series  additive profits are applied for the accounts
whose risky asset is in the form of standard contracts  such
as future contracts of usd eur  while in our case  since
we use market portfolio as the risky asset  with each
security fluctuates all the time  we use multiplicative

have the sharp ratio definition 

 

fimean  rt  
std   deviation  rt  

the performance criterion we talked so far can be expressed
as 
   
u   rt       rt     r   w   

there are cases that the markov decision processes do not
converge  q learning also suffers from instability for
optimal policy selection even when tiny noise exists in the
datasets         
in our simulation experiments  we find that the policy
search algorithm is more stable and efficient in stochastic
optimal control  a compelling advantage of rrl is to
produce real valued actions  portfolio weights  naturally
without resorting to the discretization method in the
q learning  our experiment results also indicate that the
rrl are more robust than value based search in noisy
datasets  and thus more adaptable to the true market
conditions 

   learning theories

     value function and q learning

reinforcement learning adjusts the parameters of a
system to maximize the expected payoff or reward that is
generated due to the actions of the system            this
process is accomplished through trial and error exploration
of the environment and action strategies  in contrast to the
supervised learning with decision data provided  the
reinforcement algorithm receives a signal from the
environment that current actions are good or not           
more specifically  our recurrent reinforcement learning can
be illustrated in figure   

the value function v   x    is an estimate of future
discounted reward that will be received from initial state x  
our goal is to improve policy  in each iteration step to
maximize the value function which satisfies bellmans
equation           
   
v    x         x  a   pxy   a   d  x  y  a     rv    y   

st  

  t
 rt
t t   

 

   

  t
  t
  rt   rt    

t t   
t t   
notice that in both cases we talk about above  the wealth
and sharp ratio can be expressed as the function of rt   so



a

where

state x to state y under action a   d   x  y   a  is the
intermediate reward  and r is the discount factor weighting
relative importance of future rewards to current rewards 
the value iteration method is defined as 
vt      x    max  pxy  a  d  x  y  a    rvt    y       
a

reinforcement
learning u  

input series

   x  a  is the probability of taking action a in

state x   p xy   a   is the transitional probability from

target price

trading
system 

y

y

this iteration will converge to the optimal value function
defined as 
    
v     x    max v    x 

profits losses u   
trades portf
olio weights

delay



  which satisfies the bellmans optimality equation 

transaction cost

v     x    max  pxy  a  d  x  y  a    rv     y        
a

y

  and corresponding optimal action can be inversely
determined by optimal value function 
    
a    arg max  pxy   a   d   x  y  a    rv    y   

figure    algorithm trading system using rrl
reinforcement learning algorithms can be classified as
either policy search or value search            in the
past   decades  value search methods such as temporal
difference learning  td learning  or q learning are
dominant topics in the field            they truly work
well in many application cases with convergence theorems
existing at certain conditions      
however  the value function approach has many
limitations  the most important shortcoming of value
iteration is bellmans curse of dimensionality due to the
property of discrete states and action spaces         also 
when q learning is extended to functional approximations 

a

y

the q learning version of bellmans optimality
equation is 
q    x  a      p xy  a   d   x  y   a     r max q    y  b  
y

    

b

  and the update rule for training a function approximator
is based on the gradient of the error 
 
    
  d  x  y  a    r max q  y  b   q  x  a   
 
b
  which will lead to the optimal choice of action 

 

fia    arg max q  x  a   

du t     du t drt dft drt dft  
 
 
 
 
d
drt dft d t dft   d t

    

a

in q learning  we need to discretize the states  and at
each time step  we have the price state either be rise or fall 
i e  rise  fall   and we have the corresponding action set be
 buy  sell   due to the fixed combination at any time t with
relative prices of securities and risk aversion assumption in
the formalization of the problem  we have the transitional
probability matrix as the following 

t   

     risky asset and trading signal

    

simply saying  trading signal is buying or selling
decisions of investors  and it is reversely determined by
maximizing the expectations of value functions in each
time step  in our trading model  the buy action is assigned
with value   and selling action is assigned with value    
and more concretely  buying means enhance the relative
weight of risky asset  increase   while selling indicates the
vice versa  the first panel of figure   is an example of
trading signaling by reversely solving maximization
problems based on the price information of risky assets 
the second panel is the relative weights of the risky and
riskless asset after the time series trading signal 

  in which    zt   zt       is the indicator function to be
b

b

either   or   when the condition within the bracket is true or
false 

     recurrent reinforcement learning
as we described above  the rrl algorithm will make the
trading system ft     be real valued  and we can find the

   

    

the parameters can be optimized in batch gradient
ascent by repeatedly computing value of u t  

 
   

 

   

   

   
   
time step
   buying      selling

    

    

 
    
  
                                             
time step
 st element of parameter 

    

 
weight of risky asset

notice that

dut    
d

   

 

sequence of time step       

   

risky asset price
   

optimal parameter  to maximize criterion u t after t

du t     t du t drt dft drt dft  
 
 
 
 
d
dft d dft   d
t    drt

    

    

  where 


 ft  b   zt b   zt  b       zt b   zt  b    
a   d   b

a ft  a zt a   zt  a   


b ft  b   zt b   zt  b       zt b   zt  b     

b   c  

a ft  a zt a   zt  a   


du t  t  
d t

   results and discussions

norm aliz ed ris k y as s et pric e

rise   sell   a b 
 

fall   sell   c d 

  and parameters are updated online using 

trading s ignal

 rise   buy

 fall   buy

    

dft
is total derivative that is path
d

dependent  and we use the approach of back propagation
through time bptt  to approximate the total derivative
in two time steps 
dft ft ft dft  
    
 
 
d  ft   d
so the simple online stochastic optimization can be
obtained by taking derivatives on the most recent realized
returns 

   

 

 

   

   

   

   

    

    

    

    

weight of riskiless asset

 nd element of parameter 
 

   

   

   

 

   

   

   
time series

   

figure    simulation of risky asset and trading signal

 

fi     q learning with different value functions
we conduct a group of simulation using different value
functions  the results of cumulative profits by q learning
are presented in the figure    each graph stands for a
cumulative profit gains using interval profit  sharp ratio 
and derivative sharp ratio as the value functions under one
time series simulation of risky asset  we can see the
cumulative profit varies considerably when different
functions are applied  this is consistent with the instable
property of q learning  empirical results indicate that the
change of value functions or the noisy of dataset may
greatly change the policy selection and thus affect the final
performance 

figure    trading performance using q learning by
maximizing derivative sharp ratio  sharp ratio  and
internal profit
these simulations are carried out using      time
intervals  with underlying risky asset to be artificial
normalized market portfolio price produced by geometric
brown motion  the simulation results indicate that the
derivative sharp ratio outperform in accumulating account
wealth compared to the sharp ratio and internal profit 
economically speaking  the derivative sharp ratio is
analogous to the marginal utility in terms of willingness to
bear how much risk for one unit increment of sharp ratio 
this value function incorporates not only the variance of
the dataset but also the risk aversion of the investor  and
thus more reasonable in probabilistic modeling 
when q learning algorithm applied to the noisy datasets 
properly choosing value function plays key role in the
stability of performance  as we discussed  in the cases of
using sharp ratio and interval profit  the algorithm trading is
not profitable and varies considerably when exposed to the
different value scenarios of underlying asset  another point
worth noticing here is that the transaction cost may accrue 

     recurrent reinforcement learning using
different optimization functions
stochastic gradient ascent methods are widely applied in
the backward optimization problems such as reinforcement
learning  we optimize the value criterion of general
function u t by regulating the relative weight parameter 
as we discussed in     
direction reinforcement learning uses policy iteration to
achieve the optimal action without recursively solving a set
of equations  and thus computational cheaper and more
flexible  rrl algorithm has more stable performance than
q learning when they are exposed to the fluctuating
datasets  when sharp ratio and derivative sharp ratio are
used for the optimization search  they both have a stable
and profitable performance under different underlying
price scenarios 

 

fi     recurrent reinforcement learning using
different length of backward steps
formula      gives the stochastic batch gradient ascent
algorithm  in practice  we do not include all the derivative
terms up to time t  instead we just take several previous
temporal steps for computational savings  we studied the
effects of this omission on the invest performance by taking
different number of backward time steps in the
optimization 
we use normalized s p     index as the market
portfolio and compared the performance of algorithm
trading using different amount of backward time steps 

figure    rrl algorithm trading performance using
different amount of temporal information
we use derivative sharp ratio as the objective function
subject to optimize at each time step  and choose backward
time steps to be   hours    hours     hours     hours  and   
hours respectively  the rrl algorithm has steady
increased on the account wealth in all the learning cases
and as more information incorporated  cumulative profit
grows with higher rates  when backward temporal step is
over     the investment action does not improve much if we
include more previous temporal data 

   conclusion
we have proposed and tested a trading system via
reinforcement learning method  we use value iteration and
policy iteration respectively to test the algorithm trading
performance using q learning and recurrent
reinforcement learning respectively  in the implementing
of q learning  we use interval profit  sharp ratio  derivative
sharp ratio as the value functions subject to optimize at
each time step and tested their performance  our empirical
study indicates that value function selection is very
important to achieve stable learning algorithm in the value
iteration especially applied to the noisy dataset  value
function with information on the noisy  variance  and
dynamic properties  marginal utility  has better

figure    investment performance of recurrent
reinforcement learning using stochastic batch gradient
ascent 

 

fi    

performance in the learning algorithm than that with
stationary and fixed value functions 
in general  rrl with policy iteration outperform than
q learning in the sense of stability and computational
convenience  rrl algorithm provides more flexibility in
choosing objective functions to optimize using stochastic
batch gradient ascent  more inclusion of backward
information helps the learning process  but increment of
performance tends to converge as more previous temporal
steps are incorporated 

    
    
    
    

references
   
   
   
   

   

   

   

   
   
    
    
    

    

moody  j   saffell  m   liao  y    wu  l         
reinforcement learning for trading systems and portfolios 
immediate vs future rewards 
v s  borkar and s p  meyn  the o d e method for
convergence of stochstic approximation and reinforcement
learning  siam j  control                     
michael kearns and satinder p  singh  finite sample
convergence rates for q learning and indirect algorithms 
f  beleznay  t  grobler  and c  szepesvari  comparing
value function estimation algorithms in undiscounted
problems  technical report tr        mindmaker ltd 
        
f  sehnke  c  osendorfer  t  rckstiess  a  graves  j 
peters  and j  schmidhuber  policy gradients with
parameter based exploration for control  in j  koutnik v 
kurkova  r  neruda  editors  proceedings of the
international conference on artificial neural networks
icann     
j  n  tsitsiklis and b  van roy  optimal stopping of
markov processes  hilbert space theory  approximation
algorithms  and an application to pricing high dimensional
financial derivatives  ieee transactions on automatic
control  vol      no      pp             october      
f  j  gomez  j  togelius  j  schmidhuber  measuring and
optimizing behavioral complexity for evolutionary
reinforcement learning   proceedings of the   th
international conference on artificial neural networks
 icann      cyprus       
j  n  tsitsiklis and b  van roy  regression methods for
pricing complex american style options  ieee
transactions on neural networks  vol      no     july      
d  nawrocki  optimal algorithms and lower partial
moment  ex post results  applied economics  vol      pp 
              
martin
l 
puterman 
markov
decision
processesdiscrete stochastic dynamic programming 
john wiley and sons  inc   new york  ny       
xu  x   hu  d     lu  x          kernel based least
squares policy iteration for reinforcement learning  ieee
transactions on neural networks  pp          
kang  j   choey  m    weigend  a          nonlinear
trading models and asset allocation based on sharpe ratio 
decision technology for financial engineering world
scientific  london 
darrell duffie  dynamic asset pricing theory  princeton
university press    edition       

    
    
    
    

    
    
    
    
    
    

    

    

 

l  brock  j  lakonishok  and b  lebaron  simple technical
trading rules and the stochastic properties of stock returns 
journal of finance                    
moody  j    wu  l          optimization of trading
systems and portfolios  neural networks in the capital
markets conference record  caltech  pasadena 
d  wierstra  a  foerster  j  peters  j  schmidhuber 
recurrent policy gradients  journal of algorithms       
neuneier  r          optimal asset allocation using
adaptive dynamic programming  advances in neural
information processing systems    mit press 
john moody  lizhong wu  yuansong liao  and matthew
saffell  performance functions and reinforcement learning
for trading systems and portfolios  journal of forecasting 
vol      pp                
r  sutton and a  barto  reinforcement learning  mit
press   cambridge  ma        
c  j  watkins and p  dayan  technical note  q learning 
machine learning  vol     no     pp                
leslie pack kaelbling  michael l  littman  and andrew w 
moore  reinforcement learning  a survey  journal of
artificial intelligence research  vol          
parr  r   li  l   taylor  g   painter wakefield  c    
littman  m          an analysis of linear models  linear
value function approximation  and feature selection for
reinforcement learning  international conference of
machine learning  pp          
robert h  crites and andrew g  barto  improving
elevator performance using reinforcement learning  in
advances in neural information processing systems 
peter marbach and john n  tsitsiklis  simulation based
optimization of markov reward processes  in ieee
conference on decision and control       
d  wierstra  a  foerster  j  peters  j  schmidhuber 
recurrent policy gradients  journal of algorithms       
leemon baird and andrew moore  gradient descent for
general reinforcement learning  in advances in neural
information processing systems  eds        mit press 
a  w  moore and c  g  atkeson  prioritized sweeping 
reinforcement learning with less data and less real time 
machine learning  vol      pp                
mahadevan  s     maggioni  m          proto value
functions  a laplacian framework for learning
representation and control in markov decision processes
 technical report   university of massachusetts 
richard s  sutton  david mcallester  satinder singh  and
yishay mansour  policy gradient methods for
reinforcement learning with function approximation  in
advances in neural information processing systems  eds 
      vol      pp             mit press 
john moody and matthew saffell  reinforcement
learning for trading  in advances in neural information
processing systems  eds        vol      pp           mit
press 

fi