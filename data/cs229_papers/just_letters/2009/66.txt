variants of pegasos
soowoong ryu
bshboy stanford edu

youngsoo choi
yc    stanford edu

december         

  introduction
developing a new svm algorithm is ongoing research topic  among many exiting svm
algorithms  we will focus on pegasos  pegasos uses alternating iteration schemes  which
are by alternating stochastic gradient descent steps and projection step  the pegasos can
solve a text classification problem from reuters corpus volume   with         training
examples in   seconds  we will examine the algorithms robustness by applying the
algorithm to rather simple text classification problem  namely  the one given in the cs   
homework     we also present three possible variants of pegasos  namely  undo method 
two out method and probability method  we will examine con and pro of each method 

  algorithm description
    pegasos
the pegasos is a subgradient based algorithm that minimizes the following unconstrained
strictly convex objective function 

f     


 

 

  

 
 l     x  y   
m   x   y  s

   

where

l      x  y      max      y    x  

   

however  it does not use eq    as an objective function at each iteration  instead  it uses

f    at    


 

 

  

 
 l     x  y  
k   x   y  at

   

where a set at  s whose size is k and s  a training set  the pegasos consists of two major
steps in updating t  

   substeepest gradient update t     and    projection step 
 

specifically  in each iteration  the pegasos looks for a steepest gradient search direction
given at and sets the learning rate to bet    t   then  it projects the updated t     into the
 

ball  b        

 
sqrt     

    the reason for projection is that we already know the optimal

solution      resides in b 

    a variant of pegasos
a key point in a variant of pegasos is how to deal with outliers in training set  s  there are
outliers in s  these outliers result in increase in objective function value in some iteration
since pegasos chooses at i i d  from s  to prevent this  three methods are used     undo
method    two out method and    probability method  a modification from pegasos is applied
at the end of iteration when t    is computed  in the undo method  with computed t      the
new objective function value  f  t        is also computed  if f  t        f  t   then we do not
use computed t      instead we let t    to be equal to t   if f  t       f  t     then we keep

fit    as updated  in the two out method  the training set  s  is updated at each iteration  if
f  t        f  t     then we do not use compute t    and also append a flag to each data 
  xi   yi   in at    and count the number of flags in each data in at      if the number of flags in a
data becomes two  i e n f   xi   yi          the data is taken away from st      let dt    to be a set
of those data that are taken away from st      then  we obtain st       st    dt      at the iteration
of t      st     is used as a training set  again  if f  t       f  t     then we keep t    as
updated as in undo method  the probability method is similar to the two out method  the
difference is in how to update s  in the probability method  for each data    xi   yi     the
probability of being sampled to at at t iteration  lets call that probability p  xi   yi     is set to
  in the beginning  as the iteration proceeds  these probabilities are updated by examining
the objective function value  if f  t        f  t     then we decrease p  xi   yi   for each   xi   yi  
in at   the variant algorithm is shown below 
input 

s   t   k

w 

initialize  choose
for

s t 

w 

 



 



t               t
choose

at  s   where at   k  

 
t

set

a      x  y    at   y wt   x        

set

wt           t    wt  

 

 

if

t
k



  x   y   at 

 

t

yx

f  t        f  t  

t      t

set

i               size  at  
n f   xi   yi     n f   xi   yi      

for

if

n f   xi   yi      
set

dt      dt        xi   yi      st       st    dt   

else
set

t      min         wt  
w

t  
 

output 

 
 

t   

  result
first of all  we downloaded the pegasos source code from http   www cs huji ac il 
shais code index html  there are three sample sets of training data and one test data set 
one training set of size       two other sets of musch smaller size and a test set of size     
one interesting characteristic of this data set is that l  norm of every data is one  this puts all
the data onto the surface of   spheres  we needed to look for more general data sets which
do not exhibit such a characteristic  thus  we use the training set and test sets given for
cs    homework     the size of training set is      and test set has size of     
one of the interesting characteristics of the pegasos is that we have a control on the size of
at   which is k  if k      then pegasos is very similar to stochastic gradient method  if k   s  

fithen it results in a modified gradient descent algorithm 

    undo method
what the undo method actually dose in subgradient based algorithm is to prevent the ascent
direction  please see fig     and      in the beginning of the iterations  the undo method
seems to do much better than the pegasos  however  as the iteration reaches the point
close to the convergence region  the pegasos does better than undo method  this result is
analogs to the comparison between steepest descent algorithm and conjugate gradient
method  although the steepest descent may exhibit better performance in the beginning of
iterations  the one that wins the performance is conjugate gradient method  the lesson here
is that allowing ascent direction sometimes can be poison at that moment  but can be
medicine  as described in algorithm description section  we combined the undo  two out  and
probability method and the result is that the undo method is dominant in choosing the search
direction  in fig     and      all the three methods  i e  undo method  undo probability
combined method  and undo probability two out combined method exhibit almost identical
performance  

    two out method
since weve found that the undo method should not be used  we decided not to use it in both
two out method and probability method  thus  in two out method  although f  t        f  t  
in some t  we keep the wt   as updated  only thing we change is to update s every iteration
by eliminating data whose flag is equal to two  the result is almost identical to the original
pegasos method   i e  see the fig     and      we strongly suspect that the reason for the
almost identical result is that the training set we have  i e  the size of about       is too
small to take an advantage of two out method  to take an advantage of two out method  we
need enough iteration to get rid of outliers  the average number of data taken away from the
training set until convergence is around     among    data  there may be good data in it  it
is because taking away the data whose number of flag is two is too cruel  thus  we took
away the data whose number of flag is four  then  however  the number of data taken away
from the training set is very small  i e  zero  one or two until the convergence   the reason
that we have hope in this method is that even with our training set  the two out method
exhibits slightly better performance than the pegasos right before the convergence occur  i e 
see fig       however  this is simply our hope and it need to be proven with bigger training
set in the future 

    probability method
as in case of the two out method  the probability method exhibits almost identical result to
pegasos  i e  see fig     and       this also leaves us a hope that the probability method may
  works better than pegasos when the training set size is much bigger 

figure   comparison between pegasos and undo method

figure   comparison between pegasos and undo method

fifigure   undo method is dominant in choosing search direction

figure   comparison between pegasos and two out method

figure   undo method is dominant in choosing search direction

figure   comparison between pegasos and two out method

figure   zoomed in comparison between pegasos and two out method

fifigure   comparison between pegasos and probability method

figure   comparison between pegasos and probability method

  conclusion
developing a new algorithm is a very hard job  after reading many papers and getting very
deep insight about the algorithm  then finally someone can develop what is so called noble
algorithm  throughout the project  we have tried many other algorithms which have not
been presented in this report  however the results were all pessimistic  initially  for example 
we tried to extract only support vectors from training set to increase the convergence rate 
however  due to the presence of outliers  when we extract the support vectors  svs  
outliers were also extracted with svs  this caused even worse test error than pegasos gave 
in this report  we presented three different methods as a variant of pegasos  undo method 
two out method and probability method  the undo method acted like steepest descent
method  it may converge to a good solution  but in other times  it would not converge to a
good solution  both two out method and probability method give a way of finding outliers in
training set  taking away data that gets two flags during the simulations  we may have high
risk of giving away a good data  this result can be fixed either by increasing two to four or
five  however to accomplish this  we need a bigger training set  the probability method also
shows the potential to be used in bigger training set 

  reference
   shalev shwartz  s   singer  y  and srebro  n          pegasos  primal estimated
sub gradient solver for svm  proceedings of the   th international conference
on machine learning 
   joachims  t          making large scale svm learning practical  mit press
cambridge  ma  usa
   joachims  t          training linear svms in linear time  proceedings of the
  th acm sigkdd international conference on knowledge discovery and data
mining

fi