cs     final project  single image depth estimation from predicted
semantic labels
beyang liu beyangl cs stanford edu
stephen gould sgould stanford edu
prof  daphne koller koller cs stanford edu
december         

 

introduction

sifier in the predicted semantic label of a pixel  this classifier is trained on a standard set of    filter response features     computed in a small neighborhood around each
pixel  the mrf also includes pairwise potential terms 
which define a contrast dependent smoothing prior between adjacent pixels  encouraging them to take the same
label  the weighting between the singleton terms and the
pairwise terms is determined via cross validation on the
training set  given the semantic labeling of the image  we
also predict the location of the horizon  we now begin
our discussion of the second phase of our algorithm with
an overview of the geometry of image formation 

recovering the  d structure of a scene from a single image is a fundamental problem in computer vision that has
applications in robotics  surveillance  and general scene
understanding  however  estimating structure from raw
image features is notoriously difficult since local appearance is insufficient to resolve depth ambiguities  e g   sky
and water regions in an image can have similar appearance
but dramatically different geometric placement within a
scene   intuitively  semantic understanding of a scene
plays an important role in our own perception of scale
and  d structure 
our goal is to estimate the depth of each pixel in an
image  we employ a two phase approach  in the first
phase  we use a learned multi class image labeling mrf
to estimate the semantic class for each pixel in the image 
we currently label pixels as one of  sky  tree  road  grass 
water  building  mountain  and foreground object 
in the second phase  we use the predicted semantic class
labels to inform our depth reconstruction model  here  we
first learn a separate depth estimator for each semantic
class  we incorporate these predictions in a markov random field  mrf  that includes semantic aware reconstruction priors such as smoothness and orientation  motivated
by the work of saxena et  al        we explore both pixelbased and superpixel based variants of our model 

 

   

image formation and scene geometry

consider an ideal camera model  i e   with no lens distortion   then  a pixel p with coordinates  up   vp    in the
camera plane  is the image of a point in  d space that
lies on the ray extending from the camera origin through
 up   vp   in the camera plane  the ray rp in the world coordinate system is given by
 
up
rp  r  k   vp 
   
 

fu
k  
 

depth estimation model


 
r    
 

as mentioned above  our algorithm works in two phases 
out of concern for length  we forgo a detailed discussion of
the semantic labeling phase here  briefly  our model can
use any multi class image labeling method that produces
pixel level semantic annotations            the particular
algorithm we employ defines an mrf over the semantic
class labels  sky  road  water  grass  tree  building  mountain and foreground object  of each pixel in the image  the
mrf includes singleton potential terms  which are simply
the confidence of a multi class boosted decision tree clas 

 
fv
 

 
cos 
 sin 


u 
v  
 

   


 
sin 
cos

   

here  r defines the rotation matrix from camera coordinate system to world coordinate system and k is the camera matrix        fu and fv are the  u  and v scaled  focal
  in

our model  we assume that there is no translation between the
world coordinate system and the camera coordinate system  i e   that
the images were taken from approximately the same height above the
ground 

 

fi a  image view

 b  side view

figure    illustration of semantically derived geometric
constraints  see text for details 

our feature vector includes the same    raw pixel filter features from the semantic labeler phase and also the
log of these features  these describe the local appearance of the pixel  we also include the  u  v  coordinates of
the pixel and an a priori estimated log depth determined
by pixel coordinates  u  v  and semantic label lp   the
prior log depth is learned  for each semantic class  by averaging the log depth at a particular  u  v  pixel location
over the set of training images  see figure     since not
all semantic class labels appear in all pixel locations  we
smooth the priors with a global log depth prior  the average of the log depths over all the classes at the particular
location   we encode additional geometric constraints as
features by examining the three key pixels discussed in
section      for each of these pixels  bottommost and
topmost pixel with class lp and topmost ground pixel  
we use the pixels prior log depth to calculate a depth
estimate for p  assuming that most objects are roughly
vertical  and include this estimate as a feature  we also
include the  horizon adjusted  vertical coordinate of these
pixels as features  note that our verticality assumption is
not a hard constraint  but rather a soft one that can be
overridden by the strength of other features  by including
these as features  we allow our model to learn the strength
of these constraints  finally  we add the square of each
feature  allowing us to learn quadratic depth correlations 
we note this set of features is significantly simpler than
those used in previous works such as saxena et  al       
for numerical stability  we also normalize each feature to
zero mean and unit variance 
we learn a different local depth predictor  i e   a different linear regression  for each semantic class  motivated by the desire to more accurately model the depth
of nearby objects and the fact that relative depth is more
appropriate for scene understanding  we learn a model to
predict log depth rather than depth itself  we thus estimate pointwise log depth as a linear function of the pixel
features  given the pixels semantic class  

lengths of the camera  and the principal point  u    v    is
the center pixel in the image  as in saxena et  al      
we assume a reasonable value for the focal length  in our
experiments we set fu   fv       for a          image   the form of our rotation matrix assumes the cameras horizontal x axis is parallel to the ground and we
estimate the yz rotation of the camera plane from the predicted location of the horizon  assumed to be at depth   
   tan    f v  v hz v      in the sequel  we will assume that
rp has been normalized  i e   krp k       
we now describe constraints about the geometry of a
scene  consider  for example  the simple scene in figure   
and assume that we would like to estimate the depth of
some pixel p on a vertical object a attached to the ground 
we define three key points that are strong indicators of
the depth of p  first  let g be the topmost ground pixel
in the same column below p  the depth of g is a lower
bound on the depth of p  second  let b be the bottommost
visible pixel b on the object a  by extending the camera
ray through b to the ground  we can calculate an upper
bound on the depth of p  third  the topmost point t on
the object may also be useful since a non sky pixel high in
the image  e g   an overhanging tree  tends to be close to
the camera 
simple geometric reasoning allows us to encode the first
two constraints as
t
log dp   l
f
   
 
 
 
p p
t
rgt e 
rgt e 
rb e 
dg t
 dp  dg t
   
rp e 
rpt e 
rb e 
where dp is the pointwise estimated depth for pixel p  lp
is its predicted semantic class label  fp  rn is the pixel
where dp and dg are the distances to the points p and feature vector  and  l  ll are the learned parameters of
g  respectively  and ei is the i th standard basis vector  the model 
the third constraint can similarly be encoded as dt rtt e  
dp rpt e    we incorporate these constraints both implicitly
as features and explicitly in our pixel mrf model 
    mrf models for depth reconstruc 

   

tion
features and pointwise depth estithe pointwise depth estimation provided by eq      is
mation

somewhat noisy and can be improved by including prithe basis of our model is linear regression toward the log  ors that constrain the structure of the scene  we dedepth of each pixel in the image  the output from the velop two different mrf modelsone pixel based and one
linear regression becomes the singleton terms of our mrfs  superpixel basedfor the inclusion of such priors 

fi a  sky

 b  tree

 c  road

 d  grass

 e  water

 f  building

 g  mountain

 h  fg  obj 

figure    smoothed per pixel log depth prior for each semantic class with horizon rotated to center of image  colors
indicate distance  red is further away and blue is closer   the classes water and mountain had very few samples
and so are close to the global log depth prior  not shown   see text for details 

     

pixel based markov random field

our pixel based mrf includes a prior for smoothness 
here  we add a potential over three consecutive pixels  in
the same row or column  that prefers co linearity  we also
encode semantically derived depth constraints which penalize vertical surfaces from deviating from geometrically
plausible depths  as described in section       formally 
we define the energy function over pixel depths d as
e  d   i  l   

x

p  dp    

 
 

 z

data term

pg  dp   dg    

p

pqr  dp   dq   dr  

pqr

p

x

x

x

 

 

pb  dp   db    

p

 

 z

smoothness

x

 

pt  dp   dt  

   

p

 z

geometry  see     

 


pq   exp c  kxp  xq k  measures the contrast between two adjacent pixels  where xp and xq are the cielab
color vectors for pixels p and q  respectively  and c is the
mean square difference over all adjacent pixels in the image   incidentally  this is the same contrast term used
by the mrf in the semantic labeling phase   we choose
the prior strength by cross validation on a set of training
images 
the soft geometry constraints pg   pt and pb model
our prior belief that certain semantic classes are vertically
oriented  e g   buildings  trees and foreground objects  
here  we impose the soft constraint that a pixel within
such a region should be the same depth as other pixels in
the region  i e   via the constraint on the topmost and bottommost pixels in the region   and be between the nearest
and farthest ground plane points g and g   defined in section      the constraints are encoded using the huber
penalty   e g   h dp  dg     for the nearest ground pixel
constraint   each term is weighted by a semantic specific
prior strength  gl   tl   bl  ll  

where the data term  p   attempts to match the depth
for each pixel dp to the pointwise estimate dp   and pqr
represents the co linearity prior  the terms pg   pb and
pt represent the geometry constraints described in section     above  recall that the pixel indices g  b and t are
determined from p and the semantic labels 
     
the data term in our model is given by
p  dp     h dp  dp    

   

where h x    is the huber penalty  which takes the value
x  for   x   and    x     otherwise  we
choose the huber penalty because it is more robust to outliers than the more commonly used     penalty and  unlike
the robust     penalty  is continuously differentiable  which
simplifies inference   in our model  we set         
our smoothness prior encodes a preference for colinearity of adjacent pixels within uniform regions  assuming pixels p  q  and r are three consecutive pixels  in
any row or column   we have
pqr  dp   dq   dr    
smooth 



pq qr  h  dq  dp  dr    

   

where the smoothness penalty is weighted by a contrastdependent term and the prior strength smooth   here 

superpixel based markov random field

in the superpixel model  we segment the image into a set of
non overlapping regions  or superpixels  using a bottomup over segmentation algorithm  in our experiments we
used mean shift      but could equally have used a graphbased approach or normalized cuts  each superpixel si is
assumed to be planar  a constraint that we strictly enforce 
instead of defining an mrf over pixel depths  we define an
mrf over superpixel plane parameters   i    where any
point x  r  on the plane satisfies it x      the depth
of pixel p corresponds to the intersection of the ray rp and
the plane  and is given by  it rp     
we define an energy function that includes terms that
penalize the distance between the superpixel planes and
the pointwise depth estimates dp  eq       and terms
that enforce soft connectivity  co planarity  and orientation constraints over the planes  all of these are conditioned on the semantic class of the superpixel  which we
define as the majority semantic class over the superpixels

fi   

constituent pixels   formally  we have

inference and learning

both of our mrf formulations  eq      and eq       dee     i  l  s   
fine convex objectives which we solve using the l bfgs
x
x
x
p  ip    
i  i    
ij  i   j       algorithm     to obtain a depth prediction for every pixel
in the imagefor the superpixel based model we compute
p
i
ij
   z  
 
 z
 
 z
 
 
pixel depths as dp   t rp where i are the inferred plane
orientation prior
data term
connectivity and
i
co planarity prior
parameters for the superpixel containing pixel p  in our
experiments  inference takes about   minutes per image
here ip indicates the i associated with the superpixel for the pixel based mrf and under    seconds for the
containing pixel p  i e   i   p  si  
superpixel based model  on a          image  
region data term  the data term penalizes the
the various prior strengths  smooth   etc     are learned
plane parameters from deviating away from the pointwise by cross validation on the training data set  to make this
depth estimates  it takes the form
process computationally tractable  we add terms in an incremental fashion  freezing each weight before adding the


 
h dp  it rp     
     next term  this coordinate wise optimization seemed to
p  i    
dp
yield good parameters 
where h x    is the huber penalty as defined in section       above  we weight each pixel term by the inverse pointwise depth estimate to give higher preference
to nearby regions 
orientation prior  the orientation prior enables us
to encode a preference for orientation of different semantic surfaces  e g   ground plane surfaces  road  grass 
etc     should be horizontal while buildings should be vertical  we encode this preference as
i  i     ni  l  kpl  i  l   k 

    

where pl projects onto the planar directions that we would
like to constrain and l is the prior estimate for the orientation of a surface with semantic class label li   l  we
weight this term by the number of pixels  ni   in the superpixel and a semantic class specific prior strength  l   
the latter captures our confidence in a semantic classs
orientation prior  for example  we are very confident that
ground is horizontal  but we are less certain a priori about
the orientation of tree regions  
connectivity and co planarity prior  the connectivity and co planarity term captures the relationship
between two adjacent superpixels  for example  we would
not expect adjacent sky and building superpixels to
be connected  whereas we would expect road and building to be connected  defining bij to be the set of pixels
along the boundary between superpixels i and j  we have
ij  i   j    

ni   nj conn x


kit rp  jt rp k 
  bij   lk
pbij

 

ni   nj co plnr
lk
 ki  j k 
 

    

where we weight each term by the average number of pixels
in the associated superpixels and a semantic class specific
prior strength 

 

experimental results and discussion

we run experiments on the publicly available dataset from
saxena et  al        the dataset consists of     images
with corresponding depth maps and is divided into    
training and     testing images  we hand annotated the
    training images with semantic class labels  the    
training images were then used for learning the parameters
of the semantic and depth models  all images were resized
to          before running our algorithm  
we report results on the     test images  since the
maximum range of the sensor used to collect ground truth
measurements was   m  we truncate our predictions to the
range          table   shows our results compared against
previous published results  we compare both the average
log error and average relative error  defined as   log   gp 
 g d  
log   dp   and pgp p   respectively  where gp is the ground
truth depth for pixel p  we also compare our results to
our own baseline implementation which does not use any
semantic information 
both our pixel based and superpixel based models
achieve state of the art performance for the log   metric
and comparable performance to state of the art for the relative error metric  importantly  they achieve good results
on both metrics unlike the previous results which perform
well at either one or the other  this can be clearly seen in
figure   where we have plotted the performance metrics
on the same graph 
having semantic labels allows us to break down our results by class  our best performing results are the ground
plane classes  especially road   which are easily identified
  note that  although the horizon in the dataset tends to be fixed
at the center of the image  we still adjust our camera rays to the
predicted horizon location 

filog  
     
     
     
     
     
     
     
     

method
scn 
heh 
pointwise mrf 
pp mrf 
pixel mrf baseline
pixel mrf model        
superpixel mrf baseline
superpixel mrf model        


rel 
     
     
     
     
     
     
     
     

results reported in saxena et  al       

figure    quantitative results comparing variants of
our semantic aware approach with strong baselines and
other state of the art methods  baseline models do not
use semantic class information 
by our semantic model and tightly constrained geometrically  we achieve poor performance on the foreground
class which we attribute to the lack of foreground objects
in the training set  less than    of the pixels  
unexpectedly  we also perform poorly on sky pixels
which are easy to predict and should always be positioned
at the maximum depth  this is due  in part  to errors
in the groundtruth measurements  caused by sensor misalignment  and the occasional misclassification of the reflective surfaces of buildings as sky by our semantic model 
note that the nature of the relative error metric is to magnify these mistakes since the ground truth measurement in
these cases is always closer than the maximum depth 
finally  we show some qualitative results in figure   
the results show that we correctly model co planarity of
the ground plane and building surfaces  notice our accurate prediction of the sky  which is sometimes penalized
by misalignment in the groundtruth  e g   third example  
our algorithm also makes mistakes  such as positioning
the building too close in the second example and missing
the ledge in the foreground  a mistake that many humans
would also make  
overall  our model attains state of the art results 
though it utilizes relatively simple image features  because
it incorporates semantic reasoning about the scene 
    

scn    
relative error

   

region baseline
pointwise mrf     
pixel baseline

    

   

our superpixel mrf

ppmrf     

our pixel mrf
    

    

    

    

    

log   error

    

   

    

figure    plot of log   error metric versus relative error
metric comparing algorithms from table   bottom left indicates better performance 

figure     above  some qualitative depth reconstructions
from our model showing  from left to right  the image 
semantic overlay  ground truth depth measurements  and
our predicted depths  red signifies a distance of   m 
black  m   below  example  d reconstructions 

references
    d  comaniciu and p  meer  mean shift  a robust approach
toward feature space analysis  pami       
    s  gould  r  fulton  and d  koller  decompsing a scene into
geometric and semantically consistent regions  in iccv 
     
    x  he  r  zemel  and m  carreira perpinan  multiscale
crfs for image labeling  in cvpr       
    d  liu and j  nocedal  on the limited memory method for
large scale optimization  in mathematical programming b 
     
    y  ma  s  soatto  j  kosecka  and s  s  sastry  an invitation
to   d vision  springer       
    a  saxena  m  sun  and a  y  ng  make d  learning   d
scene structure from a single still image  in pami       
    j  shotton  j  winn  c  rother  and a  criminisi  textonboost  joint appearance  shape and context modeling for
multi class object recognition and segmentation  in eccv 
     

fi