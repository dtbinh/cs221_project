click prediction and preference ranking of rss feeds
december         

steven wu

 

introduction

rss  really simple syndication  is a family of data formats used to publish frequently updated works  rss
is often used for temporally relevant information updates  an rss document consists of a series of entries 
each tagged with a title  description  link  date of publication  and sometimes other metadata 
however  it is often the case that a large amount of rss feeds in aggregate will create considerably more
information than the average user will be able to follow  in this case the user will read only those feeds that
he or she finds interesting  however  the volume of information involved can sometimes make this search for
interesting reading a time consuming and laborious task 
several machine learning methods have been applied with some success to text categorization problems 
such as bayesian spam filters for email     here we explore text categorization algorithms for the purpose of
ranking user preferences for rss entries 

 

preference and clicks

in our problem we are given a series of rss feeds and click information and must derive from these the
preferences of the user  we are given a set of candidate entries x    x  i     for each x  i  we define y  i  to
correspond to the number of clicks made on x  i    we also store publication time t i    ie the time at which
 i 
the entry was produced  and associated timestamp vectors z  i  such that zj is the time of j   th click on
x i    hence  y  i     z  i     we will also specifically discuss the time to click c i    defined as
 
 i 
z   t i  for  z  i        
 i 
c  

for  z  i       
each entry x  i  also has a corresponding feed id f  i  which specifies the specific feed that it belongs to  the
importance of this is that different rss feeds are treated in different ways by the user   a user may check
certain feeds more often or be more likely to click entries from it due to topical reasons 
we are attempting to predict preference using click data as an indicator for preference  however  clicks
are not a perfect indicator of preference  in that the average user will not have an opportunity to click every
article that interests them  it is also the case that even if an article is to be clicked  it will likely not happen
for some nonzero amount of time after that article has been published on rss 
hence we will make a few assumptions regarding click behavior  first  we assume that there exists some
latent variable i  i  tied to x  i  which is an indicator for whether an entry will ever be clicked  we also
assume that if an entry x  i    then c i  is distributed as an exponential  this is to deal with the following
difficulty  given some time t  if we have some entry xi where yi     then we know both ci and ii   however 
if  conversely  yi      then we have   possible explanations  first  the entry may not ever be read  ie ii     
second  the entry may be too old  and the reader may have seen the entry elsewhere  ie t  ti is too large 
lastly  the reader may eventually read this entry  but not have had the opportunity to yet  ie ti   ci   t 
we may now formally state that our goal is to determine p  i  i        i e  the chance that the user will
ever read a given feed entry 
 i 
with these assumptions we can find probability p yt   y  i    by specifying functions for the rate
parameter  of the exponential  f  x   and a preference function fi  x  returning a vector of  and i
respectively  we then need to decompose the distribution into a time distribution  exponential  and an
binomial indicator  the standard bayesian technique here would be to construct a generative model for y  i   
z  i    and c i  based on x  i    however  the exact motivations behind rss preference are not always very clear
and often vary from case to case  and so this is not feasible in our case  hence  it would be ideal to train

fia classifier under this model  naively  we can inject features into our data by creating for each data x  i  a
 i 
large number of yt indicating the number of clicks at each time t  however  this is not entirely feasible  as
we would have to increase an already large dataset by another very large constant factor  hence  the naive
approach fails here 
however  in general we are viewing y  i  and zi as time series  and attempting to model y and z  by
mle  given the rate parameter   we can train a regressor on
 
 
for y  i     
 i  
y
  r
exp t     dt  for y    
t
note that here y  i     p i  i       given our assumptions  in this case we can determine  merely by looking
at all entries x  i  that can no longer be clicked  ie are no longer on the rss feed  and their respective c i   
we can then simply determine  using any number of analytical methods 
with the knowledge of this distribution in mind we can then adapt traditional text categorization
algorithms to use the soft labels y    

 

data and methodology

our dataset consists of         rss entries from a total of     unique rss feeds  of which       have been
clicked  these were collected over a period of three months and represent a cross section of real life rss
usage 
for evaluation purposes we pick a time t and partition the dataset such that we train on xi where
zi     t and test on the remainder  in all the cases we use a modified vector space model where the first
dimension of our feature space for x  i  is f  i    and each following dimension corresponds to a word in the
corpus  for all tests here we train on the first     entries chronologically and then attempt to predict future
clicks on entries beyond those     

   

naive bayes

a naive bayes classifier is a special case of bayesian network applied to the task of classification  naive
bayes assumes that every feature x  i  is conditionally independent from every other feature given y  or 
formally 
y
 i 
p x  i    x y     c   
p xi   xi  y     c 
i

using bayes rule our hypothesis looks as follows 
h x    p i

 i 

q
p i  i       i p xi   xi  
    x    p y     x    q
i p xi   xi  c   ck  
 

in our case we are attempting to classify based on soft labels y   rather than discrete labels  to account
for this when training  for each entry x  i  we consider it instead to be y  i   entries with y  i       and    y  i  
entries with y  i       

   

regularized logistic regression

logistic regression is a conditional probability model in which we assume that the posterior is logistic  ie
that
p i  i     xi    

 
    exp t x 

here we can generate  by minimizing the negative log likelihood

fil     

x

log     exp t xy     

i

across our dataset  note in this case we use the calculated y   rather than y  we l  regularize our dataset
by using the lasso algorithm  which is known to have good performance with text categorization     hence
l  lasso   l       
where we use mallows cp as   we can then easily solve for the maximizing value of  using this
function with gradient descent 

   

decision trees and random forests

for the purposes of generating decision trees we use quinlans c    algorithm      again  to account for the
use of soft labels when maximizing information gain  for each entry x  i  we consider it instead to be y  i  
entries with y  i       and    y  i   entries with y  i       
we create random forests as described by breiman et al in      in the random forests technique  we
construct each individual decision tree as above  bootstrapping the data in each case  to create the complete
ensemble 

   

support vector machines

the case of svms is a unique one since svms cannot consider soft labels  hence we train the svm only
by using those entries in x that have been removed from user access and hence can no longer be clicked 
using a gaussian kernel with automatic relevance determination  ard   apart from this pre processing
the training is a reasonably straightforward application of platts smo algorithm on these points     

 

results and conclusions

we run each of these algorithms on the data set as described  the performance of each algorithm can be
seen in table   
perhaps one of the most interesting results here is the unusually good performance of naive bayes 
support vector machines and random forests have been shown to significantly outperform naive bayes in
     however  here we see that naive bayes performs only slightly worse than random forests and in fact
outperforms the svm 
we suspect this performance gap is caused by most of the predictive power of our features being in the
feed id f  i  rather than in the actual bag of words  this is intuitively sensible  as feeds  like human interests 
tend to be restricted to a few topics  and an interesting feed will tend to be clicked significantly more often 
this is also an explanation of the poor performance of logistic regression  logistic regression cannot account
for the feed ids properly as it can only separate in the feature space  in which a meaningful distinction and
ordering does not exist for the feeds   hence  it is forced to operate entirely on the textual information given 
svms suffer a similar plight  the remaining algorithms that can consider a feature independent of any given
space fare much better 
the strong predictive capability of the feed id intuitively makes sense  most rss feeds tend to be
focused around one or two topics  as do most user preferences  hence  if a user clicks on entries from a given
feed it is quite likely that he is interested in the topics that feed provides  one can think of the feed id as
a latent variable linked directly to the topic of the feed 
we also note a strong mutability in preferences  figure   shows the ability of three of the classifiers
trained on data  and then tested on increasingly more recent data  we note an almost exponential degradation of prediction quality as we increase our prediction horizon  hence  in order to properly predict clicks
we need a dynamic model  or at least one that is frequently re trained 

fi   

prediction degradation over time

   
   
   
   

clicks predicted

   

random forests
decision tree
svm

 

    

    

    

    

prediction horizon

figure    predictive performance over increasing prediction horizon  time t

    

fialgorithm
naive bayes
regularized logistic regression
decision trees
random forests
svm

rmse
    
    
    
    
    

table    rmse on testing set for the various classification algorithms 

 

further directions

as it seems the most performant feature we observed was the feed id  which seems to derive its predictive
power from being directly linked to topic  the next logical step seems to be topic models  we plan on
exploring latent dirichlet allocation and the hierarchical version thereof in order to create topic models
with rss feeds  which will allow us to classify with potentially greater accuracy than before  other possible
approaches include collaborative filtering  however  these require a volume of data that we  at this moment 
do not have access to 

 

references
   sahami  m   dumais  s   heckerman  d  and horvitz  e         a bayesian approach to filtering junk
e mail  aaai workshop on learning for text categorization 
   genkin  a   lewis  d  and madigan  d         sparse logistic regression for text categorization 
   quinlan  j         c     programs for machine learning  morgan kaufmann publishers 
   breiman  l          random forests  machine learning 
   platt  j          fast training of support vector machines using sequential minimal optimization 
technical report msr tr       
   caruana  r  and miculescu mizil  a         proceedings of the   rd international conference on machine learning

fi