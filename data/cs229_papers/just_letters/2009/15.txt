cs     grant prediction with network features
stefan krawczyk
computer science department  stanford university

stefank cs stanford edu

abstract
this project investigates supervised learning to predict grant
proposal approval or rejection  using the mimir projects
data on external grants applied for by stanford faculty  baseline features relating to the grant itself and faculty features
are used  in particular the novel use of a faculty members
network to extract features is investigated and its effects on
grant prediction 

  

introduction

grants take time and effort to put together and there is
often anxiety related to their outcome  wouldnt it be great
for faculty members to assess their grant proposal before
sending it out 
lots of research lately has been done to try to study and
analyze networks were people are represented as nodes and
edges represent some type of relationship  in social networks
this is a friend edge  in citation networks this is a cite from
one paper to an older paper  or in collaboration networks the
edge represents a link between co authors  the properties
of such networks then have been studied 
burt    showed that people who sit in structural holes  or
places of the network where they connect distant components  look to gain quicker promotions  better salaries  better ideas than their peers in the electronics company he analyzed  burt    also produces networks constraints which help
to detect these structural holes  this shows that there are
certain characteristics of a network that could be amenable
to a machine learning algorithm 
having access to a stanford faculty data set  ripe for feature
extraction  containing information relating faculty to each
other through relationship links  in addition to supplemental
information about age  tenure  etc  a network of faculty as
vertices and edges as relationships can be produced 
the saying success breeds successcan also be hypothesized

to mean that the relationships that one has with other people helps in determining your own success  can we given
the different network relationship information  help predict
grant approval and rejection of stanford faculty 
this paper is structured in the following way  section   gives
an overview of the data  section   details the approach  section   talks about the feature sets  section    showcases the
results and section   wraps up with the conclusion and future work 

  

dataset

the data comprises of a collection of sql databases  that
was put together for the mimir project wanting to study the
flow of knowledge ideas in an academic network  it contains
information from      to      on 
 external grants applied for by faculty  the applicants 
the amount proposed and awarded  sponsoring organization  school and department the grant is under 
grant approval or rejection  and whether there are continuing grants 
 dissertation committees faculty sit on and thesis details of the student
 co taught courses  from      
 co authored publications  not complete 
 supplemental information on the faculty like tenure
status  age  gender  tenure status  position  number
of appointments  courtesy appointments 
this data is both quite interesting and unique  as well as
plentiful  the grant data in particular has over    thousand
grant proposals  with a roughly       split in approvals and
rejections 
in addition to grant prediction  there are many other options with which one could pursue both supervised or unsupervised learning  for instance the data is quite amenable
to clustering  one could try to figure out which department
courtesy appointments really belong to based on the data
and see how that changes with time  there is also lots of
prediction based tasks possible for example predicting departure of faculty  or number of students graduating in a
given year  etc 

fi  

approach

the data resided in sql tables which after some manipulation was then output to csv files  feature extraction was
then performed on the data to produce the three different
feature sets 
once the feature sets were produced they were fed into three
different machine learning algorithms  logistic regression  a
linear classifier  and a support vector machine  svm   combinations of the feature sets were explored to discern the
impact of the different feature sets using two approaches 
   randomly intermingling the years  not using any cumulative features  thus training and testing on grants
from the entire time span  this was the quickest test
but not a realistic test 
   training on all previous years and testing on the next
year   thus testing on each year from      to      
this would be the realistic test as people would want
to know how the current years grant proposals are
predicted 
logistic regression was first used to test out the different
feature sets before running the other classifiers  as the turn
around time was much quicker 
the linear classifier was chosen because it was a readily available package and because the data was rather high dimensional  it was hypothesized that this classifier should be able
to find some decent hyperplane  initially gradient descent
was used  but it kept getting stuck and was changed in favour
to use the quasi newton method 
lastly the svm with a linear kernel was picked because
training it took the longest amount of time  and it was hoped
that it should be as good if not better than the linear classifier 

   

software packages

there were two software packages that were used  the java
universal network graph     jung  and the javanlp library 
jung was used for creating the network graphs so that
feature extraction on the graphs could be performed  it
was chosen because it was in java and would interface well
with the javanlp library and that it had a few network
measures metrics that would be of use in feature extraction 
the javanlp library was utilized for all of the machine
learning algorithms 

  

features

the bulk of this projects work was in feature creation and
extraction  in addition to what is described below  the log
was taken of all continuous valued features  while the square
was take only of all continuous valued features that would
not cause an possible underflow or overflow to add to the
feature mix  the following describes the three feature sets
produced 

   

grant signature features

these features were extracted entirely from the grant proposal itself  these were to be the simple baseline features
that would be added to by the other feature sets 
it was comprised of  a bernoulli bag of word model on the
project title  a bernoulli bag of word model on the sponsoring organization  the amount proposed  the school id the
grant is from turned into a categorical feature  the deparment id the grant is from turned into a categorical feature 
and the year 

   

non network faculty features

this feature set was based on anything else that could be
extracted from the sql tables that was not a network feature  that could be related to the faculty on the grant application  it was decomposed into three subsets  professor 
department and school sets  when appropriate  cumulative
features that spanned the history until the current year were
also produced 
the professor set dealt with features directly attributable to
one particular facutly member and was the largest set out
of the three 
this set comprised of  gender  tenure status  primary department id  categorical feature   school id  categorical feature   primary job class code  categorical feature   number of
appointments  h index  was only available for a select number of faculty   number of courtesy appointments  number of
dissertation students  number of publications  not complete
for any faculty   ethnicity as a categorical feature  age  hire
year  time at stanford  total number of grants awarded last
year  total number of grants rejected last year  total grants
applied for last year  cumulative number of grants applied
for  previous years grant success rate  overall grant success up to last year  total amount proposed  total amount
awarded  average amount proposed  ratio of total amount
awarded over total amount proposed  average amount proposed for awarded grants  average amount proposed for rejected grants  and ratio of average proposed amount awarded
over average proposed amount rejected 
the school and department sets dealt with aggregate features that covered the school and department respectively 
the idea would be that would help cover and school or department wide attributes related to grants  these sets had
features comprised of the aggregate grant related features
from the professor set 

   

network faculty features

this feature set was largest and took the most time to produce out of all the sets  it was produced by running a lot
of network related metrics on the produced network graphs
for each faculty member in the graph 
eight graphs were produced per year  dissertation committees   not complete  co authorship  co taught classes  new
grants awarded in that year  grants continuing  grants continuing and awarded  grants rejected that year  and a combined edge set  the edges also contained weights representing the number of interactions that the faculty had together 

fifigure    results of testing on randomly intermingled years for logistic regression 

figure    an example of a network graph created 

for feature extraction weighted as well as unweighted versions were produced where possible  an example graph can
be seen in figure   
the following was extracted from each graph for each faculty member  five structural hole signature measures     aggregate constraint  constraint  effective size  efficiency  hierarchy  barycenter value   random walk betweeness value   
betweeness centrality    closenes centrality    eigenvector centrality    clustering coefficient    number of total edges  radius
one and two features  number of tenured and untenured
links  number of awarded and continuing grants  number
of rejected grants  number of grant and publication edges
that overlapped  number of publications  neighbour edge incidence  and cumulative weighted  unweigted and average
weight edge incidence 

  

results

for all the results we had a majority baseline of roughly    
for each test 

   
     

randomly intermingling the years
logistic regression

in figure   we see the results from running logistic regression 
the best testing accuracy came from the combined grant
and network feature sets  so a positive result for the use of
network features  from the graph we can also see that grant
features by themselves do not give anything above baseline 
while adding the other feature sets allows everything to be
just above it 
the convergence parameters where fiddled with extensively
for logistic regression  but did not yield any improvement
over these results 
 

sum of distances to each vertex
measures the expected number of times a node is traversed
by a random walk averaged over all pairs of nodes
 
how many shortest paths go through me
 
based on average distance to each vertex
 
the fraction of time that a random walk will spend at that
vertex over an infinite time horizon
 
how dense is my network around me
 

figure    results of testing on randomly intermingled years for the linear classifier 

     

linear classifier

counter intuitively to what the results were in figure   
the grant features by themselves produce the best result 
adding in the other features actually hurts the performance 
but adding in the network features hurts the least  even
though they overfit terribly 
interestingly the overfitting with only the addition of the
network features  but not with the addition of the nonnetwork faculty suggests that the non network features add
a lot more nosie to the data  while the network features allow
a very clear separation  thus just adding the non network
features to the grant features yields the worst results 
the convergence parameters on the quasi newton method
that was being used in the linear classifier was played with
extensively to try reduce the overfitting when using the grant
and network features  overfitting was reduced slightly down
to      but this did not change the testing accuracy much
at all 

     

svm

the svm was a disappointment  it somehow kept breaking
when the non network faculty feature set was used  nor did
it perform anywhere near as well as the linear classifier as
it should  the regularization parameter was played with to
no avail and thus only the random intermingling years test
was completed which is shown in figure    this suggests a
broken library  as the kernel used was a linear one 
 

or user error

fifigure    results of testing on randomly intermingled years for the support vector machine 

figure    testing on all years  training on the previous years for the linear classifier

overall the top features from any of the sets turned out to
be the bag of words on the title and sponsoring organization 
table   shows the comparison between logsitic regression
and linear classifier for the grant  and grant   network feature sets  notice that logisistic regression was not able to
discern betters weights for the features as the linear classifier  they are largely similar  but with the addition of
the network features  the linear classifier is then able to find
better weights for these largely similar features 
figure    testing on all years  training on the previous years for logistic regression

   
     

training on previous years  testing on the
next
logistic regression

naturally this second set of tests should be expected to have
a poor testing accuracy at the beginning as the training set
size is rather small  this is clearly exhibited in figure    as
all the testing accuracy lines start below the baseline and
only start to creep over towards the end third of the years 
the spikes in training accuracy seems to be related to the
grant feature set  this unfortunately was not investigated
further due to time constraints 

     

linear classifier

in figure   we see that in general all the features sets are
above the baseline after the first half of the training data 
again the best performance came from just using the grant
feature set  which infact was above baseline for all the years
tested  it also exhibited a text book curve in terms of increased training data reducing the overfitting and increasing
testing accuracy 
it is cool to see that towards the end all the classifiers had
increasing accuracy  and were roughly achieving the same
result  being a nice amount above baseline  this also indicates that the non network features just required more
training data to clear away any noise that was present in
the intermingled year testing 

   

top features

overall the addition of the network features changes the
top features to be more title oriented  no network features
appear in the top twenty weighted features in either logistic
regression or the linear classifier when using the grant  
network feature sets 
looking at table   the top features when using the nonnetwork features sees the top being largely related to departments and the average minimum average amount award
for a faculty member  the title or sponsoring organization
features from the grants are not to be found in the top twenty
weights 
the combined feature sets actually sees most of the top features come from the features in the network feature set  interestingly one of its top features was on the publication coauthor graph  their betweeness centrality score  which was
how often this person was on a path to all other people in
the network 

  

conclusion   future work

the bag of word model on the grant title and sponsoring organization does surprisingly well overall  but by just looking
at the logisitic regression results one could hypothesize that
the network features can help  but then looking at the linear classifier the network features actually hurt performance 
this is probably due the overfitting that is happening  not
allowing the model to generalize itself as well as just using
the baseline features  more data or removing features would
be the next approach to see whether the overfitting can be
reduced 
from the results of the year to year tests  the non network
features showed that they were not infact a hinderance as
all the classifier approached the same performance 

firank
  
  
  
  
rank
  
  
  
  

rank
  
  
  
  

table    top features given by the classifiers with corresponding weights
lc grants
lc grant   network features
gsf sponsor org affairs                    
gsf sponsor org merck                  
gsf sponsor org american                   
gsf project title agreement                  
gsf sponsor org veterans                   
gsf project title proteases                   
gsf sponsor org defense                   
gsf project title material                 
lr grants
lr grant   network
gsf proposed total                    e  
gsf project title agreement                    
gsf sponsor org institutes                   e   
gsf sponsor org  merck                    
gsf sponsor org health                    e   
gsf project title material                    
gsf project title material                    e   
gsf project title nsf                    

table    top features given by the classifiers with corresponding weights
lc grant   non network
lc combined
lg gdpt totalappliedinyear                     
gdpt ratio aa ar                    
sq gdpt totalappliedinyear                     
net grantca barycenterscorerw                      
gpf totalawarded                  e  
sq net pubcoauthor bcentralityw                     
gpf min avg awarded                 e  
lg net grantca barycenterscorer                     

the change in top features across the sets was surprising 
especially the ones picked by the combined feature set  suggesting that network features do infact hold some promise
for use in machine learning  however its clear that the bag
of word model on the grant title and sponsoring organization proved to be the best features  when the classifier was
able to weight them appropriately  in this case 

   

future work

there is lots that could be done to build on top of this work 
for brevity the top three on my agenda are 
   investigating the svm issues  as this classifier should
give just as good performance as the linear classifier 
   looking at the network feature set and investigating
the overfitting by removing features as it seems that
classifier accuracy could be easily improved if overfitting was addressed 
   approach the grant approval prediciton by clustering
the data  and investigating whether prediction could
be improved by using this clustering approach 

  

references

    r  burt  structural holes  the social structure of
competition  belknap pr       
    r  burt  structural holes and good ideas    american
journal of sociology                      
    j  osmadadhain  d  fisher  s  white  and y  boey 
the jung  java universal network graph  framework 
university of california  irvine  california       

fi