finding a basis for the neural state
chris cueva
ccueva stanford edu

i 

introduction

how is information represented in the brain  for example  consider arm movement  neurons in dorsal premotor
cortex  pmd  are selective for reach target      the type of grasp      reach speed      reach curvature      and the
required accuracy      based on these experiments we might conclude that arm movement is represented in pmd as
a combination of these features  along with some others that we havent accounted for  
example    neurons in temporal cortex  especially the fusiform gyrus in humans     and superior temporal sulcus
portion of it cortex in macaque monkeys         preferentially fire when the subject views pictures of faces   so the
features that the neurons are encoding seem to be face and not face 
but how do we know what features the neurons are selecting for  the paradigm can be summarized as follows 
   one day   having failed to drive a unit with any light stimulus  we waved a hand at the stimulus screen and elicited
a very vigorous response from the previously unresponsive neuron  we then spent the next    h testing various paper
cutouts in an attempt to find the trigger feature for this unit      in other words  the experimenter presents many
stimuli while simultaneously monitoring neuronal activity and then concludes by identifying similarities amongst
stimuli that elicit a response       this approach runs the risk of picking features that are conceptually simple to the
experimenter but that have little to do with the features encoded by neurons  in other words  the representation used
by neurons may not be simple or readily apparent 
the goal of this project is to tackle the question  how is information represented in the brain  using machine
learning algorithms that remove experimental bias in feature selection  lets make some assumptions about neuron
firing rates and then state the goal more clearly 

ii 

modeling

modeling assumption    the neural state only depends on the instantaneous firing rates  not relative spike times 
of each of the neurons  so at any point in time we can completely specify the state by the instantaneous firing rates
of each of the neurons 
modeling assumption    each neuron fires with a poisson distribution based on some underlying firing rate 
the goal of this project is to take spike time data from simultaneously recorded neurons and find the underlying
rates that gave rise to these firing patterns  the number of underlying firing rates tell us approximately how many
features are being represented  it will be left for a future project to determine what exactly the underlying firing
rates represent 
if all of the neurons in the trial have the same underlying firing rate we can use various smoothing techniques to
find this rate as shown in figure    the acausal methods use information from the whole spike train  across all times 
to compute the frequency at a specific time 
but what happens if the neurons do not have the same underlying firing rate  the normalized variance      defined
in equation     where rtrial is the firing rate on that trial and r is the mean firing rate across all trials  is   if poisson
neurons have the same underling rate and greater than   if the underlying rates are different  the normalized variance
is shown in the bottom panel of figure   for        neurons simulated with the same underlying rate  black line  top
panel  and        neurons each having different noisy versions of this underlying rate  red lines  top panel   starting
at     ms the noisy rates decay exponentially  with a time constant of     ms  to the underlying rate shown in black 
only    out of the        noisy rates are shown in the top panel of figure    the spike times for    neurons from
each group are shown in the middle panel 
n
x
c
 rtrial  t   r t   
nv t   

r t 
n 

   

trial  

the normalized variance tells us when the neurons have different underlying rates but does not tell us what those
rates are  consider two neurons that have different underlying rates that are related at all points in time as shown
by the black bar in figure         the blue dots are the measured noisy rates for the two neurons  the firing rate

fi 

fig    

relationship between the two neurons is predicted by principle component analysis  pca   red line  and factor analysis
 fa   green line  the dotted black line shows the two sd covariance ellipse 
neuron   has a higher mean firing rate and so has a higher variance than neuron         fa  which allows for
different noise variances along different dimensions and seeks the dimension of greatest covariance  is better able to
predict the firing rate relationship than pca which seeks the dimension of greatest variance and so is biased towards
neurons with higher firing rates 
a 

initial conditions

fa notation 
x  n     i 
y x  n     x   

   
   

where x  rp and y  rq such that q   p 
the slope of the fa fit is very sensitive to the initial values of the matrices  and   for example  in figure   we
can match the covariance matrix t    perfectly to the data even if we fix one of the parameters at an arbitrary
value  for poisson spikes counted over a given interval the mean number of spikes equals the variance of spike counts 
each element of the diagonal matrix  is the variance of one of the neurons so we initialize these values to the
mean  if we initialize  with random normal elements em requires   times more iterations to converge than when
initializing with an optimum value of   we can reduce the number of iterations to     times the
p number of optimum
iterations if we provide a relevant scale by multiplying these random numbers by a factor of det s   q  p where s
is the covariance matrix of the data  however  the real problem is that  and  converge to a local optimum that
doesnt accurately predict the underlying firing rate  so given our value of  lets take the matrix derivative of the
log likelihood with respect to   set it equal to zero  and find an optimum initial   the method for doing this is
borrowed from the first step of joreskogs algorithm       there is not sufficient space to write the whole procedure
but  to start  we note that maximizing the log likelihood


m
y
 
   i 
t
t
 
 i 
          ln
exp   y           y   
   
 
   q    t       
i  

fi 

fig    

fig    

is equivalent to minimizing

     ln  t       tr s t       ln  s   q

   

where s is the covariance matrix of the data  the log likelihood for the em algorithm implemented with this optimum
initialization is shown in figure   
even given this optimum initialization i wasnt sure that the local optimum that the em algorithm converged to
was ideal for predicting the underlying rate so i also solved fa using the newton raphson algorithm 
 t       t   h      

   

fi 

fig    

where h is the hessian and


  
 t    
  tr  t       t     s  t     
i
i


   
t
t
  
t
       
t
       
  tr      
     
i j
i
j

   
   

both the em and newton raphson predictions for the underlying rate are plotted in figure    given the same initial
conditions both converge to the same  and   the log likelihood for the newton raphson algorithm as a function
of iteration number is shown in figure    it converges faster than em but is more computationally intensive 

iii 

gaussian process factor analysis     

if the underlying rate in figure   increases decreases continuously with time then fa has also solved the problem
of picking out a neural trajectory  i e  finding how the underlying rate changes over time 
for rates that change over time we would like to perform fa at each time point with the additional modeling
assumption that the underlying firing rates vary smoothly  we will use yu et al s gaussian process factor analysis
 gpfa  algorithm      
assume we have spike counts from q neurons  divide the time interval into t nonoverlapping bins  let yi t be
the square root of the number of spike counts for neuron i in time bin t              t   y  t  rq  is related to a
low dimensional latent neural state x  t  rp    p   q  via
y  t  x  t  n  cx  t   d  r 

   

where the model parameters are c  rqp   d  rq    and r  rqq which is taken to be diagonal with elements
that are the independent variances of each neuron  to ensure that the neural state  x  t varies smoothly over time let
xi    n     ki  
where xi    r t and element  t    t    of the covariance matrix ki  rt t is given by


 t   t    
 
 
  n i
 t   t   
ki  t    t      f i
exp 
 i 

    

    

 
 
f i
 r  and n i
 r  are fixed constants and  is the kroneker delta function  the model parameters
c  d  r              p are found using the expectation maximization  em  algorithm       we can then vary p to find the
optimum dimension for the underlying neural state  the neural states from all time points are grouped into a matrix
x    x              x  t    rqt which can be thought of as the neural trajectory  for example  the neural trajectory
from the previous fa example is shown in figure   in black 

fi 

fig    

iv 

to do

   gpfa works because sufficiently small portions of the neural trajectory can be approximated by neurons having
the same variance and tracing out straight paths moving in one direction  one way to extend this would be to allow
different gpfa approximations to be stitched together by letting c  ct   d  dt   r  rt in equation     
   it can take several hours to learn the gpfa parameters  one possibility for improving speed is to implement
gpfa using the algorithm recently proposed by zhao et al       

    j  messier  j  f  kalaska        covariation of primate dorsal premotor cell activity with direction and amplitude during
a memorized delay reaching task  journal of neurophysiology              
    m  godschalk  r  n  lemon  h  g  kuypers  j  van der steen        the involvement of monkey premotor cortex neurones
in preparation of visually cued arm movements  behav brain res              
    m  m  churchland  b  m  yu  s  i  ryu  g  santhanam  k  v  shenoy        neural variability in premotor cortex provides
a signature of motor preparation  journal of neuroscience                     
    s  hocherman  s  p  wise        effects of hand movement path on motor cortical activity in awake  behaving rhesus
monkeys  exp brain res              
    j  e  gomez  q  fu  d  flament  t  j  ebner        representation of accuracy in the dorsal premotor cortex  eur j
neurosci                 
    j  g  ojemann  g  a  ojemann  e  lettich        neuronal activity related to faces and matching in human right
nondominant temporal cortex  brain            
    r  desimone  t  d  albright  c  g  gross  c  bruce        stimulus selective properties of inferior temporal neurons in
the macaque  journal of neuroscience               
    d  y  tsao  w  a  freiwald  r  b  h  tootell  m  s  livingstone        a cortical region consisting entirely of faceselective cells  science               
    c  g  gross  c  e  rocha miranda  d  b  bender        visual properties of neurons in inferotemporal cortex of the
macaque  journal of neurophysiology             
     a  h  bell  f  hadj bouziane  j  b  frihauf  r  b  h  tootell  l  g  ungerleider        object representations in the
temporal cortex of monkeys and humans as revealed by functional magnetic resonance imaging   journal of neurophysiology               
     b  m  yu  j  p  cunningham  g  santhanam  s  i  ryu  k  v  shenoy  m  sahani        gaussian process factor analysis
for low dimensional single trial analysis of neural population activity  journal of neurophysiology               
     p  dayan  l  f  abbott          theoretical neuroscience  cambridge  ma  mit press 
     k  g  joreskog        some contributions to maximum likelihood factor analysis  psychometrika             
     a  p  dempster  n  m  laird  d  b  rubin        maximum likelihood from incomplete data via the em algorithm  with
discussion   journal of the royal statistical society  series b               
     j  h  zhao  p  l  h  yu        fast ml estimation for the mixture of factor analyzers via an ecm algorithm  ieee
transactions on neural networks                     

fi