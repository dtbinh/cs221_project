clustering blogs based on stylistic characteristics
matin movassate
christopher lin

matinm cs stanford edu
topher cs stanford edu

computer science department  stanford university  stanford  ca      

abstract
given a sufficiently broad genre such as technology
or politics  internet users typically have straightforward means of determining which websites and blogs
offer content of the desired category  unfortunately 
users with more specific preferences or tastes that relate more closely to style of writing rather than the
subject matter itself  for instance  observational humor or opinionated prose  will have limited means of
discovering content suited to their tastes  in order
to generate a more natural and intuitive grouping of
web pages  we applied k means clustering and principal component analysis across a diverse set of textual
features to a large corpus of blog data  our results
showed that our training corpus contained between
k      and k      natural style clusters  and that
appearance of word bigrams and use of punctuation
are the best indicators of a bloggers writing style 

 

motivation

while many content aggregation systems on the web
 such as technorati  aim to group websites and blogs
based entirely on genre  e g  grouping gardening blogs
or film review sites together   few have attempted to
track similarities that stem from literary considerations  such as the writing style used by these sites authors  e g  sentence forms and employment of particular figures of speech   the discursive tone of the writing  e g  comedic  authoritative  or conversational 
and employment of certain literary genre conventions
 e g  use of narrative and dialogue in short stories in
contrast to a prose essay  
although ignored thus far  such characteristics
strongly influence a readers perception of blog material  these language features  though difficult for
a human being to precisely classify  have a dramatic
effect on how much enjoyment a reader is able to derive from a given blog  because blog text contains
both structural and semantic signals that provide insight as to the tone  style and content of the writing 
we can develop learning algorithms that aggregate on
these features and develop a sensible grouping for our

training data 
we therefore hope to use machine learning techniques to find clusters of blogs that share common
attributes in stylistic characteristics  where groupings
can produce sensible blog recommendations by simply
suggesting blogs in the same cluster 
successful clustering can have profound implications on recommendation systems  while most recommendation systems are based on predefined genre
listings or explicit user indicated preferences  very few
recommendation systems actually base their groupings on the structure and content of the text  perhaps
this system could provide a more intuitive means of
determining blog suggestions  and thereby increase
the utility and enrich the experience of online selfexpression 
because we are employing unsupervised learning algorithms  it will be difficult for us to quantitatively
gauge the effectiveness of our methods  the whole
impetus for our application is to discover a more natural grouping of websites that can not be easily quantified by a human being  though this fact admittedly
makes the prospect of success difficult to quantify 
it makes the application as a whole much more unique
and intriguing 

 

data collection

for obtaining a sufficiently large amount of usergenerated content spanning a wide variety of different
blogs  we identified major blogging platform websites
such as blogger and wordpress as the most valuable
online resources  for amassing a sufficient corpus of
training data  we ultimately took advantage of bloggers random blog feature to extract the    latest
posts from       randomly chosen blogs  resulting in
a total corpus of roughly        blog entries  html
markup was directly downloaded with a scraper script
using a combination of pythons urllib  standard
libraries and googles data feed api  a parser was
built with the python library beautifulsoup in order to parse the downloaded xml markup associated
with each blog feed and ultimately extract the desired blog content  finally  once we attained enough

fiblog text  we stored the associated data  along with
descriptive text  while a relatively high frequency
some relevant metadata  such as date of posting  in
of nouns could indicate heavy use of lists 
a mysql database  blog text was preprocessed and
    punctuation use  frequency of punctuation
standardized to be all lower case  with extraneous forcharacters are traditionally a successful indicator
matting markup stripped out  and images  videos and
of authorship  mostly because of the opportunity
audio replaced by the tokens   image      vid   and
of variation in usage     
  audio   
    list and quotation use  frequent use of quotations and dialogue may indicate a narrative
  feature selection
style and storytelling approach to writing rather
than an expository style more typical of an essay 
by expanding upon previous work in computational
stylistics  we were able to leverage an expansive list     vocabulary size  blogs employing more verof linguistic features aimed to highlight stylistic simibose diction suggest a higher level of intellectuallarities amongst blogs  for each blog in our data set 
ism in the text and target audience 
we obtained the following values and aggregated them
all into a single float valued vector  each feature  or     readability metrics  for each blog  we calculated readability via the flesch reading ease
set of features  describes the type of writing style we
test 
were hoping to infer 
total syllables
total words
     
              
    average length for posts  sentences and
total sentences
total words
words  longer entries suggest a more involved
the ease with which a reader comprehends a doctreatment of the subject matter  longer sentences
ument can greatly affect his her enjoyment of the
suggest a more thoughtful  and perhaps streamdocument  and may also give indicators as to the
of consciousness  level of prose  and longer words
documents formality and target audience 
suggest a more verbose vocabulary 
    word and character frequencies  examin       web specific features  as web texts  blogs
can integrate rich media directly into their coning the appearance of particular words can be
tent  blogs that do so frequently contrast with
indicative of a documents overall style  as evblogs that focus on a style more consistent with
idenced by previous work on authorship attritraditional text prose  we tracked occurrences of
bution used to track correlations in word usage
embedded images  videos and audio in a blog  by
     for word frequencies  we collected the top
replacing the content of html  img  tags with
   most common words in our entire dataset and
a catch all   img   token  for instance   as well
simply calculated the frequency of each of the top
as the frequency of emoticons    d  and common
   words for each individual blog 
web acronyms  lol  
    function word frequencies  function words
are defined as tokens that have little lexical meanalgorithms and model
ing but express grammatical relationships be   
tween other words in a sentence  a  the  his  to  
we took advantage of a variety of unsupervised learnfrequencies of function words are an attractive
ing algorithms in order to generating blog clusters 
feature because they are insensitive to a particuonce we generated our feature vectors of length
lar subject matter  yet can be indicative of certain
n       for each of our m       blogs  we fed  at
types of sentence constructions and phrases     
least some portion of  the feature set matrix  rmn
    word level and character level bigrams  into the following algorithms 
word combinations such as fixed phrases and collocations up to lengths of seven have been previ      k means clustering
ously used in authorship studies of shakespearean
works and can be indicative of a particular id  our implementation of k means clustering generated
iolect  or individual style of writing       our groupings based on three input values  a feature vecbigrams involved every possible two word com  tor f    f    f            fp    p  n  fi  f  where f is
bination of the top    words in our entire dataset the original feature vector of size n   number of clus note that tokens such as   image   are consid  ters k and number of replications r  the number of
replications is the number of times the k means clusered words by our model  
tering algorithm was run  with different initial values
    parts of speech  a relatively high frequency generated at the start of each run to overcome local
of adjectives and adverbs could indicate colorful  optima 
 

fi   

principal component analysis

because the dimensions of our feature matrix are
skewed such that n  m  it makes sense to apply
some level of dimensionality reduction  so that we
may reduce noise and superfluousness in our feature
values  in particular  the high number of word and
character frequencies in our original feature vector f  
as well as word and character bigram frequencies  suggest the possibility of a great deal of noise in the data 
we use principal component analysis to attempt to
correlate features with one another and reduce the
dimensions of our feature vector 

   

k nearest neighbors
figure    plot of k values to average distance from examples

another algorithm that deserves mention is k nearest
neighbors  knn   though easy to implement  this algorithm turned out to live up to its notoriety for being
slow  since our training set was rather large  many
distance computations needed to be made  which was
simply impractical given the size m  n of our training
set and feature vectors  preliminary results were no
better than k means clustering  and were produced
at a substantially slower pace  so this algorithm was
mostly ignored for the purposes of this project 

 

to associated cluster centroids for each subset feature vector 
word bigram frequencies perform best  followed by punctuation frequencies  results for feature vectors including average
length  readability scores  and part of speech frequencies are
excluded because of extremely poor performance 

experimental results

in an effort to assess the density of our k means clustering result  we examined how the tightness of each
cluster produced correlates with the value for k  the
means for each produced cluster to assess how distinct
and densely grouped our k clusters are  the intuition
is that  the more dense our generated clusters are  the
more successful they were in finding similarly styled
blogs  the equation used to calculate the average
distance of an example from a clusters centroid was
calculated by 
 
qp
pm
 i 
 j 
 
 i 
n
pk
 x

 
  c
 j 
f   
i  
f
f
j  

pm

i  

  c i   j 

k
where we let  i         k  be the generated centroids
 rn   and c i  is the cluster index of example i  the
results we obtained for each are in figure    also 
refer to figure   for the results obtained from our
run of principal component analysis 

 

analysis and errors

in discussing the success of unsupervised learning algorithms  we will need to take a more qualitative approach in assessing the results of our methods and
algorithms  while it is relatively straightforward to

 

measure success with supervised learning algorithms 
unsupervised learning algorithms lack any clear structure a priori  and so need to be justified not just with
our quantitative metrics  but also with some qualitative insights 
for trials involving the entire feature set with k  
   k      and k       the algorithm was capable
of matching up a blog involving introspective  melancholy analysis of vintage comic books with another
blog involving a teenaged girl discussing  at a deep
and emotional level  the angst of being a high school
student  though both blogs concern entirely different
subject matters  they share striking stylistic similarities in how they forlornly and contemplatively address
their respective topics  additionally  it managed to
cluster together a how to blog on knitting alongside
a how to blog on computer modding  again  despite
concerning wildly different domains  these blogs presented similar treatments of their respective subjects 
this suggests that our features are often successful in
picking out the hidden literary details that make one
blog stylistically similar to another 
for k means clustering  a high value of k  such as
    or      which then corresponds to a small cluster
size  had mostly meaningless results  this is because
there simply was not a diverse enough array of blog
types within our data set to justify k values of     or
     groupings in this case seemed mostly arbitrary 
from observing our blog clusterings  it becomes
clear that a nontrivial portion of our dataset was
rather homogeneous  many of our clusterings seemed
to collect a skewed set of user blogs  specifically  a
large portion of our collected blogs were  strangely
enough  image galleries showcasing the latest fashion
trends  this could be because of the audience blogger

fifigure    plot of training examples and feature vectors against first two principal components  red points represent training
examples  and blue vectors represent individual features  blue vectors are labeled with numbers encoding features  the first
principal component is most correlated with features that measure the frequencies of word and character bigrams  while the
second principal component is most correlated with character frequencies 

attracts  perhaps  other than a few niche purposes 
the site is becoming an increasingly unpopular destination for online expression  meaning many serious
users are turning to competing blogging platforms like
wordpress 
in addition  since our word bigram model was built
over a language of the top    words in our dataset 
there were                 feature values devoted
to word bigram frequencies  and because very few
of these bigrams actually appear in the blog text 
most of these values end up being    perhaps a better
construction for the bigram models would have been
to build the word bigram features directly from the
most frequently occurring bigrams  instead of constructing bigrams from the most frequently occurring
unigrams  also  implementing some form of laplace
smoothing or good turing smoothing could help alleviate some of the sparsity in our bigram model 
finally  for our k means clustering trials  we ran
into the possibility of encountering optimization errors  where the algorithm would converge onto a local optima instead of a global optima  this issue was
slightly alleviated by running multiple iterations of
k means with randomized initial values 

blog clusterings  for instance  k means clustering using a combination of word bigram frequencies and
punctuation frequencies managed to link together
fashion blogs and family photo blogs that  despite focusing on different subjects  featured casual yet grammatical prose and a personally reflective tone  these
results correspond to authorship attribution literature that highlights the usefulness of word bigrams
and punctuation in computational stylistic analysis 
principal component analysis suggests that the dimensionality of the feature vector can be reduced by
combining features measuring character frequencies
and features measuring frequencies of some word and
character bigrams 

in the future  our method of measuring clustering
success will most certainly need to be improved  clustering text  especially online text  is a difficult problem  as content and style can vary dramatically from
one blog to another  thus  given a cluster  it can be
difficult for a human being  or computer program  for
that matter  to hastily scan through each blog in the
cluster and determine whether success was achieved
or not  but this is as expected  the entire purpose
of the project was to produce groupings that rely on
the deeply rooted structure of the language itself  not
  conclusion and future work simply some surface level attribute that can easily be
skimmed  to obtain as accurate a gauge on success as
overall  both k means clustering and principal com  possible  in the future  we would consider conducting
ponent analysis produced many startlingly intuitive surveys and human interaction studies  asking par 

fiticipants to provide their opinion on the legitimacy of
produced clusterings  such an endeavor turned out
to be too costly to execute in the timeframe given for
the project 
our feature vectors also had the issue of being extremely sparse  mostly due to our word bigram frequency features  when building our feature vector in
the future  it will be useful to run feature selection
algorithms leveraging attributes like mutual information  this will allow us to determine which features
are the most fruitful in our large feature set  enabling
us to reduce the dimensionality of our trial matrix
and allow for more tractable computations on our entire data set  instead of getting bottlenecked by our
very large  mostly meaningless feature vectors  
much computational power and time was also
wasted calculating blog clusterings for different cluster sizes  since the cluster number k is a nuisance
parameter of the k means clustering algorithm  in
the future  we plan to leverage techniques like v fold
cross validation to determine the optimal size of k
before running our clustering algorithm several times
for different values of k 
we would also need to consider obtaining data from
a variety of sources  instead of just a single source  in
our case  all our blog data was extracted from a single website  blogger com  while the resulting consistency in html and xml formats alleviated the
arduous task of data collection and preprocessing  it
also introduced an unwanted degree of homogeneity
into our training data  though blogger com is an
all purpose blogging platform  it seemed to attract a
surprisingly narrow set of opinions  as explained in
our error analysis  in further work  we will extract
data from other sources such as wordpress and technorati 
in the future  we could experiment with more sophisticated clustering algorithms such as fast genetic
k means clustering algorithm  this method provides promising benefits over normal k means  since
its guaranteed to converge to a global optimum and
claims to run considerably quickly     

 

siemens  john unsworth  oxford  blackwell 
http   www digitalhumanities org companion 
    koppel  m   argamon  s   and shimoni  a         
automatically categorizing written texts by gender  literary and linguistic computing       
       
    lancashire  i          empirically determining
shakespeares idiolect  shakespeare studies    
    p   
    grieve  j          quantitative authorship attribution  an evaluation of techniques  literary
and linguistic computing       
    lu  yi et al          fgka  a fast genetic kmeans clustering algorithm  symposium on applied computing 

acknowledgements

we would like to thank matthew jockers  from the
department of humanities computing  for his advice
on feature selection related to computational stylistics 

references
    craig  h         stylistic analysis and authorship studies  a companion to digital
humanities  ed  susan schreibman  ray

 

fi