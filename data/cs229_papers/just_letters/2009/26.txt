bubble clustering  set covering via union
of ellipses
matt kraning  arezou keshavarz  lei zhao
cs     stanford university
email   mkraning arezou leiz  stanford edu
abstract
we develop an algorithm called bubble clustering which attempts to cover a given set of points
with the union of k ellipses of minimum total volume  this algorithm operates by splitting and
merging ellipses according to an annealing schedule and does not assume any prior distribution on
the data  we compare our algorithm with k means and the em algorithm for mixture of gaussians 
numerical results suggest that our algorithm achieves superior performance to k means for priorless data and comparable performance to algorithms that have access to prior information for
statistically generated data 

i  p roject g oal
we want to cover a set of n points x    x    x            xn    xi  rd   by the union of k
ellipses  e    e            ek   such that the total volume of all ellipses is minimized  unlike mixture of
gaussians  we do not assume a prior distribution on data  nor do we even assume the data has a
prior distribution  consequently  the optimization problem we are attempting to solve is
minimize
subject to

pk

 
i   log det ai

kaij xj   bij k    
ij              k  
ai 

n  
s  

j       n

i           k 

which is an np hard problem due to the presence of the constraint ij              k   this constraint
specifics that we find the optimal assignment of every given data point xj to a specific ellipse eij  
ii  a lgorithm
because solving our problem exactly is np hard  we instead focus on developing an approximate
algorithm to solve this problem  rather than use em  whose performance can depend heavily on
initial starting conditions that are usually randomly generated  we sought an algorithm that is
both deterministic and as free of tunable parameters as possible  this was done to maximize the
algorithms universality  it should not depend on random numbers  nor should it have to be highly
tuned to work well on specific datasets 
bubble clustering works by iteratively splitting minimum volume ellipses that were fit to parts
of the data on earlier iterations  the intuition behind bubble clustering is that good clustering
performance can be obtained by looking at a global summary of part of the dataset  the points
within a given ellipse  and then taking a lower volume covering of that same set by splitting
the ellipse containing that set into two smaller ellipses  after splitting down to k ellipses  bubble
clustering then performs a variant of simulated annealing to escape from weak local minima  the
annealing schedule controls if ellipses are split or merged during each iteration and guarantees that
the number of ellipses converges to the pre specified value k  this is described by the pseudocode

fibelow and a more detailed description of how splitting and merging of ellipses is performed is
given in the following sections 
given data x  number of final clusters k  and an annealing schedule  anneal  fit a single ellipse
to the data  then 
for t     to k do
split ellipse
end for
for i     to length anneal  do
if anneal i     split then
split ellipse
else if anneal i     merge then
if one ellipse contained inside another then
eliminate smaller ellipse
else
merge two ellipses
end if
end if
end for
a  splitting
the density of the ith ellipse  i   is the ratio of the volume of the ellipse to the number of data
points inside the ellipse  i e 
vol ei  
i  
 
  i      i    xi  ei   
in the splitting phase of a given iteration with  ellipses  bubble clustering greedily chooses the
ellipse with lowest density  ei   where i   argmin i   and splits it into two ellipses ei    ei    it splits
i        

ei along the hyperplane perpendicular to the direction which gives maximum data variance for data
points within ei   this direction is the eigenvector corrosponding to the maximum eigenvalue for the
empirical covariance matrix of the data points inside ei   after splitting  it fits new minimum volume
ellipses around both sets of points by solving the convex optimization problem

minimize

log det a 
ij

subject to kaij xi   bij k   xi  

xi  eij

for j        

fig     splitting along the eigenvector of maximum eigenvalue for the empirical covariance matrix of the data point
inside the chosen ellipse 

fib  merging ellipses
in the merging step  we first check if there is any ellipse that is completely contained in another
ellipse  denote the ellipses by ei    x kai x  bi k       and ej    x kaj x  bj k        we want
to check if ei  ej   or equivalently  whether kai x  bi k       kaj x  bj k      
the s procedure specifies the necessary and sufficient conditions for this to occur   if    
such that q   p       xt p x     xt qx          applying the s procedure to our
problem  it is sufficient to check the feasibility of the linear matrix inequalities  lmis 
 

d
 dd t

dd
dt dd    

 



 

i  
 
 

 

t

 

   

    
 

 
  t
t
where d    ati ai      a 
aj aj  ati ai       for all pairs of ellipses
i bi  aj bj   and d    ai ai  
ei   ej   i    j  
if we cannot find an ellipse contained inside another ellipse  we merge two ellipses based on their
relative orientations and overall closeness  to measure relative orientation  we use the difference
between the mahalanobis distances of each ellipse to the center of the other ellipse  where the
mahalanobis distance to a point x from the ellipse ei with center ci and inverse covariance matrix
ai is defined as kxk ei    xci  t ai  xci    if these distances are close  then the ellipses are aligned 
consequently  we merge the two ellipses  ei and ej   that minimize  kcj kei  kci kej     kci  cj k   
where  is a tradeoff parameter  and ci and cj are the centers of ei and ej   respectivly  then  we
merge ellipses ei and ej by fitting a minimum volume ellipse around the union of points contained
in either ellipse  see figure    

fig    

merging ellipses  two black ellipses are merged into the blue ellipse 

iii  n umerical

results

a  synthetic data  no prior
we initialized k means randomly    times  and took the result with minimum total ellipsoid
volume  which was       the average volume over all iterations was       our bubble clustering
algorithm attained a total volume of       the clusters generated by the k means algorithm have
some overlapping clusters  whereas the clusters generated by the bubble clustering algorithm are
separable  figure    
b  synthetic data  mixture of gaussian prior
in this experiment  we assume a gaussian mixture model with   classes and a uniform distribution
over those classes 
p x   

 
x
 
i  

     d  

 

 
exp   x  i  t si   x  i    
 
det  si  

p

where the dimension d is set to be    the parameters si   i   i            are randomly generated and
fixed for all the data  we use this gaussian mixture model to generate     points  shown in figure
  

fibubble clustering  total ellipse volume       

kmeans clustering  total ellipse volume       

   

   

 

 

   

   

 

 

   

   

 

 

   

   

 
 

 

 

fig    

 

 

 

 

 
 

 

 

 

 

 

 

 

 

comparison between bubble clustering  left  and k means  right 
true distributionkt    
  
  
  
  
 
 
 
  
  
  
  
  

fig    

  

 

 

 

  

  

  

true distribution of the data  generated by a mixture of gaussians 

we run bubble clustering on this data set and compare it with the result obtained by the em
algorithm for mixture of gaussian  as figure   indicates  even without a prior assumption  the
bubble clustering obtains comparable results 
mixture of gaussian via em algorithmk    
  

  

  
  

  
  

  

 
 

 

 
  

  
  

  

  
  
  

fig    

  

 

 

 

  

  

  

  
  

  

 

 

 

  

  

  

  

comparison between bubble clustering algorithm  left  and the em algorithm for mixture of gaussian  right 

c  real data  blogger population
we also applied the bubble clustering algorithm to a real dataset  we obtained an xml feed from
     which provides a live feed of blogger entries  we extracted the bloggers residing in california 

fiand ran the bubble clustering algorithm to find clusters of blogger population  figure    

eureka

  

  

sacramento

berkeley

  

san francisco
stanford
fresno

  

  

  

  

los angeles

  

san diego
   

   

fig    

   

   

   

   

   

   

   

   

blogger population clusters in california

the clusters correspond pretty well to metropolitan areas such as los angeles and san francisco 
with smaller ellipses being generally indicative of high population densities over their area 
iv  c onclusions

and

f uture w ork

bubble clustering is a novel universal clustering method that does not have to rely on the kindness
of random number generators or the smoke and mirrors of tunable parameters for good performance 
it is effective over off the shelf methods for both clustering and finding small set coverings and
can also show data cluster correlations when a prior is either unknown or non existent 
the main areas for improvement involve using lookahead in both splitting and merging ellipses 
in picking which ellipse to split  our ultimate aim is to have the sum of the volumes of both
new ellipses be significantly smaller than the volume of the original ellipse before splitting  it is
computationally intensive to evaluate the volume reduction gained by splitting each ellipse and
only then pick the best ellipse to split  so we instead rely on the heuristic of density as a proxy
metric for splitting efficacy  similarly  in the case of merging  if we check all possible pairs of
ellipses to merge and only then choose to merge the two with least volume increase  the algorithm
will be drastically slowed down  finding ways to effectively prune these search spaces  as we did
by utilizing s procedure to check if one ellipse is inside another  would allow lookahead to be
implemented for both splitting and merging  which would likely lead to better performance 
v  acknowledgements
we would like to thank eric chu and brendan odonoghue for their discussions and help on
this problem with us 
r eferences
    boyd  s  p          linear matrix inequalities and the s procedure  ee    notes  winter      
    we feel fine  http   wefeelfine org  

fi