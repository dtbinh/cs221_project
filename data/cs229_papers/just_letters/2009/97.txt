learning stereo features with stacked autoencoders

daniel jin hao chia  pang wei koh  zhenghao chen
cs    final project

 

introduction

single layer stacked autoencoders have been shown to be successful in training articial neurons with receptive elds that are similar to those found in the v  cortex  but on monocular data  in this project we
investigate extending a single layer stacked autoencoder network to learn receptive elds on stereo data  and
evaluate them with respect to their eectiveness as features for object classication and their similarity to
neuronal receptive elds characterized in physiological experiments 
the primary motivation for using stacked autoencoders is their intrinsic non linearity  which allow us to
obtain higher level features by stacking more levels and iterating the same algorithm multiple times  in
contrast  stacking linear transformations such as those obtained from ica does not yield any meaningful
results  this higher level representation would be useful both as a tool for performing image and depth
recognition with stereo data  as well as a model for how the brain processes visual information 

 

methodology

we generate   x   receptive elds from a training set of           x   images  sampled from the rst
training dataset of the norb dataset      for each of the        stereo pairs of    x    images in the
norb training set  we rst whiten the data and then sample    pairs of   x   images around a   x  
bounding box centered on the middle of the image  concatenating each pair of corresponding   x   images
to form a   x   image  within each   x   image  the    columns on the left represent the image that the
left eye sees  while the    columns on the right represent the image that the right eye sees 
training of the stacked autoencoder network is done with the stanford deep learning network library 
with each of the     neurons characterized by a   x   receptive eld  the network uses a sum of squares
reconstruction error as the objective term  coupled with sparsity regularization on the bias term and l 
weight decay  optimal coecients of the sparsity regularization term and the weight decay term are found
with a grid search 
we evaluate the learnt   x   receptive elds through supervised training on the original set of        pairs
of    x    images and classication on a distinct but similar set  each    x    image is whitened and then
cropped to   x   to eliminate border eects  feature vectors are generated by convolution  we extract  x 
overlapping stereo patches  each of size   x    half from the left image and half from the right image   from
each pair of   x   images  and run each of these   x   patches through the network with the learnt receptive
elds  taking the hidden layer activations as our features  through concatenation  each pair of   x   images
therefore translates into a feature vector of length                
the stereo receptive elds are contrasted against mono receptive elds learnt on the same dataset  we
generate the latter by treating left and right pairs of images as independent  essentially learning receptive
elds on                       x   images  and doing supervised training and classication on sets of       
  x   images 
we also present results obtained from running the fastica algorithm     on the same set of         sampled
  x   images  no dimensionality reduction was done  resulting in           independent components  each
of size   x    found  classication results run on the whitened raw data  as well as on the convolution of
whitened raw data projected upon the ica bases  are also shown 
 

fidisparity tuning curves are calculated using the method described by hyvarinen et al      each curve
plots response against horizontal disparity  to calculate the response of a neuron at a particular horizontal
disparity  we use the left side of the receptive eld of the neuron itself as the stimulus  translate it horizontally
by the required amount  and nd the maximum over all vertical translations of the activation of the neuron
when presented with that stimulus  we then repeat this process with the right side of the receptive eld 
the response is taken to be the average of the activations when presented with the stimulus from the left
and the right side 

 

results   receptive fields and classification

figure    representative receptive elds from dierent areas of search space

 a  ns   stereo

 b  ns   mono

 c  lr   stereo

 d  lr   mono

figure    images of receptive elds

figure    ica bases obtained from the training set
we nd that the most neurologically accurate learnt stereo receptive elds lie in the area of moderate weight
decay and sparsity  fig      these learnt patches resemble  for the most part  a pair of gabor lters that are
phase and or positionally shifted from each other  fig   a   b   we compare these to the receptive elds
 

fi a  original image patch

 b  reconstruction by ns

 c  reconstruction by lr

figure    reconstructed patches

that result in the lowest reconstruction error  these correspond to networks that have not been constrained
much by sparsity  we term these sets of receptive elds ns and lr respectively  in comparison  fig   
shows the results of running ica on the same training set 
fig    shows an example of the reconstructed images obtained using the ns and lr stereo receptive elds 
type
stereo ns
stereo lr
mono ns
mono lr
raw pixels
ica

classication accuracy    
     
     
     
     
     
     

table    classication results on        pairs of   x   images
table   displays the results of classication through the various methods described 

 

results   neurological comparisons

 a  far  autoencoders 

 b  near  autoencoders 

 c  tuned excitatory  autoen   d  tuned inhibitory  ica 
coders 

figure    characteristic disparity tuning curves
traditionally  binocular neurons have been classied into four dierent categories  far  near  tuned excitatory 
and tuned inhibitory  based on characteristic disparity responses for each category  the receptive elds that
the single layer stacked autoencoder learn show disparity tuning curves from the rst three categories  fig 
 a   b   c   however  none of our current receptive elds match the response of a tuned inhibitory neuron
 fig   d  
in accordance with biological data      phase  fig   b  and positional  fig   a  shifts are visible  we also
report the presence of ocular dominance  in both the autoencoder  fig   c  fig   d  as well as the ica
receptive elds  though to a signicantly greater extent in the latter  to the best of our knowledge  this
phenomenon is not well understood within the neurological literature 
 

fi a  positional shift

 b  phase shift

 c  left ocular dominance

 d  right ocular dominance

figure    disparity and ocular dominance in autoencoder receptive elds

 

discussion and future directions

we have obtained some encouraging preliminary results as to how stacked autoencoders can produce receptive
elds with properties that resemble neuronal receptive elds  in particular when sparsity constraints are set
on the network  this supports sparse coding hypotheses of the workings of the visual cortex  however 
there are types of receptive elds found in the real neurons that are currently lacking in those that the
autoencoders produce  for example  those that result in tuned inhibitory disparity tuning curves  this could
be due to the dierence in the workings on the human visual system as compared to the system we are
implementing  in particular  our eyes are able to vary their focal lengths  and tuned excitatory and tuned
inhibitory neurons are thought to be related to this change in xation length     
also  stacked autoencoders can be used to produce stereo feature sets that achieve better linear classication
accuracy with a smaller number of features as compared to using raw pixel data  or linear transformations
thereof  however  the neurologically similar features that we nd are not those that result in the highest
classication accuracy  we can think of three possible reasons for this  rstly  the norb dataset is not
perfectly stereo in that the images chosen for background noise are placed at a xed disparity between the left
and right patches  which is not a totally realistic representation of natural images  secondly  the advantage
of the neurologically similar features might lie not so much in achieving the greatest classication accuracy 
but rather in optimizing a tradeo between the amount of data required versus the classication accuracy  in
this sense  the sparser feature set would be more amenable to compression via thresholding of each neurons
activity  for example  thirdly  it could be that the neurologically similar features lend themselves better to
the nding of higher level features through the stacking of additional autoencoders 
with these in mind  we propose the following future directions 
 quantitative comparison to biological data 
the methods used to generate disparity tuning curves vary from paper to paper  and we have not yet
found a way to statistically and non qualitatively measure how similar the disparity responses of our
neurons are to those of real  biological neurons  for example  the comparison in hyvarinenet al     is
done by eye  there is also a lack of clear statistics on factors such as the degree of ocular dominance
present within a set of receptive elds  we would like to investigate how we might quantitatively
measure such similarity  in order to make more condent claims about the usefulness of the stacked
autoencoders as a biological model 
 learning and evaluating on a dierent dataset 
we would like to see if other datasets give us similar results  in particular  the next step would be to
run the same algorithm on a dataset comprising true stereo images  with pictures taken at diering
focal lengths  we would also like to see if the stacked autoencoders  particularly the ns feature set 
perform comparatively better on classication tasks with a greater number of classes  norb has
   
 using non linear classiers 
our current restriction to linear classiers is a likely cause of the low classication accuracy that we
are seeing across the dierent feature sets  and might impede proper analysis of the eectiveness of
the features generated by the stacked autoencoders  memory and time restrictions due to the large
size of the feature sets prohibits more complex methods  but we would like to explore alternative
methods for classication that might result in better classication accuracy 
 extension to multi layered stacked autoencoders and addition of stochastic elements 
 

fithe learnt features can also be improved  and higher level features obtained  by stacking more
autoencoders on top of our single layer  this is the main long term aim of our work  additionally 
improvements to the learnt features can also be made through the addition of random elements into
the neural network  either through using denoising autoencoders  or by making the activation of
each neuron binary and probabilistic  instead of continuous and deterministic as it is now 
 modifying the objective function 
the current objective of sum of squares reconstruction error might not be optimal in terms of producing receptive elds that compare favorably with neuronal receptive elds  in particular  the best
receptive elds are not the ones with the lowest reconstruction error  and we are currently judging the quality of a receptive eld by eye  we would like to investigate how a modied objective
function  perhaps involving kurtosis or a similar convex function to encourage sparsity  would aect
the results we get  in particular  we would like to see if a modied objective function could help in
the automation of hyperparameter tuning  as well as in the driving of the learnt receptive elds to
match the ica bases and the neurological data 
 using ica to train the neural network 
another possible and related way to obtain receptive elds closer to the ica bases is to x the
decoder weights as the ica bases  and then train encoder weights using the autoencoder algorithm 
this should have the eect of indirectly modifying the objective function to result in learnt receptive
elds that resemble the ica bases more closely  coming up with a non linear approximation to the
linear ica bases in this manner might allow for higher level features to be learnt by repeating the
algorithm 

 

acknowledgements

we would like to thank our tas  andrew  ian and quoc for the huge amount of help that they have given
us in this project 

references
    y  lecun  f j  huang  l  bottou  learning methods for generic object recognition with invariance
to pose and lighting   cvpr        
    a  hyvarinen  j  hurri  p  o  hoyer  natural image statistics  springer        
    a  hyvarinen  fast and robust fixed point algorithms for independent component analysis  ieee
transactions on neural networks                      
    a  anzai  i  ohzawa  r  d  freeman  neural mechanisms for encoding binocular disparity  receptive
field position vs  phase  journal of neurophysiology                     
    b  fischer  j kruger  disparity tuning and binocularity of single neurons in cat visual cortex  exp 
brain res                 

 

fi