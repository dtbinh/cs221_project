enabling intelligent traffic flow management in
wireless lans using markov decision process tools
sonali aggarwal  shrey gupta  sonali  stanford edu  shreyg stanford edu
under the guidance of professor andrew ng
          

 

introduction

current resource allocation methods in wireless network settings are ad hoc and fail to exploit
the rich diversity of the network stack at all levels  we want to deploy machine learning
algorithms for assigning resources at the access points in wireless lans in a real world
wireless setup  to bring about significant improvements in throughput of the network  the
new ieee standard       n multiple antenna equipped wireless lan standard allows for
numerous options at both the phy and mac layer of a transmitting wireless node  where a
particular selection of options result in system performance maximization for a given state
of the wireless channel  network  background traffic and user application  our goal is to
use the markov model developed for the    n standard and deploy machine learning tools
to select the option at the transmitting node  from the available choices within the realm
of the standard  that maximizes system performance  within the time limit dictated by the
rate of change of the state variables  we first deployed markov decision processes  mdp  to
choose the optimal policy for obtaining the maximize throughput  however  mdps explore
the entire state space to find the optimal policy  which is a slow process and since our
network conditions keep changing frequently  markov model takes a long time to re learn the
state transition probabilities   we deployed online algorithms which are much faster  our
preliminary analytical results combined with matlab simulations have verified that our
approach outperforms existing approaches 

 

motivation

wireless lans are popping up everywhere to provide internet access for the ever more ubiquitous laptop toting customer  they are easy to install and they give users the mobility
to move around within a local coverage area and still be connected to the network  plans
are in place to cover entire cities with wlans  the wlan industry  as it is now  with
deployments only in limited places  is over   billion with over     million unique users  with
a massive widespread deployment expected to hit the consumers in a few years  the numbers
are expected to grow by an order of magnitude or more 
ieee        is a set of standards carrying out wireless local area network  wlan 
computer communication  ieee       n      is an amendment to the ieee            
wireless networking standard to improve network throughput over previous standards  behind most       n enhancements lies the ability to receive and or transmit simultaneously
through multiple antennas  the       n protocol uses mimo strategy for communication 
multiple input multiple output  mimo  communication is well known to boost the wireless spectral efficiency through spatial multiplexing  at the physical  phy  layer  advanced
signal processing and modulation techniques have been added to exploit multiple antennas

fiand wider channels 
the limiting factor in the performance of a wlan today is resources at the access point 
current resource allocation methods are ad hoc and fail to exploit the rich diversity in wireless network settings at all levels of the network stack  maximizing the system capacity 
by selecting the best policy in the wlan  to increase the data rate   is the motivation
behind our project  we developed and deployed machine learning algorithms for the resource allocation problem that seek to have a direct impact on wlan system performance 
the problem of finding the optimal control policy  amongst the various options available 
to maximize throughput is cast into a markov decision problem  mdp   our preliminary
analytical results combined with matlab simulations have verified that our approach outperforms existing approaches  but the next step is to try it on a real testbed 
the       n multiple antenna equipped wireless lan standard allows for numerous
options at both the phy and mac layer of a transmitting wireless node  where any set of
options results in system performance maximization for a given state of the wireless channel 
network  background traffic and user application  for a typical wlan deployment  the
aforementioned state variables change dynamically and at a fast rate  our goal is to use
the markov model developed for the    n standard  bia    and device a markov decision
process to select the option at the transmitting node  from the available choices within the
realm of the standard  that maximizes system performance  within the time limit dictated
by the rate of change of the state variables  system performance in terms of throughput will
be the criterion for performance 
based on the work by giuseppe bianchi  bia     which derived a markov model for
state variable change at both the mac and phy layers  we do reliable state estimation
using characteristics of the previously received packets  e g  signal strength  number of retransmissions  packet loss rate  etc   on top of any relevant header information provided
by the standard  state estimation for some of the variables can also be done via explicit
feedback from the receiving node  allowed by the standard  furthermore the cost of such
explicit feedback will be included in throughput calculation 
state and action variables used in our model
for the purpose of this class project  we plan to include the following state variables in
our model
 packet success rate  packet success rate is a good indicator of channel strength 
and is readily available via acks received from the receiving node  or via ack timeouts 
whichever may be the case  we need to choose the optimal policy which gives us the
maximum packet success rate i e  the maximum throughput 
the following actions constitute the action variable set in our mdp formulation
at the mac layer 
 mac layer packet frame length  the frame length of the mac packet affects
the packet reception probability  longer frames transport more user data for the same
header length  however requires good channel quality for a longer duration of time 
the standard provides for    different options for mac frame length 

fi user selection  in a typical wlan deployment  the transmitting node may be
sending data to multiple receiving nodes  selecting the right user plays an important
role in overall system performance 
at the phy layer  the actions are
 modulation and coding scheme  mcs   the       n standard defines modulation and coding scheme  mcs  a simple integer assigned to every permutation of
modulation  coding rate  guard interval  channel width  and number of spatial streams 
the       n standard provides for   different options for the mcs  identifying the mcs
values supported in       n devices is a good way to determine the set of data rates
that can actually be utilized by users wlan  if the state of the channel is good  mcs
with a higher index ensures a higher data rate 

 
   

implementation
the standard approach   mdp

the problem of finding the optimal action given the state of the wireless network can be
essentially posed as a markov decision process problem  we divided the problem of finding
optimal policies for all combinations of the state and individual actions into sub problems
  to start with  we have assumed a stationary model for simplicity which considers the
wireless channel to be stationary 
we attacked the problem of finding the best policy by deploying the standard approach of
repeatedly gaining experience and learning state transition function p in the given scenario
and then converging to the optimal policy  it is difficult to quantify the gains achieved
as at present the selection is done on an ad hoc basis by the settings of the network controller and those controlling individual devices in the network  the stability of the approach
assures us that we always select the action corresponding to the maximum throughput 

   

alternate approach   online learning

though the mdp approach explained is a robust approach to deal with our problem formulation  we observed that in certain conditions it failed to converge and select optimal
actions  this can be attributed to the conditions where the wireless channel is continuously
changing and the experience accumulated by the mdp is not enough to allow it to converge
to an optimal policy  thus we essentially encounter a exploration problem  which marks
a trade off between learning time and performance of a mdp   the more time we spend in
exploring the state action space the better results we get in wireless networks  the channel is
inherently dynamic due to a number of factors like fading  interference among others which
are not under the purview of the network itself  this motivates us to look for a approach
that does not require a stationary characteristic from the network channel 
since we always aim to maximize the throughput by selecting the best action in minimal
time  we decided to use an algorithm which can continuously give us a good action even while
its learning  so we followed the online learning approach to find the optimal policy  we
adapt the online learning to our problem scenario by considering an ordered pair of actions

fi a    a    and the corresponding change in throughput  t h   the change in throughput is
used as a guide to select the next action  value for the learning rate  was selected according
to the scale on which throughput was measured  and the sampling time of throughput   so
as to have only scalable changes in the action  we use the fact that in best of the conditions
the throughput is directly proportional to the the mac layer packet length and the mcs
level 
we do not directly use the current t h value to determine our next action  we average
the value of t h observed over various transitions from a  to a  and use this t havg to
determine our new action  this approach can be translated as the following algorithm 

     

online learning algorithm

algorithm   online learning algorithm
   procedure select next action a    a    t h 
  action pair  a    a   
  
if a    a  then
  
flag    
  
else
  
flag    
  
end if
  
using the t h evaluate the value t havg for action pair a    a 
  
anew   a    f lag      t havg
  
normalize anew to limit the value within the action space
    end procedure

 

simulation

we successfully deployed the above algorithm on the simulink model  to better visualize
the performance of the algorithm we carried out simulations for the marginal cases in which
the network is stationary  figure   refers to one such case in which we have quasi stationary
state with fixed mac layer collision probability        figure   lists the average throughputs observed for each of the possible actions of selecting the mac layer packet frame length 
subsequent to a successful run of the algorithm on the simulink model  we are in the
process of deploying the same algorithm on a real test bed and plan to submit the results
for publication 

 

model

the simulation were carried out on a simulink model  the complete model and related code
can be found at http   www stanford edu shreyg cs    

 

collaborator

rajiv agarwal  rajivag stanford edu   phd  candidate  under prof  john m  cioffi   department of electrical engineering  stanford university 

fi 

 

 

 

 

 

 

 

 

  

  

  

  

  

  

  

figure    mac layer packet frame length selection with fixed mac layer collision probability      
packet length
throughput

  
 

  
  
  
  
  
  
         
                                    
 

figure    net throughput values with fixed mac layer collision probability        the
zero values are so because the algorithm learns a negative t h value for adopting these
actions from any other given action  and hence never adopts the concerned actions

references
 bia    g  bianchi  performance analysis of the ieee        distributed coordination function  selected areas in communications  ieee journal on                     

fi