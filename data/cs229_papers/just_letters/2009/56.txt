rahul agarwal
daniel jachowski
shengxi david wu

robochef  automatic recipe generation
byte sized recipes
introduction 
our algorithm intends to synthesize new recipes by identifying promising mixtures of
ingredients and classifying by recipe type  our dataset comes from the international conference
on case based reasonings  iccbc  computer cooking contest   the data consists of a
smaller training set of about     recipes in xml format to allow for relatively easy parsing of
individual recipes  as well as a larger extension set for us to use once our algorithm has reached a
more scalable level 
parsing 
a significant challenge was parsing the semi structured recipe data  we used statistical
nlp techniques to help with the parsing  the recipe data was stored in an xml format  where
each recipe has its own tag as shown below 
 recipe 
 ti ginger and peach chicken  ti 
 cat entre  cat 
 in   x chicken breast halves     oz total   boned  skinless  in 
 in   oz can peach slices  lite syrup  in 
 in   ts cornstarch  in 
 in     ts grated ginger root  in 
 in     ts salt  in 
 in     c sliced water chestnuts  drained  in 
 pr  ps spray a large skillet with non stick spray  ps  ps  preheat skillet over medium
heat  ps  ps  add chicken  ps  ps  cook over medium heat for        minutes or till
tender and no longer pink  turn to brown evenly  ps  ps  remove from skillet  keep
warm  ps  ps  meanwhile  drain peaches  reserving juice  ps 
  recipe 
the first parsing task is to extract the main ingredient word from each ingredient tag  for
example  we want to extract the word  chicken  from the line    x chicken breast halves     oz
total   boned  skinless   first  numbers and units were removed from the line in a preprocessing
step  then we counted how often each word in the ingredient line occurred in the preparation
steps  and took the most common word as the main ingredient  this worked under the
assumption that the ingredient word  i e   chicken   should be the most important word in the
line  and hence should be mentioned the most frequently 
however  it was often the case where no word in the ingredient was mentioned in the
directions  for example  if the ingredients listed  beef   there would be situations where the
 

http   www wi  uni trier de eccbr   index php task ccc act  

fidirections would always refer to  meat   so  beef  would have   occurrences  to help alleviate
this problem  we took a second pass through the data  where we counted how often each word in
the line occurred as an ingredient in other recipes  the assumption for this model was that many
recipes share common ingredients  and that common ingredients  such as  beef  are the most
likely to be replaced in natural language directions  this technique yielded significantly cleaner
lists of ingredients 
to parse the units and the amounts  we assumed that the first number in the line was the
amount  and the following word was the unit  this assumption worked reasonably well  although
it failed on sentences such as a medium bag of rice  where units are not explicitly specified
numerically 
selecting ingredients 
we assume that the number of ingredients in a
recipe    follows lognormal distribution that can be
fit through the maximum likelihood estimates  from
 to serve
this lognormal distribution we sample 
as the target number of ingredients 
let the vector of  unique ingredients be
given as       ff        given an initial input
seed of favorite ingredients  we then generate a set of
ingredients that complement that seed  with an
 ingredients  the seed will be a
expected number of 
binary vector  with  s at the index of where
ingredients are given  to calculate the probability of
selecting a new ingredient  given a seed with the 
ingredients           we can use bayes rule 
           
          
          
however  as the number of ingredients in the seed grows  we find that oftentimes there is no
recipe that has all of these ingredients  but we still want to assign a score that provides some
information on how well a new ingredient might complement the existing ones  we propose the
following modification 
suppose we have  ingredients  then for a fun factor of  the probability of adding a new

ingredient  is the union of all   subsets 

for example  for     
                                                     
                                                
where we assume that the complement of each subset is independent 

 
 in expectation  after
the probability vector   is then scaled by   
such that we get 
     

using a random number generator to decide whether or not the ingredient is ultimately included 
with probability      thus  a lower fun factor increases the likelihood a less correlated
ingredient is picked and allows the algorithm to find new  plausible  ingredient clusters 

fipredicting ingredient amounts 
another important component to recipe generation is determining how much of each
ingredient is needed for a particular recipe  our approach to this problem was to create a model
of what quantities of various ingredients go together  and then use this model to estimate
ingredient amounts for new ingredient sets  although we considered using a mixture of
gaussians and principal component analysis to create this model for relative ingredient amounts
used in recipes  we ultimately selected factor analysis  this decision was made due to the high
dimensionality of the sample space  the number of ingredients in the training set  being
approximately equal to the size of the data set  without significantly more training samples 
neither of the other two options would be very reliable  using factor analysis  we were able to
reduce our n dimensional sample space down to k dimensional space using the transformation
x      z      where x is the feature vector   is the mean of the feature vectors  z is the factor
vector distributed n    i    is an error term distributed n       and  maps from rk to rn 
in training the factor analysis model  the feature vectors were n dimensional  where n
was the number of unique ingredients in our sample space  and each vector represented the
ingredients of a recipe  within each vector  i of the entries corresponded to the i ingredient
amounts used in the recipe  all other n  i entries were initialized to    some difficulties were
encountered here in that it was sometimes challenging to extract an amount for an ingredient  if
an ingredient was just salt  or bag of rice   and units were not always standardized  however 
as can be seen from the figure below  when fitting a hold out set to our model  ingredient values
were mapped similarly to the distribution we trained  suggesting that despite these parsing issues 
the resulting model is valid 
one of the major problems encountered in performing factor analysis was not that real
recipe information did not map well into the factor space  but rather that non recipe data also
mapped well onto the factor space  with too few factors  nearly all feature vectors were mapped
close to the mean  which would make it difficult to predict which recipe amount combinations
were desirable  the limiting factor in the number of features we had was that for larger k values 
we ran into underflow issues in calculating the mapping function 
therefore  we used as many factors as possible that would reliably produce a mapping
function from rn to rk  we determined that these higher dimensional transformations were more
effective by mapping randomly seeded ingredient vectors of   to    ingredients onto the factor
space  where the random seeds were within realistic amount ranges  we expected these points to
map near the mean of the trained data  because they have the right number of ingredients and
reasonable values for the amounts  but to not be strongly correlated with the actual ingredients 
because random seeding will likely yield undesirable mixes  an example of this is shown in the
graph below 
factor analysis
   
   
   
z 

   
 
    

    

    

           

   

   

   

   

    
    

training data

    
z 

random sample
validation data

fiafter performing factor analysis to find a low dimensional subspace of plausible amount
vectors  coordinate ascent was performed to select the amounts of each ingredient selected by
maximizing the likelihood of each ingredient amount  for each ingredient in the recipe  the
amount was initialized with a random value  and coordinate ascent was run to maximize the
probability of the projected vector  coordinate ascent was used instead of gradient ascent in
order to enforce the constraint that the amounts for ingredients not selected for the recipe should
remain zero 
assume there is an affine mapping from     to                      because  is not
square  we use the normal equations to get a least squares fit  giving us 
                              
based on the factor analysis model    is a low dimensional standard gaussian  so 
 
      
 
 
 
                  
   
  exp       
    ff
now we want to maximize       with respect to the amounts of our chosen ingredients      
because there is a linear relationship between     and          we can derive the following
update rule 
      
 
 
         
 
  
where we define 
      
 
      
 

    bc        
         
   exp       
 
    ff


here     is the de column of matrix    and   is our learning rate  this update was run in order
on each coordinate for which there was a valid ingredient  until convergence  for simplicity  we
assumed that the unit associated with each ingredient was the most frequently occurring unit of
that ingredient  due to numerical instability issues  we had trouble getting the amount vector to
converge  which would be the focus of a future experiment 

classifying recipes 
each recipe in the training set was hand tagged as an appetizer  entre  dessert  or other 
then  using the ingredients as features  we generalized the binary classification nave bayes
model into one that classifies for multiple classes  the user could input what type of category
they wish to cook  and our algorithm would keep generating sets of ingredients until it finds one
that matches their category 
in short 
f g happetizer  entree  dessert  othert
 
   uvingredient  is in recipe      
to form a prediction  we find
          
arg max        
 
            
with the nave bayes assumption of
c

        b         
 d 

fiand the maximum likelihood estimates are given by 
 
 

  i    
 d  ug     h  
e fd          
    t    f 

 d  uh 
 

 d  uv     
e        

further studies 
in order to extend this project  we would like to obtain more data to get a denser sampling
of the  recipe space   this would allow us to fit a more accurate distribution of recipe
combinations  yielding more plausible combinations and amounts of ingredients  furthermore 
having a more densely sampled space would allow us to use more search techniques over the
distributions  such as genetic algorithms or simulated annealing  it would also allow us to use
other techniques to create a model for good recipe amount combinations  such as mixture of
gaussians and principal component analysis 
the natural language processing problems inherent in extracting useful information from
recipes is another area in which this project could be expanded on  having more accurate
techniques for extracting ingredient names from recipe lists could help reduce errors and
redundancy in our list of ingredients 
although we were ultimately unable to tackle the problem of synthesizing preparation
instructions  more intelligent parsing techniques would also help achieve this goal  lack of
reliable parsing techniques is a huge obstacle for this goal  as it requires understanding where
verbs are in sentences and which objects they are operating on  another major obstacle in
synthesizing instructions is defining a structure for ingredient steps  the structure we envisioned
consisted of each instruction step equating to a recipe state  and one could move to different
recipe states through actions such as mix  stir  or bake  if the data could be parsed
sufficiently well  we could try to fit some sort of markov model  such as a hidden markov
model  to the data in order to learn transitions between different cooking actions and
ingredients  combining this step generation with our existing ingredient clustering method would
then be able to produce complete recipes 

fi