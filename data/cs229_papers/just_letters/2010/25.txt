 

a better bcs
rahul agrawal  sonia bhaskar  mark stefanski
i  i ntroduction
most ncaa football teams never play each other in a given season  which has made ranking them a long running
and much debated problem  the official bowl championship series  bcs  standings are determined in equal parts
by input from coaches  sports writers  and an average of algorithmic rankings  these standing determine which
teams play for the national championship  and  in part  which teams appear in each bowl  playoff  game 
we seek an algorithm that ranks teams better than the bcs standings do  in particular  since the bcs standings
serve to match teams in bowl games  we want better bowl game prediction accuracy  this is a uniquely difficult
prediction problem because  by design  teams squaring off in a bowl game are generally evenly matched and come
from different conferences  which means they are unlikely to have many common opponents  if any 
ii  t he m odel
we model the outcome of a game between two teams as the difference between their  noisy  competitiveness
levels  we assume a team has a certain fixed average ability  i   and that for each game a multitude of independent
random factors collectively determines to what extent that teams performance falls short of or exceeds its average
ability  the central limit theorem suggests that we model these random factors as   n           therefore  team
is competitiveness in any given game is distributed as xi   i     n  i         so when team i plays team j  each
team independently samples from its competitiveness distribution  and the resulting score difference is distributed
as xj  xi  n  j  i         
 k 
we denote by yi j the actual score difference of the kth contest between team i and team j  and to avoid
redundancy  we adopt the convention that the score difference between team i and team j where j   i is team js
score less team is score  so our model says that for the kth meeting between teams i and j 
 k 

 k 

yi j   xj  xi   j  i   i j  

 k 
where the i j      n           are mutually independent 
this model easily extends to m games played among a common
teams plays every other exactly once 

 
 
y   
x   x 
           
     
    
  
  
  
  
    
   
 
 
 
 

 
 
 y  n   xn  x              

 
 
 y      x   x               

 
 
       
      
  
  
  
  
    
   
 
 
 
 

 
 
 y  n   xn  x               

 
 
    
   
  
  
  
  
     
    
 
 
 
 
yn  n
xn  xn 
 
       
   z  
 
 z
ym 

amn

pool of n teams  for instance  if each of the n
 
  
 
 
 
  
 
 
  
 
 




   
 
    
     
   
 


  




      
   n 




           

               




         


    n 
 


 n 
   
    n
 
      z      
n  n
   x  n 
 
   z  
m 

if teams i and j do play each other multiple  k      times  then there will be k  duplicate  rows of a corresponding
   
 k 
to the yi j           yi j entries of y  if teams i and j do not play at all  then yi j will be absent from y  as will the
corresponding row of a  in reality  many teams might not have played each other in the current season  or  in
some cases  ever  so many of the yi j could be absent from y  in any case  our model reduces to y   ax    
iii  u nweighted l inear r egression
a  finding the maximum likelihood abilities
since we can rank teams by their abilities  our goal here is to find the maximum likelihood estimate of ability
vector x   to do so  we must set a reference value for the i because otherwise there is no way of distinguishing
between x and  x that differ by a constant offset k n  
a x   a x   k n     ax   ka n   ax  

fi 

that is  k n  n  a  by construction of a  so we introduce the condition  tn x               n      which sets
the average ability of the pool of teams to zero 
we would like to find the maximum likelihood estimate of x by the least squares approximation
  t
at a
a y
and then adjust the result so that  tn x      but a is never full rank because its nullspace is always nontrivial  and
therefore at a cannot be full rank  or  consequently  invertible   however  under certain conditions on the games
played  teams are competitively linked  
 t
 
 ac   m   n   n
a
is skinny  m      n  and full rank  see proof   in the appendix   note that
 
 
 t
 
 
 
 
  n x  

y
a
  z 
  z 
   z  
 yc   m    

ac

so we can compute
x   atc ac

 

c

atc yc  

importantly  x is the maximum likelihood estimate of x   not just the minimizer of   ac x  yc     because x must
also minimize   ax  y     see proof   in the appendix   the minimizer of which we know is the least maximum
likelihood estimate of x  
b  finding the maximum likelihood noise parameter
with the maximum likelihood abilities x   we can predict whether team i will beat team j on average by
determining if i   j   but to determine the probability with which team i beats team j  we need to estimate  
to this end  note that y is an m dimensional gaussian vector distributed as y  n  x       imm    then
max log p y x    




 
 
 t
 
 

exp

 y

a
 
  
i
 
 y

a
 
  max log
mm
x
x

 
   m        imm     


 
  max  log    m     m     y  ax  t  y  ax  

 
 
  min m log         y  ax      

 
taking the derivative of the above with respect to   setting it equal to    and solving gives the maximum likelihood
estimate
  t
  y  a atc ac
ac yc   
  y  ax   



  
 
 
 m
 m
max log p y x    
x  

 

which is precisely the root mean squared error of the observed score differences  each of whose variance is       
since gaussians are completely determined by their mean and variance  the maximum likelihood predicted
outcome of a game between team i and team j is distributed as n  j  i           it follows that we would expect
the score difference between team i and team j to be  on average  j  i   and that team j defeats team i with
probability
 

j  i
p  j defeats i     
 
  
where  is the standard normal gaussian cdf function  more generally  we have a complete description of the
predicted outcome between any two teams 

fi 

iv  l ocally w eighted l inear r egression
we look to improve our predictions by giving increased importance to the most relevant observed outcomes 
consider a motivating example  team a defeats team b by    points  team b defeats team c by    points  and
team c defeats team a by    points  we are then asked to predict the outcome of a second meeting between team
a and team b  unweighted linear regression would assign each team an equal ability and thus would predict a draw
in team a and team bs second meeting  this is indeed the most likely explanation of the data given the model 
but one would think that team as defeat of team b matters more than team bs transitive defeat of team a 
in predicting the outcome between team i and team j  we consider the most relevant observed outcomes to be the
ones closest to the match up between teams i and j   i  j   to capture the notion of distance between games  we
construct a graph in which each vertex represents a team and edges join teams that have played  then to quantify
the distance between match ups  i  j  and  k  l   we compute the length of the smallest cycle containing i  j  k  and
l  adding an edge between i and js vertices and between k and ls vertices  if necessary   we denote this measure
of distance dg   i  j    k  l   
from these graph distances we can specify any number of weight schemes  in trying to predict the outcome of
team i and team j  we choose to weight each observed outcome  k  l  as
wi j  k  l    dg   i  j    k  l    
where we set      because this value minimized win loss bowl game prediction error over the past three seasons 
 i j 
to predict yi j   we construct a diagonal matrix hmm where the diagonal element corresponding to yk l is set
to   wi j  k  l   to enforce the  tn x     constraint  we construct


c
 tm
 i j 
 hc   m    m     
 m h i j 
 i j 

where c  r is a very large constant  then the maximum likelihood estimate of x
weighted case is

 
t
 i j 
 i j 
 
a
h
a
atc h i j 
yc  
c
x
c
c
c

for predicting yi j in the

yet again  we must solve for the maximum likelihood parameter using ac and yc instead of a and y to ensure
 i j 
the matrix to be inverted is full rank  but this does not result in a worse fitting x  see proof   in appendix  
 i j 
instead of having a single x that gives an immediate ranking of the teams  we have distinct x for each outcome
 a b 
 a b 
predicted  returning to the motivating example  it could  and indeed should  be the case that a
  b
 b c 
 b c 
 a c 
 a c 
but b
  c
and c
  a
  so we have traded consistency of predictions for relevance of predictions 

v  i mplementation and r esults
a  data processing
we trained on the regular season data  and tested on the post season bowl data  we processed this data by
assigning each team an index  and removing any duplicate games  if team i plays team j  then team j also plays
team i   since data collection was time intensive  we collected data for four seasons only 
b  results and analysis
the errors in table i show our unweighted and weighted linear regression algorithms have similar performance 
both found the           bowl games particularly difficult to predict 
season
         
         
         

unweighted linear regression
avg  train avg  test
w l train w l test
     
     
    
    
     
     
    
    
    
     
    
    
table i
s ummary of e rrors

weighted linear regression
avg  test
w l test
     
    
     
    
     
    

fi 

win loss error  w l   the number of games whose outcome we predicted incorrectly  divided by the total number
of games predicted 
average absolute error  avg    the sum of the absolute differences between the actual score margin and our predicted
score margin  divided by the total number of games predicted 
for each of the past three seasons  we trained our algorithm on the regular season data and tested on the bowl
game data  we compared our algorithms prediction success rate to the official bcs rankings  the official bcs
computer rankings  and espns power rankings  these rankings only has predictions for about half of the bowl
games each season  which is why our algorithms win loss test error here differs from that of table i  
season
         
         
         

bcs overall
    
    
    

bcs comp  avg 
    
    
    

espn power ranking
n a
    
    

us  unweighted 
    
    
    

us  weighted 
    
    
    

table ii
c omparison of r anking s ystem w in  l oss p rediction s uccess r ate

surprisingly  over the past three seasons  the experts bowl predictions have been wrong more often than not 
also  our algorithms performed best  in terms of average training and test error  as well as win loss training and
test error  on the           season data  but this was the expert rankings wost year  both of our algorithms
equalled or outperformed the expert rankings each of the past three seasons 
c  predictions
table iii shows our predictions for this current seasons upcoming major bowl games  for each game  we
predicted the winner  the margin of victory  and also computed the probability of the win  derived from     our
two algorithms have made near identical predictions  and both predicted a comfortable win  with probability      
for second ranked oregon over first ranked auburn in the national title game 
major bowlgame matchups
auburn    
oregon    
wiconsin    
tcu    
arkansas    
ohio state    
oklahoma    
connecticut
virginia tech     
stanford    

winner
oregon
tcu
ohio state
oklahoma
stanford

point margin  unweighted 
 
 
 
  
  

point margin  weighted 
 
 
 
  
  

probability of win
      
      
      
      
      

table iii
o ur p redictions for u pcoming           m ajor b owl g ames

vi  c onclusion
the structure of the ncaa football season poses some challenges to the formation of a fair ranking system 
since the vast majority of teams do not play each other  ranking them in a way that is fair and completely assesses
their relative abilities is not an easy problem and has resulted in a lot of criticism of the current bcs rankings 
our algorithms predicted bowl game results for the past three seasons with much better accuracy than both chance
and expert predictions  we expect our algorithms predictions for the this seasons upcoming bowl games to be
similarly successful 
there are plenty of opportunities to extend our work  for one  we could explore different weight schemes for
weighted linear regression  more broadly  since there nothing ncaa football specific about our model or our
algorithms  we can apply them to other sports leagues  especially those with more teams than games played per
season  and even competitive activities beyond sports 

fi 

vii  a ppendix
a  proof  
given a vector of results y  we can construct a graph g in which each vertex represents a team  an edge joins
two vertices if and only if the two teams represented by those two vertices have played each other  and the outcome
is contained in the dataset   we say the pool of teams is competitively linked if g is connected 
we claim that ac is full rank if and only if g is connected 
if g is connected  then for any two teams i and j we have
xi  xk 

 

yi k 

xk   xk 

 
  
 

yk   k 

xkk  xj

 

ykk  j  

summing these equations 
yi j   xi  xj   yi k    yk   k            ykk  j  
now we can show n  ac        so suppose y      then for every i and every j
    yi j   xi  xj  
hence x    x            xn   then the constraint  tn x     implies x    x            xn      so y     implies
x      hence n  ac       and ac is full rank whenever g is connected 
if g is not connected  then we can partition the xi into i and j where no team in i has played a team in j
 and  of course  vice versa   then define x   rn where x i      i  if i  i and x i      j  if j  j  clearly 
x       and ac x       so in this case n  ac         hence if g is not connected  then ac is not full rank   this is
what intuition would tell us  if the pool of teams is not competitively linked  then there is no way of comparing
connected component of teams to another  
b  proof  
we claim that
min j x    minn  y  ax t h y  ax    minn  yc  ac x t hc  yc  ac x    minn jc  x 

xrn

xr

xr

xr

in the special case h   inn and hc   i n    n      this claim becomes
min   y  ax      minn   yc  ac x     

xrn

xr

note that
min jc  x     yc  ac x t hc  yc  ac x     y  ax t h y  ax     hc           tn x    minn j x 

xrn

xr

n

for all x  r since  hc           so it suffices to show minxrn j x   minxrn jc  x   to this end  let
x    arg minxrn j x   then define a     tn x    n to be the average of the entries of x    using a and the fact
that  n  n  a  
min j x 

xrn

 

j x   

 

 yc  ac x   t hc  yc  ac x   

 

 yc  ac  x    a n   t hc  yc  ac  x    a n   

 

 y  a x    a n   t h y  a x    a n       hc           tn  x    a n    

 

 y  ax    aa n   t h y  ax    aa n      hc        tn x   a tn  n   

 

 y  ax   t h y  ax       hc       an  an  

 

 y  ax   t h y  ax   

  jc  x   


min jc  x 

xrn

fi