cs    final project  enhancing automated question classification
william mee wmee stanford edu
seung yeoul yang syyang stanford edu
   december     

abstract

 

question datasets

this class project investigated improving classification of
short questions  we first established a baseline for further work by comparing a number of different off the shelf
classifiers  we gathered a new corpus to add to ones already available  we then implemented and applied semisupervised classification  adding large amounts of unlabelled
data in an attempt to boost classification accuracy  finally we investigated feature expansion  both via semantic
knowledge and with augmentation by automatically learned
topics 

we made use of two corpora for this project  firstly  tests
were done on the trec    qa dataset      this data has
     short questions which are labelled with   coarse categories  abbreviation  description  entity  location  human
and number  and    fine subcategories  an example question from this dataset is
when did the berlin wall go up  

which is assigned to the coarse fine category num date 
the second corpus is one which was gathered for this
work from yahoo  answers      the only online question sys  introduction
tem we are aware of which has a public api  answers data
is structured into a hierarchy of themes  for example enthis work investigated classification of short  closed ques  vironment global warming  we selected questions from
tions  examples of these are
the history theme  since many of these are in closed form 
and manually labelled the data using the same categories
what was the united states first national monuas the trec    questions  the english questions downment 
loaded were those marked as resolved  reverse sorted by
how do you say red in spanish 
number of responses  although we filtered out questions
which were not in closed form and de duplicated  we did
apart from being short  such factoid questions are wellnot do spelling correction or make grammatical changes  as
structured  they generally have a question word  one verb
such this yahoo  history data is significantly less regular
and one or two nouns  this makes question classification
than the trec    dataset 
a domain distinct from document classification  the question classification task is important as a part of a larger
automated question and answer system 
comparison of supervised classiwe began by analyzing the effectiveness of several clas   
sifiers  including naive bayes and support vector machines
fiers
on labelled training and test questions  we then used this
as a baseline to explore the following series of experiments
an important initial step was to establish a baseline comto improve the accuracy of the classification
parison for different supervised classifiers on short ques   a semi supervised approach in which we applied expec  tions  for algorithm implementations  we made use of
tation maximization  em  on a combination of labelled the weka machine learning project      this allowed us
to quickly compare three different classifiers  naive bayes 
and unlabelled questions
naive bayes multinomial and svm   it also allowed us to
experiment with the effects of feature selection  stopwords
   feature expansion using lexical information
and stemming  in initial experiments  we ran the candidate classifiers against training subsets of various sizes and
   feature expansion using automatically learned topics
measured the error rate of the resultant classifier by using
details of these experiments follow 
   fold cross validation against the same training data 
 

fipurposes in many other experiments 

figure    baseline classifier comparison  fine categories

 

   

classification

semi supervised classification combines labelled and unlabelled data to improve classification accuracy  while this
is an active field of research in machine learning in general
      there is little application of this to the question answer
domain  however  the intuition that additional structure
implicitly provided by unlabelled data which could help classification seemed applicable  a key motivation is that with
question classification  or text classification in general  there
is an abundance of unlabelled input data  which we would
like to take advantage of for improving our classifier  our
own data collection effort was proof of how difficult and
tedious data labelling can be 
in this part of the project  we implemented the combination of a naive bayes multinomial classifier with expectation maximization in two variations suggested by nigam et 
al     the basic approach here is a generative one  which
supposes every document is generated from a probabilty
distribution parameterized by   the probabilty distribution
consists of a mixture of components cj  c    c         c c    
in addition  the training data d consists of labelled data 
dl   and unlabelled data  du   such that d   dl  du  
parameter estimation is done with the labelled data and
then repeatedly refined by applying to the unlabelled data
and retraining in a form of expectation maximization 

feature selection  stemming and stop
words

we experimented with using mutual information measures
to reduce the numbers of features  in all experiments  this
increased the error rate of the categorizing  and the fewer
features used the more the error rate increased  for example  selecting the top     features for the naive bayes
multinomial on a training set of      questions reduced accuracy from       to        using only     features further
reduced this to        one explanation of this is that the
questions being categorized are very short  so almost all the
limited number of features have some discriminating use 
we used all features for the rest of this project 
we also experimented with using a set of stop words
obtained from the apache lucene project  similarly to the
feature reduction  in all experiments leaving out the stop
words slightly degraded the performance of the classifiers 
for example  the accuracy of naive bayes multinomial on
     questions decreased from       to      
while some experiments saw a decrease in accuracy when
using stemming  most had an improvement  we therefore
consistently used stemming for the rest of this project 

   

semi supervised
using em

   build an initial classifier from dl only and calculate
parameters 
   repeat until convergence 
e step  use the current  to classify du
m step  re estimate  given estimated component
membership of all data

this approach is mathematically correct only for a naive
bayes multinomial classifier  this base algorithm is then
refined in two variants detailed below 

comparison results

the different classifiers displayed different performance
characteristics with training set size  a summary of our
baseline is given in figure    the data had stemming applied to the fine categories in the trec    dataset  it is
unsuprising that that the svm with linear kernel provides
the best accuracy for any input dataset size  and that accuracy increases with dataset size  however since one of the
core experiments we performed  semi supervised learning
using expectation maximization  is based on a naive bayes
multinomial classifier  we used this classifier for comparison

   

em 

assuming there is one to one correspondence between a
class and a mixture  i e   each class is generated by a single mixture  the em  algorithm suggested by nigam et al
allows the contribution of unlabelled data to be weighted
to avoid dominance when the ratio of labelled to unlabelled
data is low  in this scenario the log likelihood equation can
 

fibe stated as 

figure    em  test results
 c 

x x

   d  z    logp     

zij log p  cj   p  di  cj     

di d l j  


  

 c 
x x


zij log p  cj   p  di  cj     

di d u j  

where zij the probability of di belonging to cj   and     
     determines the contribution of the unlabelled data
to the overall log likelihood 

   

multiple mixture component per class

the multiple mixture component per class approach relaxes
the assumption in em   allowing for a many to one correspondence between mixture components and classes  this
is motivated by an effort to model documents as close to
reality as possible  since the number of latent variables may
be greater than the number of classes  to find the posterior
probability p  cj  di    once we obtain the probability for each
mixture component  we then need to sum the probabilities
over the classes to which each mixture component belongs 
note in our implementation  the number of mixture components is set to be identical for each class  furthermore 
we heuristically pick a small number for the number of mixture components since having a large number of mixture
components drastically increases the training time 

   

figure    multiple mixture component test results

test results

we tested both the em  and multiple mixture model
against uiuc dataset and questions collected from yahoo 
history data  however  neither of them managed to achieve
a significant improvement over off the shelf classifiers  in
fact  our experiments showed that in the context of question classification  incorporating unlabelled data can actually deteriorate classification performance 
as can be seen in figure    the result of em  was worse
than the baseline performance of naive bayes multinomial 
the result was obtained in    fold cross validation 
figure   compares the performance of baseline naive
bayes multinomial classifier with multiple mixture component classifiers of varying mixture component size  denoted
as k  unfortunately  the algorithm did not do too well on
our data sets  we tried running the classifier with a higher
k such as      and so on  but all of them gave the same
result as in the case when k      one possible explanation
for the poor performance may be that questions violate the
model assumption enforced by multiple mixture component
model 

 

semantic expansion

in this section of our project  we investigated augmenting
features of the dataset with ones obtained from semantic
structure  the intuition here is that  because the factoid
questions are almost all single  short sentences  the probability of a feature being present in both training and test
questions selected from same category is much lower than
in longer documents  and augmenting features with more
generalized version would improve classification accuracy 
our approach was to to identify nouns and verbs in the
questions  and then expand these with hypernyms which
provide generalizations  for example  the hypernyms of
car include both motor vehicle and compartment  following similar experiments         we used a part of speech
tagger in an initial step  and then used a sequence of
wordnet    lookups to do the expansion  wordnet maps
a word to one or more senses  each of which is associated
 

fiwith a symset of words which share the same set  each
symset is in turn linked via hypernym  and other  pointers
to other symsets 
the tokens we chose to expand  as well as the expansion
policy within wordnet  were investigated in a series of experiments  in terms of selecting candidate token  we looked
at generalizing all nouns  all verbs and then just the first
noun as a substitute for the lead noun  as suggested by     
we then experimented with aggressive expansion  in which
all senses of the word were generalized  versus conservative
expansion  in which only the first sense of the word was
generalized  see figure    

figure    single noun hypernym expansion test results

figure    hypernym expansion  with conservative approach
highlighted

figure    token frequencies with hypernym expansion of
nouns

   

test results

classification accuracy was consistently improved by hypernym expansion of nouns  with the best results obtained in
aggressively expanding a single noun only  as shown in figure    the improvement was measured across classifiers 
the impact of expanding verbs in a similar way had an
insignificant impact on accuracy  possible explanations include that the hypernym network is less dense in wordnet
for verbs than for nouns and that the verbs are less useful for
descrimintation  indeed in a large number of the questions 
the only verb is to be 
the positive impact that the hypernym expansion has on
frequency of token occurrence is demonstrated in figure   
the heuristic of using the first noun as the lead noun
is possible with the well structured question dataset  but
would not be applicable in a normal text with longer sentences 
in a second set of experiments  we applied the same expansions to the yahoo  history      question dataset  the
baseline classification accuracy using naive bayes multinomial on this data was        which is comparable to the
    accuracy on the trec    data although the number
of categories dropped to    from     details of the improvement given by hypernym expansion is given in table
  

 

latent dirichlet allocation

another classification method we considered was latent
dirichlet allocation  lda   lda represents a document using a generative model where given a class  the document
is generated from multiple mixture components   note that
lda relaxes the many to one correspondence assumption
in the multiple mixture component model  discussed in the
previous section  by assuming a many to many correspondence between mixtures components and classes  experiments have shown that the weakened assumption improves
classification for text documents     
for our purpose  we used topics obtained from lda to
augment the features in each training example to improve
classification performance  using the mallet software package  we first ran lda on the training examples to extract
topics from the training examples  the number of the topics was empirically set to           we then augmented the
 

fianalysis  while the latter did not prove effective  noun hypernym expansion based on a simple heuristics was shown
to consistently improve classification accuracy across clasdescription
accuracy precision recall sifiers and granularity of categories  use of more sophisticated word sense disambiguation and better identification
baseline nb multi 
    
    
    
of the lead noun of the sentence is likely to improve this
conservative hyp  exp      
    
    
more  but was beyond the scope of this project 
our work demonstrated how some aspects of machine
learning are robust across different datasets  while others
original features with the topics  and trained an svm with are restricted  sometimes in subtle ways  which limits their
polynomial kernel on the new data  however  as shown in applicability  both the failure of semi supervised classificafigure    adding the topics to the original features actu  tion and the effectiveness of semantic feature augmentation
ally degraded the performance slightly  in hindsight this is were unanticipated results of this work 
somewhat expected because we were not adding any new
information to the features 
table    hypernym expansion of yahoo  history data

references

figure    lda topic augmentation test results

    andrew kachites mccallum 
mallet 
chine learning for language toolkit 
http   mallet cs umass edu       

a maurl

    xin li and dan roth  learning question classifiers 
the role of semantic information  in in proc  international conference on computational linguistics  coling  pages              
    donald metzler and w  bruce croft  analysis of statistical question classification for fact based questions 
journal of information retrieval                 
    george miller  wordnet a lexical database for english 
communications of acm                    

 

    david m  blei andrew y  ng and michael i  jordan 
latent dirichlet allocation  journal of machine learning research                  

discussion and conclusion

in this project we broadly examined approaches to improving
    kamal nigam  andrew kachites mccallum  sebastian
classification of closed form factoid questions  an initial
thrun  and tom mitchell  text classification from laexamination of the impact of stemming  stop words and
beled and unlabeled documents using em  in machine
feature reduction and comparison of several well known sulearning  pages              
pervised classification techniques allowed us to establish a
baseline to measure later work on 
    nist trec    qa track  http   trec nist gov 
one of the core aims  the application of semi supervised
data qa  
classification by adding a large amount of available non    weka machine learning project 
weka 
url
labelled data to a limited set of labelled data  was examhttp   www cs waikato ac nz ml weka 
ined in the form of expectation maximization which has
successfully been applied to datasets of documents  we
    yahoo  answers  http   answers yahoo com  
both implemented multiple versions of this technique  but
failed to use it successfully  presumably because the genera       xiaojin zhu  semi supervised learning literature surtive model on which this approach is based  while it may still
vey  technical report  computer sciences  university
apply to questions  is not measurably manifested because
of wisconsin madison       
of question length 
this lead to investigations of how the limited features
available could be augmented  we did this both through
application of external semantic knowledge and with topics
automatically learned from the dataset using latent dirichlet
 

fi