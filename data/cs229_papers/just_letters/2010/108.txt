cs    project  particle classification
kahye song
kahye stanford edu
december        

 

introduction

in electron tomography  averaging multiple particles of the macromolecule of interest is an essential tool to achieve high
resolution reconstruction  a particle means one measurement of the object of interest in structural biology  averaging
requires aligning and classifying particles to obtain the highest resolution      these tasks are very challenging because of
low snr below    and missing frequency information      along z axis  due to the image acquisition restrictions  in fig 
   we can clearly see the differences between two different layering in the averaged reconstructions left column   however 
in a single particle before averaging right column   we can hardly see any structure at all due to low snr  this makes the
classification task challenging for humans do with naked eyes  it took      and      particles to create averaged single and
double s layer pore respectively 

figure    xy yz and zx cross sections of averaged upper left  and single particle before averaged  upper right  single s layer
structure  cross sections of averaged bottom left  and single particle before averaged  bottom right  double s layer structure
also  the size of volumetric feature  n     is prohibiting when processing      or more particles  in this project  we are
focused on    evaluating a supervised classifier  svm and a unsupervised one  k means and    finding low dimensional
features of s layer of bacterial cells which have two categories  single layer and double layer  the classifiers and features are
going to be evaluated jointly according to the classification accuracy and training and prediction time 

 
   

classification methods
support vector machine svm 

given a set of instance label pairs  xi   yi    i           m  xi  rn   yi           solving the following optimization problem in
eq    provides the maximum margin classifier
m

x
 
min wt w   c
max    yi wt xi      
w  
i  
 

   

fiwhere c     is a penalty parameter  this is the exact formulation implemented in liblinear svm library      in our
particular problem  the double layer particles are labeled   and the single layers are labeled    

   

k means

given a training set x   x         xm   xi  rn but not the associated labels yi   k means clustering algorithm aims to
partition these observations into k sets  k  m  with associated centroids                  k   so as to minimize the sum of
within cluster distances in eq        
m
x
j x     
d xi   ci  
   
i  
n

here d   is a distance metric function in r and ci is the index of a partition which an observation xi belongs to  the
algorithm starts with a random set of centroids and divide the observations into partitions which minimize j x    in eq    
then it updates the centroids as the mean of the partitioned observations  the algorithm typically uses euclidean distance
as a distance metric  however  it can be generalized to use different distance metrics with proper centroid update equations 
xt y
  where        is l 
in this project  normalized correlation is used as a distance metric  it is defined as dnc  x  y         x    y  
norm  this is equivalent to euclidean distance if all data points are normalized  the normalized correlation is scale invariant
and it is more robust to distinguish data points which are only differ in a subset of dimensions  especially in low snr  this
is well known phenomenon in high dimensional clustering problem      the double slayer and the single slayer do resemble
each other a lot and it is mostly distinguishable along z axis  therefore the normalized correlation metric did better job in
clustering particles than euclidean distance 

 

feature selection

four features listed below are tested on both svm and k means classifiers 
   volume   this is a raw observation of a hexagonal slayer structure  it is in r        
    d fft of volume   this is a set of  d fft coefficients of a raw volume  it is in c         
   collapsed volume   this is a sum of a raw volume over x and y axis where the double layer is not visible  it is in r    
   fft of collapsed volume   this is a set of  d fft coefficients of a collapsed volume  it is in c     
i tried to use pca to reduce the original feature space which is the original volume space  due to the large dimension r        
of the data points  it cannot run in matlab  i did not tried to use pca on the collapsed volume since it is already in a
manageable low dimension such that it does not really justify running pca to reduce the feature space further  also  a study
suggests that classifying principal components does not necessarily improve the classification accuracy     

 

data and software

    particles of single s layer and     particles of double s layer are randomly mixed and split into    groups of approximately equal number      of particles  the ratio between the single and double layer particles in each group is kept equal 
to evaluate how the number of training data impacts the classification accuracy  a sequence of k fold cross validation has
been performed  the number of training data starts from using only   group out of    and increases upto   groups  the
rest of the data is used for testing  for a given number of training data     experiments are carried out by selecting different
training data groups  the size of each particle is            voxels 
i have used liblinear svm library http   www csie ntu edu tw  cjlin liblinear   used in problem set   for
svm and kmeans provided in matlab  the distance metric used for k means is correlation 

 

fi 
   

performance summary and discussion
prediction accuracy

there are many metrics to evaluate a classifier  one of the widely used metrics is a precision recall curve  precision is
defined as a ratio between the number of true positives and both true and false positives  recall is defined as a ratio between the number of true positives and the number of both true positives and false negatives  to evaluate the prediction
performance of a classifier     fold cross validation is performed on     data points  all eight combinations between four
features and two classifiers are tested on the same training and testing data sets  precision and recall in fig    are the average of the values calculated on the testing data set labels given by the classifier trained on the training data set of all    trials 
most of the classifier feature combinations performed well achieving more than      on both precision and recall  k means
in general tends to label fewer positives achieving better precision than recall  svm tends to be more aggressive on labeling
positives achieving better recall than precision  when we look at the overall accuracy  which is defined as the ratio between
the number of both true positives and true negatives and the total data size  svm outperforms k means on classifying any
features tested  see fig     this is an expected result since supervised learning can utilize correct labels of training data
points  the performance of k means is especially worse than that of svm when there are not many training data available  classifying the fft coefficients of the raw volume or the collapsed volume with k means did not provide satisfying
result  the fft coefficients of collapsed volume were not very easy to distinguish between the single and double layers and kmeans failed even to start  therefore the data point is missing for the  d fft of collapsed volume and k means combination 

   

training efficiency

training efficiency is a metric to evaluate how many training data points are needed to attain certain level of prediction
accuracy  the prediction accuracy is the ratio between the number of correct labels given by a classifier trained on a training
data set and the size of the testing data set  here the plotted accuracy value is an average of    trials on different training data set with the same size  the testing data set of each trial is the rest of the data points which are not used for training 
as expected  svm utilized the training data more efficiently achieving better accuracy than k means on classifying each
feature  see fig     it is noticeable that classifying collapsed volume with both svm and k means have high training data
efficiency  the accuracy of classifying any size of training data remains consistently high around      or above  however
classifying whole volume with very small training data set suffers the accuracy  this can be explained by the increased snr
by integrating over the less relevant dimension of the volume  therefore the classifiers are not as confused as the raw volume
when there are not many training data points available 

   

computational efficiency

it is also crucial to have reasonably short training time when there can be      or more particles to classify  as predicted
by the size of each feature  classifying the collapsed volume or its fft coefficients using both svm and k means took a lot
shorter training and testing time  the increase is linear on the dimension of the features and square root of the training data
size  see fig     note that this observation is only partial to these particular implementations 

 

fifigure    from top to bottom     precision   recall    training efficiency training data set size vs  accuracy     computational efficiency training data set size vs  train time 

 

fifeatures
volume
fft 
collapsed volume
fft

dimension
   of real numbers 
      
      
  
   

precision
   
       
       
       
       

recall
   
       
       
       
       

accuracy
   
       
       
       
       

train time
 sec 
        
        
      
      

table    summary of the classification performance of all features using svm  train data size        of the total 
features
volume
fft 
collapsed volume
fft

dimension
   of real numbers 
      
      
  
failed to

precision
   
       
       
       
distinguish 

recall
   
       
       
       
failed

accuracy
   
       
       
       
at

train time
 sec 
        
       
     
the first run 

table    summary of the classification performance of all features using k means  train data size        of the total 

 

conclusions

classifying noisy volume data is a challenging problem for humans because of very low snr and large data dimension 
however it turned out that by exploiting some prior information  we can achieve high classification accuracy with very short
training time compared to classifying the volume itself  here  it is known that one distinct difference between single and
double slayer is that these layers are along z axis which is the direction normal to the surface plane  the hexagonal shapes
on the surface plane are not exactly the same between these two classes of layers but this is more subtle to detect than
the number of layers  therefore summing over the xy plane which does not provide much information can speed up the
procedure without sacrificing too much accuracy 
both classification methods can classify the particles reasonably well  however  if we can have some learned data points 
svm can classify about         more accurately than k means  however  in reality it is almost impossible to tag single
particle by naked eyes  therefore using k means to classify the particles can be a good start  once some data points can be
tagged  we can switch to svm to refine the classification  the number of training data does not seem to impact the accuracy
too much for both svm and k means one it is more than      of the total data set or more 

references
    f  moussavi j  smit k  h  downing m  horowitz f  amat  l  r  comolli  subtomogram alignment by adaptive fourier
coefficient thresholding  journal of structural biology                     
    peer  zimek arthur kriegel  hans peter  krger  clustering high dimensional data  a survey on subspace clustering 
pattern based clustering  and correlation clustering  acm transactions on knowledge discovery from data         
     
    andrew ng  cs    lecture notes on the k means clustering algorithm       
    c  j  hsieh x  r  wang r  e  fan  k  w  chang and c  j  lin  liblinear  a library for large linear classification  journal
of machine learning research                   
    k  y  yeung and w  l  ruzzo  principal component analysis for clustering gene expression data  bioinformatics 
                

 

fi