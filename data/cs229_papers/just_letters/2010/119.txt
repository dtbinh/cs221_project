automated agent for the game roshambofu
adrian marple

 

introduction

roshambofu is a game hosted by moola com that involves two online players playing a modified version
of the well know game ro sham bo  first  every round   points are put up for grabs by the winning
player  in the event of a tie  the points from the previous round carry over to the next round  in this
game  the twist is that one of the three options  i e  rock  paper or scissors  is chosen as a bad  bad in
this case means that the other two hands receive a   point bonus when thrown  finally  the game will
last no more than six rounds and if all games are tie the game is declared a draw 

 

model as an mdp

roshambofu leads itself very naturally to be described as a markov decision process  one simplifying
assumption  however  that was made for the mdp model was that actions do not depend on the choice
of the bad hand  in other words  the actions available are to play the bad hand  call this action a    to
play the hand that beats the bad hand  a    or to play the hand that loses to the bad hand  a    this has
the advantage that the state does not need to reflect the hand that is currently bad and reduces the
number of states by a factor of three  unfortunately  this has the disadvantage that the resulting agent
will not be able to react to something like a predisposition to select rock regardless of the bad hand 
at the very least the mdp must reflect the number of consecutive ties that occurred
immediately before the state  if this is not the case then the mdp will not have a fully accurate reward
function  beyond this three different mdp schemes were used  one which reflected the last hand the
agent used  one which reflected the last hand the opponent used  and one which reflects both of these
two quantities  as described above there are three actions available for all states  namely a   a   and
a    unlike the standard mdp definition  given a state and an action both the reward and the successor
state will be chosen by a probability distribution  note that this still reduces to an mdp where actions
from the states s  to s  have no reward and transition to dummy states with reward and successor state
fixed for all actions according to the above probability function  thus while the problem statement being
used is not strictly an mdp  it will be referred to as one for the rest of this paper   furthermore  rewards
will be computed to be points gained by the agent minus points gained by the opponent to make this a
zero sum game  more concretely  figure   shows the rewards of this mdp 

fiopp hand agent hand
tie count    
a 
a 
a 
tie count    
a 
a 
a 
tie count    
a 
a 
a 
   

a 

a 

a 

 
  
 

 
 
  

  
 
 

 
  
 

 
 
  

  
 
 

 
  
   
 
 
  
   
   
figure    rewards for mdp

  
 
 
   

  

reinforcement learning strategies

   

learning rate

in order for the agent to not pick a policy without an adequate sample size to estimate the probability
distribution  an adjustable learning rate was used  by this it is meant that  with probability alpha instead
of choosing what the agent believes to be the optimal policy it will choose randomly from the available
actions  this rate will decrease for every state as more data about that state has been accumulated 

   

file storage

in order to have memory across different sessions with the agent  all prior experience is stored in a file 
upon the start of a new session  the contents are virtually experienced by the agent  finally  as the
agent gains more experiences they are stored in the same file  furthermore  this file will store
additional events  such as victories  defeats  and draws  this additional information can be used
afterwards to constructs things such as winning rate over time 

   

inheritance

for agents that do value iteration before every move to choose which hand to play  there are only two
things that differ between any two mdp s  first is the state transitions as a function of previous state 
agent hand played  and opponent hand played  second  for rewards there must be a function that
returns the number of previous consecutive ties  as a result  once a basic mdp agent was written any
further agents only had to inherit from that agent  change the number of states potentially and override
the two previously mentioned functions 

fi  

automation

in order for the agent to receive information and actually be able to make moves  it needed a way to
interact with the website that hosts ro sham bo fu  the method chosen to do this was so called screen
scraping using the java robot class  the robot class gives the programmer the ability to see rgb pixel
values for any pixel on the desktop given an x and a y coordinate  furthermore  it simulates mouse
moves and button events as well as keyboard events 
when the actual flash game was is session  an arbitrary unique pixel was selected to be the
reference pixel found by scanning desktop pixels until the matching color is found  then to find the bad
hand  using known offsets from the reference pixel and known colors it was possible to determine either
which hand was the bad hand or that the bad hand was not yet displayed  this process polled until a
bad hand was determined  making a move is as simple mapping from an integer value  representing
rock  paper  or scissors  not a   a   or a   to an offset from the reference pixel and a mouse click at that
point  finally  the opponent hand was determined in exactly the same way the bad hand was
determined  only the offsets and colors were a little less straight forward to find 
the process of navigation between games boiled down to a series of button clicks  for a single
button click first an small image of the button was recorded  then the program scanned for a duplicate
of that image on the desktop and clicked at the upper left corner of the found image  because the
website hosting ro sham bo fu was not purely deterministic  especially because game matching
sometimes failed  moderate human supervision was required either to click until the button the
program expected is present  or restart the program at the designated page 

  

results

   

basics

for each of the three agents    games were played as described in the automation section  in addition 
an agent that chooses hands at random played    games as a control to compare to  finally  an
additional    games were recorded while testing  since each game took about   minutes from start to
start  and over     games total were played  more than    hours of moderate human supervision was
required to gather the data for the following results 

   

win rate

the agents performed dramatically better than random winning about     of the games played  while
the random agent won slightly more than half the games played  interestingly  the was no significant
difference between the performance of the   different agents   todo talk about possible reasons why
this was so  

fi   
  
  
  
  
  
win
 

  
  
  
  
 
random agent

my hand agent

opp  hand agent

both

figure   win rate of different agents

   

expected score

   
 
   
   
expected score
   
per hand
   
 
random agent

my hand agent

opp  hand
agent

figure   expected score peformance

both

fiin reality  the agents were not trying to win  but to score high  thus analyzing score performance more
accurately reflects how well the agents did given this objective  note that while score is clearly highly
connected to wins  the agent that reflected both the agent s and the opponent s previous hand had a
higher expected score per hand  but a slightly lower win rate 

   

game theory considerations

it is interesting to consider what game theory predicts to be a stable equilibrium compared what
opponents actually play  playing against an opponent using the nash equilibrium strategy no matter
what hand is played the expected score is    and deviating from that strategy only allows an opponent
to increase its expected score  that strategy when the previous hand was not a tie is to play a      of
the time  a      of the time and a      of the time  figure   represents how far off online opponents
differ from this strategy when unbiased by seeing their opponent play 

nash equilibrium  
actual first hand  
  

  

  

   

  

  

 
a 

a 
hand

figure   first hand probabilities

a 

fi