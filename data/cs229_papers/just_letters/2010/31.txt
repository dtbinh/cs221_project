scene text understanding

sanjeev satheesh
department of computer science  stanford university

abstract
the task is character recognition from real
world images such as sign boards  this report presents a work in progrss along with
the some interesting results  sparse autoencoders have been used to learn a codebook of
basis functions  and the settings for obtaining
the best results on this task is studied

   introduction
while optical character recognition of scanned documents is considered a solved problem  with nearly
    accuracy  detecting and reading text in natural
scenes is still a very open problem  this is due to
various reasons such as non uniformity of text fonts
seen  skew from the photograph  and improper lighting  glare  illumination and other such factors  the
accuracy of text detection and recognition from natural scenes is around      this problem is known in
literature as scene text understanding or more informally as photoocr  in this project the problem of
scene text recognition is undertaken  given a bounding box around each of the characters in a photo of a
natural scene the task is to recognize what the character within the bounding box is  this report presents
a work in progress along with the results obtained so
far 

   datasets
the following datasets have been assembled 
 icdar           dataset  a set of     images
of real world signs used in the icdar robust
reading competition  lucas       
 microsofts scene text understanding dataset
composed of     real world signs  used in

ssanjeev stanford edu

 epshtein et al        
 dataset composed of     sign boards used for
scene text reading used in  weinman et al  
     
 synthetic data  for learning the general features
of all characters  this includes   sets of      
images for each character  each image is    by
   with a single character on it  the easy set has
the character centered on the image without any
distortions  the characters in the medium dataset
are distorted  tilted and or translated towards a
corner  but on a plain background  and those in
the hard dataset are distorted and on background
that is obtained from a realworld photographs 
the first three datasets provide bounding boxes for
each character in the image  along with a label but
together have only      images  all the datasets have
been homogenized into a single format for easier access 

   feature learning
the features to use for classification  especially in the
case of image processing tasks  is often a result of
weeks and sometimes months of research and is usually
specialized for that application  one way to automate
the process of choosing features is feature learning 
the stacked sparse autoencoder method presented in
 papusha et al   provides a method for learning the
bases that a dataset is made of  each class of data in
any dataset is made of different combination of these
bases  and noting which combination of features reconstruct the data item usually forms a good feature vector for classification  the implementation used in this
project follows closely the one in  papusha et al    and
the loss includes reconstruction error  sparcity penalty
using cross entropy error and regularization 
to learn the bases  the sparse autoencoder requires a
large number of patches  the patches were obtained
from the easy dataset by taking     by   patches
from each of the samples  the autoencoder was then

fiscene text understanding

trained on these patches with a target activation of
     and regularization constant at       for exactly
    iterations of batch gradient descent 
     whitening
the whitening transform is recently found to be
very effective in improving the performance of autoencoders  we tried two different versions of whitening
transforms 

r f     fe  f  fo  
where the first component is a whitening filter  which
attenuates the lower frequncies and amplifies the
higher frequencies  and the second component is a low
pass filter which cuts out the power at the highest
spatial frequencies 
 

     zca
nearby pixels in images are correlated  and if left unaltered  the autoencoder will learn second order relationships like this correlation  which is not of practical
purpose for this task  the zca whitening transform
converts the dataset so that the covariance matrix of
the pixels of the resulting transformed dataset is a diagonal matrix  krizhevsky   hinton         the diagonalization is achieved by calculating the pca of the
data matrix  because the image statistics are roughly
stationary  the eigenvectors of the covariance matrix
will essentially be equivalent to the fourier bases 

figure        bases learnt after applying the combined
whitening low pass filtering on images 

     binarization
an experiment was attempted to learn bases from the
binarized data items  for binarization  we used the
thresholding method listed in  wayne       

figure        bases learnt after zca whitening of the images 

     combined whitening low pass filter
the
seminal
work
on
autoencoders 
 olshausen   field        use a similar whitening transform  but operate directly on the frequency
 
spectrum of the images  natural images have    f
power spectrum which results in large inequities
in variance along different bases with low frequencies having high variance  to remove this skew
we sphere the data by equalizing the variance
in all directions  friedman         the combined
whitening low pass filter used to preprocess the data
had a frequency response of

figure        bases learnt from the binarized data 

all three sets of bases though different visually gave almost similar results in the classification task  results
mentioned in the results section all use bases from figure  
the effect of the three transforms on a data item in
the easy dataset is visualized in the figure    zca 
though much popular in general did not give as clear

fiscene text understanding

whitening transforms or better bases  it is clear that
the second method provides much clearer bases than
others 

figure    the first column shows the original image  the
second one is the result of zca  the third is the result of
the combined whitening low pass filter and the last column
shows the binarized image

figure    plot showing performance of feature extraction
using sum pooling and max pooling against the number of
images per class in the upper case easy dataset

   feature extraction
for extracting the features from a sample data item 
each of the learnt bases is densely convolved on the
data item  and sigmoid  wx   b  is calculated which is
the same as the activation of the hidden layer of the
sparse autoencoder  the activations are then pooled
into   or   regions using max pooling and sum pooling 
max pooling gave better results than sum pooling in
the classification task  as seen in figure  

   classification
for multinomial classification softmax regression with
l  regularization was used  svm with l  regularization was also tried but it did not offer any significant
improvements in time or accuracy 

   results
unless otherwise specified  the following results are experiments run on the the upper case characters of the
easy dataset in grayscale 

figure    plot showing the performance increase obtained
from increasaing the number of pooling regions against the
total number of images from all classes  classification was
done using non regularized softmax regression

     classification results

increasing the number of pooling regions from   to
  gave marginal but consistent improvement in test
accuracies  figure   demonstrates this behaviour 

as the previous plots indicate the best results on
easy dataset was using max pooling which topped at
    this is not the ideal result as commercial ocr
engines can be expected to perform much better and
have accuracies around      on the hard dataset 
the best results were at     test accuracy on the full
set of        images of upper case characters  on the
icdar dataset the best results were around     for
both the lower case and upper case characters  the
learning curves has not converged even though the full
set of      character samples were exhausted  the results are better than the preliminary results obtained
using hog features on all the datasets 

     number of bases used

   future work

increasing the number of bases learnt did not improve
the results as the bases starting duplicating 

these results could be improved by small percentages
still more  among the things to try are

     type of pooling
figure   shows the effect of different types of pooling 
the results are on the upper case characters of the
easy dataset 
     count of pooling regions

fiscene text understanding

computer vision and pattern recognition  cvpr  
     ieee conference on  pp            ieee 
     
friedman  j h  exploratory projection pursuit  journal of the american statistical association          
              issn           
krizhevsky  a  and hinton  ge  learning multiple
layers of features from tiny images  masters thesis  department of computer science  university of
toronto       
lucas  s m  icdar      text locating competition
results  in document analysis and recognition 
      proceedings  eighth international conference
on  pp        ieee        isbn            
figure    plot showing the performance on the icdar
data  the features used are the     bases learnt from the
combined filter whitening transform

 move to color  with   times more data in the
feature vectors  the accuracies may improve by a
few points
 sparse autoencoders seem to be able to learn
bases that are more or less edge detecting filters 
use other methods such as k means to learn bases
which could be more diverse 
 learn bases better suited for this classification
task by using tuning methods
 test on the other real world datasets
in the tests on the icdar data  we tested on upper
and lower cases separately  and training and testing
were both on the icdar data  in order to build a
robust system we need to use the real images only for
testing  testing for accuracy on this setup are being
conducted  also in the pipleline is a full    class  upper lower digits classification test 

acknowledgments
this is joint work with blake carpenter  carl case 
bipin suresh  tao wang and advised by adam coates 
the generation of the synthetic datasets was done by
bipin suresh  the experiments using hog provided
in the results section was provided by bipin suresh
and tao wang 

references
epshtein  b   ofek  e   and wexler  y  detecting text
in natural scenes with stroke width transform  in

olshausen  b a  and field  d j  sparse coding with
an overcomplete basis set  a strategy employed by
v   vision research                         issn
          
papusha  i   ngiam  j   and ng  a  stacked autoencoders for semisupervised learning 
wayne  n  an introduction to digital image processing  prentice hall international london       

weinman  j   hanson  a   and mccallum  a 
sign detection in natural images with conditional random fields 
in machine learning
for signal processing        proceedings of
the        th ieee signal processing society workshop  pp          ieee        url
http   ieeexplore ieee org xpls abs all jsp arnumber     

fi