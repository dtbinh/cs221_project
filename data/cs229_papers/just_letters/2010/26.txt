accent classification
phumchanit watanaprakornkul  chantat eksombatchai  and peter chien

introduction
accents are patterns of speech that speakers of a language exhibit  they are normally
held in common with people of similar ethnic  national  or social background  accent
classification is the problem of finding out what accent people have  that is taking speech in a
language from speakers from various ethnic or national backgrounds and using that speech to
figure out what backgrounds the speakers are from  this provides for a way to extract useful
information   where a person is from   without directly asking that person about it  and instead
just listening to them speak  however  this is not always possible for humans  as sometimes
they encounter accents that they have not heard before  or they come across accents that are
only faintly noticeable  and then they are not able to classify the accents correctly 
this paper considers applying machine learning to the problem of accent classification
in the hopes of being able to reliably classify accents that are in some data set  which can be
much greater than any one human has the time to listen to 
further motivation for examining the problem of accent classification comes from
the problem of speech recognition  currently  accented speech poses problems for speech
recognition algorithms  as different pronunciations of the same words are not recognized the
same way by speech recognition algorithms  indeed  sometimes it is hard even for native
speakers of a language to discern exactly what someone else is saying if they have a thick
accent  if speech could first be classified by accent  the speech could then be passed on to
some more specialized speech recognition algorithm  such as one that was trained on people
with the same accent as the speaker 
dataset
we used the cslu  foreign accented english v     dataset   linguistic data
consortium catalog number ldc    s     it is composed of english speech from native
speakers of    different languages  the data format is   channel    bit linearly encoded wav
files sampled at  khz  although each file contains english speech  the files contain different
words and are not necessarily complete sentences  we did not work on all    accents  we
picked three accents   cantonese  hindi  russian   to work on because it would take a lot more
time to run experiments on    classes than   classes but classifying speech into    and  
classes are algorithmically equally challenging  as they both require multi class classifiers 
feature extraction  mfcc and plp
to convert an array of sample amplitudes from sound files to a more useful
reporesentation  the first features we used are mel frequency cepstral coefficients  mfccs  
this is the most popular feature in speech recognition  mfccs separate the samples into small
windows because the whole sample arrays hold too much information  so  we divided the sound
into small chunks of time  we run discrete fourier transform dft  on each window to get the
power spectral  which holds frequency information of the signal over time  then we convert the
resulting spectral into values on the mel scale  which is a perceptual scale of pitches judged by

fihuman listeners  next  we do a discrete cosine transform of the log mel scale values  which
gives the vector of mfccs  we used the first    coefficients as our base feature  because using
all the coefficients would have given us too many features  we also added an energy of the
window to the vector  resulting in    features  we then appended the vector with delta  and
delta delta  first and second order derivatives of the mfcc  of the vector to get    features in
the end  this accounts for changes in the base features 
the other feature that we tried is perceptual linear prediction plp   plp is similar to
mfcc  they both separate the signal into small windows   we used equal window size for both
plp and mfcc so that we could combine the features   and run dft to get the power spectral 
plp then does critical band spectral resolution by converting the spectral into values on the
bark scale  next  plp does pre emphasis based on loudness  runs inverse dft  and does
linear predictive analysis  lpc  on the result  in our experiment  we did plp to order     so 
we got    features as we also added energy to the plp vector  we then computed delta  and
delta delta to get    features just like with mfcc 
to conclude  for each sound sample  our features are for a list of windows  in each
window  we have    features from mfcc and    other features from plp  the number of
windows is different for each sound sample as they are of different lengths  but the window size
is fixed 
classifier  support vector machine  svm 
our classifier is based on windows  instead of the whole sound sample  as we have
features for each window  the classifier tries to determine which acccent a window belongs to 
this is done by combining all the windows in the training data and labeling each window by
accent of the sound sample file the window belongs to  then we simply trained svm on these
windows  to predict the accent of a sound sample  we use svm to label each window in that
sample and then we pick the most frequent label as the label for that sample  this approach
sounds very simple  but it ended up not working  we tried this on just    mfcc features  without
energy  deltas  plp  and it took forever to train  one reason for that is that we have about     
windows per sound sample and about     samples per accent  so  there is a lot of data 
we tried to make svm run in a reasonable amount of time by throwing away most of the
windows for each sound sample randomly before we trained the svm  for    mfcc features
with    windows per each sound sample  svm took an hour to train  the overall accuracy was
        doing the same thing on    plp features  the accuracy was        
the result we got is better than what would be expected from randomly labeling the
test set  however  randomly picking    from      windows to use for training is not a reliable
method  we wanted to find a reliable method that could run in a reasonable time 
classifier  gaussian mixture models  gmms 
it turns out that we were able to use gmms to do this efficiently  we did this by having  
gmms   one for each accent  we trained the gmm of each accent class by using the features
extracted from the windows of the samples from that accent class as our training data  we
predicted the accent class of the testing data by calculating the probability that the testing data
belongs to each accent class and then outputting the accent class that gave the highest
probability  to calculate the probability of testing data belonging to an accent  we did feature

fiextraction on the testing data first to get the same windows and the same features as before 
we used the trained gmm for a particular accent to calculate the probability of each window
belonging to that accent  then we multiplied  or summed the logs of  the probabilities of the
windows together to get the probability of each sample having the accent  in this way  we found
which accent is most probable for that sample 
we trained gmms with the em algorithm  we initialized em by using the mean from kmeans and using a covariance matrix outputted by k means   this seemed to get better results
than initializing the covariance matrix to be the identity  one big problem we encountered was
that our covariance matrix became singular often 
because we were working with many features  we ran into a lot of problems with the
covariance matrices becoming singular  or rather close enough to singular that floating point
errors would occur  this comes about because if many entries in the covariance matrix are
small  the determinant becomes very small as it is the sum of the products of entries of the
covariance matrix  we mainly dealt with this problem in two ways  first  we switched from using
full covariance matrices to only using diagonal covariance matrices  which were simpler  it turns
out that we get better results by using the diagonal matrices as our covariance matrix  the other
method we tried is to apply variance limiting on them  variance limiting is when we do not allow
any of the variances to be smaller than a certain value in magnitude  i e  every entry of the
diagonal of the covariance matrix must be larger than a certain value in magnitude  the second
way we dealt with the problem is linearly scaling up the numbers that we were using as the
features  this helped because linearly scaling should not change the results that the gmm gets
us  but it should increase all of the variances  making it less likely that the covariance matrix will
be singular 
using both mfcc and plp
we tried to model on just mfccs  just plp  and also tried combining the two  when we
combined plp and mfccs  we used   gmms for each accent   one for mfccs and one for
plp  we dealt with these two separately and we combined the result by multiplying the
probabilities we got from each of them 
another way we could have combined both features was that we could have just
concatenated the vectors to get    features for each window  however  that seemed to be too
many features for us  therefore  we did feature selection using forward search on the   
features to select just a few features to use  we picked forward search instead of other feature
selection techniques because it is the fastest one 
configuring our learning model
we tried to find the best number of gaussians to use for the gmms for each accent class
by training on the    mfcc features with different numbers of gaussians  the results are shown
in the following diagram  we found that it was best to use    gaussians for each accent class 
however  the effect that the number of gaussians on accuracy is very small 

fifor feature selection  we used forward search to select features from the    plp and
mfcc features  the classifier that we used was one gmm with   gaussians for each accent
class  we did not use the number of gaussians we got from the testing above because we
tried these two things in parallel  we picked   because we wanted the number of gaussians
to be small to make feature selection run faster  we know that the impact of the number of
gaussians is very small  so this should not have posed much of a problem for the accuracy of
our feature selection  in any case  the feature selection process took a long time to run  the
results are shown in the following graph  we can see that starting from around six features 
the change in the accuracy is no longer strictly increasing  and it also becomes negligible  we
found it surprising that more features would not help us  we still believe that six features is not
enough  however  our experiment says otherwise  we do not think we overfitted the training
data because we still got similar results  that is accuracy of     using    mfcc features just like
for testing the number of gaussians   when we used the same training and test data  therefore 
the problem probably lies in the features that we used 

as a final result  the best accuracy we got was        using   features  the forward
selection picked   of the   features from mfccs 
future work
the most problematic part of our research is that our features do not model different

fikinds of accents well  mfccs and plp are suited for general audio  not specific to voice
or accent  for the future  we would like to get more specific features  we considered trying
prosodic features  as many papers suggested this  however  we need to be able to recognize
words in the sound sample to be able to apply prosodic features  this is because we would
have to label words in the dataset  which takes a lot of time if it is done manually  or  we
would have to do speech recognition before running our classifier to determine where words
are  which seems inappropriate for our case given that speech recognition does not run well
on accented speech  and one of the purposes of accent classification is to aid in speech
recognition  therefore  it was not feasible for us given the limited time we had  however  it might
be worth considering in the future 
one other thing that we failed to take into account is pauses  there are a lot of windows
which are just pauses  these pauses are the same for every accent  so  the features from these
windows do not help in classification 
we could also try decreasing variation on our dataset by only using male or female voice
or dividing the dataset into male and female voices given that pitch is often very different in male
and female speech 
references
dan jurafsky 
hong tang and ali a  ghorbani  accent classication using support vector machine and
hidden markov model  university of new brunswick
ghinwa choueiter  geoffrey zweig  and patrick nguyen  an empirical study of automatic
accent classification
karsten kumpf and robin w  king automatic accent classification of foreign accented
australian english speech  speech technology research group  the university of sydney
nsw       australia
scott novich  phil repicky  andandrea trevino  accent classification using neural networks 
rice university   http   cnx org content col           
john h l  hansen and levent m  arslan  foreign accent classification using source
generator based prosodic features  robust speech processing laboratory  duke university 
douglas a  reynolds  and richard c  rose  robust text independent speaker identification
using gaussian mixture speaker models  ieee 
muchael j  carey  eluned s  parris  harvey lloyd thomas  and stephen bennet  robust
prosodic features for speaker identification  ensigma ltd 
andre g  adami  radu mihaescu  douglas a  reynolds  and john j  godfrey  modelling
prosodic dynamics for speaker recignition 
hynek hermansky  perceptual linear predictive  plp  analysis of speech  speech technology
laboratory  division of panasonic technologies inc   santa babara ca

fi