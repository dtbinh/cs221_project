measuring occlusion invariances in deep networks
kyunghee kim
oracle corporation
kakooyang gmail com

abstract
in computer vision  finding robust features invariant to many artifacts such as translation 
reflection  occlusion  and view angles is important to improve the performance of the
classification  goodfellow et al     measured invariances to translation and rotation of the
features learned from stacked autoencoder networks    and convolutional deep belief
networks     this project measures invariances to the occlusions of those deep networks
with two object categories  car image data and computer image data from the image
dataset labelme    were used to train and test deep autoencoder networks    and
convolutional deep belief networks     the result with car image data show that in the
test using deep autoencoder networks the occlusion invariance score achieved better
score in layer   than in layer    however  layer   was measured with the lowest score
among three layers  convolutional deep belief networks show higher invariance score in
layer   than in layer    the test with computer image data learned more invariant features
in layer   than in layer    even if the most of the result does not comply with the general
characteristics of deep networks that deeper layers learn more invariant features than
shallower layers  this result suggests that we need to further examine the properties of
deep networks more specifically with regard to translation  rotation  occlusion  view
angles  illumination and so on 

   introduction
image data in computer vision might have various artifacts such as illumination  rotation  translation 
occlusion  etc  human eyes can recognize objects even if the appearance of the object is affected by those
variances  to achieve the high performance of computer vision in object recognition problem  the features
extracted from the objects also have to be invariant to the transformation or deformation so that the
classifier can recognize objects despite the presence of variances in the input 
deep architectures consist of several layers and the hierarchical architectures in deep learning make it
possible for upper layers to learn more invariant features than lower layers           goodfellow et al 
presented formulas to calculate the invariance score in each layer in deep architectures and showed that
stacked autoencoder networks and convolutional deep belief networks learn more robust feature
representations when rotation was applied to the input     this project presents a formula to measure
invariances in deep networks when the object in the input image is occluded and measures the invariance
scores in deep autoencoder networks and convolutional deep belief networks 

   related work
hinton et al  made the deep belief networks with a restricted boltzmann machine in each layer     they
showed that lower layers in the network learn low level features and deeper layers in the networks learn
more abstract features  bengio et al  presented a deep network with an autoencoder neural network in each

filayer     lee at al  built a convolutional deep belief network using probabilistic max pooling and showed
that their architecture learns useful object features    
goodfellow et al  measured invariances in deep networks on synthetic images and natural video data    
they tested two transformations i e   translation and rotation  while invariance scores to the rotation were
measured higher in deeper layers in both stacked autoencoder networks and convolutional deep networks 
invariance score to the translation in stacked autoencoder networks was the lowest in the deepest layer 

   occlusion and data collection
labelme    dataset contains good examples of occlusions of cars  some of these examples are illustrated in
figure   

figure    this figure shows three examples found in labelme    dataset   a  shows one object car  with its
full body   b  shows one object car  occluded by a bar in front of it   c  shows two cars  one is occluded by
a red car in front of it and the other red car  is occluded since the original picture did not take its full body 
figure   shows three examples of training dataset and three examples of test dataset  figure    a   c  are
training examples and figure    d   f  are test examples  each image was cropped from the images in
labelme    dataset  which contain a car as a object in them and each cropped image was rescaled to    by
   pixels and also grey scaled to a black and white image  the training data and test data for computer are
attached in appendix 

 a 

 b 
 c 
 d 
 e 
figure    training data and test data of cars used in this experiment

 f 

   deep network architectures
    deep autoencoder networks
a  deep  belief  network  with  a  restricted  boltzmann  machine  in  each  layer  introduced  by  hinton  et  
al      was  used  for  this  experiment     in  each  layer   the  activation  of  each unit  hi  i          m  to an input 
  is computed as

where h x  is the unit activations 
is a weight matrix  is a bias in hidden layer and sigmoid is a
sigmoid function  the networks in this experiment consists of three layers and were trained in a greedy
layer wise way as presented in      the first layers receives    by    patch of an image as an input  the
training set size of car images is     and test set size of car images is      the training set size of computer
images is     and the test set size of computer images is      the batch size is    in both cases 

fi    convolutional deep belief network
cdbn    consists of two layers in this experiment  each layer contains convolutional units and maxpooling units  the receptive field size of each convolutional unit is    by    pixels  each max pooling unit
has the receptive field size of    by    pixels in the first layer and    by    pixels in the second layer  the
size of the receptive field in each max pooling unit is determined by implementing max like operation over
four neighboring convolution units  the training set size of car images is     and test set size of car images
is      the training set size of computer images is     and the test set size of computer images is     

   occlusion invariance measure
as a feature detector  a hidden unit has to respond strongly if there exists a feature that it represents in the
input  and if the feature it represents does not exist  the hidden unit must not respond to the input  to be
invariant to the input  the hidden unit should respond to the input even if there has been applied some
transformations or deformations to the input such as translations  reflections  rotations  and occlusions 
therefore  if the hidden unit is invariant to occlusions it has to respond to the input strongly even if some
part of the object is hidden or not seen for some reason  for example  an object might be hidden by some
other objects or the original image was taken with only the part of the object as shown in figure   already
in part   
to ensure that the units are selective  global firing rate  g i   is defined to be the proportion of the units
that respond to the features present in the inputs  in this experiment the global firing rate was set to be
      the global firing rate is also used to compute the threshold  ti  which determines whether the unit is
 on  or  off  
to measure the robustness of the units  local firing rate  l i   is defined as the proportion of the units that
fire with the input  the unit is determined to be  on  when it is over the threshold  and  off   otherwise  to
express it with an indicator function  it is defined as fi x      hi x    ti   o x  is a set of stimuli that
deforms the input with occlusion  z  similarly as defined in      is a set of inputs that near maximally
activate the units  finally  the local firing rate is defined as

invariance score for a unit hi x  is defined local firing rate over global firing rate i e   s i    l i  g i  

   results
     deep autoencoder networks
figure   shows the occlusion invariance test result using deep autoencoder networks with car image data 
the second layer learned more invariant features than the first layer  however  the third layer learned the
least invariant features 
     

 

     

 

     

 

     

 

 

layer 

layer 

layer 

 a  average invariance score of all units

 

layer 

layer 

layer 

 b  average invariance score of top     units

figure    occlusion invariance test result in deep autoencoder networks with car image data  for both all
units and top     units  p         

fifigure   shows the occlusion invariance score in deep autoencoder networks with computer image data 
the first layer learned the most invariant features and the second layer learned the least invariant features 
     

  

     

  

    

  

    

 

 

layer 

layer 

 

layer 

 a  average invariance score of all units

layer 

layer 

layer 

 b  average invariance score of top     units

figure    occlusion invariance test result in deep autoencoder networks with computer image data  for
both all units and top     units  p         
    convolutional deep belief networks
figure   shows the occlusion invariance test result in cdbn with car image data in figure    a  and with
computer image data in figure    b   in the test with computer data the second layer learned more invariant
features than the first layer  whereas in the test with car data the first layer learned more invariant features 
 

     

 

     

 

     

 

     

     

 
layer 

layer 

 a  invariance score of cdbn with car data

layer 

layer 

 b  invariance score of cdbn with computer data

figure    occlusion invariance test result in cdbn with car image data  a  and computer image data  b 

   discussion and conclusion
this experiment measured the invariance scores in deep autoencoder networks and convolutional deep
belief networks when occlusion was present in the image data  two object categories were tested  i e   car
images and computer images  as a result  only the test using cdbn with computer images showed the
result that agrees with the property of the deep networks generally thought that deeper layers learn more
invariant features then shallower layers  other tests showed the result different from this generally believed
property of deep networks as illustrated in part    results above  meanwhile  in the test with stacked
autoencoder in goodfellow et al    s work  the invariance score to the translation decreased as the layer
went higher  which also did not comply with the characteristics of deep networks  however  in their work 
the invariance test with rotation and the invariance test using cdbn with translation showed the result that
follows the general characteristics of deep networks  to summarize all these results in previous work and in
this experiment  we need to further examine properties of deep networks with respect to various
transformations and deformations in different architectures  the suit of formulas suggested in this work and
previous work presented by goodfellow et al  provides a way to calculate to the invariances in deep
networks  lastly  to mention the weakness of the dataset in this experiment  the data includes not only the

fieffect of occlusions but also other artifacts such as illuminations or different view angles  for example  to
compare the two images in figure    a  and  b   their view angles are slightly different from the perspective
of the viewer who took these photos  therefore  as a future work  we need to independently test each
artifact and we need to test more diverse object categories whereas this work tested two object categories 

references
    i  j  goodfellow  q  v  le  a  m  saxe  h  lee  a  y  ng  measuring invariances in deep networks 
nips       
    y  bengio  p  lamblin  d  popovici  and h  larochelle  greedy layer wise training of deep networks 
nips       
    h  lee  r  grosse  r  ranganath  and a  y  ng  convolutional deep belief networks for scalable
unsupervised learning of hierarchical represenations  icml       
    b c  russell  a  torralba  k  p  murphy  and w  t  freeman  labelme  a database and web based tool
for image annotation  mit ai lab memo       
    g  e  hinton  s  osindero  and y  w  teh  a fast learning algorithm for deep belief networks for
scalable unsupervised learning of hierarchical representations  icml       
    h  larochelle  d  erhan  a  courville  j  bergstra  and y  bengio  an empirical evaluation of deep
architectures on problems with many factors of variation  icml       

appendix 
figure   shows three examples of training dataset and three examples of test dataset  figure    a   c  are
training examples and figure    d   f  are test examples 

 a 

 b 
 c 
 d 
 e 
figure    training data and test data of computers used in this experiment

 f 

fi