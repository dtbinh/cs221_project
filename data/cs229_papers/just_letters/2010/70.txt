subspace clustering with applications to dynamical vision
 cs     final project 
adel javanmard

mahdi soltanolkotabi

december         
  

introduction

data that arises from engineering applications often contains some type of low dimensional structure that enables
intelligent representation and processing  this leads to a
very challenging problem  discovering compact representations of high dimensional data  a very common approach
to address this problem is modeling data as a mixture of
multiple linear  or affine  subspaces  given a set of data
points drawn from a union of subspaces  subspace clustering
refers to the problem of finding the number of subspaces 
their dimension and the segmentation of data  in this sense 
subspace clustering can be thought of as a generalization
of the principal component analysis  pca  method  in
pca  the underlying assumption is that the data is drawn
from a single subspace while in subspace segmentation  data
is assumed to be concentrated around multiple low rank subspaces 
a number of methods have been developed to address the
subspace clustering problem  including algebraic methods 
spectral clustering based methods and statistical methods 
on the heels of compressed sensing and low rank matrix recovery           a new class of algorithms have very recently
emerged  these algorithms try to represent each data point 
in a union of subspaces  as a linear combination of all other
points  by enforcing a low dimensional structure on this
representation  e g  l  norm for sparsity or nuclear norm
for low rankness   an affinity matrix is built which is subsequently used for subspace segmentation  this new approach
resolves the exponential complexity issues of some of the
previous methods and is more robust to noisy data  motivated by this approach  we propose a new algorithm  called
nuclear    for the segmentation problem  moreover  we
present an iterative scheme as an alternative method for
handling large scale problems 
subspace segmentation has many applications in computer vision  image processing  and system theory  in this
project we will mainly focus on motion segmentation  and
use the nuclear   algorithm for this problem  based on the
simulation results on the hopkins    motion database  the
performance of nuclear   is quite competitive 

  

subspace separation

    noiseless scenario
let x be a matrix in rnn with columns drawn from a
union of k subspaces  si  ki     of unknown dimensions  embedded in a larger space with dimension n  the data has an
ambient dimension n    more specifically x    x         xk  p  

figure    a set of points concentrated around the union
of subspaces s    s    and s   

where xi  rnni is the set of ni samples drawn from
the ith
p subspace si and p is a permutation marix  also
n   ki   ni denotes the total number of samples  assuming that the subspaces are of low dimension and independent
 
  the goal is to segment all data points into their respective
subspaces  see fig    for an illustration  
there are two algorithms that use low dimensional representation for subspace clustering  both of these algorithms
have provable recovery gaurantees when the subspaces are
independent         we will briefly describe them in the next
two subsections  based on these two algorithms we will propose a new algorithm  nuclear     which is subsequently
described 

     

low rank representation  lrr     

in this setting the assumption is that each data vector
can be represented by a linear combination of a small number of vectors in a dictionary a    a    a         am    i e  
x   az  where z  rnn is the coefficient matrix  a
natural choice for the dictionary is a   x  i e  one tries
to write each point as a linear combination of all the data
points  this is a reasonable assumption if the sample points
are sufficiently dense in their respective subspaces  in this
sense each xi is self expressive   xi   xi zi for some zi 
rni ni   hence  the permuted block diagonal matrix z  
diag z    z         zk  p satisfies x   xz 
pk
 
the subspaces are independent if and only if
i   si  
ki   si

fithe idea of lrr method is to enforce a low rank structure on z as a surrogate for block diagaonal ness of z by
solving the problem 
min

rank z 

s t  

x   az

z

     

after finding z one can perform subspace segmentation by
spectral clustering algorithms such as normalized cut      it
is well known that       is np hard  a common relaxation
to rank minimization problem is to replace the rank function
with the nuclear norm  resulting in the following problem 
min

  z  

s t  

x   az

z

     

it has been shown in     that this problem effectively recovers the block diagonal structure when the subspaces are
independent  the details of lrr is shown in algorithm   
algorithm    subspace segmentation by lrr
input  data matrix x  number of subspaces k 
   solve the optimization problem to get z   
   construct an undirected graph by using  z        z   t  
as the affinity matrix of the graph 
   use normalized cut to segment the vertices of the
graph into k clusters 

      sparse reconstruction method  sr     
sparse reconstruction method  sr  enforces sparsity on
the coefficient matrix z by minimizing its    norm  formally  it solves the problem 
min

  z   

s t  

x   az 
 zii       

z

     

note that without the constraint  zii        the method is
prone to return the trivial solution z   i  after finding
the affinity matrix z  similar to lrr  the algorithm uses
spectral clustering algorithms for subspace clustering  it has
been shown in     that this problem also recovers the block
diagonal structure when the subspaces are independent 

      proposed method  nuclear     algorithm 
the lrr algorithm is pretty robust to  gross  noise  however  it shows poor performance if the independence assumption about the subspaces is not valid  on the other hand 
sr looks for the sparsest coefficient matrix z and is more
likely to work when the subspaces are not independent 
motivated by this observation  we propose the nuclear  
algorithm as a unifying approach that incorporates lrr and
sr methods  this algorithm obtains the coefficient matrix
z by solving
min

kzk    kzk

s t  

x   az

z

     

note that by setting       the nuclear   algorithm reduces to sr  also  in the limit     nuclear   reduces
to lrr  therefore  it can be thought of a generalization
of both lrr and sr methods and is likely to inherit the
benefits of each of them simultaneously  some of the useful properties of nuclear   norm and its applications proof

guarantees in data modeling in general  and more specifically subspace extraction  is discussed in a paper by the
second author      

    noisy scenario
according to      the main challenge of subspace segmentation is to handle noisy in data  i e   to handle the data
that may not strictly follow subspace structures  the authors in     claim that the lrr method has a better performance in the presence of noise compared with previous
algorithms  as the experimental reports in     demonstrate 
the sr based segmentation method are quite competitive in
real world applications  however  since sr finds the sparsest representation of each data vector individually  it is not
guaranteed to capture the global structure of the data  this
drawback can adversely affect the performance when the
data is grossly corrupted 
assume that a percentage of the data vectors are grossly
corrupted and the others are contaminated by small noise 
therefore  x   x    e  where x is the observed data and e
is the noisy part  in order to recover the low rank matrix z
from the observed data  lrr solves the following problem 
min

  z       e     

s t  

x   az   e

z e

     

where          denotes the       norm   
similarly  the nuclear   algorithm is modified in noisy
scenarios by introducing a regularization term as follows 
min

  z        z       e     

s t  

x   az   e

z e

     

      iterative method for solving large scale problems using nuclear  
the problem       is a convex optimization problem and
can be solved using semidefinite programming solvers  these
solvers are based on interior point methods  and are problematic when the size of the matrix is large because they
need to solve huge systems of linear equations to compute
the newton direction  in fact  they can only handle n  n
matrices with n       therefore  we need to find another
way to solve the above problem which is scalable to large
matrices  to this end  we convert       to the following
equivalent problem 
min

  j         j          e     

s t  

x   az   e 
z   j 
j    j 

z e j   j 

     

using the augmented lagrange multiplier  alm  method 
an iterative scheme can be proposed to solve this problem 
consider the following augmented lagrange function 

 tr



y t  x

  j         j          e     





 az  e    tr y t  z  j      tr y t  j   j   


     x  az  e   f     z  j     f     j   j     f   
 

for matrix a    a    a      p  an    where ai denotes the ith
column    a      is given by n
i     ai    
 

fithe above problem is unconstrained and can be minimized
with respect to j    j    z  and e  respectively  by fixing the
other variables  this results in the following update rules 

where o is the hadamard product and m   is a mask matrix
that has   on the blocks and zeros everywhere else 

algorithm    solving problem       by alm
input  data matrix x  regularization parameters    
 i  fix the others and update j  by



 initialize  z   j    j       e      y    y    y      
j    arg min  j       tr y t  z  j      tr y t  j   j   
         u                 and         
while
  x  az  e     or   z  j     

    z  j     f
or
  j
   j      do
 
   fix the others and update j  by

  arg min  j         j   z    y   y       f
j    d    z    y   y      
 
 
 
  arg min   j         j   z    y   y       f  
   fix the others and update j  by

 
j    s   j    y     
 ii  fix the others and update j  by
   fix the others and update z by

 
j    arg min   j        tr y t  j   j        j   j     f
z    i   at a    at  x  e    j     at y   y     
 

  arg min   j          j   j    y      f
   fix the others and update e 
 
define q    x  az   y    and let qi be the ith column
 

  arg min   j          j   j    y      f  
of q  then  the ith column of e  denoted by ei updates

 
ei   max   qi              qqii   
 iii  fix the others and update z by
   update the multipliers
z    i   at a    at  x  e    j     at y   y      
y    y     x  az  e  
y    y     z  j    
 iv  fix the others and update e by
y    y     j   j    


   update  by    min   u   
e   arg min   e        tr y t  x  az  e 
end while 

    x  az  e   f
 
    experiment on noiseless data
 

  arg min   e          e   x  az   y       f  
we construct   independent subspaces  si   i   whose ba
 
sis  ui   i   are       matrices with entries i i d  n        
thus each subspace has rank   and the ambient dimension is
here       is a penalty parameter  note that although the
    we construct a      data matrix x    x         x    by
steps  i    ii   and  iv  are convex problems  they all have
sampling    data vectors from each subspace by xi   ui ci  
simple closed form solutions 
   i    with ci being a       i i d  n        matrix 
let s   r  r denote the shrinkage operator s  x   
the affinity matrix z obtained by performing noiseless
sgn x   max  x         and extend it to matrices by apsr  lrr  and nuclear   on this data is shown in fig    
plying it to each element  furthermore  let d  x  denote
as can be seen the off block diagonal terms are very small 
the singular value thresholding operator for matrices given
by d  x    u s   v    where x   u v  is any singular
value decomposition  it is not hard to see that the solutions
to step  i  and  ii  are respectively given by
j    d    z    y   y      
j    s   j    y     
the solution to step  iv  is given by virtue of lemma    
in      define q    xaz y    and let qi be the ith column
of q  then  the ith column of the solution e  denoted by
ei is given by
ei   max   qi     

qi

 
    

  qi    

 

 

  

  

  

  

  

  

  

  

  

  

  

  

  

  
  

  

  

  
 

 a 

  

  

  

  

  

  

  

  

 

  

 b 

recall         precision       

  

  

  

  

  

  

  

  

  

recall         precision       

 
  
  
  

the inexact alm method is outlined in algorithm   

  
  
  

  

experiment on synthesized data

in this section we present the result of performing nuclear  lrr and sr on synthesized data  we perform experiments in both the noisy and noiseless case  our criteria for
evaluating the performance are 
km   oz  k 
precision  
 
kz  k 

km   oz  k 
recall  
km   k 

  
  
  
 

 c 

  

  

  

  

  

  

  

  

  

recall         precision      

figure    comparison of the affinity matrices produced
by  a  sr   b  lrr   c  nuclear   on noiseless synthesized data 

fithe value of recall and precision using each algorithm is
presented in the caption of each subfigure  notice that all
algorithms have precision         this is in line with theory because with high probability the subspaces ui will be
independent in this case and thus we expect all algorithms
to perform well 

 

 

  

  

  

  

  

  

  

  

  

  

  

  

  

  
  

  

  

  
 

    experiment on noisy data

  

  

  

  

  

  

  

  

 

  

  

 a  sr

we sample     data vectors from   subspaces constructed
in a similar way as in example        this time  a percentage
of the data are grossly corrupted by a multiplicative gaussian noise n          and the rest are contaminated by small
additive gaussian noise n            we use the modified versions of lrr and nuclear   for noisy case to segment data 
the parameter  in lrr was set to      the parameters
 and  in nuclear   were set to     and      respectively 
the results are summarized in table    these results verify
that nuclear   is more robust to gross noise 

  

  

  

  

  

  

  

  

 b  lrr
 
  
  
  
  
  
  
  
  
  
 

  

  

  

  

  

  

  

  

  

 c  nuclear  
figure    comparison of the affinity matrices produced

table    experiment results on noisy synthesized data
lrr
nuclear  
percentage of large noise

recall

precision

recall

precision

    
    
    

   
   
   

    
    
    

    
    
    

       
       
       

    comparison of robustness to the independence assumption
we construct   independent subspaces  si   i   whose basis  ui   i   are       matrices with entries i i d  n        
thus each subspace has rank   and the ambient dimension is
    we construct a      data matrix x    x         x    by
sampling    data vectors from each subspace by xi   ui ci  
   i    with ci being a       i i d  n        matrix  the
affinity matrix z obtained by performing noiseless sr  lrr 
and nuclear   on this data is shown in fig     note that the
subspaces are not independent in this case    subspaces of
dimension   in r      as can be seen in fig     lrr has a lot
of nonzero coefficients in the off block diagonal part  were
as the non zero coefficients in z using sr and nuclear  
is almost concentrated in the block diagonals  verifying our
previous intuition that sr and nuclear   should perform
better in this setting 

by sr  lrr  nuclear   without the independence assumption

    formulation of motion segmentation as
subspace separation
consider a feature point y  in   d space  y  r     and
its projection on the   d image plane  x  r     under the
affine camera model  these two quantities are related by a
linear transformation 
 
y
     
x   af
 
where af  r   is known as
more specifically as described in
 
     
af   k       
     

the affine motion matrix 
    
 

  
r t
   t
 
 
 

here k  r   is the camera calibration matrix and  r  t  
se    is the relative orientation of the image plane with respect to the world coordinates  suppose we have access to
the trajectories of p feature points of a rigid object  obtained from f   d image frames taken by a moving camera
 denoted by  yf p  fp     p
     f   where yf p is the projection of the
p th point onto the image plane in the f  th frame    the
linear constrains of       can be lumped together in the form 
x   af y

  

motion segmentation

one of the main problems in dynamic vision is the analysis of dynamic scenes  in these scenes  in addition to the
motion of the camera  there are multiple moving object in
the scene  usually independent of each other   thus in order to analyze dynamic video scenes an initial step is motion
segmentation  i e  given multiple image frames of a dynamic
scene taken by a  possibly moving  camera  the goal is to
cluster the trajectory of feature points   on the moving objects  according to the different motions these trajectories
belong to  in literature many different camera models have
been proposed  in this project we will focus on the affine
camera model 

where
 
x   x      x p
 
  
    
  
x        
 
 
   
xf   xf      xf p
   
a 


y    yp
     
af            y    
  
 
af
 

where af is the affine motion matrix at frame f  these
linear constraints relates the   d coordinates to the tracked
feature points  notice that
rank x    rank af y    min rank af    rank y      

fithis latter fact shows that the trajectories of feature points
from a single rigid motion will all lie in a linear subspace
of r f of dimension at most four  in case there are k moving objects the trajectory of the feature points will lie in
a union of k linear subspaces in r f   thus the problem
of motion segmentation  separating feature points based on
their movement  becomes equivalent to the separation of
data drawn from multiple subspaces  based on the subspace
they belong to 

  

table   

classification error with algorithm specific prepost processing
algorithm
gpca
lsa
sr
lrr
nuclear  

mean

      

     

     

     

     

to give a visual verification of our algorithm fig    shows
the result of applying nuclear   to one video sequences in
the hopkins     data base 

experiments on hopkins    

in this section we test nuclear   on real motion segmentation tasks  some previous state of the art methods are
also included for comparison 
we evaluate nuclear     lrr and sr on the hopkins    
motion database      the database consists of     sequences
of two and three motions which can be divided into three
main categories  checkerboard  traffic  and articulated sequences  the trajectories are extracted automatically with
a tracker  and outliers are manually removed  therefore  the
trajectories are corrupted by noise  but do not have missing
entries or outliers 
we consider two settings on this database  the first one is
to compare all algorithms under the same circumstances  i e 
all algorithms use the raw data without any special preprocessing and we use the same spectral partitioning technique
on all of them  in the second setting  different algorithms use
specific preprocessing and postprocessing techniques to enhance their performance  table   and   shows that nuclear  is competitive in both settings  another interesting result
is that the new class of algorithms   sr  lrr  nuclear    
outperform more classical approaches in motion segmentation 
the basic algorithm of nuclear   was able to achieve an
error rate of        this performance can be enhanced by
some additional pre post processing techniques as follows
 these are the same tricks used in the lrr method  
first  we notice that the data base has low noise level 
therefore to avoid overfitting  we randomly chose     of the
entries and corrupt them by adding small gaussian noise 
second  since the affinity matrix z is asymmetric  we convert
it into a positive semi definite  psd  matrix z  by solving

figure    visual performance verification of nuclear  
algorithm on an example frame from hopkins     

   references
    e  candes  j  romberg  and t  tao  stable signal recovery

   
   

   
   

min kz  k   kek 
z 

s t 

z   z    e 
z    

   

with  set to be      third  inspired by       we decompose
 
z  into z    qqt and define l    qqt     where q is q

with normalized rows  fourth  we use l           as the
affinity matrix for spectral clustering  finally  in the interest
of fair comparison since there is a degenerated sequence in
hopkins     and previous results disregard the degenerated
sequence so do we 
table   

   

   
   

classification error on raw data

algorithm

gpca

lsa

mean

na

     

ransac

     

algorithm

sr

lrr

nuclear  

mean

     

     

     

    

    

from incomplete and inaccurate measurements 
communications on pure and applied mathematics 
                     
m  fazel  matrix rank minimization with applications 
phd thesis     
e  j  candes and b  recht  exact matrix completion via
convex optimization  foundations of computational
mathematics  vol     no     pp               
g  liu et al   robust recovery of subspaces structures by
low rank representation  arxiv          v 
s  rao et al   motion segmentation in the presence of
outlying  incomplete  or corrupted trajectories  ieee
trans  on pattern analysis and machine intelligence  vol 
    no      pp                  
e  elhamifar et al   sparse subspace clustering  in ieee
conference on computer vision and pattern recognition 
vol           pp            
j  shi et al   normalized cuts and image segmentation 
ieee trans  on pattern analysis and machine intelligence 
pp                
j  yang et al   a fast algorithm for edge preserving
variational multichannel image restoration  siam journal
on imaging sciences  vol     no     pp                
r  tron and r  vidal  a benchmark for the comparison of
  d motion segmentation algorithms  in ieee conference
on computer vision and pattern recognition        pp 
    
f  lauer and o  christoph schn spectral clustering of
linear subspaces for motion segmentation  in ieee
international conference on computer vision       
m  soltanolkotabi  the power of nuclear   relaxations in
modeling data  stanford university technical report 
under preperation 

fi