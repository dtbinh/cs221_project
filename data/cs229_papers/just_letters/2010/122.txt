 

online learning of control policies for dynamical
systems based on input output data recorded on
the fly
roberto a  bunge
abstracta computationally ecient learning algorithm
is presented which learns an adequate control policy to drive
a dynamical system from an intial state to a nal desired
state  it does this by taking actions  storing the reward
received  and using the bank of recorded data to estimate
the best action in future  possibly unvisited  states  a natural threshold separating positive and negative actions is
set  indicating if random exploration of the action space or
if exploitation of the accumulated data should be undertaken  thus ensuring that the goal is always approached 
the algorithm is limited to problems where maximizing the
reward at each time step leads to a good long term performance  the designer is required to set a number of parameters which depend on the scale of the dierent variables
involved  as well as providing a reasonable path planning
function  results on three control problems are presented
showing that the algorithm reaches near to optimal control
policies  and that it can handle undamped inertial transient
dynamics  as well as cross coupling of control inputs 

i  introduction
in this project we explore the problem of learning an
adequate control policy so as to drive a dynamical system
from an initial state to a nal desired state  using only input output data recorded on line  and without estimating
an explicit model for the system  be it a deterministic linear representation or a transition probability model  the
motivation for this goal has dierent sources  in the rst
place  estimating a linear representation assumes that the
system  to some extent  has close to linear dynamics  this
may not be the case in many problems  and if it were  we
would still rely heavily on the designers knowledge about
specic aspects of the system to be eective in generating
a good control algorithm  secondly  we want to generate a
framework that is based on generic concepts  such that the
architecture can be scaled and expanded to dierent cases 
without changing the general setup too much  thirdly 
trying to estimate a transition probability model  requires
that we be operating in a specic region of the state space 
so that the system is going over the same states quite often  since we are envisioning applying this algorithm to
navigation problems  rather than strictly regulation problems  by denition we expect to traverse the state space
making this approach ill conditioned 
in addition  this work is an attempt to resemble certain
aspects of how humans learn to control systems on the
y  this is  by testing controls  memorizing good ones
and using these to infer good controls for new situations 
a very important question arises about human control 
is there an explicit function minimization or function approximation instance in the human brain that is central to
our eectiveness to drive systems  our intuition is that

no  humans dont rely on a mathematical representation 
but instead a very malleable and gapless system of guessing and checking  mixing and matching  and an ability to
recall from memory the previously generated mappings between situations  actions and rewards  we are going to try
to follow these overarching concepts in the present work 
striving to use mathematical tools that resemble these features 
one of the restrictions for the approach here presented 
is that it is limited to problems where the optimal route
to the goal is that of the shortest route  since the agent
is going to be designed so as to maximize a given reward
at every time step  then  problems where an initial period
of low rewards is required in order to ever reach the goal 
would not be eectively solved by this approach  still  the
subset of problems for which it could work is representative
of many interesting control problems 
the proposed solution has been applied to three dierent control and navigation problems  positioning a massspring damper in  d controlling the force applied  positioning a free mass in  d controlling two orthogonal forces 
driving a  d tank from a given position to a nal one 
a  an overview of the architecture
the agent will be fedback a scalar reward after every
action it takes from every state it is in  this state actionreward data will be stored in a bank of previous experiences  at each new state  the agent can try to estimate
the reward for all possible actions from the data it has
previously collected  if the new state isnt too dierent
from the previous states it has been in  then it might be
able to infer quite well the reward for all the actions  doing this estimate  it can pick the one that  surpassing a
given threshold  has the highest estimated reward  if there
is none that surpass the goodness threshold  the agent
decides to pick an action randomly with the intention of
exploring the state action reward space  thus expanding
its bank of knowledge about the problem  thus  we have
instances of  reward estimation  reward evaluation  taking
an action  receiving a true reward  and nally storing this
information in the data bank for future use  below is a
diagram which shows this 
in the coming sections we will discuss with more detail
our proposed reward function  our reward estimation strategy and how we will deal with the expanding size of the
data bank 

fi 

when we are exactly on desirde nal state  lets suppose 
we somehow reached the nal desired state  then the rst
term is zero  and the optimal action would be to take no
action  or take the neutral action   so as to remain in the
same state  but this makes the reward be exactly zero  so
given the threshold chosen  this optimal action will never
be taken  on the ip side  if we allow zero to be part of the
acceptable actions  there is a high chance of ending up in
a situation where the neutral action is taken forever  since
the neutral action by denition gives a zero reward  and
the bank will never be expanded so as to nd an action
with a better estimated reward  this could be dealt with
in dierent ways  one is to include a non zero probability of taken a random action regardless of the estimated
rewards  thus keeping the door open for new information
coming in  which will hopefully indicate towards an action
that is better than the neutral one  this option is not
favored because it adds an instance of unnecessary randomness  and the agent may still be slow in getting out of
this singularity  another option is to redene the reward
function as 
fig     overview of algorithm

 
r st   at    

ii  designing the reward function
as already mentioned  the agent has to be provided with
a reward function  which when maximized at each time
step gives a good long term performance  to fulll this
objective  the reward function has a path planning component embedded in it  so  for each state  we have a path
planning function which indicates what is the desired next
state 
sdesired
t  

  fplan  st  

   

hence  at every time step there is an error with respect
to the future desired state 
t   st  sdesired
t  

   

after an action has been taken the system transitions
to a new state  and we can asses the eectiveness of the
action by comparing the magnitude of the error before and
after the action  we add a diagonal matrix with positive
entries so as to compensate for scale in dierent error terms
and also to incorporate some priority as to which state
variable we want to follow more  following this  we build
the reward function 
r st   at     tt dt  tt   dt  

   

a very natural threshold  helpful in discriminating good
and bad actions  arises from this reward function  if the
reward is positive  then the error has been reduced  if it is
negative the error has grown  following this  the threshold
is set equal to zero  this structure for the reward function
is general in the sense that it can be applied directly to
any system  one draw back of this reward function and
the threshold chosen  is that it works very well when we are
far away from the desired nal state  but has a singularity

tt dt  tt   dt      
tt dt  tt   dt  

if t    
otherwise

   

in addition  since its almost impossible that the nal desired state will be exactly reached  then we will set a sphere
about the desired state  which will be our goal  thus  the
path planning function is modied to 
 
sdesired
t  

 

st
fplan  st  

if  st  sdesired  r
otherwise

   

this modication in the reward and path planning function should prompt the agent to nally rest at some point
which we consider near enough the nal desired state 
iii  estimating the reward function
there are dierent ways of estimating some value from
collected data  the one that seemed more general  robust and malleable was using a weighted linear regression
approach  this method manages in a very natural way interpolation and extrapolation  allowing the agent to take
large adventurous steps from regions that have been explored  to possibly unexplored ones  as the agent acts 
the region is populated  so wlr gradually disregards the
far away data and pays attention to the data nearby  another advantage is that it doesnt put too much pressure
on the specic features we pick to make a precise estimation  only that these features be indicative in some way of
the estimated variable  in this architecture we pick state
errors and actions as a basis functions  thus releasing pressure from de designer to pick the correct features for this
estimation  obviously  if the designer provides better features than these  the estimation will be more precise and
the learning faster 

fionline learning of control policies for dynamical systems based on input output data recorded on the fly 

a covariance matrix is used in the wlr algorithm  accounting for scale and variability in each of the dimensions  this matrix in a way indicates how fast the paying attention factor decays with distance to the current
point  it also gives the shape of the decay function  we
will pick a diagonal matrix  since we wont require the designer to have a priori knowledge of which directions should
be treated specially  the higher the value of the diagonal
entries  the farther away we look for information in that
dimension  the smaller the value  the more we disregard
far away information  so  if we have nearby data  setting
a small value will lead to more precise estimations  on the
ip side  if we reach a region for which weve got no nearby
data  a small value will mean  that we wont be able to get
anything out of the far away data we have collected  so 
a compromise has to be reached for these values  setting
them to anywhere between      and     of the expected
range for each of the variables involved has shown to work
well 
iv  managing the growing size of the data bank
the problem with the wlr approach is that being nonparametric  it carries and goes over every data point for
each estimation  as time goes by  the size of the recorded
data grows exponentially  so each estimation gets more and
more expensive  this problem will be alleviated by implementing the following reasoning  first  for every new data
that comes in  well check what is the discrepancy between
the estimated reward and the actual true reward  if the estimated reward is very similar to the true reward  which is
something that happens often   then it means that adding
this data would be in a way redundant  so we dont need
to keep it  secondly  after some indication of learning performance has been reached  for every step we make  and
consequently every new data we record  we will delete one
of the previously recorded data points  thus stabilizing the
size from that moment on  before deleting  we will rst
do the redundancy check just mentioned  if the data isnt
redundant  then we delete a point randomly and keep the
knew one  this scheme turns out to be ecient in the long
run at keeping the data size small while preserving en even
distribution of data points across all the traversed stateaction space  thus ensuring a good learning perforamce 
v  applied problems
a  positioning a mass with spring and damper in  d
we presente here results on the classical mass springdampere system  the general set up for this case is given
by 
 
st  

xt
xt

 

d   diag        
this prioritizes velocity tracking over position tracking 

 
fplan  st     sdesired
 
t  

 
xt

 

this plannig function makes the rewards function comply with the general requirement we stated above  the
features and the covariance used in the wlr were 
 
 
x  xdesired
 s   
x  xdesired

   diag         
the entries in  correspond to  error in position  error in
velocity and force applied  respectively  the action space
has been discretized to                                    this
allows for powerful actions as well as some ne regulation 
we approached this problem for two choices of  and
wn   the damping ratio and natural frequency respectively 
the rst is       wn      while the second one is        
wn     for both we have allowed for reinitialization of
the trial once the goal has been reached  or after some
maximum time has passed  allowing for learning to occur 
below is shown the trayectory evolution for the rst case 
it celarly shows how the agent learns from previous trials 
making its control policy better 

fig     subsequent trials for the free mass case        wn     

below we have plotted for both cases the learnt trajectory  with the controls policy overlayed on each of the
plots  in order to benchmark the learning capabilities  we
have calculated separately the optimal trajectories and
control sequences from the knowledge of the linear system
using the same metric  trying to maximize the reward
function at every step  so  for each of the cases we show
alongside the optimal trayectory and optimal controls 

fi 

position trajectory
 

 

y position  m 

 

 

 

fig     learnt and optimal trajectory and control policy for       
wn      case  learnt left  optimal right 

 

 
 

 

 

 
x position  m 

 

 

 

fig     dierent trials of  d positioning of a free mass 

tive  as trials advance  the route taken converges on the
optimal direct route 
c  driving a tank in  d

fig     learnt and optimal trajectory and control policy for         
wn      case  learnt left  optimal right 

for the free mass case  its outstanding how similar the
agents learnt trayectory and control sequence is compared
to the optimal  it can be seen how it rst accelerates the
mass  and as the objective is approached then controls are
reverted to bring it smoothly to a stop  except for some
really minor dierences we could say they are exactly the
same  this example shows how the architecture proposed
is indeed capable of nding the optimal policy  with respect
to the dened reward function  in the damped oscillatory
case we can also see an outstanding similarity between the
learnt and the optimal trajectory and control sequences 
overall  these reults show that the architecture is capable
of dealing eectively with damped inertial transient dynamics  obtaining a near to optimal policy  the algorithm
has also been tested for the csae of pure osillatory dynamics          wn       showing similar results  wich are not
included here for brevity 
we also note here that the strategy to stabilize the size
of the data bank has been successfull  since it allowed us to
simulate many trials rapidly  less than    seconds  which
is bearable for the designer   while preseserving enough
information for learning to occur 
b  positioning a free mass in  d
to test how the approach reacts to scaling  we show the
same problem with an extra dimension  i e  we added a
force in the y direction   the parameters were expanded
accordingly to accomodate the change  the part that will
be suering most is the wlr  since adding another   dimensions will put more stress on it  below we show a run
with a few trials 
in its rst attempt  the mass is wiped around the objec 

lastly  we present a case in   dimensions where we have
a strong cross coupling of control inputs  requiring coordination to achieve the goal  this is the case of the  d tank 
the equations of motion for this system are 

v        


 x   v sin  
y   v cos  



       
where v and  are the forward speed and heading angle  respectively  and   and   are the left and right track
speeds  which are the control inputs  by setting these
equal  we move in a straight line  by setting them exactly
opposite we turn without changing position  the state and
path planning function for this problem were 


xt
st    yt 
t


fplan  st     sdesired
t  


 

 
 
atan xt  yt  

the rest of the parameters were set to 
d   diag        
this choice of d accomodates for the fact that the angle
is in radians and the positions are in the order of   meters 
   diag                 
for the wlr  the base variables were  error in x  error
in y  error in   and the two control inputs   and    
below is shown a run with successive trials 
in the rst attempt  the agent takes a zigzagy route  but
eventually seems to be heading towards the origin  already

fionline learning of control policies for dynamical systems based on input output data recorded on the fly 

the necessity of a reasonable path planning function that
has to be provided by the designer 

learnt path
 
 

viii  acknowledgements

 

y  m 

 
 
 
 
 
 
 

 

 

 

 

 

x  m 

fig     dierent trials for driving a  d tank to the origin 

in the second and third trials the agent has learnt a route
close to the optimal direct route  this case shows that the
architecture handles dynamical systems with cross coupled
control inputs 
vi  conclusion
the learning architecture here presented has proven effective in learning adequate control policies for a variety
of problems  by simply interacting with the environment
and exploiting past experiences in a natural manner  the
specics of the dynamical system are coped with in a
generic and robust way  dealing eectively with cases of
undamped inertial transient dynamics  as is the case of
the  d and  d mass  and cross coupled controls  as is the
 d tank  in many cases the algorithm converges to a control policy that is close to the optimal policy  reecting a
good learning quality  although for every problem a number of parameters have to be set by the designer  these
only require intuition of scale for the given problem and
not hard technical knowledge  issues of computational efciency have been dealt with eectively by the scheme presented of checking for redundancy and deleting data points
randomly for each new one that is added  after a certain
level of learning has been reached  this has reduced notably the computational cost  withou aecting the learning
capabilities of the algorithm  the  d mass problem is an
indication that scalability to higher dimensional problems
might be possible 
vii  future work
future work on the topic will involve extending the architecture to higher dimensional problems as is the control
of a xed wing aircraft  adding a soft action picking 
where there is a probability of picking one of the candidate
actions  those that surpass the threshold   might make the
agent converge faster to the optimal policy  and would also
protect it from singular bad sequences it cant get out of 
also  adding longer time horizons and optimizing with respect to a sum of rewards should denitely be the next
step  since this will allow for problems where initial negative rewards are actually optimal  and will also alleviate

i would like to thank prof  ng for his always kind attention to my inquiries and for setting up this outstanding
course on machine learning  this is my rst formal exposure to it and it has left me wanting more  id also like
to thank the sta for their predisposition to make these
projects a success  especially quoc le  for always being
attent to my questions and needs  i would nally like to
thank adam coates for his advice  and also for pointing me
in dierent interesting directions i could take this research
eort in the future 

fi