recognizing facial expressions from videos using deep
belief networks
cs     project
advisor  prof  andrew ng
adithya rao  adithyar stanford edu   narendran thiagarajan  naren stanford edu 

abstract  deep learning techniques have been shown to
perform well for problems such as image classification and
handwriting analysis  in this project we aim to apply these
deep learning techniques to recognize facial expressions
from videos  we employ sparse feature selection to
improve the efficiency and accuracy of the classification
     we begin by considering images  and extend the
algorithms to classify videos  we first train a sparse
autoencoder for a known set of images  to verify the
correctness of implementation of the encoder  we then
pre train the autoencoder with our dataset of facial
expressions  and use the weights to classify test images  we
use softmax and logistic regression for classifying  the
results show that the individual expressions are classified
with high accuracy  and the performance further improves
by including facs and tracking labels 

i 

facial expressions from video frames instead of images 
related work in      used adaboost and svms for feature
selection and classification respectively from video sequences 
whereas our approach would use dbns  using unsupervised
deep learning  the sparse autoencoder will automatically
choose a set of high level weights  which removes the
necessity to perform feature selection separately  deep belief
networks are very suitable to this problem because they have
shown promise in machine learning problems that involve
human perception such as vision  audio  touch etc 

ii 

preliminary steps

    sparse autoencoders
we began by writing the algorithm for the sparse autoencoder
using matlab and followed the guidelines provided in the
cs    course website  we implemented a neural network
with an input layer having    units  a hidden layer having   
units and the output layer with    units 

introduction

deep learning involves learning a hierarchy of internal
representations using a stack of multiple modules  each of
which is trainable and can implement complex non linear
functions  deep learning allows us to go from low level
representations to high level representations and is more
efficient and accurate than shallow architectures such as
kernel machines  a deep architecture achieves this efficiency
by trading space for time and using sparse feature encoding
     deep learning techniques have been shown to perform
significantly better than other techniques for problems such as
image classification and handwriting analysis     
here we apply deep learning for recognizing facial
expressions in videos  previous work in     and     addressed
the problem of generating facial expressions with images as
inputs  we aim to extend this work to automatic recognition of

fig    weights generated by the sparse autoencoder for the data from
the cs    course website 

fithe algorithm  explained in      performed the following
steps 
 i 
run a feedforward pass on our network on input
images  to compute all units activations 
 ii 
perform one step of stochastic gradient descent
using backpropagation
 iii 
perform the updates to employ sparsity constraints 
the weights that we obtained using the assignment data are
shown in fig    the output weights are seen to closely
resemble the expected output  this validates the correctness of
our implementation of the autoencoder 
    facial image data
dataset  as a step towards analyzing facial expression from
videos  we first try to classify facial expressions from images 
for this purpose we initially used the facial expression
database from      this dataset consists of faces showing  
different classes of emotion  namely    surprise  fear  disgust 
contempt  happiness  sadness  anger and neutral expressions 
preprocessing  we used partially preprocessed images from
     where the images have been converted to grayscale and an
oval mask has been applied to them  so that only the
information that contributes to recognizing the expression
remains  thus  the background and the hair are not considered 
the images have also been resized to smaller dimensions  fig 
  shows the images before and after preprocessing 
we applied the sparse autoencoder to the facial image data  to
obtain the corresponding weights  we tuned our parameters
alpha        beta      lambda         and ran the algorithm
for   million iterations  among the different combinations of
parameters that we tried  the above values seemed to work
best 

fig    the images above are from the dataset      and the images
below show the same after preprocessing

fig     weights obtained after running the sparse encoder on
preprocessed facial image data

iii 

methodology

    facial expression dataset
we use the cohn kanade dfat     dataset      which is an
au coded facial expression database  it has both posed
 about     sequences  and non posed  spontaneous 
expressions with validated emotion metadata  a total of  
expressions are present in the dataset  the target expression
for
each
sequence
is
fully
facs
coded 
    preprocessing
we performed the following steps for preprocessing the data 
a  instead of applying an oval mask to the image as in the
preliminary step  we extracted face patches from the dataset
using the automatic viola jones face detector package      
b  the extracted face patches had varying lighting conditions 
and needed to be normalized so that we avoid learning lighting
features at the expense of face details  to normalize the data 
we first resized each image to    x    image patches using a
bilinear transformation  the minimum and maximum pixel
intensities for each image were calculated  and each image
was scaled accordingly such that the pixel values were now
between    and     thus all the images in the dataset were
resized and normalized  to be provided as input to the
autoencoder 
    high level feature extraction
in our model      soft binarized pixel inputs are connected to
a hidden layer of     logistic units  which are trained by our
sparse autoencoder to match the output to the input  we tried
different architectures for the neural net  which are explained
in more detail in the next section  the feed forward pass and
the back propagation of errors were performed for each
iteration of the stochastic gradient descent  and sparsity
constraints were employed  this resulted in the weights shown
in fig    some weights indicate local structure useful for

fifig    methodology for feature extraction and classification 

representing distinct features and edges  while others show
more global structure 
    learning the joint distribution of facs labels and
features
for classifying the images  we first use   of k softmax
regression to represent the identities of the facial expressions 
after obtaining the weights  the feature activities for each
image were concatenated with discrete facs labels and
tracking labels  the combined vector is then the input to our
softmax classifier  a face image can only be associated with a
single identity  and each identity label is set to   with
probability 

where s is our input vector including the facs labels  and w
are the parameters of the softmax regression problem  we split
our dataset into training and test data  and train our softmax
layer to estimate the parameters using gradient descent  once
the parameters have been estimated  we input a test image to
the softmax layer  which will calculate the probabilities for
each identity  the expression with the highest probability is
chosen as the identity of the test image 

all expressions  we ran the autoencoder for different sets of
alpha  beta parameters for a large number of iterations and we
identified the best set of weights  fig    
in other variants of the autoencoder  we tried with   hidden
layers  containing     and     units respectively   because for
a given dbn adding a new hidden layer could improve the
performance  previous work in     used two hidden layers 
with the number of hidden units very close to the number of
input units  we also set the number of hidden units to     and
measured the weights  however  we observed that adding a
hidden layer or increasing the number of hidden units did not
lead to a marked improvement in the resulting weights 
    feature activations for each expression
the sparse autoencoder activates different neurons for each
expression  this is visualized in fig    where the activations
for surprise and anger are compared by plotting the difference
in their feature activations  it is clearly seen from the figure
that some neurons have higher activations for surprise
 positive spikes   while some are more active for anger
 negative spikes  

apart from softmax regression  we also use logistic regression
to classify individual expressions  fig    shows the entire
methodology described above 
iv 

experiments and results

    choosing the right autoencoder
the first step in the pipeline is to identify the features using
autoencoders  our first approach is to use the autoencoder
from our preliminary experiments  sec      which consists of
a single hidden layer with     units  the reason for picking
    is to extract the most important features across images of

fig    comparison of neuron activations for surprise and anger 
positive spikes indicate neurons active for surprise and negative
spikes indicate neurons active for anger 

fi    choosing the classifier
we picked the best set of weights from the autoencoder and
then trained a softmax layer using our dataset  the training set
size was approximately    percent of the entire dataset size
 training set size        test set size        with just the
feature activations as input to the softmax layer  the resulting
accuracy is       the accuracy of a random guessing
algorithm is of         possible output classes   also the
algorithm classified majority of the test cases as disgust and
surprise and worked poorly for fear and anger 
in order to improve the accuracy of the predictions  we applied
logistic regression to individual expressions  by doing so  the
accuracy greatly improved  and the correctly predicted
expressions increased significantly  table   shows the
resulting values for each expression 
    choosing the input vector
given our extracted features and classifier  we experimented
with different input vectors to the classifier  we observed that
including facs labels and tracking values along with the
feature activations from the autoencoder gave better results
than using feature activations alone  this is also consistent
with the observations in      fig     shows the comparison
between the accuracy values for both cases  fig    shows the
tpr vs fpr scatter plots for both input vectors  where a
particular point represents a particular expression  when the
facs data is included  most of the points lie in the upper left
corner of the plot  indicating better performance 
    using video sequences
extending from images to video sequences  we used our
model to predict expressions for sequences of images 
showing the peak expression for a particular face  fig   shows

label
anger
disgust
fear
happiness
sadness
surprise

table  
accuracy
tp
    
  
    
  
    
  
    
  
    
  
    
  

tn
   
   
   
   
  
   

fp
  
 
 
  
  
  

fn
  
  
  
 
  
 

a  input  feature activations   facs data

label
anger
disgust
fear
happiness
sadness
surprise

accuracy
    
    
    
    
    
    

tp
  
  
  
  
 
  

tn
  
   
   
   
   
  

b  input  feature activations only

fp
  
  
 
  
  
  

fn
 
  
  
  
  
 

fig    prediction accuracy for feature activations   facs labels  
tracking data  and for feature activations only 

the probabilities for a sequence of five images for a particular
expression  the model correctly predicts a high probability for
one of the expressions and low probability for the others 
classifying sequences of images leads to a better performance
of the model  rather than classifying an individual image
alone  including a sequence of images would also help in
identifying transitions of expressions 
v 

conclusions and future work

our experiments and results show that deep learning and
logistic regression indeed produce good predictions for
recognition of facial expressions  increasing the number of
hidden layers and the number of units per hidden layer did not
seem to significantly improve performance  an interesting
problem in this direction would be to investigate techniques
for estimating the required number of hidden layers and units 
for a given image recognition dataset 
further  our experiments also show that including facs
labels along with the feature activations significantly improves
performance  classifying sequences of images leads to better
performance than classifying individual images  this indicates
that recognizing expressions from videos is more accurate
than recognizing them from a single image  these results
suggest that for such problems involving facial expressions  it
may be helpful to have facs data available along with the
image and video data  currently  high quality automatic facs
labeling with coded aus are available publicly only for a
small number of datasets  thus  developing an automated
method for facs labeling is an important challenge that could
be pursued in the future 
another extension to this work would be to attempt to
combine audio data with facial expressions  in order to
understand the effect of audio visual cues used in videos to
convey emotions 

fifig    tpr vs fpr

fig    probabilities of each expression for a sequence of
images

vi 

references and links
    j m  susskind  g e  hinton  javier r  movellan and
adam k  anderson  generating facial expressions
with deep belief nets  affective computing       
pp          
    sabzevari et al   a fast and accurate facial
expression synthesis system for color face images
using face graph and deep belief network
    bartlett m s   littlewort g   frank m   lainscsek c  
fasel i   javier movellan recognizing facial

expression  machine learning and application to
spontaneous behavior
    lee h   grosse r  ranganath r  ng a  
convolutional deep belief networks for scalable
unsupervised
learning
of
hierarchical
representations proceedings of the twenty sixth
international conference on machine learning       
    ranzato m  boureau y   lecun y   sparse feature
learning for deep belief networks
    d  liu  l  lu  and h  j  zhang  automatic mood
detection from acoustic music data  in proceedings of
the international symposium on music information
retrieval  ismir     baltimore  md  usa  october
           
    li fen chen and yu shiuan yen          taiwanese
facial
expression
image
database
 http   bml ym edu tw download html  
brain
mapping laboratory  institute of brain science 
national yang ming university  taipei  taiwan 
    agarwal n  cosgriff r   mudur r   mood detection 
implementing a facial expression recognition system 
 cs    project        
    andrew ng   cs   a lecture notes  sparse
autoencoders 
     http   vasc ri cmu edu idb html face facial expression
 
     lucey p   cohn j f   kanade t   saragih j   ambadar
z   the extended cohn kanade dataset  ck    a
complete dataset for action unit and emotionspecied expression  computer vision and pattern
recognition workshops  cvprw        ieee
computer society conference on  june      
     viola p   jones m   robust real time face
detection  international journal of computer
vision       

fi