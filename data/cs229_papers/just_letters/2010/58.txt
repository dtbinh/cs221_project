task recommendation on wikipedia
eric huang

hyung jin kim

hajoon ko

stanford university

stanford university

stanford university

ehhuang stanford edu

evion stanford edu

gogo th stanford edu

abstract
in many open source projects and user generated content
websites  one challenge is matching contributors with tasks
so that contributors are able to work on things on which
they have the most expertise or interests  thus increasing
their productivity  in this project  we study one particular domain  wikipedia  an online collaborative encyclopedia  our goal is to make finding articles on wikipedia to
work on easier for editors  we created an edit graph of
wikipedia  and formulate the problem as a link prediction
problem  using topological features  we applied and evaluated various machine learning algorithms on the data  we
found that decision tree is the best performing algorithm 
achieving     testing accuracy 

keywords
wikipedia  recommender systems  collaborative filtering  network analysis

  

introduction

recently  we have witnessed a shift on the internet from
static content distribution to more and more dynamic usergenerated contents  coupled with this is an increase in
the formations of online collaborative communities  opensource efforts  and crowdsourcing  yahoo  answers  quora 
github  amazon mechanical turk  and wikipedia  to name
a few  according to social science theory  reducing the cost
of contribution has an effect on increasing peoples motivation to participate  these online collaborative communities
and efforts are made possible with the convenience of the
internet  in this paper  we are interested in the domain of
wikipedia  a web based collaborative encyclopedia  particularly  we are interested in the question of how to reduce
the cost of editors finding finding wikipedia articles to improve  by reducing the cost of finding articles that align
with the editors interests  we can potentially improve the
quality and quantity of the articles on wikipedia  we aim

permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page  to copy otherwise  to
republish  to post on servers or to redistribute to lists  requires prior specific
permission and or a fee 
copyright      

to build a recommender system for suggesting wikipedia
articles to editors 

   

related work

our work is inspired by suggestbot     which is a software
system that matches people with work on wikipedia  it employs three approaches to the problem  using text analysis 
collaborative filtering and hyperlink following  their collaborative filtering algorithms uses the jaccard coefficient
to measure similarity between editors  and recommend by
looking at the value of the jaccard coefficient directly  our
approach employs many more topological features and uses
machine learning algorithms 
in      the authors evaluated various link predictors on several social networks  in      the authors applied a link prediction approach to collaborative filtering  however  they
make recommendations on each linkage measure separately 
and solely on the value of the linkage measures  in      the
authors also applied supervised learning methods to solve
the link prediction problem  and studied the features  however  they used different datasets which are much smaller
graphs in comparison with the wikipedia network 

  

problem formulation

we formulate our problem as a link prediction problem 
we consider the entire wikipedia network as a undirected
bipartite graph  where editors and articles are nodes in the
graph  and an edge between a particular editor article pair
represents that the editor had edited that article at some
point in the past  to decide whether a particular article is
a good candidate to recommend to some editor  we build a
classifier and predict whether an edge is likely to form between that particular editor article pair in the future  that
is  whether the editor will edit that article some time in the
future 
more formally  consider a bipartite graph g  with two sets
of nodes  ne and na   and a set of edges e  where an edge
exists between some ne i  ne and na j  na if editor i
has edited article j at some point in the past  for any given
editor  ne i   we want to output a list of na j s that ne i is
likely to edit in the future 

  

approach

we apply a supervised learning approach to our link prediction formulation of the problem  in short  we obtain
training samples from the graph in the past  we extract topological features from the snapshots of the graph  and evaluate the trained models on the testing samples in the time

fi common neighbors in our bipartite graph  for an
editor  article pair   e  a   this is defined to be  n  e  
n  a    since this feature is adapted to be the intersection of the articles this particular editor edited 
and the articles that the editors who edited the article in question edited  it basically captures the notion
of people who edited this article also edited    this
measure implies the commonality between the editor
and the article 

period following the training period  the idea is that we
believe the topological features from the graph  which represents the interactive relationships between editors and articles  have predicting powers and are informative to whether
a link will form between the two nodes in the future 
more formally  we partition our data into two sub ranges 
defined by four points in time  t    t   t    t    the
training samples are obtained from the first sub range   t    t    
and the testing samples are from the second sub range  t    t    
the positive samples are the editor article pairs that did not
have an edge between them in t    but had an edge by t   
meaning that the editor edited that particular article during this time frame  the negative samples are those that
did not have an edge between the pair in both t  and t   
representing that the editor did not edit that article  we
train our models with samples from  t    t     then  we make
predictions with our trained models on editor article pairs
in t    finally  we evaluate our predictions by examining t   

   

 jaccards coefficient this is also known as neigh e n  a  
 
borhood overlap  in our graph  this is defined as  n
 n  e n  a  
this feature is similar to common neighbors  but it
is normalized by the total number of neighbors  which
should make it more informative of the commonality
between the editor and article 
 adamic adar this measures uses the frequency of
common p
features to compute similarity between two
 
  in our
nodes as z features shared by x y log f requency z  
context  the feature
is
the
neighbors 
and
this
measure
p
 
is defined as zn  e n  a  log  n
 
 z  

algorithms

we will apply various standard classification algorithms
for our task  including logistic regression  svm  decision
tree  multilayer perceptron  naive bayes  and finally bagging  we will evaluate these algorithms and compare their
performances 

   

 preferential attachment this is similar to the sum
of neighbors measure above and suggests how active
the editor is and how popular the article is  it is defined as   n  x     n  y   

features

the main component of our task is to come up with a
list of features that we believe are informative to feed into
the machine learning algorithms  in this project  we chose a
set of topological features between an editor article pair in
the graph  since we have a bipartite graph  we adapted the
features that are commonly used in unipartite graphs  we
adopt the following notations  for a node x  we define n  x 
to be the set of xs neighbors  and n  x    yn  x  n  y   or
the set of xs neighbors neighbors 
we consider the following features 
 sum of neighbors for an article  this is the number
of editors that edited it  for an editor  this is the
number of articles he has edited  this number may be
meaningful as the more articles an editor edited before 
the more likely he will edit more articles because it
suggests that he is more active  if an article is edited
by many editors  it may indicate that it is a popular
topic  it is controversial  or it concerns a difficult topic 
 editing frequency for an article  the frequency at
which it was edited by people  for an editor  the frequency at which he edits articles  this is similar to the
sum of neighbors  but is limited by the number within
a certain time frame  thus adding some temporal information  for example  an article with many total
edits may only have a few in the past year  suggesting
that the content is rather complete and needs no new
editing 
 shortest distance this is the minimum hop count
between an editor and an article  we hypothesize that
the shorter the distance between the editor and article  the more likely the editor will edit the article 
this measure characterizes the degree of separation
between an editor and a article  or how closely related
are the editors interests and the articles topic 

  

dataset

we used the processed metadata for all revisions of all
articles on wikipedia as of                 this dataset is
  gb compressed  and    tb decompressed 

   

data processing

as mentioned  the size of this dataset is extremely large 
however  the revision history also included other information that we do not need for this project  such as external
links  images  etc  it also contains all editing records if an
editor edits the article multiple times  because we need a lot
of ram  we used amazon ec  in order to deduplicate these
records to get the record of the first edit between an editorarticle pair  from this extracted data  we further processed
it assigning ids to the editors as they are usernames or ip
addresses in the original format  we further reduced the size
of the data by removing timestamps  and partitioning the
edges into different files as grouped by the year they were
created  the resulting processed data is a total of    gb
containing all editor article edges of the wikipedia network 

   

network characteristic

before we proceeded on the link prediction task  we wanted
to have an understanding of the general characteristics of the
graph  in       our graph consists of           nodes and
           edges since the inception of wikipedia in      
by       it grew to           nodes and            edges 
by       it had            nodes and            edges  with
near    million editor nodes  it shows that the growth of
wikipedia really took off since      and had been accelerating 
figure   a  and   b  show the degree distribution of articles and editors respectively  they follow a power law distribution  which is typically expected in a real network  since
we are performing link prediction  we are interested in what

fi a  log log plot of degree
distribution  articles  

 b  log log plot of degree
distribution  editors  

 c  log log plot of newly
added edges to an article
node distribution 

 d  log log plot of newly
added edges to an editor
node distribution 

figure    node degree distribution and newly added edges distribution in      

figure    testing accuracies compared to baseline 
figure    cumulative shortest path distribution

the distribution of the number of newly added edges to the
graph  figure   c  and   d  show the distribution of newly
added edges between the year of      and       they also
follow a power law distribution  as we would expect only a
few number of articles would have many edits and a few
number of editors would be the power users  we also looked
at the distribution of shortest paths in the graph  as shown
by figure    it shows that most of the pairs have a distance
of three to four  and no longer than eight  this shows that
the graph exhibits the small world phenomenon  which is
also typically expected in a real social network 

   

obtaining samples and features

to obtain the samples for training and testing  we chose
the four times that define the training and testing periods
to be  t    t                   and  t    t                    we
chose those times because the number of edge additions in
both periods are comparable  and we chose the range of one
year as it reduces possible variance if the range is too short 
again  since the dataset is so large  instead of using all the
edges  we only obtained a subset of the edges to be samples 
we decided to select samples as follows  for positive training
samples  we randomly select an edge in the      snapshot
of the graph  we check that both nodes exist in the     
snapshot of the graph  if they do  we add this pair to the set
of positive training samples  we then fix the editor node 
and randomly select an article in the      graph  it there is
no edge between that pair in       we add that pair to the

set of negative samples  we decided to fix the editor node
for choosing the negative samples for two reasons  first 
since there are so many pairs of nodes in the graph that
are not linked  it is possible that we just select pairs that
have little in common  this makes the problem possibly
less interesting and easier  the second reason is that since
our goal is to ultimately recommend articles  we would like
to know whether our approach can actually distinguish good
candidate articles from worse candidate articles for the same
editor  thus  for each chosen editor  we have a positive
sample and a negative sample  the testing set was also
obtained by the same method 
using this method for choosing editor article pairs  we
computed the aforementioned features on all the pairs for
the training and testing sets  we obtained four thousand
samples in training set  and the same number in testing set 
both sets are balanced with     positive and     negative
samples 

  

results

we applied various machine learning algorithms  we trained
the models with the obtained training set and evaluated
them on the testing data  for multilayer perceptron and
svm  we performed a cross validation to select the optimal
parameters before testing  table     summarizes the results 
the result shows that the topological features are indeed
informative for predicting links in the wikipedia edit graph 
looking at the testing accuracy  we see that most algorithms
are in the     range  except for naive bayes with        
this suggests that the naive bayes model probably is not

fitesting accuracy
bagging reptree 

      

decision tree   reptree

      

decision tree   j  

      

svm  rbf kernel 

      

logistic regression

      

multilayer perceptron

      

naive bayes

      

class
 
 
 
 
 
 
 
 
 
 
 
 
 
 

tp rate
     
     
    
     
     
     
    
     
     
     
     
     
     
     

fp rate
     
     
     
     
     
     
     
     
     
     
     
     
     
     

precision
     
   
     
     
     
     
     
     
     
     
     
     
     
     

recall
     
     
    
     
     
     
    
     
     
     
     
     
     
     

f value
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    testing results 

as powerful as the other algorithms for capturing the relationships between the nodes as suggested by the features 
this makes sense since the naive bayes model assumes all
features to be independent  which is often a wrong assumption because many of the features are somewhat correlated
as they all concern the topology around the two nodes  we
also see that decision trees have the highest testing accuracy  since decision trees can represent non  linear decision
boundaries  it may be able to train more suitable models
for this domain  with bagging  we were able to boost the
decision trees performance by a little bit to         which
is     better than the baseline predictor which would have
a     accuracy 
the testing accuracy gives us a sense of how well the models can predict whether links will form in the wikipedia edit
graph  however  since we are interested in recommending
the articles to editors  we are not necessarily as interested
in the prediction of the negative samples because we will
only be recommending the links that the algorithms predict
to exist  thus  the more suitable measures to compare are
precision and recall for the positive classes  we see that
with bagging  decision trees are able to achieve a precision
of      and recall of       this means that if we recommend
articles that the algorithm predicts will form to the editor 
we would be right     on average  and of the articles that
the editor will edit  our recommendations would cover    
of them on average  note that the naive bayes model has
the highest precision of       however  its recall is also really low at around       meaning that it only covers a small
subset of the articles that the editor will edit  making it a
not so desirable model overall  the f value is a harmonic
mean of precision and recall  so it takes both values into account  we see that decision trees have the highest f value 
and can be considered the best model in this domain 
we also carried out ablative analysis in order to gain insight into which features are more informative than others 
using the decision tree  we report the testing accuracy of the
model excluding one feature at a time  table   summarizes
that result  we see that when the featurenew links to article added in the past year is removed  the testing accuracy
decreases the most  suggesting that this is more informative
than others  this implies that the number of editors that
edited an article in the past year is a significant indicator of

excluded feature
neighbors of editor
neighbors of article
shortest distance
common neighbors
jaccards coeff
adamic adar
preferential attachment
new links to editor
new links to article

testing accuracy
    
     
     
     
     
     
     
     
    

table    ablative analysis result 

whether another editor will edit that article 

  

discussion

it is important to note that due to time constraints  we
evaluated these algorithms on historical data and based on
the articles that the editors actually edited  one might ask 
what is the use of a recommendation system that recommends what the editors would edit regardless  one answer
is that by recommending and presenting these articles early 
we are potentially reducing the cost of contribution of the
editors  even though they would eventually edit the articles anyways  it may cost them time and effort to search
or discover those articles by themselves  by reducing this
cost  the editors may have more time to work on more articles  we have then effectively increased the participation
rate by reducing cost of contribution  potentially leading to
increases in quality and quantity of articles on wikipedia 
also  note that the testing accuracies reported above are
again evaluated on the actual edits made by the editors 
which does not completely represent the effectiveness and
accuracy of the recommendations  this is so because the
instances counted as wrong classifications in the evaluation
might not be wrong had we actually deployed the system 
the recommendation we presented might lead the editor to
edit articles he would not have otherwise  thus  the system might have a higher accuracy than the reported testing
accuracy numbers 

fi   

future directions

many future improvements could be made upon this project 
 additional features 
in addition to topological features  we may consider
page to page link relationship  text similarity between
articles  categories of articles  and single article characteristics  such as length of article  topic  etc 
 vary length of learning period 
in our experiments  the samples were obtained with
the time frame being one year in length  it may be
interesting to explore how accuracy might change if
we vary the length of the learning period  perhaps
links formed in shorter intervals are more predictable 
or maybe by lengthening the period  we allow more
time for the recommendations to take effect as people
have more time to react to the recommendations 
 actual system deployment 
as previously mentioned  our system is not actually
deployed  so we do not observe the actual effects the
recommendations have on the editors  this limits the
evaluation of our methods effectiveness 

  

conclusion

in this project  we studied the problem of recommending articles on wikipedia for editors to work on  we formulated the problem as a supervised learning link prediction problem on a bipartite edit graph of wikipedia  we
processed and extracted relevant data from the extremely
large wikipedia revision history dump  we studied the network characteristics of the wikipedia edit graph  finally 
we applied machine learning classification algorithms on the
data and achieved     accuracy with decision trees  using
only topological features  we obtained promising results and
opened up many directions for future work 

  

references

    cosley  dan and frankowski  dan and terveen  loren and
riedl  john  suggestbot  using intelligent task routing to
help people find work in wikipedia  proceedings of the   th
international conference on intelligent user interfaces  iui
     honolulu  hawaii  usa
    liben nowell  david and kleinberg  jon  the link
prediction problem for social networks  proceedings of the
twelfth international conference on information and
knowledge management  cikm    
    mohammad al hasan and vineet chaoji and saeed salem
and mohammed zaki  link prediction using supervised
learning  in proc  of sdm    workshop on link analysis 
counterterrorism and security 
    zan huang and xin li and hsinchun chen  link prediction
approach to collaborative filtering  in proceedings of the
joint conference on digital libraries  jcdl     acm
    the wikipedia dataset was created from a publicly
available database snapshot by gueorgi kossinets  cornell
university  supported by nsf grant bcs         

fi