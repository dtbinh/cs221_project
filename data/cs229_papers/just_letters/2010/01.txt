machine learning for sentiment analysis on the experience project
raymond hsu
bozhi see
computer science dept 
electrical engineering dept 
stanford university
stanford university
hsuray cs stanford edu bozhi stanford edu

abstract
the goal of sentiment analysis is to extract
human emotions from text  this paper applies various machine learning algorithms to
predict reader reaction to excerpts from the
experience project  metrics such as accuracy of prediction and precision recall are presented to gauge the success of these different
algorithms  we propose a system to process
the documents and to predict human reactions 
as well as provide results  we discuss various methods and their advantages and disadvantages in sentiment analysis for these documents  finally  we comment on applying our
findings to sentiment analysis in a more general sense 

 

introduction

one application of machine learning is in sentiment
analysis  in this field  computer programs attempt to
predict the emotional content or opinions of a collection of articles  this becomes useful for organizing data  such as finding positive and negative reviews
while diminishing the need for human effort to classify
the information 
   

related work

much literature in the field of sentiment analysis
have focused on different classification models for
text  previous approaches include hand coded rules
 neviarouskaya et al          the winnow algorithm
 alm et al          random k label sets  bhowmick
et al          support vector machines  svm   koppel and schler         and naive bayes  mihalcea and
liu         however  previous work has done classification only on three or fewer categories   typically
positive  neutral  and negative  our work attempts to
extend this by inferring specific emotional reactions
rather than broad categories 
   

problem

we will perform sentiment analysis on confessions
from the experience project   ep   a collection of
short  user submitted posts reflecting the writers
 

http   www experienceproject com

alan wu
electrical engineering dept 
stanford university
alanw stanford edu

thoughts and actions  ep further allows other people to
express reactions to these pieces by voting on five predefined categories  thus providing labeled data of readers reactions for use in a classifier  the five numbered
categories in ep  along with our descriptions  are 
   sorry  hugs  offering condolences to the author 
   you rock  positive reaction indicating approval
and offering congratulations 
   teehee  reader found the anecdote amusing or
humorous 
   i understand  show of empathy towards the author 
   wow  just wow 
amazement 

expression of surprise or

we consider two tasks  in the first task  for a given
confession  we predict which label will receive the
most votes  the max label task   this is similar to
traditional multi class classification tasks  with the exception that our ground truth labels are only partially
correct  it is likely that the same confession can elicit
different emotions from different people  so we need
to take into account that confessions are not something
that can necessarily be neatly partitioned into disjoint
categories  hence we have a second task where we
predict which labels will receive at least one vote  the
label presence task   the label presence task tries
to answer the question  what are all the emotions that
readers feel after reading this confession  
useful applications of our findings include sentiment
detection and classification in social networking sites 
where these kinds of text often appear  many of the
confessions appearing in ep are similar in style to status updates on popular platforms  our findings on feature selection may be used to guide sentiment analysis
for these social networks in the future 
the paper is organized as follows  in our model  we
will describe how we plan on predicting the human reaction to the passages of text  we will describe each
stage of our algorithm and its purpose  in the results
section  we will look at the improvement in our metrics
as each stage is added  for both tasks  in our discussion
section  we will analyze the challenges faced as well

fias reasons why our techniques improved our prediction
accuracy  we also discuss other attempted methods that
were less successful in predicting reactions and explain
why we think they did not work as well 

separate feature  we use bow features as our initial
feature set for our system  this basic model acted as
a test bench for us to observe the changes needed to
make to our model better 

 

      wordnet synsets
in order to further improve the quality of the feature
set and decrease overfitting  we used wordnet to map
the words in the confessions onto their synonym set
 synset   by mapping words into their synset  we made
the assumption that the words of similar meaning elicit
similar emotions  this reduces the number of unique
features we have and also improves the coverage of
each feature  this technique also allows us to handle
words that do not occur in our training data if they happen to be in the same synset as words that do occur in
our training data 

model

figure    final model diagram 
our system consisted of first processing the confessions in order to extract a feature set  before passing the
data into a supervised learning algorithm 
   

parser

in order to refine our data and improve the feature
set  we removed all html tags using a python parser 
this was essential towards refining our dataset because
html tags do not convey emotions and would skew
our feature vector by including phrases that have no semantic meaning  e g   nbsp   
emoticons  on the other hand  are an excellent way
of conveying emotions through text because it captures
the emotion of the writer by including a facial expression  therefore  we captured this unique feature set and
used it to improve our feature vector 
   

spell checking

one of the issues we encountered in our earliest models
was overfitting  on closer inspection of the raw data 
we noticed that there were many spelling errors  in
order to reduce problems of overfitting as a result of
having too many unique spellings  we ran the raw data
through a spell checker and corrected all the spelling
errors 
   

features

we considered three features in our model  bag of
words  wordnet  synsets  and sentiment lexicons 
      bag of words  bow 
the bow model is the most basic feature model in sentiment analysis  it treats each unique word token as a
 

http   wordnet princeton edu 

      sentiment lexicons
sentiment lexicons are groupings of words into emotion and content categories  we used two of them in
our system because we found they improved performance  we used them by replacing the original words
with their sentiment lexicon category  the first sentiment lexicon we used was language inquiry and word
count  liwc   pennebaker et al          a handengineered set of words and categories used by psychologists to group words by similar emotional and
subject content  we also used features from the harvard inquirer  stone et al          which also categorizes words by emotional and subject content  like
liwc  the harvard inquirer was also hand engineered
by psychologists for the purpose of analyzing text 
both lexicons have been used in previous work on sentiment analysis 
   

tf idf

not surprisingly  function words such as and  the 
he  she occur very often across all confessions 
therefore  it makes little sense to put a lot of weight
on such words when using bag of words to classify the documents  one common approach is to
remove all words found in a list of high frequency
stop words  a better approach is to consider each
words term frequency inverse document frequency
 tf idf  weight  the intuition is that a frequent word
that appears in only a few confessions conveys a lot of
information  while an infrequent word that appears in
many confessions conveys very little in formation  we
produce weights for each word via the following equation 
ni j
tfi j   p
k nk j
 d 
idfi   log
 dti  
tf idfi j   tfi j idfi
 tfi j   importance of term i in document j

fi ni j   number of times term i occurred in document
j


p

k

nk j   total number of words in document j

 idfi   general importance of term i
  d   total number of documents in the corpus
  dti    number of documents where the term ti appears
   

svm

we used support vector machine  svm  as the final
classifier to make predictions for both tasks  for the
label presence task we train five svms that perform
binary class classification  one for each category  however  this is insufficient for the max label task since it
is a multi class classification task  a common solution is to build multiple one versus all svm classifiers
and combine them to perform multi class classification
 rifkin and klautau         for each category  we use
the five binary class svms from the label presence task
to predict whether a confession belongs to that category
or not  we make a prediction in the max label task by
running all five binary class svms and choosing the
category with the most positive value 

 

longer have the signficant overfitting problem encountered under the bow model  adding the liwc and inquirer features on top of synsets further improves accuracy by     giving us our highest accuracy for the max
label task of      this produced our best model  svm
with synset and sentiment lexicon features  notably 
precision increases significantly when we added sentiment lexicons  we considered adding features from
additional sentiment lexicons as well  but did not find
any improvement in accuracy so we omit them from
our model  this demonstrates that the sentiment lexicons are a useful feature for sentiment analysis and that
groups of related words can provide very useful information about expected user reactions 

results

we discuss the results of the two tasks separately 
   

max label task results

to evaluate our results on the max label task  we first
established a naive baseline for comparison  the baseline is to simply always predict the most popular category  category     we then compare the baseline performance across different models  results are shown
in figure   
the baseline accuracy is     with very low precision and recall  our most basic model  svm with
bow features  improves upon the accuracy of the baseline by     even though the increase in accuracy is
small  we see signficiant increases in precision and recall  the increase in precision and recall is because
our bow model makes predictions across all five categories  thus  we conclude that raw words without
any attempt at feature reduction or sentiment labeling are sufficient to give some information about what
sort of reaction users will have to that text  however 
upon further analysis of the bow model we find that
it overfits the training data  achieving upwards of    
training accuracy  compared to     testing accuracy  
the bow model is therefore not generalizable and we
turned to synset features to reduce the overfitting 
using synsets in place of words further improves the
values of all three metrics most notably in the form
of an additional    increase in accuracy  we also no

figure    performance of different models on max label task 

   

label presence task results

we used the same models in the label presence task as
the max label task  once again we established a naive
baseline  in this case the naive baseline is to just predict that a label is present for all confessions if it is
present for the majority of confessions  otherwise the
baseline predicts the label is not present for all confessions  the baseline accuracy is      results are
reported in figure   and values are the unweighted averages across the five categories  we use unweighted
rather than weighted average to account for the unbalanced distribution of votes across categories 
once again the basic bow model is able to beat the
baseline  accuracy increases to     along with a signficant increase in precision  in contrast to the max label
task  when we move to synset features we actually see a
decrease in performance compared to bow  however 
when we added lexicon features to synsets  this model
was the best  achieving a high accuracy        precision        and recall        therefore the best model
for both tasks is to use synset and lexicon features 

fifigure    human comparison for max label 
figure    performance of different models on label
presence task 

 
   

discussion
comparison to human prediction

one might ask what is the difficulty of our two tasks
and what level of accuracy would be considered successful  to answer the question of how hard the two
tasks are  we can compare our systems performance
against that of humans  we conducted an scaled down
version of the experiment where we had humans attempt the same two classification task as our models 
performance at the human level is often considered the
target goal in sentiment analysis  notice that we are
not asking humans what are their reactions  we are asking them to predict what they think other people would
have voted on 
we provided training examples and asked two human subjects to perform the two tasks on    testing
examples for each task  figures   and   show the performance of our system against two human subjects   
for the max label task our system had lower accuracy
than humans while on the label presence task our system had slightly better accuracy than humans  on the
whole  performance of our system approaches the level
of humans  a more interesting finding is that both tasks
are difficult for humans as well  the max label task is
especially challenging and neither human subject was
able to reach     accuracy 
   

other attempts

in addition to what we used in our final model  we had
other work that taught us more about extracting emotion from ep 
 
for the max label task  due to the unbalanced distribution
of categories we used a balanced human testing set instead of
a random subset of the original testing set  note that this is a
harder problem for our svm classifier since it was trained on
an unbalanced training set  as a result the numbers reported
here are lower than the ones reported in results 

figure    human comparison for label presence 
      naive bayes
initially we worked with both naive bayes and svm
classifiers  however  due to the unbalanced distribution
of categories  naive bayes tended to classify the vast
majority of test examples as the most popular category 
we were unable to correct for this and dropped its use
in favor of svm 
      latent dirichlet allocation  lda 
the idea that a given topic can elicit a given emotion
can be useful to predict the presence of categories  one
possible feature model is to select the topic of the confession  and find the vote distribution for that particular
topic  if the confession contains a single topic  we can
model the probability of a reader selecting vote category v of confession k as 
x
p k   v   
p v ti  p k   ti  
i

where ti is each topic  and p k   ti   is the probability of the topic occurring in confession k  in the max
label task  we find the v that gives maximum p k   
this formula also works if the article contains multiple topics  by assuming that p k   ti   is the proportion of the article containing topic ti   and that the relationship between topics and vote distributions is linear  we create a heuristic in which we find a least
squares estimate of the parameters p v ti    assuming

fi k 

p k   v      v   vmax    in order to increase the difference between the maximum category and the others 
lda allows us to estimate the presence of topics
 blei et al          whether it be the likelihood of a
particular topic in each document  or the proportions
of various topics within each document  thus  we ran
lda on our documents after the preprocessing to get
a new set of features over bow  with various numbers
of topics  next  we took the derived features and tried
our heuristic  first we ran gibbslda      varying the
number of topics assumed to be in the collection  next
we used our heuristic on the predicted p k   ti   in each
scenario  our resulting test accuracies are shown in figure   
in addition  we tried to improve our feature set by
including lda derived features  we ran an svm on
the combined features but found that this did not give
significantly better performance 

be too neutral or too general to actually be good indicators of mood  for example  one of the topics found
with lda turned out to contain the topic about relationships  it is possible for someone to complain angrily
about their current relationship  cry over the impending
end of a relationship  or laugh because of a happy moment during the relationship  all of these get mapped
into the same topic  but each has a substantially different mood 

 

acknowledgments

the authors would like to thank josh falk for his help 
and professors dan jurafsky  andrew ng  and chris
potts for their guidance 

references
cecilia ovesdotter alm  dan roth  and richard
sproat        emotions from text  machine learning
for textbased emotion prediction  in proceedings of
human language technology conference and conference on empirical methods in natural language
processing  pages        
plaban kumar bhowmick  anupam basu  and pabitra
mitra        reader perspective emotion analysis
in text through ensemble base multi label classification framework  computer and information science 
           november 
david m  blei  andrew y  ng  and michael i  jordan 
      latent dirichlet allocation  journal of machine learning research             january 

figure    accuracy using lda derived features 

 

conclusions

overall  the success and failures of all these different
approaches gave us a good overall picture of the challenges of sentiment analysis on the experience project 
and provide some guidelines for sentiment analysis
with other sets of data in the future  first  we note the
use of colloqiual and slang language in most of the confessions  the use of spell checking corrected for this
somewhat  nonetheless  the synset and sentiment lexicons we used are better suited to more formal styles
of writing  an alternative approach is to replace our
synsets and lexicons with slang versions or even the
automatic generation of sentiment lexicons on a slang
corpus 
another area of interest is the difficulty in correlating topics with sentiment  intuition says that topics
themselves should portray different sentiments  and so
should be useful for sentiment analysis  this method
turns out to be fairly crude  as sometimes topics may
 

http   gibbslda sourceforge net 

moshe koppel and jonathan schler        the importance of neutral examples in learning sentiment 
computational intelligence               
rada mihalcea and hugo liu        a corpus based
approach to finding happiness  in aaai      symposium on computational approaches to analysing
weblogs  pages         aaai press 
alena neviarouskaya  helmut prendinger  and mitsuru ishizuka        recognition of affect  judgment  and appreciation in text  in proceedings of
the   rd international conference on computational
linguistics  pages        
james w  pennebaker  roger j  booth  and martha e 
francis        linguistic inquiry and word count 
liwc     operators manual  university of texas 
ryan rifkin and aldebaro klautau        in defense of one vs all classification  journal of machine learning research            december 
philip j  stone  dexter c  dunphy  marshall s  smith 
and daniel m  ogilvie        the general inquirer 
a computer approach to content analysis  mit
press  cambridge  ma 

fi