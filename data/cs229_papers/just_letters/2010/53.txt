storage efficient nl means burst denoising for programmable cameras
brendan duncan
stanford university

miroslav kukla
stanford university

brendand stanford edu

mkukla stanford edu

abstract
an effective way to reduce noise in images involves taking a burst of snapshots  aligning them  and averaging them
together  however  a burst of photos takes up a lot of memory  and most users only take single photographs  in this
paper  we examine a novel way to denoise photos using only
select regions from a burst of snapshots  first  we train an
svm to recognize image regions that stand to benefit the
most from burst denoising  then  whenever a burst of images is taken  only those regions selected by the svm as
beneficial are stored  on a programmable camera  such
as the frankencamera  these regions can be selected and
stored automatically  finally  a non local means denoising
algorithm is performed offline  where the select regions are
leveraged to improve the original image  the end result is
noise reduction comparable to burst denoising  without the
associated storage cost 

   introduction
an equation for signal to noise ratio in digital cameras is
given below 
p qe t
sn r   p
 
p qe t   dt   nr 
where p is the number of photons per second  qe is the
quantum efficiency  t is the exposure time  d is the dark
current noise  and nr is the read noise  noise that is constant across images is not shown in the equation because it
can be calculated and removed easily 
the above equation shows that increasing the exposure
time will increase the signal to noise ratio  however  it is
not always possible to increase the exposure time  for example  increasing the exposure time of a handheld camera
can result in a blurry image due to camera shake  instead 
the photographer can take a burst of short exposure images 
align them  and average them together  since averaging together several exposures is effectively increasing the exposure time  the equation shows that this will increase the signal to noise ratio 

while this avoids the pitfalls of simply increasing exposure time  aligning and averaging entire images can introduce motion blur and ghosting effects  moreover  it is
expensive to store an entire burst of photos 
in this paper  we introduce a new technique which stores
a representative subset of image regions from the entire
burst of photos  because we are using small regions instead
of entire photos  this will reduce ghosting effects caused by
motion across the images  also  by storing only a subset of
image patches  we address the problem of memory usage 
we use an svm to determine which patches are the
most important to store  and collect patches from different
regions of the image to encourage variety in the patches 
these patches can then be used to perform non local means
denoising 

   previous work
the bilateral filter     can be used to denoise images
while preserving edges  this approach  however  still has
the disadvantage of performing spatial filtering in a neighborhood  which results in textures being smoothed  this
results in an undesirable loss of texture  and leads to poor
results around edges 
another simple denoising technique is non local means
 nl means       which instead averages together pixels
with similar surrounding regions  the proximity of the pixels is not taken into account at all  this is an effective technique because it reflects the fact that repeated patterns are
often found in separate regions of an image  and thus similar pixels may not be in the same immediate neighborhood 
nl means is also more effective at improving noisy pixels  since an entire neighborhood is used to determine pixel
similarity  rather than just a single noisy pixel value 
burst denoising and video denoising methods  such as
    and      are also common  one advantage of these
methods is that temporal filtering is used  which reduces
potential blurring  when the images in the burst are properly aligned  these techniques effectively increase the exposure time in the aforementioned snr equation  while
these techniques deal with motion blur and ghosting artifacts  they are expensive because of the need to store an

fientire burst of photos 
in addition to these traditional approaches to deniosing 
there are some existing techniques which explicitly use machine learning to denoise photos  one of them  by yang et
al       trains an  svm to approximate a bilateral filter 
our approach leverages the predictive power of machine
learning to supplement the aforementioned nl means algorithm  and applies the nl means algorithm in a novel way 
     

bilateral filter in detail

the bilateral filter formula is given below 
  x
gs kp  qk  gr kip  iq k  iq  
bf  i p  
wp
qsp

where g is the gaussian function
x 
 
g x     e     
  
p and q are x  y pixel coordinates and ia is the pixel value
at pixel coordinate a  the gaussian functions gs and gr
provide the weights for the weighted average of the set sp
of pixels iq surrounding pixel ip   wp is the sum of the
gaussian weights calculated for each pixel in sp   w p is the
normalizing term  
again  the bilateral filter is useful in that it can reduce
noise while preserving edges to some degree  however  because it performs spatial filtering  there will be some blurring of more complex regions   namely  textures and edges 
by training our svm to determine the difference between
the bilateral filtered and high quality images  we can isolate
these complex regions and store them for denoising 

for each pixel  we obtain an n   dimensional vector 
comprised of the pixels values of the surrounding n  n
region  this n   dimensional feature vector is then mapped
to a value z  which is the corresponding pixel in the difference image  to perform this mapping  we learn a mapping
function using  support vector regression      given a
set of m training examples    x    z           xm   zm     where
 
xi  r n   is a feature vector and zi  r is the corresponding target variable  training our  svm requires solving the
following optimization problem 
pl

  p  

min 

   wt w   c

s t 

wt  xp     b  zp     p
zp  wt  xp    b     p
p   p     p           l

w b  

p    p

predictions within  of the true value are not penalized  c is a constant term penalty for predictions that are
not within  of the true value  and p   p are slack variables that control the upper error bound  we used the support vector regression provided with svm lib     
and a simple linear kernel  for speed purposes  we ran   fold cross validation multiple times to select the appropriate
parameters  and the parameter values we decided on were
c            

   methodology
     obtain training data
first  we obtained an image pair consisting of a noisy image and a high quality image of the same scene  to do this 
we used a tripod to take a burst of high iso photos  then
averaged these images together  a single high iso image
serves as our noisy image  and the averaged image serves
as our high quality image 
we then perform bilateral filtering on the noisy image 
with s     r         see figure     we subtract the bilateral filtered image from the high quality image and square
the result  so that we get an absolute measure of how far off
the bilateral filtered estimate is from the high quality photo 
we call this the difference image  see figure     we will
want to train our svm to predict difference images 

   noisy input image

     train an svm to find important patches
we define an important patch as an n  n region that
cannot easily be denoised using a simple bilateral filter 

figure    difference image  red   high  blue   low 

fi     use the svm  take a burst of images  and store
the important patches
take a burst of images  the first image will be stored
in its entirety  we call this the this the base image  and
the other images the support images  our trained svm
will examine each support image and determine some x
important patches for each image  these patches will be
stored  and the support images will be discarded 
to encourage variety in the x patches  we split each support image into k subregions  such that the top x important
patches are distributed evenly among the subregions  for
our tests  we split up the images into      subregions 
it would be expensive to have the svm make a prediction on each patch in each region  so we instead choose
patches at random within each of the k subregions  our
svm then determines the top x k most important patches
it encounters in each of these subregions  these regions will
be stored  the rest will be discarded 

figure    zipper ground truth difference image

     piece together the final denoised image using
nl means
we now have the base image and x important patches 
we will employ this collection of important patches to denoise our base image using a modified nl means algorithm 
for each pixel p in the base image  we construct a vector
 
vp  r n     we compare these vectors to all patches in
the set s  which consists of surrounding patches in the base
image and the set of important patches determined in the
previous set  the pixel is then set to be the weighed average
of pixels with patches similar to its own  this calculation is
performed offline 
the formula for the weighted average is as follows 
rp  

figure    zipper svm difference image

  x
gr kvp  xk    x
wp
xs

   svm results
we trained our svm on the          image depicted
in figure    our features were the      patches that surround each of the pixels  and our target values were the
corresponding values in the difference image  depicted in
figure    again  our svm was trained using using c    
and       
below are the results of svm predictions  along with the
ground zero truth images   that is  the actually difference
images  each of these images is individually scaled to better show the relative importance of each image region  the
zipper image was          pixels  the text image was
         pixels  and the face image was           
pixels 

figure    text ground truth difference image

figure    text svm difference image

fifigure    noisy base image

figure    face ground truth difference image

figure     bilateral filtered image

figure    face svm difference image

we thought that these predictions looked quite good  especially given the small size of the training image  our
svm was especially good at recognizing edge regions  almost all of which were important  we would have liked
more importance to be placed on textures  but our difference
images do not weigh textured regions very heavily  also 
we had to use a linear kernel since running higher dimensional kernels took prohibitively long  in light of this  we
were surprised by the effectiveness of the svm 

figure     our result

   denoising results
here is our denoising result for the zipper image  we
used    support images  and our algorithm determined a
subset of the important regions which took up roughly one
pictures worth of storage  for the nl means  we used   
     

figure     high quality image

fi   discussion and future work
we thought that our result looked noticeably sharper than
the bilateral filtered image  it clearly performs less blurring
across edges  however  when compared with a simple nlmeans algorithm on the base image  our results were only
very slightly better in terms of mean squared error  there
were slight or no visual difference  this leads us to conclude that  because nl means is already an effective offline
denoising algorithm  the additional regions detected by our
svm are not necessary for offline denoising  we hypothesize that the extra regions may be helpful for a scene that
contains many unique regions  since our images had repeated patterns  however  we were not able to test this case 
although our regions were not very helpful for offline
nl means denoising  they may be more useful for online denoising  if we store these patches in a kd tree or
other data structure that allows quick searching for similar
patches  it may be possible to denoise images on the fly
in the camera  using only important regions  since running
nl means on a camera would be too expensive 
in summary  we found that our trained svm was very effective at recognizing those regions of an image for which
bilateral filtering is inefective  using these extra regions to
perform nl means denoising significantly reduced noise 
and our approach noticeably outperformed the bilateral
filer  however  our results were very similar to those obtained when performing nl means on the base image alone 
which already closely approximates an aligned and averaged burst of snapshots  we conclude that providing the
nl means with additional image regions from a burst of
photos is mostly unnecessary  nonetheless  the effectiveness of the svm at predicting important edge and texture
regions is noteworthy  and while using these regions to assist an nl means algorithm proved unnecessary  this application of svms might be relevant to related problems in
computational photography 

references
    e  p  bennett and l  mcmillan  video enhancement using perpixel virtual exposures  acm siggraph      papers       
    a  buades  b  coll  and j  morel  a review of image denoising
algorithms  with a new one  multiscale modeling and simulation  a siam interdisciplinary journal                 
    c  c  chang and c  j  lin  libsvm  a library for support
vector machines       
    h  j  seo and p  milanfar  video denoising using higher order
optimal space time adaptation  in proceedings of ieee international conference on acoustics  speech and signal processing       
    c  tomasi and r  manduchi  bilateral filtering for gray and
color images  in proceedings of the sixth international conference on computer vision       

    v  vapnik  the nature of statistical learning theory 
springer  new york       
    q  yang  s  wang  and n  ahuja  svm for edge preserving
filtering  in cvpr       

fi