toward mechanized music pedagogy
lingfeng yang
stanford university

abstract
we can adapt existing techniques for the statistical modeling of music to the modeling of the quality of musical performance  resulting in a fine grained statistical understanding
of hardness in music performance  this drives ai assisted
applications for improving skill  we present modeling results
using various probabilistic models to model performance in
the simplified setting of rhythm games 

 

introduction

learning a musical instrument is hard  individually  thousands of dollars in fees are spent taking lessons  and years
spent practicing at a suboptimal level  practice without
constant expert supervision often involves neglecting the real
technical shortcomings and worse  learning the wrong habits 
the rare state in which we are addressing our shortcomings
and learning the right habits is known as deliberate practice
 ericsson et al         how can machines help us be in this
state more often by automatically pointing out what needs
work through the statistical modeling of player performance 
first is the educational value of having the statistics alone 
for example  if we know we miss a note sequence particularly
often through the statistics  we become better informed of
what note sequences to practice  second is the automation
of secondary pedagogical elements such as practice songs  by
obtaining the most likely to miss note sequences  we can use
techniques from algorithmic music to synthesize a practice
song that specifically targets a particular technical weakness 
third  is an objective  detailed metric of improvement  if
an accurate statistical model formed for predicting missed
note sequences becomes increasingly inaccurate for data the
player feeds it in the future  presumably after practicing and
getting better  we can tie technical improvement to the lowered empirical probabilities of missed note sequences versus
the predicted probability 
in this paper we present how the variable order markov
model  a model often used to statistically model music  may
be used to form similarly detailed models of the quality of
musical performance 

 

related work

this is an extension of the work in  yang       
music cognition  the most related field would be in music cognition  music cognition is concerned about better
understanding how people relate to music  recently  there
has been some interest in modeling music cognition  honing
       it has not been established yet what good models
are in this space  seeing how well our proposed models predict player performance may lead to a better understanding
of how hard songs are hard and easy ones  easy 
video games and player modeling  the next most related
field is in video games  recently  there have been academic
research efforts in modeling players of video games   pedersen et al        discusses how to model fuzzy concepts such
 e mail 

lyang cs stanford edu

as fun and satisfaction in platform games  there is also
work in modeling player performance  but for different game
types   drachen et al        
algorithmic music  finally  the models we plan on using
can also be found in algorithmic music  which is concerned
with the modeling and generation of music itself   conklin
      provides a survey of statistical techniques   mccormack       gives an overview of music generation techniques 
there is also work in using statistical models to predict the
names of songs  brochu and de freitas        in particular 
we propose to adapt such methods from modeling or generating music to predicting player performance  broadly  we
are interested in gaining a statistical  music theoretic understanding of skill development in musical performance 
markov models  markov models have traditionally been
used to obtain detailed statistics of sequential information 
including music  the simplest kind of markov model is the
markov chain  where each member of a sequence of random
variables is conditionally dependent on the previous member  variable order markov models  vmms   the class of
model used in this work  are an extension of markov chains
that allow context specific conditioning on contexts of varying length  we draw our methods heavily from  begleiter
et al        which gives an overview of the available techinques for training vmms  other literature covering vmm
training exist  such as  schulz et al         a recent development is markov random fields  which additionally generalize
the shape of conditioning contexts  in fact  markov random
fields were used recenty in  lavrenko and pickens       to
model polyphonic music 

 

problem domain

in a rhythm game  the player hits notes by pressing buttons
on an input device  each button on the input device corresponds to a note  these input devices tend to be modeled
on actual instruments  but the number of notes they encode
is much lower  commonly  the number of notes is around   
during the course of the game  the players goal is hit
incoming notes in time to the music  i e   after starting a
game  at each point in a sequence of predetermined time
intervals  the player has the chance to hit a specific note
within a timing window  at high levels of play  this can
become very difficult  it is common for a rhythm game to
replicate the notes in a technically difficult real musical piece
one for one 
we can formalize these concepts as follows  first is the
representation of songs in the rhythm game  a song is a
sequence of  note  time  pairs 

s     si    ni   ti    ni  p  ti  r  ti  ti    n
i    
where n is the number of notes in the song and each ni
is from a finite set of pitches p                  h   we may
represent the players input as a similar sequence

fiuser input

missed notes as a skill metric  in this setting we define skill
as the ability to reproduce the note sequence accurately  i e  
the more notes missed relative to the same song  the lower
the skill of the player  however  there are still many different
ways in which to measure whether a note is missed 
one way is a timing measurement  for each si    pi   ti   
i such that there is no sj    nj   tj    s within a timing
window of t  of ti    ti  tj     t    
j  in general  this is
a boolean predicate ps i   s          note that this does
not capture all the ways of missing a note  in particular  it
does not capture the case where the player plays too many
notes 
this then leads to a labeled version of s  si  

labeled reference

i      ni   ti    ni  n  ti 

reference

timing window

ti    m
i    

a
b
c
a
b
c
a
b
c

time
si      ni   ti   p  si     si  s  ni  c  ti  r  ti  ti    n
i    
miss rate  we can then consider si as arising from some
probability distribution p over all xi  si   we formulate
the miss rate at each note  the context specific probability
that the player will miss a note  based on this notion 
mi   p   xi        x        xi   
a statistically fine grained notion of miss rate is important because it allows for more effective applications for skill
development  note that we do not yet deal with polyphonic
music  where  ni   ti   pairs with the same ti are allowed  currently being investigated is adapting the markov random
field method for collecting statistics on polyphonic music
 lavrenko and pickens       to collecting statistics on performance quality taking polyphonics into account 

 

system overview

to serve as a platform for this research  we have developed a
rhythm game that is a spinoff of the beatmania iidx  konami       rhythm game  it augments the regular game with
a data collection mechanism  the game is compatible with
existing file formats used by the beatmania modding community  with this  we may test our ideas on a catalog of
songs that are already available 
in a typical session  the application may be played like
the original beatmania iidx  the player starts the game and
selects from a menu of songs to play  plays the song  and can
optionally play another song  unlike most rhythm games 
however  detailed data is collected per play  after playing
each song  the sequences s labeled with missed notes from
i and the timestamp of the song along with text delimiting
the play session is appended to a log file 
in addition to the data collection are features that represent explorations into how we can apply the results of modeling the data to assisting skill development  we include a
facility for generating practice songs that consist of repeats
of the most likely note sequences that result in a missed note 
as the scope of this paper deals with the modeling aspect 
we will not go into it here 
labeling of data  chordal and other timing information is
then stripped to produce a string with alphabet consisting
of consecutive hit miss symbols si   hn  m n  where n is
one of p musical pitches 
ha m b hc ha m c hb m c ha m c      

figure    we take a reference song  blue  top  and user
input over that song  green  middle  to obtain a labeling of
the reference song with each note as hit  blue  or missed  red 
 bottom   here we give an example for   pitches  a  b  c  
we use   pitches in our actual system 

the miss rate at note i may then equivalently be calculated as
mi   p  si   m n pitch si     n  s        si    

 

modeling

in the setting of mechanizing music pedagogy  there are three
aspects we require of any model that can realize the probability query above 
   the model makes accurate predictions  this is clear 
the query given above should be able to predict what
notes the player will get wrong 
   the model generalizes  the query given above should
be able to perform these predictions on data that is not
from the training set 
   the model is resilient to data starvation  because
we target individual players  one should not be forced
to generate a vast dataset of performances before the
model is usable  the query should give accurate results
from a single player on a single playthrough of a single
song 
an established method for modeling probabilities over
strings is the markov chain of order k  also known as the
k gram model  each consecutive occurence of k symbols in
the string is matched against the symbol that comes next 
by accumulating a list of  k sequence  next symbol  pairs 
one also accumulates the conditional probability
p  si  si        sik   
these models can be very accurate  however  in this case
we are interested in the contextual miss rate from the beginning of the entire song  if we were to use fixed order markov
models  this would require multiple models of very high order  for a typical song  in the hundreds   this would require

fian enormous amount of performance data from each player 
as this means the model is not resilient to data starvation
we cannot use a k gram model for our purposes 

abracadabra abracadabra abracadabra abracadabra
a
a b
a b r

 

abracadabra abracadabraabracadabra abracadabra
a b r ac a b r ac a b r ac a b r ac
ad
ad ab
ad ab ra

variable order markov models

because fixed order models cannot be applied practically to
our modeling problem  we turn to variable order markov
models  vmms   vmms are an extension of fixed order
markov models wherein one is allowed to condition on any
context of arbitrary length  not just those of a fixed length 
as is the case in fixed order markov  n gram  models 

figure    the lz    algorithm applied to the string abracadabra 

agafaeadacadaeafaaabaca

a
b
c
d
r

agafaeadacadaeafaaabaca
agafaeadacadaeafaaabaca



agafaeadacadaeafaaabaca
figure    a possible vmm for a string 

a
b
c
d
r

allowing conditioning on arbitrary length contexts lets
one concentrate the probability mass to contexts of different
lengths  potentially resulting in much better accuracy and
generalization while requiring much less data 

   

a
b
c
d
r

a
b
c
d
r

a
b
c
d
r
a
b
c
d
r

a
b
c
d
r

a
b
c
d
r

a
b
c
d
r

training the vmm

several algorithms exist for learning variable order markov
models  essentially what needs to be done to learn a vmm
is to pick a context to match to each symbol in the alphabet  and then learn the probabilities using these  symbol 
context  pairs  the variation is in how contexts are selected 
to gain high accuracy and generalization  good vmm
learning algorithms generally learn contexts that maximize
the likelihood of the training sequence xt    x        xt  

p  xt     

t
y

p  xi  x        xi    

i  

the likelihood is taken to be the joint probability of all
symbols in the sequence occuring given all previous symbols 
it is not trivial to determine which contexts will do this
given finite computational resources  several algorithms select these contexts based on information theoretic principles 
in fact  any lossless compression algorithm that is based on
storing a dictionary of phrases additionally induces a vmm
over the input sequence  begleiter et al        
the lz    algorithm  one such compression algorithm is
the lz    algorithm  which compresses well  acheives high
likelihood  while also being extremely fast  the lz    algorithm works as follows  see figure     a string is consumed
from beginning to end  building up a phrase dictionary  the
shortest phrase not in the dictionary is parsed and inserted
into the dictionary at each step 
the lz ms algorithm  the lz ms algorithm is a variant 
it has two parameters  m and s  lz ms performs the lz   algorithm on s      symbol shifts of the input string
and backtracks by m symbols each time a phrase is parsed 
accumulating a more complete dictionary in the process 
further details are available in  begleiter et al         we
employ the lz ms variant in our vmm learning algorithm 

figure    the decision tree corresponding to the phrase
dictionary of abracadabra 

from phrase dictionary to decision tree  the lz    algorithm produces a set of phrases given an input string  there
is a corresponding tree from which conditional queries of any
context length may easily be calculated  we may consider
the phrase set as being generated by taking nondeterministic
walks in the tree up to one level before the leaves  further
details of how this is works is available in  begleiter et al 
      
consider the phrases  a  b  r  ac  ad  ab  ra   the first character in each of the phrases must be from the alphabet
 a  b  c  d  r   hence the first level in the tree is  a  b  c  d  r  
next  we consider the second character given each of the first
characters  given a  it can be  c  d  b   and given b  there are
no choices  and given r  it can be  a   we proceed in a
similar fashion until all the phrases in the dictionary are exhausted  then add another alphabet  a  b  c  d  r  to the tree 
an example of a conditional probability calcuated with walks
in the tree 
cond n    c n  

x

c s 

ssibs n  n 

p  b ar    cond   a  r    b        

 

results

a test of the lz ms based vmms prediction accuracy and
generalization was run on a single playthrough of a single
song  the resulting labeled string comprised     symbols
in length  the vmm was trained on     symbols with a
contiguous section of     symbols left out to serve as the
test set 

fireceiver operating characteristic
 avg performance in leave     out cross validation on single     note playthrough
   

true positive rate

   
   

random performance
vmm lzms
vmm lzms context 
  gram
  gram
  gram
  gram
  gram

   
   
      

   

   
   
false positive rate

   

   

figure    the receiver operating characteristic of the
lz ms vmm model on the prediction query p  xi  
m n pitch xi     n  x        xi     note the inferior performance of both the lz ms vmm model and   to   gram models on the simplified fixed order k prediction queries p  xi  
m n pitch xi     n  xi        xik   

receiver operating characteristic  in figure   we see that
the algorithm achieves better than random performance in
its receiver operating characteristic  which is obtained by using the predicted probability p along with some threshold
probability          where if p     we decide that the note
was missed  and if the note actually is some m n  it is a true
positive  else it is a false positive  the receiver operating
characteristic is the curve obtained by plotting true false
positive performance over several           the same evaluation metric is used in  lavrenko and pickens        we
believe a similar metric is appropriate for predicting missed
notes as it is for note occurences in general 
comparison with other models  there are two decisions
we made in our setting that demand comparison  one is the
choice of vmms over fixed order markov models  another is
the choice of conditioning starting from the very beginning
of the song
p  xi   m n pitch xi     n  x        xi   
which is a variable order query  versus on some fixed order
context
p  xi   m n pitch xi     n  xi        xik   
in figure    we see that prediction accuracy and generalization ability of the lz ms vmm algorithm is crippled
when given the fixed size query  while fixed order models do
better with the fixed size query  they do not do as well as
the lz ms vmm algorithm on the variable order query  finally  the ability of the variable order query to predict note
misses is demonstrated to be superior to that of the fixedsize query  at least with these particular realizations  vmms
and fixed order markov models  
we do not include fixed order model results on the more
variable order query because it requires the learning of an
order at least     markov model  which in this setting would
clearly not work due to data starvation  running such a

model on our training set resulted in uniform answers  as
the model learned a very low probability of seeing any note
conditioned on a length     context 
certainly other alternatives exist  like smoothed combinations of fixed order models and alternative ways of learning
vmms  they are not included because we consider these
essentially vmms  and our comparison is between vmms
and non v mms  not among vmms 
performance  the faster performing a learning algorithm
is  the better it can be used in a real time application such
as rhythm games  it is better to get feedback in form of an
updated distribution quickly after playing each song  for
the     note training set referenced in figure    the learning algorithm constructed the phrase dictionary and decision
tree on the order of tenths of seconds on an      ghz intel
xeon  using   core   miss rate queries approached real time
performance  on the order of hundredths of seconds   this
performance profile allows the learning algorithm to be used
for every song that the player plays  and the prediction algorithm for individual notes in a playthrough  the learning
and prediction algorithms were implemented in haskell with
little regard to space leaks from lazy evaluation and minimal
memoization  even higher performance should be in reach
through program optimizations 

 

limitations and future work

augmentations to the system  one natural next step is to
push the software out to more users and have the collected
data be available in a cloud services like fashion  this would
enable the collection of data at a large scale  in turn enabling
the usage of learning algorithms that require much more data
but can potentially be much more descriptive  in particular  it would enable a player to compare performance versus
other players  and to see how other players improved their
technique  in terms of how the learned statistics changed for
another player based on what songs she played 
markov random fields  besides not being able to deal with
polyphonicity  the criteria of vmms that a context be contiguous is quite limiting  there is potential to use a better
model  we are currently evaluating markov random fields
using the method of  lavrenko and pickens        the drawback of this method are that it is much slower than using
vmms with lz ms  it could only be used in an offline manner  however  it may prove to be more accurate  the features learned by the method may also be qualitatively insightful to the musician interested in improving technique 
interactive machine learning  a potentially useful augmentation to any context based probability model  which relates
to interactive machine learning  is to let the contexts be selectable by the user  fixing a set of them  and then selecting
the rest based on some other algorithm  this is because often the user has a pretty good idea of what kinds of musical
contexts they make mistakes in 
other applications  currently considered applications include the automatic synthesis of hard enough practice songs
and tools to track technical progress at a fine grained level 
what other applications are there 
finally  the general area of ai assisted human learning is
a promising research direction  weve spent a lot of time
teaching computers how to do things better so much that
the computers might as well be teaching us 

fireferences
begleiter  r   el yaniv  r   and yona  g        on
prediction using variable order markov models  journal
of artificial intelligence research               
brochu  e   and de freitas  n         name that
song  a probabilistic approach to querying on music
and text  advances in neural information processing systems           
conklin  d        music generation from statistical models  in proceedings of the aisb      symposium on artificial intelligence and creativity in the arts and sciences 
citeseer       
drachen  a   canossa  a   and yannakakis  g       
player modeling using self organization in tomb raider 
underworld  in proceedings of the ieee symposium on
computational intelligence and games  cig       milano  italy  http   www  itu  dk  yannakakis cig   ioi 
pdf 
ericsson  k   krampe  r   and tesch r
omer  c        the role of deliberate practice in the
acquisition of expert performance  psychological
review new york               
honing  h        computational modeling of music cognition  a case study on model selection  music perception
              
konami        beatmania iidx 
lavrenko  v   and pickens  j        polyphonic music modeling with random fields  in proceedings of the
eleventh acm international conference on multimedia 
acm         
mccormack  j        grammar based music composition 
complex systems             
pedersen  c   togelius  j   and yannakakis  g       
modeling player experience in super mario bros  in ieee
symposium on computational intelligence and games 
      cig              
schulz  m   weese  d   rausch  t   d
oring  a   reinert  k   and vingron  m        fast
and adaptive variable order markov chain construction 
algorithms in bioinformatics         
yang  l        modeling player performance in rhythm
games  to appear in  siggraph asia      technical
sketches 

fi