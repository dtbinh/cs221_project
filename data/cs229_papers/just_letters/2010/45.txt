soft co clucstering via extension of plsa
dakan wang
 

introduction

in recent years  co clustering has emerged to be a powerful data mining tool for
two dimensional co occurrence and dyadic real data  typical examples include
co clustering word document matrix  user query matrix  etc  co clustering algorithms always outperform the traditional one way clustering in problems with
sparse and high dimensional co occurrence data  since they simultaneously cluster rows and columns of a matrix  co clustering has been applied to recommendation systems  microarray analysis  etc 
there have been already many works on algorithms for co clustering           
    proposed an information theoretical co clustering algorithm  however  their
method is under the hard setting  i e   it just allows one row or column to belong
to only one cluster  such restrictions are not suitable in real world applications 
in real life  one object can belong to more than one categories  and a soft coclustering framework allowing mixed membership may make more sense  it is
worth pointing out that     also proposed a soft co clustering model  however 
their model is a bit complex  we will propose a very simple and intuitive model 
we would like to extend the plsa to a soft co clustering model which allows
that both columns and rows can belong to different clusters  the idea is that
we add two latent sets of nodes to the model  the two latent sets correspond to
the clusters for rows and columns correspondingly  the training of the model
is similar to plsa and is very efficient and easy to implement 

 

model

in this section  we will describe the   layer probabilistic latent semantic analysis  plsa  and a corresponding em algorithm 
following the idea of plsa  we assume that the probability of a word w
given a document d is defined as follows
xx
p w d   
p w z p z s p s d 
   
zz ss

here z is the word cluster  and s is the document cluster  this equation can
be interpreted as follows
 pick a document topic s from d with probability p s d 
 

fi 

 

 pick a word topic z from s with probability p z s 
 generate a word w from word topic z with probability p w z 
from the above formulation  we can write the likelihood function to be
x
l  
n di   wj   log p wj  di  p di  
i j

 

x

n di   wj   log

i j

 

x
i j

 

x

p wj  zk  p zk  sl  p sl  di  p di  

k l

n di   wj   log

x

q zk   sl  di   wj  

k l

p wj  zk  p zk  sl  p sl  di  p di  
q zk   sl  di   wj  

inference

we briefly summarize the em algorithm here  detailed derivations are similar
to those instructed in the class 

   

e step
q sl   zk  di   wj     p

   

p wj  zk  p zk  sl  p sl  di  
k l p wj  zk  p zk  sl  p sl  di  

m step
p sl   zk   di   wj     q sl   zk  di   wj  p di   wj  
p
i l p sl   zk   di   wj  
p wj  zk     p
i j l p sl   zk   di   wj  
p
i j p sl   zk   di   wj  
p zk  sl     p
i j k p sl   zk   di   wj  
p
j k p sl   zk   di   wj  
p sl  di     p
j k l p sl   zk   di   wj  

 

   

   

equivalence with information theoretical co clustering

    defined a co clustering algorithm from information theory perspective  if x
and y are respectively clustered into x
b and yb  the loss in mutual information is
b yb     d p x  y    q x  y   
i x  y    i x 

   

here d     is the kl divergence  and q x  y   is defined to be
q x  y    p b
x b
y  p x b
x p y b
y 

   

fi 

 

where
p b
x  yb   

xx

p x  y 

xb
x yb
y

p x b
x   

p x 
p y 
p y b
y   
p b
x 
p b
y 

   

now we prove the equivalence of our model and that in     under the hard
setting  we rewrite the likelihood function of our model to be
x
x
l  
n di   wj  
log p wj  zk  p zk  sl  p sl  di  p di  
i j

 

x

 

k l

n di   wj   log

x

i j

k l

x

n di   wj   log

x

i j

p wj  zk  

p zk   zl   p di  sl  p sl  
p di  
p sl  
p di  

p wj  zk  p zk   sl  p di  sl  

if we restrict the above equation to hard setting  we get
xx
x
l 
n di   wj   log p wj  zk  p zk   sl  p di  zl  
k

   

k l

   

di sl  wj zk

l

on the other side  in information theoretical co clustering      we want to
minimize
i d  w    i s  z 
xx
x
p di   wj   log p di   wj  

 

k

l

xx



k

l

di sl  wj zk

x

p di   wj   log p wj  zk  p zk   sl  p di  sl  

   

di sl  wj zk

notice that tbe first term is constant given a collection of documents and words 
thus we only need to maximize
xx
x
p di   wj   log p wj  zk  p zk   sl  p di  sl  
    
k

l

di sl  wj zk

which differs from our maximum likelihood formulation by only a constant
scalar 

 
   

experiments
datasets

in order to evaluate our algorithm performance more extensively  we conducted
experiments using two datasets from different domains  the first domain is

fi 

 

 
image
graphical
file
program
fotmat

tab     extracted topics
 
 
 
drive
god
israel
scsi
think
jew
mb
believe
arab
disk
christ
war
control athetist kill

 
space
nassa
orbit
launch
earth

the    newsgroups dataset  which is a text collection of approximately       
newsgroup documents across    different newsgroups   another dataset comes
from the wikipedia xml dataset  from which we had sampled a subset of  
document topics composing a total of       articles as what was done in     
some preprocessing has been applied to on the raw text data  we had firstly
performed the porter stemmer on terms and removed all the stop words from a
stop word list 

   

word and document clusters

our algorithm could return document and word clusters easily by doing a sorting
on the corresponding p w z  and p d s   in this section  we would list the   most
possible words in   topics from the    newsgroup dataset  such a table can be
directly constructed by sorting p w z   the results are shown in table    from
table   and the ground truth labeling  we could see that topic   to topic  
will correspond to comp graphics  comp sys ibm pc hardware  talk religion misc 
talk politics mideast and sci space  we could observe that the word clusters we
got are meaningful and representative of different topics 

   

document clustering performance

we compared our proposed  plsa in terms of clustering accuracy with two
baselines  itcc and ldcc  we conducted our experiment on the wikipedia
xml corpus to directly compare our model with the results in      the metrics
we used are precision  recall and f  measure calculated over pairs of points 
in our co clustering model  for two documentsp
d  and d    we determine whether
they belong to the same class by calculating s p s d   p s d    and if the value
is larger than      we consider d  and d  to belong to the same category  as
     we also tested the performance of our algorithm with the number of topics
set to be    and    
table   shows the result of comparisons  it shows that our algorithm could
significantly outperform the baseline methods  in terms of precision and f score  we also would like to add a side note that since our algorithm is based on
the more efficient plsa model  the computational complexity of our algorithm
is significantly lower than that of ldcc 

fi 

 

tab 
algorithm
 plsa
ldcc
itcc
 plsa
ldcc
itcc

   performance comparison
s
precision recall f  score
        
     
     
        
     
     
        
    
     
        
     
     
        
     
     
        
    
     

finally  we show the rearranged co occurrence matrix after co clustering 
we sampled a subset of    topics from    newsgroup dataset with       documents and        words  the left subfigure of figure   shows the original
word document matrix and the right subfigure shows the re ordered matrix by
arranging rows and columns according to the document cluster order and the
word cluster order  respectively  note that in the right subfigure  the clusters are
not falling around the main diagonal since that requires doing topic alignment
between word topics and document topics 

fig     co occurrence matrix before and after co clustering

references
    inderjit s  dhillon  subramanyam mallela  and dharmendra s  modha 
information theoretic co clustering  in proceedings of the ninth acm sigkdd
international conference on knowledge discovery and data mining  kdd     pages
      new york  ny  usa        acm 
    m m  shafiei and e e  milios  latent dirichlet co clustering  in data mining       
icdm     sixth international conference on  pages               
    hanhuai shan and arindam banerjee  bayesian co clustering  in proceedings of
the      eighth ieee international conference on data mining  pages        
washington  dc  usa        ieee computer society 

fi