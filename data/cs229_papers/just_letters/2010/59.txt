automatic virtual camera view generation for lecture videos
derek pang
sameer madan
serene kosaraju
tarun vir singh
cs     project report  fall     

abstract
classx  an interactive online viewing system developed at stanford university  currently offers
automatic tracking of the lecturer to create a
virtual camera view for students  however  a
tracking based camera view does not mimic a human operator naturally  to address this issue 
we propose a new learning algorithm to automatically generate a professional virtual camera
view by learning the behavior of a human camera operator  our algorithm is based on a human saliency model which predicts the viewers
center of attention and additional features which
drive the cinematic decision making process of
the system  experimental result reveals our
system can provide an effective camera view
that is almost indistinguishable from a humangenerated camera view 

   introduction
growing internet access  increasing network throughput 
improving computer hardware and enhanced video compression are providing a boost to inexpensive online delivery of lecture videos  however  most lecture capture systems depend on human camera operators as well as manual
work of post  production and online video publishing  thus
resulting in expensive solutions 
the classx lecture capturing system  mavlankar et al  
      developed at stanford university offers a solution to
these problems by offering an interactive lecture viewing
system  each viewer can interactively choose an arbitrary
region of interest  roi  for viewing on the video  apart
from allowing the user to control pan tilt zoom  classx
offers a tracking mode  the roi video streamed in tracking
mode is generated through automatic cropping and mimics
a human camera operator  similar to the approach in  nagai         this approach differs from prior work employing a camera that physically moves to track the lecturer or
multiple cameras that cover different regions  rui et al  
      bianchi        zhang et al          however current
classx s tracking algorithm always follows the lecturer 
and does not produce a natural camera view that is relevant to the viewers 
heck  wallick and gleicher  heck et al         have proposed a system called virtual videography for lecture
recordings  it allows a computer system to employ the
art of videography and mimic videgrapher produced video 

dcypang stanford edu
sameer   stanford edu
serenek stanford edu
tarunvirsingh stanford edu

while unobstrusively recording a video scene  similar to
classx   the system uses unattended  stationary video
cameras to record the event and automatically produce a
professional video sequence by simulating various aspects
of a production crews  motivated by the virtual videography system  we propose a supervised learning based algorithm to predict a professional produced camera view for
lecture videos  different from hecks system  our learning algorithm make use of a human saliency model which
predicts the viewers center of attention and other features
that motivates cinematic decisions in camera operation 

   system overview
in this project  we consider a simple virtual camera with
two degrees of freedom  horizontal position and a bi level
zoom level  the orientation of the camera is ignored because the position of the virtual camera is flexible to directly capture the scene along a horizontal axis  therefore 
the virtual camera will not capture the scene at an angle and will not introduce additional geometric distortion 
furthermore  since the trajectory of the lecturer is often
constrained in the horizontal direction  we can avoid moving the camera vertically  our camera also only supports
a bi level zoom factor for simplicity 
a system overview is shown in figure    our system will
first obtain an input video recorded from a static hd camera  as well as the locations of any writing boards in the
scene  then  our system will compute the relevant features  such as center of attention  lecturer position  to predict the expected camera operation and trajectory based
on supervised learning techniques  finally  we will perform
a consistency check and apply kalman filter to our result
to reduce irregularities and noises in our prediction  after
the camera view is decided  we can crop the relevant region
of the video and encode the video content for delivery 

   feature extractions
to successfully operate an automatic camera  the system
must learn where to capture the most relevant information
to the viewer and how to make certain cinematic decisions 
such as panning and zooming  in this section  we propose
a set of features that are easily extractable from a lecture
video and other system inputs for making robust camera
view prediction 

fiautomatic virtual camera view generation for lecture videos

cinematic decisions
prediction

extracted
features

visual saliency model
 itti et al        

feature
extraction for
camera
operations

lecturer tracking

predicted
panning

trainer

extracted
features

true camera
trajectory

trainer

true zooming
labels

feature
extraction for
camera
trajectory

board locations

true panning
labels

panning svm

camera trajectory
prediction

trainer

training set
with human operated camera

consistency check

regression

predicted
trajectory

kalman smoother
smoothed
trajectory

zooming svm
predicted
zooming
consistency check

video cropper

automatic camera view

figure    system overview
     lecturer tracking
one obvious feature to drive the cameras attention for
lecture capture is by tracking the location of the lecturer 
by having a set of target templates  we utilize conventional background subtraction  piccardi        and template matching techniques to track our target  in this case 
the position and velocity of the lecturer are the features
used for prediction 
     human saliency model for visual attention
one of the important rules in videography is to direct the
viewers attention toward what is important  katz        
without specifying a tracking target  a more generic approach is to predict general viewers visual attention on
a video sequence  a biologically plausible computational
model of human visual attention has been proposed by itti
et al  itti et al          in this project  we simplified ittis
model by only computing motion and color contrast features  skin color detection is also added to compute the
skin color response on a video frame because the presence
of people often dominates viewers center of attention in
a lecture video  after all features are computed for each
frame  we normalize each feature response between   and
  and combine them linearly to form a saliency map  a
binary threshold is then applied on the saliency map to
eliminate regions with weak salient response  finally  the
center of attention is calculated by taking the median of
the locations of the high response salient regions 
     features for cinematic decisions
the main challenge to automatically producing effective
lecture videos is synthesizing the decision making process

of human videographers  lecturer tracking and human
saliency offers features that suggest where the center of
action in a given scene is  however  they do not consider
certain features that influence an operators cinematic decisions in panning and zooming  for example  a lecturer
might abruptly walk away from and into a particular board
that he she is discussing  in this situation  the camera
should be focused on the board to avoid abrupt camera
movement  therefore  features from tracking will fail to
capture this cinematic decision  thus  we investigated the
following features for making a better panning decision by
avoiding the aforementioned problems 
 the distance between the current center of attention
and its moving average over multiple intervals this
measurement suppresses abrupt panning changes and
promotes long distance camera panning 
 shifts in center of attention  a fast shift in center ofattention implies panning should be invoked 
 long term turnaround time of a lecturer  a binary
feature that denotes whether the lecturer has returned
to a particular writing board given a threshold time 
 the distance between the center of attention and the
center of the nearest board  when the distance is
high  camera has a high tendency to pan way from
the board
the features for deciding zooming decision also use an analogous set of features 
 the distance between the lecturers position and the
center of the nearest board  when the distance is low 
the lecturer is likely to be staying in the same board
and therefore we should perform a zoom in operation 

fiautomatic virtual camera view generation for lecture videos
 the distance between the predicted trajectory and the
center of the nearest board  since panning and trajectory prediction are independent of zooming operation  we could make use of the trajectory generated
by our learning algorithm  the predicted trajectory
also provides a more stable trajectory compared to
the center of attention and the lecturers trajectories 
 distance of near future center of attention movement 
high distance movement indicates the viewing target is likely to be moving and the camera should be
zoomed out 
in addition  we have also studied the effectiveness of the
following features for both panning and zooming features 
 face detection  when a lecturer is writing and facing
toward the board  the camera should zoom into the
board for a better view of his writing  therefore  we
utilized a face detection algorithm proposed by  viola   jones        to detect whether he is facing the
board or the camera 
 eigen gestures  similar to eigenface  we apply the
principal component analysis  pca  analysis in our
training data and generate a set of eigen images for
recognizing the gestures that are often occurred during panning 
please note that the same set of features can be applied
to different classroom settings and can enable a learning
algorithm adaptive to different environment 

   cinematic decision prediction
     support vector machines for cinematic
decisions
the cinematic decisions  specifically panning and bi level
zooming  considered in our virtual camera operations can
be generalized as as classification problem  one approach
to solving classification problem is support vector machine
 svm   using our feature set as discussed in sec       a
panning svm and a zooming svm can output a bi level
decision for the panning and the zooming operation respectively on each video frame  to exploit the possible
co occurence relationships between each feature  we employ a gaussian kernel with l  norm distance  where the
parameters are selected by a five fold cross validation  we
can also substitute the gaussian kernel with a linear  polynomial  or a sigmoid kernel  but these kernels do not yield
the optimal performance in our application  a multi level
svm can be used for predicting multi level zooming operations if necessary 
     consistency check
the sequence output of our svm decisions is often noisy
and might affect the users viewing experience  thus  we
perform a consistency check to remove thin troughs and
flatten out spikes observed in our decision sequence  the
implications of such an action is very different in the case
of panning and zooming  in the panning case  it is always
better to favor a panning decision for preventing viewing
target moving out of the viewing regions  therefore  all
parameters in panning svm is optimized for higher recall

rate  conversely  in the zooming case  it is always preferable to perform a zoom out operation to ensure all vital
information is inside the viewing region  therefore  we optimize the precision rate for the zooming svm 
to rectify our outputs  we use two parameters which define
the minimum allowable duration of a spike and a trough to
eliminate sporadic changes in camera operations  to optimize the recall rate of panning decisions  we choose a high
minimum allowable duration for trough and a low value
for spike  in the zooming case  we choose a low allowable
duration for trough and a high value for spikes to obtain
a higher precision value  this rectification process ensures
the flow of our camera operation is consistent 

   camera trajectory prediction
a professional lecture capturing system must provide a stabilized camera view that captures the users region of interest  to ensure the same viewing experience in our system 
we predict the trajectory of our systems camera view based
on the features that indicate both the center of visual attention on a scene and the predicted camera operation that
controls the actual camera trajectory 
using the present and future information of the lecturers
position and the center of attention  we formulate a linear
regression model to predict where the camera would most
likely go to  however  this prediction does not totally reflect the operators decision on panning in a lecture setting
as discussed earlier 
to accurately predict a natural camera trajectory  we combine our regression prediction with our camera panning
prediction discussed in sec     when a non panning decision was made  the predicted camera trajectory would
move to the nearest board and stop until a panning decision was outputted  when a panning decision arrived 
the trajectory would follow the output of the regression
model for panning  after the camera trajectory is generated  a kalman filter can also be applied to reduce the
irregularities and noises in our predicted trajectory  the
parameters of the kalman filter can be estimated through
expectation maxizmation  em  algorithm  please refer to
appendix in our report for a full derivation of the kalman
filter 

   experimental result
     experiment setup
in our experiment  we used two board writing style lectures  each lecture is approximately    minutes long and
is composed of        video frames  for feature selection 
parameter estimation  and kernel selection  we used a fivefold cross validation by dividing each video lecture into five
continuous segments  for our performance evaluation  we
trained our model on a single video lecture and apply the
prediction on the other lecture video  we then switched
the role of both sets and obtained an average performance
of both testing sets  to extract our ground truth data 
we asked a human spectator to view the content and determine the appropriate camera viewpoints using a mouse
and a keyboard through a software interface on a pc 

fiautomatic virtual camera view generation for lecture videos
table    svm performance
panning zooming
precision
   
   
recall
   
   
accuracy
   
   
table    mse performance comparison
method used
mse
 
mse
reduction
lecturer tracking
           saliency based
center of                
attention prediction
proposed method without               
consistency check
proposed method with con                
sistency check
proposed method with
         
   
ideal panning labels
     feature selections
during our feature selection process  we found that face
detection and eigen gesture features actually harm our prediction performance because they actually do not have a
strong correlation with panning and zooming decisions 
for example  the lecturer often faces forward in both nonpanning and panning situations  furthermore  if we compare the eigen gestures of the lecturer in both of these situations  the eigen gestures exhibit similar characteristics 
therefore  we removed these features from our model 
     cinematic decision predictions
table   shows the performance of our panning svm and
zooming svm prediction respectively  as discussed in sec 
   the panning svm is optimized for recall and the zooming
svm is optimized for precision  in general  we found that
panning prediction is a harder problem because panning
also depends on the contextual information of the lecture
that our system cannot capture 
     camera trajectory prediction
table   reports the mean square error  mse  between
the ground truth trajectory and the trajectory generated
by different methods  we compare each method to the
tracking based trajectory used in the classx system  if
we only use the center of attention as our camera trajectory  we can reduce the mse by     compared to trackingbased approach  using our proposed method  we observed
a even larger reduction of     and     without consistency check and with consistency check respectively  we
also show an ideal lower bound for trajectory prediction
to be a reduction of     if we can perfectly predict the
panning decisions 
fig   a  illustrates sample trajectories generated by different methods  compared to the tracking based trajectory 
both the center of attention and our method provided a
more stabilized trajectory  furthermore  our method also
offers a better prediction in the camera center and eliminates some of the unnecessary panning  fig   b e  shows
the ground truth and the predicted labels for panning and
zooming decisions  in general  our algorithm correctly pre 

table    system complexity
implementation execution
time
per
frame
lecturer tracking
c c  
  ms
visual saliency
c
  ms
prediction
matlab
  ms
other
matlab
  ms
operation

total

   ms

dicts large camera panning and provides stable zooming
predictions  for qualitative result  we have recorded an
example video clip to compare between the ground truth
and our proposed method  the video clip can be viewed
via http   mars  stanford edu dcypang cs    report  
     system complexity
table   shows the performance of our system tested on
the stanfords corn server with   core opteron      and
  gb ram  we also developed a low complexity saliency
extraction and lecturer tracking using c c    other operations  such as feature extraction  regression  and svm
prediction  are implemented using matlab  on average 
our proposed system only requires a processing time of   
ms per frame 

   conclusion
in this report  we proposed a supervised learning algorithm that takes advantage of conventional tracking techniques and the state of the art human saliency model to
automatically operate a virtual camera for lecture video
capture  our system has resolved the restrictions imposed
in the current classx system  experimental results reveals our system can provide a indistinguishable camera
view from human generated view and can offer a low cost 
low complexity solution to the automatic camera view generation for online lecture viewing  for future work  we will
extend our model evaluation to multiple classes with different classroom settings  a subjective test should also be
conducted for evaluating the quality of the automaticallygenerated camera views 

   acknowledgement
we would like to acknowledge the support of the classx
team  namely sherif halawa  ngai man cheung  mina
makar and prof  bernd girod  for offering their helps and
providing the lecture video contents for our experiment 
we thank huizhong chen and fan wang for providing the
face detection software  lastly  we would like to thank
prof  andrew ng and the teaching assistants for giving us
a learning opportunity to apply machine learning in this
project 

references
bianchi  michael  automatic video production of lectures
using an intelligent and aware environment  in proceedings of the  rd international conference on mobile
and ubiquitous multimedia  mum     pp          new

fiautomatic virtual camera view generation for lecture videos
   

   

horizontal coordinate

   

   

   

   

   
ground truth
proposed method
lecturer tracking
center of attention

   

  
 

    

    

    

    
frame number

    

    

    

    

    
frame number

    

    

    

    

    

    

    

    

ground truth

panning label

 a 

 
 
  
 

    

    

    

predicted

panning label

 b 
 
 
  
 

    

    

    

    
frame number

ground truth
zoom label

 c 
 
 
  
 

    

    

    

    
frame number

    

    

    

    

    

    

    

    

 d 
predicted
zoom label

 
 
  
 

    

    

    

    
frame number

 e 

figure     a  comparison of trajectories given by the ground truth  lecturer tracking  center of attention and our proposed
method   b  ground truth panning labels  c  predicted panning labels  d  ground truth zooming labels  e  predicted
zooming labels
york  ny  usa        acm 
heck  rachel  wallick  michael  and gleicher  michael 
virtual videography  acm trans  multimedia comput 
commun  appl      february      
itti  l   koch  c   and niebur  e  a model of saliencybased visual attention for rapid scene analysis  pattern
analysis and machine intelligence  ieee transactions
on                    november      
katz  steven  film directing shot by shot       
koch  c  and ullman  s  shifts in selective visual attention  towards the underlying neural circuitry  human
neurobiology                    
mavlankar  a   agrawal  p   pang  d   halawa  s   cheung  n   and girod  b  an interactive region of interest
video streaming system for online lecture viewing  special session on advanced interactive multimedia stream 

ing  proc  of   th international packet video workshop 
     
nagai  takayuki  automated lecture recording system with
avchd camcorder and microserver  in proceedings of the
  th annual acm siguccs fall conference  siguccs
    pp        new york  ny  usa        acm 
piccardi  m  background subtraction techniques  a review 
in systems  man and cybernetics       ieee international conference on  volume    pp             vol   
october      
rui  yong  gupta  anoop  grudin  jonathan  and he  liwei  automating lecture capture and broadcast  technology and videography  multimedia systems         
     
treisman  anne  a feature integration theory of attention 
cognitive psychology                 

fiautomatic virtual camera view generation for lecture videos
viola  paul and jones  michael j  robust real time face
detection  international journal of computer vision 
                
zhang  cha  rui  yong  crawford  jim  and he  li wei 
an automated end to end lecture capture and broadcasting system  acm trans  multimedia comput  commun  appl              february      

a    camera dynamics model
to realize the physical behavior and the natural constraint
of a real world camera system  the trajectory of a camera
can be described by a stochastic linear time invariant  lti 
dynamic model 
 
 

as i    bu i    w i   i           n
cs i    v i   i           n  

   
   

where s i   r  is the the camera trajectory state that
comprises its position p and velocity p i    u i   r is
a scalar acceleration p i  input   w i   r  is a random
variable that represents the process noise and exogenous
effects  v i   r is a random variable that denotes the
observation noise  a  r   is the state transition matrix 
b  r   is the input control matrix  i is the time step
of the system indexed by the video frame number and n
is the total number of frames in a video sequence  we
assume w i   n     q  and v i   n     r  are zero mean
iid drawn from a multivariate normal distribution with
covariance q and r respectively 

a    kalman filter for camera trajectory
prediction
let y i  be the predicted trajectory output of our lti system  since y i  is only an approximation to the camera
trajectory  y i  can be regarded as the noisy observation
y i  as shown in      we can then apply kalman filter to
predict the true state s i  of the camera  since u i  is an
unknown input  we rearrange     and incorporate p i  as
part of the state s i   r  to be predicted 
s i        as i    w i   i           n

   

 

where w i   r   w i   n     q  


    t
 
a        t 
     
and
c  



k i 
s i i 

 
 

p  i i    c t  cp  i i    c t   r    
s i i       k i  y i   c s i i      

p  i i 

 

 i  k i c p  i i     

and the estimated trajectory is  
y i    c s i i  

a  appendix

s i     
y i 

and the estimation step 

 

 

 



 

if a more complex dynamic model is needed  a and c can
also be estimated by using an em algorithm  after kalman
filter is applied  s i  can be computed by the following
update step 
s i i    

 

p  i i    

 

as i    i     
ap  i i at   q 

fi