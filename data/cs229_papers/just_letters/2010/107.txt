cs    autumn     

neuro fuzzy inverse forward models
brian highfill
stanford university
department of computer science

abstract  internal cognitive models are useful methods for the implementation of motor control     
the approach can be applied more generally to any intelligent adaptive control problem where the dynamics of
the system  plant  is unknown and or changing  in particular  paired inverse forward models have been shown
successfully to control complex motor tasks using robotic manipulators      one way this is accomplished is by
training a pair of multi layered neural networks to learn both the forward model of the plant dynamics and an
inverse controller model simultaneously using an extension of the backpropagation algorithm  this paper
explores a variation of the traditional multi layered network used for teaching an inverse forward model pair
 ifmp   we investigate the use of a simple fuzzy neural system for implementing both models 

i  introduction
an inverse forward model pair  ifmp  is a
modeling technique used for control which attempts
to invert the dynamics of some unknown system 
this allows for the indirect manipulation of system
outputs  distal variables       ifmps have been
applied to nonlinear adaptive control systems to
learn complex motor tasks using robotic
manipulators  a common training technique used for
such systems is backpropagation on a pair of multilayered feedforward neural networks  previous work
on ifmps indicates that learning can be extended to
any supervised learning method which can learn in
multiple layers      the current paper explores the
training of an ifmp on a fuzzy adaptive standard
additive model  asam  
asams are shallow  single layered  networks
which rely on a set of natural language like fuzzy ifthen rules to map inputs to outputs  asams  like
multi layered networks are universal approximators
but have the added advantage of being grey box 
one can look inside a trained asam and interpret
the induced fuzzy sets  unlike multilayered networks
where hidden layers are difficult to decipher   by
extending gradient descent to asams in inverseforward pairs  one would be able to solve complex
adaptive control problems while also enabling

insight to the structure of the learned control law
and plant dynamics in an intuitive rule based format 

ii  methods
a  controlling distal outputs
an environment can be thought of as a mapping
from actions to observed outputs  if we could invert
that mapping  we would be able to indirectly
manipulate the environmental outputs as we please 
this can be accomplished using an inverse model  an
inverse model is a mapping from desired outputs to
actions which  when composed with the
environment  forms an identity relation  sometimes
it is desired  or useful  to model the dynamics of the
environment as well  a forward model is a mapping
from actions to outputs which aims to approximate
the same mapping of a  real  environment 
given some nonlinear  or possible nonstationary  environment  plant   our goal then  is to
learn an inverse mapping of the environment from
the desired sensation
to the observed
sensation  output 
which enables it to be
controlled  figure   shows the general setup with
the inverse model  controller or learner  in series
with the environment 

fics    autumn     

environment  this is accomplished by learning a
forward model  we can learn a forward model by
observing training pairs
through the
setup of figure   
fig     training the forward model from environmental response
to actions 

we will assume that the full state of the
environment
is observable  using a forward
model as in this setup helps us to avoid a subtle
difficulty in learning inverse models  figure   shows
the convexity problem      the environment is in
general a surjective function which may map more
than one action to a single output  if we were to
directly model the inverse function by treating it as a
standard supervised learning problem  the inverse
model would effectively select a centroid of the
inverse image for a desired sensation  however  this
centroid need not be an element of the inverse
image of the desired sensation  in other words  the
inverse image is nonconvex     

fig     learning the forward model 

an auxiliary feedback controller is utilized to
maintain stability of the dynamic system  the
feedback controller is typically a simple
proportional derivative  pd  controller 
assume  now  that we already have an accurate
forward model which correctly maps from actions
to
  by holding the forward model
fixed  we can train the composite system of figure  
to be the identity mapping from
to
by implicitly training the learner to be the inverse of
the forward model  figure   shows the generic
setup  the backpropagation algorithm is often
utilized to this end in multilayer networks by
propagating errors backwards through the forward
model  without changing it  and finally backwards
through the learner 

fig     the convexity problem  multiple actions map to the same
sensation  the inverse model must select a proper action from the
inverse image of a desired sensation     

modeling the inverse learning problem in
the goal direct form of figure   allows the inverse
model to select a single action from the inverse
image given a desired output  the particular action
chosen  however  cannot be predicted as it depends
on the order of the sequence of training examples
    
b 

fig     learning the inverse model  controller  

learning the models

in order to learn the inverse model  we
must first estimate the forward dynamics of the

we can simultaneously learn the forward model
while trying to learn the inverse controller  logically 
there are two iterative phases per training input 

fics    autumn     

first  the forward model is tuned by observing the
actual response of the environment to actions
 figure     then  holding the forward model fixed  the
inverse model is tuned via backpropagation so that
the composite inverse forward pair realizes the
identity map  to learn the forward model
simultaneously  we can use the response of the
 stabilized  inverse model to learn on actual desired
trajectories     
c 

adaptive standard additive model

the forward and inverse models have been
often implemented using a standard feedforward
hidden layered neural network and trained via
backpropagation  although this method is powerful 
these networks are black boxes and intuition about
what they learn cannot be gained by simply
observing the learned weights  adaptive neuro fuzzy
systems are also powerful  they are universal
approximators      but have the added advantage
that they are grey box  by looking at the learned
fuzzy if then rules  one can understand the learned
structure in a natural language way 
the neuro fuzzy system presented is the
adaptive standard additive model  asam   on each
of the inputs  a set of
fuzzy if then rules  sets 
are used to predict the output  each fuzzy rule is
activated to a degree in
depending upon the
input   rule activation functions can be chosen with
many different shapes  for example  triangle or
gaussian  radial basis  functions  see figure        
once a given rule is activated  the corresponding
fuzzy then part set is asserted  which has its own
shape   the final decision  output  can be made by
adding together all the asserted then part sets and
finding the centroid 

fig     gaussian activation  set membership  functions on the
input 

let the sam be a mapping
 
further  define
be the fuzzy
membership function for rule j 
be the fuzzy then part set for rule j 
and
  then 

using gradient descent  an adaptive
learning law is derived to adjust the parameters
above by minimizing the squared error cost function
over a set of training examples 
d  gradient decent learning laws
we now derive the learning laws for an ifmp for an
environment with one continuous state variable and
on continuous action and a single output  each
model will be an asam with a factorable gaussian
activation functions  let the forward model be 

also  define 

define the squared error to be minimized as 

fics    autumn     

then
then 

where 

with 

now  for the centroid 

similarly  let the inverse model be 

for updating the volume 

also  define 

for updating the gaussian means 
finally 

fics    autumn     

simulated dynamics

 

for updating the gaussian dispersions 
output y

   

 

    

  
 
 

 
 

 

 

 

 
 

state x

 

action u

fig     simulated environmental dynamics 
forward model approximation

simulations

we simulated learning the forward and inverse
models separately  first  we assumed that the
environment has the known dynamics  see figure   
given by 

 

estimated output y

iii 

   

 

    

  
 
 

 
 

 

 

 
 

action u

fig     learned forward model 
mse vs  iteration
    

    

    

mean squared error

to simulate learning of the forward model  we
used a gaussian asam with    rules on each input
 state and action   using       for all learning rates
and allowing the forward model to sample each of
the environment outputs in response to a uniform
sampling of the input and control space over
and for     
iterations  the forward model converged to a good
approximation to y x u  

 
 

state x

   

    

    

    

    

 

   

    

    
    
iteration number

    

    

fig     convergence to learned forward model 

we then trained the inverse model using
the gradient descent learning law derived above

fics    autumn     

using the same type of asam and with         for
all learning rates  the inverse model converges  after
     iterations  to the model shown in figure   

inverse model approximation

iv 

conclusion

the applications of this method of control are plentiful  it is
especially useful for environments with non linear or nonstationary dynamics where control is possible only through
adaptive controller updates  additionally  the benefit of utilizing a
neuro fuzzy controller under the ifmp paradigm is that the
mystery of what is learned is partially removed  one can directly
observe the learned fuzzy rules in the form of fuzzy if then
statements 

estimated inverse action u

 

   

v 

 

    
 

  
 

   
 

 
 

    

 
state x

 

  

fig     convergence to learned inverse model 

y 

references

    m  i  jordan  d  e  rumelhart  forward models 
supervised learning with a distal teacher  cognitive
science                    
    b  kosko  fuzzy engineering  upper saddle river 
new jersey  prentice hall        pp         
    t  m  mitchell  machine learning  boston 
massachusetts  mcgraw hill        pp         

fi