hessian free deep learning
subodh iyengar
december         

 

introduction

optimization techniques used in machine learning play an important role in the
training of the neural network in regression and classification tasks  predominantly  first order optimization methods such as gradient descent have been
used in the training of neural networks  since second order methods  such as
newtons method  are computationally infeasible  however  second order methods show much better convergence characteristics than first order methods  because they also take into account the curvature of the error space  additionally 
first order methods require a lot of tuning of the decrease parameter  which is
application specific  they also have a tendency to get trapped in local optimum
and exhibit slow convergence  thus newtons method is absolutely essential to
train networks with deep architectures 
the reason for in feasibility of newtons method is the computation of the
hessian matrix  which takes prohibitively long  influential work by pearlmutter
    led to development of a method of using the hessian without actually computing it  recent work     has involved training of a deep network consisting
of a number of restricted boltzmann machine using newtons method without
directly computing the hessian matrix  in a form of hessian free learning 
the method had exhibited success on the mnist handwriting recognition data
set when used to train an restricted boltzmann machine using hintons    
method  with a better quality solution for classification tasks 
the proposed work for the cs    project aims to improve upon the method
of hessian free  hf  learning and apply it to different classification tasks  to
do this  the hessian free learning method will be implemented and results for
the experiments using mnist will be replicated  through analysis  it is aimed
to propose further modifications that will improve the method and also run it
on different classification tasks 

 

the hessian free method

unlike gradient descent  in newtons method the update equation uses a second
derivative hessian to compute the next step 

 

fix   x  h    f

   

however  computing the inverse of the hessian is an o n    operation  for
neural networks with deep architectures  the number of weights is large  thus
the hessian is a very big matrix  an inversion would take a lot of time and
is simply unacceptable for practical applications  thus  newtons method has
almost been neglected in work on training neural networks 
for practical purposes though  it is not necessary to invert the hessian matrix  a procedure to calculate it could be formulated as 
p   h    f

   

hp   f

   

the resulting equation is of the form ax   b and can be solved using some
method like conjugate gradient  provided that h is positive definite  however
computing the hessian itself is an o n    operation  involving perturbing each
individual weight and propagating it forward 
the hessian free method has existed in literature     for a long time  the
idea behind the hessian free method derives from the equation      instead
of physically computing the hessian which is time consuming  we only need to
compute the product  hp  a matrix vector product  we can then solve the same
equation using conjugate gradient for p  this can be done by taking the finite
difference of gradient computed in the direction of p as follows 
f     d   f   
   
 

however it is found that the hessian matrix is usually very unstable  schraudolph adapted pearlmutters r propagation     method to use the gauss newton
approximation to the hessian matrix  the gauss newton matrix g is guaranteed to be positive definite and is quite a good approximation to the hessian 
as well as being cheaper to compute 
in a recent paper  martens     uses this technique effectively to train a deep
network without pre training  modifications were proposed for example using
mini batches and modifying the stopping conditions of cg to the hessian free
algorithm to make it more suitable for machine learning applications 
his findings indicate that the method performs well without pre training
and beats the simple back propagation learning procedure in various data sets
making it a very interesting method to study 
hp   lim

 

the problem with the hessian

in the optimization framework proposed above  the conjugate gradient method
converges to a solution  provided that the hessian matrix is positive definite 
however for practical neural networks that represent non convex functions  the
 

fihessian is not guaranteed to be positive definite  this fact had a profound effect
on this project on which a lot of time was expended experimenting with the
hessian only to discover that it was indeed negative definite  when optimizing
the weights of the deep network using the hessian free method  the loss function
is found to increase rather than decrease  to ascertain whether the hessian
matrix is indeed positive definite would involve computing its eigenvalues  and
examining whether each on is positive  which is a computationally intensive
task  instead  the graphs below that show the behavior of the residue of the
conjugate gradient step cg  vs  number of iterations of cg confirm that the
hessian is negative definite  the residue  given by the green line  increases
instead of decreasing 

 a  residue using gauss newton

 b  residue using hessian green 

to subvert this dilemma the hessian matrix is approximated by the gaussnewton approximation of the hessian h   j t  j where j is the jacobian
matrix of the neural network  so the cg system that must be solved becomes
   
 j t  j   i p   j t  e
where e is the loss function output  i e  the error term  and j t e represents
the gradient value at that point  the gauss newton matrix is guaranteed to be
positive definite as shown by the cg minimization step in fig    a  

 

implementation and experiments

initially  the hessian free method was implemented over the code provided by
hinton for his science paper      however it was found that the stacked rbms
and the other complexities of hintons code resulted in a lot of memory leaks and
the program used to run out of memory easily  to put things into perspective 
the network used in hintons case is a                              network 
thus the jacobian matrix has     million x size of minibatch entries of doubles
and the multiplication of j t by j thus incurs a huge memory overhead 
 

fithus hintons code was reimplemented from scratch and the code was systematically tested on various sizes of neural networks to prove the correctness if
the method  in the smaller networks the solutions converged to zero very fast 
within   epochs  it was tested against plain gradient descent backpropagation
and it did significantly better than the latter method  in bigger networks  the
method took time to converge  hessian free was tested mainly on   neural
network architectures              and                    
to circumvent the issue of memory  and due to project time constraints  the
networks were trained on a limited number of minibatches from the mnist
dataset of size     each and tested on       patterns on mnist  the method
was run for a particular number of epochs until it was believed that it had
converged to a low enough value of reconstruction error  the convergence characteristics of each experiment are shown in figures 

  

  

  

  
  

  

  

  

 
  
 
  
 
  

 

  

 

 
 

 

 

  

  

  

  

  

  

 

  

 

 c  test error vs  epochs  small network 

  

  

  

  

   

   

   

 d  error vs  epochs  hinton 

figure  c  show the convergence of hessian free on the             network with    minibatches  figure  e  and  f  shows the convergence of hf on

  

  

  

  

  
  
  
  
  
  
  
  
  
 

  

 

 

  

   

   

   

   

 

   

 e  training r  testing b  error vs  epochs

 

 

  

  

  

  

  

  

 f  training r  testing b  error vs  epochs

fithe                              network with    minibatches and     minibatches respectively for training  figure  d  shows the results of hinton on
his deep architecture  the table shows the resulting reconstruction errors for
various experimental parameters  the graphs are compared with the results obtained from hintons experiments and are found to be not quite competitive with
the pretraining methods  however the achievement of this project was in getting
the reconstruction error to fall consistently  since  in numerous initial experiments the reconstruction error used to decrease initially and then increase later 

 

network size

minibatches

           
                  
                  

  
   
  

epochs training error
  
   
  
    
   
   

test
error
   
     
     

discussion

the hf method is found to perform reasonably well for small data set size and
for minibatches of smaller size  the method as coded in this report is still not as
competitive as it should be against cg back propagation  however with future
work on the improving the method  it should become more competitve  there
could be a lot of refinements the algorithm as coded in this report to make
it more memory efficient like using schraudolphs method for calculating the
gauss newton approximation as proposed by martens 

 

conclusion

in the course of the project  martens algorithm has been replicated to some
extent  the overarching objective of the project  i e to apply the methods in applications was not met  however encouraging results were obtained by applying
the method to mnist  instead this shall be kept in the scope of future work 
further effort must be expended to understand the hf algorithm better and
implement some of the more finer aspects of it 

 

acknowledgements

i would like to thank quoc le for his help during this project without whom
the project would not have reached even this stage 

references
    martens  j  deep learning via hessian free optimization  icml       

 

fi    pearlmutter  b  a  fast exact multiplication by the hessian  neural
computation       
    hinton  g  e  and salakhutdinov  r  r reducing the dimensionality of
data with neural networks  science  vol       no        pp            
   july      
    http   www cs toronto edu  hinton matlabforsciencepaper html
    numerical optimization  nocedal and wright

 

fi