multiple experts with binary features
ye jin   lingren zhang
december        

fi  introduction
our intuition for the project comes from the paper supervised learning from multiple experts  whom to trust when everyone lies a bit by raykar  yu  etc  the paper
analyzed a classification problem where instead of observing a true classification of
each data point  we observe some classification from several experts  the project will
attempt to solve a variation of the problem in which the features are binary instead
of real valued  in addition  we generalized the problem to do a n class classification
instead of a binary classification 

  model description
    training data
the data set contains m training examples     x i     y  i     i           m   where  x i   
 i 
 i 
 i 
 i 
 x    x       xk   with xj         and  y  i   ar with a               n   representing
the n different classes  i e  the feature space is k dimensional and there are r experts
providing estimates of the true y  i   

    model assumptions
      naive bayes assumption
similar to the spam classification example given in class  we make the naive bayes
 i 
assumption  assume that for the ith training example  the xj s are conditionally
independent given the true y  i    then we have the following property which is convenient 
 i 
 i 
 i 
p x    x         xk  y  i   

 

k
y

 i 

p xj  y  i    

   

j  

      characteristic matrix for each customer
 r 

for the rth expert  we define his her characteristic matrix to be m  r    where mp q  
p  yr   q y   p  for p  q              n   i e  the entry on the pth row and qth
column is the probability that the rth expert gives classification q given that the true
classification is p  notice that each row of this matrix has to add up to one  hence
the degree of freedom is n  n     instead of n     i e  we should really describe each
customer using a n   n     matrix instead of a n  n matrix  but for symmetry
and simplicity we keep it that way for now 

 

fi  single expert case  a generalization to spam
classification
we use two sets of parameters to model this problem 

y   p  y  i    y 
 i 

j y   p  xj     y  i    y  
note that p
we only consider              n   as parameters  n can be calculated as
n  
n      p  
p  
the joint likelihood is 

l y   j y    
 

m
y
i  
m
y

 i 

 i 

 i 

k
y

p y  

i  

 

m
y

 i 

p x    x         xk   y  i   
 i 

p xj  

j  

y i 

i  

k
y

x

 i 

 i 

j yj  i      j y i    xj  

j  

set the partial derivatives of l to   and we derive the maximum likelihood estimators 

pm

  y  i    y 
m
pm
 i 
 i 
  y xj
i     y
  p
 
m
 i    y 
i     y

y  
j y

i  

  multiple expert case
    likelihood function
we need to use the characteristic matrices m  r  as well as the parameters used in the
single expert case  y   j y    let     m  r    y   j y    we can calculate the likelihood
function 

 

fil    

m
y

 i 

 i 

p  y         yr   x i     

i  

 

 

m x
n
y

 i 

 i 

p  y         yr  y  i    n  x i     p  x i   y  i    n   p  y  i    n   

i   n  
m x
n
y
i   n  

r
y

 
m

 r 
 i 

n yr

r  

k
y

 i 
xj

 

 i 
 xj

j n     j n  

n  

j  

however  l   is quite difficult to maximize because of the summation in the formula
 and hence taking the log likelihood does not simplify the problem very much   the
solution is to use the em algorithm with  y    y            y  m    as latent variables  now
consider the new likelihood function 

l  y      

m
y

 i 

 i 

p  y         yr   x i    y  i     

i  

 

 

m
y

 i 

 i 

p y         yr  y  i    x i     p x i   y  i     p y  i     

i  
m
y

r
y

 
m

r  

i  

 r 

k
y

 i 

y  i   yr

 i 
xj
j y  i 



 i 
 xj
j y  i 

    

 

 
y i   

j  

    the em algorithm
      e step
 i 

 i 

we need qi  y  i     p y         yr  y  i    x i     p x i   y  i     p y  i       and thus
 i 

 i 

qi  y     pn

 i 

p y         yr  y  i    x i     p x i   y  i     p y  i     
 i 

 i 

p  y         yr  y  i    n  x i     p  x i   y  i    n   p  y  i    n   

 q
q
 i 
 i 
xj
 r 
r
k
 xj
y i 
r   my  i   y  i 
j   j y  i      j y  i   
r

 
 q
 i 
 i 
pn qr
xj
 r 
k
 xj
n
 i 
n  
r   m
j   j n     j n  
n  

n yr

to initialize  because during the first e step  we do not have  to let us calculate
p
 i 
 i 
qi  y  i      we can set qi  y  i      r  r
r     yr   y   

 

fi      m step
given qi  y  i    calculated in the e step  we want to maximize

l    

m x
n
x

qi  n  log

i   n  

 

m x
n
x

 

r
y

qi  n 

i   n  

 r 

m

 i 

n yr

r  
r
x

k
y

log m

 r 

j n     j n  

 i 
 xj

 

 
n

j  

  log n  

 i 

n yr

r  

 i 
xj

k
x

 
 i 

 i 

 xj log j n       xj   log    j n   

j  

l
 r 
mn p

    for p              n     bear in mind that mn n     
p
 i 
is not a free parameter   we get mn p  m
i   qi  n   yr   p   hence
setting

 r 
mn p

 i 
qi  n   yr
i   p
m
i   qi  n 

pm
 

  p 

i  

n   pn pm

i   qi  p 

p  

lastly  we set

l
j n

 

pm

i  


qi  n 

j n

pm

qi  n 

 i 

xj
j n

 i 



mn p

   
pn  
n  

n is not a free

qi  n 
 
m

i  

 

 xj
 j n

p  

 

l
setting 
    for n              n     again  n     
n
p
parameter   we get n  m
i   qi  n   hence

pm

pn  

   


equal to zero  and we get

pm
 i 
i   qi  n xj
p
 
 
m
i   qi  n 

   

it is worth noting that n   j n derived here in the m step is the same as mle in the
single expert case  except all the   y  i    n  terms are substituted with qi  n  

    missing labels
one of the technical details that we need to deal with is the missing labels  meaning
 i 
that not all experts give classifications to all training examples  i e  yr does not
necessarily exist for all i and r  it turns out that we can make a small change to
our algorithm to take care of this issue  let ri be the set of experts who classified
training example i  then the likelihood function becomes

l    

 

m x
n
y

y

i   n  

rri

m

 r 

k
y

 i 

n yr

j  

 

 i 

xj
j n

 i 

 xj

    j n  

 
n  

ficonsequently  the e step can be modified to
q
 i 

qi  y    

rri

m

 r 
 i 

 q

y  i   yr

pn q
n  

m

rri

k
j  



 r 
 i 

n yr

with the initial step to be qi  y  i     

 
 ri  

p

 i 

xj

 i 

 xj

j y i      j y i   



y i 

 
 i 
 i 
qk
xj
 xj
n
j   j n     j n  

rri

 i 

  yr   y  i    

for the m step  the update formulae for n and j n does not change  the formula for
 r 
mn p can be rewritten as
p
 r 
mn p

 

 i 

qi  n   yr   p 
p
 
i rri qi  n 

i rri

    laplace smoothing
another technical detail that we may encounter is that the denominators in the e step
and m step formulae p
may be zero  asp
a result  we need to apply laplace smoothing 
q
 n 
and
in the m step  since m
i rri qi  n  might be zero  consider the case
i   i
where nobody ever gave a classification of n in the trainingp
set  and the first m step
 i 
 
 i 
right after the initial e step which is to set qi  y      ri   rri   yr   y  i      the
formulae can be replaced with

 i 

p

qi  n   yr   p     
p
 
qi  n    n
pm i rri
qi  n     
n   i  
m n
pm
 i 
i   qi  n xj    
 
j n   pm
i   qi  n     
i rri

 r 
mn p

one can also sanity check that
formulae 

pn

 r 

p  

mn p     and

pn

n  

n     using the smoothed

in the e step  applying laplace smoothing might be difficult since each
 
y
rri

m

 r 

k
y

 i 

y  i   yr

 i 

xj

 i 

 xj

j y i      j y i   

 
y i 

j  

term can be very small and it is hard the estimate the order of magnitude of these
terms  as a result  adding   to the numerator and n to the denominator  or generally

 

fiadding some pre determined constant c to the numerator and n c to the denominator 
may destroy most of the meaningful information in the qi  y  i    distribution  making
them all equal to n    however  the good news is that because of the smoothing applied
 r 
in the m step  one is guaranteed that mn p   n   j n         and the denominator will
be non zero  hence there is no need to apply smoothing in the e step 

 

fi