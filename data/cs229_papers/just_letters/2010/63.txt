local image histograms for learning exposure fusion
justin solomon and ravi sankar

 

introduction

weighted histograms  evaluation of the learned exposure fusion function can be made considerably more efficient by taking advantage of the fact that histograms
change with relatively low frequency across most images  a key contribution of this project is a new algorithm
for efficient evaluation that makes image processing byexample a more feasible task 
this project represents one of the first data driven
techniques in computational photography introduced to
the graphics community  its timings compare favorably
with comparable approaches and more standard image
processing techniques  and the output exhibits relatively
few undesirable artifacts 

local image histograms provide valuable descriptors of
the behavior of an image around a given pixel  rather
than bucketing intensity values for all the pixels in an
image  local histograms are defined separately for each
pixel and represent the distribution of nearby intensities 
several image filters  including the median  mean shift 
and bilateral  can be described and computed using these
histograms 
in the siggraph      conference  a new technique
was introduced for efficiently computing such local histograms at all points in an image      while previous
algorithms had time complexity proportional to the support of the histogram  the new approach always runs in
o    time per pixel  taking advantage of efficient gaussian blur operations to simplify the local histogram computation process 
while     applies local image histograms to the reexpression of previously known image filters  these histograms also provide interesting features that could be
used to apply machine learning to computational photography  in this project  we propose the application of this
technique to learning reasonable approaches to exposure
fusion  in which images of a scene taken at different exposures are linearly combined at each pixel to generate a
meaningful output using the entire dynamic range of the
display device 
ideally  although there obviously is a strong correlation between the histograms and values of adjacent pixels in a photograph  the histograms provide sufficiently
strong descriptions of local behavior that this dependence
can be ignored  thus  we use examples of successfullyfused images and their per pixel histograms as training
data to learn a function mapping a single pixel and its
corresponding histogram in each of the different exposures to a single output  this dependence likely is nonlinear  since local histograms often exhibit complex or
even bimodal distributions  for this reason  we apply the
least squares svm kernelized regression technique 
which implicitly makes use of high order features      in
the particular setting of image processing with locally 

 

background

local image histograms are well explored objects in
computer graphics and provide a generalized framework
for expressing several common image filters  the histograms are stored at all pixels of an image and represent
the distribution of nearby intensities  a general theory of
image processing can be built around the processing and
manipulation of these histograms  providing a common
language used to describe various operations  for instance  the median filter seeks the     point of the cdf
of each pixels local histogram  the mean shift snaps to
nearby histogram peaks  and the bilateral can be written
as the ratio of two histogram values  these simple operations provide strong evidence that local histograms are
valuable features in themselves that contain considerable
information about a pixel and its neighborhood 
here  we apply the technique introduced in     for
computing per pixel local histograms  while nave methods have existed for some time in which the intensities of
a given neighborhood of a pixel are simply binned  these
methods have two severe drawbacks  first  the binning
procedure takes longer depending on neighborhood size 
local histograms representing hundreds of nearby pixels will take much more time to compute than histograms
of the one ring of a pixel  more importantly  these sorts
of local histograms are anisotropic  often accidentally
favoring certain directions over others by using square
 

fievaluation as efficient as possible  thus  before describing the entire exposure fusion algorithm  we proceed by
describing an algorithm for efficient evaluation of f  
substituting in the gaussian radial basis into equation    we find our final expression for the exposure fusion function 

neighborhoods and     weights  instead      describes an
efficient technique for using isotropic  weighted neighborhoods around each point  the algorithm can be described using gaussian blurs of the input image passed
through various look up tables  and thus the local histograms for all pixels can be computed in o nmh  time
for an n  m image with h histogram bins 
of course  there is no reason why an exposure fusion
function should be a linear function of the counts in local
histogram bins  thus  it is necessary to apply a nonlinear
regression technique that is able to find more complex
relationships between input and output variables  luckily  although we have a relatively high dimensional problem  a glut of training data is available  every image contains a diverse set of features and thousands of pixels with
uniqueif similarhistograms 
still  the regression problem here is difficult  since it
involves fitting a fairly complex function of several variables  several parametric and nonparametric techniques
were implemented and tested  including regression trees 
gaussian processes  and support vector regression  for
the most part  these techniques were fairly extreme examples of either under fitting or unacceptable timing and
were discarded  one algorithm  however  that performed
fairly well in both arenas is the least squares support vector machine  ls svm   introduced in      the ls svm
attempts to re express a least squares problem similar to
gaussian process regression in the language of support
vector machines  ls svms tend to produce considerably more support vectors  raising the amount of time it
takes to evaluate the regression function  but produce reliable fits for exposure fusion 

 

f  x    b  

i k x  x i   

 i  k 

   

for exposure fusion  our feature vector x can be written as the sum of orthogonal vectors x   xhist   xrgb  
where xhist contains histogram samples from the different exposure images and xrgb contains the pixel color in
each exposure  the key observation here is that xhist
changes with relatively low frequency across the image
determined by the neighborhood size of the histograms 
intuitively this makes sense since the sets of nearby intensities to two adjacent pixels should have considerable
overlap  thus  xhist can be computed and manipulated
in a much smaller image than the full sized output 
in particular  we expand the exponent as follows 
 i 

kx  x i  k   kxk    xhist  xhist
 i 

  xrgb  xrgb   kx i  k 
the kx i  k  term can be precomputed for each data point
x i  ahead of time  similarly  the kxk  term can be computed once for an input x before evaluating the sum over
i  these simplifications leave only the two inner product
 i 
terms  the xhist  xhist term comprises the most computational work if computed directly  since there are      samples in histogram space over multiple exposures
that must be multiplied  fortunately  as argued above 
the histogram dot product terms can be computed on a
lower resolution grid and upsampled since they change
with low frequency  this leaves only the rgb inner products  which take considerably less time 
the expansion above moves the learned exposure
function from an intractable curiosity to a practical approach that can be applied to photographs in reasonable
 i 
time  to demonstrate that approximating xhist  xhist on
a low resolution grid does not significantly affect output 
figure   compares upsampled and exact versions of these
values to show how similar they are 

in the end  performing exposure fusion from the lssvm output involves evaluating the following function
at each pixel 
m
x

i ekxx

i  

performing fusion

f  x    b  

m
x

   

i  

where x              x m  is the set of support vectors  b and
 i   are values from regression  and k is the kernel
function  for this project  we use the gaussian radial
basis functions k x  y    exp kx  yk    as kernels    technical approach
although the algorithm easily could be adapted to other
kernel functions 
as described earlier  our algorithm learns a function from
since the regression function f is evaluated at each rn to           where n is the number of histogram feapixel of the image  it is highly important to make the tures and colors are the output in rgb  as input  the
 

fi a 

the learning stage outputs ls svm parameters b and
i described in equation    the number m of support
vectors often is close to the number of input images 
which images it slow to evaluate the regression function 
to speed up evaluation  the simple pruning technique described in     is implemented  in which support vectors
corresponding to small i are discarded after fitting an
ls svm  the reduced set of support vectors is used to
train a new ls svm  this process is repeated several
times  each time eliminating    of the training data until
the set of support vectors is approximately     smaller 
given the trained ls svm  running exposure fusion
on a series of input images is a relatively straightforward
process  each pixel of the input is independently mapped
to rn using its local histograms and intensities  then  the
ls svm is used to predict the fused intensity in rgb
space by evaluating f with the acceleration described in
section   

 b 

figure     a  exact and  b  upsampled values of kx 
x i  k for a sample image  note that they are virtually indistinguishable other than boundary artifacts  despite the
fact that  b  was computed on a grid    smaller 

learning algorithm takes p input photographs  taken at
different exposures and aligned ahead of time using a
standard approach  for each image  the algorithm produces local histograms at all the pixels using h bins  the
vector for a given pixel is computed by concatenating the
histograms from each of the exposure images and appending the color of the pixel at each exposure  thus 
we have n   p h       since colors are represented using rgb values  in practice  we use    buckets  h      
and three exposure images  p       giving n      dimensions for each feature vector 
as training data  a number of photos were taken indoors and outdoors on the stanford campus under varying lighting conditions  these images were fused using
two different algorithms  both implemented in the photomatix pro software system  both of these approaches 
however  generated artifacts in various parts of the input
images  to ensure that training data did not include these
incorrect pixels  masks were drawn by hand highlighting
parts of input images that were particularly well fused 
since each image had a relatively large number of pixels and inputs were chosen because of their interest or
variety of lighting  it was possible to be relatively conservative in choosing only particularly well fused pixels
for training  given this conservative selection process as
well as the use of two different algorithms for generating
training data  an ideal learned model might even outperform either of the two training techniques 
the training data process described above generates
more data than is usable or necessary for training a   dimensional ls svm  each input image contains millions of pixels  much of which are redundant for training
purposes since they are similar to pixels nearby  thus 
images were downsized to          before histogram
computation  additionally  input pixels were chosen randomly from the masked regions 

 

implementation

the method described above was implemented using
c   and matlab  we implemented     as an addition to
the imagestack image processing library in c     using
a fast iir gaussian kernel for generating histograms  imagestacks built in ransac image alignment method
was used to align images before fusion or training to ensure that the same pixel in the different exposures corresponds to the same location in the scene  alignment and
histogram generation take place on the order of seconds
for even relatively large sized images and thus represent
a reasonable approach for generating training data and
feature vectors on input images  additionally  more suitable camera hardware could reduce alignment computation time or make it unnecessary  if photos could be taken
one after the other 
the ls svmlab library was used to learn the regression function f    the library not only finds i and b but
also estimates the standard deviation parameter  using
heuristics from the set of training data  ls svmlabs
methods for evaluating the resulting function  however 
were deemed slow and were re implemented with the
modifications from section   in c   
we collected    test case photographs  creating    images worth of training data  one for each fusion method
implemented in photomatix pro   this amount is clearly
overkill given the dimensionality of our features and can
 
 

 

http   code google com p imagestack 
http   www esat kuleuven be sista lssvmlab 

fi a 

 b 

 d 

treme cpu 
figure    final page  shows one example where the
learned fusion function was less successful  visible artifacts appear in regions that are not well represented in the
training data and manifest themselves as low frequency
noise similar to the histogram distance functions  the
main source of artifacts in the other examples provided
here are due to problems with the alignment program
rather than the exposure fusion function 

 c 

 e 

figure    a sample training image at  a  dark   b    conclusions and future work
medium  and  c  light exposures  photomatix pro fused
these together to produce  d   and the pixels marked in the proposed technique yields a successful approach to
the mask  e  were selected as potential training data 
exposure fusion that has proven effective on a variety of
input images  with the modifications explained in section    the approach can be carried out very efficiently 
especially when compared to the hours taken to evaluate
the learned filters proposed in      the closest comparable
paper in the graphics literature 
although exposure fusion is a valuable application in
itself  perhaps the most worthwhile result of this project
is a generalized approach to using ls svm regression
for learning image filters using local histograms  the
concept of learning an image filter is a valuable one 
since artists manipulating photographs may prefer describing a complex filter by example rather than in the
mathematical formalism of image processing  additionally  a system that could recognize patterns in how photos
are edited could attempt to predict desirable filters before
figure    an example of successful learned exposure fu  an artist begins his or her work 
sion with source images above 
a number of filtering applications could benefit from
a similar approach  potential applications of histogrambased image filter learning could include 
be used for testing as well as model fitting  figure  
shows an example training image 
 edge preserving blurring
 deconvolution

 

results

 automatic upsampling

figures   and    final page  show examples of successful
learned exposure fusion output  in general  the algorithm
outlined above was successful for fusing a variety of input images  many of which contrasted significantly with
those found in the training data 
the examples in figures   and   were produced after pruning over     of the support vectors using the
method described in section    additional pruning led to
visible artifacts in the output images  whereas less pruning created no noticeable increase in quality  the final
method runs in approximately one to two minutes per
megapixel on a laptop equipped with an intel core   ex 

 application of painterly or other artistic effects
 scratch removal
 depth of field fusion
each of these applications could be approached with little to no modification of the ls svm method detailed
here  thus  an additional worthwhile goal for future research would be to improve the evaluation time of equation   as much as possible  while the downsampling approach presented here helps improve runtimes considerably  equation   still requires repeated evaluation of the
 

fiexp   function for each pixel  which could be improved
by use of various approximations and look up tables 
potential improvements aside  the success of the approach as is indicates that automatic learning and evaluation of image filters is a worthwhile and promising avenue for future research 

references
    hertzmann  aaron et al  image analogies  siggraph       los angeles 
    kass  michael and justin solomon  smoothed local histogram filters  siggraph       los angeles 
    suykens  j a k  et al  least squares support vector
machines  singapore  world scientific       
    suykens j a k  and j  vandewalle j   least
squares support vector machine classifiers  neural
processing letters                    

figure    a less successful exposure fusion test 

figure    additional examples of fused images 

 

fi