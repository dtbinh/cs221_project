travel time estimation using floating car data
ra sevlian
this project explores the use of machine learning techniques to accurately predict travel times
in city streets and highways using oating car data  location information of user vehicles on a road
network   the aim of this report is twofold  rst we present a general architecture of solving this
problem  then present and evaluate few techniques on real oating car data gathered over a month
on a   km highway in new delhi 

  floating car data based trac estimation
data used to estimate the travel times on road networks come in two varieties  one being xed
sensors on the side of the road such as magnetometer detectors or highway cameras        

the

second method is oating car data  fcd   floating car data are position xes of vehicles traversing
city streets throughout the day 

the most common type of fcd comes from taxi s or delivery

vehicles which are on main arterial roads and highways throughout most of the day 
this second approach has many positive and negative attributes that must be dealth with to
provide accurate travel time inference  first  fcd is perhaps the most inexpensive data to attain 
since many taxi services  and delivery companies automatically gather this data on their vehicles
for logistic purposes  second  position xes are generally very accurate  since gps is used and this
has high accuracy  there are however many disadvantages as well  first  fcd is usually sampled
infrequenty  on the order of     minutes  the reason for this is that taxi or delivery companies do
not need such ne time granularity of their vehicles position  therefore quiet a bit of preprocessing
needs to take place in order to snap sets of points onto the proper streets with the possibility
that multiple paths might have lead to the same pair of traverse points  another disadvantage of
this method is that a high density of data is required to get meaningfull travel time predictions
for a given road network  keeping all this in mind  constructing more and more accurate travel
time predictions can be a fruitful algorithms machine learning statistical modelling problem with
various problems to tackle 

  prediction architecture
building a trac estimation  system using millions of incoming fcd streams is a computational
and algorithmic challenge  various subcomponents requiring signicant research work need to be
developed and integrated  here we give a breif overview of the various processing steps required as
well as the issues explored in this report  figure     presents a high level view of such a system 

 

fi  prediction architecture

fig     travel

time

 

prediction

 t  x  y         tn xn yn   

architecture 

for

single

user 

input

is

sequence

of

fi  preprocessing

 

  preprocessing
    motion detection
since the incoming streams of trac data come from taxi cabs  delivery vehicles  or other comercial
eet vehicles  there will always be a certain amount of ambiguity between a slowdown in trac
and a commercial stop by the vehicle   i e  for a taxi customer  or a delivery vehicle dropping o
packages   therefore  any further processing must clean out all unwanted stops that are in the gps
logs  the most common and inuitive technique  and that which is used in this project is to track
the the number of consequtive gps points that are less than a specied distance

dmax

from each

other 
at each iteration a running tally
of each other  if

nt

nt

dmax
nmax then the previous nmax points are labeled
 nmax d   a supervised learning algorithm is can

is kept for the number of gps points that are within

is greater than some threshold

as invalid  to compute the threshold parameters

technique  in live taxi data  the taxi driver will activate a counter to track the distance crossed
by the vehicle and the time spent farying the customer accross town  this is used to construct the
proper labels for motion and stopping of the taxi  if this secondary informaiton is not available 
another option is to visually inpect a large data set of gps data  and manually identify the regions
where a stop has taken place 

    map matching
map matching is a widely studied problem in transportation research is perhaps the most computaionally dicult and important subcomponent  the input is a time indexed sequence of gps
coordinates

 xi yi  

 t  x  y         tn xn yn    where ti

is a standard unix timestamp data structure  and pairs

represent latitude and longitude coordinate  map matching attempts to eciently and accu 

rately match consecutive points to sequence of links representing road segments in a stardard city
map  standard map matching outputs 



n
o
n
o
  
  
t  start t  end e n ne        tn start tn end en
n ne   

 

th
represents a set road segment links for the i
path inthegral  also  ei is the set of
ne 
all indices representing such path edges  figure       shows a simple example of map matching 

here

ein

map matching is done in a variety of ways depending on a tradeo of accuracy and eciency 
for the task of realtime high capacity map matching  several techniques involving hueristic graph
search are shown in            these techniques follow a general strategy of greedily adding road
segments to a solution set as points are processed  each candidate road segment receives a score by
a distance function dened on road segments and gps probes  a standard distance function is is
given by nding the shortest distance between the gps point and point on the road segment 
given a road segment dened by all convex combinations of two gps coordinates  a and b  
ea b     x  y                x  y    a         b  points inside the road segment are dened as 
ea b      a         b   with the standard distance function used map matching algorithms is 
n mind

   xy  a   
gp s  ea b     xy  
  and    xy  b    
d   xy    e    

min dgp s  a  xy   dgp s  b  xy   

where

dgp s  p  p   and    p    p   

else

represent standard geographic distance and angle between

two gps coordinates  after the candidate pool grows to a specied size  pruning via a hueristic
shrinks the candidate pool 

fi  supervised learning for trac estimation

 

fig     example of map matching of set of gps points  input is set of points with outputted path

integrals 

  supervised learning for trac estimation
this report explores the use of supervised learning for trac inference on links on a road network 
each technique relies on dierent modelling assumptions and produces dierent estimation results
and has particular advantages and disadvantages that dictates use in individual situations 
the most general form of learning and prediction is that after learning takes place  some model
parameterized by



will be available 

some prediction function 

f


  
ein ne   
i

will use the

learned parameters  and output an estimate of the travel times through a new set of links

 i 
en nei  

    regression technique 
regression techniques rely on an additive model for travel times 

that is the travel time for a

commuter traversing a set of links will be the sum of the travel times for the set of links traversed 
therefore if the current link travel times are already known  then the estimated travel time for
some path through the network in gure       will the sum of the link travel times 

f



ein

 p
 

  nei t  en  
nei

 

under a regression model  each t

 ei     i

therefore given link travel times  any observed travel

time will follow


p
p
 
y i   nei t  en    n
nei t  en     
y   x   e
throughout this project  we make dierent assumptions on the observed travel times depending
on whether long term averages  or short term deviations are being computed 

fi  supervised learning for trac estimation
     

 

long run historical travel time

a goal of trac ow analysis is having long term historic data on ow volume through various
links  to construct a regression model taking into account spatial variation of historic tt values 
we can assume that adjacent links are distributed normally 


t  en    t  en     n       n               n    
this enforces a penalty on sudden changes to the historic means  as well as makes estimation
of parameters tractable when the maximum likelihood estimator is underdetermined 

with this

assumption we construct the following maximum a posteriori estimator 

 p
p
log p y i   nei t  en     m log p  t  em    t  em    
t e 

 
p
p
p
  arg min i    y i  nei t  en         k m t  em    t  em   k 

   arg max

p

i

t e 

  arg min ky  xk     kdk 


which is simply a ridge regression or l  regression with smoothness penalty on the variation
of historic mean along dierent links     

     

short term incidence detection on live data

given a short time window  on order of   hour  we assume the distribution on any given day for link

t  ei     i   i where i is

a historical average and

that day from the historical average 

i is

random variable representing the deviation

the important assumption here is that this deviation is

distributed with a heavy tail   i e  no deviation with high probability and large deviation with low
probability   to model this  we use a laplacian prior distrbution 

p i       e 


   

given a set of historic data  and a small number of measured daily measured paths  we can use
this model to computer short term deviations from the historic values 

 p
   arg max p log p y i   p

i
nei t  en    
m log p  t  em   
t  e 


p
p
p
 
 
  arg min i    y i  nei t  en         m kt  em  k 
t  e 

 

  arg min ky  x      k      k   k 


therefore in every small time window that live probe data arrives  an l  regression is performed
estimate deviations from the historic travel times  since probe volume is small compared to the
number of links  this technique makes intuitive sense  when performing live prediction we assume
that the parameters

           are already computed  therefore in each p
time window used
  i are computed and predictions are given by nei  n   n   

for

regression  the parameters

     

computing model parameters

in modelling travel time distributions  we explicitley separate long term and short term behaviour to
easily estimate the model parameters  with this  parameter estimation is performed in two stages 
first  a subset of the training data is aggregated and crossvalidation is performed to determine
and appropriate

   

in the second stage  the short term model parameter

 



is computed with

similar cross validation with training data  see note       this data processing model is shown in
gure          this is a visual interpretation of how data processing takes place  each block is a
tuple of paths and travel times  i e  x  and y from gure        

fi  experimental results

 

fig     data processing model for estimating historic means for link travel times  as well as l 

regression parameters  test data used daily to compute short term deviations from historic
average 

tab     input data statistics                           days      am 

min

max

mean

std

  

   

    

    

raw data in sector

  

  

    

    

processed path integrals

 

    travel time median backprojection 
the second general technique used in travel time estimation is to merely backproject the travel
time distances across each traversed link weighted proportionally to the lenght of the links  after
many paths are processed  each link will map to a list of backprojected travel time values  the
travel time estimate of the link is simply the median over the distribution  we use this algorithm
as a naive baseline to compare the more sophisticated model 

  experimental results
    preprocessing results
components of the system architecture were implemented according to gure     to process oating
car data from approximately    taxi s over the course of about one month in urban new delhi 
the total dataset of gps logs span    days  dates          to            the data was sampled
infrequently at about     minutes  the reason for this is that some gps devices sampled every  
minutes while some sampled every   minutes  since the focus of this report is the core inference
techniques  a single   km highway  gure        was chosen for analysis instead of a region of the
city  or the entire city  instead of an entire city  another reason why a long highway was chosen
is that the fundemental assumption that link travel times can be added to estimate the combined
travel time is a more accurate model for highways than for city arterial roads     

  recall that this separation between long term historic data and short term uctuations is an assumption we
explicitly make  a more thorough model would merely include daily laplacian deviations into the rst model 
however comstructing an estimator for this latent variable model  requires an em algorithm  this is a denately a
future extension of our work 

fi  experimental results

 

fig     single road learning experiment 

fig     track creation process   primary reprocessing step 

the data included all fcd logs that were contained in the geospatial coordinates shown in gure
       therefore much of the data belonged to taxi s that might have traversed nearby streets  to
isolate the data required for inference of the highlighted street  full map matching on the data was
implement and used to seperate fcd data traversing nearby streets  additionally  stop detection
as described earlier was used to further remove artifacts that would cause false positives in incident
detection  statistics on the raw input data and nal post processed data are given in table       
for this project  additional processing was done to remove any artifacts caused by missed detections
of taxi stops  from table       it is apparent that a signicant proportion of the data contained
stops or belonged to paths on nearby streets 
figure       shows a sample of the processing done  the each green point was determined an
acceptable path integral end point  while red points were rejected  the dashed line indicates the
start and end of each path integral  all logs for this example were sampled uniformly  by visual
inspection  one can infere that the upper left quadrant will have much lower travel times  than the
lower right portion of the street 

fi  experimental results

 

tab     main results comparing   algorithms 

technique

error rate    of true 

std dev 

n

historical average

       

   

   

historical average and incidence detection

      

   

   

median backproject

      

   

   

    

condence interval

             
             
              

    inference results
supervised learning of post processed data follows the model presented in gure         

 
ntrain

here 

blocks  each representing one day s paths  were chosen to learn the historical travel time

model  from the entire

 
ntrain

  fold cross validation was used with varying model parameter

to estimate the optimal historical travel time

 

that is the

 

 

yielding the lowest cross validation

error for the data was chosen as the optimal parameter  figure       shows the results from cross
validation  the historical travel times  optimal

  and

minimum training error 

error for training historic travel times yielded an optimal

         

and

cross validation

m semin       

fitting the parameters for the daily incidence model follows an identical methodology as for the
long term historic mean estimation 

 
ntrain

days of data are used  wher for each day  leave one out

cross validation  since the number of path integrals is on the order of          is performed where
parameter



is estimated with the training data and used to predict the travel time of the one out

path integral tt  as before 

 

is varied and an optimal value can be determined by looking at

the lowest test error 

    test prediction
finally with the model parameters computed  the incident detection technique is tested as follows 
all data not used in the rst two phases  ntest   is tested for each individual day  whereby all but
one of the path integrals is assumed to be the day s probe data  i e  leave one out model validation 
 

this data is used to to nd the sparse deviations from the historic mean

  

by solving the

lasso problem presented earlier  the deviations are then incorporated with the historic mean to
provide the predicted travel time 

fi  conclusions and future directions

 

in this report we tested   separate algorithms to compare their results  multiple random assign 

 

 

ments were made between the    les and the subsets used for training and testing  ntrain   ntrain   ntest   
first we merely use the historic means as a default estimate  without updating the link travel times
by any live data  second we apply the incidence detection technique introduced in the paper  finally we apply the naive median backproject technique  table       summarizes the nal computed
test errors for the dierent techniques discussed 

the results show an improvement of incidence

detection over merely using the historical average via regularized least square 

the worst tech 

nique was the median backproject  the error values are in percentage of true link travel time  a
simple statistical analysis of the incidence detection results shows that the improvement is statistically signicant  under

   

condence interval there is a statistically signicant dierence in each

algorithms results 

  conclusions and future directions
this report presents a unied architecture for floating car data based travel time inference as well
as proposes an incidence based inference technique that is evaluated and shown to be quiet eective
at estimating travel times on various links in transportation networks  the largest impediment in
achieving more accuracy appears to be the amount of data required  the road network was chosen
since from the data available  it had the highest density of fcd  regardless  at the rush hour time
of     am an average of    or so path integrals were constructed  therefore future tests require a
higher density of fcd  also  a unied em algorithm  mentioned in the footnotes  for training the
historical data  while incorporating the sparse deviations model should be investigated 

references
    dongdong wu  tongyu zhu  weifeng lv  xin gao      a heuristic map matching algorithm
by using vector based recognition   computing in the global information technology       
iccgi       international multi conference on   vol   no   pp            march     
    dongdong wu  tongyu zhu  weifeng lv  xin gao     a quick map matching algorithm by
using grid based selecting   computing in the global information technology        iccgi
      international multi conference on   vol   no   pp            march     
    marchal f   j k  hackney  k w  axhausen  ecient map matching of large gps data setstests on a speed monitoring experiment in zurich   strc     
    rajagopal r   large monitoring systems  data analysis  design and deployment        
    kwong  r  kavaler  r  rajagopal  and p  varaiya  arterial travel time estimation based on
vehicle re identication using wireless magnetic sensors  transportation research part c      
    hastie  t  tibshirani r  and friedman j  the elements of statistical learning  data mining 
inference  and prediction       

fi