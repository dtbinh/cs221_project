a framework for recognizing hand gestures
david knight
cs    student

matthew tang
cs    student

hendrik dahlkamp
mentor

christian plagemann
mentor

december         

 

introduction

traditional methods for user input  consisting of fixed key input and point click devices  are slowly
being supplemented and enhanced by touch technologies and their associated touch gestures  while
these methods have proven effective  they are limited in scope in that they require a physical object
or surface with which a user must interact 
the use of hand gestures in free space is often seen as an intuitive next step in the progression of
user input technologies  a system that takes cues from such gestures would unshackle a user from
physical devices  using such systems  a user only needs their body motion to signal specific actions
that can be picked up by a camera and interpreted by appropriate computer vision algorithms 
in this paper  we explore the use of machine learning to process and interpret image sequences in
order to correctly recognize and classify a couple simple hand gestures  our work is part of a larger
body of research by computer science ph d  student hendrik dahlkamp and computer science
postdoctoral researcher christian plagemann to create a gesture based ui that allows users to
manipulate virtual objects in  d space  an active infrared camera is used to capture both infrared
intensity data as well as depth information  unlike a passive rgb camera  the depth information
captured by the infrared camera greatly simplifies segmentation of foreground objects  existing
components of dahlkamps vision system can track the movement of a hand once it has been
recognized as such  crop out the region around the hand  and perform the necessary background
subtraction to produce the training sets we use  recognizing a hand gesture  then  requires that we
be able to    differentiate a hand image from a nonhand image  and    correlate a sequence of
hand images with a specific gesture  this paper comprises the development of these two functional
blocks using machine learning techniques 

 
   

hand classification
training data

given an image  we need to be able to classify it as a hand image or nonhand image  to do
this using a machine learning algorithm  we need an appropriate set of training data  this data is
obtained by capturing typical scenes of people giving hand gestures in a room with an active infrared
camera  the pre existing hand tracker provided by hendrik dahlkamp then crops out   x   pixel 
  bit grayscale images from these scenes and performs the appropriate background subtraction on
them  some of these images contain hands  while others contain various other objects in the scene 
 

fi a  hand image

 b  hand image

 c  nonhand image

figure    sample hand and nonhand images 

labeling these images is a matter of separating the hand images  e g  figs    a     b   from the
nonhand images  e g  fig    c   

   

feature selection

the main features for characterizing our various images are normalized bin values from a pyramid
of histogram of oriented gradients  phog   obtaining a histogram of oriented gradients  hog 
involves calculating a gradient direction and magnitude for every pixel in the image and binning
these gradients by their direction with a weight based on their magnitude      phog extends this
method by calculating a histogram for a region  subdividing the region into sub regions  calculating
histograms for each sub region  and repeating this process  we used an implementation of phog
created by the visual geometry group at the university of oxford      allowing us to configure
subdivision level depth  bin resolution  and histogram range 
our use of hog is motivated by the similarity of our problem of recognizing hands to the
objective of detecting humans in the original hog paper      additionally  intuition suggests that
gradient direction information should be able to highlight differences between a hand versus other
blob shapes  phog gives us the added flexibility of specifying sublevels on which to take hog
measurements  in addition to other hog related parameters  the exact parameters we used were
chosen by performing       cross validation on our test data 

   

training and testing

recall that our goal is to label and track hand positions  we can therefore afford to occasionally
classify a hand as a nonhand since we would  over a large number of frames  overwhelmingly
label the image content as a hand  once a hand has been confirmed over a few frames  the
object tracker will start providing a steady stream of known hand images  however  we can ill
afford to classify a nonhand object as a hand on more than a few occasions  since the hand
tracker would start tracking this nonhand object  the precision of our algorithm  then  is more
important than its recall 
using a simple support vector machine  svm  with a linear classifier  and using       cross
validation to train and test  we find that we are able to quickly achieve a precision of        and
a recall of        on a training set of size         these results were obtained by using phog
with   bins level  and   sublevels  resulting in a feature vector size of                       
we next add to our feature vector a histogram of intensities  binning intensity values along with
our original oriented gradient values  this quick addition  effectively doubling the length of our
 

fifigure    hidden markov model 

feature vector to     dimensions  manages to improve our precision to        and our recall to
        which is sufficient for our purposes 

 
   

gesture classification
hand position discrimination

in the previous section  we were able to differentiate between hand and nonhand images with
high accuracy  that classifier  used in conjunction with the hand tracker  will now give us a
sequence of hand images from which we can infer gestures  to do so  we first characterize a gesture
with a few defining positions  for example  a grasping motion might be characterized by an open
palm followed by a closed fist  or any number of intermediate positions  for the sake of simplicity
we will consider only a two state model for our gesture   thus  one way to recognize a gesture
from our set of images is to label any sequence consisting of an open hand followed quickly by a
closed hand as a grasp gesture 
to classify hand positions  we take our image sequence and manually label the images preceding
a grasping motion as open and label the images following a grasping motion as closed  we
then attempt to train a machine learning algorithm on this set of data in order to give us an
initial classification of hand states  we find that with phog and binned intensity values  we are
unable to achieve an accuracy greater than      various attempts to improve accuracy using these
features  e g  using a mixture of gaussians model  proved ineffective  our improved approach is to
use the images pixel intensity values as a feature vector coupled with a svm with a second order
polynomial kernel  this method yields a classifier with     accuracy        cross validation on a
training set of        and is used in the final recognition system 

   

modeling image sequences with a hidden markov model

so far  we have attempted to label every image in our sequence independently  ignoring the correlation between temporally adjacent images  we can exploit this temporal correlation by modeling the
sequence with a hidden markov model  suppose that at time t the hand we are tracking has a true
state  open or closed  given by xt   given an image of the hand yt   we can calculate p yt  xt   by
fitting a logistic function to the margin output of the svm classifier  furthermore  we can estimate
p xt    xt   by counting transitions in our training sequences  that is  given a typical sequence of
p  
x    x            xn ordered in time  we estimate p xt     a xt   b    n  n
n     xn     a   xn   b  
using forward recursion     we can calculate  on the fly  p xt  y t   where y t    y    y            yt    this
allows us to take into account all previous observations rather than just the current observation  to
improve the accuracy of our estimate xt   we can delay estimation until time t   d  allowing us to use
forward backward recursion to calculate p xt  y t d    this will introduce a delay d in our system  but
 

fifigure    predictions on a captured    frames s image sequence  shown as gesture sequence   in figure    
images that did not resemble either open or closed hands were labelled      introduced a   frame delay
for the forward backward recursion  all parameters were obtained by training on   other gesture sequences
of similar length        cross validation  

allow us to consider future observations in our estimation of xt   we note here that the larger the
delay d  the larger the number of running estimates we need to store  since p xt  y t d   is calculated
from p xt  y t    p xt   y t              p xt  y t d     however  this extra storage can be considered negligible
because the delay d should only be on the order of a few frames  the fundamental tradeoff in
choosing d is therefore accuracy vs  system response time 
we can now define grasp and release gestures as points in the frame classification output
where confidence values transition from      to      and      to       respectively  without
using forward backward recursion  the raw frame by frame output of classification is fairly noisy as
is shown in figure    results in figure   show that trying to identify gestures from the raw output
results in an overall high number of false identifications  in particular  some gesture sequences show
more false identifications than true identifications when using the raw output  applying forwardbackward recursion significantly reduces the number of false identifications  retains most of the true
identifications  and slightly increases the frame classification accuracy 
finally  we note that there is a large amount of freedom in choosing criteria for a grasping
event  for example  we can declare a gesture occurrence when our estimated state is open for
more than some fixed number of frames  followed by closed for some fixed number of frames 
such a criterion would filter out overly quick open close open events  requiring the user to hold
his her hand closed for a fixed duration before a grasping event is recognized  additionally  the
derivative of the classification output at a transition point can provide information about the speed
of a gesture  different gestures can be defined for the same basic motion that occurs at different
speeds 

 

conclusion

while using forward backward recursion greatly reduces the number of false gesture identifications 
a few false identifications remain in many gesture sequence tests  most of the anomalies come
from interpreting quick bursts of classification uncertainty as a full transition and can likely be
addressed by imposing more advanced criteria for registering a gesture as was explained in the
 

fifigure    gesture recognition results both with and without forward backward recursion  fbr   gestures
were identified by state transition without consideration for gesture speed  results for each sequence were
obtained by training on the   other gesture sequences of similar length        cross validation  

previous section  also  forward backward recursion slightly decreased the number of true gesture
identifications in a few of the sequence tests  the exact cause of this effect needs to be further
investigated  but it might actually be desirable behavior if the gestures that are being skipped
over are due to uncertain user input  the next step in this project is to implement the gesture
recognition system in c so that it can be integrated with the rest of the vision system developed by
dahlkamp and plagemann  this integration will allow the vision system to be tested as a whole 
overall  we have provided a basic framework with which to classify hand gestures  given
appropriate methods for distinguishing between hand images and nonhand images  and for
tracking a hand once it has been identified  we can produce a sequence of hand images that can
be further analyzed for the purposes of gesture recognition  given a specific gesture we want to
detect  we identify key positions of that gesture and develop classifiers that are able to correctly
bin the captured images into one of the identified key positions  we can then estimate transition
probabilities between these positions by counting and averaging transition occurrences in a typical
sequence of gestures  allowing us to use forward backward recursion to obtain accurate estimates
of the underlying state at any given time  various criteria can then be used to recognize a gesture
 e g  when these key positions have been realized in a certain order   providing us with a robust
hand gesture detector 

references
    n  dalal  b  triggs  histograms of oriented gradients for human detection  proc  ieee
conf  computer vision and pattern recognition  vol                   
    a  bosch  a  zisserman  pyramid histogram of oriented gradients  phog   university of
oxford visual geometry group  http   www robots ox ac uk  vgg research caltech phog html 
    l  e  baum  an inequality and associated maximization technique in statistical estimation
for probabilistic functions of markov processes  inequalities  vol               

 

fi