object detection with partial occlusion
based on a deformable parts based model
johnson hsieh  johnsonhsieh gmail com   alexander chia  alexchia stanford edu 
abstract    object occlusion presents a major challenge for robust object detection in static images  we describe
an object detection system that explicitly models and accounts for arbitrary but consistent occlusion patterns  our
model builds on the state of the art object detection system based on a deformable parts based model  weve
augmented this model with latent binary visibility variables for each pixel  as well as pairwise consistency
visibility potentials  we will show an efficient inference algorithm for matching our visibility model to a test
image during detection  in training  we employ a latent svm learning framework  as well as reusing parts of the
inference algorithm that weve developed for matching  our system is trained and tested on the pascal object
detection challenge dataset 
 

introduction

occlusion is a common problem in real world images  and videos  and presents a major challenge for object detection  in
the caltech pedestrian dataset      for example  over     of pedestrians are occluded in at least one frame of a video
sequence and     are occluded in all frames  where the occlusion was categorized as heavy in nearly half of these cases 
dollar et al      showed that detection performance drops significantly even under partial occlusion  and drastically under
heavy occlusion  if only fully visible examples are used during training  then many positive examples are simply
discarded and the training distribution does not match the test distribution  if partially occluded examples are included 
this may lead to tolerance of occlusion in the test set  but the trained model will be more noisy and less robust 
the current state of the art object detection system built by felzenszwalb et al  represents highly variable objects using
mixtures of multiscale deformable part models      our system builds on top of this model and augments it with latent
binary visibility variable for each pixel  as well as pairwise consistency visibility potentials  this augmented object
detection system will allow us to explicitly model and account for arbitrary but consistent occlusion patterns
 

background and related work

there have been previous attempts to model occlusion for the object detection task        the work done by vedaldi and
zisserman     is the most similar to ours  they propose using binary variables to indicate the visibility of the cells inside
a detection window  however  rather than inferring the values of those variables  they treat them as a deterministic
function of the position of the bounding boxes  i e   when the bounding box is partially outside of the image  those
variables corresponding to the cells that are outside of the image will be set to invisible  occlusion is therefore modeled
only on image borders  and occlusion within the image frame is ignored 
 

object models

the felzenszwalb system is the current state of the art object detection system  in order to see the benefits of modeling
latent visibility features  we will analyze three simple baseline models to illustrate the headroom for which we will work
towards 
in each of the three baseline models  we used some predetermined visibility mask for each training and test image  the
first model uses the ground truth visibility masks  which we obtained through amazon mechanical turk  the second and
third models use a precomputed segmentation library to obtain the masks  which we will describe in detail later  given
these predetermined masks  we build a model using the masked training examples  and also use the masks in the testing

 

fiphase  we would like to see whether we could actually do better than felzenszwalbs model given that we now have prior
knowledge of the object visibility in each image  were using predetermined masks for now  so that we dont have to
worry about inferring the masks yet  in either training or testing 
given the pixel level visibility mask  we convert it into a cell level visibility mask and apply it to the  cell level  hog
features      we also take the complement of the mask and apply it to the hog features to get a set of dual masks  we
have done a preliminary study which showed some object classes  such as the bicycle class  gained significant
performance improvements from using the positive masked hog features  while other classes  like the horse class 
experienced significant performance improvements from using the inverse masked hog features  see fig      thus we
hope that by using a set of dual masked hog features  we can compensate for this variability and achieve reasonable
performance across all object classes 
in order to convert from the pixel level visibility mask to the cell level visibility mask  we used the fraction of visible
pixels in each cell as the cell level visibility  note that our given pixel level visibility mask is a binary mask  but the celllevel visibility mask is now a weighted mask taking on values between zero to one for each cell  to improve accuracy  we
expand the pixel mask by   pixels and performed a gaussian blur before converting it to a cell level mask  this helps to
avoid artificial sharp edges  which could be undesirably learned  the result is that both masks in the dual keeps the edge
features of the object  but the positive mask significantly downweights the background features while the inverse mask
significantly downweights the interior features of the object 
where h has c cells and hk i  is the hog

formally  felzenszwalbs hog features are defined as
feature vector of the kth cell in the part pi 
in our model we modify the hog features to be
 i 
 i 
  i   h  i         v  i 
  h  pi  v      h  pi   v  i    v     v  i h  i       v  i 
c hc     v
c  hc  

       h  pi   v  i    v     h  pi   v  i    v  
where v is the set of binary pixel level visibility variables  and
is the weighted cell level visibility variable for the kth
cell in the part pi  note that
corresponds to the positive masked hog features  and
corresponds to the
inverse masked hog features 
   

matching

once weve modified the hog features with the visibility information  we run the felzenszwalb training procedure 
which performs optimization using a coordinate descent approach  and employs a latent svm implemented by stochastic
gradient descent 
however  because weve modified the feature vector by incorporating a set of dual masked features  the matching process
becomes more complicated  specifically  how do we score a match to find the best match  in the felzenszwalb model 
the set of filter weights fi is learnt for each part pi  the score for the best root and part location match  not accounting for
part deformation  is then
in our model  we learn two sets of filter weights  one set for the positive masked features  the other for the inverse masked
features  but to calculate the score  we have various options 
option     sum  
option     overall max  

 

fioption     max per part  
in our experiments  we found that taking the overall max  option     gave best results 
   

training

again  we notice that the new dual masked features complicate the training procedure  we can reuse the matching logic
as described above  but we now have further complications with learning the latent object parts  we have several options 
   learn two sets of part anchors and deformation costs for each of the positive and inverse masked features     restrict the
anchors to be the same  but learn two sets of part deformation costs     learn only one set of part anchors and deformation
costs  to be shared by the two sets of masked features in the dual  due to time constraints  we only explored the first
option  learning one object class over the pascal voc     data set took    hours on a    ghz   core intel xeon
processor x     with    gb of memory running ubuntu lucid 
   

masks

as mentioned above  we obtained three sets of visibility masks  the first set is obtained via human annotation through
amazon mechanical turk  amt   the second and third sets are obtained through an image segmentation library written
by pawan kumar  who works in prof  daphne kollers research group  this segmentation library is trained on the
voc     data set  and given a test image  returns an output image with segmented regions  with each region labeled with
a predicted object class  we use this output in two ways     take the regions with our desired class label as the visibility
mask     include all regions within the desired objects bounding box  allowing at most     of each region to be outside
the bounding box  figure   illustrates an example of these   masks 
a 

b 

c 

d 

e 

fig     a  original image  b  ground truth visibility mask obtained from amt  c  image segmentation regions  d  segmented regions labeled as
bicycle class  e  union of segmented regions with     of each region inside the bounding box 

 

results

in order to evaluate our models  we plotted the precision recall curves and compared the average precision scores  we
also broke down the evaluation and analysis into occluded vs  non occluded image sets  the voc     dataset included
annotations for each object instance indicating whether or not they were occluded  voc called them truncated  

 

fifigure   shows the pr curves for the bicycle and horse models generated from the ground truth masks  we see that
overall  both sets improved decently  more importantly  the occluded subset improved significantly  especially for the
horse class  this looks very promising  in figures   and    we show the pr curves for models generated from the two
precomputed segmentation outputs 
a 

b 

fig     bicycle and horse models generated from the ground truth masks  we breakdown the image set into an occluded subset and a non occluded
subset  a  bicycle  b  horse 
a 

b 

fig     bicycle and horse models generated from the precomputed segmentation masks  using segmented regions with the desired class label as the
visibility mask  we breakdown the image set into an occluded subset and a non occluded subset  a  bicycle  b  horse 

a 

b 

fig     bicycle and horse models generated from the precomputed segmentation masks  using all segmented regions that are     within the
bounding box  we breakdown the image set into an occluded subset and a non occluded subset  a  bicycle  b  horse 

the first segmentation model  which used the segmented region class labels  performed quite poorly for both the bicycle
and horse object classes  the second segmentation model  which used the regions within the bounding box as the
visibility mask  performed much better  we see that the occluded subsets received the biggest gains  and the occluded

 

fisubset for the horse class improved the most  by almost     average precision  ap  points  the overall average precision
for the bicycle class stayed about the same compared to felzenszwalbs results  and for the horse class  we see a small
      gain in ap  however  if our data set included many more occluded images  the overall performance would
increase  as shown by our improved detection performance on the occluded subset 
figure   shows our other experimental results  before coming up with the dual mask model  we investigated a positiveonly visibility mask model and an inverse only visibility mask model  the positive only mask model gave the bicycle
class a slight increase in performance  but performed poorly on the horse class  the inverse only mask model yielded the
exact opposite performance for the two classes  this suggests that different parts of the image contain valuable features
for different object classes  and gave us the motivation to explore a dual mask model 

a 

b 

fig     other experimental models we built for the bicycle and horse classes  a  bicycle  b  horse 

in figure    we show an example of the bicycle models that we trained  note that for the visibility part of the dual model 
the background regions are significantly downweighted  while for the inverse visibility part of the dual model  the interior
regions of the bicycle features are downweighted  also note that both parts of the dual keep the edge features fully  this
is a consequence of how we applied the visibility and inverse visibility masks to the hog features and is described
previously in section   
 

conclusions and future work

our research showed that there is promise in building an object detection model on top of the state of the art object
detector by augmenting it with explicit visibility variables  our results showed that  given decent pixel level visibility
masks  detection performance improves significantly for the occluded subset  and slightly for the overall test set 
in this quarter  we did not have enough time to continue on to implement the inference algorithms that would determine
the visibility masks automatically at training and testing  we have worked out most of the math for this inference
procedure  which includes pairwise consistency visibility potentials as mentioned in the abstract  in the future  we aim to
implement this algorithm and analyze the performance results  in section     above  we mentioned various other ways to
learn the dual feature sets especially for the latent object parts  part of our future work would also be to explore these
options in detail and hopefully find a method that exceeds our current performance 

 

fia 

b  i 

b  ii 

fig     bicycle models  a  felzenszwalbs model  b  our dual masked model  i  the visibility part of the dual model  ii  the inverse visibility part of
the dual model 

 

acknowledgements

the research presented in this paper was work done in conjunction for the cs   a autumn      class  we worked with
tianshi gao and prof  daphne koller throughout the quarter  and have received much valuable advice and feedback from
them  we also thank pawan kumar for his kind permission for the use of his image segmentation library 
references
    p  dollar  c  wojek  b s   p  perona  perdestrian detection  a benchmark  in cvpr        
    p  felzenszwalb  r  girshick  d  mcallester  and d  ramanan  object detection with discriminatively trained part based
models  in pami        
    j  winn  j  shotton  the layout consistent random field for recognizing and segmenting partially occluded objects  in cvpr 
      
    x  wang  t  han  s  yan  an hog lbp human detector with partial occlusion handling  in iccv        
    a  vedaldi  a  zisserman  structured output regression for detection with partial truncation  in nips       
    n  dalal  b  triggs  histograms of oriented gradients for human detection  in cvpr        

 

fi