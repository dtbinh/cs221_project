unsupervised learning of multimodal
features  images and text
maurizio calo caligaris
advised by andrew l  maas and andrew y  ng
cs    final project
 maurizio amaas ang  cs stanford edu

 

in trod u cti on

multimodal learning involves relating information from di sparate sources  for example 
wikipedia contains text  audio and images  youtube contains audio  video and text  and
flickr contains images and text  our goal is to find meaningful representations of mutimodal
data so as to capture as much information as possible 
hand engineering task specific features for single modalities  e g  audio or vision  is by
itself a difficult task and is often very time consuming  the challenge gets significantly
pronounced when the data comes from various different modalities  e g  images and text  
thus  we propose an unsupervised learning model which uses images and tags from flickr to
learn joint features that model image and text correlations  furthermore  we demonstrate
cross modality feature learning  in which better features for one modality  e g  images  can
be learned if multiple modalities  e g  images and text  are present during feature learning
time 
in the following sections  we present the network architectures we use to learn bi modal and
cross modal features  we describe an experimental setting which demonstrates that we are
indeed able to learn features that effectively capture information from different modalities
and that we can further improve on computer vision features if we have other modalities  e g
text  available during feature learning time  we then conclude and offer suggestions for
further work 

 

dataset

the nus wide dataset provided by the university of singapore    contains links to
        images from image sharing site flickr com  together with their corresponding tags   
the nus wide dataset also provides a list of      unique tags which appear more than    
times in the dataset and also appear in the wordnet  by considering only tags present in this
list  we essentially get rid of the problem that many tags in flickr are noisy  e g  misspelled
tags or in tags in another language  or are irrelevant for the task of feature learning  e g 
proper names  model of the camera used to take picture  

 

meth od ol ogy

a key challenge in this work is to figure out a way to combine both visual and ling uistic
aspects in a way which allows for an autoencoder to learn meaningful representations of the
data  since the correlations between image and text data are highly non  linear  it is hard for
an autoencoder or a restricted boltzmann machine  rbm  to form multimodal
representations of the data when fed in unprocessed text and images as input 

 

these images were obtained by randomly crawling more than         images from flick r s
publically available api  and after removing duplicates as well as those images that contain
inappropriate length width ratios or whose sizes are too small 

fisubsequently  we describe the deep learning approach we used to learn features that jointly
model image and text correlations  taking as input images of variable size along with their
corresponding tags 

   

vi s u a l a s p e c t

to learn the visual features  we have used the  visual words  model often used in computer
vision  in this model  we dense sample each image to extract low level sift descriptors  we
then apply the k means clustering algorithm to find cluster centroids  which forms a
 codebook  or  visual words  of canonical descriptors  finally  we use the codebook to map
input patches into   of k code vector   hard assignment    and ultimately represent each
image as a  histogram  of visual words  a      dimensional vector in which the k th entry
indicates how many times the k th canonical descriptor appears in a given image  that
histogram is length normalized to take into account the variability in size of the different
images 
   

linguistic aspect

we use a bag of words model to represent the tags associated with an image  using the
dictionary provided by the nus wide dataset  we represent text data corresponding to a
each image as a      dimensional binary vector whose i th entry is either   or   depending
on whether the i th word from the dictionary belongs to the list of tags for the given image 
since the tags are so sparse       tags per image on average  it is difficult for an
autoencoder or a restricted boltzmann machine rbm  to learn meaningful representations
of the data  thus  we map the binary valued vector of words into a more compact vector
space as in      more specifically  we form a vector space model which learns semantically
sensitive word representations via a probabilistic model of tag co occurrences and represents
each word in the dictionary as a    dimensional vector  for a given document  we use as
features the mean representation vector  an average of the word representations for all words
appearing in the document 
   
j o i n t f e a t u re l e a r n i n g
having learned a vector space
model for the linguistic data 
we concatenate the mean tag
vectors
representing
each
image to the corresponding
visual histogram  and feed that
as input to an autoencoder 
which attempts to reconstruct
both
modalities 
the
autoencoder consists of one
input layer  one over complete
hidden layer which captures
cross modal
correlations
between image and text  this

figure     joint feature learning architecture

layer is what constitutes our  joint  features   and a linear output layer which we set to be
equal to the input  given that the text features are very low dimensional compared to the
visual features  we modify the objective function of the autoencoder to account for different
weighting between image and text features  the final objective function for the model is
given by

fiwhere h is the hidden layer representation  w    and w    are the weights going from input to
hidden layer  and from hidden to the  linear  output layer   and  are the visual and textual
feature vectors respectively and  is a free parameter of the model which controls the
relative importance of the visual and linguistic features  the first two terms thus correspond
to the reconstruction errors for the different modalities  whereas the last two terms are
regularization terms which tend to decrease the frobenius norm of the weight matrices and
prevent overfitting  we run stochastic gradient descent to find the optimal weight
parameters  which are used to compute the hidden activations for each example  our desired
joint feature representation  
   

c ro s s   m o d a l i t y l e a r n i n g

the joint feature learning model is not very robust to missing modalities   so it can t be used
in settings in which multiple modalities are available during training time but not in test ing
time  thus  we propose two alternate models which improve existing computer vision
features if textual information
is available during feature
learning time 
in the first model   crossmodal i   figure  a   we train
a network which learns to
reconstruct the text features
given only visual features as
input  therefore  if we only
have visual input available at
test time  we use the learnt
weights to compute the
corresponding hidden unit
activations and hence obtain a

figure  a 

figure  b 

multi modal representation of the data  we also propose a
similar network   cross modal ii   figure  b  which is trained to reconstruct both
modalities when given only visual data  we hypothesize that in the process of reconstructing
both modalities  the network will learn a better hidden representation of the data 

 

e xp eri men ts

we evaluate the performance of the individual components
task  the nus wide dataset provides ground truth for
    concepts  or  tags   which mainly correspond to the
most frequently occurring tags on flickr and are
consistent with concepts commonly used in image
categorization  see table     for each of the        
images  the dataset provides a binary valued   
dimensional vector  in which its i th entry indicates
whether the i th concept corresponds to the image
 which does not necessarily mean that the i th concept
appears in flickr as a tag for the given image   the fact
that the ground truths for the    concepts were manually
annotated circumvents the problem that tags in flickr are
generally incomplete and thus allows us to execute
supervised learning experiments 
to evaluate our
system in reasonable time  we only consider the    most
frequently occurring concepts for our experiments 

of the system on a tag suggestion
concept

occurrence     

sky

   

water

   

clouds

   

sunset

   

beach

   

tree

   

reflection

   

animal

   

street

   

sun

   

fifor each of the    most frequently occurring concepts in the dataset   we remove the
corresponding tag from the dictionary  otherwise the task is trivial for components which
take inputs relating to text  and train our models  joint and cross modal  in an unsupervised
fashion to learn an appropriate feature representation for the data  we use the ground truths
for these labels to separately train l  regularized logistic regression classifiers  each of
these tag detectors learns whether a specific concept corresponds to a given image  we use a
training set size of       examples and test on another       examples  using the area
under the  roc  curve  auc  metric to evaluate performance of each component of the
system on each category 
we pca whiten the visual input  histogram  to     dimensions  which we have found is
enough  and normalize the tag vector representations so as to have unit variance and zero
mean  we train the networks using    x over complete hidden layer representation and using
a weighting parameter     to control the relative importance given to the textual features
 for joint network and for cross modal ii   the parameter  was chosen over a small grid
search of parameters to find the one with best performance on the training set 

 

resu l ts
f e a t u re r e p re s e n t a t i o n
r a w ta g s   b a g o f wo r d s  
sift
r a w ta g s   s i f t
s e m a n t i c wo r d ve c t o r s
lsa
j o i n t f e a t u re s
cross modal i
c ro s s   m o d a l i i

mean auc
     
     
     
     
     
     
     
     

table    performance of individual components and combination of components of our
system on the tag suggestion task 
table   shows the auc scores of the different components averaged over the    concepts  it
turns out that linguistic information is generally more useful for this task than the visual
features  although sift does better than text on suggesting  clouds  for instance   when
simply concatenating sift histograms to the textual features  the performance decreases
because of the poorer visual features 
the non linearity introduced by the transformation of the tags into a more compact feature
space makes it harder for a linear classifier to perform well in the task of tag suggestion 
however  our semantic word vector representation still does better than other methods such
as latent semantic analysis  lsa  and is what ultimately allows us to train autoencoder
models that outperform any other part of the system  in particular  the joint feature learning
model achieves the best results on this task by taking into account all of the information
available visual and semantic and combining them to form meaningful representations of the
data  moreover  both cross modal networks outperform sift  which shows that better
features for computer vision can be learned if semantic information is available during
training 

      vi s u a l i z a t i o n s o f l e a r n e d f e a t u re s
to get an idea of how well the cross modal system is doing at reconstructing the input when
given only image features  we looked at a few examples in the test set  for which the system
was only given access to the sift features but not to its corresponding tags  and computed
the mean tag vector reconstructions by the cross modal ii system  by the way the system is
built  it is not possible to recover the actual tags  but by looking at the tags closest in the

fivector space model to the reconstruction vector we can qualitatively see that the system is
indeed doing a reasonable job at reconstructing a semantic representation of the original
input 

flickr tags  january  sailboats  surrey 
reconstructed  water  rocks  sailboat 
sea  agua  pier  fishing  canoe  creek 
seascape 

flickr tags  child  baby  infant  newborn
reconstructed  adorable  sweater  hair 
expression  smiling  fingers  look 
playful  fluffy  cute 

figure    visualizations of cross modal ii reconstructions 

 

con cl u si on s and fu rth er work

we have shown that our system effectively relates information from disparate data sources
by learning meaningful representations that capture correlations across different modalities  
we envision this work may have practical applications  such as in image retrieval or in the
organization of large personal photo albums  given that the system can be used to
automatically suggest tags  categorize images and find visually and semantically similar
images  and as a matter of fact we already have  
moreover  this work can have a significant impact in the area of computer vision research 
as a next step  we would like to evaluate performance on a standardized dataset  in
particular  we may use the same approach used to improve sift features to improve the
currently best performing image features on a dataset such as imagenet  and use those learnt
features to beat the current state of the art in imagenet 
a c k n o w l e d g me n t s
i would like to thank the university of singapore for providing the nus wide flickr
dataset  i would further like to thank andrew maas and prof  andrew ng for their invaluable
guidance on this project  as well as jiquan ngiam for his very helpful collaboration 
r e f e re n c e s
    k  e  a  van de sande  t  gevers   c  g  m  snoek         evaluating color descriptors for object and
scenerecognition  ieee transactions on pattern analysis and machine intelligence   in press  
   a  l  maas   a  y  ng          a probabilistic model for semantic word vectors  nips      workshop on deep
learning and unsupervised feature learning 
    t s chua  j  tang  r  hong  h  li  z  luo    yan tao zheng         nus wide  a real world web image
database from national university of singapore  acm international conference on image and video retrieval 
   j  ngiam  a  khosla  m  kim  j  nam  h  lee   a  y  ng          multimodal deep learning  nips      workshop
on deep learning and unsupervised feature learning 
    g  hinton   r  salakhutdinov          reducing the dimensionality of data with neural networks  science 
              

fi