parallel unsupervised feature learning with sparse autoencoder
milinda lakkam  siranush sarkizova
 milindal  sisis  stanford edu
december         

  introduction and motivation
choosing the right input features for a particular machine learning algorithm is one of the
deciding factors for a successful application but often a time consuming manual task  unsupervised feature learning algorithms provide an alternative solution by automatically learning features 
however  they are usually computationally intensive  in this project we explore different implementations of the sparse autoencoder  sae  learning algorithm  in terms of development platform as
well as optimization approach  with the objective of creating a cost  and time efficient user friendly
parallel implementation  in particular  we first focus on comparing m atlab and python as implementation frameworks and stochastic gradient descent  nonlinear conjugate gradient and l bfgs
as optimization techniques  we then evaluate two parallel implementations based on custom parallel frameworks in m atlab and python tailored towards similar algorithms 

  algorithms overview
the sparse autoencoder is a self thought algorithm for learning features derived from unlabeled
data  at its core  an autoencoder is simply a supervised learning algorithm  based on neural networks  which tries to learn the identity function i e  it uses y i    x i  to learn hw b  x   x  a sparse
autoencoder is an autoencoder which further tries to minimize the number of active hidden units
in the neural network by imposing a sparsity constraint      given a neural network architecture  a
set of unlabeled training examples  x i   rn   i           m  and a cost function j w  b  x   we train the
neural network by performing a minimization of the cost function j w  b  x   which for the sparse
autoencoder is given by 

j w  b  x    


  
  m
 l 
j w  b  x     w  w j    sparsity  kl p kp j  

m i  
j
j

the minimization can be done online  stochastic gradient descent  or offline  using the classical numerical optimization techniques such as the conjugate gradient or the l bfgs   the cg
 

fialgorithm is a modified sd technique with the successive descent directions chosen to be conjugate
to preceding directions and an accurate line minimization is performed along each search direction 
the lbfgs algorithm is a quasi newton method where gradient information from successive iterations are used to build an approximate hessian     

  implementation
we approached the sae algorithm via a stochastic gradient descent implementation in both
m atlab and python  as expected  this algorithm ran fast  as it only considers one training example at a time to make an update  but it is not readily parallelizable because every subsequent
update is based on the one self contained operation  in view of achieving a parallel implementation 
we then trained the neural network using nonlinear conjugate gradient  cg  and l bfgs to solve
the minimization problem  we saw that the line search component of these algorithms takes the
majority of the run time as it involves numerous evaluations of the cost function and its gradient 
therefore  we achieved significant speedup in gradient evaluation by finding and using the analytical  rather than numerical gradient  it turned out that the analytical gradient of the cost function 
as well as the cost function itself  are summation over all training examples and hence the entire
algorithm easily lends itself to parallelization  with this motivation  we used the scipy optimize
optimization package to implemented python versions of both methods and gauged their performance with benchmark tests against the corresponding m atlab minfunc implementations  we
saw that pythons performance is comparable to m atlab but also that  for large training sets 
l bfgs is significantly faster than cg as summarized in figure   and figure   

 a  matlab performance for cg

 b  python performance for cg

figure    m atlab  a  vs python  b  performance of cg 

 

fi a  matlab performance for l bfgs

 b  python performance for l bfgs

figure    m atlab  a  vs python  b  performance of l bfgs 

  parallelization
after determining the advantageous performance of lbfgs we proceeded to parallelize the
algorithm using the parallel m atlab framework developed by adam coates  the framework is
a realization of the map reduce concept and is tailored towards distributing an algorithm over a
large number of nodes with seamless support of data locality  a similar framework in python was
developed by another part of the project group  it will be valuable to asses the performances of
the two custom frameworks against a general distributed application framework and we are thus
currently working on a hadoop implementation  we note that we did not initially focus on such
standard frameworks since the overhead of adapting our algorithm to run on these systems could
be high 

  results
the performances of the two parallel implementations was tested with a varying number of
workers as well as training examples  we used the stanford corn cluster for all testing and we
note that the results might be slightly skewed due to differences in machine configurations  table  
and table   provide a summary of the m atlab and python run times respectively  overall with
m atlab   we achieve speedup in most cases  except for  k  but  as expected  it is more significant
when the number of training examples is larger  the speedup for    k and   million training
examples going from   to   and   to   workers is close to a factor of two which is a promising result 
similarly  with python we achieve speedup for the larger number of training examples  however 
with    k the speedup factor is close to two only between   and   nodes and lower between   and
  nodes  further testing is necessary to determine whether this is due to the framework or due to
the testing environment 

 

fiworkers
 
 
 
 

 k
  
  
  
  

  k
   
  
  
  

   k
   
   
   
   

 million
java outofmemoryerror
java outofmemoryerror
    
    

table    custom parallel matlab framework   total running time in seconds for different number
of workers and training examples 
workers
 
 
 
 

 k
  
  
   
   

  k
   
   
   
   

   k
    
    
    
   

table    custom parallel python framework   total running time in seconds for different number
of workers and training examples 

  conclusions and future work
in conclusion  we have seen that python is a feasible development platform as compared to
m atlab   furthermore  amongst readily parallelizable algorithms for neural network training 
l bfgs is more favourable than cg in terms of speed  parallelizing an efficient implementation of
l bfgs  yields promising results in the setting of both m atlab and python parallel frameworks 
in order to get a more in depth understanding of performance numbers and trade offs we would
like to run additional tests on standardized machines and with other  possibly more sophisticated 
algorithms  we are also interested in testing the parallel implementation against a widely accepted
system and are thus currently building a hadoop implementation 

acknowledgements
thanks to prof  andrew ng for the opportunity to work on this project and his guidance
throughout the quarter  thanks to adam coates for the introduction and code of the parallel
m atlab framework  and thanks to everyone in the extended project gourp  in no particular
order    bobby prochnow  abhik lahiri  raghav pasari  matt sparks  quinn slack  ali yahya 
juan batiz benet 

references
    ng  andrew  cs   a lecture notes  sparse autoencoder   stanford university  nov
          available at http   www stanford edu class archive cs cs   a cs   a      

 

fisparseautoencoder pdf 
    boyd  stephen et  al   distributed optimization and statistical learning via the alternating
direction method of multipliers   stanford university  working draft november    
      available at http   www stanford edu  boyd papers pdf admm notes draft pdf
    nocedal  jorge wright stephen j   numerical optimization  springer verlag  second edition 
available at http   users eecs northwestern edu  nocedal book 

 

fi