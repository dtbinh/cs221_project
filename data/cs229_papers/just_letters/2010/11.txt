a survey of document clustering techniques
 
comparison of lda and movmf
yu xiao
december         
abstract
this course project is mainly an overview of some widely used document clustering techniques  we
begin with the basic vector space model  through its evolution  and extend to other more elaborate and
statistically sound models  we compare two models in detail  the mixture of von mises fisher and latent
dirichlet allocation  since they have drawn wide attention in recent years due to their good performance
over other models  finally  we propose that more experiments need carrying out over multiple topic
documents  or other objects  

keywords  vsm  lsa  plsa  k means  hierarchical clustering  lda  movmf  spherical admixture
model 

 

background

nowadays the information on the internet is exploding exponentially through time  and approximately    
are stored in the form of text  so text mining has been a very hot topic  one particular research area
is document clustering  which is a major topic in the information retrieval community  and it has found
broad applications in real world  examples include search engines  typically  a search engine often returns
thousands of pages in response to a broad query  making it difficult for users to browse or to identify
relevant information  clustering methods can be used to automatically group the retrieved documents into
a list of meaningful categories  as is achieved by enterprise search engines such as northern light and
vivisimo or open source software such as carrot   also  google is known to use clustering methods to
match certain websites with a query  since a website can be viewed as a collection of topics  multi topic
document   and a query itself is a topic or a combination of several topics  this has been studied extensively
by the search engine optimization community  as to find a way to optimize a website  determine the optimal
bids on certain keywords  and thus improve the roi of online campaigns  finally  with the rising of social
network in recent years  such as facebook and twitter  more semantic data are available now which convey
considerable amount of information  take twitter as an example  there are approximately   m tweets per
day  which is equivalent to      tweets per second  researchers from the northeastern university college
of computer and information sciences and harvard medical school have developed an innovative way of
tracking the nations mood using tweets  and another two cmu researchers find twitter posts in line with
opinion polls  all these researches show the power of social computing in providing accurate assessments on
many sorts of issues  at almost no cost and on a large scale  likewise  document clustering techniques can
be used to group tweets into relevant topics  in aid of the current mere trends function used by twitter 
for all these reasons  we find document clustering techniques valuable and therefore worth studying 
the remainder of this paper is organized as follows  in section   we introduce the basic vector space
model and address its limitations  in section   we introduce some dimensionality reduction techniques  in
section   we compare two attractive models  lda and movmf  over multiple topic documents 

 

fi 

the vector space model  vsm 

   

document preprocessing

before we represent documents as tf idf vectors  we need some preprocessing  there are commonly two steps 
 first  we need to remove stop words  such as a  any  what  i  etc  since they are frequent and
carry no information  a stop words list can be found online 
 second  we need to stem the word to its origin  which means we only consider the root form of words 
for example  ran  running  runs are all stemmed to run  and happy  happiness  happily are all stemmed
to happi  there are certain criteria  and the standard algorithm is the porters stemmer  which is
also free online  a more elaborate way of stemming is by using the wordnet  which in addition to
suffix stripping also groups words into synsets  and leads to an ontology based  instead of word based 
document clustering method  related work can be found in      

   

tf idf matrix

the vector space model is the basic model for document clustering  upon which many modified models
are based  we briefly review a few essential topics to provide a sufficient background for understanding
document clustering 
in this model  each document  dj   is first represented as a term frequency vector in the term space 
d jtf    tf j   tf j        tfv j   

j              d

   

where tfij is the frequency of the ith term in document dj   v is the total number of the selected vocabulary 
and d is the total number of documents in the collection 
next  we weight each term based on its inverse document frequency  idf   the basic idea is that if a
term appears frequently across all documents in a collection  its discriminating power should be discounted 
so finally  we obtain a tf  idf vector for each document 
d j    tf j  idf    tf j  idf         tfv j  idfv   

j              d

   

put these tf  idf vectors together  we get a tf  idf matrix 
 tf  idf  ij   tfij  idfi

   

i              v  

j              d

   

similarity measure and clustering algorithm

the most commonly chosen measure is the cosine similarity  as mentioned in      the choice of a similarity
measure can be crucial to the performance of a clustering procedure  and the euclidean or mahalanobis
distance behave poorly in some circumstances  while the cosine similarity captures the directional characteristics which is intrinsic of the document tf  idf vectors  in addition  the cosine similarity is exactly the
pearson correlation  which gives it a sound statistical stance 
there are two typical categories of clustering algorithms  the partitioning and the agglomerative  kmeans and the hierarchical clustering are the representatives of these two categories  respectively  there
are many comparisons between k means and hierarchical clustering  but our consideration is speed  since
we are going to apply clustering algorithms on big social network data  which is always of gb or tb size 
 the ultimate goal is to build a map reduce procedure to parallelize these clustering algorithms  but this
goal may not be reached in this project due to time limit   and the hierarchical clustering is extremely
computational expansive as the size of data increases  since it needs to compute the d  d similarity matrix 
and merges small clusters each time using certain link functions  in contrast  k means is much faster  it is an
iterative algorithm  which updates the cluster centroids  with normalization  each iteration and re allocates
each document to its nearest centroid  a comparison of k means and hierarchical clustering algorithms can
be found in      

 

fi   

the baseline and its problems

document clustering by using the k means algorithm with cosine similarity  spkmeans  to the full space
vsm has been considered as a baseline when doing performance comparison  this algorithm is straightforward and easy to implement  but it also suffers from high computational cost  which makes it less appealing
when dealing with a large collection of documents  the problem is the curse of dimensionality  v is large 
generally  there are more than thousands of words in a vocabulary  which makes the term space high dimensional  hence  various dimensionality reduction techniques have been developed to make improvements
above the baseline 

 

dimensionality reduction techniques

as to our knowledge  there are two main categories of dimensionality reduction techniques  one is to first
write down the full vsm matrix  and then try to reduce the dimension of the term space by numerical linear
algebra methods  another is to try using a different representation of document  not as a vector in a space 
from the very beginning 

   

feature selection method   lsa

the latent semantic analysis method is based on the singular value decomposition  svd  technique in
numerical linear algebra  it can capture the most variance by combinations of original rows columns  whose
number is much less than the original matrix  in addition  the combinations of rows  term  always show
certain semantic relations among these terms  and combinations of columns  document  indicates certain
clusters  after svd  the k means algorithm is run on this reduced matrix  which is much faster than on
the original full matrix  but the problem with this approach is that the complexity of svd is o d     so as
the number of documents increases  the computation of svd will be very expansive  and therefore the lsa
approach is not suitable for large datasets 

   

alternative document representations

there are other document representations and similarity measures besides vsm and cosine similarity  such as
tensor space model  tsm  and a similarity based on shared nearest neighbors  snn   etc  these alternatives
may be effective in some special cases  but not in general 
one significant step in this area is the introduction of the concept of latent topics  it is similar to the
latent class label in the mixture of gaussian models  this concept has led to a series of generative models 
which specify the joint distribution of the data and the hidden parameters as well  among these models
are the probabilistic latent semantic analysis  plsa   the mixture of dirichlet compound multinomial
 dcm   the latent dirichlet allocation  lda   the plsa has a severe overfitting problem  since the number
of parameters estimated in the model is linear in d  the number of documents  on the other hand  lda
specifies three levels of parameters  corpus level  document level  and word level  and the total number of
parameters is fixed  k   k  v   where k is number of latent topics regarded as given  this multilayer
model seems more complicated than others  but it turns out to be very powerful when modeling multiple topic
documents 
another model that attracts my attention is the mixture of von mises fisher  movmf  model proposed
by banerjee  et al      the vmf distribution is one of the simplest parametric distributions for directional
data  and has properties analogous to those of the multi variate gaussian distribution for data in rd   it
has a close relationship with k means   cosine similarity  spkmeans   while its computation via an em
procedure can be severely reduced when adopting a hard decision  hard movmf   when adopting a soft
decision  soft movmf   it shows an annealing characteristic  and performs much better than hard movmf 

 

fi 

comparison of lda and movmf

   

related experiments

due to the appealing properties of lda and movmf  comparisons have been done by many researchers 
related work can be found in            the conclusions from these studies are listed below 
 while lda is good at finding word level topics  vmf is more effective and efficient at finding documentlevel clusters 
 generative models based on vmf distributions are a better match for text than multinomial models 

   

further analysis

it seems that movmf performs better than lda according to several indicators  but here we propose a
question  what if each document consists of multiple topics  just as website mentioned at the very beginning 
rather than of only one topic  in this circumstance  will lda perform better than movmf  actually  one
can use the soft movmf to address the multiple topic issue  given an instance  the soft movmf assigns
posterior probabilities to each topic  and therefore performs a fuzzy clustering  each document can be
viewed as a weighted combination of topics  but with these probabilities  how can one further group them
into clusters  another question is that although people can represent a document in this way  but it is not
intuitive  since the model assumes each document has only one topic as the initial setup  furthermore  it is
computational expansive to use the soft movmf  and thus making it less attractive 
by contrast  lda is naturally created to model multiple topic documents  but not restricted to this  
the multiple topic feature is rooted in the model itself through the word level topic structure  as a result 
lda can find much more topics than movmf  for example  with d documents  movmf can not identify
more than d topics  while such limitation does not apply to lda  in todays internet world  this is a huge
advantage  since the online information is topic intensive  it should not be surprising that one single website
could contain hundreds of topics  the power of lda is that it can not only cluster these objects  such as
documents   but also extract features  topics  from these objects  and these extracted features  topics  can
be further pushed into other applications  for example  they can be used to match a query entered into
a search engine  a search query is very short so it is not appropriate to view it as a document and then
assign it to some predefined clusters  or view it as a cluster centroid and then find its nearest neighbors  by
contrast  it is very easy to match it with a topic  since a topic is generally described by a few keywords  by
using this method  one can easily match multiple topic sources with a query by first identifying the most
related topics  and then ranking these sources according to the matching score 
one thing we noticed during this project is that most of the well know document data  such as classic  
reuters        and    newsgroups  etc  are all single topic documents  many comparisons of lda and
movmf have been done on these data  as in            this is unfair for lda due to the reason mentioned
above  so in order to make a more fair comparison  we need to use some multiple topic document data  one
possible way of doing this is by mixing up these existent single topic documents 

   

recent work

topic models remain hot during these years  and lots of improvement has been done on lda since the initial
lda model was proposed  such as the hierarchical topic models      the correlated topic models      a
recent paper introduced a kind of hybrid model  the spherical admixture models       it combines the
directional feature of the von mises fisher distribution which has been found to often model sparse data
such as text more accurately than multinomial distribution          and the multiple topic feature of lda 
in addition  it relaxes the the data distribution of von mises fisher from the positive quadrant to the whole
unit hypersphere  modeling both word frequency and word presence absence  as a result  this model is more
complicated in sense of its nested structure and multiple layer of parameters  hence its estimation methods
need some attention 

 

fireferences
    arindam banerjee and sugato basu  topic models over text streams  a study of batch and online
unsupervised learning  in sdm  siam       
    arindam banerjee  inderjit s  dhillon  joydeep ghosh  and suvrit sra  clustering on the unit hypersphere using von mises fisher distributions  journal of machine learning research             
     
    d  blei and j  lafferty  correlated topic models  advances in neural information processing systems               
    david m  blei  thomas l  griffiths  michael i  jordan  and joshua b  tenenbaum  hierarchical topic
models and the nested chinese restaurant process  in advances in neural information processing
systems  nips        
    david m  blei  andrew y  ng  and michael i  jordan  latent dirichlet allocation  j  mach  learn  res  
                
    inderjit dhillon  jacob kogan  and charles nicholas  feature selection and document clustering  in
michael w  berry  editor  survey of text mining  pages        springer       
    imola fodor  a survey of dimension reduction techniques       
    thomas hofmann  learning the similarity of documents  an information geometric approach to document retrieval and categorization  in sara a  solla  todd k  leen  and klaus robert mller  editors 
nips  pages         the mit press       
    thomas hofmann  probabilistic latent semantic analysis  in proc  of uncertainty in artificial intelligence  uai    stockholm       
     thomas hofmann  probabilistic latent semantic indexing  volume      of sigir forumspecial issue 
pages       new york  ny        acm 
     guy lebanon  information geometry  the embedding principle  and document classification  in in
proceedings of the  nd international symposium on information geometry and its applications       
     kristina lerman  document clustering in reduced dimension vector space  http   www isi edu  lerman papers lerman   pdf       
     t p  minka and john lafferty  expectation propagation for the generative aspect model  in proceedings
of the   th conference on uncertainty in artificial intelligence  pages              
     joseph reisinger  austin waters  bryan silverthorn  and raymond j  mooney  spherical topic models 
in johannes frnkranz and thorsten joachims  editors  icml  pages         omnipress       
     steffen staab and andreas hotho  ontology based text document clustering  in intelligent information processing and web mining  proceedings of the international iis  iipwm   conference held in
zakopane  pages              
     m  steinbach  g  karypis  and v  kumar  a comparison of document clustering techniques  technical
report         university of minnesota       
     eric p  xing  andrew y  ng  michael i  jordan  and stuart russell  distance metric learning  with
application to clustering with side information  mit press  pages              

 

fi