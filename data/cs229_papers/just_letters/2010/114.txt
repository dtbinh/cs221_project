reservoir uncertainty assessment using machine learning techniques
jincong he

abstract petroleum exploration and production are associated with great risk because of the uncertainty on subsurface
conditions  understanding the impact of those uncertainties
on the production performance is a crucial part in the decision making process traditionally  uncertainty assessment is
performed using experimental design and response surface
method  in which a number of training points are selected
to run reservoir simulations on and a proxy model is built
to give prediction for the whole design space  the quality of
the response surface strongly depends on the method used
to construct them  in this paper we evaluate the use of thin
plate spline  tps   artificial neural network  ann  and support
vector regression  svr  for this application  results show
that when properly tuned ann and svr provide superior
performance to tps 

i  introduction
reservoir uncertainty assessment is one of the most important aspects in production decision making  because of
the heterogeneity underground and also the scale of the oil
fields  even with todays advanced seismic and well logging
techniques  many of the reservoir parameters can not be
measured accurately  some of these parameters  such as the
permeability  porosity and the residual phase saturation  may
have a large impact on the oil production forecast  therefore 
quantifying the production forecast uncertainty arising from
the parameter uncertainty would play an important role in
monitoring the risk of investment 
mathematically  the problem can be formulated as
r   h x    x         xn  

   

xi are uncertain reservoir parameters such as permeability multipliers  porosity and residual saturations  r is the
response function of the reservoir under a specific set of
parameters  it is usually taken as some economical metric of
the field  such as net present value  npv  of the development
project  given the probability distribution of xi to be pi  x  
our goal is to find the probability distribution of p  r  
the ideal method for uncertainty assessment would be
monte carlo simulation  by sampling a large number of
points according to the parameter distribution and running
simulations on each of them  we can calculate the statistics
of results and estimate their uncertainty  however  by the
law of large
 numbers  monte carlo simulation will only
display    n convergence i e  quadrupling the number of
sampled points will only halve the error  on the other
hand  production simulation  the process to evaluate r  for
practical sized reservoir is very time consuming  one single
sample may require hours to obtain  for typical problem
only    to     samples would be available  therefore  monte
carlo method is seldom used in practice 

fig    

illustration of experimental design procedure

experimental design method provides an alternative to
monte carlo simulation  as shown in fig     the basic idea
behind this methodology is to sample a few training points in
the parameter space in a way that maximum inference can be
attained with minimum cost  after that  a proxy model will
be built based on the training points and be used to estimate
the response for the whole parameter space  finally  monte
carlo simulation will be carried out using the proxy model to
calculate the probability distribution of the response  two key
steps that affect the performance of the algorithm the most
are the design step  how the training points should be chosen 
and the proxy building step  how the proxy should be built 
    reviewed and compared many of the current experimental
design techniques  for the proxy building  thin plate spline
and artificial neural network  ann  are commonly used 
as far as we know there has not been any investigation on
the applicability of support vector regression  svr  for this
problem  this paper focuses on evaluating and comparing
the three approaches  we will proceed as follows  first a
brief overview of thin plate spline  ann and svr will
be given   then all three methods will be tested on an
uncertainty assessment case for a synthetic reservoir  finally
comparisons of the results from different methods will be
made in terms of reliability of their estimation on the output
statistics  accuracy of their point wise estimations and their
capability in terms of estimating the influence of uncertain
parameters on the economical or recovery responses 
ii  methods
a  thin plate spline  tps 
thin plate spline  tps  is a popular technique for multivariate interpolation  the name thin plate spline  first introduced by duchon    to geometric design  refers to a physical
analogy involving the bending of a thin sheet of metal  given
a set of training points   x i    y  i     i                 m   thin
plate spline tries to bend the metal sheet so that it passes
through the training points exactly with the least amount of
energy  mathematically  we try to minimize

fiet p s  

m
x

kh x i     y  i  k    p

qj    p j and qjn   pnj   for each of the hidden layers  we
have

z
 x h f d

   



i  

where y   h x  is our hypotheses  which will be discussed
later  p is the smoothing coefficient   is the n dimensional
space where x is defined  x h is the hessian matrix and   f
is the frobenius norm  the first term of eq    characterizes
the mismatch error while the second terms  which is also
called the bending energy  characterizes the smoothness of
the function 
in thin plate spline  the function is parameterized as
a linear combination of the thin plate spline radial basis
function as follows 
m
x
h x   
ci  kx  xi k 
   
i  
 

where  r    r log r and c    c    c            cm   are the
weights 
with this parameterization  the problem in eq    is transformed to
c   arg min et p s
c

   

details for solving the problem in eq    can be found in
     in this work  we used the spline toolbox in matlab 
b  artificial neural networks  ann 
ann is a nonlinear regression model inspired by the
structure and the functional aspects of biological neural
networks  a neural network consists of input nodes  output
nodes and interconnected groups of artificial neurons  a
feedforward network consists two or more layers of these
nodes  the first layer is the input layer  consisting of all input
nodes  the last layer is the output layer  consisting of all
output nodes  all the other layers are termed hidden layers 
consisting of neurons  fig    shows an example network with
one of each kind of layers 

qji    

ni
x

i i
wjk
pk   bij

   

k  

where w i is the weight matrix and bi is the bias vector
between the ith and the i    th layers  inside the neurons 
the input is processed by a transfer function  which can be
any differentiable function but is usually selected to be the
log sigmoid function as follows
g x   

 
    ex

   

to specify neural network  it is sufficient to find a set of
weights and bias vectors that minimized some kind of error 
in this work  this process is done by levenberg marquart
backpropagation algorithm  starting with a set of randomly
generated initial guess of weights and bias vectors  the
algorithm alters the parameters to minimize the error between
the desired and the actual output 
in this study we employ the neural network toolbox in
matlab and use a feedforward artificial neural network with
one hidden layer of log sigmoid neurons  the number of
nodes on the input layer  nin equals the number of uncertain
parameters and there is only one node on the output layer 
i e  nout     
c  support vector regression  svr 
among the many proxy building approaches  support
vector regression is the least investigated for reservoir uncertainty  in  sv regression  our goal is to find a function
f  x  that has at most  deviation at the training points  and
at the same time is as flat as possible  the detail derivation
of various kinds of support vector regression was presented
in     and      for completeness an abbreviated description
is also included here 
for a linear case where y   f  x     w  x    b  this
translates into the following constraint optimization problem 
m

minimize

x
 
kwk    c
 i   i  
 
i  


 yi    w  x i    b     i
subject to
yi     w  x i     b     i

i   i   

fig     an example neural network consisting of one input layer  one
output layer and one hidden layer  from wikipedia 

denote pij and qji to be the input and output of the jth
node on the ith layer and ni to be the number of nodes on
the ith layer  then for the input and output layers we have

   

   

here         is the inner product of two vector  kwk    
w  w   measures the flatness of the proxy and the constraints requires the proxy approximates all training points
with  precision  i   i are slacks variables introduced to
allow trade offs between the flatness of the function and the
compliance with the  deviation constraints  c characterizes
the amount of penalty for violating the constraint 
by formulating the lagrangian  the dual optimization
problem of eq    can be written as

fipm
    i j    i  i   j  j     x i    x j   
pm
pm
 i    i   i     i   yi  i  i  
   
 pm

 


 
 
 
i
i
i  
    
subject to
   i   i  c


minimize

eq    is a convex optimization problem which can be
solved efficiently by many optimization package  e g  osl 
cplex  or minos  with i   i solved from eq     our
proxy function can be written as follows
h x   

m
x

 i  i     x i    x    b

    

i  

see     for more detail on the computation of the constant
bias term b 
its straightforward to kernelize the complete algorithm as
the training problem in eq    and the prediction problem
in eq     are all in terms of the inner product of two
features  therefore by replacing   x i    x j    with any
kernel function k x i    x j     we can introduce nonlinear
features without the need to calculate x or w explicitly  the
gaussian kernel in eq    


kx  zk 
 i 
 j 
k x   x     exp 
    
   
in this work we used libsvm     for our svr implementation  the effect of various kinds of kernels and different
choices of parameters are also investigated for our problem 
iii  test result
the three methods discussed in section ii are tested on a
reservoir production uncertainty assessment problem  figure
  presents the top view of the reservoir model used in
this problem  the model is generated using the sequential
gaussian simulation method  there are totally   producers
 marked by dots  and   injectors  marked by triangles   flow
simulations by reservoir simulator gprs     are run from
day   to day     and the total oil production qo   water
production qw and water injection qinj are recorded  we
study the effect of the seven parameters presented in table iii
on the uncertainty on net present value  npv   as defined
below 
npv   qo po   qw pw   qinj pinj

    

here  po   pw and pinj are the prices of the oil and cost of
produced water treatment and cost of injected water  they
are set to be     bbl      bbl and     bbl respectively 
six training data sets with the sizes of                     
and     are generated using latin hypercube sampling    
on the parameter space  each training result is generated by
running the gprs simulator and therefore is considered to be
true without error  a test set of       samples is generated
in the same way  the three methods presented in section ii
are trained on each of the training set and are evaluated on
the test set  the evaluation metric is the mean prediction
error of the test data set 

fig     top view of the reservoir model used  log transmissibility in xdirection is shown

table i
u ncertain parameters
param 
h
poro
xpermmul
ypermmul
swr
sor
cr

param  description
net thickness  ft
porosity
x permeability multiplier
y permeability multiplier
residual water saturation
residual oil saturation
rock compressibility

min
  
   
   
   
    
    
 e  

max
  
   
 
 
    
    
 e  

a  base case
there are two parameters  c and   in the svr method
and one  number of hidden neurons  in the ann method 
their effect on the proxy will be investigate in section iiib  for the base case  we set c               and we
use   hidden neurons for the ann method  figure   shows
the learning curves for all three methods  it is clear that
the error of all three method decreases as the number of
training data increase  however  using the same amount of
training data  the error from tps is always at least one
order of magnitude higher than the error of svr and ann 
comparing svr and ann  when the number of training
data are small  svr provides superior result to ann  as
the amount of the available training data increases  the error
of the neural network decrease faster than that of the svr 
and ann outperform svr when the number of training
data is    or higher  for practical applications  the maximum
number of simulation can be run normally ranges from   
to      therefore  both svr and ann are good candidates
for building a good proxy on training data of that size  it
is also noticeable that the error of ann does not decrease
monotonically  this may in part due to the fact that the initial
guess of the weights in ann are randomly generated and the
optimization problem of the weights may have multiple local
minima 

fifig    

learning curve  average prediction error vs training set size

fig    

effect of  on the svr model accuracy

fig    

effect of c on the svr model accuracy

b  parameter investigation
as mentioned above  there are two parameters  c and  
in the svr method and one  number of hidden neurons  in
the ann method  it would be of interest to investigate the
effect of those parameters on the model accuracy and provide
guidelines for practical uses 
figure   is a log log plot of the average prediction error
of the svr model for different value of  for three training
sets of different size         and       it can be seen
that the average error decreases with smaller   and when
 is small enough  the error stays constant  as  is the
tolerance by which the proxy can deviate from the training
points  the result of figure   indicates that the proxy should
approximate the training point perfectly  this is reasonable
for our application because the training data are generated
by simulation and therefore have no error  if the training data
were collected from experiments with errors  the optimal
choice of  would be larger than    figure   shows the
average prediction error of the svr model for different value
of  for three training sets of different size         and      
it is clear that the average error decrease as c increase  and
stays at constant value when c is large enough  note that c
is the coefficient of the slack variables and it characterizes
the penalty for the proxy to violate the c deviation at training
points  figure   indicates that the penalty for large deviation
at training points should be very high  or in other word 
the slack variables should not be introduced  this is  again 
because we have perfect training data from a simulator 
the above observations suggest that for our application the
introduction of slack variable may not be necessary and the
inequality constraints in eq   can be simplified to be equality
constraints  this would be an interesting topic for future
work 
figure   shows the effect of using different number of hidden neurons on the average prediction error  the fluctuation
of the result dues in part to the random components in the
ann method  and in part to the sensitivity of the method to
the number of neural used  despite the fluctuation  it can still
be observed that there is an optimal choice of the number

of hidden neurons for each cases  the result deteriorates
if too many or too few neurons are used  there is still
not consensus in the literature on how the optimal number
of neurons should be chosen       proposed the following
formula  and rounding it to the next integer  
nneurons  

p

ninput parameters  noutput parameters

    

in our case  the above formula would suggest using only  
hidden neurons  which is clearly not optimal  more sophisticated method should optimize the parameters by some crossvalidation techniques such as leave one out cross validation 
figure   shows the cumulative distribution of the      
results from the true simulations  and the three proxy models 
it can be seen that results from ann and svr match with the
true solution reasonably well  they almost overlap   while the
result of tps deviate slightly  table iii b shows the relative
error of the estimation of               percentiles  p   
p    p    for the three methods  these three quantities are of
significance because they represent the pessimistic  average
and optimistic estimations of the production response  it
can be seen in the table that ann and svr provides

fiparameters for monte carlo simulation  results shows that
using the same amount of training data  svr and ann
always outperform tps when the parameters of the methods
are properly selected  when less training data is available 
svr seems to be more accurate than ann 
the parameter sensitivity of svr and ann is also investigated  for svr  it is suggested that c should be taken to be
very large and  should be close to   for the application we
are interested in  for ann  the performance of the method
depends heavily on the the number of hidden neurons  and
a good choice of this parameters may require optimization
using cross validation techniques 
r eferences
fig     effect of the number of hidden neurons on the ann model accuracy

very accurate estimation of these quantities  in terms of
computational efficiency  the generation of       samples
using reservoir simulator takes more than   days while the
three proxy models only take several seconds to generate
      predictions  therefore  uncertainty analysis based on
these proxies is very efficient 

fig    

the cumulative distribution of the results from different methods

table ii
r elative error for estimations of p    p   and p  
p  
p  
p  

spline
      
     
     

ann
     
     
     

 svr
     
     
     

iv  conclusion and discussion
in this study  we investigated the use of thin plate spline
 tps   artificial neural networks  ann  and support vector
regression  svr  for uncertainty assessment of the reservoir
production activities  using a small amount of training data
from a simulator  these three methods were used to build
a proxy of the reservoir response with respect to uncertain

    b  yeten  a  castellini  b  guyaguler  and w  h  chen  a comparison
study on experimental design and response surface methodologies  in
     spe reservoir simulation symposium  the woodlands  texas 
usa  january      
    jean duchon  splines minimizing rotation invariant semi norms in
sobolev spaces  in walter schempp and karl zeller  editors  constructive theory of functions of several variables  volume     of lecture
notes in mathematics  pages        springer berlin   heidelberg 
              bfb        
    b  li and f  friedmann  novel multiple resolutions design of
experimental response surface methodology for uncertainty analysis
of reservoir simulation forecasts  in      spe reservoir simulation
symposium  the woodlands  texas  usa  january      
    a  j  smola and b  scholkopf  a tutorial on support vector regression 
neurocolt technical report   tr               
    b  scholkopf  a  j  smola  r  c  williamson  and p  l  bartlett  new
support vector algorithms  neural computation                    
    s  s  scholkopf  s  k  shevade  c  bhattacharyya  and k  r  k 
murty  improvements to platts smo algorithm for svm classifier
design  neural computation                  
    chih chung chang and chih jen lin 
libsvm  a library
for support vector machines       
software available at
http   www csie ntu edu tw  cjlin libsvm 
    h  cao  development of techniques for general purpose simulators 
phd thesis  stanford university       
    k  fang  uniform design  application of number theory in test design 
acta mathematicae applicatae sinica       
     t  master  practical neural network recipes in c    academic press 
     

fi