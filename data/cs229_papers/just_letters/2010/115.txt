cs    final project   fall     

 

predicting wireless channel utilization at the phy
jeffrey mehlman  stanford networked systems group  aaron adcock  stanford e e  department

abstractthe ism band is an extremely over utilized communications medium  wireless lan technology  bluetooth  and
landline wireless devices all make use of the ism band  the
support vector machine is proposed as a tool for predicting
channel availability and thus using the band more efficiently 
data processing  feature extraction  and results are presented 
suggestions for future work in this area are also presented 
index termswireless communications  machine learning 
support vector machine

i  i ntroduction

t

oday  wireless communications are an integral part of
our daily lives  depending upon the form and type of
communication  e g  cellular  wireless internet  etc    wireless
protocols use different bands of the radio spectrum  unlike
cellular applications  wireless internet protocols  wifi        
use the unlicensed industrial  scientific  and medical  ism 
bands to communicate 
because it is unlicensed and thus free for use  subject to
some basic rules   the ism wireless spectrum in the       ghz band is an extremely over utilized communications
resource  wireless lan technology  landline wireless telephones  microwave ovens  bluetooth peripherals  and other
devices generate both communications traffic and interference
in this band  we contend that there are repetitive patterns over
short timescales in these bands which can be used to predict
the channel state in the near term future 
as examples of patternistic channel users  we highlight
the function of microwave ovens  as dumb interferers  and
the        media access control  mac  protocol  as an
intelligent user   microwave ovens generate rf energy in
the        ghz band during portions of the   hz ac power
waveform  aside from the initial event of the microwave
turning on and the final event of the microwave turning off  the
oven rf energy generation follows a periodic   hz activity 
on the other hand  the        mac protocol provides a
simplistic probabilility mechanism for user channel access
where a user tries to access the channel in a randomly chosen
     s time slot      if a user attempts to access the channel
and it is busy  it backs off and waits another random period
of time before trying again  in a busy channel  if the user fails
in consecutive attempts  it will wait longer and longer periods
of time to access the channel  the number of users and their
traffic demands have serious impacts on the likelihood of the
channel being utilized at any given time  unfortunately  this
probability model is complex due to channel variation  random
traffic patterns and the number of users  and the specific
backoff algorithms used  an analytic approach to finding this
time varying probability distribution is difficult at best 
machine learning algorithms which provide reliable prediction of future channel availability would have implications

fig    

ism band time domain sample   ms 

for a number of ubiquitous technologies as well as nextgeneration products like cognitive radio  for current protocols
like        wifi  a channel prediction mechanism could allow
users to better select time slots for transmission at lower risk
of interference  bringing more energy efficient communication
and better overall use of the channel  in the future  new technologies will make use of already licensed bands by only using
them when it does not interfere with the primary  licensed
user  this is generally referred to as cognitive radio      for
future cognitive radio protocols  predicting channel freedom
to ensure that they do not interfere with the primary user is of
the utmost importance      these secondary users of licensed
channels  as well as users of the ism bands  would benefit
greatly from a robust channel utilization prediction model  it
would allow them to make better use of the spectrum  picking
the right time in which to attempt transmission  unfortunately 
since these protocols can not communicate directly with the
other users in the band  we must find alternative  passive
methods with which to predict channel usage  in this project 
we attempt to create a prediction model using only observed
signal measurements at the physical layer  phy  
using machine learning techniques  this project aims to
utilize signal measurements at the physical layer of wireless
ism band to predict the state of the channel in the near
future  we have found no prior research using this approach at
the physical layer  in section ii  we review our methodology
for this experiment  including data collection  preprocessing 
and feature selection  additionally  we review our supervised
machine learning algorithm  making use of the support vector
machine  svm   in section iii  we train and test our algorithm

fics    final project   fall     

 

trying a variety of different parameters on portions of a
   ms data sample and provide results  in section iv we
review our results from this experiment  as well as some
interesting findings  throughout the project a number of issues
with using a svm for this application and the size of the
collected data were encountered  some of these issues are
highlighted in the paper  though the methods of this paper
require significant computational performance and thus are
not feasible for implementation in a wireless communication
device  they do yield promising directions for future work 
ii  m ethodology
in this section  we review the methods used in this project 
beginning with live channel measurement  we highlight methods and considerations for our data needs  next  we review
data preprocessing and feature selection for our learning
model  lastly  we highlight our chosen machine learning
algorithm  the support vector machine  svm   as well as some
parametric decisions for this algorithm 
a  data acquisition
in order to test our hypothesis  we require a large set
of live channel measurements over the entire    mhz ism
band  instead of attempting to build large sets of convincing
simulation data of wireless traffic  we decided it would be
better to obtain live channel measurements  the stanford
networked systems group  snsg  has access to a rusk
channel sounder  the appropriate piece of equipment for
collecting this type of data  we collected a    mhz wide set
of baseband data  centered at     ghz  we took two data
samples from bytes cafe  the first contained approximately
 ms of raw signals and the second an additional    ms sample 
the  ms sample is shown in figure   above and was used for
initial algorithm testing  the larger data sample was    mb in
size  while the smaller sample was only  mb  the number of
samples is deceptive as the number of time samples is not as
important as the number of packets observed  because of this 
and computational considerations  the data was decimated to
obtain a more reasonable data set  because each wifi packet
can be hundreds of microseconds long  the  ms sample does
not include enough packets for anything other than cursory
testing  using scripts provided by rusk  the channel sounder
data files can then be converted into matlab  mat files
containing the raw data and measurement parameters  with the
raw data from the sample sets in hand  we can begin extracting
features for our prediction algorithm 
b  data preprocessing
the chosen feature set often determines whether or not
a learning algorithm is effective  in this case  the raw data
contains a huge amount of uninteresting information and
noise from the channel  to counteract this  features need to
be extracted that are useful for predicting future availability
of channels  to select the features  we again note that the
most ubiqituous user of the ism band is        wifi  which
subdivides the ism band into smaller frequency channels  in

fig    

ism channels  http   www moonblinkwifi com  point freq cfm 

fact  every ism band protocol known to the authors uses only
some smaller subchannel of the ism band  thus  we make the
obvious choice to move our data into the frequency domain
using a fast fourier transform  fft   and then subdivide the
fft into smaller frequency subchannels  at this point  we also
remove the    mhz of additional spectrum collected with the
rusk channel sounder  and keep only the    mhz band        ghz  of interest  channel access  in the frequency or time
domain  is determined by a power threshold for that band 
as shown in figure    the        family of protocols
splits the ism band into    overlapping   mhz channels 
with center frequencies spaced  mhz apart  each wifi access
point  ap  operates on one of these bands  in a large campus 
each ap is instructed to use orthogonal bands  thus channels
      and    are the bands most often utilized  since channel
availability is essentially a function of the average power in a
channel  features representing the power in the each channel
of interest are the most obvious to extract  to calculate the
fft  more than one time domain data point is required  we
slide a time window of length t    n over our set of raw
time measurements  and calculate an fft for each window 
the sliding window is stepped by some fraction of t leaving
us with a spectrogram  a set of frequency measurements over
time  of the original channel measurements 
since the length of an fft is the same as the length of
the time window used  the number of fft samples is much
larger than the number of wifi ism channels  to further
reduce our feature space  we average the fft samples into   
bins of  mhz over the ism band  next  we found that these
channel power measurements contained a significant amount
of noise from random wireless channel fading and background
interference  this made it difficult to set any clear decision
boundary for whether or not a packet was in progress  as
such  we needed a filtering algorithm which would preserve
sharp packet transitions while reducing signal variation due to
high frequency noise 
for each frequency channel time series  we utilized total
variation regularization  tvr  de noising   which is specifically designed to remove noise while preserving large sharp
transitions  to denoise a time series x  rn with tvr  we
solve a modified tvr optimization problem
min    log x   x      
x

n 
x
i  

 xi    xi  

   

fics    final project   fall     

fig    

noisy and de noised power time series  ism  mhz channel     

where x is the de noised signal estimate and the parameter
 controls the relative penalty for signal variation  since
interference and changes in signal to noise ratio  snr  are
typically only interesting over orders of magnitude  using the
log power on each channel seems a natural choice 
an example of the resulting denoised power time series for
an ism  mhz  frequency bin number    is shown in figure
   in the upper plot  the average power over time for a  mhz
channel is shown  in the lower plot  tvr is applied to this
same time series to remove noise while preserving packet
transitions  the dotted red line shows our cutoff for deciding
whether or not a packet transmission is in progress  which
gives the label for our supervised learning algorithm  this is
a much cleaner decision boundary in the lower plot  and gives
more consistent positive negative labels for the data set 
c  feature extraction
within each channel  we contend that channel usage will
exhibit patterns over time  as a function of the number of users 
their traffic demands  the state of the channel interference 
etc  this hypothesis regarding patternistic use is at the core
of this research effort  we must extract features which clearly
represent these patterns  and allow us to accurately predict
future patterns 
   raw feature vectors  our first attempt at building a
feature vector for a single channel involved a simple approach 
looking at a set of previous power levels for that channel  thus 
each  label  feature vector  pair for a data point at time t would
consist of
 a label indicating whether or not a packet transmission
occurred at time step t            in the future  note 
a time step is approximately   s 
 a feature vector consisting of n   previous power level
samples  from t n to t  
unfortunately  the packet patterns we are interested in occur
over relatively long time scales  tens of milliseconds   while
individual packets occur over much shorter ones  hundreds of
microseconds   in order to capture individual packets as well
as the large scale patterns  the feature vectors would have to
be very long  additionally  these feature vectors would include
significant amounts of useless information  we refer to this
approach as the raw feature vector 

 

fig    

transition feature vector extraction

   transition feature vectors  to reduce the dimensionality of the feature vectors and to focus on packet patterns 
we developed a new approach  in this case  the  label feature
vector  pair for a time step t consisted of
 a label indicating whether or not a packet transmission
occurs at some time step t            in the future
 one feature with the current power level at time t
 additional features containing time elapsed since the last
np t packet transitions
packet transitions are defined as the times where a packet
starts or ends  the number of previous transitions we consider 
np t   effectively determines how far back we look in the
packet history for the current sample  while the current power
level determines whether or not a given feature vector fi
occurs during a packet or not  extraction of a packet with
  transitions feature vector is illustrated in figure   
   prediction distance    the parameter  is an important
design consideration  ideally  we would like to be able to
predict channel utilization at relatively distant times in the
future  with this capability  a user could make improved
decisions about the likelihood of succesful channel access 
it stands to reason that the performance of our prediction
algorithm should also become worse as we attempt to predict
further into the future 
   miscellaneous  during the course of the project  several
other methods for feature extraction were tried  for many
applications  if the channel is open  it can be used immediately
 this would not be true in the case of cognitive radio   as
such  one of the variants on the methods proposed above
involved restricting the data to just the times when the channel
was being used  in this case  the attempt was to predict when
a transition would occur  initial attempts at implementing this
were slightly more complicated and did not give improved
performance over the method outlined above  this case is
mentioned as it emphasizes that the transitions are more
interesting than the time spent within a packet or in channel
dead time 
d  application of machine learning
the problem that needs to be solved on each channel is a
binary classification problem with input data in  n   for the
raw feature vectors  and data in  np t      where np t     is

fics    final project   fall     

fv length
   of transitions 
  
  
  
  
 
  
  
  
  
  
   
    

 

fv selection

predict steps ahead

svm c value

contiguous  first    
contiguous  first    
contiguous  first    
random     
random     
random     
random     
random     
random     
random     
random     
random     

 
  
  
 
  
  
  
   
  
  
  
  

     
     
     
     
     
    
     
     
     
     
     
     

channel
 train test 
     
     
     
     
     
     
     
     
     
     
     
     

training error

test error

 
      
      
      
      
      
      
      
      
      
      
      

      
      
    
      
      
      
      
      
      
      
      
      

table i
svm c lassification r esults

the number of time since transition features we look at  plus
the current power level  since the patterns we are attempting
to classify are quite complicated  and there are no prior results
directly studying these patterns  we attempted to classify the
data using a common and flexible approach  the svm  we
began with a linear kernel  but expected that the linear kernel
would not be optimal  as such  we also tested the gaussian
kernel and the polynomial kernel  since they were easy to
implement and are widely used  the gaussian kernel generally
had the best performance  and was used for the tests discussed
in the results section 
we form the svm dual optimization problem with considerations for using different kernels  where
 
max  t   t diag y kdiag y 

 
s t i  c  i   
t

 y  
generates the optimal decision boundary 
we note at this point that the c parameter  which effectively
determines how much we penalize incorrect labelings  is
chosen to be very large for this application  c         this
is due to the fact that there tend to be large dead times in
some of the channel measurements  where no packets are
transmitted  and small c values led to a skew towards no
packet labelings  see figure    top plot   if the number of  label samples outweighs the number of   label samples by a
large factor  small c values leads to the algorithm treating a
significant portion of   labels as outliers and misclassifying
them 
after preparing transition feature vectors  we consider two
train test classification cases 
   select a contiguous subset of feature vectors from the
beginning of a time series  and then test on the latter half
of that series  we refer to this as contiguous training
selection 
   select a random subset of feature vectors from the entire
series  and then test on the remaining portion of the
series  we refer to this as random training selection
in the first case  the goal was to show a causal classification
ability  i e  the svm could be trained earlier in time  and then

the results used later in time with good results  we expected
the traffic patterns to remain the same over two time periods in
close proximity  due to findings discussed in section iii  the
second case with random subset of samples was introduced 
this random subset better represents the patterns found over
the entire time series because it comes from the entire time
series  if the second type of train test subset selection works
better  it would indicate that  given a set of training samples
which represent all patterns found over the entire time series 
the svm is capable of predicting channel utilization in the
future  a set such as this would need to be constructed from
samples coming from a large variety of wireless traffic patterns
and situations 
iii  r esults
a  raw feature vectors
we first consider classification of the raw feature vectors
using an svm  this optimization yields a trivial result  where
if    the future prediction distance  is relatively small many
of the feature vectors contain power measurements that fall
within the same packet as the label  in this case  the svm
essentially guesses that if the input is high  it will continue
to remain high  and vice versa  if an input feature vector
represents a transition from high power to low power  or
vice versa   the svm takes the former half of the transition
  high power   and guesses that the output will continue to
remain high  if  is very large and the algorithm tries to look
more distantly into the future  the svm is less successful with
training and entirely unable to classify a test set  greater than
    error   this is because the raw features do not contain
enough of the packet history  and the information they do
contain is obscured by unimportant features  to make anything
other than very near term predictions 
if the feature vectors were significantly longer  it is possible
that they would catch multiple packet transitions and then
begin to classify based upon long term patterns  but this
would require a very large number of features and thus an
extremely large training set  additionally  a great majority of
these features would not have any direct relationship with
future prediction of packets  and thus would be innefficient
for training the classification algorithm  it is clear that this

fics    final project   fall     

fig    

svm output  tracking but not classifying

approach is not tenable for packet traffic pattern matching 
we do not consider this type of feature vector any further 

b  transition feature vectors
in table    we summarize results for a number of runs
with transition feature fectures  including svm training across
a number of parameters  with both contiguous and random
subset training selection  the transition feature model clearly
performs better for predicting future channel usage than the
raw feature model  this model begins to take into account the
length of time that the channel has been in the current state
and the length of previous packets dead time  this results in
an svm output that could be used as a probability  by fitting a
sigmoid to the output of the training set  of the channel being
available in  time steps 
   contiguous training selection vs random training selection  as shown in the table  the contiguous training set
selection does not perform well for predicting future channels
use beyond the next time sample  even when the test set and
training set come from the same channel data  this means that
simply setting up a program that re optimizes the svm using
recent channel data will not achieve the performance desired 
there is however  some hope here as well  in figure    we see
that the svm does in fact track packet transitions  during a
long dead period  the output of the svm begins to rise towards
the classification boundary but does not reach it  if we make
use of a probabilistic output extension to the svm and then
augment it with markov forward recursions  it is possible that
our results would improve  this is an avenue for future work 
additionally  random training sample selection from the
entire data set gives much better performance  this is because
the training set now contains samples very similar to the
samples in the test set  this method cannot be used in practice
as a wireless device needs to be able to operate causally  this
result does suggest that a training set  with enough samples and
features  would have enough predictive power for to be useful
in many applications  unfortunately  the size of training set
required was computationally beyond our ability to use and 
as can be seen from table    one channel of data is not enough
data to make accurate predictions on a different channel 

 

in more closely examining the svm results with random
training  we find that only     of the training set  which
is in turn only     of our sample set for a given channel 
are support vectors  from the kkt conditions  we know that
alpha values between the bounds    and c  are associated
with support vectors  this means that only    of the total
feature vector set is needed to provide     test accuracy 
in the future  we plan to investigate more deeply if there is
special meaning or importance to that    of vectors  or if
they are just randomly chosen vectors that happen to satisfy
the optimization objective in each run of the svm  if there
are common characteristics of support vectors  we may be able
to train an svm algorithm  or some other channel utilization
prediction algorithm  for that matter  on only these types of
vectors  this also may lead to a more robust solution across
many different channels  and the ability to identify proper
training data subsets for all channel patterns 
iv  c onclusions
the application of a svm to predict future channel use
from physical layer measurements was studied  the time
domain data had to be taken into the frequency domain
using sliding window and a fft  this allowed individual
channels of interest to be used as training and test data 
total variation regulation smoothing was used to remove noise
while preserving the sharp packet transitions present in the
data  even after splitting the data into channels and denoising 
additional feature extraction was needed to obtain manageable
feature vectors  the performance of the algorithm was not as
good as desired  though the algorithm worked for predicting
channel use on the same channel as the training set  it failed
to achieve reasonable performance on dissimilar channels  to
get good results  the training set had to have samples with
very similar features as those in the test set  as seen in the
superior performance of the randomly selected training sets
as compared to the time contiguous training sets  while this
conclusion seems obvious  it does suggest that a large training
set with a variety of traffic pattern sample vectors might
achieve reasonable performance  also  the behavior of the
svm output suggests that a probabilistic model  in conjuction
with such a training set  would be able to give improved
predictions of the length of the current packet or dead time 
future work with an svm approach would involve finding
new features and trying additional kernels  also  building a
training set of appropriate size and content would determine
the effectiveness of the svm for this task  other future work
would include finding appropriate statistical models  beyond
using just the svm output values  to assign probabilities to
different channels or time slots that represent the potential
availability of that channel time for use by the device 
r eferences
    haykin  s  cognitive radio  brain empowered wireless communications 
selected areas in communications  ieee journal on   vol     no    pp 
          feb      
    part     wireless lan medium access control  mac  and physical layer
 phy  specifications  ieee standard       
    kung  ling hung  channel selection for cognitive radio terminals 
cs    project report      

fi