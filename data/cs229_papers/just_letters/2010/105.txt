beating elo
jeffrey s  pennington
december         

 

introduction

the elo system is a rating system of competitive head to head games  it or small modifications of it are
used by the united states chess federation  uscf   the world chess federation  fide   american college
football and basketball and major league baseball 
the elo system assigns a single rating to each player and calculates expected outcomes based on the
rating differences 
ea  

 
  

e ra rb   

 

eb  

 
  

e rb ra   

   

where ea and eb are the expected outcomes for players a and b  and ra and rb are their ratings  here
we designate a win by    a loss by    and a draw by      the spread of the ratings is set by   which for
chess ratings is usually taken to be         log              
the result of each game updates each players rating according to an update rule  the update rule is
usually based on the difference between the actual outcome and the expected outcome 
ra  ra   k ia  ea   

   

where ia is the actual outcome of the game  the factor k is a constant which  given   determines the
volatility of each players rating  the different elo chess rating systems use different values of k  usually
between    and     most systems take k   k ra   to be a decreasing function of the rating  so that higher
rated players ratings are less volatile 

 

elo versus the rest of the world

the website kaggle com hosted a competition to find an approach that predicts the outcomes of chess games
more accurately than the elo rating system  the contest provides a dataset of        games from    
consecutive months between       of the top chess players  each element of the dataset contains four pieces
of information  the month  a number identifying the white player  a number identifying the black player 
and the score  the score is   for a white loss      for a draw  and   for a white win  the test data contains
  additional months of data for the same players  the submissions are evaluated based on the rmse of the
players predicted total scores per month and their actual total scores per month during those   months 

 

feature selection

we illustrate the difficulty of selecting good features with a simple linear regression model  a little thought
suggests that perhaps some good features to include for each player are 

percent of games won lost drawn
f   
total number of games played
 

fithis gives very high training error  and indeed f  does not really capture the information very well  one
might consider 

f  as white
f   
f  as black
or  better  yet 

f 

 

















the
the
the
the
the
the

mean
mean
mean
mean
mean
mean

of
of
of
of
of
of

f 
f 
f 
f 
f 
f 

of
of
of
of
of
of

all
all
all
all
all
all

opponents
opponents
opponents
opponents
opponents
opponents

defeated as white 
drawn as white 
lost to as white 
defeated as black 
drawn as black 
lost to as black

we select the n                   most relevant features from set f  using stepwise regression and plot the
learning curve in figure  
learning curve
training error
test error
best score

rmse  per player per month 

 

   

   

   

   

   
 

 

 

 

 
  
  
number of features

  

  

  

  

figure    learning curve for linear regression  the dotted line shows the competitions best submission  the test error was
calculated after the full data set became public 

the learning curve shows that the test error is high even though the training error is relatively low  an
indication that our feature set is suboptimal  f  has    elements and it has only included the relevant data
of each players opponents  to add more features  we would need to include some information about the
opponents of each players opponents  actually  the number of features is exponential in the recursion depth 
d 
 f         d

   

it is impractical to include features for a recursion depth larger than about    but  in principle  d should be
as large as the total number of games played by the player with the most games  which in this data set is
over      this obstacle is not limited to linear regression and will be present with any model  it is therefore
necessary to consider an entirely different approach 

 

fi   

optimal ratings

one observation about the fitted linear regression model is that the coefficients of the features associated
with the white player are roughly equal and opposite in sign to the coefficients of the features associated
with the black player  this indicates that a good model might be to model the predicted outcomes as 
pi       rwi  rbi  

   

where pi is the predicted outcome of the ith game             is the mean score  wi and bi are the white
and black players in game i  and rwi and rbi are their ratings  which we will attempt to fit 
this model has        features and         training examples  it is only numerically tractable because
the model is linear and the design matrix is quite sparse  which allows for certain iterative least squares
algorithms to succeed  we solve this problem using the lsqr function in matlab  we find 
training error          

test error         

   

which is a substantial improvement from the previous model 

 

time dependence

performance in chess  like all games  varies over time  for this reason  one would expect a players recent
games to be the best indicator of his future performance  the most convenient method to account for this
phenomenon is by weighting each month differently in the least squares calculation 
x
error  
wi  ti   yi  pi     
   
i

where yi is the actual outcome of the ith game  pi is given as in     and wi  t  is a weight function  the
variable ti is discretized by month  as dictated by the data  as a model for wi  t   we choose a monotonically
decreasing function  the simplest such choice is 
w t    t  

   

post analysis shows that the optimal value for  is       which gives some improvement 
training error          

test error         

   

unfortunately  fitting  by cross validation is difficult  this is due to an unfortunate feature of the training
data  namely that it is heavily end weighted  there are many more games in the last    months than in the
remainder of the data set  this can be seen in figure     
to make matters worse  the function w t  turns out to be crucially important to achieving optimal scores 
and the simple power law model is actually quite poor  as a demonstration of this  figure     shows the
optimal weighting function as determined by post analysis  such a model achieves a test error of        well
below the winning score for this competition  we expect that a weight function which is not monotonically
decreasing will not generalize to other contexts  but such a model may be necessary to win the competition 

 
   

optimization
reduce over fitting

there are several further techniques that should be employed to optimize predictions  first  there are many
players who engage in very few games  and as such their optimized ratings will be skewed  there are a
number of ways to reduce such over fitting  but we are constrained by the very large size of the design
 

filocally reweighted least squares

    

 
   

best model
optimum weights
 from postanalysis 

   
   

    

weight

number of games

    

    
    

   
 
   
   
   

   

   

 
 

  

  

month

  

  

 

   

  

  

  

  

  

month

  

  

  

  

   

figure    the number of training examples per month  the

figure    the optimal weight function and the power law

last months are more populated  making cross validation difficult 

approximation  the optimal weight function produces a test
error of       

matrix  one method which is computationally efficient is to introduce a damping factor to the least squares
fit 
min  ar  y     d   r    
r

   

the damping factor d governs the penalty for ratings which differ from zero  a quadratic penalty is not
necessarily optimal  but such a mechanism is simple to incorporate into an iterative least squares algorithm
and the problem remains numerically tractable 
in addition to penalizing outlying ratings  it is also possible to penalize outlying predictions  if br gives
the predicted values for the test set  then we can optimize the following 
min  ar  y     d    r     d    br    
r

    

the above problem remains feasible  and post analysis shows that the optimal values for the parameters are
d      and d          this produces a substantial increase in performance  and the resulting scores are now 
training error          

test error          

    

which would put the model into   th place in the competition 

   

include quadratic ratings

evidence shows that at the highest level of chess competition  draws are more frequent  that is to say  the
prediction pi should tend towards     as rw  rb increases  this behavior cannot be captured by the current
linear model  which only uses the rating difference rw  rb  
this suggests we introduce a more complicated model  but unfortunatley we are computationally limited 
running a single minimization with a non linear model  using gradient descent  takes approximately fifteen
minutes on modern hardware  while this is reasonable for the computation of a final prediction  this is
prohibitive for the lengthy process of model  and parameter selection 
one compromise is to employ a non linear model to the linearly determined ratings  one might question whether this is a reasonable procedure since the ratings were already optimized based on their linear
differences  figure     shows that indeed there is additional information in the ratings beyond their linear
differences  notice that ra  rb is a good predictor for the overall score  but that as ra   rb increases  the
likelihood of a draw increases  this is in accord with the phenomenon we expected to observe 
so  given the linearly fitted ratings as features  we can generate a non linear fit to the data to improve
our results  it turns out that a quadratic model has the best performance  and decreases the error a small
amount 
training error          

test error          
 

    

fifigure    the observed score as a function of the linearly fitted ratings  the likelihood of a draw increases with the sum of
the ratings 

   

post processing

finally  a bit of post processing can be performed on the predictions for further optimization  binning the
predictions near         and    to those values helps a small amount  curiously  it turns out that previous
games between the same opponents are actually very poor indicators of future outcomes  this problem is
exacerbated with the optimal ratings because the ratings are chosen to fit past outcomes  this particular
limitation can be alleviated to some extent by rescaling the predictions between players who have previously
played each other closer toward the mean  all together  these post processing procedures can reduce the
overall error to the final result 
training error          

 

test error          

    

conclusions

the elo benchmark for this competition was         this benchmark was easily surpassed with a simple
implementation of the optimal ratings method  further modifications and optimizations allowed this model
to achieve a minimum score of        well above the winning score for this competition  unfortunately  the
parameter choices for this particular model are unintuitive  unlikely to generalize  and impossible to obtain
without the full data set on hand  nevertheless  the basic framework allows for the construction of very
good models with reasonable parameter choices  the best model we found with realistic parameter values
obtained a score of         which would finish in the top ten of the competition  unfortunately  many of the
improvements to the model came after the competition ended  so they couldnt be submitted 
one potential drawback to the optimal ratings method is that for large datasets it is computationally
impractical  this problem has a fairly straightforward remedy  once the overall scale and mean of the
ratings are set  ratings which are calculated within inclusive sub populations are comparable  this means
that optimal ratings can be calculated in separate  manageable blocks  moreover  the final ratings could be
constructed from an average of several different partitions of the entire population into such blocks  in this
way  ratings could be assigned regardless of how large the population happens to be 

 

fi