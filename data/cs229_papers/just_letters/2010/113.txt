assessing the value of ebay listing features
leon lin    leonlin stanford edu 
vasant ramachandran   vasantr stanford edu 
december         

abstract
we used machine learning to access the value of additional features sellers can use to highlight their ebay item listings 
the set of features a user can add to their listing includes subtitles  extra photos  a pop up photo view of an item  a listing
designer  and a bold listing of their item in search results  as ebay fixes the costs for these features  we implemented a
price prediction scheme to determine which listing features add most value to an item and identified the relative
importance of specific features in price determination 
motivation
in       online marketplaces such as ebay  yahoo  auctions  and amazon marketplace accounted for     of all online ecommerce      with over     billion of product sales and over         registered users considering revenue made from
online auctions their primary source of income in           ebay also serves as a significant source of revenue for a vast
portion of users  while there is a large component of uncertainty caused by human behavior in predicting the final prices
of ebay listings      numerous studies have been performed suggesting that there is a high correlation between certain
features of an ebay listing and the final buying price of the listing      demonstrates a machine learning algorithm
incorporating decision trees which was able to predict the final buying price within     up to     of the time on certain
item keywords  as a result  knowing which features of a product listing to include can be a highly profitable asset for ebay
sellers  especially the over         sellers who in      cited ebay as their primary source of income      the goal of this
project is to access the validity of this notion  thereby drawing conclusions regarding what is the optimal feature set to
purchase when listing an item  as well as what is the optimal pricing scheme for listing features a substantial amount of
work has been done on analyzing auctions using data mining and statistical techniques  perhaps most related to our current
study is     which makes price predictions based upon the number of pictures  feedback rating  and description of the item 
to our knowledge  there has been no work specifically focused on the influence of item listing features which sellers can
add for a fee on price prediction  indeed  such prior research seems highly unlikely given that ebay did not define their
current set of fee based listing features until october of          
feature set and data
the fixed price addable features are described by ebay as a subtitle        to capture the interest of buyers when they
view search results by displaying more information below the title  gallery plus        which displays     pixel pictures
from your listing in search results when buyers scroll over a gallery picture  pictures       per photo  first photo is free 
which allow potential buyers to see more images of the item being sold  a listing designer        to enhance item
descriptions with an attractive theme that complements the type of item being sold  and bold        to make the listing
title bold in search results 
thus  in assessing the influence of each of these features on the final selling price of an auction  we were able to draw
conclusions regarding the optimal set of features to purchase for an item listing  we used our own web crawler written in
java to extract the features of particular completed ebay listings and manually checked each item listing to insure the
accuracy of our training data  we focused specifically on listings returned by a search for garmin nuvi   w and garmin
nuvi    wt  under the gps systems category on ebay with the specified condition  new  these items were selected
on the basis that they were very popular items and had high keyword specificity  we represented a feature vector as a set
of four boolean values  to indicate presence or absence of a feature  and an integer to represent the number of photos 
research methodology
in order to achieve our goals of predicting the end price of a listing of the item based upon an observation of the features
associated with the listing  as well as understanding the relative importance of each feature  we defined our predictions as

fia choice of one of ten    price intervals  buckets  between set minimum and maximum prices  since most of our features
were categorical  this multinomial outcome generation allowed us to compare the relative merits of three separate
approaches to price determination  a decision  cart  tree approach  softmax regression  and a nave bayes classifier
system to sort observations into buckets 
classification and regression trees
to compare the results of traditional machine learning algorithms to a decision tree approach to the problem  we trained a
cart  classification and regression tree  on the input data  for our categorical numeric variable input set  the split
criterion looks like xj  v  with v  wj  in our model  wj refers to the collection of all possible categories of variable xj 
which for the gallery  subtitle  designer listing  and bold options is just  indicating absenceand  indicating
presence  the number of photos variable takes on multiple integer values  the terminal nodes  leaves  contain a  value 
an estimate for the price  in the leaf  in practice this value is taken to be the average of all observed y values whose
decision path ends in the leaf      once a prediction is made  following a particular decision path to a leaf node   we place
the leaf estimate in its price bucket 
because a decision tree model typically overfits the data  we introduced a pruning phase which used    fold cross
validation on our training set to optimize our tree to a minimal cost cart  the cost of the tree is the sum over all terminal
nodes of the estimated probability of a node times the cost of a node  while cost of the node is the average squared error
over observations at the node     specifically  the cost represents the inaccuracy of the absolute price prediction when
compared to the actual price  rather than the discrepancy between the price bucket prediction and the actual bucket  in our
case  we averaged the cost of each of the ten carts built during the cross validation process  and sought to minimize this
cost function iteratively in a matlab routine by successively pruning nodes and calculating subtree costs to achieve the
optimal pruning sequence  an estimate for the best level of pruning is defined as the smallest tree that is within one
standard error of the minimum cost subtree     while regression trees can be represented as an amalgamation of simple ifthen rules  a stronger relative importance metric for each feature was desirable in order to determine a weight for each
feature  for a single cart model  the following formula measures the importance of variable x j    
 

     

      vn   xj
  

the summation covers the non terminal nodes in tree b  which has k leaves    denotes the indicator function  while n is
the split variable of node n  the feature on which the split is made   in order to measure the relative importance of the j th
feature  the improvement in average squared error as a result of all splits on the j th feature is calculated 
the factor      measures the improvement in squared error as a result of the split in node n 
 
              
   
here wl and wr are the probabilities that the decision path turns to the left or right child node of node n  and y l and yr are
the average price predictions of paths going through the left and right child nodes     once the cart is constructed and
pruned to optimally  the relative importance metric    for price prediction can be calculated for each feature  to achieve a
large    the feature must be used in several even splits with a significant difference in the average price prediction down
all possible left and right paths     finally  the    are normalized in our model to an average feature weight of   
  

 

 

     
    

multinomial logistic regression
in place of a linear or logistic regression model for price prediction  the cart is able to determine with greater flexibility
which features to use in order to predict the target variable  price   even if the relationship between price and the feature
set is nonlinear  also  the cart is capable of handling categorical variables and missing values if an untransformed data
set is used to train or test      the drawback of the cart model is stability  our data set size is limited  and the model
responds adversely to slight changes to the data set  removing duplicate listings  combining listing sets for very similar
products  etc which necessitates consideration of alternative models  absolute price prediction with a linear regression
model performs poorly on a numeric transformation of the observation feature set  as the set consists of boolean and
discrete integer variables  however  the price bucket prediction model works well with multinomial classification and
regression  treating each bucket as an outcome       to perform multinomial classification of the observations into price
buckets  a multinomial logistic regression model was trained on the data  the weights  for each feature are determined
on the training set to maximize the log likelihood of the training observations under the softmax regression model  and

fithen each test observation is classified to the price bucket with the largest probability      this maximum entropy model
does not assume feature independence 
   
pr yi      

          
multiple nave bayes classifiers
we created multiple binary classifiers assuming feature independence  with each classifier learning whether the end price
of the auction would be greater than the minimum of a specific price interval or not  the predicted interval is then selected
by combining the results from all the classifiers  inconsistencies between classifiers that should predict the same outcome
on test data are resolved in favor of the lowest possible interval  this technique was motivated by the scarcity of training
examples for any one specific item in the online auction  as binary classification would be more robust under a sparse
training set than a more complicated classification 
optimizations 
many large ebay sellers will tend to sell a particular item in bulk  thereby listing the same item multiple times with the
same feature set  thus  to prevent highly replicated listings from skewing our model parameters  we performed duplicate
elimination  where all items with the same title  features  and seller were condensed into one listing  the selling price of
the condensed listing was the average price of the replicas 
to combat the problem of limited training data for each model  we decided to create models which combined the training
data from both our queries  performing such a merger  however  presented a trade off  while we would have more data
with which to train our model  we introduce a new component of variability between each of the individual listings  in
practice  the variability turned out to be minimal  as the items are very similar  we merged the two data sets by first taking
the average final selling prices of the results retrieved from the two keywords  respectively  with duplicates removed  and
found that the garmin nuvi    wt sold at an average price of         while the garmin nuvi    w sold at the average
price of         upon transforming the    w data set in this fashion  we found that the new distribution had a variance of
    upon removal of       of the outlying data points  the garmin nuvi    wt data set had an equal mean to the
transformed data set and a variance of      therefore  assuming that features influence the selling price of twin items in
similar ways  we concluded that a merger between the two data sets was now possible 
results and analysis
   fold cross validation error in price bucket prediction   

  
  
  
  
  
  
 

merge optimized data    
rows 
garmin nuvi    w     
rows 
cart

softmax
regression

multiple nave
bayes classifier

garmin nuvi    wt    
rows 

our intuition about the use of duplicate elimination to reduce cross validation error was proven correct  as was our choice
to merge data sets  softmax regression exhibits significantly poorer results than the other models  partially due to the use
of discrete  integer  features  which do not translate as well as expected to the continuous probability prediction that the
multinomial logit requires  additionally  if the multinomial logit is used to model choices  it assumes independent
irrelevance of alternatives  iia  which is not always desirable       this assumption states that the odds do not depend on
other alternatives that are not relevant  but this is not true with market psychology  as buyers typically violate rational
choice theory  i e  the choice between a bold  subtitled car  a subtitled car  and a subtitled bus       also  the collinearity
assumption  that the attributes are not linearly related  in a regression model is hard to apply to the discrete case  where
presence implies identical values and unintended correlation       due to the use of the optimal pruning scheme  the

ficart  which was the predictably best performer  does not over fit as much expected and appears relatively stable under
the merge of the two training sets  the nave bayes classifier exhibited a training error of only         on all three data
sets  but a significantly higher cross validation error was observed due to potential overfitting  due to the high variance of
the data  there were only nine to ten occurrences  in cross validation  of inconsistencies in classification  where a classifier
with a lower threshold would predict a   while a higher threshold classifier would predict     suggesting clear
stratification of the pricing 
relative importance of features cart   

 
   
merge optimized data

 

garmin nuvi    w

   
 

garmin nuvi    wt
bold

subtitle

gallery
plus

designer
listing

photos

the relative importance of each feature is also presented above  despite ebays pricing scheme  our findings suggest that
the designer listing feature was the most important feature for determining the final selling price of an item  followed
closely by the subtitle feature while additional photos and gallery plus proved to be the least important feature  the
significance of designer listings can be explained by the fact that the designer listing feature offer the largest visual impact
of all the features  giving the seller complete control over the html displayed in the item listing  this might prove
particularly significant to the keywords being studied in this paper  since potential buyers of gps systems often care most
about features and specifications  which can be most effectively presented to the user in tables and charts provided by the
designer listing feature  at the same time  potential buyers of new gps systems are likely to be less swayed by the
presence of additional photos  since the user is not likely to gain much information from seeing additional photos of the
product  this holds especially true for our data set  where we intentionally tried to minimize the item variability amongst
different listings  therefore  for our data set in particular  listings are unlikely the capture the attention of users by
displaying additional photos of the item to users because all photos illustrate the same item  what the seller is truly able to
customize is  instead  the actual presentation of the item and its description and specifications  which can be significantly
enhanced through the listing designer 
human vs  machine learning
a    fold cross validation on the cart cost  which calculates squared error in absolute price prediction across the tree 
showed us that the error is comparable in value  when normalized to a percentage  to the bucket prediction cross
validation error shown above  thus  the bucketing did not introduce significant error to our cart model  which was
contrary to expectation     
another hypothesis we wanted to test was the correlation between seller feedback score and the total amount that a seller
spent on listing features when using ebays pricing scheme vs  using a pricing scheme that was deemed optimal based
upon the weights outputted by our model  for this pricing scheme  we set the feature with the highest weight at maximum
price and scaled the other feature prices accordingly  over time  we hypothesized users with higher feedback scores will
tend to purchase features which carry the most weight  and thus will have the highest probability of increasing the final
selling price of the listed item  we found that there was higher correlation between the log of seller feedback scores to
money spent on listing features when pricing by our optimal weight scheme vs  the log of selling feedback scores to
money spent on features when using ebays pricing scheme  while the correlation between feedback score and price spent
on listing features was relatively small in both cases  it is nonetheless interesting to note that experienced sellers tend to
select listing features which correspond more closely to the strategy suggested by our weighting system  rather than the
system proposed by ebays current pricing scheme  therefore  sellers on ebay are learning the same model as our
machine learning algorithms are 

fi  
  
  
  
  
  
  
 
 

merge optimized data
garmin nuvi    w
garmin nuvi    wt
cart cost   

listing price score
listing price score
correlation with ebay correlation with our
pricing   
pricing   

conclusions and future work
the largest limitation of our project was the scope of our training set  while we would have liked to utilize a much larger
and diverse training set  our abilities to procure a larger training set were curtailed by our inability to automatically retrieve
feature information for each item  ebay blocks any scraping of completed listings  and so our feature vectors had to be
manually created  this only allowed us to train on a data set of     listings in total due to time constraints with fetching
the data set  additionally  we were only able to train on models on items from a specific category  which is quite a limiting
constraint given that feature weights would likely vary heavily depending upon the category being investigated  for
instance  the presence of photos would probably be much more important when purchasing rare paintings vs  purchasing a
new electronic item  since electronic items are the same from listing to listing when searched under the same keyword 
nevertheless  we were able to establish a sense of relative importance for each feature  a relatively accurate pricepredictor  and an optimal pricing scheme for the feature set 
references
  

ghani  rayid  and hillery simmons   predicting the end price of online auctions   accenture technology labs     web     nov 
     
    number of active ebay sellers    google answers  ebay     jan        web     nov       
 http   answers google com answers threadview id         
   heijst  dennis v   rob potharst  and michiel v  wezel   a support system for predicting ebay end prices   decision support
systems                    web     nov      
   l  breiman  j h  friedman  r a  olshen  c j  stone  classification
and regression trees   nd edition  chapman   hall  ny       
   d  bryan  d  lucking reiley  n  prasad  d  reeves  pennies from ebay  the determinants of price in online auctions  working
papers       department of economics  vanderbilt university  november       may      version  
   a e  roth  a  ockenfels  last minute bidding and the rules for ending second price auctions  evidence from ebay and amazon
auctions on the internet  american economic review                        september 
   shanshan wang  wolfgang jank  galit shmueli  explaining and forecasting online auction prices and their dynamics using functional
data analysis  journal of business and economic statistics  in press 
    seller update  september        ebay  ebay  n d  web     nov       
 http   pages ebay com sell july    update details index html      
    promoting your item with listing upgrades   ebay  n p   n d  web    dec       
 http   pages ebay com help sell promoting ov html  
    hilbe  joseph m          logistic regression models  chapman   hall crc press 

fi