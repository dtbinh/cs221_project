a reinforcement learning approach for pricing derivatives

thomas grassl
susquehanna international group

abstract
analytical solutions to the derivatives pricing problem are known for only a small subset of derivatives and are usually based on
strict assumptions  practitioners will therefore frequently resort to numerical approximation techniques  in this paper  i will formulate a simple markov decision process for
which the optimal value function will  in a
non arbitrage world  be equivalent to a given
derivatives fair price function  this means
that derivatives pricing can be understood as
a reinforcement learning problem  in order
to solve this problem i will propose a simplified version of the kernel based reinforcement
learning algorithm suggested in     and     

   introduction
one of the biggest challenges in finance is to properly price derivatives  analytical fair price models are
only known for a small subset of derivatives and are
generally based on rather strict assumptions  practitioners thus usually turn to numerical approximation
techniques in order to estimate the fair price  common
approaches rely on the use of monte carlo simulations
 see     or      or on methods rooted in dynamic programming  see     or       the latter cleverly relate
the fair price of a derivative to the optimal value function of a markov decision process  mdp   but require
explicit knowledge of the mdps transition probabilities in order to solve for the optimal value function 
similarly  also monte carlo methods need to know the
probability distribution of the state space in order to
generate random samples from which they can extract
their fair price estimates  in reinforcement learning on
the other hand  state space and transition probabilities
are only used implicitly as the learning is based on trajectory samples from the mdp  we can thus learn our
pricing model directly from data without having to as 

last revision  december          

tgrassl stanford edu
grassl thomas gmail com
sume that the underlyings price follows a specific price
process  this means that one of the biggest pitfalls of
most derivatives pricing methods can be avoided 

   theoretical framework
     the derivatives pricing problem
let u be a risky asset with a stochastic price process
given by  ut   t      we can construct another risky
asset d such that its spot price dt will be a deterministic function of ut at exactly one point in the future 
i e   at some time t the prices of d and u will be
related according to dt   g ut    in finance  such
an asset d is called a derivative  with underlying u
and maturity t   the function g is called the payoff
function of the derivative d 
the derivatives pricing problem deals with the question of how the price of d depends on ut at times
t   t   in absence of arbitrage and under the assumption that  ut   is a markov process    the fair price  dtf
is simply the discounted  expected payoff of d given
ut   i e   if r is the risk free interest rate then
dtf   er t t  e g ut   ut   

   

the difficult part in     is to determine e g ut   ut   
the function g might not be analytically tractable  or
even if it is  finding the expected value of g ut   requires detailed knowledge of the structure of the probabilistic price process  ut   t      analytical solutions to this problem depend on specific assumptions
on  ut   which may break down when applying the resulting model in practice  the classic black scholes
model for example assumes that the returns of ut are
sampled from a continuous  log normal distribution 
and by doing so fails to acknowledge the existence of
jump discontinuities or fat tails that can often be observed in real world price data 
 
this definition was chosen because it simplifies much
of the subsequent work  however  most results should also
be applicable to more complicated derivates 
 
note that this is the only assumption that we will impose on the stochastic process  ut   t     
 
note the difference in notation between spot price dt
and fair price dtf  

fia reinforcement learning approach for pricing derivatives

     trading as a markov decision process
suppose now that a trader holds exactly one unit of a
derivative d at time t  at each time prior to expiry
t   he can either sell d at the current spot price dt or
decide to hold on to it  if he still holds d at time t  
both selling and holding will have the same outcome 
the derivative will be executed  or sold  for the price
dt   g ut    a trading episode ends as soon as d is
either sold or executed  we will denote the traders
possible actions as

as if d is being sold
at  
ah otherwise
and assume that neither action can affect the markets
behaviour  i e   dt   and ut   or  more generally  the
market state mt   are independent of at   only the
traders position qt   depends on his actions  combining mt and qt yields the state of the world at time
t  st    mt   qt    the state action reward in this model
will simply be equal to the traders monetary compensation  i e 

if at   as   t   t
 dt
dt   g ut   if t   t  qt    
   
r st   at    

 
otherwise
for the sake of simplicity  we will assume that all
episodes end at time t   if qt     for t   t   then the
trader will simply have to choose ah at each timestep
between t and t  and thus receive zero reward  
using r st   at   as defined above  the sum of expected
future rewards of a given strategy  or policy   discounted to time t can be written as
  t
 
fi
x
fi

r  t 
v  st     e
e
r s    s    fi st  
  t

from this is it obvious that the value function v  of
our trading strategy obeys the bellman equation
v   st     r st    st      er e  v   st     st    

   

note now that because of the special structure of the
rewards outlined in      the state action value function
q satisfies
 r
e e  v   st     st   if at   ah

q  st   at    
   
dt
if at   as
     equivalence of optimal value and fair
price function
for the policy h where the trader will hold d until expiry  the corresponding value function v h can
easily be computed 
v h  st     er t t  e  g ut   ut    

using     we can see that this is nothing else than the
fair price of d 
v h  st     dtf  

   

suppose now that h is not optimal  i e   that there is
a state s where it is better to sell than to hold 
qh  s   as     qh  s   ah   
using equation     this implies that
d   er e  v h  s      s    
the assumption that  ut   is markov implies that
er e  v h  s      s     er t    e  g ut   u     df
and thus that d   df  
since in a no arbitrage world dt   dtf for all t  it
follows that no such state s can exist  and therefore
that h is an optimal strategy  a direct implication
of this is that the optimal value function v  satisfies
v   st     dtf

   

for all states st   this shows that in the absence of
arbitrage  learning the optimal value function v  is
equivalent to learning the fair value function dtf   in
other words  by solving the above mdp for v    we will
be able to solve the derivatives pricing problem 
note that the above derivation required that dt   dtf  
the result however remains valid even if the market
tends to underestimate the fair price  i e  dt  dtf  
since h would still be an optimal policy  if we allow
dt to be greater than dtf   then also the resulting value
function would be greater than dtf   it would thus provide a measure for the expected extent of mispricings
that a trader can exploit in this flawed market 
     generalizing the fair price model
in the above section i have shown that for a specific
derivative d its fair price can be determined by learning the optimal value function of a simple mdp  while
theoretically useful  this approach would likely be unpractical in real world trading as traders would be required to maintain a separate model for each derivative
they are interested in  pricing models with analytical
solutions  e g   the classic black scholes model  do not
suffer from this shortcoming and can easily be applied
to a whole class of derivatives  such a generalization is
achieved by finding a suitable parameterization of the
considered derivative  the classic black scholes pricing
approach for example parameterizes a european option as a tuple consisting of expiration time t   strike

fia reinforcement learning approach for pricing derivatives

price k and the volatility of returns  of the underlying asset  the trading mdp can be generalized with
a similar trick  we simply absorb  the characteristics
of a derivative d into the state st by using a suitable
parameterization d and define an enhanced state
sd
t    st   d    
as long as the mapping d  d is bijective  it is imd
possible to move from a state sd
t to st   where d    d 
this means that we will learn the optimal value function v   sd
t   using the same trajectories as before and
f
thus that in a non arbitrage world v   sd
t     dt   if
on the other hand d  d is not a bijective mapping
 d
then either v   sd
t     v  st   is the desired outcome
d
d
for st   st or  needs to be improved 
note that for a bijective parameterization on a discrete
state space  this trick only provides a unified notation
for accessing the fair price model of distinct derivatives  learning the fair price still takes place separately
for each of the considered derivatives  this will however change when we attempt to compute approximate
value functions over continuous state spaces 

so large that the problem is computationally unmanageable  to avoid this  we will assume that the trading
mdp has a continuous state space 
most canonical reinforcement learning algorithms and
their corresponding convergence guarantuees deal with
the problem of approximating optimal policies for finite state spaces and can usually not easily be generalized to the continuous case  instead  reinforcement
learning in continuous state space often attempts to
approximate the optimal value function directly from
a given sample of trajectories from the mdp 
one approach that is equipped with a guarantueed
convergence to the optimal value function v  is
kernel based reinforcement learning  see       a
meaningful modification to it is provided in      it is
shown that finding the approximate value function of
an exact  continuous mdp can be understood to be
equivalent to finding the exact value function of an approximate  finite mdp while still maintaining the same
convergence guarantuees  applying this approach to
the trading mdp essentially means that we will interpret samples of a high dimensional discrete mdp as
samples from a continuous mdp which we will then
solve by approximating it with a simpler discrete mdp 

   implementation considerations
     exploitation vs  exploration
a problem inherent to reinforcement learning is the exploration exploitation dilemma where a reinforcement
agent faces a trade off between maximizing the shortterm reward by exploiting his current knowledge of the
mdp or maximizing the long term reward by exploring unknown regions of the state action space  see      
the special nature of the trading mdp eliminates this
dilemma  the state st is described as a tuple  mt   qt  
where the traders position qt is directly and deterministically affected by his actions  a transition from mt
to mt   will thus provide information regarding both
possible action choices as and ah   this property of
the trading mdp simplifies the collection of data as
only the transitions of the market states mt need to
be observed 
     kernel based reinforcement learning
trading is essentially a finite mdp  a trader only needs
to act at discrete times  e g   when new information becomes available   prices and sizes can only change in
discrete increments and the action space in the above
mdp consists of only two choices  ah and as   however  the dimensions of this mdp can quickly become
 
for a practical example of how to absorb the derivatives parameters into the state space see section       

     constructing an approximate mdp
suppose that we have sampled n transitions
 s i    s i    a i     where we transitioned from state s i 
to s i  as a result of action a i    from an mdp
m    s  a  t  r  with continuous state space s  discrete action space a  transition probabilities t and
reward function r  jong and stone  see      use
these samples to approximate m with a finite mdp
m    d  a  t   r  where d    s i    is the set of successor states  for some kernel function  with suitably
chosen bandwidth b and premetric d  t and r are defined as
 


d s s i   
 
if a i    a
s a 
 i 
z
b
t  s  a  s    
 
otherwise


 i 
x
 
d s  s  
r s  a    s a

r s i    a i   
z
b
 i 
i a  a


x
d s  s i   
s a
z  

 
b
 i 
i a

 a

the authors argue that the exact solution of m   v   
converges to the exact solution of m   v    as the number of samples increases 
in our trading mdp  the market transitions independently of the chosen action a  this yields the following

fia reinforcement learning approach for pricing derivatives

strike price   and underlying prices given by

simplified version of the above expressions 


 
d m  m i   
t  s  a  s i      t  m  m i      m 
z
b


 i 
x
 
d m  m  
r s  a    m

r s i    a 
z
b
i
x  d m  m i    
zm  
 

b
i

ct   k ct  
instead of learning ct directly  one can thus learn ct
and recover ct as required  using strike relative prices
ut
k yields a more compact price space and makes it
feasible to efficiently learn our model using data from
a variety of derivatives with potentially vastly different
price levels 

for the state action value function q  it follows that
x
q s  a    r s  a    er
t  m  m i   v  s i   
i


 

r s  a 
if a   as
p
er i t  m  m i   v  s i    otherwise

where v  s    maxa q s  a   thus  estimating v  boils
down to two steps  generating t by observing enough
market transitions  and then solving the mdp m  for
example by using value iteration   a significant computational obstacle is that the size of t is quadratic
in the number of observed samples  in      the authors
suggest that very small entries of t should be set to
zero  in the below example this resulted in a very
sparse transition matrix  without noticeably affecting
the approximation quality of the model 

   example  pricing a european call
     the classic black scholes approach

dut   ut dt   ut dwt
where wt is a wiener process  using a non arbitrage
argument it can be shown that  has to equal the riskless interest rate r  in such a setting  the black scholes
model  see      defines the fair price of a european call
option ct with strike price k and expiration t as
   

where n is the standard normal cumulative distribution function and
d     

note now that wt models the randomness in the
black scholes world  since wt is a wiener process 
the future price risk
  wt  wt   is distributed according to n      t  t   this suggests that in order
to capture the random extent of future price movements  it is not necessary to explicitly use t  t and 
as separate state features 
instead we can use a single

feature equal to  t  t 
these observations indicate that a compact but reasonably complete definition of a market state is


ut 
mt  
  t  t  
k
this definition should enable us to easily combine
derivatives with different underlyings  strike prices  expiration times or volatilities into the same model  ut  
k and t t are all directly observable while the volatility  needs to be estimated from historical data  
     results

suppose that the underlyings spot price ut follows a
log normal random walk with drift  and volatility 

ct   ut n  d     ker t t  n  d   

ut
k 

ln ut  k     r         t  t 

 
 t t

     parameterizing the state space
equation     can be normalized by recognizing that
ct is the product of k and the price of a call ct with
 
a pruning threshold of       reduced the fraction of
nonzero entries from     to    

i initially implemented the above version of kernelbased reinforcement learning using a euclidian distance metric d and a gaussian kernel   intuitively  a
small bandwidth b should result in a very bumpy value
function approximation v  as each sampled transition
 m i    m i    can only noticeably affect predictions in
its close proximity  as b increases  the value function
should take on an increasingly smooth shape  initial
experiments confirmed this inuition  but also unveiled
that  for large values of b  v  was consistently overestimating the fair price of out of the money options 
for large b  the perceptive regions of  around sampled transitions will frequently overlap  which means
that high prices from rather far away can be propagated through to regions of the state space where the
considered option is nearly worthless  this problem is
intrinsic to the chosen algorithm 
we can ameliorate this problem by recognizing that
 
note that  is needed for differentiating between different underlyings  if we would focus on a single underlying
we could disregard  as the model should pick up this information directly from the data 

fia reinforcement learning approach for pricing derivatives

   

vt

   

   

   

   
    

    

 
 

    

u         
t

 
   

   

 
   

 
   

  

t



t

figure    approximated value function v  for a european
call with strike k     

    
    
    
    

this change greatly improved the quality of the model 
figures     and     show the fair price approximation
 using b         and its associated error as compared
to the black scholes price for a european call with
strike price     annualized volatility of     and an annual riskless interest rate of     learning was based
on data from      randomly constructed european
calls  with different underlyings  strike prices  expirations etc   and roughly       randomly generated
transitions  the returns of the respective underlyings
were sampled from a log normal distribution and the
spot prices of the derivatives were assumed to be equal
to the exact black scholes price  the resulting rootmean squared error of this approximation corresponds
to a   value of less than      which is a promising first
result  the models behaviour was consistent across
successive runs with different initializations of the random number generator 

   conclusion
i showed how the derivatives pricing problem can be
written as an mdp to which reinforcement learning
techniques can readily be applied  the proposed algorithm  a version of kernel based reinforcement learning  delivered enouraging results on a simple test problem  but extensive testing is needed in order to evaluate the quality of this approach in more realistic scenarios  applying the model to american or asian options and avoiding the quadratic storage requirements
for t by making better use of its sparseness could be
promising directions for future research 

vt  ct

the euclidian distance measure is unbiased with regards to the single components of a state  e g   points
given by  x    c  x    and  x    x    c  are equally far
way from  x    x     such symmetry is not really desired when pricing options  it seems to be a better idea
to compare options with similar strike relative prices
but different risk than to compare options with similar
risk but different strike relative
 prices  furthermore 
notice that the future risk  t  t is decreasing with
inreasing t  since the true value function reflects the
expected future reward  its approximation should ideally only depend on states that truly lie in the future 
i e   on states where the future risk is smaller than
in the current state  if we would strictly enforce this
forward looking perspective  estimates close to expiry
would suffer from a lack of data  this suggests that
we need a compromise that is looking forward more
than it is looking backwards  these ideas can be incorporated into the model by modifying the distance
function d such that its contours are
 egg shaped curves
tilted towards smaller values of  t  t 

    
    
    
    
    
    

    

 
 

    

u         
t

 
   

   

 
   

 
   

  

t



t

figure    approximation error vt  ct

references
    hatem ben ameur  michele breton  lotfi karoui  and pierre
lecuyer  a dynamic programming approach for pricing options
embedded in bonds  journal of economic dynamics and control 
                       
    fischer black and myron scholes  the pricing of options and corporate liabilities  the journal of political economy               
     
    mark broadie and paul glasserman  pricing american style securities
using simulation  journal of economic dynamics and control                           computational financial modelling 
    nicholas jong and peter stone  kernel based models for reinforcement
learning in continuous state spaces  in icml workshop on kernel
machines and reinforcement learning  june      
    francis a  longstaff and eduardo s  schwartz  valuing american
options by simulation  a simple least squares approach  review of
financial studies                     
    dirk ormoneit and saunak sen  kernel based reinforcement learning 
machine learning             november      
    richard s  sutton and andrew g  barto  reinforcement learning  an
introduction  adaptive computation and machine learning   the mit
press  march      
    john n  tsitsiklis and benjamin van roy  regression methods for
pricing complex american style options  ieee transactions on neural
networks                  

fi