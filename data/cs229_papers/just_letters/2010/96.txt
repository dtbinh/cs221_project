using two lenses for depth estimation and simulation of low depth of field lenses
andy l  lin

introduction
recently  there has been a   d camera craze in the photography and video industry  for example  the
recently launched fujifilm w  uses two lenses to capture two shifted images  these images are then used in
  d displays to simulate depth  often with the help of polarized glasses  there are more capabilities that
multi lens cameras can offer  other than just capturing   d scenes 
one of such possibilities that has yet to be explored in the literature  is using inexpensive   lens cameras to
simulate the artistic effect of low depth of field  dof  lenses  these low dof images which maintains focus
on a small area of the scene  and blurs out the background are often the hallmark of professional and artistic
photography  the way these lenses achieve low dof is by using large apertures  unfortunately  these low
dof lenses are very expensive themselves and can only be used in expensive digital single lens reflex
 dslr  cameras  one viable alternative is to use inexpensive   lens cameras to simulate the out of focus
blurring effect of high end lenses and cameras 
by using the parallax information in the two images captured by the single camera  a machine learning
algorithm can estimate a depth map for the scene by estimating the disparity between the two images  with
the proper depth information  an algorithm can then apply a lens blur function that varies with the depth of
the scene  simulating the effect of an expensive low dof lenses with large apertures 

prior work
extensive amounts of previous studies have been dedicated to recovering depth maps from single images  as
well as multiple images  for example  saxena et  al     uses a combination of monocular and binocular cues
to recover a depth estimation using the help of machine learning algorithms  a collection of state of the art
techniques and algorithms is described and documented in     
since we are using a system with   lenses  it is sensible to focus on binocular stereo reconstruction
algorithms  the simplest of such algorithms is described in      for every point in one of the binocular pair
images  the simple algorithm uses a sliding window approach to find the corresponding shifted window in
the other image which yields a minimum sum of absolute differences error  this distance is then a good
estimate of the shifted location that yields the maximum correlation and thus a good estimate of the
disparity between the two images at a specific pixel 
simulating lens blur functions depending on depth is also a well studied problem  these blur functions have
been experimentally determined and there are also simple equations that can be used to completely
determine the type and amount of blur of a lens based on the focal length and the distance of interest in a
scene 
some simple image editing software suites have implemented simple filters that attempt to reproduce the
artistic effect of low depth of field  however  these filters are usually not adaptive to the scene of interest 
for example  one simple filter is a progressively blurring filter that leaves the center of the scene in focus
and applies a stronger blurring function as we approach the edges of the images  while this type of filter
can produce images have only portions of the scene in focus  this type of filter cannot produce the sharp

fidifferences in focus and defocus necessary for artistic photography  while image segmentation algorithms
may be able to solve this problem  they are often unreliable and often make mistakes 

the stereo reconstruction algorithm
classical techniques
the sum of absolute differences method is a simple way to obtain a depth estimate from a binocular pair     
although computationally inexpensive and relatively easy to implement  this algorithm has a few
weaknesses  for example  in uniform texture regions of the image  it is difficult for the algorithm to find
differences in the sum of absolute differences at different apparent disparities  thus  this baseline image
makes many errors in uniform texture regions     
another weakness of the algorithm is trade offs in window sizes  for small window sizes  the algorithm is
susceptible to the noise in the image  so the resulting depth map will also be noisy  for large window sizes 
the algorithm is less susceptible to noise  however  it is unable to properly calculate depth at depth map
boundaries since each window will contain multiple disparities and attempt to calculate one value for
multiple depths in the scene 
a simple way to improve upon this simple sum of absolute differences algorithm is to apply a simple median
filter on the depth map  median filters are able to remove noise without blurring decision edges  thus 
applying a median filter to a depth map obtained by a small window size can reduce noise without degrading
the performance of the algorithm near decision edges 
another way to improve upon the sum of differences algorithm is by employing a bilateral filter on the
window  bilateral filters are known to be able to blur uniform regions of images  but leave the edges of the
image intact      by applying a bilateral weighting function to the window depending on the pixel values of
that window  we can then obtain depth maps which perform better near edges  but still retains performance
elsewhere 

machine learning techniques
to address these two main weaknesses  we can employ machine learning algorithms  saxena et al       uses
a markov random field model to combine depth maps obtained by different methods  as well as to enforce a
continuity constraint  this method used is very effective in obtaining accurate depth maps from existing
depth maps of different sources  moreover  the algorithm is able to generate these depth maps that are
robust to noise  particularly in uniform regions  due to the clever continuity constraint  however  one
drawback of this approach is that it requires linear programming  which is somewhat computationally
intensive 
often times  a photographer wishes to receive real time feedback on the photograph he she has just taken so
he she can choose to take another photograph if the current one is not satisfying  the markov random field
method requires linear programming  so may take some time to process  thus  a simpler approach was
chosen for the project in order to reduce computational complexity to provide immediate feedback for the
photographer  there are several different classical approaches to depth estimation  a simple way to
improve upon these existing approaches is to use different approaches for different parts of the scene 
machine learning is used to determine which model to use  at specific regions of the scene  for example  in
areas that are close to depth map edges  it is desirable to use a small window size scheme in order to take
advantage of its behavior near object edges  on the other hand  it is desirable to use a large window size
scheme in areas that are far from object edges in order to reduce noise 

fiany feature that could be extracted that aids in determining which optimal model to use is useful  other
features used in the study included  image uniformity  sum of absolute differences error in optimal shift
position  vertical position in image  estimated depth of scene 
as mentioned before  small window size sum of difference algorithms are unable to properly estimate depth
in areas of the image which are uniform  by choosing uniformity of the image as a feature allows the
algorithm to avoid using small window sizes for these regions  the minimum sum of absolute differences
error at each point is a feature similar to distance from object edge which can help determine if a specific
classical method is working well at a particular point  vertical distance and estimated depth of scene are
features  taken from one of the classical methods   which both determine rough distance of the scene is
meant to take advantage of the fact that object further from the camera tend to include denser details per
pixel  perhaps a smaller window size is needed for areas of the scene which are far away 

experimental setup
data
the middlebury stereo project database was used for training and test sets  the database consisted of left
and right images as well as ground truth images for various scenes  ground truth images were obtained
using structured light as described in      for the study    random binocular pairs were chosen for the
training set  and    binocular pairs were used for the test set 

comparisons
the sum of absolute differences sliding window approach was the baseline for comparison      as
mentioned before  this algorithm is susceptible to trade offs in window size  this median filter approach was
the second algorithm used for the comparison  another method to deal with window size trade offs is
applying a bilateral weighting function on the window  this bilateral filter approach was the third
algorithm used for comparison 
the possible conventional methods used for machine learning selection consisted of   x  sum of absolutedifferences   x  sum of absolute differences    x   sum of absolute differences    x   sum of absolute
differences  median filter on  x  sum of absolute differences  blurred median filter on  x  sum of absolutedifferences  and   x   bilateral filter  the reason why the blurred median filter on  x  sum of absolute
differences method was included was to provide a means for the algorithm to generate smoother data  this
smoothed data is necessary because there is no formal smoothness constraint  the hope is for the algorithm
to choose this model in areas of large errors  in efforts to keep the final depth map as smooth as possible 
two types of machine learning classifiers were used for determining which model to use  the nave bayes
classifier and logistic regression  the classical method that resulted in the best estimation of depth at a
specific point was treated as the ground truth for that point 
mean square error  mse  for the depth maps was chosen as the error criteria  it is most effective to
evaluate the error of these images by evaluating the error from the obtained depth maps  the reason for
this is that the final blurred image will be completely dependent on these depth maps  evaluating error on
the final blurred image makes it more difficult to determine difference in quality 

results
the new machine learning technique was able to outperform all conventional algorithms on the dataset used
for the study  a preliminary trial of logistic regression versus the nave bayes technique was performed 

fithe nave bayes technique was able to outperform the logistic regression
ssion method  which also took much
longer to evaluate  thus  the nave bayes approach was used for the remainder of the study  as illustrated
in figure     the new machine learning technique consistently outperforms the conventional methods  on
average  there is a     improvement in performance over the median filter technique   see figure   for the
blurred images produced by these depth maps 
though the obtained results show a great improvement in mse compared to conventional methods  there
are still improvements to be made  most importantly  the estimated depth map contains sharp changes in
depth that do not exist   even though some efforts were made to reduce these effects  these sharp changes in
depth cause artificial artifacts in the final blurred image  perhaps more stringent conditions on continuity
should be used to prevent these artifacts from appearing in the future 

figure    ground truth depth map  upper
upper left   estimated depth map  upper right   blurred image generated with
estimated depth map  lower right   blurred image ge
generated with ground truth depth map  lower
lower left   blurred image
generated with estimated depth map  lower
lower right 

fi   
   
   
depth
error    
 mse 
   

window
median filter
bilateral filter
nave bayes

   
 
binocular pairs
figure    mse comparisons of depth map produced by different methods  the horizontal axis represents different test
data  the average values are shown on the right  

conclusion
with the help of machine learning  an algorithm which improves upon conventional techniques for depth
estimation of binocular scenes was obtained  the machine learning technique consists of using features
such as distance from decision boundaries and image uniformity to help determine which conventional
depth estimation algorithm to use in different areas of the image  the final result is depth maps which have
less error than the conventional depth estimation results  when this depth map is used as the basis for a
lens blurring function  plausible images that appear to be produced by large aperture lenses can be
produced 

acknowledgements
thanks to professor andrew ng and the cs    staff for such an inspirational quarter  also  thank you to
professor brian wandell for providing inspiration and advising me this quarter 

references
    a  saxena  j  schulte and a  ng  depth estimation using monocular and stereo cues  proceedings of the   th
international joint conference on artifical intelligence       
    d  scharstein and r  szeliski  a taxonomy and evaluation of dense two frame stereo correspondence algorithms 

international journal of computer vision                  april june      
    david a  forsyth and jean ponce   computer vision  a modern approach   prentice hall       
    volker aurich and jorg weule  non linear gaussian filters performing edge preserving diusion  proceedings of the
dagm symposium       
    d  scharstein and r  szeliski  high accuracy stereo depth maps using structured light  ieee computer society
conference on computer vision and pattern recognition  cvpr        volume    pages          june      

fi