tranfer learning in large scale datasets
huizhong chen  bowen meng

ying chang

email  hchen   bowenm stanford edu
cs    project milestone

email  changy stanford edu
co work for cs    a with professor daphne koller

abstractconventional image classification techniques aim to
predict class labels by training a classifier with the provided
training labels  but the internal relationship between classes
has been ignored  in this project  we explore the feasibility of
transferring knowledge between classes to help boost up the
classification accuracy  two transfer learning approaches have
been studied  namely  instance transfer by jointly optimizing
classifiers via grouping source and target training examples 
and parameter transfer by exploring the relationship between
classifier parameters  we demonstrate the parameter transfer
scheme achieves a remarkably better performance compared to
conventional image classification techniques and the relatively
simple instance transfer approach 

the correlation between the source and the target classes  to
overcome to drawbacks of instance transfer  in section iv 
we propose a novel method by exploring similarity measures
in svm parameters to regularize the cost function of the
target class svm  experimental results and discussions will be
presented in section v  it is shown that our proposed parameter
transfer method outperforms the no transfer baseline  as well
as being more robust than the instance transfer scheme 

i  i ntroduction

the    class scene dataset is adopted as the database to
test image classification algorithms  some sample images from
the dataset are shown in fig   in this section  we describe
our baseline scene classification algorithm using histogram of
oriented gradients  hog  features     and the implementation
of the   vs all svm 

to better reflect the richness of our visual world  several
large scale computer vision datasets have emerged recently 
e g   the    class scene dataset          imagenet     and
sun     dataset  although thousands even millions of images
are collected in these datasets  some classes only have a
small number of instances due to the intrinsic long tailed
distribution of objects in the real world  for those classes with
few instances  it is hard to obtain high performance classifiers
by treating and learning each individual class classifier independently because of the lack of training data  in fact  treating
each class independently ignores a lot of infrastructures in
the class space  for example  although imagenet is organized
according to wordnet     from the semantic aspect  the
hierarchical structure sometimes correlates with the visual
content  i e   classes which are close to each other on the
hierarchical tree are usually visually similar  so questions like
what can be shared and transferred between classes and how
to share transfer so that the classification performance can be
improved  give rise to the motivation of this work 
our objective of this project is to improve the classification
accuracy of the target class by migrating the knowledge from
the source class  while not degrading the classifier performance
for other classes in the dataset  the report is organized as
follows  section ii describes our implementation of the   vsall svm for image classification without doing knowledge
transfer  which serves as the baseline for comparison with
our transfer learning performance  then  in section iii  we
show that by jointly train an svm using the source and target
training examples  it is possible to increase the classification
accuracy compared to the result by training target class alone 
especially when the number of target training examples is
scarce  however  the performance of instance transfer depends
on certain choice of kernels  and more importantly  relies on

ii  c ontent based s cene c lassification u sing svm

a  dense sampling hog
xiao et  al      have reported that the densely sampled hog
features give the best scene classification performance on both
the    class scene dataset and the sun dataset  therefore 
hog has been selected as our features  which will be feed
to the svm for image classification  the performance can be
further boosted by aggregating other feature descriptors such
as sift      gist     and ssim     to the hog result  but in
the interest of computational complexity we do not perform
such analysis in this project 
the computation of hog features follows the same approach as described in      histogram of oriented edge descriptors are densely extracted from the image at steps of
  pixels  then  every      neighboring hog descriptors
are concatenated to form a     dimensional descriptor  the
descriptors are quantized into     visual words using k means 
finally  three level spatial histograms are computed on grids
of            and       which means for each image  the
feature is a                             dimensional vector 
after extracting image features  kernels need to be computed and feed to the   vs all svm to perform scene classification  we have chosen two types of kernels  the linear
kernel and the kl  kernel  i e  histogram intersection   the
linear kernel is selected for computational convenience  while
the choice of the kl  kernel follows the definition of hog
similarity metric as described in     

fifig    

sample images from the    class scene dataset

b    vs all svm for classification
we employ libsvm      to implement a   vs all svm 
with the aim to predict the class labels of testing examples  note that there are more than two classes in the
dataset  hence the   vs all svm has to loop through all
the classes  the implementation is described as follows 
for i     to  number of classes  do
assign class i labels to     all remaining classes to   
train   vs all svm on training set 
compute confidence scores on testing set 
end for
after the svm training and confidence score computation  for
each testing data  we now have confidence scores whose size
is equal to the total number of classes  the class label of a
testing sample will then be predicted as the one which gives
the highest confidence score among all classes 
iii  t ransfer l earning   i nstance t ransfer
in order to improve image classification performance 
classes that have similar semantic meanings can be merged
to train the svm  this is called instance transfer because the
instances of the source class is migrated to the target class 
rohrbach et  al       have proposed using semantic relatedness
between class labels to determine if knowledge transfer is
advantageous  but in this project  the source and the target
classes are manually assigned according to their labels  after
identifying the source and the target  training examples from
the source class are assigned to have the same label as the
target class to jointly train the svm  we implemented the
svm in a flexible and efficient way such that the training
kernel matrix needs only be computed once for the whole
dataset  during transfer learning  the corresponding kernels of
the source class  the target class  and the remaining classes are
selected from the kernel matrix of the whole dataset  in section
v  the performance of scene classification with instance transfer is evaluated  we will see that instance transfer sometimes
outperforms the baseline result when no transfer is carried
out  but it is constrained by certain choices of svm kernels 
and the performance depends on the correlatedness between
the source and the target  to overcome the limitations of

instance transfer  we propose another approach by transferring
knowledge among classifier parameters 
iv  t ransfer l earning   parameter t ransfer
besides instance transfer  it is also possible to transfer
knowledge in the parameter domain by seeking the relationship
between the source and the target classifier parameters  this
type of transfer learning is called parameter transfer  previously proposed framework and methods for multi task learning
are based on the assumption of the relatedness of the tasks 
for example  evgeniou et  al       consider that the classifier
parameters  of all classes are close to some mean parameter
    but the assumption does not include the information about
how close each of the parameters are to the mean parameter
    neither did it specify such an     in our work  we assume
that the source class could help the target class in two ways    
when their classifier parameters are close related  the source
class parameters helps to pull the target class parameters close 
   when their classifier parameters are far away unrelated 
then the source class parameters should push the target class
parameters away towards better values  to sum up  the idea is
that how much the source could assist the target is determined
by the similarity of their classifier parameters 
the relationship in terms of t and s can be written as 
t   s   
where t and s are the parameters for the target class and the
source class respectively  and the sub indices t and s denote
the target class and the source class hereafter  ws   functioning
as the w  as described above  is an improvement evgenious
method      since it is based on the infrastructure of different
classes rather than assumptions  the difference of the source
and the target parameters is   which specifies the distances
in different dimensions of the classifier parameter 
therefore  the objective function of the svm can be written
as 

 
 
i
min j t     t t t    t diag     c
 
 
i
 

  t
 
t t    t  s  t diag   t  s  
    
 c
i
   
i

fi i 

 i 

yt  tt xt   b      i
i   

s t 

for i        m
for i        m

we conclude parameter transfer is well suited for knowledge
transfer between arbitrary classes 
a  evaluation of instance transfer

where diag   is a square matrix with its diagonal elements

being                  n  t   here  i    pre 
pre   and 
t i
s i  
denotes a weighting factor controlling how close we force the
two group of parameters t and s to be  a larger  indicates
a closer relation between the parameters t and s and vice
pre
pre
versa  t i
and s i
are the pre computed i th parameters
for the target class and source class by using ordinary svm 
pre
pre  
therefore   t i
 s i
  is essentially the i th empirical
distance between the target and the source class  which will be
employed as a prior knowledge for the following later transfer
learning stage  as shown in   the empirical distance is used as
the denominator so as to normalize the penalty term introduced
by the assumption of parameter similarity  with the tool of
lagrangian  we can formulate the dual problem as 

 
max
i  t diag yt  xtt diag        xt diag yt  
 
i
 st diag ys  xst diag 


 xt diag yt  
  

   

s t     i  c

i yii    
i

where  is the dual optimizer for the same problem  xt
and xs are the training features for the target class and the
source class respectively yt    y            y  m    are the target
class training labels and ys are the source class training
labels  diag        is the inverse of the square matrix
whose diagonal is       mathematically  diag         
 
diag    
   st is the pre computed dual optimizer for the
source class  and it is resulted by substituting the svm

 i   i 
equation s   i s i ys xs into      hence  we have the
the dual optimization in the kernel form as in      note that

xtt diag        xt and xst diag    
 xt can be treated
as new kernels and can be computed efficiently  we call

xtt diag        xt and xst diag    
 xt the reweighted
kernels  in our work  the computation of the reweighted
kernels is carried out using linear kernel rather than kl  or
other commonly used kernels  because xtt diag        xt

and xst diag    
 xt only have effective distance meanings
in the linear form 
v  e xperimental r esults
in this section  the performance of both instance transfer
and parameter transfer will be evaluated  we will show that although the relatively naive instance transfer scheme improves
the classification performance under some circumstances  it
suffers from two major drawbacks     not well generalizable
to different types of kernels     the choice of the source
and target largely affects the classifier performance  on the
other hand  the parameter transfer scheme has demonstrated its
superior ability in overcoming these two drawbacks hence and

our experiment is performed on the    class scene dataset 
where the classification accuracies of svm with and without
instance transfer have been evaluated  to study the effect of
the size of the training set  the number of training examples
for the target class varies from   to      whilst the number
of training examples for the source and each of the remaining
   classes is kept at      to eliminate the randomness of the
experiment  each test is performed    times  every time using
randomly sampled images to train and test the svm 
   instance transfer   source and target closely related 
in our first experiment  the mit highway class and the mit
street class are used as the source and the target respectively 
these two classes are closely related so a better classification
accuracy should be resulted from transfer learning  as depicted
in fig  a  if the kl  kernel is used  with the help of the
source class  the scene recognition accuracy of the target
class is significantly better when the target class has very few
training samples  the target class recognition accuracy with
and without the source converges as its number of training
samples increases  this agrees with our intuition that transfer
learning will be most beneficial when training data is scarce 
however  for the linear kernel case  fig  a shows that instance
transfer actually hurts the svm performance  this implies that
instance transfer may not generalize well to different types of
kernels 
it is also worthwhile to learn the effect of instance transfer
on the classification performance averaged over all the data
classes  ideally  transferring knowledge to the target class
should not degrade the classification performance for other
classes  fig  b plots the recognition accuracy averaged over
all the training classes  for the kl  kernel case  as illustrated
in fig  b  instance transfer offers a better accuracy when the
number of target training samples is small  however  as the
target training set grows  svm without instance transfer starts
to give a higher accuracy  this is because as the number of
target training examples increases  the knowledge from the
source becomes less and less useful  in fact  the source can be
regarded as a kind of noisy training examples for the target 
at a certain point  when the negative effect of the source class
outgrows the positive knowledge it offers  instance transfer can
be harmful and degrades the average classification accuracy of
all classes  for the linear kernel case  unfortunately  instance
transfer always gives a worse performance compared to the no
knowledge transfer baseline  the poor performance of instance
transfer using the linear kernel again verifies our understanding
that instance transfer does not generalize well to the linear
kernel 
   instance transfer   source and target not related  in
section v a   we have observed that when the source and
target classes are closely related  instance transfer sometimes
offers a better performance than the baseline of not doing
any transfer learning  it is interesting to further investigate

fitarget class  source   target   

all class  source   target   

   

  

  

  

  

classification accuracy

classification accuracy

  

  

  

  

  

  

  

  

  

instance transfer  kl  
instance transfer  linear 
no transfer  kl  
no transfer  linear 

  

 

 

  

  

  

  

  
  
number of training samples

  

  

  

instance transfer  kl  
instance transfer  linear 
no transfer  kl  
no transfer  linear 
  

   

 

  

  

  

  

 a 

  
  
number of training samples

  

  

  

   

 b 

target class  source   target   

all class  source   target   

   

  

  

  

  

classification accuracy

classification accuracy

  

  

  

  

  

  

  

  

  

instance transfer  kl  
instance transfer  linear 
no transfer  kl  
no transfer  linear 

  

 

 

  

  

  

  

  
  
number of training samples

  

  

  

   

 c 

instance transfer  kl  
instance transfer  linear 
no transfer  kl  
no transfer  linear 
  

 

  

  

  

  

  
  
number of training samples

  

  

  

   

 d 

fig     classification accuracy of instance transfer   a  target class accuracy  target mit street  source  mit highway  b  all class accuracy  target mit
street  source mit highway  c  target class accuracy  target mit street  source mit coast  d  all class accuracy  target mit street  source mit coast 

the performance of transfer learning under the scenario when
the source and the target are not closely related  therefore 
we select the source class to be mit coast and the target
class to be mit street  and perform the same experiment
as described in section v a   as can be seen in fig  c and
fig  d  for both the kl  and the linear kernels  the instance
transfer learning curve always underperforms the original no
transfer learning curve  the degraded performance of instance
transfer can be easily understood  as the source class examples
now behave like mislabelled training samples  since these
mislablled training samples are grouped with the target class
training samples to jointly train the svm  the instance transfer
performance is expected to be lower 
to summarize  under certain circumstances  instance transfer offers a better classification accuracy  however  instance
transfer is very sensitive to the specific choice of the svm
kernel  as well as choice of the source and the target classes 
in the next section  we will demonstrate that our parameter
transfer scheme outperforms instance transfer in terms of these
two aspects 
b  evaluation of parameter transfer
details of the parameter transfer algorithm have been described in section iv  as a comparison  the transfer learning
scheme proposed in      has also been examined  the difference between our proposed method and evgenious method
is that  we compute the reweighted kernels by using the

parameter empirical distance between the source and the target
classifiers  whereas the work in      simply assigns a constant
to the parameter distance  fig   plots the learning curves for
our proposed method  evgenious method and the no transfer
learning case  the experiment setup is the same as described
in section v a for the instance transfer scheme  but we only
use the linear svm kernel for the parameter transfer since the
reweighted kernel has physical meanings only when the kernel
is in the linear form  fig  a and fig  b simulate the scenario
when source and the target are closely related  whilst fig  c
and fig  d consider the case when the source and the target
are unrelated  to study the effect of the weighting factor 
in      learning curves are plotted for            and      
as illustrated in fig    regardless of the relationship between
the source and the target classes  our proposed reweighted
parameter transfer always outperforms the result when no
transfer learning is conducted  also note that given a certain
choice of   our method offers a better performance than the
non reweighted parameter transfer algorithm 
recall that the instance transfer scheme is sensitive to 
   the choice of kernel     the choice of the source and
target  comparing our reweighted parameter transfer scheme
to the relatively naive instance transfer scheme  it is obvious
that reweighted parameter transfer overcomes these two major
drawbacks  as can be seen in fig    even though the linear
svm kernel is used  the classification accuracy for parameter

fiall class  source   target   
target class  source   target   

  

  

  

  

  
  

classification accuracy

classification accuracy

  

  

  

  

  

  
  

parameter transfer reweighted  beta    
parameter transfer reweighted  beta     
parameter transfer reweighted  beta      
parameter transfer nonreweighted  beta    
parameter transfer nonreweighted  beta     
parameter transfer nonreweighted  beta      
no transfer

  

  

 

  
 

  

  

  

  

  
  
number of training samples

  

  

  

parameter transfer reweighted  beta    
parameter transfer reweighted  beta     
parameter transfer reweighted  beta      
parameter transfer nonreweighted  beta    
parameter transfer nonreweighted  beta     
parameter transfer nonreweighted  beta      
no transfer

  

   

 

  

  

  

  

 a 

  
  
number of training samples

  

  

  

   

 b 

target class  source   target   

all class  source   target   

  

  

  

  

  
  

classification accuracy

classification accuracy

  

  

  

  

  

  
  

parameter transfer reweighted  beta    
parameter transfer reweighted  beta     
parameter transfer reweighted  beta      
parameter transfer nonreweighted  beta    
parameter transfer nonreweighted  beta     
parameter transfer nonreweighted  beta      
no transfer

  

  

 

 

  

  

  

  

  
  
number of training samples

  

  

  

   

 c 

parameter transfer reweighted  beta    
parameter transfer reweighted  beta     
parameter transfer reweighted  beta      
parameter transfer nonreweighted  beta    
parameter transfer nonreweighted  beta     
parameter transfer nonreweighted  beta      
no transfer

  

  

  

 

  

  

  

  

  
  
number of training samples

  

  

  

   

 d 

fig     classification accuracy of parameter transfer   a  target class accuracy  target mit street  source  mit highway  b  all class accuracy  target mit
street  source mit highway  c  target class accuracy  target mit street  source mit coast  d  all class accuracy  target mit street  source mit coast 

transfer still outperforms the no transfer learning baseline 
more importantly  the reweighted parameter transfer is robust
to arbitrary choice of the source and the target  in other words 
given any combination of source and target classes  reweighted
parameter transfer always offers a higher classification accuracy no matter whether the source and target are semantically
closely related or unrelated 
vi  c onclusions
in this project  we demonstrated the usefulness of transferring knowledge in multi task learning  our proposed
reweighted parameter transfer scheme provides a significantly
improved performance over non reweighted parameter transfer
and no transfer  also  reweighted parameter transfer is not
sensitive of the choice of source and target classes and hence
is more reliable than instance transfer 
at the current stage  our reweighted parameter transfer only
works for the linear kernel since the parameter distance is
physically more interpretable in the linear form  in future  it
is interesting to further understand the parameter distance and
investigate the feasibility to generalize our work to accommodate other types of svm kernels s
r eferences
    s  lazebnik  c  schmid  and j  ponce  beyond bags of features  spatial
pyramid matching for recognizing natural scene categories  in ieee
conference on computer vision and pattern recognition        pp  ii 
         

    f  f  li and p  perona  a bayesian hierarchical model for learning
natural scene categories  in cvpr        pp  ii         
    j  deng  w  dong  r  socher  k  l  l  j  li  and l  fei fei  imagenet 
a large scale hierarchical image database  in ieee conference on
computer vision and pattern recognition       
    j  xiao  j  hays  k  ehinger  a  oliva  and a  torralba  sun database 
large scale scene recognition from abbey to zoo  in conference on
computer vision and pattern recognition       
university 
wordnet 
 online  
available 
    princeton
http   wordnet princeton edu
    n  dalal and b  triggs  histograms of oriented gradients for human
detection  in cvpr        pp         
    d  g  lowe  distinctive image features from scale invariant keypoints 
international journal of computer vision  vol      no     pp        
nov       
    a  oliva and a  torralba  modeling the shape of the scene  a
holistic representation of the spatial envelope  international journal
of computer vision  vol      pp               
    e  shechtman and m  irani  matching local self similarities across
images and videos  in computer vision and pattern recognition       
cvpr     ieee conference on        pp     
     c  c  chang and c  j  lin  libsvm  a library for
support
vector
machines 
     
software
available
at
http   www csie ntu edu tw  cjlin libsvm 
     m  rohrbach  m  stark  g  szarvas  i  gurevych  and b  schiele 
what helps where   and why  semantic relatedness for knowledge
transfer  in computer vision and pattern recognition  cvpr       
ieee conference on        pp          
     t  evgeniou and m  pontil  regularized multitask learning  in
proceedings of the tenth acm sigkdd international conference on
knowledge discovery and data mining        pp         

fi