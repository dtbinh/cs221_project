object identification in dynamic environments
andrew chou  elliott jin  wendy mu
december         

 

introduction

pipeline  the pr  uses the robot operating system  ros   so the code for our project is all in a
form that can be run on the pr   the duration of
each scan was     seconds  giving on the order of
        points in each point cloud  which was generally dense enough for our algorithms 

the overall problem our project tackles is to have a
robot be able to recognize objects that have been introduced or moved around in an environment  more
specifically  given a room before and after new objects have been introduced  we would like to be
able to recognize how many new objects there are 
whether they should be picked up or not  navigate
to where those that should be picked up are  and
pick them up 

   

 

in practical applications  the robot would take the
before and after room scans at two different times  so
it would be difficult to ensure that the robot will be
in the exact same position and orientation for both
scans  as a result  the  d point clouds from the before and after scans will not even have constant elements aligned  such as a wall or big table  in order to
align the before and after scans of the room  we used
the iterative closest point  icp  algorithm  we
used a pre existing implementation of the icp algorithm that ros already contained  this was the an
iterative approach using the levenberg marquardt
algorithm  which is used to minimize the non linear
least squares error  because the robot at the point
would be in the perspective of the second scan  we
transform the old point cloud to be aligned with the
new point cloud 

applications

the main application for this would be to have a
robot be able to clean a room  by having it first scan
to see what the clean room should look like  and then
go back after some period of time when the room
has been changed  and clean up the room  other
practical applications include setting up a room or
searching for objects in a crowded environment 

 

point cloud alignment

methods and data

to collect and test data  we used the personal robot
   pr    which has the capability to take laser scans
of its environment  we used the pr  to take repeated scans of a room  before and after we introduce new objects to the environment  the laser
scans returned  d point clouds that represented the
objects and layout of the environment  additionally 
we preprocessed the  d point cloud files to make the
format compatible with the rest of our processing

 

background subtraction

to find the differences between the two scans  we removed all the points in the second scan that were less
than some small fixed distance away from any point
 

fiin the icp aligned first scan  after background subtraction  we are left with a point cloud that is a
subset of the points in the second scan  representing the objects that have been introduced into or
moved in the environment  in particular  we only
compute the difference one way so that objects that
have been moved are treated the same as objects
that have been introduced  and objects that have
been removed are ignored  this makes sense because
practically  the robot should only deal with what is
in the room at the time of the second scan 

   

 

   

 

   

removebackground pointcloud before  pointcloud after 
begin
for i      to after size step   do
for j      to before size step   do
if dist after i   before j     threshold 
continue 
fi
after remove i  
od
od
end

 
   
 
   
 
   

      
    

figure    before aligning before  red  and after
 blue  scans

this code implemented directly turned out to be
slower than we would prefer given the size of the
point clouds  so we have also implemented some optimizations by taking advantage of knowing the ranges
of the x  y  and z  coordinates 

 

   

 

we empirically determined a suitable value for the
threshold  which we set to be      meters  or   centimeters 

   

 

 

   
 
   

identifying object clusters
and location

 
   
 
   

   

   
     
      

clustering

in the point cloud resulting from background subtraction  we have only the points that correspond to
new or moved objects in the second scan  if more
than one object has changed  we would like to be
able to know exactly how many objects there are 

figure    after aligning before  red  to after  blue 
scan

 

fihere  we assume that none of the objects are touching  otherwise from only a  d point cloud it would
be difficult to tell whether two touching objects were
one oddly shaped object or actually two  assuming
objects are not touching  figuring out which points
correspond to different objects becomes a clustering
problem 

 

   

in order to determine how many new objects were
introduced  we used a variation of single linked clustering  two points belong to the same cluster if
there is a path between them consisting of intermediate points such that no distance between intermediate points is greater than some fixed minimum distance  we used an agglomerative method
in which we iterate through the list of points not yet
in a cluster  and add them if they are less than the
fixed minimum distance away from any point in the
current cluster that we are building  if no points
are less than this distance away  then we move onto
building a new cluster  using this algorithm  we can
determine how many clusters  objects  there are and
which points correspond to each object 

 

   
 

 
   

   
 

   

 
 

   

   
 

figure    after background subtraction and clustering  this figure shows the new scan with objects introduced  blue   and the two clusters that were recognized by our clustering algorithm after the background was subtracted  a juice carton  red  and a
backpack  green 

the algorithm we used is as follows 
find neighbor pointcloud cloud  int   labels 
int position  int curr label 
begin
for i    position     to cloud size step   do
if  labels i      
dist cloud position   cloud i     threshold 
labels i    curr label 
find neighbors cloud  labels  i  curr label  
fi
od
end

fi
od
return curr label 
end
the method find clusters gives each point a label corresponding to the object it belongs to  the method
returns the total number of clusters that we found in
the point cloud  representing the number of changed
objects between the first and second scans 
to remove noise that could show up as clusters  we
only considered point clusters that had more than
    points  which we empirically determined would
be a good cutoff given the minimum size of the set of
objects we were considering and the amount of time
we spent scanning the room  how dense the point
cloud scan was   we also empirically determined a
suitable value for the threshold  which we set to be
     meters  or   centimeters 

find clusters pointcloud cloud  int   labels 
begin
curr label      
for i      to cloud size step   do
if labels i     
num labels    num labels     
labels i    num labels 
find neighbors cloud  labels  i  num labels  
 

fi   

location

leave one out cross validation so that we could potentially verify that we wont have overfitting based
on our ratio of features to number of data points 

to represent the location of each object  we use the
center of mass of the object based on the points in
the cluster that correspond to the object 

 
 

object classification

the code for our project uses ros  robot operating system      that runs on the pr   so all of this
process can be done on the robot  we have pipelined
the process so that  when already given data from
the before and after scans of the room  the robot can
automatically align the point scans  subtract out the
background  pick out the clusters that are objects 
navigate to the object  and point its head towards
the object 

it is possible that new objects will be introduced
into the room that we may not want the robot to
pick up  such as furniture or other large objects that
it cannot physically pick up  thus  we additionally
implemented binary classification on point cloud objects to help the pr  decide which objects should
be picked up  we would like to pick up objects that
are trash  to pick features for this classification
problem  we thought about properties about objects
that distinguish trash from non trash  for instance 
a smaller object is more likely to be trash than a
much larger object  etc 

   

integration with the pr 

in ros  each step must be implemented as ros
node  which is an executable that can communicate
with other ros nodes  and in particular  the pr  
we have created the following ros nodes 
   point cloud listener with icp  takes a scan of
the new room and aligns the old scan with the
new scan 

features

   background remover  subtracts out common
elements between the two aligned scans 

the main features we chose to examine were related
to the principal axes of the object  or the lenghts
of an appropriately rotated bounding box  the ten
features we calculated were 

   cluster finder  classifies the remaining points
into different clusters while discarding noise 
   classifier  classifies whether the robot should
attempt to pick up the object or not 

   volume of the bounding box
   area of each of the three distinct face of the
bounding box

   navigation server  sends navigation goals to
the pr  to navigate to objects in its environment and point its head towards the object 

   lenth of each of the three sides of the bounding
box
   the ratio of each of the three pairs of the bounding box

 

we implemented batch gradient descent  because
our training set would just be the set of clusters representing objects that we obtained from the robot 
and this is a small enough set that non stochastic
gradient descent would not be too slow  since
we had relatively few data points  we implemented

conclusion

this paper shows the fundamental steps needed to
consider having a robot clean a room  there are
many challenges to this process that can be addressed using concepts in machine learning  as
this paper has shown  among them are aligning
 

fiscans taken from different perspectives  finding differences between rooms  clustering objects  classification  and navigation  other challenges in the future may include aligning scans from vastly different
perspectives  as opposed to from mostly the same
position   more advanced object classification  and
learning how to pick up different objects 

pr   as well as professor ng for his suggestion to
work on the project of having a robot clean a room 

  

references

    ros wiki  http   www ros org 

 

future work

although we have created and tested all the individual ros nodes mentioned above  we have not fully
tested the integrated nodes together as a complete
pipeline  in the future  we would like to integrate all
these nodes so that the robot can automatically do
all of these steps without human intervention 
we would like to further optimize some parts of the
process in terms of speed  especially the background
removal and clustering algorithms 
additionally  we would like to gather much more
data so that we could have more training examples
to test each part of our pipeline more rigorously  but
especially to test the object classification code that
we have written 
we would like to add grasping capabilities at the
end of our pipeline  that is  given that it is in front
of an object that it needs to pick up  have the pr 
actually pick up the object from the environment 
finally  we would like to implement further improvements to object classification  for instance  in addition to being able to only classify trash and nontrash  it would be helpful if the robot could determine what kind of object it is so that after picking
it up from the environment  it could put it back in
the correct place 

  

acknowledgement

we would like to thank ellen klingbeil and morgan
quigley for their help on working with ros and the
 

fi