cs    term project

predicting tags for urls based on delicious data
 jayesh vyas   varun katta 
   motivation
today bookmarking websites like delicious allow us to bookmark and tag urls  tagging helps
in labeling  organizing and categorizing urls  and can also help in discovering new documents
for topics of interest  predicting tags for a document can have the following uses 




suggesting tags for a document url when a user is attempting to tag as untagged url 
by tagging large number of urls documents  a topic based browsing system can be built
for users to serendipitously discover urls content on a topic of their interest 
invalidating query cache of a search engine when a new document is indexed   queries
which have the predicted tags of a new document are likely to retrieve that document  so
when a new fresh document is indexed by search engines  they can invalidate result caches
for queries which have any of the tag terms in them  this is useful for documents going
through fresh documents pipeline of search engines  which documents are refreshed
much faster than time to live duration in results cache 

   data processing
we got the tagged bookmarked delicious urls from      this data is in json  we wrote
processors to convert the delicious json feed of   million urls to a list of a url to tags
mappings  we sorted the tags by frequency  and picked    popular tags and about      of
these urls which have exactly one of these    tags  we wrote a simple crawler and an
html parser to crawl these urls and extract text from them  the tags that we picked were 
photoshop  art  education  food  research  mac  business  shopping  google  linux 
   feature selection
as with most text classifiers  we stemmed the words using a standard stemmer    and discarded
the stopwords  single character tokens were dropped  very long tokens  of length     
characters  e g  httppagead googlesyndicationcompageadimgadidx  which crept in because
of our primitive html parser were also dropped and documents with very few features   
   words  were also dropped  after all these steps  we were left with total       features  or
stemmed words  
   multinomial naive bayes
the documents were vectorized and converted into sparse matrix  as in problem set     we
divided the data into a test set of     documents  and training data of different sizes       
                    documents  to run multinomial naive bayes 
one interesting aspect of this classification problem is that the document classes are soft 
and one document may belong to more than one class  although our training data says that
a document belongs to only one class  so it might be worthwhile predicting more than one
tag for a document and check if it matches with the tag in the test data  the good thing about
generative learning algorithms like naive bayes is that they build a class model out of training
data  and so we can output class probabilities for every class  thus we can output the most

filikely tag  the second most likely tag  the third most likely tag and so on 
so for experimentation  we compared the first and the second predicted tag against the tag in
training data  and plotted the error  as expected  the fraction of documents for which even the
second prediction is wrong is considerably lower than the fraction of documents for which the
first prediction is wrong  these errors are plotted against the training data size in the figure  
below 

figure    second prediction   best tag   next best tag combined for prediction and analysis

note that error for second prediction means that both the first and second prediction for the
tag of the test document did not match the actual tag of the document 
looking at the graph  we suspected that the model could be suffering from high variance
as training errors were going down as training size increased and also due to the large gap
between training and test errors  we tried the following techniques to improve the performance
by increasing the training examples and reduced the number of features 
improvements 
increased crawled documents 
we increased the crawled documents from      to       
improved feature selection 
we improved our parsing to remove features which had no value like spurious occurrences of

fijavascript and css  we built a blacklist of words we could safely drop and removed all features
which belonged to the blacklist  this blacklist was based on html tag names  reserved words
for javascript  css  we also used a whitelist of all valid english words from dictionary    and
included only those features which belonged this whitelist  we also dropped documents which
were not pure html like pdfs due to large amounts of meta data in these files   we computed
tf idf scores for words in each document and dropped all terms from a document with tf idf
scores below a certain threshold   we also dropped all features with very low frequency       
this reduced the size of our features from        to        we used multinomial naive bayes
to build a model based on the new improved feature set and increased the number of training
examples 
figure   below shows the the plot of test error as we increased the training set size 

figure  

though there is no perceived difference in the test error  it appears from this exercise  we got
rid of all features which were not contributing to any learning  as we keep increasing the training
size  the error rate decreases at a very low rate 
support vector machines 
we also experimented with svm based approach for multi label classification  we used
liblinear    for svm based classification  for svm training  we used the same improved feature
set used for naive bayes previously  we observed that the error rate improved  as excepted 
as we increased the training set size  we let the regularization parameter c default to liblinears
default value  we also set the n fold cross validation mode parameter v to   during svm

fitraining exercise  this is was done to reduce the risk of over fitting during training  see figure  
below for the plot of test error against training data size 

figure  

as a further improvement  we checked  if we could improve training error by tweaking the
regularization parameter c  in particular  we used   fold cross validation mode and varied
c from   to    for a particular training set size  we didnt observe much change in the
training error by varying c from   to     figure   below shows the plot of test error versus
the regularization parameter c  though our search best value of c was not exhaustive  this
exercise gave us quick insight into whether we could improve the model during training 

fifigure  

   conclusion
we tried to apply text classification techniques to auto tag delicious urls  we were intentionally
cavalier in picking urls  and didnt try to curate the url set to crawl predominantly rich  text
only pages  this and the fact that some of the documents could be labeled with more than  
unique tag  soft tagging  and noise in data  mis labeling  could be reasons for lower precision 
combined error rate of best and next best tag referred above as second prediction has a much
lower error rate as shown in naive bayes results above  and is encouraging 
further work 
 extend the model to include larger number of tags  may be       
 improve the html parser to discard useless data and javascript 
 feature selection  try feature selection techniques like filter feature selection using mutual
information 
 document selection  documents with very few tokens are likely to be misclassified  an
improved html parser might fix this also  
   references
   arvind narayanan  dataset of delicious com bookmarks with      million entries  http   
arvindn livejournal com        html 
   lingua stemmer  http   snowhare com utilities modules lingua stem 
   word dictionaries  http   infochimps com collections moby project word lists
   liblinear  http   www csie ntu edu tw  cjlin liblinear 

fi