joint supervised learning of ratings and rankings
rafael moreno ferrer  avinayan senthi velayutham  and ying wang
advisor  ramesh nallapati

abstract supervised recommender systems can be broadly
classified into  a  regression models that predict ratings of
unlabeled items and  b  ranking models that rank items
according to an order of interest  although both models assess
the value of an item  regression may be less precise than ranking
because the regressor does not learn a distinction between
two different training items with identical ratings  ranking
systems do not have this drawback because they assume perfect
ordering between all pairs of training items  however  ranking
requires the annotator to make judgments between all pairs of
items  which can be more time consuming than simply rating
each item independently 
in this work  we investigate the possibility of combining
rating and ranking systems  our goal is to assess whether
ratings prediction can be improved by supplementing a regression model with ranking information  we present two support
vector machine models that learn jointly from ratings and
rankings 

i  i ntroduction
rating and ranking are well known problems in machine
learning with widespread applications  rating is common in
services like the netflix movie recommender  while ranking
is essential in web search  in ratings prediction  a regression
model trains on a set of user annotated ratings  typically on
an integer scale of   to    and learns to predict absolute
ratings for unlabeled items  in ranking  the model trains on
a set of pairwise rankings annotated by the user and assigns
relative values to unlabeled items in order to place them in
an ordered list 
although these problems are related  they have complementary trade offs in terms of acquiring human annotation
for supervised learning  to acquire labeled data for ranking 
the human annotator needs to consider all pairs of items 
which is significantly more expensive than assigning a rating
to each individual item  conversely  ranking annotations
often provide fine grained information that ratings cannot 
because in rating systems with a small set of rating values 
many items will have identical ratings 
in this work  we go beyond the individual models and
formulate joint supervised models based on training data for
both rating and ranking  in doing so  we aim to improve upon
the coarseness of a ratings only model as well as investigate
the inherent similarities between regression and ranking 
we use a dataset of text documents scored for their
readability  described in section ii  in section iii  we define
the existing svm models for rating and ranking  then in
section iv  we present two models for learning jointly from
ranking and rating  and we discuss their results in section
v  we conclude the paper in section vi by listing the next
steps in this project 

ii  dataset
our data comes from the darpa machine readability
dataset  a set of     text documents whose readability levels
have been assessed by eight human expert annotators with
professional experience in linguistic analysis  each document
is labeled with a readability rating on a scale of   to   
and the documents are further divided into subsets of    or
fewer  over each of which an annotator ranked the member
documents  every annotator rates and ranks each of the    
documents exactly once  since the rating values assigned to
a given document vary according to an annotators tastes 
we considered every annotators data separately  for each
annotator  we split the     documents into a     document
training set and a     document test set  the training set
was further split into a     document development training
set and a    document development test set  the features
in our models were a set of    preselected nlp features
aimed at assessing readability  provided courtesy of ramesh
nallapati  
iii  d efinitions and n otation
we now define two existing svm models for regression
and ranking  these are the models we build upon in our
formulation of the joint models 
a  ratings model
our ratings model is an svm regression model with m
training examples    
m

minimize

s t 

x
 
 
 i   i  
kwk   c
 
i  



i
yi   hw   xi  i   b      i  

 hw   xi  i   b   yi     i  
i


i   i
   
i

where yi denotes the rating label for document xi   and  is a
tolerance parameter  in our tests  we set  to zero to penalize
any deviation from the true label on a training example  this
choice is makes the model more compatible with our later
error measurements using average absolute error 
b  ranking model
for ranking  we use ranking svm    
x
 
 
minimize
kwk   c
i j
 
pi j p

fi 
s t 

hw   xi  i  hw   xj  i
i j

    i j   pi j  p
    pi j  p

in this model  p is the set of all pairwise orderings from
the training set  for example if pi j  p   then
p a human has
ranked document xi higher than xj   hence  pi j p denotes
sum over all orderings in p  
iv  t wo models for joint learning
we now present two models for learning jointly from
ratings and rankings 
a  perturbation model
in the perturbation model  we work on breaking the tie that
exists between many documents that have the same rating 
for each group of n documents that have the same rating
r for a given annotator within a ranking subset  we apply a
perturbation in each documents rating based on the rank
ordering  the highest ranked document receives a rating of
r    the lowest receives a rating of r   and the rest of the
n    documents receive ratings evenly spaced between the
r    and r   based on their rank position  these adjusted
ratings then become the inputs into svm regression 
b  joint svm model
in the joint svm model  we combine the constraints of
svm regression and ranking svm 
m

minimize

x
 
 

kwk   creg
 reg i   reg i
 
 
i  
  crank

m
x

rank i j

pi j p



yi   hw   xi  i   b 





 hw 
 xi  i   b   yi


s t  reg i   reg i



hw   xi  i  hw   xj  i




rank i j

 reg i   i

 reg i
  i
   
i
    rank i j   pi j  p
    pi j  p

v  r esults and d iscussion
a  perturbation model
table i shows the results for the perturbation experiment 
the first row shows the baseline performance of an svm
regression algorithm for each of the eight annotators  by
baseline performance  we mean the test error obtained in
each annotator without any perturbation on the labels  the
second row contains the performance obtained by perturbing
the ratings labels based on ranking information and feeding
them into an svm regression algorithm  annotators marked
with an asterisk     are those whose results pass the statistical
significance test with a p threshold of      
the results were obtained by running svm regression on
the development set to obtain the cost parameter c with
minimum absolute error for each annotator  which gives
us the baseline for our experiment  next  we perform a
grid search along a combination of values for the cost

parameter c and the perturbation parameter   we report
the minimum absolute error obtained from the grid search
for each annotator  we use the c and  corresponding to this
minimum absolute error while applying the svm regression
model on the official test set  with     documents  and
report its absolute error  the svm regression model was
implemented using matlab and cvx software    
table i
p erturbation e xperiment p erformance i mprovements

baseline 
unperturbed
svm absolute
error
perturbed
svm model
absolute error
improvement

baseline 
unperturbed
svm absolute
error
perturbed
svm model
absolute error
improvement

ann    

ann    

ann    

ann    

      

      

      

      

      

      

      

      

     

    

      

    

ann    

ann    

ann    

ann    

      

      

      

      

      

      

      

      

      

      

     

     

as evidenced by the results in table i the improvements
are modest and not entirely consistent across annotators 
however  these results reveal that incorporating additional
information from rankings to the regression algorithm  done
by means of label perturbation in this experiment  can
improve the predictive ability of the ratings only model 
hence  our next step is to implement the joint svm model
which incorporates the ranking information as part of the
optimization constraints 
b  joint svm model
the table ii below shows the results of the joint svm
model for each of the eight annotators  the first row contains
the baseline performance on svm regression alone  and the
second row contains the performance on the joint model 
which includes      to      pairwise ranking constraints
per annotator  annotators marked with an asterisk     are
those whose results pass the statistical significance test with
a p threshold of       the baseline performance in table ii
differs from that in table i because of the different sets of
creg values used during the grid search 
to get these results  we first ran svm regression for each
annotator on the development set to select the regression cost
parameter c with the lowest absoluted errors  then  for each
annotator  we ran our svm joint model on the development
set with creg set equal to the c parameter we obtained from
the svm regression model  we tested values of crank from
 
the set  creg      creg      creg     
creg    and chose whichever
value of crank minimized test error  using these chosen
parameters  we ran svm regression on the official set to get

fitable ii
j oint m odel p erformance i mprovements

baseline 
svm
regression
absolute error
joint
svm
model
absolute
error
improvement

baseline 
svm
regression
absolute error
joint
svm
model
absolute
error
improvement

ann    

ann    

ann    

ann    

      

      

      

      

      

      

      

      

    

    

    

    

ann   

ann    

ann    

ann    

      

      

      

      

      

      

      

      

    

    

    

    

our baseline values and on the svm joint model to assess
the new models performance 
as table ii shows  adding ranking constraints to a regression model can indeed improve regression performance 
though the improvements are only by modest margins  in
our testing  we also tried to gauge the effect of the volume of ranking constraints by using smaller subsets of the
constraints available for each annotator  in one test  we
removed ranking constraints randomly  reducing the number
of constraints from the complete set of about       to     
and then     and      however  this decrease revealed no
clear pattern in the algorithms performance  most likely due
to the random nature of the selections  then  to determine
whether the quality of the constraints mattered  we tested
the joint model using only pairwise ranking constraints
for items with tied ratings  this reduced the number of
ranking constraints to about     per annotator  while this
setting achieved improvements over the baseline errors  the
improvements were only on the order of     percent  and half
of the annotators did not pass the statistical significance test 
these results suggest that it is better to use the entire set of
ranking constraints rather than a smaller  targeted subset 
while it is encouraging to find that regression performance
can improve with rankings data  we would still like to
see greater improvements and to investigate the relationship
between regression and ranking more thoroughly  there are
several other tests wed like to try  which we will discuss in
section vi on future work 
vi  f uture w ork
the objective of our work was to determine how valuable
rankings data can be in situations where the rating labels
are coarse  it may be possible to gain more insight into the
value of rankings by training with less ratings data  that is 
we would withhold some rating labels from our training set
while keeping all the ranking constraints in place  while in
a regression only model the performance might be expected

to decline with less ratings data  we hope to see that the
added rankings information in a joint model can counteract
that decline  we would especially expect to see a stronger
counteracting effect in the case where the data trained on
had more rating ties than the original data 
additionally  we may try adjusting the loss functions in
the optimization objective of the svm joint model or combining pairwise constraints with another regression model
altogether  in previous informal experiments  we combined
pairwise restrictions with regularized least squares regression
but did not achieve any favorable results  but perhaps with
more careful parameter selection we will have greater success  finally  a richer appreciation of the value of a joint
model could be provided by training and testing on different
datasets 
vii  acknowledgements
we would like to thank ramesh nallapati  research associate in the stanford natural language processing group  for
advising us and providing us with the machine readability
dataset  we would also like to acknowledge the stanford
nlp group for allowing us to use its javanlp software 
r eferences
    smola  a  j   and scholkopf  b           a tutorial on support vector
regression  neurocolt  technical report series 
    joachims  t           optimizing search engines using clickthrough
data  acm conference on knowledge discovery and data mining
 kdd  
    boyd  s   and grant  m         cvx  matlab software for disciplined
convex programming  software version       http   cvxr com cvx 
    ng  a          lecture notes from cs    machine learning course
 fall        department of computer science  stanford university 
ca 

fi