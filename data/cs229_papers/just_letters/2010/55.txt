 

using local steering kernels to detect people in
videos
arthur louis alaniz ii  christina marianne g  mantaring
department of electrical engineering  stanford university
 aalaniz  cmgmant   stanford edu

abstractlocally adaptive regression kernels  lark  can be
used as effective descriptors of generic objects  and have been
used in object detection algorithms that do not require prior
training      this paper demonstrates how these kernels can be
applied to the problem of people detection  taking into account
the fact that a person can appear in a wide variety of poses  and
can come in different shapes and sizes 

i  i ntroduction
eing able to automatically detect people in videos is
the first step in a wide variety of tracking applications 
which are in turn used in video surveillance systems  traffic
monitoring  and human animation  among others  one way
of doing people detection is through motion segmentation 
where moving objects are identified and separated from nonmoving ones  however  people may not necessarily be moving
all the time  moreover  in scenes with a lot of clutter  it can
be difficult to differentiate moving people from other moving
objects  like cars  another way of doing people detection
is through face detection  unfortunately  more often than
not  peoples faces in videos can be partially or completely
obscured  which leads to a decrease in the robustness of this
kind of system  finally  a third way of tackling this problem
is by using external sensors  such as those on motion capture
suits  still  this is only really practical for closed and controlled
environments  as you cannot expect every person to wear a
sensor 
one effective way of approaching this problem would be
to create a visual template of the person  and then use feature
matching algorithms to search for this template in a scene 
there has been a lot of recent work on doing visual recognition
using only a single query image  and this paper makes use of
the generic object detection algorithm proposed by seo and
milanfar     

zero mean iid noise values  and p is the total number of
pixels  if we assume that z   is locally smooth to some order
n   then one method for estimating the function at a point x
would be to expand it into the n term taylor series     

 
z   x  xi  x  
  
   n  
       
z  x  xi  x n
n 

z xi    z x    z  x  xi  x   

b

ii  p revious w ork
a  kernel regression
in kernel regression  regression functions are used to model
picture data  in particular  we can represent the measured pixel
data as


x i
yi   z xi     i   xi  
  i                 p
x i
where the yi s are the measured pixel data  xi s represent the
pixel coordinates  z   is our regression function  the i s are

 
 xi  x t 
  
 hz x    xi  x         
t

z xi         z x    xi  x   

h   hessian
since the hessian is symmetric  and using the halfvectorization operator  vech       this simplifies to    
z xi         t  xi  x  
 t vech  xi  x t  xi  x          
estimating the parameters  i   would now allow us to estimate our regression function 
since we assumed that the data was locally smooth  we need
to be able to weight the data such that pixels which are close
by have a larger weight than pixels which are far away  to
do this  we use kernel functions k        which can be any
function that achieves our desired penalty characteristics 
to solve for our parameters  we now have    
p

 i     arg

min x
 yi        t  xi  x  
 i   i  

 t vech  xi  x t  xi  x              kh  xi  x 
where
kh  z   

 
k h   z 
det h 
h  r   smoothing matrix 

b  locally adaptive kernel regression
in kernel regression  we made use of spatial differences to
weigh the input values  in locally adaptive kernel regression 
we not only make use of spatial differences  but also the
difference in data  pixel gradients   in particular  in steering

fi 

kernel regression      this is done by setting the smoothing
matrix hi to be
 
hi   hci 
where h is a global smoothing parameter and ci is the covariance matrix at the ith pixel  an estimate of this covariance
matrix can be obtained using the following formula     
  p
 
p
f
 x
 f
 x
 
f
 x
 f
 x
 
x
j
x
j
x
j
y
j
pxj wi
ci  pxj wi
xj wi fy  xj  fx  xj  
xj wi fy  xj  fy  xj  
where fx and fy are the derivatives along the x and y
directions  and wi is a window surrounding the pixel 
if we choose our kernel function to be a gaussian kernel 
then the local steering kernel  lsk  at a pixel xi will now be
given by
p
det ci  
 xi  x t ci  xi  x 
exp 
 
k xi  x  hi    
 h 
 h 
because the smoothing matrix is now a function of the local
pixel data  represented by the covariance matrix   this has the
effect of spreading the kernel along local edges      figure  
shows how a gaussian kernel adapts to the image data inside
the red box 

kq  xi  x  hi  
wq  xi  x    pp  
l   kq  xl  x  hl  
k j  xi  x  hi  
wtj  xi  x    pp   t j
l   kt  xl  x  hl  
let wq and wtj denote the collection of normalized lsks
for all the xi s in the query  and all the xi s in the j th
patch in the target  respectively  after normalization  principal
component analysis  pca      can then be used to reduce the
dimensionality  applying pca to wq and extracting the top
d eigenimages gives us the collection of eigenimages  fq  
and the projection space aq that was used to obtain these
eigenimages  we then project wtj onto aq to obtain ft  
which is a a collection of eigenimages for the target that are
in the same space as that of the query 
with these reduced descriptors  we can now compute the
similarity between two patches  for this  the cosine similarity
measure is used  the similarity of the j th patch in the target
to the query is given by
j   

ft
fq
 
 
kfq k kft k

from this measure  we generate the resemblance map by
calculating the resemblance
f  j    

figure   

steering kernel

 j
    j

for all patches in the target 
finally  significance tests and non maximum suppression
are applied to find the objects  first  the resemblance map is
thresholded by the overall threshold    ideally set to       see
    for details   to determine if there are any objects present
in the target  if no values are above     then no objects are
present  then  the resemblance map is thresholded by a second
threshold     which is extracted from the pdf of f  j   and
is set so that only    of the resemblance values are above
it  this gives a     confidence level in the produced data 
the last step is to apply non maximum suppression to find
the locations of the objects 

c  object detection using local steering kernels
local steering kernels represent the local structures in
images  and give us a measure of local pixel similarities 
given a query image q  target image t  overall threshold    
and a window size p     the generic object detection algorithm
proposed by     involves the following 
first  the lsks for the  grayscale  target and query images
must be computed  let these be denoted by kq  xi  x  hi  
and ktj  xi  x  hi    where the subscripts q and t denote the
lsks for the query and target  respectively  and the superscript
j denotes that the kernels were computed for the j th patch in
t that is the same size as q 
now  the lsks are too dense to use as effective descriptors 
and so the next step would be to reduce the dimensionality of these vectors  before this  however  we first need to
normalize our data  normalization of kq  xi  x  hi   and
ktj  xi  x  hi   is given by the following formulas     

iii  m ethodology and r esults
a  person detection using the generic object detection
framework
the first thing we did was to implement the generic object
detection algorithm that was proposed in     and discussed
in section      arbitrary query and target images were used 
and the main purpose of this first step was just a sanity
check as to how well the algorithm worked for detecting
people  figure   shows our query and target images  and the
intermediate results we obtained as we stepped through the
algorithm 

if our query and target images are the same size  then
only one resemblance value is produced  and the significance
tests can be replaced with a single comparison to some

fi 

queries in this manner would allow us to capture more generic
human features  the second method would be to use logistic
regression to properly determine the threshold that we needed
to use in order to classify patches as either a person or not a
person 
our proposed method for generating a query image is as
follows  given a training set composed of images of humans
and non humans  we first take the lsks of all the images of
humans  as well as the lsks of all these images mirrored
horizontally  mirroring removes the bias introduced by rightand left facing images  once we have obtained these kernels 
we average them out  averaging helps remove the background
details that we do not want to appear in our feature vectors 
and it also unifies the many different poses that are possible  in
addition  we average the kernels and not the images themselves
because the lsks have the nice property of being invariant to
noise and illumination changes 
after we have our query  we can now use it to do lark
object detection  using a second training set as the target
images  once we have our output resemblance values  we
use logistic regression on these resemblance values  instead of
thresholding   obtaining a training error for the second training
set 
figure   illustrates this methodology  the output of logistic
regression is a weight vector that we can use for testing 
to test  we simply take the generated query image  run
lark object detection  and then use the previous weights
to determine how to classify the output 

figure   

generic object detection using lark

predetermined threshold to determine whether the resemblance
value is good enough to classify the target  from hereon  we
will refer to this special case of the algorithm as lark object
detection 
we quickly realized that  because people came in a wide
variety of sizes  shapes  and forms  no single query image
could capture all the information that could be used to describe
a person  for instance  a person whose arms and feet were
spread out might end up looking more like a starfish than a
person who was standing straight  another thing we realized
was that  because of this variety of forms  the significance
tests that the paper had proposed would not hold for our case 
determining the thresholds to be used seemed like a hit andmiss process  and no value seemed to work for all images 
finally  a third realization was that pca extracted information
with the most variance  however  this did not necessarily mean
that it extracted the vectors that identified people  if the query
image had a background filled with detail  this information
was likely to appear in the eigenimages as well 
b  query image generation and threshold determination
to mitigate these effects  two methods were used  first 
instead of using a single query image  we proposed to train
query images from given pedestrian datasets  generating

figure    initial algorithm for generating a query image and performing
person detection

we used the daimler pedestrian classification database    
to do our training and testing  the database consists of      
unique images each of pedestrians and non pedestrians  and
each image is an   x   grayscale image  in addition  each
image is mirrored and shifted by a few pixels  to obtain       
total images each 
using this database  we created five subsets of images 
where each subset was composed of     unique images  we
ran our initial algorithm using two of these subsets  one to

fi 

produce the query image and one to train on  while varying
the different parameters  eg  window size  number of pca
dimensions  global smoothing value  etc   this allowed us
to obtain the values that would maximize the accuracy vs
precision value  where accuracy and precision are given by
accuracy  

true positives   true negatives
total training samples

precision  

true positives
total positives

the last parameter that we had to determine was the
bias parameter used in logistic regression  the accuracy vs 
precision curve generated by varying this bias parameter is
shown in figure   

however  it will be unable to identify more unique poses or
forms  what we do  then  is take all the positive images that
query   failed to classify in training set    and use these to
generate a second query image  this second query will now
have information that contains more details that the first query 
since it is averaging over a smaller subset of images  these two
queries are then used to perform classification on training set
   using two queries will produce two resemblance values  and
logistic regression is used to classify the outputs  again  the
misclassified positive images are used to create query    which
will contain even more details than the previous two queries 
logistic regression is used to determine the final weight vector 
which  along with the three generated queries  can now be used
for testing 

figure   
figure   

accuracy and precision vs varying bias

accuracy vs precision for varying bias values

c  chained query image generation
despite having a more general query image  we found that
our results  although better than before  were still unsatisfying 
thus  we proposed to chain our initial algorithm  creating
multiple query images out of the positive images that our
classifier failed to classify  and then using logistic regression
on the multiple queries  figure   shows a version of this
iterative algorithm that has three stages 

figure   shows the accuracy and precision for obtained after
testing with       and   stages  vs the bias value used in logistic
regression  where all instances of logistic regression use the
same bias   note that after bias        both the accuracy and
precision suddenly drop  this is due to the fact that the bias
value used in stage   greatly affects the output of the entire
algorithm  in our case  making the bias value higher that     
suddenly prevented a whole bunch of positive images from
being classified  as a results  subsequent query images were
more generic than we wanted  which made the performance
of the algorithm suffer 
holding the bias for stage   constant at       we then
repeated the same experiment  and obtained the plots shown
in figure    note that these look much smoother than the
previous plots obtained 
after consolidating this information into an accuracy vs
precision plot  we obtain figure   
clearly  as we increase the number of stages  our overall
test accuracy and precision increases  our final figures after
running the   stage algorithm were an accuracy of around    
and a precision of around     
d  computational complexity

figure   

  stage algorithm for query generation and person detection

the query generated in the first stage is meant to capture the
most generic information that can be used to identify persons 

from the previous section  we showed that increasing the
number of stages increases the performance of the system 
unfortunately  this also results in increasing the number of
computations needed  which makes the algorithm run much
slower 

fi 

v  f uture w ork

figure    accuracy and precision vs varying bias  with first stage bias fixed
at     

as of now  our algorithm is a classifier that can determine
whether image patches are human or non human  to extend
this so that we can detect people in bigger target images  we
simply need to classify overlapping patches in the target as
human or non human  and then run non maximal suppression
to remove redundant detections 
another line of work would be to extend our algorithm to
be scale and rotation invariant  the former can be done by
building an image pyramid consisting of target images that
have been upsampled and downsampled  and then running the
detection algorithm on the entire pyramid  then  to make it
rotation invariant  we can rotate the query image by varying
degrees  and then run the detection algorithm  note that since
we are detecting people  it is highly unlikely to find rotated
people  except maybe if a person is lying down  as compared
to standing up   so rotation invariance isnt as important as
scale invariance 
finally  since we want to be doing this for video  then we
can add a tracking algorithm so that we do not have to redetect
and reclassify patches for every frame  once a person has been
detected  then we can use a method like optical flow to track
that person in the scene 
r eferences

figure   

final accuracy vs precision plot

one disadvantage to using lark as our base framework is
that it is slow  the main bottleneck in our algorithm was in
computing the lsks  which involved several linear filters  as
well as matrix multiplications 
another bottleneck was memory  to generate the query
images  we had to store the kernels of all positive training
samples in memory so that they could be averaged later on
 we found that the optimal window size to use was  x   and so
each pixel would get converted into an    dimension vector  
thus  the number of training samples per stage was bounded
by the number of kernels which we could store in memory 
increasing the number of samples beyond a certain number
resulted in a much slower algorithm 

iv  c onclusion
using a training free generic object detection framework to
detect persons is hard  because people come in many different
forms  shapes  and sizes  thus  no single query image can
accurately describe each and every person  however  we can
generate multiple query images from training sets  and then
use logistic regression to unify the results produced by these
multiple queries  we showed that chaining our initial algorithm
increased both accuracy and precision  and adding more stages
to this chain resulted in better performance for our classifier 

    hae jong seo  peyman milanfar   training free  generic object detection using locally adaptive regression kernels   ieee transactions on
pattern analysis and machine intelligence  vol      no     pp            
aug        doi         tpami         
    h  takeda  s  farsiu  and p  milanfar  kernel regression for image
processing and reconstruction  ieee transactions on image processing 
vol      no     pp           feb       
    d  ruppert and m  p  wand  multivariate locally weighted least squares
regression  the annals of statistics  vol      no     pp           
september     
    m  p  wand and m  c  jones  kernel smoothing  ser  monographs on
statistics and applied probability  london  new york  chapman and hall 
     
    jolliffe  i  t          principal component analysis  springer verlag  pp 
     doi         b       isbn                  
    s  munder and d  m  gavrila  an experimental study on pedestrian
classification  ieee transactions on pattern analysis and machine
intelligence  vol      no      pp            nov       
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin 
liblinear  a library for large linear classification  journal of machine
learning research                    
    hae jong seo  milanfar  p      using local regression kernels for statistical object detection   image processing        icip         th ieee
international conference on   vol   no   pp                  oct      
doi          icip             

fi