predicting magazine sales using machine learning
mei ling  zan  chu  freeman fan  yisha peng
abstract
in this project  we apply machine learning techniques to a real world problem of predicting magazine sales 
i e  the number of magazine copies to be placed at newly opened newsstand locations using past data
gathered from existing stores  given the raw data from hearst corporation regarding store sales  store
locations  demographics and other related facts  we designed a nonlinear multi class svm classifier to
predict the amount of magazine sales at newly opened stores  a c svc model with radial basis function
 rbf  kernel is built for analyzing this highly heterogeneous set of data  cross validation via grid search is
applied for parameter tuning  in view of the nature of our problem  root mean square error  rmse  is used
to measure the prediction accuracy  the theoretical framework and experimental results for this problem are
discussed in our article 
    introduction
   
problem statement
in this project  we attempt to solve an important
problem faced by magazine publishers today 
which if successful  can dramatically help
publishers maximize their profit potential  we are
provided with over   gb of data by hearst
corporation  containing            store sales
observations with a total of     features  as well
as other related demographics data for predicting
the sales in all of the newly opened stores  we
narrow down our scope to stores located in
california because we would like to focus on
applying machine learning techniques to find an
excellent model rather than tackling the
computational difficulties arising from using the
entire dataset  after this filtering process  we
obtain approximately        sales records from
the past   years  we would like to predict the sales
of individual magazine issues at     newlyopened stores  with a high accuracy in our
resulting prediction  we can estimate precisely the
number of magazine copies to be placed at these
stores according to the predicted demand  this
information can be used in distributing magazines
to stores so that both under stock  lost sales  and
over stock  cost of unsold copies  can be
minimized 
sales prediction can be transformed to a multiclass classification task  in the following sections 
we make use of a nonlinear multi class support
vector machine to solve our problem 

   

data description

all data is stored in   database tables  while the
main table is the sales table where four
variables in each record of sales are used as
foreign keys to locate related records in the other
tables  namely issues  wholesalers 
stores and zip   

zip   table
s no variable name
  zip code
  zip  
  households
  individuals
  occupation   professional technical
  occupation   sales service
  occupation   farm related
  occupation   blue collar
  occupation   other

       

store chain table
s no variable
  city
  ctry
  chain key
  zip code
  zip  
  state
  chain or cot desc
  store key
  households
   individuals
   occupation prof technical
   occupation sales service

s no
 
 
 
 
 
 
 
 
 

sales table
variable
chain key
issue key
store key
title key
wholesaler key
dollar volume
draw
returns
sales

s no 
 
 
 
 
 
 

issue table
variable
can cov prc
issue key
iss cd
on sale date
off sale date
us cov prc

wholesaler table
s no variable
 
city

   occupation farm related

 

wholesaler key

   occupation blue collar

 

st

   occupation other

       

figure   relational diagram of the dataset
    methodology
   
c support vector classification  c svc 
      two  class c support vector
classification
given training vectors xi  rn   i         l   in two
classes  and a vector y  r l such that yi         
c svc  boser et al         cortes and vapnik 
      solves the following primal problem 

fil
  t
w w  c  i
 
i  

min
w  b  

subject to yi   w t    x i    b      i  

min

wij  bij  ij

subject to   w ij  t   x t    bij     tij   if x t in the ith class 
  w ij  t   x t    bij     tij   if x t in the jth class 

i     i         l 
its dual is

  t
 q  et 
 
subject to y t     
   i  c   i         l  
min


where e is the vector of all ones  c    is the
upper bound  q is an l by l positive semi definite
matrix  where qij  yi y j k  xi   x j     and the kernel
is k  xi   x j      xi  t   x j     here training vectors

x i are mapped into a higher  maybe infinite 
dimensional space by the function   
the decision function is
l

sgn   yii k  xi   x   b  
i  

the four common kernels are 
linear  k  xi   x j    xi t x j  
polynomial  k  xi   x j      xi t x j  r d       
radial basis function  rbf  

k  xi   x j    exp      xi  x j            
sigmoid  k  xi   x j    tanh  xi t x j  r   
     

multi class c support vector
classification

we use the one against one approach  knerr et
al         in which k  k         classifiers are
constructed and each classifier trains data from
two different classes  for training data from the ith
th

and j classes  we solve the following twoclassification problem 

  ij t ij
  w   w  c      ij  t  
 
t

tij    

in classification  we use a voting strategy  each
binary classification is considered to be a voting
where votes can be cast for all data points x   in
the end  each point is designated to be in a class
with maximum number of votes 
in cases where two classes have identical votes 
though it may not be a good strategy  we simply
select the one with the smallest index 
there are other methods for multi class
classification  some reasons for why we choose
this one against one approach and detailed
comparisons are in hsu and lin        
   

model selection

four common kernels are mentioned in the
previous section  therefore  we must decide
which one to try first  the penalty parameter c
and kernel parameters are then chosen according
to the selected kernel 
     

rbf kernel

in general  the rbf kernel is a reasonable first
choice  this kernel nonlinearly maps samples into
a higher dimensional space so that  unlike the
linear kernel  it can handle the case when the
relation between class labels and attributes is
nonlinear  furthermore  the linear kernel is a
special case of rbf  keerthi and lin        since
the linear kernel with a penalty parameter c has
the same performance as the rbf kernel with
some parameters  c       in addition  the sigmoid
kernel behaves like the rbf kernel for certain
parameters  lin and lin        
the second reason for choosing the rbf kernel is
due to the number of hyperparameters which
influences the complexity of model selection  the
polynomial kernel has more hyperparameters than
the rbf kernel 

fifinally  the rbf kernel has fewer numerical
difficulties 

e g  walking along a path  which can be hard to
parallelize 

     

for easy implementation  we consider each svm
with parameters  c     as an independent problem 

cross validation and grid search

there are two parameters for c svc with an rbf
kernel  c and    it is not known beforehand which
c and  are best for a given problem 
consequently some kind of model selection
 parameter search  must be done  the goal is to
identify good  c     so that the classifier can
accurately predict unknown data  i e  testing data  
we select our parameters  c     using cross 

as they are different jobs  we can easily solve
them in parallel  note that now under the same  c 
    the one against one method is used for
training multi class data  hence  in the final model 
all k  k         decision functions share the same
 c     

validation via parallel grid search 
for medium sized problems  cross validation
might be the most reliable way for parameter
selection  first  training data is separated into
several folds  sequentially a fold is considered as
the validation set and the rest are for training  the
average accuracy of predictions on the validation
sets is the cross validation accuracy 
our implementation is as follows  first  we
provide a possible interval of  c     with the grid
space  e g  c                                         
then  all grid points of  c     are tried to see
which one gives the highest cross validation
accuracy  finally  we use the best parameters to
train the whole training set and generate the final
model 
the grid search is straightforward and seemingly
nave  in fact  there are several advanced methods
which can save computational cost by  for
example  approximating the cross validation rate 
however  there are two motivations why we
prefer the simple grid search approach  one is that 
psychologically  we may not rest assured using
methods which avoid doing an exhaustive
parameter search by approximations or heuristics 
the other reason is that the computational time
required to find good parameters by grid search is
not much more than that required by advanced
methods since there are only two parameters 
furthermore  the grid search can be easily
parallelized because each  c     is independent 
many advanced methods are iterative processes 

figure   contour of cross validation accuracy via
grid search
    numerical experiments
this section describes the main procedures and
experiments we carried out in implementing our
nonlinear multi class c svc model and gridsearch method described above using the
libsvm package  source codes are available at
https   afs stanford edu download  path  afs ir user
s f w fwf public cs    magsales project zip 

   
     

data preprocessing
data formatting and feature extraction

the original dataset is given in csv format  we
parse the dataset and import it into mysql using
a ruby script in order to facilitate data
manipulation in later stages  in order to better
understand the machine learning techniques in
each of the trial models  we choose to confine our
scope of training and testing to the stores in
california only  observations and attributes are
extracted from the mysql database  in total 
there are        training data and     testing data
after filtering and extraction  the extracted dataset
is further transformed by our java program in
order to comply with the libsvm input format 

fi     

imputation of missing data

we tried two different ways to impute missing
data  replace the missing data with average values
of existing data or delete the entire features or
observations  among these two methods  deleting
the features or observations with missing data
gives better testing results  therefore  we set the
following two rules in our deletion process 
   if a feature contains more than     zero
value entries  the feature is regarded as
uninformative and removed 
   if an observation contains more than    
zero value attributes  the observation is
regarded as incomplete and removed 
     

scaling of data

the values of our features range from    to
        to avoid domination of features with
large numerical range and to reduce
computational difficulties  we scale each feature
to the range        for both training and testing
datasets 
     

labeling of classes and categorization of
features

this sales prediction problem can be considered
as a multi class classification problem  due to the
highly unbalanced training dataset label  e g 
approximately     of   labels     of    
label   we classify the labels into    classes 
namely                         
and      this varying bin width labeling
method would yield good prediction result from
the originally unbalanced labeled dataset 
furthermore  in order to use svm  we have to
transform some of the non numerical attributes
into m category numerical attributes  for example 
label   is used to represent san francisco   
to represent palo alto  and so on  in addition 
some of the attributes require further
interpretation and manipulation in order to be
effectively used in svm  for example  the issue
code entry         which means issue     
      is split into two features      and    

   

model selection results   parameter tuning
with cross validation

using       training data to carry out multiple
experiments for the svc parameters tuning  we
compare the effect of parameters tuning on
prediction accuracy on the     testing data 
without
with
parameters
parameters
tuning
tuning
c
 
     

    
          
cross val  accuracy
       
        
prediction accuracy
       
       
rmse 
       
      
table   comparison of performance on
parameters tuning
  this result surpasses the first place entry on hearstchallenge com 
a past machine learning competition having the same task and
same dataset as ours 
  in sales prediction task  root mean square error  rmse  is a
more effective measure than accuracy  since it measures how close
the predicted sales are compared to the true sales  this is more
important than finding how many sales are predicted exactly
correct  which is measured by accuracy 

the results indicate that parameter tuning is
essential to making accurate predictions 
we found that parameter tuning must be
performed for each training dataset  as well as for
each set of features  a set of parameters that
works well on one dataset cannot be assumed to
work well on another dataset 
   

different sizes of training datasets

we use the optimized parameters  c       
gamma           
obtained
from
our
parameters tuning procedures for training on two
datasets of different sizes 
    
     
number of
training data
       
       
accuracy
      
      
rmse
table   comparison of performance with different
training dataset sizes
a larger training set gives us better prediction
accuracy 

fi   

using different sets of features

using      training data      testing data  and
optimized parameters after tuning  we compare
the effect of different sets of features on
prediction accuracy  we manually select   
features which we believe might be more relevant
to predicting sales  e g  income  mean value of
housing  etc   
   selected
    total
feature set
feature set
        
       
accuracy
      
      
rmse
table   comparison of performance using
different sets of features
the results indicate that using all the given
features increases prediction accuracy 
   

scaling  un scaling of data

using      training data and     testing data  we
compare the effect of scaling feature values into
the range of        on testing accuracy 
with scaled
with unfeatures
scaled features
       
       
accuracy
      
      
rmse
table   comparison of performance on features
scaling
the result indicates that data should indeed be
scaled 
    discussions and conclusions
in some situations the procedure outlined above is
not good enough  and other techniques such as
feature selection may be needed  our experience
indicates that our procedure works well for data
which do not have many features  if there are
thousands of features  there may be a need to
choose a subset of them before giving the data to
the svm  in our problem  we only have    
features and our procedure works well  this
conclusion is verified in the section of numerical
experiments  where we compare our prediction
accuracy based on     features with that based on
   features 

    future work
for this project we select a random subset from
the entire data provided  perform parameter tuning
on this subset  and train our model on this subset
as well  to further improve accuracy  we envision
using the same random subset to carry out
parameter tuning  possibly better region only
grid search on the complete dataset  and then train
our model on the entire dataset  or possibly
parallelizing the process to reduce computation
time 
    acknowledgements
we would like to acknowledge professor andrew
ng and the ta team of cs     for their advice on
the project  we would also like to thank the
developers of libsvm for their useful toolkit and
hard work 
    references
b  e  boser  i  guyon  and v  vapnik  a training algorithm for
optimal margin classifiers  in proceedings of the fifth annual
workshop on computational learning theory  pages         
acm press       
chih chung chang and chih jen lin  libsvm   a library for
support vector machines        software available at
http   www csie ntu edu tw  cjlin libsvm 
c  cortes and v  vapnik  support vector network  machine
learning                   
c  w  hsu  c  c  chang  and c  j  lin  a practical guide to
support vector classification  department of computer science 
national taiwan university       
c  w  hsu and c  j  lin  a comparison of methods for multi class
support vector machines  ieee transactions on neural networks 
                    
s  s  keerthi and c  j  lin  asymptotic behaviors of support vector
machines with gaussian kernel  neural computation                       
s  knerr  l  personnaz  and g  dreyfus  single layer learning
revisited  a stepwise procedure for building and training a neural
network  in j  fogelman  editor  neurocomputing  algorithms 
architectures and applications  springer verlag       
h  t  lin and c  j  lin  a study on sigmoid kernels for svm and
the training of non psd kernels by smo type methods  technical
report  department of computer science  national taiwan
university       

fi