topic modeling in financial documents
patrick grafe
department of computer
science
stanford university

pgrafe stanford edu

abstract
this paper describes the application of topic modeling techniques to quarterly earnings call transcripts of publicly traded
companies  earnings call transcripts represent an interesting
case for analysis because the document is relatively unstructured and potentially more informative than   k and   q
disclosures due to the question and answer session consisting
of unprepared statements  this paper addresses the clustering of these documents as well as the segmentation of individual documents into clusters for products and industries
the company is active in  the goal is for each transcript
to be assigned to some number of topics  and the specific
segments of the transcript which address a given topic to
be specified as well  thus  not only will the documents be
classified as covering some set of topics  but the documents
themselves will be partitioned into different sub topics  i
will discuss progress i made in achieving these goals as well
as challenges and issues which remain  this work could
prove useful in financial document summarization as well as
improving search and display of documents and information
relevant to a users search and interests  furtheremore  applying nlp and machine learning concepts to financial document analysis is increasingly being used by trading firms
and hedge funds to gain competitive advantage 

documents and serving users appropriate documents including those that  while not about a specific company  may be
highly relevant due to a shared industry or product  the
following sections discuss all major aspects of this task including  data gathering  preprocessing  segmentation  clustering  and analysis 

     earnings call transcripts
petrowhawk energy q      
devon energy q      
emergent biosolutions inc  q      
biovail corp q      
medtronic q      
prudential financial q      
cardiovascular systems inc q      


preprocessing
remove stop words  use porters stemming algorithm 

vector space model

keywords

remove rare and common words from the dictionary 

latent dirichlet allocation  clustering

  

introduction

the goal of this project is to effectively discover common
topics among a large data set of earnings call transcripts
of publicly traded companies  each transcript will be assigned to some number of topics  and the specific segments
of the transcript which address a given topic will hopefully
be specified as well  thus  not only will the documents be
classified as covering some set of topics  but the documents
themselves will be partitioned into different sub topics  this
work would be useful in improving the search of financial

latent dirichlet allocation model

  

data preprocessing

the data set was gathered by scraping      earnings call
transcripts from seekingalpha com  these transcripts represent approximately one years worth of financial data  the
transcripts were first stripped of html markup while preserving all other punctuation  character case  and stop words 
the data was then tokenized using the treebank word tokenizer provided by the natural language toolkit  nltk 
python package  commonly used words in the english language  known as stop words  are also subsequently removed
in order to reduce the number of features as well as to
prevent clustering from being affected by such content free

fitable    topics
oil and biotech
real es  media
natural
tate
and netgas
working
gas
patient
occupants network
drill
trial
tenant
brand
rig
clinic
hotel
software
barrel
fda
revpar
wireless
acreage
dose
music
tv
haynesville cancer
noi
video

misc

gas
scrap
mario
russo
glenrock
gasoline

words  finally  to further reduce and improve the feature
set  the tokens were stemmed using nltks implementation
of porters stemming algorithm so that words with common
roots would now be counted as the same 

performing lda across the entire data set  this allows a
company such as halliburton which works in multiple industries  oil and natural gas  construction  military work 
to be correctly grouped into each of these three industries
more effectively 
the first step was to split each document into its constituent
paragraphs  for earnings call transcripts  there were many
paragraphs which consist of simple greetings  introductions  and relatively content free one line questions  some
of the one line paragraphs represent questions  which should
properly be grouped with their associated answers  but for
simplicity  i chose to remove these paragraphs from my analysis  thus any paragraphs with fewer than     characters
were removed 

  
   

vector space model

initially  i represented the data set using unigram  bigram 
and trigram counts  however  after confronting severe memory limitations  i opted to use a vector space model using
simply unigrams which would likely be similarly effective
and considerably more memory efficient  thus each word
encountered in the documents are stored in a dictionary 
and the id of those words are only included in a given bag of
words model if the document itself contains that word  the
remainder of my work uses this bag of words vector space
model  i considered making use of bigrams in my models 
however  from prior knowledge about the relative ineffectiveness of bigrams in sentiment analysis i decided not to do
so 

  

clustering using lda

originally  i intended to make use of k means to cluster the
data  however  the limitations of k means especially with regard to its inability to maintain multiple topic distributions
for each document led me to try latent dirichlet allocation
instead  initial analysis using lda proved unsuccessful with
this data set because the clusters formed along the lines of
common words found in a majority of the documents with
topic salience matrices lead by words such as million  income  growth  etc  i removed stop words in hopes of
avoiding such a situation  however  it became clear i also
needed to remove words common to my data set specifically 
thus in all future analysis  words that appeared in a very
large percentage of the documents  or in the next section 
segments  were removed  similarly words found in very few
documents were also removed as well  the thresholds used
were arbitrary  removing words that appear in more than
   percent of the documents as well as those that occurred
in fewer than   percent of the documents  these thresholds
represent significant tuning parameters for the clustering algorithm which i will discuss later in my analysis  some
results using this analysis are shown in table   

  

intra document clustering

given the fact that most companies operate in a limited
number of industries  i performed clustering in this step with
k      the results were somewhat disappointing for this
intra document clustering  unlike with my previous analyses  words such as million  income  and revenue were
not removed because a majority of the segments made no
mention of them  as a result  the intra document clustering
resulted in one or more clusters of very basic accounting terminology common to all corporations  on the positive side 
for many companies there were one or two correctly identified clusters clustering on industry specific terms  overall 
for this data set  i do not believe this method holds much
promise because a review of the companies involved shows
that most do not have a clear presence in multiple industries
and thus clustering within those documents is futile 

document segmentation

to deal appropriately with companies involved in multiple
industries  i used an approach described by tagarelli and
karypis     which involved splitting each individual document into paragraphs and then proceeding to cluster the
paragraphs into segments within the document  these clusters are then used in lieu of the complete documents when

  

clustering document segments
paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

paragraph  

lda

lda

lda

cluster  

cluster  

cluster  

cluster  

cluster  

cluster  

cluster  

cluster  

cluster  

lda

fionce the clustering within each document was complete 
these segment clusters for the entire data set were further
clustered using values of k     and k       given that
the majority of companies operate in one primary industry  no significant changes were observed when clustering
segments as compared to documents  the only significant
change was that since each document always contained a
cluster on accounting and finance terms  there was a clearly
defined cluster on these terms in the final set of clusters  the
following tag clouds are two representative examples of the
results seen during this analysis  one is a cluster of networking companies and the other is the cluster on accounting
concepts  graphics courtesy of wordle net  

seen was clustering around words which arent indicative of
any particular industry  product  or place  i attempted to
deal with these problems by tweaking the thresholds i had
previously used to remove words common to most documents and words that are very rare  but unfortunately doing
so only made things worse  even for k     

  

  

  

tuning parameters

since my dataset is currently unlabeled  choosing k as well
as some other previously mentioned tuning parameters was
a dificult and imprecise task  the choice of k depends on
how ambitious i wish to be with clustering the data  clustering for k     worked surprisingly well with four very well
clustered industries  biotech  energy  real estate  and technology  and one miscellaneous category  for k       the
resulting clusters were considerably less homogenous than
for k      there were more incorrect classifications and
some incorrect mixing of some categories  however  with
these drawbacks also came some improvements  the new
clusters covered more narrow industries  most prominently 
there were now two clusters related to real estate  one of
which focused on hotels and rental properties  another cluster was found which was mostly representative of the semiconductor industry  choosing values of k greater than   
was not as successful  most clusters then fell under the category of miscellaneous  while some clearly defined clusters
such as oil and natural gas remained 
one of the problems seen at values of k greater than     was
that clusters would form around proper names and places 
such as john  needham  and mario  another problem

conclusions

the results obtained in this project have been mixed at best 
it is clear that these techniques hold potential to correctly
group companies into their respective industries and special
areas  the energy  biotech  real estate  and technology industries are very clearly defined and grouped  however  for
every successful clustering  theres a disappointment  for
all values of k analyzed  including k      there was always
at least one cluster that can at best be described as miscellaneous  this may be acceptable at times  for example 
its probably not possible to create a reasonable way to split
the data set into a mere   categories  unfortunately  since
theres no automated way to determine which of the clusters
should be classified as miscellaneous  this makes the usefulness of these techniques questionable  these techniques also
have some potential to cluster well on an intra document
level as well  but again the same caveats apply  furthermore  theres a lot of noise in the data when examining at
the segment level  fortunately  for intra document segment
clustering  if the words common among all documents  ie
accounting jargon  boilerplate  etc  are removed as a preprocessing step  then very different and likely more meaningful
results will be found  overall  despite my belief that this
would be a useful data set to analyze  it has proven to be
more difficult than expected 

future work

in the future  i will need some specific metric to compare
the quality of clustering obtained using different values of k 
for my current analysis  i experimented with the following
values of k                 unfortunately  since i am working
with an unlabeled data set and with an ambiguous objective 
its difficult to quantify success and what is the optimal value
of k  in the future  i will need to create a small labeled
test set  which will allow me to quantify my success and
provide a metric  namely perplexity and classification error 
to determine the optimal value of k 
additionally  for values of k larger than     there is an interesting phenomenon where proper names appear to be of
great importance to topic discovery  this may help improve
topic modeling for the training set because some names are
indeed commonly seen only in a specific industry  for example  one investment capital corporation  needham capital
partners  invests heavily in semiconductor and other high
tech corporations  consequently  needham capital representatives attended and asked questions at teleconferences
for such companies  as a result  needham was often highly
indicative of a semiconductor or high tech grouping  this
may cause problems in generalization however  especially for
more common proper names such as john 
for this work to be useful it will be important to be able
to determine which clusters are reasonable and appropriate
and which consist of all thats left over just thrown into
one  for the case of intra document clustering  this seems

fipossible to do either in an intelligent manner or in a brute
force manner  ie by eliminating a cluster automatically based
on the prominence of accounting terminology in its term
salience matrix 
finally  i originally intended to try out the correlated topic
model proposed by blei  et al      this topic model extends
lda with the observation that topics are not independent 
thus allowing correlations  due to time constraints  i was
unable to implement this  but i feel it may be useful in the
future 

   

references

    a  tagarelli  g  karypis  a segment based approach to
clustering multi topic documents       
    d  blei  j  lafferty  correlated topic models 

fi