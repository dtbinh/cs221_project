identifying  keywords  in  random  texts  
ibrahim  alabdulmohsin  
gokul  gunasekaran  
  
december            
  
abstract

the  subject  of  how  to  identify  keywords  in  random  texts  lies  at  the  heart  of  many  important  
applications   such   as   document   retrieval    bibliographic   databases    and   search   engines    it   has  
received   a   fair   interest   in   the   research   community   ever   since   the   late       s    where   most  
approaches  to date  have  mainly  relied  on  frequency  analysis  and  word  association   in  this  
project    we   present   a   different   approach   to   achieve   such   objective   by   using   machine  
learning  algorithms  such  as  support  vector  machines   svm   and  gda   both  with  uncertain  
label  classification   while  such  approach  uses  minimal  linguistic  tools  and  relies  instead  on  
pure  numeric  features   which  might  seem  at  first  to  be  at  odds  with  the  original  objective   
the   motivation   behind   the   adoption   of   such   machine   learning   algorithms   is   driven   by  
similar   successful   applications   in   various   other   disciplines   such   as   optical   character  
recognition   both  svm  and  gda  achieve  a  very  good  performance  in  keyword  identification   
where  gda  usually  performs  much  better  than  svm     

  

   introduction  

automatic  keyword  identification  can  be  informally  described  as  a  process  by  which  
a   short   list   of   keywords   is   extracted   out   of   a   much   larger   text   with   little   loss   of   information   
it  is  used  to  provide  an  efficient  method  for  both  humans  and  machines  to  quickly  identify  
the   content   and   type   of   texts   and   documents    such   process   of   automatically   identifying  
keywords  in  random  texts  using  computer aided  tools  and  algorithms  is  a  crucial  task  in  the  
modern  information  era   it  lies  at  the  heart  of  many  important  applications  such  as  indexing  
in  bibliographic  databases  and  internet  search  engines  as  well  as  classification  in  directory  
services    however    because   a   measure   for   information   loss   clearly   differs   from   one  
application   to   another    a   generic   customizable   approach   is   also   needed   that   can   have   a  
wider  impact  and  applicability   
the   subject   of   keyword   identification   has   received   a   fair   interest   in   the   research  
community   ever   since   the   early   dawn   of   computers   in   the       s       probably    one   of   the  
earliest  attempts  to  tackle  a  related  subject  was  an  experimental  inquiry  conducted  by  m   e   
maron   in           which   demonstrated   an   efficient   use   of   bayesian   analysis   to   categorize  
documents   based   on   their   clue   words          in   a   different   study    bayesian   networks   were  
used  to  assist  in  indexing  articles  through  the  use  of  a  thesaurus   because  the  identification  
method   was   hard coded    no   training   was   required   in   such   study          furthermore    many  
sophisticated   techniques   have   been   developed   based   on   frequency   analysis   that   range   from  
the  vey  simple  methods   such  as  the  use  of  middle  frequency  words   to  advanced  methods  
such  as  tf idf   which  takes  into  account  the  frequency  of  a  word  across  multiple  documents  
      in  addition   frequency  analysis  has  also  been  supplemented  by  the  use  of  linguistic  tools  
such  as  stop  words  and  stemmers        the  use  of  frequency  analysis  and  linguistic  tools  is  
probably  the  most  pervasive  approach  nowadays   
support  vector  machines   svm   is  a  learning  algorithm  developed  by  vapnik  and  his  
colleagues  in  the  early      s  as  a  method  for  obtaining  optimal  margin  classifiers  in  high 
dimensional  feature  space           its  immense  success  and  popularity  has  made  it  one  of  the  
most  important  supervised  learning  algorithms  in  use  today   with  regard  to  our  application   
svm   enjoys   key   advantages   that   make   it   very   suitable   for   automatic   keyword   identification   

fifor   instance    because   svm   works   purely   on   numeric   features   with   no   regard   to   their  
underlying   principles    such   as   language   grammars    it   is   highly   extensible   to   multiple  
languages   which  is  ideal  for  our  application   in  addition   svm  is  easily  customizable  to  the  
needs   of   a   particular   application    for   instance    while   keyword   identification   might   be  
needed  in  some  cases  to  point  to  the  general  nature  of  an  article  as  in  directory  services   it  
may  also  be  needed  in  other  applications  to  indicate  both  nature  and  contents  of  that  article  
such  as  in  search  engines   using  svm   the  same  algorithm  is  used  by  all  applications   where  
only   the   method   of   labeling classifying   words   during   training   would   differ    despite   such  
potential    we   are   not   aware   of   any   previous   attempts   to   use   svm   in   automatic   keyword  
identification   and  we  hope  that  this  project  will  shed  some  light  into  this  topic   
gaussian   discriminant   analysis    gda    is   a   generative   learning   algorithm   that  
attempts  to  use  the  training  set  to   create   models  for  classes  independently   as  opposed  to  
discriminative   learning   algorithms   that   merely   attempt   to   separate   classes          in   our  
application   gda  enjoys  three  key  advantages   first   in  nearly  all  of  the  features  being  used   
such   as   word   frequency    the   probability   of   observing   a   keyword   increases   as   the   feature  
value   increases    for   example    when   stop   words   are   removed    remaining   words   with   high  
frequencies  are  more  likely  to  be  keywords  within  the  text   thus   the  probability  mass  of  the  
two   classes   should   be   nearly   separable   in   rn   using   multivariate   gaussian   densities    where  
the   probability   mass   of   the   negative   class   lies   closer   to   the   origin    also    because   gda   is   a  
generative   learning   algorithm    it   can   make   full   use   of   the   training   set   by   attempting   to   build  
a   model   of   each   class   independently   of   the   others    whereas   discriminative   learning  
algorithms  such  as  svm  tend  to  perform  poorly  on  highly  unbalanced  training  sets   in  our  
application    keywords   constitute   less   than        of   the   entire   training   sets    so   balancing   the  
set  in  svm  means  removing  more  than       of  the  data   a  clear  disadvantage   third   gda  is  
a   simple   learning   algorithm   that   is   fast   during   both   training   and   testing    and   is   less  
susceptible  to  overfitting     

  
   methodology  
as   is   common   in   machine   learning   implementations    the   general   methodology   is  
divided   into   four   main   phases    preprocessing    normalization    training       feature   selection   
and  testing   we  describe  each  of  these  steps  in  details  next     
   

preprocessing  
the  primary  objective  of  the  preprocessing  phase  is  to  extract  feature  vectors  out   of  
texts   in   their   native language   format    in   order   to   accomplish   this   task    a   text   goes   in  
sequence   through         linguistic   processing    such   as   stemming   and   stop   word   elimination   
      feature   extraction    and         labeling classification    for   the   choice   of   features    we   have  
decided  to  extract  a  total  of      features  initially  at  our  disposal   in  addition  to  the  frequency  
of   the   token   across   the   entire   text    the   initial   set   of   features   include   both   attributes   of   the  
entire  text  itself  such  as  its  length   in  words  and  sentences  as  well  as  attributes  of  the  token  
itself   such   as   its   type   and   length   in   characters    depending   on   performance   during   the   cross 
validation  phase   some  of  these  features  have  been  removed  while  others  have  been  slightly  
modified  as  will  be  discussed  in  the  results  section   
in   addition    because   the   identification   of   keywords   is   a   subjective   measure   that  
partially  depends  on  the  human  reader   a  strict  labeling  scheme  may  not  be  advantageous   
for   example    in   the   context   of   machine   learning    the   word   learning   has   definitely   a   far  
greater  weight  than   say   optimization   while  the  latter   in  turn   may  have  a  greater  weight  
than    for   example    labeling    in   order   to   incorporate   uncertainty   in   classification    different  
possible   approaches   can   be   used    for   instance    one   approach   would   be   to   reformulate   the  
optimization  problem  by  incorporating  uncertainty  and  solve  its  dual   in  a  manner  similar  

fito   standard   svm   implementations    another   approach   to   incorporate   uncertainty   is   to   use  
svm regression    which   is   the   approach   we   adopted   in   this   project    using   the   latter  
approach    the   labels   y  i     take   values   in                where   prediction   is   determined   by   a  
threshold  on  the  margin  predicted  by  svm   in  gda   one  the  other  hand   we  decided  to  use  
two   different   covariance   matrices   for   the   positive   and   negative   classes    which   had   led  
experimentally   to   a   much   better   performance   than   conventional   gda    to   incorporate  
uncertainty   maximum   likelihood   estimation   yields   the   following   equations    which   are  
identical  to  the  m step  update  rule  in  the  em  algorithm   
m

 j     m  w i   y  i    j    

  

  

  

  

  

  

     

i  

 m
 m
 j     w i x  i    y  i    j    w i   y  i    j    
  
  
  
  
     
 i  
 i  
 m
 m

 j     w i   y  i    j    x  i    j    x  i    j   t   w i   y  i    j       
  
     
 i  
 i  
  

here    j        denotes   the   class   type    while   w i          denotes   the   certainty   of  
classification     

   
normalization  
normalization  
is   a   process   of   rescaling   parameters   so   that   they   are   all   given   equal  


weight   during   optimization    in   the   normalization   phase    we   have   experimented   with  
different   rescaling   methods   such   as   linear   rescaling   and   standard   gaussian   normalization   
given   by     value  min    max min      and     value  mean    std      respectively    we   also   tried   to  
use  raw  data  directly  without  normalization   finally   we  also  used  the  linear  normalization  
method   given   by   the   equation   value    value  max     the   latter   normalization   approach   
combined   with   the   feature   selection   method   discussed   in   the   subsequent   section    yielded  


the  best  results     
   

training  and  feature  selection  
in  order  to  train  both  svm regression  and  gda   we  collected  a  list  of      articles  that  
covered  various  topics  and  varied  in  length  from       to  over        words  per  article   each  
document  was  preprocessed  in  python   and  a  label  for  each  word  was  manually  assigned  by  
the   user    for   stemming    we   used   the   porter   stemmer   in   python    while   stop   words   were  
collected   in   a   single   file   from   various   sources    however    we   have   decided   to   retain   stop  
words   that   are   emphasized     for   instance    while   new   is   typically   a   stop   word    new   in  
new   york   is   not    when   stop   words   are   removed    the   final   training   set   contained  
approximately         data  samples       of  which  are  for  the  positive  class     
for  the  svm regression  training  phase   we  used  svm light   which  is  a  very  popular  
implementation  of  svm  developed  in  c        inherently   svm  offers  a  large  degree  of  freedom  
that  needs  careful  assessment   for  instance   we  have  various  choices  for  the  kernel  function  
and   its   parameters    the   tradeoff   parameter    and   also   the   choice   of   features    in   gda    by  
contrast    training  is  relatively  straightforward  using  the  closed form  equations   given   above   
where   the   only   freedom   available   is   in   the   choice   of   features    in   order   to   determine   the  
performance   of   a   particular   choice   during   training    we   used   the   hold out   cross   validation  
method   where       of  the  data  are  used  for  training  and       are  used  for  cross validation   
to   measure   performance   during   cross validation   and   testing    we   computed   the   weighted  

                                                                                                                
   in  our  application   emphasis  is  detected  when  the  first  letter  is  capitalized  because  only  documents  in  text  

format  were  used   however   the  method  presented  herein  should  be  easily  extensible  to  additional  kinds  of  
emphasis  such  as  the  use  of  bold  or  italic  fonts  in  html  documents     

fiaverage   of   false   positives   and   false   negatives    where   a   misclassified   example   is   weighted   by  
the  uncertainty  of  its  label   obviously   because  the  negative  class  constitutes  more  than       
of   the   entire   data   set    the   probably   of   false   positives   and   false   negatives   was   a   more  
descriptive  measure     
  
feature  
description  
rationale  
        token  type  
        token  length  
        fraction  of    
              emphasis  
        freq   x  
log document length   
    freq   x  
log document length   
  
     freq    x  
log document length   

         if  the  token  is  alphabetic    
                if  the  token  is  numeric  
              otherwise  
   of  characters  in  the  token  
   fraction  of  time  the  word  is  
emphasized   e g   capitalized      
   frequency  of  occurrence  of  the  token  
in  the   st  block  of  the  text   multiplied  by  
the  natural  logarithm  of  document  
length   in  words   
   
   
   frequency  of  occurrence  of  the  token  
in  the    th  block  of  the  text   multiplied  
by  the  natural  logarithm  of  document  
length   in  words   

the  nature  of  the  token  should  be  
factored  in  during  prediction     
  
emphasized  tokens  tend  to  be  keywords  

keywords  tend  to  appear  persistently  
throughout  the  text  and  not  only  in  a  
single  block   also   document  length  is  
important   for  instance   most  words  
tend  to  have  high  frequencies  in  short  
documents   which  does  not  imply  that  
they  are  keywords     

table      a  list  of  the  final  features  used  in  automatic  keyword  identification  

  
   

testing  and  results  
the  final  set  of  features  is  depicted  in  table     above   as  shown  in  table      some  of  the  
features   extracted   initially   were   removed    while   others   have   been   slightly   altered    for  
example   while  the  use  of  document  length  as  a  feature  is  important   e g   all  words  in  short  
documents   will   have   unusually   high   frequencies    both   svm regression   and   gda   
unfortunately   tended  to  overfit  such  feature   to  solve  the  problem   we  removed  document  
length   as   an   explicit   feature   but   included   it   implicitly   as   shown   in   the   table    using   this  
feature   vector    both   svm regression   and   gda   yielded   good   results    where   gda   typically  
outperformed   svm   as   shown   in   figure        for   example    gda   correctly   detects    on   average   
      of   all   keywords   while   svm   detects         of   keywords   only    as   weighted   by   the  
confidence   of   classification    in   addition    the   percentage   of   false   positives   is         in   gda   and  
      in   svm    given   the   nature   of   the   task   where   small   errors   are   tolerable   in   favor   of   an  
automated   approach   to   keyword   identification    such   results   are   indeed   promising    also    the  
svm   kernel   that   worked   best   for   this   application   was   the   polynomial   kernel    where   c       
d       in  order  to  illustrate  the  effectiveness  of  such  approach   figure     presents  a  snapshot  
of   a   short   article   and   the   top   keywords   identified   by   both   svm   and   gda    clearly    the  
keywords   suggested   by   gda   are   almost   indistinguishable   from   what   a   human   reader   would  
suggest  for  that  article     

   conclusions  

the   subject   of   automatic   identification   of   keywords   in   random   texts   has   many  
important   applications    in   the   past    most   approaches   focused   on   the   use   of   frequency      
word   association   and   bayesian   analysis   to   predict   keywords    such   approach   has   limitations  
that  can  be  avoided  through  the  use  of  purely  numeric  machine  learning  algorithms  such  as  
svm   and   gda    with   a   carefully   designed   feature   set    both   learning   algorithms   can   yield  
excellent   results    in   general    gda   with   different   covariance   matrices   is   recommended  
because  it  is  faster  to  train  and  predict   simpler   and  yields  much  better  results  than  svm     
  

fi  

figure      a  summary  of  testing  results  for  both  svm regression  and  gda  
  
pc cooling systems are one of the hottest   and coolest   computer hardware accessories available  most
computers  however  still do not come with a cooling system more advanced than the typical heat sink and cpu
fan  some computers  however  are being sold with liquid cooling systems  such as the apple g  power mac 
the dual processors undoubtedly put out a great deal of excess heat that a simple copper heat sink and fan alone
can not properly dissipate  intel is also getting into the new cooling trend  claiming that it s newest processors
 of the pentium   line  are approximately    degrees hotter than any chip currently produced by amd
 advanced micro devices       additional text 
here are some popular options if you are looking to add a new cooling system to your computer 
   water cooling system    additional text 
   memory cooling system  additional text 
   heavy duty fan system  additional text 
       conclusions 
                                                                                                                               svm  keywords   in  decreasing  order    
cooling  fan  processor  heat  system  computer  fan type  pentium  amd 
high end  cpu  intel  micro  devices  powerful  memory  sink  heavy duty 
ram  advanced  
  
gda    keywords   in  decreasing  order    
cooling  system  computer  processor  heat  fan  hardware   

figure      a  sample  of  keywords  predicted  by  svm  and  gda  for  an  article  on  pc  cooling  systems  
  

references  
     m e   maron   automatic  indexing   an  experimental  inquiry   jacm   volume     issue               
      l    de   campos    j    fernndez luna    j    huete    and   a    romero    automatic   indexing   from   a  
thesaurus   using   bayesian   networks    application   to   the   classification   of   parliamentary   initiatives   
proc  ecsqaru                         
      g    salton    automatic   text   processing    the   transformation    analysis    and   retrieval   of  
information  by  computers   addison wesley   reading   pennsylvania          
     b   boser   i   guyon   and  v   vapnik   a  training  algorithm  for  optimal  margin  classifiers   fifth  
annual  workshop  on  computational  learning  theory   pittsburgh          acm  
     a   ng   support  vector  machines   cs     class  notes           
     a   ng   generative  algorithms   cs     class  notes           
      t    joachims    making   large scale   svm   learning   practical    advances   in   kernel   methods      
support  vector  learning   b   schlkopf  and  c   burges  and  a   smola   ed     mit press            

fi