recycler bot
cs     project report
jiahui shi
advisors  morgan quigley and alan asbeck
abstract we consider the application of using personal
robot    pr   to sort out different categories of recyclable trash
on a conveyer belt  currently including glass bottles  plastic
bottles and cans  the robot will be able to find and recognize
these items and send them to the right recycle bins  we will
describe the algorithms which incorporates visual and tactile
perception  show the current results and briefly talk about some
ideas we would like to explore in the future 

i  i ntroduction
collecting and sorting recyclable trash is a trivial and
tedious task for humans  there are lots of recyclable items
left on messy tables or placed in wrong recycle bins and
wasted  in this project we will let pr  take over the job of
recycling trash  there are a few scenarios we considered to
work on  one is to let pr  move around in a building  collect
recyclable items and send them to recycle bins  another
scenario is to look into a recycle bin and pick out the
misclassified items  the third scenario is to stand in front
of a conveyer belt with trash on it  then pick out and sort the
items of different categories  this project tries to achieve the
third scenario  while solving this problem will greatly help
solve the other two as well  the robot uses stereo cameras to
find recyclable objects on a conveyer belt  use the pressure
sensor array on its gripper to sense the materials  classify
them into different categories  then grasp and throw them
into proper recycle bins 
this project involves visual perception  tactile perception
and grasping  currently  we are working on three categories
of objects  cans  glass bottles and plastic bottles  we use
vision information to find objects and further incorporate
tactile information for classification  now the bottles and
cans are described as cylinders with and without bottle caps 
we fit the  d point clouds retrieved from the stereo cameras
to cylinder models  the features include the height and radius
of the cylinder and the existence of bottle caps  the tactile
features we used are the relationships between the fingertip
movement distance  the total force and the force variance on
the pressure sensor 
ii  data from sensors
there are two pairs of stereo cameras installed on pr s
head with two different baselines as shown in figure    there
is also a projector which can project texture in front in
order to find the depth of textureless regions  we used both
the narrow stereo camera and the wide stereo camera with
the texture projector turned on  the narrow stereo camera
retrieves more accurate  d point clouds in the projected

fig    

two pairs of stereo cameras

fig    

fingertip sensor

area while the wide stereo camera finds larger but more
noisy point clouds  we only need the wide camera to find
bottle caps since they are missing in the point cloud from
the narrow camera 
the tactile information is sensed by the two pressure
sensitive fingertips on pr s left gripper as shown in figure
   the pressure sensor array on each fingertip comprises   
pressure elements as illustrated in figure    a      array
on the front  one on the back and   on each edge  we can
also know the distance between two fingertips from the joint
encoder on the gripper 
iii  o bject detection from stereo vision
this part introduces how we detect objects from the
 d point cloud  the general idea is to first remove the
background  then find big clusters in the remaining point
cloud  fit the clusters to cylinders  find bottle caps above
the cylinders and apply some shape features to classify the
objects 
a  background removal
in order to isolate the point clouds of target objects
from the background  we first detect the conveyer belt and

fifig    

output from fingertip sensors

remove all the points except for those above the scope of the
conveyer belt  we first discretize the space into a  d grid 
the cells with a significant number of points are marked as
occupied  then we build a histogram according to the z axis
and find the z value with the largest number of occupied
cells  this value is regarded as the height of the conveyer
belt plane  in order to get rid of the points higher than the
plane but not directly above the conveyer belt  we find the
x y scope of the conveyer belt plane  leave a small margin
and only keep the points within this scope 
b  clustering using ann  approximate nearest neighbor 
after removing the background  we get point clouds of
the surfaces of cans and bottles  which are noisy and in the
shape of multiple cylinders  as plastic and glass bottles are
partially transparent  we only have the point clouds of their
labels  the clusters and corresponding cylinders are found
iteratively one by one in a greedy manner 
in each iteration  we apply ann  approximate nearest
neighbor  to find the biggest cluster in the point cloud  here
we use stann  simple  thread safe approximate nearest
neighbor  c   library    this library provides the algorithm
to find the k nearest neighbor points of a query point  by
iteratively finding the nearest neighbor points of each of the
k neighbor points  we get a cluster  k is set to be    after
finding all the clusters  we pick the largest one  the search
stops when the largest cluster is too small 
c  cylinder fitting using ransac
within the largest cluster we found  we use ransac
 random sample consensus  to fit a cylinder model  since
currently we only work on the standing up cylinders  we can
project the  d points in to the x y plane and fit the  d points
to a circle  for each iteration  we pick three points randomly
and find the circle defined by these points  by allowing a
  available

online  http   sites google com a compgeom com stann 

fig    

bottle detection from a  d point cloud

tolerant margin around the circle boundary  the number of
cluster points within the margin is counted as the consensus 
repeat this process until the maximum iteration is reached
or a circle with a large enough consensus is found  then the
optimal circle for the all the consensus points is found by
least square fitting  thus  we get the radius and position of a
cylinder  the points belong to this cylinder will be removed
from the point cloud for the next cylinder search iteration 
d  bottle cap detection
the existence of a bottle cap is an intuitive feature to
distinguish between cans and bottles  since the point clouds
of caps sometimes do not appear in the narrow stereo point
cloud  here we use the wide stereo point cloud though it
is more noisy  the cap is found by locating a small cluster
above each cylinder found in last step  the height of a bottle
is the distance from the top of its cap to the conveyer belt 
the result of object detection from stereo vision is shown
in figure    the grey points is the original point cloud
from the narrow stereo camera  the red points is the largest
cluster  the cylinder which bounds the bottle cap indicates
the position and size of the bottle and its green color means
it is classified as a glass bottle from its appearance 
e  comparison with object detection in  d images
we also considered to detect the bottles and cans directly
from a  d image in order to find them beyond the scope
of stereo vision  we tried the code of felzenszwalbs discriminatively trained deformable part models     with the

fifig    

bottle detection from a  d color image

result shown in figure    after running for about half a
minute on matlab  the three standing up bottles and cans
were successfully detected  but when they are placed in
another orientation  for example  laying down  the algorithm
failed to find them  it might be possible to slightly modify
the algorithm to adapt to different orientations  but we will
leave this problem to the future and focus on stereo vision
at this moment 
our algorithm on  d point cloud is more efficient  in our
final experiment  we actually only need to find one largest
cluster and the corresponding most confident cylinder for
each grasping  the vision processing can take less than one
second though it depends on the amount and quality of the
 d points 

fig    

total force versus gripper movement distance

iv  tactile perception
it is very hard to distinguish between plastic bottles and
glass bottles only from the visual feedback  here we utilized
the tactile perception  we let pr  use its left gripper to
squeeze an item and sense the material  after setting the
maximum effort of the gripper  the fingers will gradually
close and squeeze the item  the feedback from the fingertip
pressure sensors is a    dimensional array  here we only
consider the      array on the front of fingers  we observed
the total force on all sensors  the variance of force on
different positions and the gripper movement distance  their
relationships are illustrated in figure   and    from figure
   the plot of total force versus gripper movement distance 
we can find that the can has multiple sudden big changes
of shape  the plastic bottle changes more smoothly  and for
the glass bottle  the gripper stopped moving at a certain point
since it cannot squeeze it  figure   illustrates the relationship
between the total force and the variance  the force variance
on cans is smaller than that on plastic bottles because the
touching area of a can gets flatter than that of a plastic bottle 
there can be used as features for classification 
v  g rasping
after successfully finding an object from vision  the robot
will grasp and send it to a proper place  in order to avoid
touching other bottles  the gripper first moves above the

fig    

total force versus force variance

found object  then it moves down and close the gripper 
at the same time  it squeezes the object and do the tactile
classification  we used the robot arm motion planning algorithms provided in ros      now the robot is able to grasp
an object using either the left or the right arm  we will design
a planning algorithm for two arm grasping in the future 
vi  o bject classification
to classify the detected object  we use the tactile features
as described in the previous section  the height and radius
of the cylinder  as well as the existence of a bottle cap  the
result is shown in figure    the upper picture is from the
left lens of the narrow stereo camera  the lower picture is
the result shown in the  d point cloud  where we use red
cylinders to mark cans  green for glass bottles and blue for
plastic bottles 

fifig    

classification result in the  d point cloud

vii  r esults
the classification works for almost      accuracy  the
major cause of failure is the occasional inaccuracy and
damping of arm motion  which may push over the bottles 
figure   shows a sequence of pictures of how pr  works 
it can successfully sort out a variety of bottles as shown in
figure     a   minute demonstration video is available online
at http   www youtube com watch v dok wppujia  
viii  f uture work
we will apply more robust algorithms in order to deal with
more complicated situations  including objects in arbitrary
orientations  occluding each other and finally piled up  in
such complicated cases  we will try incorporating  d color
images with  d point clouds for the segmentation and
object detection  we also want to include more categories
of recyclable items in the future  such as paper boxes and
bottles  more advanced algorithms for tactile sensing will
help distinguish between different materials 
ix  acknowledgement
the project has been made under the guidance of morgan
quigley and alan asbeck  it would not have been possible to
implement this idea without their sound ideas and direction 

fig    

pr  working in front of a conveyer belt 

fifig     

result of sorting a variety of bottles 

r eferences
    p  f  felzenszwalb  r  b  girshick  and d  mcallester  discriminatively trained deformable part models  release    ieee transactions
on pattern analysis and machine intelligence  vol      no     september      
    m  quigley  b  gerkey  k  conley  j  faust  t  foote  j  leibs  e 
berger  r  wheeler  and a  y  ng  ros  an open source robot
operating system  in proc  open source software workshop of the
international conference on robotics and automation  icra        

fi