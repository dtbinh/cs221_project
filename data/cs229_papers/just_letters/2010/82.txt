optical character recognition for handwritten hindi
aditi goyal  kartikay khandelwal  piyush keshri
stanford university
abstract
optical character recognition  ocr  is the
electronic conversion of scanned images of hand written
text into machine encoded text  in this project various
image pre processing  features extraction and
classification algorithms have been explored and
compared  to design high performance ocr software for
indian language hindi based on devanagari
script  the best performance obtained with handwritten
letters is       using one against all svm
technique and extracting features using hog
 histogram of gradient  technique 
   introduction
    motivation
ocr finds wide applications as a
telecommunication aid for the deaf  postal
address reading  direct processing of documents 
foreign language recognition etc  this problem
has been explored in depth for the latin script 
however  there are not many reliable ocr
software available for the indian language hindi
 devanagari   the third most spoken language in
the world      provides a good starting point for
the problem and presents a good overview  the
objective in this project is to design high
performance ocr software for devanagari script
that can help in exploring future applications such
as navigation  for ex  traffic sign recognition in
foreign lands etc 
    framework description
      hindi language fundamentals
the hindi language consists of    vowels and
   consonants  the presence of pre and post
symbols added to demarcate between consonants
and vowels introduces another level of
complexity as compared to latin script
recognition  as a result  the complexity of
fall  

deciphering letters out of text in devanagari
script increases dramatically because of presence
of various derived letters from the basic vowels
and consonants  in this project emphasis has been
laid on recognizing the individual base consonants
and vowels which can be later extended to
recognize complex derived letters   words 
      dataset generation
because of the limited scope of work being
done in this realm  standard hand written dataset
for hindi is not readily available  hence  the
entire dataset has been generated by taking hand
written samples from    different users 
    approach
the approach followed during the project was
to formulate a large systematic standard dataset 
extract important features from the scanned text
images and to implement high performance offthe shelf classification algorithm  the following
sections discuss about the methodology
implemented throughout the project to improve
the performance of the system 
   methodology
    training set generation
the handwritten data set was manually
generated for each of the    fundamental
characters  to standardize image preprocessing
steps  a standard template was created consisting
of    blocks      x     pixels each  on a    x  
sized sheet  where each block contained exactly
one character  samples from    different users
were obtained to account for varied human
calligraphy style and fonts sizes  while generating
samples  angular skewness was avoided  all
characters were written with a standard black pen 
these sheets were then scanned to generate the
character images  a complete dataset of         samples for each of the    characters was

cs    machine learning

 

fiacquired  resulting in a dataset of      
characters 
    image preprocessing
the scanned image was first converted from
rgb scale to gray scale  it was then splitted into
individual character blocks using matlab script
to obtain raw individual character samples  the
following preprocessing and noise removal
techniques were used on raw samples to obtain a
clean dataset 
      median filtering  scanning process
introduces irregularities such as speckle noise
and salt and pepper noise in the output image 
median filtering was employed  to remove such
effects  where each pixel was replaced by the
median of the neighboring pixels 
      background removal  to model the
background noise due to scanning  a white page
was scanned with the same scanner and this image
was subtracted from each of the character images 
hence eliminating background and any residual
background noise  highlighting only the character
sample 
      thresholding  to remove any residual
irregularities and to increase image contrast  all
pixel values above     were scaled upto     
also  all pixels lying at the boundary within a   
pixel wide strip were scaled upto     to ensure a
clean boundary 

fig   image after preprocessing 

      sparsity removal  it was observed that the
image matrix was sparse and the character size
within the image was much smaller than the
fall  

complete image  hence  a matlab script was
written to create a tight bounding box around the
character and to extract pixels into a
   px    px matrix thus increasing the density
of useful character information 
after creating individual character images by the
above process  any abnormalities in the data set
were also removed manually 
    feature selection extraction
the following feature extraction methods
were employed and tested on a training set to
determine the most optimal set of features for
our specific handwriting recognition problem 
      raw pixel data
the most basic feature set for any image is its
pixel intensities  and thus this was the first set of
features that were employed  while these set of
features are easy to design  they lead to a very
high dimensional feature vector        
dimensional for a    px    px image  
reducing the image size to extract reasonably
sized feature vectors blurs the image and leads to
loss of information  however  a large feature
vector size results in massive learning time of
high complexity classification algorithms such as
svm and also results in over fitting  highvariance  issues 
      histogram of oriented gradients
 hog 
to overcome the problems associated with
the features generated from raw pixels 
histogram of oriented gradients  hog 
features     were used  this feature set is
independent of the image size and captures
localized
information
about
intensity
gradients  the hog window size and the
number of bins in the histogram can be varied to
analyze the performance of classification with
respect to feature size  thus  this provides a
flexible set of representative features and helps to
deal with both high bias and high variance issues 

cs    machine learning

 

fidesign parameters for hog features
number of hog windows per bound box 
along horizontal     
along vertical    
number of bins in histogram    
filter kernel 
along horizontal           
along vertical           
      sparse autoencoder
the third method being analyzed in the
design process for feature extraction was sparse
autoencoder  a multi layered neural network  as
in      the basic back propagation algorithm was
used to determine the weight matrix 
after the weight matrix    x    was obtained 
each image was divided into    matrices  patch
size  x   and an estimate of the feature vector
 length       for each patch was calculated  for
every image  the vector with maximum norm
was chosen as the feature vector 
design parameters for the sparse autoencoder
activation function  hyperbolic tangent
  of inputs     
 of layers    
  of hidden units     
weight decay parameter  lambda         
learning parameters  alpha         beta    
sparsity parameter  rho         
number of iterations     million
      analysis
techniques

of

feature

extraction

firstly  the above features were tested on a
reduced training set    character labels  using
nave bayes as shown in table  
observations
 since the raw pixel data was very high
dimensional  the run time for these features
was very high  as expected  the larger image
matrix     px    px  performed better in
terms of accuracy  plotting the smaller image
showed that a large amount of the
fall  

information was lost as a result of blurring
effects 
 the performance of hog features was found
to be much better than the sparse
autoencoder in terms of test error  this can
be explained by the fact that the hog
features best captured the distribution of
intensity gradients and edge directions  it was
also observed that the sparse autoencoder
had higher run time for both training the
neural network and extracting the features
from individual images  making its realization
difficult for large feature vector dimension 
 increasing the size of the hog feature vector
led to higher test errors primarily because of
over fitting of parameters 
 higher test errors were obtained with raw
pixel features as compared to hog features
showing that most of the character
information is contained in the gradient of
intensity and not the absolute intensity values 
owing to better accuracy and low run time 
hog features were used for subsequent analysis
and classifier formulation 
feature extraction
raw pixel
 image size    px    px 
raw pixel
 image size   px   px 
sparse autoencoder
hog
 feature vector dim       
hog
 feature vector dim        

test error
      
   
      
     
     

table   test error for different features 

    classifier
after selecting the feature extraction
technique  while choosing the classifier algorithm
for ocr the following three classifiers were
analyzed 

cs    machine learning

 

fi      nave bayes
for a quick and dirty implementation  the
naive bayes algorithm was used  since the pixel
values and hence  successive features are
dependent on the character label  the nave bayes
assumption  features being conditionally
independent of output labels  may not hold  this
explains the low accuracy that was obtained with
this classifier as shown in fig   
      support vector machines
to use svm in the multi class case  the svm
and kernel methods matlab toolbox was used 
in particular  the one against all approach
was used  the errors for gaussian and
polynomial kernels were evaluated  the
variation in error with modification in the cost
parameter c were also observed  specifically 
decreasing c from      to     and then to   
and   led to an increase in the test error as well as
an increase in the running time of the algorithm 
increasing c from      to       showed a
similar increase in test error 
final parameters used 
cost parameter c       
lambda         
      adaboost
to use adaboost in the multiclass model  the
gentle adaboost for multiclass classification
matlab toolbox was used  in particular  we
used the decision tree as the underlying base
learner  using the perceptron as the base
learner led to very high run times of the
algorithm and hence this weak learner was not
used on the entire dataset 
final parameters used 
lambda         
epsilon        
   test results
for evaluating the final errors for each
classifier  we used the leavemout cross
validation with m  size of test set  fixed at    
samples  this was done for    iterations with the
fall  

test set being randomly chosen in each iteration 
the following curve shows the plot of test and
training errors for svm  gaussian kernel  as a
function of training set size 

train accuracy  

test accuracy  

   
  
  
  
nave
bayes

ada
boost

svm
svm
linear gaussian

fig    test and training error over whole dataset 

    observations and conclusions
 the nave bayes classifier gives high errors
probably because the nave bayes assumption
of xij being conditionally independent of yi
does not hold here 
 the test error for adaboost  fig    was seen
to be surprisingly high for an iterative
algorithm trying to minimize error in every
iteration  this can be attributed to the use of
a binary weak learner such as decision trees 
which seem to break down with the increase
in the number of classes 
 as seen in fig    svm shows higher test error
for
smaller
training
data
but
asymptotically gives better results than
either naive bayes or adaboost 
 the one againstall svm approach with gaussian
kernel  cost parameter         was found to have
the highest test accuracy of        this method
also gave      accuracy for the training set
 fig      one possible explanation for this
seems to be that the hog features  with
regularization  give linearly separable data
points that can be correctly separated by a
maximum margin hyperplane 
 the svm linear kernel gave about    more
error than the gaussian kernel probably
because the dataset was linearly separable

cs    machine learning

 

fieven in a finite feature dimensional space and
hence using a gaussian kernel did not yield
significant improvements 

linearly separable dataset and very high
accuracies for the test set  this basically
implies that the intra class features were
kind of similar while there was significant
difference between inter class feature
vectors 
ii  clean data set  it was ensured that the
dataset contained no rotated   distorted
images 
iii  simplistic character set  the project
recognizes basic consonants and vowels
instead of complete words 
 the confusion matrix showed that some
particular combinations of characters were
being confused consistently by all the three
classifiers  these were also verified to be
visually similar to the human eye 

fig   test error vs training data size

   acknowledgements
we thank prof  andrew ng  andrew maas
and quoc le for their guidance and support
throughout the project  we would also like to
thank all those who contributed towards the
generation of our data set 

fig   training error vs training data size


some of the reasons of getting high value of
accuracies with svm could be 
i  linearly separable data set   as shown in
     svm achieves zero training error for

fall  

   references
    c v  jawahar  r  kiran  a bilingual ocr for
hindi telugu documents and its applications 
icdar  aug    vol   
    n  dalal  b  triggs  histogram of oriented
gradients for human detection  conference on
computer vision and pattern recognition       
vol     pp          
      a  ng  cs    notes   sparse autoencoder 
winter    stanford university 
    m s  jelodar  m j  fadaeieslam  n  mozayani 
m  fazeli  a persian ocr system using
morphological operators  world academy of
science  engineering and technology        pp 
       
    a  ng  cs    notes   support vector
machines  fall    stanford university 

cs    machine learning

 

fi