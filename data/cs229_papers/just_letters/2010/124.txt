graduate school application evaluation based on svm
wei li  yu zhou  yi yang
introduction
these days  more and more chinese undergraduates consider pursuing a masters
degree in the united states  but admission standards are distinct for each school and it
is hard to decide which to apply  school selection alone has already become a big
business in china  a list of graduate programs suitable for an applicant can be
charged     dollars in china    in this paper  support vector machine  svm  and
logistic regression are applied for analyzing historical admission data of each
university to automatically predict the admission decision for chinese applications 

svm method
support vector machine learning algorithm is among the best supervised learning
algorithm  it makes no strong assumption about the data and the performance is very
good  the downside of it is computational inefficiency  but as is stated later  our
sample size is small  which makes svm the perfect algorithm here 

problem formulation
phds admission is mainly based on paper publication  research experience and
recommendation letters  which are hard to quantize  so this project focuses on
masters admission  moreover  different majors have distinct standard so the data mu
be separated by majors  since the number of ee applicants is among the highest    in
china  our project focus on prediction of ee masters application  the data was
collected from famous realated bbs     since very few applicants are willing to share
their background  only two hundred samples have been colleced  among which the
largest data size for a single graduate school is     nonetheless it turns out that even
with very limited data the result is quite satisfactory 

feature sele ction
the following popular six features of applicants ware used to train the svm
algorithm  undergraduate institute  gpa  gre verbal scoregre quantitative score
toefl scoreand  number of paper publications 
all the applicants in our sample havent change their major from undergraduate to
master  and none of them have an advanced degree  smo algorithm is applied to
solve the constrained optimization problem through both linear and quadratic kernels 
all the data have been preprocessed to the same scale before application 

experiments and results
each university has its own admission standard  if the svm is trained with all the data
from all universities and then tested for a particular one  the prediction rate should be

filow  tamu is taken for example 
kernel
linear
quadratic

prediction rate
     
      
table  

as a result  it should make more sense to train and test on the same university  in this
case  the training data is really scarce  as a result  leave one out cross validation has
been proposed to test the prediction rate of the learning algorithm 
here is the test of data from each university with reasonable sample size  all the six
features have been used 
university

sample size

tamu
purdue
ncsu
ucsd
u michigan
ucla
ut austin
u pittsburgh

  
  
 
 
 
 
 
 

prediction rate for prediction rate for
linear kernel
quadratic kernel
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
table  

two important points could be observed from the results 
   the prediction rate is relatively high for large sample size  as the sample size
decreases  the prediction rate would drops down 
   the prediction rate for linear kernel is better than the prediction rate for quadratic
kernel in all cases 
since there are six features  which are comparable to some samples  the high
prediction error probably results from high dimension of feature space  forward
selection procedure is used to add one feature at a time until reaching the best feature
set  the prediction rate is as follows 
university
sample size
prediction rate for prediction rate for
linear kernel
quadratic kernel
tamu
  
   
   
purdue
  
   
   
ncsu
 
   
   
ucsd
 
   
   
u michigan
 
   
   
ucla
 
   
   
ut austin
 
    
    
u pittsburgh
 
   
   
table  

fitable   showed that the prediction rate is better with shrinked feature set  even
when the sample size is as small as   or    the prediction rate reaches an average of
     when the sample size goes above     the prediction rate achieves about     
also  linear kernel outperforms quadratic kernel in all cases  which indicates the
relationship among the factors could be considered linear  the result here is indeed
practical and could serve as a useful indicative for graduate school selection 
here is best feature set for each university with linear kernel 
is included and   indicates the its excluded  
university
graduation
gpa
gre
gre
university
quant
verbal
tamu
 
 
 
 
purdue
 
 
 
 
ncsu
 
 
 
 
ucsd
 
 
 
 
u michigan
 
 
 
 
ucla
 
 
 
 
ut austin
 
 
 
 
u pittsburgh  
 
 
 

   indicates the feature
toefl

paper

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

table  

although the sample set is small  the results really make sense  its well known that
undergraduate institute and gpa are the most important factors to indicate an
applicants academic background  the main reasons are that undergraduate institute
could reflect the overall academic standard of a students education and gpa could
measure his performance under such standard  in most universities feature selection
results  these two features are included  gre quantitative score was also important for
ee applicants since it is an indicator of ones quantitative skills  gre verbal score
and toefl score only indicate language skills which are less relevant to ee study so
they are not included  since most master applicants have no paper publications  it
would be no surprise to exclude paper for either kernels
in the end is a graph indicating the effects of feature selection and kernel choices on
the prediction rate 

fiprediction rate vs  features   kernels

au
st
in
pi
tt
sb
ur
gh

uc
la

best feature set 
linear kernel
best feature set 
quadratic kernel
full feature 
linear kernel
full feature 
quadratic kernel

target university

u

ut

an

mi
ch
ig

uc
sd
u

nc
su

pu
rd
ue

ta
mu

prediction rate

    
   
   
   
   
   
   
   
   
   
  

figure  

comparison with logistic regression
logistic regression is used in comparison to the svm learning algorithm 
the learning rate was set    batch gradient ascent is used for data separated by schools
and tested with   features  here is the result 
university

sample size

prediction rate

tamu

  

   

purdue

  

   

ncsu

 

   

u michigan

 

   

ucsd

 

   

ucla

 

   

ut austin

 

   

pittsburgh

 

   
table  

it shows that as the sample size got smaller  the prediction rate becomes unstable 
when analyzing the result  some parameters of toefl are found negative  thus  the
ee departments might not care much about toefl score above minimum
requirements  the result becomes much better when excluding toefl scores 
university

sample size

prediction rate

tamu

  

   

purdue

  

   

fincsu

 

   

u michigan

 

   

ucsd

 

   

ucla

 

   

ut austin

 

   

pittsburgh

 

   
table  

here is a graph comparing the best prediction rate of svm and logistic regression 
the svm method performs better than logistic regression method 

    
    
   
   
   
   
  

svm

ur
gh

pi
u

u

university

tt
sb

ti
n

a
ut

au
s

uc
l

ga
n

d

ch
i

mi

uc
s

u
nc
s

pu
r

ta
m

du
e

logistic
regression

u

prediction rate

svm vs  logistic regression

figure  

conclusion
svm with linear kernel can generate good result for graduate admission selection
problem  the most significant features are undergraduate institute  gpa  and gre
quantitative scores  while gre verbal scores  toefl  and paper are much less
relevant  the only concern is that the data set is too limited  if we can get enough data
from department admission office  the svm should be able to achieve more accurate
results  finally  we compared the result with that of the logistic regression algorithm 
and we can see that the svm algorithm improves the prediction accuracy a lot 

reference
  
  
  
  
  

cloud apply  http   www cloudapply com index do
http   liuxue eol cn shenqing               t                shtml
http   bbs gter net bbs  http   bbs taisha org 
cs     lecture notes  http   www stanford edu class cs    materials html
elements of statistical learning  second edition 
http   www stat stanford edu  tibs elemstatlearn 

fi