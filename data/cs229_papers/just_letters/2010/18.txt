maul  machine agent user learning
robert holley and daniel rosenfeld
cs    project report
          

abstract
we describe implementation of a classifier for user agent strings using support vector machines 
the best kernel is found to be the linear kernel  even when more complicated string based kernels  such
as the edit distance kernel and the subsequence kernel  are employed  a robust tokenization scheme is
employed which dramatically speeds up the calculation for the edit string and subsequence kernels by
shortening the effective string length 

 

introduction

strings and built classifiers for them  one of the first
such entities was microsoft  who included a recognition engine  browscap dll  and a pattern file  browscap ini  with its early web servers      the browser
capabilities project  bcp  has kept these files up
to date for the web development community  despite
microsofts abandonment for more sophisticated  and
proprietary  methods      however  the maintenance
of the project requires human parsing of new useragent strings every week and subsequent updating
of the recognition engine and pattern file  bcp reports that they receive several dozen new user agent
strings per week   gary keith  the proprietor of bcp 
has an automatic script run every sunday morning 
the output of which he parses in the afternoon and
subsequently updates his file of highly structured
regular expression searches  a more recent effort 
browserscope  which started as uaprofiler   took a
similar approach  using a regular expression based
parsing engine to identify specific browsers  browsers
only        the parsing engine required regular updates to remain up to date with new user agent
strings  in       the author of uaprofiler  steve
souders  reported finding    new user agent strings
per day  which he examined every day at   am over
morning coffee       other efforts such as those by
user agent string info and useragentstring com  also
use brittle parsing rules and curated data just like
browserscope and bcp          other efforts to categorize user agent strings include entire communities
such as agentarius net  which has created a structured
database of over         user agent strings 
our machine learning approach to user agent

a user agent string is an http header sent along
with a request for a web page  often but not always by
a web browser      the intent is to inform the server
of the capabilities of the software being used by the
client  the user agent is one of the most important
signals to differentiate a desktop browser from a mobile device or an automatic crawl  in addition gathering statistics on them provides insights on changes
in browser  operating system and device usage  they
are also frequently misused  in e g  cloaking a web site
to make it look different to a search engine crawl 
user agent strings can contain loosely structured
tokens on engine  browser  version  build date  etc 
but their format was never strictly standardized 
    as the number of web access devices increases 
especially with new generation mobile devices and
browsers  the numbers of different user agent strings
is rapidly growing and diversifying  extensions and
plugins can often mutate user agent strings in unpredictable ways  insert  splitting  duplicating  and
re ordering tokens   firefox   and internet explorer
  will soon ship with completely reformatted strings 
some mobile operators have begun introducing custom http headers to extend the traditional role
of the user agent      web spiders and crawlers are
also an important and unpredictable contributing factor  an overview of the development and mutation
of user agent strings is given in       while     is a
public list of over       currently unique strings 
for over a decade  those interested in tracking the
proliferation of the web have collected user agent
 some

unscrambling necessary 

 

fistring parsing could provide partially automated
parsing even on new user agent strings  absolving
the need for a human to keep user agent parsers up
to date  although we are quite sure gary and steve
will continue updating their parsers each sunday and
over morning coffee  perhaps some day in the future
this will not be necessary 

 
   

codes  the user agent strings were tokenized in order to build feature vectors and also speed up the
implementation of the non vector based string kernels we used to classify the strings  tokenization is
an atomistic deconstruction of each user agent string
at arbitrarily chosen break characters  see figure    
the tokenization scheme used in our classifiers was
to break apart the string at any instance of          
other break characters were also attempted  such as
  and    however little improvement resulted 
the tokens were further coalesced to increase the
robustness of the classifier  instead of having a
unique token for every number  all numbers of the
same length were represented by a single token  further coalescing was performed so that instances of
http and  http were represented by a single token 

computational approach
data

we have assembled an annotated dataset consisting of        user agent strings  the strings were
acquired from a variety of sources  user agents org 
     strings  ua tracker com         strings  useragent string info       strings         total excluding duplicates                the annotation consists
of agent type  browser  bot   agent family  firefox  googlebot  etc    family version  os  windows 
linux  etc    and os version  the annotation was
performed by using two common parsing engines
 uasparser      and uaparser  formerly browserscope  formerly ua profiler         and merging the
result  since uasparser returns the most information
 a python dictionary with user agent type  family 
and os  it is used as the primary source of information whereas uaparser is used to generate version information for browsers  uaparser is primarily geared
toward parsing browsers and therefore is not useful
for bots and the other types of entities on the web 
this approach was taken due to the lack of availability of annotated sources  some of the annotated
user agent strings were curated by hand due to the
failure of the parsers  the data were parsed and annotated using a series of python scripts and python
versions of the afore mentioned parsers 

   

figure    example of the tokenization process  notice that the string is de constructed at any instance
of one of the following characters           

   

learning

generally  given a new user agent string  our goal is
to guess whether the string is from a bot  a browser
or a mobile browser  if the string is from a browser 
we will also want to guess the browser type  family  and os type  our goal is to build several classifiers using different techniques in order to find the
most robust for classifying user agent strings  for
performing svm training and prediction we use the
libsvm string library library  see implementation
section below           
the kernels we chose to test are the linear  radial
basis function  gaussian   edit string  tokenized  
and subsequence  tokenized   for the linear and rbf
kernels  the feature vectors were the number of instances of each token in a user agent string  the edit
string kernel is defined as

data processing

the data generated by the above methods were additionally processed to reduce and standardize the
number of classes  specifically  the os information
was parsed to collapse all versions of windows  linux
and mac os into only three oss and move all other
os information into the os version field  all special
versions modes builds of specific browsers were collapsed into their respective families  all validators
and bots were given the type field robot since
validators are also non human web crawler applications  the dataset is stored in both a flatfile format which allows for facile reading and editing by a
human and a sqllite database for use by classifier

k x  z    exp   ld x  z  

   

where ld is the levenshtein edit distance between
the strings x and z  as input parameters  the edit
distance kernel takes the decay factor   the subsequence kernel is calculated in a less straightforward manner  this kernel seeks to compare all
non contiguous sub strings of length p between two
strings      given that the string is expressed in an
p
alphabet of size  a   the feature vector  x   r a   
each element in the feature vector corresponds to a
 

fipossible sub string  u  of length p in alphabet  a   kernel precomputes all necessary powers of lambda 
the entry for each element is
so that subsequent calls can retrieve the appropriate
values from a look up table 
x
length i 
u  x   
   
this doubled the performance of our svm  but
iinstances u x 
training with a small fraction of our training data
still took over an hour  we determined that the perwhere the notation instances u  x  means all in  formance of the subsequence kernel was cubic in its
stances of the sub string u in x  the inner product input string  thus  the most effective way to speed
between two vectors is then simply the normal in  up the algorithm would be to reduce the size of the
ner product between two feature feature vectors  x   input  to do this  we introduced a tokenizer  which
 z   any sub string which is not present in a string mapped ascii strings to sequences of tokens  reprehas a   entry in the u position of the feature vector  sented as   byte unsigned integers   this had the efindicating that the inner product corresponds to a fect of dramatically increasing the alphabet size  from
sum over all shared length k sub strings  as inputs    to        while dramatically decreasing the average
the subsequence kernel takes in a desired sub string string length  we made the necessary generalizations
length p and a decay factor  
in the data structures and control logic  and templaall of our classifiers are of the multi class vari  tized the kernel function  replacing char  with t   
ety  our multi class classifiers are of the one vs  one  allowing us to use the exact same code for both data
max wins variety  this means that for a problem types 
one vs  one classifiers 
of k classes  we form k k  
one major problem with libsvm string was that
 
for prediction on an input string  the input string is it didnt include a good memory ownership model for
passed through all classifiers  and the class that was string data  in particular  it would assume that the
predicted most frequently is returned as the predic  data pointed to by a char  remained immutable betion 
tween calls into the library  and would not make any
internal copies of the training data  this assumption
was not valid for our use cases  leading to crashes  we
  implementation
thus introduced a memory management system where
in our efforts  we developed a versatile software we make copies of input data and track whether strucstack for classifying user agent strings  the code is tures were allocated within libsvm or outside of it 
open source and available at https   github com  allowing libsvm to clean up its own memory but
bholley maul  in this section  we briefly describe avoid clobbering the memory of its caller 
the key components 

   
   

libsvm string

python framework

since we were operating primarily on string data 
we decided that it would be unnecessarily painful to
write all of our code in c or c    we settled on
python as a harness language  and wrote a flexible
and robust framework on top of libsvm string using
the ctypes module of python  this turned out to be
less straightforward than we had originally thought 
and we eventually had to make a few small changes
to libsvm string to work around ctypes bugs on
certain operating systems 
in the end  it turned out to be worth it  using
our maul framework  any cross validation procedure with any set of parameters can be implemented
with only a few lines of code  see maulharness py
and maulbatch py   the framework includes automatic model saving  data selection  and built in
validation  the data was managed with an sqlite
database  so that training and testing data could be
selected quickly and efficiently from our highly heterogeneous data sources  finally  it allows string and

at the heart of our implementation is libsvmstring  a superset of libsvm that includes the ability
to classify string data       when invoked with the
proper parameters  libsvm string accepts arrays of
characters  rather than vectors  as input data  and
operates on them with a string kernel 
libsvm string comes standard with an implementation of the edit distance kernel  for comparison  we also implemented a recursive subsequence
kernel  basing our work on example code from     
unfortunately  the subsequence kernel turned out to
be significantly slower than the edit kernel  with time
spent in the kernel function dominating training and
testing time  during profiling  we discovered that half
of the total computational time was spent inside the
pow   function computing various powers of   the
decay parameter   since this parameter is constant
for a given svm  we restructured the code to make
the subsequence kernel stateful  on the first call  the
 

fivector svms to be instantiated and used with the
same code  as a result  the finished framework made
it extremely easy to compile our final results  we
wrote a small python program to repeatedly call the
cross validation routine with different sets of parameters  and let it run overnight 

 

            firefox        is a bot disguised as a
browser  and was misclassified by uaprofiler  useragentstring com  and user agent string info  the
robot with the user agent string  arabybot  compatible  mozilla      googlebot  fast crawler     
http   www araby com    is a bot that was not seen
before  and went unclassified by uaprofiler  useragent string info  useragentstring com and agentarius  the browser with the user agent string  googlebot    
  http   www googlebot com bot html 
msie      windows nt      googlet    net clr
            net clr                 net clr
            net clr            is a browser disguised as a bot  and was misclassified by useragent string info and bcp  the more browser centric
parsers uaprofiler and useragentstring com did get
this one right  
for the type classifier  b mb r  some effort was
expended in cross validating the input kernel parameters  however  little if any improvement was
achieved  changing the parameters     and p did
not result in any improvement in the performance of
the edit and subsequence kernels relative to the linear
kernel  changing the parameter c had some effect on
the performance of the classifiers  although c      
was close to optimal and only improvement at the
hundredth of a percent level was achieved for the linear classifier with c        this lack of improvement
could be due to the ease in classifying a user agent
string  and testifies to the power of the svm based
classifier with respect to this data class 

results and discussion

classifiers using the afore mentioned kernels were
built to determine whether a user agent string is
a browser robot mobile browser  b mb r   which
family  ie  chrome  firefox  etc   a brower belongs
to  fam    and which os a browser type user agent
string reports  os   the below table summarizes the
accuracy results for the given classifiers  these classifiers were run using the parameters  c               
p             
kernel accuracy   
linear
rbf
edit string
subsequence

b mb r
     
     
     
     

fam 
     
     
     
     

os
     
     
     
     

the above table demonstrates that the linear classifier  despite its simplicity  performs better than all
other kernels  this is surprising given the more complicated feature space implied by some of the other
kernels  for example  both edit string and subsequence kernels include information regarding the ordering and position of tokens within a string  our
results suggest that this information is of little use
in classifying user agent strings  possible reasons
include the shuffling of tokens in user agent strings
due to browser plugins  browser build differences  or
other mechanisms  in robot vs  other classification 
the robot strings are typically short and in a more
standard format  suggesting perhaps ordering information is not necessary for good classifiers 
the data was further inspected post hoc to determine whether our svm based classifier was superior to regular expression based parsers  some types
of strings are expected to be difficult to classify for
standard parsers  specifically  robots never seen before  robots disguised as browsers  and browsers that
are partially disguised as robots  we have found an
instance of each of these cases  where when the below strings are excluded from the data they are still
classified properly  yet several online tools fail at classification  the examples we found were 
the robot with the user agent string  mozilla    
 compatible  butterfly     
 http   labs topsy com butterfly   gecko

 

extensions

our classification efforts were highly successful  but
there is still much to be done  the mozilla corporation has expressed interest in using maul in its
metrics   analytics group  so we hope that work on
this topic will continue  user agent classification is
a rich and nuanced problem  and there are a number of interesting extensions that we did not have the
resources to explore  we list a few of them here 

   

improved cross validation and
tokenization

we did some cursory testing to select our set of token delimiters  but it was far from a rigorous process 
it would be interesting to use formal cross validation
and feature selection techniques to select the optimal
separators 
also  our attempt to cross validate the input kernel parameters could be improved by using k fold
 

fi   

cross validation  instead of a held out test set  

larger datasets

our training set was large  but not exhaustive  there
are many other sources of user agent data on the
the svm treats all tokens as equally distinct  regard  web  including sites like agentarius net  as mentioned
less of the true similarity between any two tokens  we above   and we believe that the size of our dataset
implemented some manual coalescing of tokens  for could increase by an order of magnitude with reasonexample  we replaced any integer with its number of able effort 
digits  ie     and    are treated as the same number  
nonetheless  there are many other approaches worth
exploring 
  summary of results

   

improved token coalescence

 other common patterns  for example  email addresses 
a new class of text data has been successfully classified using svm techniques and the results of using
 coalescing the most infrequent tokens into a
several kernels have been compared  little improvesingle other token
ment is found over using the linear kernel  a tok using one of the string kernels to identify the enization scheme is described that allows for large
nearest neighbor to an infrequent token  this speed improvements in using string based kernels 
would allow for user agent strings with new an easy to use python based framework was develtokens to be classified  despite having tokens in oped that allows for rapid training and testing using
the classifier feature set 
a modified libsvm string library 

references
    http   www useragentstring com   web            
    header examples  http   www betavine net bvportal resources vodafone mics examples   web       
    browserscope  uaparser  source repository  https   github com dinomite uaparser  october      
    chih chung chang and chih jen lin  libsvm  a library for support vector machines        software
available at http   www csie ntu edu tw  cjlin libsvm 
    fielding et al  hypertext transfer protocol  http      rfc       network working group       
 section        
    ralf herbrich  learning kernel classifiers  mit press       
    gary keith  browser capabilities project  http   browsers garykeith com             
    scott j  lecompte  http   www ua tracker com  web       
    huma lodhi  craig saunders  john shaw taylor  nello cristianini  and chris watkins  text classification using string kernels  journal of machine learning research                 
     jaroslav mallat  http   www user agent string info  web       
     steve souders  user agents in the morning  http   www stevesouders com blog            useragents in the morning   january      
     steve souders  ua profiler  http   www stevesouders com ua        
     andreas staeding  http   www user agents org  web       
     guo xun yuan  libsvm string  an extension to libsvm for classifying string data        software
available at http   www csie ntu edu tw  cjlin libsvmtools  libsvm for string data 
     nicholas c  zakas  history of the user agent string  http   www nczonline net blog            historyof the user agent string    web  january      

 

fi