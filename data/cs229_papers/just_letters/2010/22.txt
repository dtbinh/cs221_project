multiclass clustering using a semidefinite relaxation

jason lee
institute of computational and mathematical engineering
stanford university
email  jdl   stanford edu

abstract
we propose a semidefinite relaxation for graph clustering known as max cut clustering  the clustering problem is formulated in terms of a discrete optimization
problem and then relaxed to a sdp  the sdp is solved using a low rank factorization trick that reduces the number of variables  and then using a simple projected
gradient method  this is joint work with nathan srebro at the toyota technology
institute chicago and part of research was performed at the toyota technology
institute chicago 

 

introduction

graph clustering is often formulated as a discrete optimization problem with the goal of balancing
two criteria  cluster coherence and cluster size balance  we first discuss the specific case of binary
clustering to highlight the tradeoffs between cluster coherence and cluster size  we then formulate
this tradeoff as a discrete quadratic integer program and discuss the connection with the max cut
problem  as with max cut  the discrete problem can be relaxed to a semidefinite program  sdp 
that can be solved with standard solvers  to make the algorithm scalable  a low rank factorization
approach similar to     is used to solve the sdp 
   

graph clustering

there are many possible clustering objectives that have been proposed in the literature and it is simple to construct new objectives that achieve the two desired properties of cluster size and quality 
the two most common formulations are known as the ratio cut     and normalized cut        
both objectives are discrete and known to be np hard  however there is a continuous eigenvalue
relaxation of the problems which leads to spectral clustering on a graph laplacian  due to the ease
of computing eigenvectors and the intricate connection with spectral graph theory  graph laplacian
relaxation methods are commonly used  see     for further discussion and comparison of graph
laplacian based clustering methods  a problem with the eigenvalue approach is the eigenvalue relaxation is a very loose approximation to the original discrete optimization problem  several authors
have proposed tighter relaxations using semidefinite programming         but these methods only
scale to small size problems  instead of trying to design better clustering objectives  which has
been extensively studied  we propose a simple discrete objective  that admits a solvable  yet tight
relaxation through the max cut sdp 

 
   

binary clustering
max cut problem

the well studied max cut problem  partition the vertices into c and c such that the number of
edges between c and c is maximized  can be formulated as the following binary quadratic integer
 

fiprogram 
maximize

  xx
qij     xi xj   subject to x i      for all i 
  i j

   

the max cut problem attempts to maximize the number of edges cut  so a first attempt at using it
for clustering is to let q   w   however  this leads to the trivial solution of c   v   a simple fix
is to define q   j  w   eet  w for some       where j is the matrix of all ones and e is
the vector of all ones  this choice of q forces balanced clusters while minimizing the inter cluster
edges  thus we solve the following problem  which is equivalent to max cut with q   j  w  
x
maximize
 wij   xi xj subject to x i     
   
i j

p
p
p
the objective function can be rewritten as i j wij xi xj    l xl     the term   l xl    can
be viewed as a penalty function for unbalanced clusters since perfectly balanced clusters satisfy
p
l xl      using this observation  equation   is equivalent to
x
x
wij xi xj subject to x i     and  
xi     b
   
i j

i

thus equation   is an example of a bi criterion objective  maximizing the objective leads to maximizing w  c  c  w  c  c  while minimizing  c  c  
we can easilyp
modify the penalty function
p
to penalize the difference in volume by replacing   i xi     b with   i di xi     b 
   

semidefinite relaxation

the discrete optimization problem proposed in equation   is known to be np hard to solve  so we
reforumlate it as a continuous problem 
x
maximize
 wij   xij subject to x   xxt and xii     
   
ij

this reformulation is equivalent to the discrete problem  however  it is non convex due to the rank
  constraint on x  by dropping this constraint  we arrive at the semidefinite relaxation of max cut 
x
maximize
 wij   xij subject to xii      and x   
   
ij

the semidefinite relaxation is a convex problem that can be efficiently solved using standard interior
point solvers  unfortunately  these solvers do not scale to problems with more than a few hundred
variables  to motivate our solution method  we first study an equivalent vector formulation of the
semidefinite relaxation 
x
maximize
 wij      vi   vj   subject to   vi         and vi  rn
   
ij

in the vector formulation  each binary variable xi is replaced with a vector vi  rn   the variables
in the vector formulation is v  rnxn where vi are the rows of v   this semidefinite program has
n  variables  a key idea from burer and monteiro     is that the number of variables can be reduced
if we constrain each vi  rr for r   n  for r      the rank constrained formulation is recovered 
burer and monteiro show that if r is large enough the solution to the non convex problem with
vi  rr is equivalent to the global optimum of the sdp  equation     the final reformulation that
we solve is the rank r constrained relaxation to the discrete problem 
maximize tr  w  j v v t   subject to   vi       
   

and

v i  rr

   

projected gradient method

to solve equation    we use a simple projected gradient algorithm  the projected gradient algorithm
updates with the rule
v  p v     w  j v  
   
where p can be computed by normalizing the rows of v   this algorithm is extremely simple and
efficient  each iteration involves matrix multiplication and normalizing the rows of v   the storage
required is nr variables and in the experiments we choose r      furthermore  we are guaranteed
the global optimum of the sdp formulation if rk v      r     
 

fi   

max cut clustering

we first formulate the max cut clustering as a discrete problem of the form   and then employ the
same relaxation as described in the binary case  let k denote the number of clusters and xia       
be
p cluster indicator variables where xia     means node i belongs to cluster a  the xia satisfy
a xia      k to ensure each node belongs only to one cluster  a binary quadratic program for
multiclass clustering can be posed as 
x
maximize
wij ab xia xjb
   
ij ab

subject to
x

xia      k 

a

    
x
i

xia

n
      k 
k

and

x ia    

    

wij ab     a   b wij where wij is the weighted adjacency matrix  the second constraint is
a cluster size constraint thatpforces each cluster topbe of similar size  using a quadratic penalty
function on the constraints a xia      k and i xia       k  nk   this can be converted to a
max cut type problem similar to equation     similarly  a low rank factorization type relaxation can
be used to solve this  the final problem in vector form is 
x
maximize
 wij ab   a   b   i   j     via   vjb   subject to   vi         and vi  rr
ij ab

    
this equation can be rewritten in the form of equation   by defining dij aa     and qii ab      d
is rank k and q is sparse  the objective is tr  w  d  q v v t  
   

computational considerations

in the binary case  the algorithm requires a matrix matrix multiplication at each iteration  the
required work per iteration is o n  r  and the storage required is the matrix v which is o nr  
where n    v   
in the multiclass case  the algorithm also requires a matrix matrix multiplication at each iteration 
the required work per iteration is o n  k   r  and the storage required is o nkr   the adjacency
matrix w is frequently sparse and the two penalty terms are sparse and rank k  respectively 
   

recovering the discrete solution

     

rounding scheme

after solving the optimization problem given in equation    we have a vector vi for each vertex 
the goemans williamson randomized hyperplane rounding assigns xi   sgn   r  vi    where
r  n     i   we repeat randomized hyperplane rounding r times and choose the assignment with
largest objective as the final clustering  in the multiclass setting  the cluster labels are chosen as
ci   argmaxa   r  via    this is repeated r times and choose the assignment with largest
objective  we also post process the best label selected by hyperplane rounding using the relaxation
labeling method developed for inference in markov random fields  see     for details 

 

experimental comparison

for all the experiments  we first build a k nearest neighbor graph with k      and weights wij
defined as wij   max si  j   sj  i    with si  j    exp 
from xi to its k nearest neighbor 
 

  xi xj    
 
 i 

and i equal to the distance

p
in the conversion to a max cut problem  we omit terms of the type ia ia xia i e  linear terms in xia   the
linear terms can be handled within our p
low rank factorization framework by introducing the dummy variable
x  and replacing all linear terms with ia ia xia x    this observation allows us to do map estimation in
markov random fields using the same low rank relaxation technique  in the multi class clustering case  the
linear terms correspond to uniform priors over the label set and thus do not change the solution  see     for
details  experimental results for the mrf are not reported here due to lack of space 

 

fidescription
dataset
pdigit      
pdigit      
pdigit           
pdigit           
pdigit           
pdigit               
pdigits                 
mnist           
mnist               
mnist           
mnist           
mnist                     
mnist           
mnist               
mnist           
mnist                   
mnist       
mnist       
mnist         
newsgroupsa      
newsgroupsa      
newsgroups       
newsgroups         
newsgroups            
newgroups            

nc
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

k
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 

max cut
    
 
     
    
     
     
       
    
    
     
     
     
    
      
     
      
      
      
      
    
      
    
      
     
      

misclassification rate
njw k  sm k  njw  k   
     
     
     
 
 
 
     
     
     
     
    
     
     
     
     
     
     
     
     
   
      
     
     
     
             
    
     
     
    
     
     
     
     
     
     
     
    
    
             
    
     
    
    
             
      
             
      
     
     
      
             
      
    
     
     
     
    
     
             
      
     
     
    
             
     
      
    
    

sm k   
     
 
    
    
     
     
      
    
    
     
    
     
    
      
     
      
      
      
      
    
      
      
    
      
      

table    nc is number of points per cluster and k is the number of clusters    different methods
are compared  our low rank method  shi malik spectral clustering with k eigenvectors  ng jordanweiss spectral clustering with k eigenvectors  shi malik spectral clustering with k     eigenvectors 
and ng jordan weiss spectral clustering with k     eigenvectors 

we test on the following datasets 
   pendigits        handwritten digit dataset  each data is   x y plane measurements of the
pen position 
   mnist        handwritten digit dataset  each data point is a    x    image of a single
handwritten digit 
      newsgroups  each data point is a vector of term frequency  data is collected from   
newsgroups on different topics 
   synthetic two moons dataset  this experiment is reproduced from      
for all the experiments         and   
r      

wtot
  v     

the algorithm is run for      iterations with

we compare against   different variants of the spectral clustering algorithm by shi malik and ngjordan weiss with    replications of k means  the spectral clustering is run with k and k    
eigenvectors    the clustering accuracy is evaluated by the number of mis clustered points each
algorithm makes  the lowest among all k  permutations is reported  
 
spectral clustering is generally done with k eigenvectors where k is the number of classes  we also test
with k     because many of the errors in spectral clustering are due to the k and k     eigenvalue being very
close  in fact  spectral clustering with k     eigenvectors does better in our experiments 

 

fi a  spectral clustering

 b  max cut clustering

figure    comparison of spectral clustering  left  with

max   cut

clustering  right  

references
    samuel burer and r  d  c  monteiro  a nonlinear programming algorithm for solving semidefinite
programs via low rank factorization  mathematical programming  series b                   
    l  hagen and a b  kahng  new spectral methods for ratio cut partitioning and clustering  computeraided design of integrated circuits and systems  ieee transactions on                   september
     
    jianbo shi and jitendra malik  normalized cuts and image segmentation  ieee transactions on pattern
analysis and machine intelligence                     
    andrew y  ng  michael i  jordan  and yair weiss  on spectral clustering  analysis and an algorithm  in
advances in neural information processing systems  pages         mit press       
    ulrike von luxburg  a tutorial on spectral clustering  statistics and computing                  
        s               z 
    eric xing  eric p  xing  michael jordan  and michael i  jordan  on semidefinite relaxations for normalized k cut and connections to spectral clustering       
    tijl de bie and nello cristianini  fast sdp relaxations of graph cut clustering  transduction  and other
combinatorial problems  j  mach  learn  res               december      
    m  pawan  kumar v  kolmogorov  and p  h  s  torr  an analysis of convex relaxations for map estimation 
     
    timothee cour and jianbo shi  solving markov random fields with spectral relaxation  in proceedings of
the eleventh international conference on artificial intelligence and statistics  volume          
     jason d  lee  benjamin recht  ruslan salakhutdinov  nathan srebro  and joel a  tropp  practical largescale optimization for max norm regularization  in advances in neural information processing systems  mit press       

 

fi