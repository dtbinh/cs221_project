beating the ncaa football point spread
brian liu
mathematical   computational sciences
stanford university

patrick lai
computer science department
stanford university

december         

 

introduction

over recent years  professional sports betting has grown to be a multi billion dollar industry  college football is one of the most popular sports that individuals bet on and has become a major contributor to the
overall size of the sports betting market  there exists many different types of sports bets that can be made
on college football games  the most popular one is the point spread bet  point spread betting involves a
number known as the spread which represents how much a team is favored to win by  for example  if a
team has a spread of    then the team is favored to by win   points  therefore  a person placing a bet on
a team with a spread of    will only win if the team wins by more than   points  the purpose of the point
spread is to create a fair betting market by offsetting a priori biases towards a given team  point spreads
are not static values in that sports bookmakers continually update point spreads to account for changes in
the betting market  in theory  either team in a given match up would have roughly a     chance of winning
when the spread is taken into account  expert sports bettors believe that achieving a consistent success rates
of     or higher has become extremely difficult as bookmakers have begun to employ more sophisticated
techniques in creating spreads      another type of sports bet is the straight bet  this is simply a bet on
which team will win a given match up without regard to the spread  in this project we focus on both the
point spread bet and the straight bet 
the goal of this project is to learn  explore and apply modern machine learning techniques to a data set of
ncaa division i fbs football statistics in order to predict the outcome of a given match up between two
opposing teams  we wish to devise a binary classification system that can consistently predict match up results at an error rate that is significantly better than random guessing and at a level that is competitive with
expert sports bettors  specifically  we will experiment with three popular binary classification algorithms 
logistic regression  support vector machines  with different kernels  and adaboost  different variants   we
test these algorithms by training models for predicting the winning teams in games with point spreads and
in games without point spreads 

 

data set

we obtained our data set from two sources  the detailed team statistics for each game from the      and
     seasons were scraped from http   www cfbstats com  a college football statistics repository  as
the data is not provided in a format that can be easily downloaded  we resorted to downloading the entire
website page by page for offline processing  we then created a html scraper in java to extract  format 
combine and transform the statistics into consistent and easily consumable format  the point spreads for
each game from the      and      seasons were retrieved from http   www sbrlines com  a sportsbook aggregation site  we extracted the team identifiers  game date and the opening point spreads from
this source and matched the point spreads to specific teams using the team identifiers and the game date 

 

firesulting in our raw data set  the raw data set is then processed with a java program and used to generated
training and testing examples  finally  the examples are saved in a csv format usable by our machine
learning algorithms  note that although we gather data for both the      and      seasons  we only use the
     season in experimenting with machine learning algorithms  the data from the      season is used as
initial values for computing examples for games that occur early in the      season 
one problem we came across in gathering our data set was missing values in the data  however  we noticed
that all of the missing values came from events which occur very rarely in college football games  these
rare events usually dont affect the outcome of a game  we simply disregarded these rarely occurring events
since we suspect that they would ultimately have very low prediction power in our models 

 

features

the raw data set contains    fundamental college football statistics that captures each teams performance
in a given game  using the raw data set  we could easily compute new features and generate examples
for feeding into our machine learning algorithms  our initial examples were generated by first computing
a cumulative average of the statistics in order from earliest games to most recent games  this gave us an
average of a teams statistics up to a specific week in the season which could then be used in predicting
future games the team plays in  this method of generating examples prevents us from making predictions
on games using statistics generated in future games  which would distort our test error  for example  if
we wanted to predict the outcome of a game at week k  we can use the teams cumulative averages from
week k     which we know doesnt incorporate future statistics  to train our model  for each game we
create an example by concatenating the two teams cumulative statistics up to  but not including  the date
of the game  this resulted in a total of     examples with     features per example  features besides the
fundamental statistics were added   each example is labeled   if the home team wins and   if the away
team wins 
passes attempted
passes completed
passes intercepted
pass completion percentage
pass rating
passing touchdowns
passing yardage
rushes attempted
rushing average
rushing touchdowns
rushing yardage

interceptions
interception yardage
passing touchdowns allowed
passing yards allowed
rushing touchdowns allowed
rushing yards allowed
sacks
sack yardage
tackles for loss
tackles for loss yardage
forced fumbles

points gained
points allowed
red zone scoring percentage
red zone field goal percentage
red zone touchdowns allowed
red zone field goals allowed
third down conversion percentage
third down conversion percentage allowed
quarterback hurries
passes broken up
kicks or punts blocked

figure    a subset of the fundamental statistics used as features

   

momentum

one important aspect that can determine the outcome of a game is a teams momentum going into the
game  momentum shifts can occur abruptly and last for several games  e g  a change in team strategy
or an important player gets injured  we used two techniques to account for momentum in our models 
first  instead of using a cumulative average of the fundamental statistics we use a k game moving average
of the fundamental statistics  second  we computed    new features for each team that track momentum at
different time horizons 

 

fiaverage wins in last   games
average wins in last   games
average wins in last   games
average wins with the spread in last   games
average wins with the spread in last   games
average wins with the spread in last   games

average losses in last   games
average losses in last   games
average losses in last   games
average losses with the spread in last   games
average losses with the spread in last   games
average losses with the spread in last   games

figure    momentum tracking features

 

machine learning algorithms

in this project we experimented with three popular binary classification algorithms  each of these machine
learning algorithms are different in how they learn from data and will allow us to draw deeper insight into
our problem 

   

logistic regression

logistic regression is a very popular machine learning algorithm  its simplicity serves as a good point of
comparison for our other two machine learning algorithms  we used an implementation of logistic regression
 generalized linear model  included in matlabs statistics toolbox     

   

support vector machine

soft margin support vector machines are seen by many as the best off the shelf machine learning algorithm 
we tested three of the primary svm kernels  linear  polynomial   rd degree  and radial basis function
        in order to select the optimal svm  we varied the box constraint parameter c and selected the
value that gave us the lowest test error  this parameter selection technique also served to prevent overfitting
the data  we used an implementation of svm included in matlabs bioinformatics toolbox     

   

adaboost

we experimented with two variants of adaboost  gentleboost and modestboost  gentleboost is a more
robust and stable version of the original adaboost algorithm  empirically  gentleboost performs slightly
better than the original adaboost on regular data  but is significantly better on noisy data and is more
resistant to outliers  modestboost is a regularized version of the original adaboost that has better generalization  for our weak learner we chose decision trees  cart  with two split points  we boosted these
weak learners for     boosting rounds  selected to prevent overfitting  to obtain our final classifier  boosted
decision trees select relevant features automatically  making it an interesting algorithm to use on our data
which has a feature set that may contain irrelevant features  we used an implementation of gentleboost
and modestboost included in an adaboost matlab toolbox     

 

evaluation methodology

we estimate generalization error using hold out cross validation where our data is split into     training set
and     test set  all test set examples are from games that occur after games in the training set examples 
this is necessary to ensure that we arent distorting our generalization error estimate by training on data
generated in future games  we also compare our results from the machine learning algorithms with random
guessing  always predicting the home team will win and always predicting the away team will win 

 

fi 

results
always home
always away
random
logistic
svm  linear 
svm  poly 
svm  rbf 
gentleboost
modestboost

train error
      
     
  
  
  
     

test error
      
      
      
      
      
      
      
      
      

always home
always away
random
logistic
svm  linear 
svm  poly 
svm  rbf 
gentleboost
modestboost

 a  results for straight bets

train error
      
      
  
  
  
  

test error
      
      
      
      
   
      
      
      
      

 b  results for point spread bets

figure    game outcome prediction results

 

analysis of results

our results show that we are able to successfully predict the outcome of straight bets with an accuracy of
    using gentleboost  this result is slightly more accurate than previous work that has been done in
predicting outcomes of other sports games         in the case of predicting point spread bets  svm with
a  rd degree polynomial kernel performed the best  we were able to beat random guessing by    and
achieve an accuracy of      this accuracy  which may seem low  is competitive with professional college
football bettors who consistently achieve accuracies of     in betting on college football point spread games
     it is interesting that were able to achieve much higher accuracy in predicting the straight outcomes
of games compared to games with points spreads  intuitively this makes sense because the purpose of the
point spread is to create a market such that either team has roughly a     chance of winning  however 
we wanted to investigate our data further to determine why this may be the case  we plotted the outcomes
of games against the first two principal components in order to draw insight into why we were not seeing
better performance in predicting the outcomes of games with point spreads 

 

axis  

 

 

 

home team loses
home team wins

 

 

 

 
 
 
 

axis  

 

 

 

home team loses
home team wins

 

 

 

 

 

 

 

 

axis  

 

 

 

 

 

 

axis  

 a  game outcomes without point spreads

 b  game outcomes with point spreads

figure    game outcomes against top two principle components

 

filooking at figure   a  we can see that there is a difference in the distributions of wins and losses from
straight outcome games along the second principal component  y axis   it seems that more losing labels are
cluster towards the top and more winning labels are clustered toward the bottom  thus  we would expect a
binary classification algorithm to find a decision boundary and allow us to predict with good accuracy when
we have a win or a loss  figure   b  shows the top two principle components labeled with outcomes of games
with point spreads  in this plot  it is almost impossible to differentiate between the two distributions as
winning and losing labels are evenly mixed  both groups of data seem to be similarly distributed along the
top two principle component axes where variance should be highest  in this case any binary classification
algorithm would have a very difficult time separating the data because  according to the features we defined 
both groups are distributed similarly 

 

conclusion

though we were only able to beat random guessing by    in predicting point spread games  our results were
still competitive with expert college football bettors  we believe the primary reason our results were not
better is due to the set of features we used to train our machine learning algorithms  alternatively  the fact
that it is difficult to predict outcomes of games with point spreads may suggests that that market for point
spread bets are efficient in that it is impossible to predict using historical data 

   

future work

an interesting avenue to explore in the future would be to work on constructing and discovering new features
that can more accurately capture the effects of real game situations to the winning losing margin of games 
from our analysis of the first two principle components of the data  it is apparent that our current set of
features does not capture enough information to allow us predict point spread bets no matter which binary
classifier we choose  some additional sources of data such as opponent rankings and play by play statistics
may help in constructing features that will be useful for predicting outcomes of games with point spreads 

references
    alexander vezhnevets  gml adaboost matlab toolbox      http   graphics cs msu ru ru science 
research machinelearning adaboosttoolbox
    dr  bob sports  about dr  bob  http   www drbobsports com about cfm
    dr  bob sports  best bets  http   drbobsports com analysis cfm
    hamadani babak  predicting the outcome of nfl games using machine learning  project report for machine learning  stanford university  http   www stanford edu class cs    proj     
babakhamadani predictingnflgames pdf
    kevin gimpel  beating the nfl football point spread  project report for machine learning  carnegie
mellon university  http   www cs cmu edu kgimpel papers machine learning project      pdf
    matthew beckler  hongfei wang  michael papamichael  nba oracle  project report for machine learning  carnegie mellon university  http   www mbeckler org coursework                 report pdf
    the mathworks  inc  statistics toolbox      http   www mathworks com products statistics
    the mathworks  inc  bioinformatics toolbox      http   www mathworks com products bioinfo

 

fi