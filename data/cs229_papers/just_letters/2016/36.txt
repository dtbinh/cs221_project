learning hypernymy in distributed word vectors
via a stacked lstm network
irving rodriguez irodriguez stanford edu
june        
abstract
we aim to learn hypernymy present in distributed word representations using a deep lstm neural network  we hypothesize that the semantic information of hypernymy is distributed differently across the components of the hyponym and hypernym vectors for varying examples of
hypernymy  we use an lstm cell with a replacement gate to adjust the
state of the network as different examples of hypernymy are presented  we
find that a seven layer lstm model with dropout achieves a test accuracy
of       on the linked hypernyms dataset  though further comparison
with other models in the literature is necessary to verify the robustness
of these results 

 

introduction

in the last several years  distributed word vectors have shown the ability to learn
semantic and syntactic information without receiving explicit inputs outlining
these properties  particularly  both the word vec    and glove     models have
shown that distributed word vectors cluster together in space based on linguistic
similarities and that these relationships can be quantitatively captured with
simple vector algebra and analogy tasks 
how these properties manifest themselves within the word vectors  however 
is not well understood  this is especially true of paradigmatic linguistic relationships like hypernymy and synonymy  there is significant linguistic benefit
in understanding these complex properties   a robust hypernymy model would
greatly facilitate automatic taxonomy construction  for example  and wolter and
gylstad have noted that paradigmatic relationships tend to be similar across
languages and could be used to improve machine translation models     
previous models have failed to capture the full linguistic complexity of hypernymy  these models often rely on supervised methods that are not linguistically
consistent with the properties of hypernymy  namely asymmetry  transitivity 
and hierarchy  as such  we attempt to remedy these shortcomings through a recurrent neural net that maps a hyponym vector to one of its hypernym vectors 
specifically  we set out to learn the mapping from a hyponym to a hypernym
 

fivector based strictly on the components of the distributed vectors with a stacked
lstm network 

 

methods

we set out to build a deep recurrent neural network to learn a function h hwo
that produces the hypernym vector given the input hyponym vector of word wo  
hwo  

   

stacked lstm model

a long short term memory  lstm  models seem well suited for our task  we hypothesize that hypernymy  and other linguistic relationships which distributed
vector models seem to automatically learn  are baked directly into the individual components of the vectors  the lstm architecture contains a cell state
ct at each given time step t  in our case  this represents our activation matrix
which maps a hyponym vector to its hypernym vector  the hidden layer then
consists of four calculations dubbed the forget  input  activation  and output
gates  at each time step  we input the hyponym vector hto into the first cell 
it    wi h t  e   wi hto   bi  
ft    wf h t  e   wf hto   bf  
ot    wo h t  e   wo hto   bo  
ct   tanh wc h t  e   wi hto   bc  
where i  f   o  c denote the forget  input  and output  and activation gates
with their respective bias terms  and h t  e denotes the predicted hypernym in
the previous time step 
in essence  each of these gates calculates which components of the vectors
within the cell carry meaningful information and updates the cell state accordingly  the forget gate scales down the components of the input state ct   
the input gate scales the components of wt to be added to the cell state  the
activation gate determines the new component values to be added to the cell
state  and the output gate yields a vector to scale the predicted hypernym 
our lstm cell combines the input and forget gates into a replacement gate
which only adds the output of the input gate to the cell state in components
which were forgotten  we then update the cell state and calculate the predicted
hypernym 
ct   ft  ct        ft    ct

   

hte   ot  tanh ct  

   

 

fiwhere  denotes element wise multiplication 
we hypothesize that the input  forget  and output layers should be able to
identify the components of the hyponym vectors which best predict hypernymy
and update the weights accordingly  since hypernymy is hierarchal and a single
word can have hypernyms on varying levels of generality  these components
could change from example to example but should be handled appropriately at
each step t in the replacement gate 
we minimize the quadratic loss function over our training set using minibatch sgd
 
  hi  hwe     
   
j   m
  i   e
and optimize in the number of layers in the model and the number of hyponymhypernym pairs processed at each iteration of mini batch sgd 
in order to prevent overfitting and improve our models robustness outside
of the validation set  we introduce dropout in between the penultimate and final
layers for our best performing model 
we implement our stacked lstm model using tensorflow 
   

   

piecewise projection model

we compare our performance to the supervised model outlined by fu et  al    
to replicate results  we use k means clustering in order to cluster the vector
difference hwo  hwe into k clusters for each word pair  wo   we   in our dataset 
for each cluster  we then learn a linear projection j  hwo   that maps a hyponym
to its hypernym and classify positive pairs when  j  hwoi    hwei      for some
hypersphere radius  in our difference vector space  we minimize the quadratic
loss over the dataset to learn the projections 

   

distributed word vectors

we first train highly dimensional word vectors using the publicly available word vec
module     due to time constraints  we use a single optimized cbow model to
obtain our pre trained vectors for use as inputs into our lstm network 
our model uses mikolov et  als method for consolidating common phrases
into a single token       we thus learn an individual vector for these phrases
 consisting of up to   words  like new york times  and treat them as a single
hyponym or hypernym 

   

data

the linked hypernyms dataset  lhd  provided by kliegr as part of the dbpedia project contains    m hyponym hypernym pairs       these pairs are
generated by parsing the first sentences of wikipedia articles for syntactic patterns  i e  x is a y  for hypernyms matching the article title 

 

fithe lhd set contains tokens for phrases longer than   words  like history of the united states  and also contains many words which are not in the
vocabulary of our vector model  as such  we prune these pairs from the set 
this leaves roughly     million hyponym hypernym pairs  we then sample    
of the remaining pairs to and evenly split them for use as our validation and
test sets 
we also evaluate the performance of our model on the bless dataset used
in previous hypernymy classifiers           the dataset contains    k example
pairs parsed from the wackypedia corpus using similar syntactic patterns  we
attempt to perform the same    fold cross validation in order to compare our
accuracy with these classifiers 
the most common tokens for both datasets are shown in appendix a  both
of these datasets are publicly available          

figure    plots of the   principal components for each data set  left  bless 
right  linked hypernyms

 
   

results
word vector model

we trained a skip gram model     choosing the set of vector dimensions  window
size  and negative sampling size  denoted d  w  sn   respectively  based on the
performance of the given model on the analogy task provided in the google
word vec implementation      we use the july      english wikipedia dump
as the training set for our word vectors 
the parameters which achieve the highest accuracy are d        c  

   sn       fig    

   

piecewise projection model

we trained our piecewise projection model using our pre trained word vec vectors  we reserve     of the data for our validation and test sets and training on the remaining pairs  we choose the optimal k   based on the f 
 

fifigure    analogy task accuracy of word vector models  the accuracy of the
model with optimal parameters is shown as the black dot  parameters are fixed
at  d        c      sn      
score of each model with    fold cross validation on the training set and find




        fig    
      lhd
       and  klhd
      bless
 kbless
we then use the optimal learned parameters to evaluate the model on the
respective test set  we report an f  score of       on the bless set and      
on the lhd set 

figure    f  score of training phase for k clusters and a threshold   left     k
pairs  bless  right     m pairs  lhd 

   

stacked lstm model

for the stacked lstm model  we vary the number of layers from nl     to   
additionally  we tune the number of examples fed into the mini batch sgd at
any iteration  due to time constraints  we only add dropout to the two models
with the highest validation accuracy  we then test these models on the bless
set instead of tuning them once more on the validation set of bless 
for a given output vector  we count correct predictions as hypernym tokens
that one of the three vectors in our word vector space with the highest cosine
similarity to the predicted vector 
we find that that the models with bs      and nl        perform best on
the lhd set  we report the test accuracies of these models and their dropout
counterparts in figure   

 

fifigure    test accuracies of models with highest validation performance on both
the bless and lhd sets  f  score for piecewise projection classifier 

figure    accuracies for the piecewise projection and stacked lstm models
over the lhd set 

 

fi 

discussion

our stacked lstm model has achieved moderate results on both bless and
lhd  though they are difficult to compare due to the widely varied results of
the piecewise projection model on the two datasets 
the small size of the bless set  the abundance of many general words
 object  artifact  and the repetition of several hyponyms  castle  glove  cottage 
are likely biasing both models  it is clear that the lstm model without dropout
is overfitting on the bless set because of its small size relative to the number
of total cells in the network  though the model still performs well on the
test set  this is likely due to the test set containing words which also appear
in the training and validation sets  table    whose hypernyms the model has
memorized 
on the other hand  it seems that the piecewise projection is also simply
learning features of the words in the bless set  for a low threshold  increasing
the number of clusters greatly increases the models f  score on the validation
set  figure     we would expect only a modest increase as our number of clusters
approaches some true value that describes the different levels of hypernymy
that exist in english  nevertheless  all values of k converge to the same f  score
on both the bless and lhd sets  and we find two very different optimal values
of k for both sets  this suggests the model is not learning linguistic information
about hypernymy but is rather optimizing for the structure of the bless set 
nonetheless  the test accuracy that the seven layer lstm achieves on the
lhd set shows promise for learning a hypernymy mapping  the neural net
should improve  or stabilize  as the dataset grows  especially if dropout prevents
it from memorizing as it trains on more data  the significant positive offset
in validation accuracy during training as the batch size is increased suggests
that the model may benefit from seeing more levels of hypernymy in a single
iteration  though the information that the network state receives at a given
time step from previous time steps is not particularly clear  the broad diversity
of tokens present in the lhd set suggests that hypernymy is indeed complex
but not so broad that it manifests itself differently in word vectors in different
domain spaces and across pairs of varying generality  the model was able to
pinpoint the location of the inputs hypernym with some success in spite of these
complexities without making any assumptions about hypernymys behavior  like
the piecewise projection model does 
we note that the model still has a major shortcoming in that it only predicts a single hypernym vector for a given hyponym  hypernymy  however 
is a many to one mapping  and several papers     have shown that the hypernyms of a single noun do not cluster together in vector space  as such  this
stacked lstm model  if successful  is still unable to disambiguate between the
many hypernyms that a word may have  for example  there is little recourse
for choosing between fruit and food for the word apple and also for a
polysemous hypernym like company  even so  it would be possible train a
model for different domains  more generally  the model could use the predicted
vector to generate a probabilistic distribution of possible hypernyms and con 

fidition on other context words  say  those that appear with the hyponym in a
given sentence  to select the most likely hypernym from this group 

 

conclusion

we note that future studies are necessary in order to verify and improve the
validity of our stacked lstm model  if the model indeed succeeded in learning
a hypernymy mapping  it should perform well at predicting the hypernyms of
a noun hierarchy  as such  a well documented hypernymy tree like that of
wordnet would serve as a reliable benchmark for testing this model 
additionally  further tests can use different word vector models to see if
other learned representations  such as those in glove  are better at learning
semantic relationships which are more easily parsed from the vector components 
if the lstm cell is indeed capable of identifying these relationships  similar
models could be built for other semantic relationships like synonymy 

references
    m  abadi  a  agarwal  p  barham  e  brevdo  z  chen  c  citro  g  s  corrado  a  davis  j  dean  m  devin  s  ghemawat  i  goodfellow  a  harp 
g  irving  m  isard  y  jia  r  jozefowicz  l  kaiser  m  kudlur  j  levenberg  d  mane  r  monga  s  moore  d  murray  c  olah  m  schuster 
j  shlens  b  steiner  i  sutskever  k  talwar  p  tucker  v  vanhoucke 
v  vasudevan  f  viegas  o  vinyals  p  warden  m  wattenberg  m  wicke 
y  yu  and x  zheng  tensorflow  large scale machine learning on heterogeneous systems        software available from tensorflow org 
    e  a  et  al  a study on similarity and relatedness using distributional and
wordnet based approaches 
    m  b  et  al  how we blessed distributional semantic evaluation 
    r  f  et  al  learning semantic hierarchies via word embeddings 
    t  m  et  al  distributed representations of words and phrases and their
compositionality 
    t  m  et  al  efficient estimation of word representations in vector space 
    y  b  et  al  a neural probabilistic language model 
    y  goldberg and o  levy  word vec explained  deriving mikolov et al s
negative sampling word embedding method 
    google  word vec  tool for computing continuous distributed representations of words 

 

fi     t  kliegr  linked hypernymys  enriching dbpedia with targeted hypernym
discovery 
     n  nayak  learning hypernymy over word embeddings 
     f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion  o  grisel 
m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikitlearn  machine learning in python  journal of machine learning research 
                  
     j  pennington  r  socher  and c  d  manning  glove  global vectors for
word representation  in empirical methods in natural language processing
 emnlp   pages                
     j  weeds  learning to distinguish hypernyms and co hyponyms 
     b  wolter and h  gyllstad  collocational links in the l  mental lexicon
and the influence of l  intralexical knowledge       

 

fiappendices
appendix i

here  we present a list of some properties of the bless and weeds datasets 

  

fibless hypernym

weeds hypernym

artifact     

confederate    

object     

anthropology    

animal     

knoll    

creature     

artillery    

chordate     

feature    

vertebrate     

sociability    

food     

caterpillar    

device     

therapy    

produce     

delight    

beast     

fleck    

implement     

truthfulness    

good     

murder    

mammal     

letter    

commodity     

voting    

vehicle     

significance    

bless hyponym

weeds hyponym

castle     

stock    

hat     

gown    

glove     

beachhead    

bomber     

throbbing    

cloak    

matron    

robe     

codification    

blouse     

anglican    

sweater     

underbrush    

vest     

scratch   

dress     

skit    

shirt     

livelihood    

fighter     

adjective    

sweater     

manager    

cottage     

nursery    

table    dataset comparison the top    most common hyponyms and hypernyms in the bless and weeds data  the bless data contains      pairings
while the weeds data contains         

fihyponym

hypernym

saw

artifact

banana

food

rat

rodent

beetle

arthropod

jacket

clothing

radio

system

dress

good

cabbage

food

rat

rodent

cat

beast

frigate

ship

car

vehicle

saw

artefact

saw

object

bear

vertebrate

bear

chordate

coat

clothing

salmon

food

hotel

building

pistol

device

jacket

commodity

sword

implement

bear

animal

bear

creature

cabbage

veggie

mackerel

chordate

pine

tree

flute

artifact

saw

utensil

cat

mammal

table    indicative pairs  all indicative pairs for k      clusters 
  

fi