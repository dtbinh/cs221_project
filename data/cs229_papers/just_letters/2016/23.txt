 

campus location recognition using
audio signals
james sun reid westwood
sunetid jsun     rwestwoo
email  jsun     stanford edu  rwestwoo stanford edu

i  i ntroduction
people use sound both consciously and unconsciously
to understand their surroundings  as we spend more time
in a setting  whether in our car or our favorite cafe  we
gain a sense of the soundscape   the aggregate acoustic
characteristics in the environment  our project aims to
test whether the acoustic environment in different areas of
stanford campus are distinct enough for a machine learning
algorithm to localize a user based on the audio alone 
we limit our localization efforts to seven distinct regions
on stanford campus as enumerated in section iii c  we
characterize the locations as regions because we hope to
capture qualitative rather than quantitative descriptions  for
example  the huang region includes the outdoor patio area
as well as the lawn beside the building  furthermore  we
restrict our efforts to daytime hours due to the significant
soundscape differences between daytime and nighttime 
a significant advantage of audio localization is the qualitative characterization on which we focus  specifically  an
acoustic environment does not generally linearly vary with
position  for example  any point within a large room will
likely have common acoustic characteristics  however  we
expect a drastic soundscape change just outside the door or
in another room  and that difference can be of significant
value  however  gps may not capture this change for two
reasons 
   this change may be below current gps accuracy
thresholds  typically       feet 
   gps only produces lat long data  an additional layer
of information is needed to provide information about
the precise boundaries of the building 
furthermore  gps fails to distinguish accurate vertical position  e g  floors   which may be of special interest in
buildings such as malls or department stores 
ii  r elated w ork
a previous cs    course project identified landmarks
based on visual features          gives a classifier that can
distinguish between multiple types of audio such as speech
and nature      investigates the use of audio features to perform robotic scene recognition      integrated mel frequency
cepstral coefficients  mfccs  with matching pursuit  mp 
signal representation coefficients to recognize environmental

sound      uses support vector machines  svms  with audio
features to classify different types of audio 
iii  s ystem d esign
a  hardware and software
the system hardware consists of an android phone and
a pc  the android phone runs the android     operating
system and uses the hi q mp  rec  free  application
to record audio  the pc uses python with the following
open source libraries 
 scipy
 numpy
 statsmodels
 scikits talkbox
 sklearn
the system also makes use of a few custom libraries
developed specifically for this project 
b  signal flow
an audio input goes through our system in the manner
below 
   the audio signal is recorded by the android phone
   the android phone encodes the signal as a wav file
   the wav file enters the python pipeline as a sample
instance
   a trained classifier instance receives the sample
a  the sample is broken down into subsamples of  
second in length
b  a prediction is made on each subsample
c  the most frequent subsample prediction is output as
the overall prediction 
a graphical illustration of this is shown in figure   
we have designed the system with this subsample structure
so that any audio signal with length greater than   second
can be an input 
c  locations
the system is trained to recognize the following   locations 
   rains graduate housing
   circle of death
intersection of escondido and lasuen

fi 

table i    samples gathered at each location
rains
   

circle
   

tressider
   

huang
   

bytes
   

oval
   

arrillaga
   

fig     system block diagram

  
  
  
  
  

tressider memorial union
huang lawn
bytes cafe
the oval
arrillaga gym

these locations were chosen for their geographical diversity 
as well as the variety of environments  locations      and  
are indoors whereas locations        and   are outdoors 

fig     sample distribution by day

v  audio f eatures
we investigated the use of the following features 



iv  data c ollection
a  audio format






mean amplitude in time domain
variance of amplitude in time domain
fourier transform     bins 
autocorrelation function     bins 
spd     bins 
   mel frequency cepstral coefficients  mfccs 

we collected data using a freely available android application as noted in section iii a  monophonic audio
was recorded without preprocessing and postprocessing at
a sample rate of      khz 

we observed best performance using mfcc and spd features for a total of    features  these   feature types are
described in the subsequent subsections 

b  data collection

a  mfcc

data was collected on   different days over the course of
  weeks  each data collection event followed the following
procedure 

mfccs are commonly used to characterize structured
audio such as speech and music in the frequency domain 
often as an alternative to the fourier transform        
calculating the mfccs proceeds in the following manner
    

   hold the android recording device away from body
with no obstructions of the microphone
   stand in a single location throughout the recording
   record for   minute
   restart if recording interferes with the environment in
some way  e g   causing a bicycle crash 
   split recording into    second long samples
in total  we gathered     recordings of   minute in length 
for a total of      data samples of    seconds in length 
even though our system is designed to handle any inputs of
length greater than   second  we standardized our inputs to
be    seconds for convenience 
we also attempted to maintain sample balance amongst
the   locations while also diversifying sample collection
temporally  the distribution of samples by location is in
table i  the distribution by day and time is given in figure   

   divide the signal into overlapping windows
   for each windowed signal 
a  take the fast fourier transform  fft 
b  map powers of the fft onto the mel scale  which
emphasizes lower frequencies 
c  take the logarithm of the resultant mapping
d  take the discrete cosine transform  dct 
e  output a subset of the resulting dct amplitudes as
the mfccs
we used      ms windows and kept the first    mfccs as is
standard      this creates multiple sets of mfccs per signal
 one per window   to summarize all of these coefficients  we
take the mean over all windows of a signal  figure   shows
two example sets of mfccs that obtained from different
locations 

fi 

fig     sample mfccs at bytes and the circle

b  spectrogram peak detection  spd 
spd is a method we developed for finding consistent
sources of spectral energy over time  first  spd generates a
spectrogram using short period ffts  obtaining the energy
of the signal as a function of both time and frequency  the
method then finds the local maxima in frequency as defined
by a window size 
a local maximum is marked    and all other elements
are zero  finally  this matrix is summed across time to give
a histogram of local maxima as a function of frequency 
finally the method bins the results according to a log scale 
spd finds low signal to noise ratio  snr  energy
sources that produce a coherent signal  e g   a motor or fan
producing a quiet but consistent sum of tones  since all
maxima are weighted equally  spd attempts to expose all
consistent frequencies regardless of their power  we show a
comparison of spd outputs between the circle and bytes in
figure   

fig     variance explained vs   of principal components
we also projected our samples onto the basis defined by
the first   principal components for visualization  certain
regions were clearly separablein this basis  such as in figure    other regions were not quite so obviously separable 
as shown in figure  

fig     rains vs tressider using the first   pcs

fig     sample spds at bytes and the circle

c  principal component analysis  pca 
we investigated the redundancy in our features by doing a
pca on our data set using the above features  figure   plots
the fraction of variance explained vs the number of principal
components used  we saw that the curve is not steep  and
   of our    features probably do in fact encode significant
information 

fig     oval vs circle using the first   pcs

fi 

vi  m ethods and r esults
using the mfcc and spd features  we investigated the
following classifiers 
 svm using gaussian and linear kernels
 logistic regression
 random forest
 gaussian kernel svm with logistic ensemble
described in more detail in the next section
when picking the hyperparameters to use for each classifier 
we did a         split of our training dataset and then
searched over a grid of parameters  evaluating based on
accuracy of classification 
for logistic regression and svm  we also compared
the use of one vs one  ovo  and one vs rest  ovr  multiclassification schemes  we found no significant difference
in performance for logistic regression and linear svm 
however  ovr gaussian svm exhibited much worse performance than ovo gaussian svm 
a  voting
as described in section iii b  our prediction method
offers the following advantage  a test sample  with single
label  is made up of multiple subsamples  each of which is
processed and classified  the final prediction for the sample
is made on a basis of majority vote from each subsample 
which significantly reduces our test error  our original
implementation broke voting ties randomly  when analyzing
the predictions of the gaussian kernel svm  we noticed
that     of misclassifications resulted from incorrect tiebreaks  and       of misclassifications occurred with voting
margins of at most    we investigated   approaches to
improving performance in these scenarios 
our first attempt used the total likelihood produced by the
svm predictions across    subsamples  while this approach
seemed sound in theory  the small training sample size make
the likelihood estimates highly inaccurate  and this approach
did not change overall performance 
our second approach was to use the gaussian
svm logistic ensemble method mentioned in section vi 
previous testing indicated that our gaussian kernel svm was
prone to overfitting  while the linear logistic classifier tended
to have a better balance between training and test error 
the final method we chose was to employ the ensemble
only when the voting margin for the svm is no more
than    for these close call scenarios  the logistic classifier
calculates its predictions for all subsamples  the svm votes
are given     x weight to prevent any potential future ties 
and the highest total is chosen  this method provided a     
generalization error reduction 
it is also interesting to note how test error varied as we
changed the duration of our test sample  effectively changing
the number of votes per test sample  using our ensemble  we
achieved just under     error with    second test samples
 figure     this audio length is likely too long for most
applications  but it is noteworthy nonetheless 

fig     error vs  number of subsamples
b  generalization
we distinguished between   types of testing errors 
   cross validation error   error on the testing set when
we split the data set completely randomly
   generalization error   error on the testing set when we
split based on random days 
our data has a significant temporal correlation  we discovered that the typical cross validation error was too optimistic because audio samples recorded on the same day can
be significantly more correlated to each other than to audio
recorded on different days  we were able to decrease our
cross validation error to around    using a gaussian svm 
however  when we attempt to use this seemingly general
classifier on a completely new days data  we discovered it
was actually very overfitted 
with this in mind  we were able to reduce our generalization error to a bit less than     using a gaussian svm
with logistic classifier ensemble as described in vi a  to
calculate generalization error  we did a form of   fold crossvalidation  we held out all samples from a single day for
testing while using all other days for training  and then
we repeat for all   days during which we had gathered
data  we finally do a weighted combination to calculate
the generalization error  weighting based on the number
of samples in each held out day  table ii gives a summary
of our results 
table ii  classifier comparison
classifier

x validation

generalization

gaussian kernel svm

      

      

linear kernel svm

      

      

logistic

      

      

random forest
gaussian svm   logistic
ensemble

      

      

      

      

using the svm logistic classifier  we generated the
confusion matrix in figure   averaging over all hold out
trials 
our classifier did relatively well in terms of accuracy

fi 

fig     overall confusion matrix
fig      human confusion matrix

fig      confusion matrix with balanced classes

for most regions  however  the oval and circle are often
confused for each other in a relatively balanced manner 
but the circle is frequently missclassified as rains whereas
rains is not often mistaken for the circle  to eliminate any
effects due to our data collections minor class imabalance
 table i   we also trained on a completely balanced data set
to obtain figure    
there are no major changes when balancing the dataset 
this suggests that the oval and circle are very similar in
terms of soundscape and temporal variability  a conclusion
that is also supported by pca in figure    however  the
circle is likely very similar to rains on certain days  but
rains has a more constant soundscape that is easy to identify 
c  classifier evaluation
as the final step in evaluating our system  we compared
the performance of our classifier to peoples ability to
localize based on audio clips  we created a small game
that would present the user with a random    second audio
clip from our dataset  the user would then choose from
which of the   locations the audio was taken  the pool
of participants comprised of stanford cs    students and
other attendees of our poster presentation  the results are
shown in table     the sample size only consisted of
   sample points  furthermore  we acknowledge that they
did not explicitly undergo any training and relied only
on recall  however  it seems apparent that even stanford
students  who frequent the chosen locations  are ill adept
at identifying them by sound alone  as a baseline  random
prediction would give     error on average with   labels 
of the    audio samples  students accurately located only
   of them for an error rate of        this is much higher
than our classifiers generalization error of        
vii  f uture w ork and c onclusion
a major challenge in this project was data collection 
due to the limited number of audio samples collected 
our efforts to develop additional relevant features generally

resulted in overfitting  significantly increasing our training
set may allow exploring additional features  in particular  we
believe hour of day and day of week could be significant
additions  especially to mitigate the temporal challenge of
classification  as discussed in section vi b  we observed a
gap between cross validation error and generalization error 
as we utilized more data  we observed this gap lessening
even with just the current set of features  we expect that
our algorithms ability to predict new data would continue
to improve with additional training data  finally  increasing
our training set would make the likelihood estimates of our
classifiers more accurate  thus  it may be worthwhile to
revisit the use of likelihood estimates in our voting scheme
as described in section vi a 
the student testing we performed  as described in section vi c  demonstrate the challenges of audio based localization  users frequently noted that their    second clip
did not seem to match the typical soundscape of the area
they imagine  given the variability of soundscape at each
region between different times and days  we are encouraged
by our algorithms performance  however  significant work
remains to be done before conclusions can be reached about
the feasibility of this method for broader applications  in
particular  it is unknown how scaling the number of regions
affects prediction accuracy  it would also be interesting to
see our chosen features and techniques applied to very
different environments with the same number of regions 
r eferences
    a  crudge  w  thomas  and k  zhu  landmark recognition using
machine learning  cs    project       
    l  chen  s  gunduz  and m  t  ozsu  mixed type audio classification
with support vector machine  in      ieee international conference
on multimedia and expo  july       pp         
    s  chu  s  narayanan  c  c  j  kuo  and m  j  mataric  where am
i  scene recognition for mobile robots using audio features  in     
ieee international conference on multimedia and expo  july      
pp         
    s  chu  s  narayanan  and c  c  j  kuo  environmental sound
recognition with time and frequency audio features  ieee transactions
on audio  speech  and language processing  vol      no     pp      
      aug      
    g  guo and s  z  li  content based audio classification and retrieval
by support vector machines  neural networks  ieee transactions on 
vol      no     pp               
    j  j  aucouturier  b  defreville  and f  pachet  the bag of frames
approach to audio pattern recognition  a sufficient model for urban
soundscapes but not for polyphonic music  the journal of the acoustical society of america  vol       no     pp               
    l  rabiner and b  h  juang  fundamentals of speech recognition 
     

fi