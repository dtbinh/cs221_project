exploration of reinforcement learning to snake
bowei ma  meng tang  jun zhang

abstract in this project  we explored the application of
reinforcement learning in the problem not amenable to closed
form analysis  by combining convolutional neural network and
reinforcement learning  an agent of game snake is trained to
play the revised snake game  the challenge is that the size of
state space is extremely huge due to the fact that position of
the snake affects the training results directly while its changing
all the time  by training the agent in a reduced state space 
we showed the comparisons among different reinforcement
learning algorithms and approximation optimal solution  and
analyzed the difference between two major reinforcement
learning method 

fig     game snake

i  introduction
since video games are challenging while easy to
formalize  it has been a popular area of artificial intelligence
research for a long time  for decades  game developer have
attempted to create the agent with simulate intelligence 
specifically  to build the ai player who can learn the
game based on its gaming experience  rather than merely
following one fixed strategy  the dynamic programming
could solve the problem with relative small number of
states and simple underlying random structure  but not the
complex one 

action state pairs  so we could introduce neural network to
store q factor of each state 

the reinforcement learning is one of the most intriguing
technology in machine learning  which learns the optimal
action in any state by trial and error  and it could be useful
in the problem not amenable to closed form analysis 
therefore  we selected and modified the snake game to
investigate the performance of reinforcement learning  the
goal of this project is to train an ai agent to perform well
in a revised snake game 

ii  work review
there are abundant works about the artificial intelligence
research for game agent  in      miikkulainen showed that
soft computational intelligence  ci  techniques such as
neural network  have been excelling in some challenging
field  where the relatively standard  labor intensive scripting
and authoring methods failed  at the same time  recent
research focuses not only on games that can be described
in a compact form using symbolic representations  such
as board and card games  but on more complex video games 

snake is the game popularizing over the world with nokia
mobile phones  it is played by sole player who controls
moving direction of a snake and tries to eat items by running
into them with the head of the snake  while the foods would
emerge in random place of bounded board  each eaten item
makes the snake longer  so maneuvering is progressively
more difficult  in our game  we slightly changed the games
rule into scoring as many points as possible in fixed time 
instead of counting points in unlimited period 
q learning is an example of a reinforcement learning
technique used to train an agent to develop an optimal
strategy for solving a task  in which an agent tries to learn
the optimal policy from its history of interaction with
the environment  and we call the agents knowledge base
as q factor  however  it is not feasible to store every
q factor separately  when the game need a large number of

the sarsa algorithm is an on policy algorithm for
temporal difference learning  compared to q learning  the
major difference of sarsa is that the maximum reward
for the next state is not necessarily used for updating the
q values  instead  a new action  and therefore reward  is
selected by using the same policy that determined the original
action 

td gammon  a backgammon playing program developed
in     s  is one of the most successful example in
reinforcement learning area  in      td gammon used a
model free reinforcement learning algorithm similar to
q learning  and approximated the value function using a
multi layer perceptron with one hidden layer  
tsitsiklis and van roy     showed that combining modelfree reinforcement learning algorithms such as q learning
with non linear function approximators  or indeed with
off policy learning could cause the q network to diverge 
subsequently  the majority of work in reinforcement
learning focused on linear function approximators with
better convergence guarantees 

fiwith the revival of interest in combining deep learning
with reinforcement learning  sallans and hinton    
illustrated that deep neural networks could be used to
estimate the environment  while restricted boltzmann
machines could benefit the estimation of the value function 
iii  method
a  mathematical analysis of snake




abstractly  each step of the snake game could be
considered as finding a self avoiding walk  saw  of
a lattice in r    a realization of snake game can be
mathematically represented as
 the m  n game board is represented as g  
 v  e   where v    vi   is the set of vertices 
where each vertex corresponding to a square in the
board  and e    eij  vi is adjacent to vj   
 the snake is represented as a path  u         uk   
where u    uk are the head and tail  respectively 
np hardness  viglietta           proved that any game
involving collectible item  location traversal and oneway path is np hard  snake is a game involving traversing a one way path  i e   the snake cannot cross itself 
thus  if we have no prior information about the sequence of food appearance  picking every single shortest
saw for each step in an episode is np hard 

algorithm    the deterministic heuristic algorithm
for snake
  build the graph g    v  e   each square vij  v
  while snake s   s         sk    v and food f 
 f    v do
 
build  vij   vkl    e  where vij   vkl 
  s with
 i   k  j   l  or  i   k  j   l 
 
initialize m  n array d
 
assign d i  j    length of path between vij and
f or m ax n u m if no path exists
 
find the min d i  j   with  vij   s     e
 
if d i  j     m ax n u m then
 
find path p    vij   p         pn    f  
 
assign s    f  s    pn       sk   pnk
  
if s  and sk is connected then
  
assign s    vij   s    s      
  
continue to step  
  
end
  
else
  
find the longest path between s  and sk  
  
p    s    p    p       pn    sk  
  
assign s    p    s    s      
  
continue to step  
  
end
  
end
   end

b  approximation algorithm for snake
due to the np hardness of finding optimal solution for
snake  we developed a heuristic algorithm that does considerably well for snake problem and used it as benchmark for
the reinforcement learning algorithms we will discuss later 
the algorithm is shown in algorithm  
c  reduction of state space for reinforcement learning
to conduct and implement successful reinforcement learning algorithms to play the game  one of the fundamental
obstacles is the enormous size of state space  for instance 
denote the size of the game board as n  n  naively  using
the exact position of the snake and food  each cell could
be parameterized by one of the four conditions   contains
food  contains the head of the snake  contains the body of
the snake  blank   by simple counting principle the size of
the state space  s  satisfies  s    n    which is immense
when n is large and learning in state space of such size is
infeasible  hence  to accelerate the learning rate  one simple
reduction technique is to record only the relative position
of the snake and the food with the current condition of the
snake  precisely  each state in the reduced space is of the
form
 ws   wl   wr   qf   qt  
in the above expression  ws   wl and wr are indicator functions whether there is a wall adjacent to the head in straight 
left and right directions respectively  and qf   qt are the
relative position of the food and tail with respect to the head 

using such mapping the total size of state space is only
              and the performance is would be improved
prominently in the learning process 
d  reinforcement learning   q learning
in q learning  an agent tries to learn the optimal policy
from its history of interaction with the environment  a
history is defined as a sequence of state action rewards 
  s    a    r    s    a    r    s    a    r        

   

for the snake game  the process is indeed a markov decision
process  and the agent only needs to remember the last
state information  we define the experience as a tuple of
  s  a  r  snext    these experiences will be the data from
which the agent can learn what to do  as in decision theoretic
planning  the aim is for the agent to maximize the value of
the total payoff q s  a   which in our case is the discounted
reward 
in q learning  which is off policy  we use the bellman
equation as an iterative update
qi    s  a    es    r    max
qi  s    a   s  a  
 
a

   

in the above equation  s  s  are the current and next state  r is
the reward   is the discount factor and  is the environment 
and it could be proven that the iterative update will converge
to the optimal q function  since the distribution and transition probability is unknown to the agent  in our approach
we use a neural network to approximate the value of the

fiq function  this is done by using the temporal difference
formula to update each iterations estimate as
 

 

q s  a   q s  a     r   max
q s   a    q s  a      
 
a

the action set includes three
  turn left  turn right  go straight  

possible

outcomes

algorithm    the algorithm for sarsa
 
 
 
 
 
 

an  greedy approach is implemented here  the exploration probability is  is changed from     to     with a
constant density      during training  once it reaches     
it holds constant  this propels the agent to explore a lot of
possibilities in the beginning of the game when it doesnt
know how to play the game  this leads to a great amount of
random actions thus enable the agent to exploit much enough
to narrow down the optimal actions 
e  reinforcement learning   state action reward stateaction sarsa 
state action reward state action sarsa  is an onpolicy reinforcement learning algorithm which estimates the
value of the policy being followed  we could describe an
experience in sarsa in the form of 
  s  a  r  s    a   

   

this means that the agent was in state s  did action a 
received reward r  and ended up in state s    from which
is decided to do action a    this provides a new experience
to update q s  a  and the new value which this experience
provides is r   q s    a     so sarsa could be described as
below 
q st   at    q s  a     r    q s    a     q s  a  

   

so sarsa agent will interact with the environment and
update the policy based on the actions taken  and thats why
its an on policy learning algorithm  q value for a stateaction is updated by an error  adjusted by the learning rate
  q values represent the possible reward received in the next
time step for taking action a in state s  plus the discounted
future reward received from the next state action observation 
the algorithm of sarsa goes in algorithm 
iv  results

 
 

 
  
  
  
  

start the game snake
while do
initialize q s  a  arbitrarily
choose a from s using  greedy policy
while do
take action a
choose a from s using  greedy policy
q st   at   
q s  a     r    q s    a     q s  a  
ss
aa
end
update game parameter
end
table i  rewards table
case
reward

eat food
    

hit wall
    

hit snake
    

else
   

b  learning curve
the learning curves for q learning  fig      and sarsa
 fig      are shown below respectively  the red dash lines
represent the average learning curves 
we could easily observe that the performance of agent
with q learning get improved faster than that of agent with
sarsa in the beginning  i e  in a short run  agent with qlearning algorithm outperforms the agent with sarsa  but
as the number of training iterations increases  the performance of agent with q learning doesnt improve much  while
the performance of agent with sarsa still gets improved
comparatively significantly  q learning does well compared
to sarsa   when the training period is short  but in the
long period  sarsa wins 
the reason why the agent of q learning doesnt perform
well in some cases in the long period training is that qlearning algorithm would reinforce its self righteous q value
even with a very small learning rate  this would lead to
considerably volatile performance  although the agent with
sarsa seems to outperform the agent with q learning algorithm in a long period training  it also has a comparatively
sluggish learning curve in our cases 

a  tuning parameters

c  performance comparison

the discount factor  was set to be       moderately
decreasing learning rate starting from        and the
rewards were set as shown in table   
we want the agent to control the snake to go to the food
quickly  and the last column in table   is a punishment for
taking for one movement  which encourages the agent to
traverse shorter walk to the food  this negative rewards acts
as similar function as the discount factor   we performed
trial and error on different combinations of reward for
different cases to get current combination of reward values
as one optimal combination among all the trials 

we took the performance of approximated optimal solution algorithm as the benchmark  as we mentioned before 
due to the fact that snake is a np hard problem  the best
benchmark we could get here is the approximated optimal
solution  and it could be shown later that our agents with
reinforcement learning algorithms could not beat this approximated solution even with a considerable long training
period 
then we compared the performance of the agents based on
two reinforcement learning algorithms with the benchmark the performance of our approximated optimal solution  at the

fifig     learning curves from q learning

fig     performance comparison among   different algorithms

fig     learning curves from sarsa

same time  we consider the effects of training period on the
performances of snake agents  we performed              
training iterations on agents based on different reinforcement
learning algorithms and evaluate their performances respectively 
we set the time limit to be   minute for the game 
run      tests for agents with different algorithms and
compute the average score different agents could achieve 
the reason why we choose   minute to be the time limit
is that within   minute  the agents with those reinforcement
learning algorithms could control the snake survive so we
could make sure that the difference lies only on whether
different agents could find shorter path to the food    minute
is a comparatively long time period to show the significant
performance difference among different algorithms while it
is a relatively short time period that      tests could be
handled with our pc in a reasonable running time 
the results are shown in figure   and table   
table ii  performance comparison
method
iterations
   
   
   

optimal

sarsa

q learning

      
      
      

      
      
      

      
      
      

its worth mentioning here that the performance of our
approximated optimal solution algorithm is not related to
the number of training iterations  only the performances of
agents with sarsa and q learning algorithm are directly

related to the number of training iterations  the performance
of the approximated optimal solution is shown to be a
benchmark in the same table and figure as performances of
agents with different reinforcement learning algorithms 
our results show that within the range of     to    
training iterations  a larger number of training iterations
would lead to better average scores for both q learning
and sarsa algorithm  given the same number of training
iterations  agent trained in sarsa has better performance
than that trained in q learning 
v  future work
a  explore the stability of q learning algorithm
we found that in some training cases  even with a
decreasing exploration probability  the performance of qlearning algorithm is not very stable  so its worth exploring
principles and methods to improve the stability of the qlearning algorithm  for example  one intuitive way to address
this problem is to add various tuning parameters to improve
the probability of convergence for q learning algorithm 
b  study the other state space approximation methods
other approximation of the state space could be explored
for better performance  currently  we use a quadrant view
state mapping technique as discussed in the previous section 
using such means  the snake does not have a good sense
of precaution for hitting himself  hence  a more rigorous
state mapping technique should be developed  such reduction
mapping must not only approximate the relative position of
the head and the food  but also obtain a concise sense of the
position of the body without tremendously enlarging the size
of the state space 
c  expected sarsa
in order to further improve the learning rate of the snake
agent  expected sarsa could be used  van seijen et al 

fi           provide a theoretical and empirical analysis of
expected sarsa  and found it to have significant advantages over more commonly used methods like sarsa and
q learning 
vi  conclusions
in this project  we have shown an implementation of both
q learning and sarsa  by approximation of state space  in
neural network  we anticipated that the difference between
performances of q learning and sarsa might become
apparent after long training period  and the outcome verified
our expectation  also  we compared this two methods
with an approximated optimal solution and found that
neither of the two agents could achieve the performance
of the approximated optimal solution  while they exhibited
prominent learning  also  we observed the instability of
q learning algorithm in some cases and its worth exploring
feasible solutions for future work 
r eferences
    risto miikkulainen  bobby bryant  ryan cornelius  igor karpov 
kenneth stanley  and chern han yong  computational intelligence in
games 
url ftp   ftp cs utexas edu pub neural nets papers miikkulainen wcci   pdf
    gerald tesauro  temporal difference learning and td gammon  communications of the acm                   
    john n tsitsiklis and benjamin van roy  analysis of temporaldifference learning with function approximation  automatic control 
ieee transactions on                     
    brian sallans and geoffrey e  hinton  reinforcement learning with
factored states and actions  journal of machine learning research 
                 
    lucas jen an application of sarsa temporal difference learning to
super mario
url  http   x ro de downloads mariosarsa pdf
    giovanni viglietta        gaming is a hard job  but someone has to do
it 
    van seijen  h   van hasselt  h   whiteson  s  and wiering  m         
a theoretical and empirical analysis of expected sarsa       ieee symposium on adaptive dynamic programming and reinforcement learning
pp         
url  http   goo gl oo lu

fi