how much is my house worth 

predicting housing prices in mckinney  texas

nadin el yabroudi  paul m r  harrison

abstract
the goal was to give homeowners a prediction on their
houses closing price which would be put on the market  classification and regression methods were used
as well as natural language processing on each houses
remarks  the best set of models found used lasso l 
regularization  prediction performed remarkably better
than classification  and predicted close prices with    
confidence with this interval           to         

the goal of this project is to create a model that can best predict the closing price for a house before it has been placed on
the market  this is important as it can help homeowners not
only estimate a listing price but serve as a second opinion
if need be  the project is based on data from the company
opendoor  which uses machine learning to predict housing
prices and make selling your home easier  our dataset is
from home sales in the small town of mckenney texas from
late      to mid       although it has only about      observations  it provides many different features for the data
such as area of the house  types of floors  number of bedrooms  etc 

dataset and features
cleaning the data set included eliminating features that were
relevant only once the house had been sold since we were
only interested in predicting the price that a house should
be given when entered into the market  we then converted
categorical features such as the type of flooring of the house
into binary ones  we also created new features such as binary features for the season in which the house was listed on
the market and the age of the house  these modifications to
our dataset largely increased the number of features in our
training set so that at the end we had    features  once we
had cleaned our data set we looked at the shape of the distribution of the target feature  the closing price of the house  to
understand how to best fit the data  the closing price seemed
to follow a normal distribution skewed left  and the log norcopyright c       association for the advancement of artificial
intelligence  www aaai org   all rights reserved 

mal of the distribution looked closer to a normal distribution not skewed in any direction  we decided to test how our
models did predicting the log of the closing price as well as
just the closing price 

training methods
our training and model selection methods throughout the
project were as follows  we split our dataset into a training
and testing set by a       split  throughout the first part
of the project we used k fold cross validation with k   
to approximate a test error  this proxy test error helped us
make decisions about which models to select and what to try
next without including bias into the testing phase  during the
testing phase we trained on the whole training set and tested
on the test set only with the best model found with the cross
validation error 
as a baseline model  we decided to use a naive bayes
classifier for our predictions  in order to use this model we
needed to discretize our values including our target variable  market research on online housing real estate engines
such as zillow  showed that it is common to split houses
into brackets of approximately           using this value
for our bucketing  we were now trying to predict the price of
a house within ten possible buckets  first  we implemented
two types of naive bayes  gaussian naive bayes and multinomial naive bayes  multinomial naive bayes performed
much better probably because a greater majority of our features were binary  than continuous  nonetheless  multinomial naive bayes only achieved     accuracy  even with
the log of the target values  to explain the performance of
naive bayes  we obtained correlation plots on features conditional on their actual target price  figure     the sample
plot shows high dependency between the continuous features  especially in the more extreme closing price values 
where we had less data points 
to improve our classification we needed to relax the
model assumption  so we decided to try a multinomial logistic regression  or logit classification on the same bucketed dataset  this model outperformed naive bayes reaching an accuracy of     after all variables  such as regular 

fifigure    correlation plot of features conditional on
          closing price bucket

ization  interaction terms  and time  which we address later
in this paper   were maximized  we believed that discretizing our features  by which we were losing information for
each sample  was causing the low accuracy with classification  therefore  we decided to try a regression model 
our first regression model was ordinary least squares linear regression without any regularization and found that
for certain samples  the model predicted prices up to     
times too big  suggesting that regularization was key to this
model  lasso linear regression and ridge linear regression
performed much better  to evaluate the regression model
we used the metric mean squared percentage error  mspe  
since the price values were numbers on the range of  e  by
using a percentage difference metric we could more manageably compare the errors  using the log of the target values
on the lasso regularization we were able to obtain a      
mean squared error 
finally  we decided to compare whether regression had
really done better than classification  we could see that a
      mean squared error was well within the range of
         with which we had bucketed our features before 
yet  we had not tried to see if classification did better with
smaller bucket sizes  as such we ran logit classification
on a data set with the target values discretize in buckets
of           yielding    buckets  however  this model did
much worse than with the original bucket size  most likely
because creating more buckets meant that each bucket had
less samples  therefore  we could predict with more granularity the price using regression and with a high accuracy 

optimizing performance for model variables
   time dependence  our goal was to predict the closing
price of houses in the future  which meant time was an
important factor in our modeling  the dataset was ordered

chronologically by when the house was placed on the
market  our worry was that if we only trained on houses
sold a year ago market changes through time would not
be taken into account  therefore  to test the importance of
time in our model we trained our models on a chronologically ordered dataset and on a dataset in random order  for
classification it was hard to find a pattern between whether
random or chronological did better  for the logged target
values chronological did better  but for the others random
did better  however  for regression we could clearly see
that random dataset was doing far better than chronological one 
therefore  we decided that time did play an important part
in our dataset and we wanted to account for it by adding
a feature to our training matrix  using the date in which
a house was placed in the market  we created a feature
which determined how recent the sample was  for both
classification and regression  we saw that the time feature
improved the performance of our model under cross validation 
   feature interaction terms  we had learned from naive
bayes that our features had high dependency  hence  we
wanted to use this fact to improve our model  as such we
added interaction features of degree two to our dataset 
in other words  we created    choose   new features
which represented the product of every pair of features 
this brought the number of features in our model up to
      which was more than the number of samples in our
dataset  to counteract the significant increase in dimensionality of our model  as we will see in the next section 
we used aggressive regularization  using the feature interactions we were able to improve our models slightly 
for the logit classifier we obtained       accuracy without interaction terms  and       accuracy with interaction terms  for lasso linear regression we obtained      
mspe without interaction terms  and       mspe with
interaction terms 
   optimizing performance with regularization  for both
the logit classifier  and lasso linear regression  regularization played an important part in model performance 
first for classification we had to select the appropriate
c value  which was inversely proportional to the intensity of regularization  we found that for the interaction
terms we needed to use strong regularization  c        
while for the dataset without the interaction terms only
a small amount of regularization was necessary  c     
this suggests the the interaction terms resulted in overfitting our model  but regularization helped counterbalance
this  similarly  for regression we had a constant to control
the regularization  alpha  which in this case was proportional to the data  we see that once again interaction terms
require strong regularization  alpha      where the noninteraction terms data set required only alpha         in
other words  our new interactions regression model had
    x more regularization  more interestingly we see that
without interaction terms  ridge linear regression does
better than lasso  but the opposite is true for the dataset
with the interaction terms  further proving that these in 

fifigure    num words remaining vs    threshold for
words to be removed

teraction terms are overfitting to the data  additionally
it was found that l  regularization which is stronger i e
more aggressive in how it feature selects than l  was crucial to finding our optimal model 

remarks analysis
the dataset also contained descriptions on each of the sample homes and we wanted to understand whether these could
help our model predict the closing price  therefore  we ran
bag of words on the remarks and then used tf idf to remove the words that appeared more than    of the time
which would not help us distinguish the comments  we used
   as we noticed in figure    that after this value there
was a steep decrease in the number of common words  and
we feared a higher value would eliminate important words 
figure   shows this compromise of removing unnecessary
words as a function of this threshold percentage 
next  we wanted to reduce the dimensionality of this matrix to add to our input train matrix and not have too many
features  to do so we used two techniques  principal components analysis or pca and the anova f statistic  for
pca we selected the top     principal components and for
the anova f stat matrix  we selected the top     of words
that gave the most information about the closing price 
appending these two matrices  pca and anova fvalues  to our original matrix and running both classification
and regression on it had little to no impact on prediction 
for regression both remark matrices seemed to be eliminated by regularization since we obtain the same mspe  and
for classification pca and f value matrices gave lower accuracy rates  suggesting overfitting  therefore  we can conclude that regardless of the dimensionality reducing method 
the remarks are not useful for predicting closing prices 

model comparison
below we present a summary of the results for the different
model iterations that we have discussed previously 
classification

cv accuracy

nb multinomial log y

     

nb gaussian log y

     

logit log y chron

     

logit log y rand

     

logit log y

     

logit inter terms log y

     

logit half buckets log
y

     

logit pca log y

     

logit f stats log y

     

it is interesting to note that for both classification and regression  the best model included interaction terms  and the
log of the target values  this demonstrate that the relationship between features were very important for prediction as
well as having a normal distribution on the target values 
regression

cv mspe

ridge log y

     

lasso y

     

lasso log y chron

     

lasso log y rand

     

lasso log y

     

lasso inter terms log y

     

lasso pca log y

     

lasso f stats log y

     

testing and conclusion
using our test set on the lasso model with interaction terms
and the log of our target values i e  our best model we obtained an mspe of        which was even lower than what
we obtained with cross validation on our training set  because we observed that our models residuals followed a normal distribution  as in figure    it made sense to find a confidence interval on the mean of these values  the result of
this     confidence interval on this test set was          
to          
from a first glance looking at figure   and our corresponding     confidence interval we can see that we have
correctly classified house prices with a far greater accuracy

fifigure    normal distribution of residuals on the test set

figure    boxplot for residuals on the testing set

figure    residual plot on the testing set
figure    histogram to show assymetry in residuals for
price ranges           to        

than previous classification methods  which gave at most a
    accuracy score  however  we wanted to evaluate our
model with more granularity this model by examining its
performance on each of the buckets that we had used for
classification  first  as we can see by the residual plot  figure    our model does well in the middle to low price ranges
of the houses  but does poorly on expensive houses  furthermore  the boxplot  figure    shows that the median for the
residuals on the price ranges                    is close to
zero  which also happens to be the price ranges most common in our training set  this suggests that with more samples for more extreme values  our model could do better  the
boxplot also allows us to examine the variance of the residuals for each price range and we can see that for the most
common price ranges the variance is much lower than for
other ranges  overall it is also interesting to note that for
the highest two price brackets our model is precise but inaccurate  namely  it is offset by a systematic prediction error 
therefore  an easy fix to our model would be to consistently
add about          to our prediction in these bucket ranges 

although we wanted to also find confidence intervals on
the mean of each of the buckets  we can see by figure   that
even on the buckets with many samples the distribution of
residuals could not be well defined by a normal distribution 
once more  figure    shows that our model predicts lower
values than it should  in fact  we can see generally how as
the model predicts on more expensive houses our residuals
trend higher  we also see overpricing on the first two buckets  which implies a more optimal i e  flatter set of thetas
could do better  yet  the overpricing is less extreme than the
underpricing  which is indicative that perhaps a nonlinear
function would model these trends on extreme values more
precisely 
another model which has great potential in improving
prediction due to this method modeling feature dependencies is random forest regressors  preliminary research
showed that this model has high potential with an mspe
of      and     r squared value  further research on the
varying factors of random forest regressors could improve
the models performance 

fireferences
    de oliveira  luke  in person interview  may          
    shirani mehr  houshmand  in person interview  june         

 

fi