the automated travel agent  hotel recommendations using
machine learning
michael arruza  john pericich  michael straka
june        
abstract

expedia users who prefer the same types of hotels presumably share other commonalities  i e   non hotel commonalities  with each other  with this in mind  kaggle challenged developers to recommend hotels to expedia
users  armed with a training set containing data about    million expedia users  we set out to do just that  our
machine learning algorithms ranged from direct applications of material learned in class to multi part algorithms
with novel combinations of recommender system techniques  kaggles benchmark for randomly guessing a users
hotel cluster is          and the mean average precision k     value for nave recommender systems is         
our best combination of machine learning algorithms achieved a figure just over       our results provide insight
into performing multi class classification on data sets that lack linear structure 

 

introduction

recommending products to consumers is a popular application of machine learning  especially when there exists substantial data about the consumers preferences  kaggle has provided a dataset containing substantial
information about    million expedia users and has posed the challenge of recommending hotel clusters to these
users     the difficulty lies in taking user information that lacks linear structure and using it to recommend hotel
clusters  furthermore  the sheer volume of possible prediction classes also poses a challenge  accordingly  this
project focuses first on applying well known machine learning algorithms to the dataset and then tweaking as
well as combining these algorithms so that they produce the correct classifications on the dataset 

 

related work

a large amount has already been done and written about item recommendation systems      a common technique
for these systems is to use a user item matrix combining features about the users and items along with user
feedback for the items  however  these methods proved to be inapplicable for our project  as the anonymized
nature of the target variables made it difficult to obtain relevant features for them  more applicable to our
project  previous work has been done on hotel recommendation systems by gao huming and li weili  who
showed good results using a combination of clustering and boosting algorithms      while their results are not
comparable to ours due to the large differences in the datasets used  it is notable that both their paper and ours
show promise in using clustering and boosting for hotel recommendations 

 

dataset and features

the data consisted of    million users and anonymized features including aspects
such as number of children  number of rooms booked  destination visited  sate of
check in and check out  location of user when booking  whether or not the booking
was part of a package  etc  we were tasked with using this data to predict which
hotel cluster the user was going to book into  some feature engineering was used
on the data set to extract more useful features  the date of check in  date of check
out and date of booking were removed and replaced with length of stay  in days  
month  valued from   to      year  valued      or   as the data was from     
to       and week  numbered from one to      the purpose was to discretize the
figure
  
three  dates into a format more amenable to our learning algorithms  it should be noted
dimensional
pca
plot that the given data is known to have a data leak affecting roughly thirty percent
of the three most popular of the test data and if used would boost our accuracy scores by roughly twenty
hotel clusters  each is given
a different color 
 

fito twenty five percent  however  for the purposes of this paper we opted not to
utilize this data leak when testing our algorithms  as we felt that exploiting the
leak would not yield interesting results generalizable to other problems  the size
of the data set and limits on computational time also encouraged us to randomly sample subsets of the data to
use when testing  as using the entire data set would have been prohibitively expensive 
for initial data visualization  we compressed the dataset into   dimensions using basic pca dimensionality
reduction  to reduce the visual clutter of the data  we only plotted the   most popular hotel clusters  as seen in
figure    this figure supported our hypothesis that user similarity would yield good results  as users who chose
identical hotel clusters tended to be clustered together 

 
   

methods
baseline methods

our first algorithm used the naive bayes conditional independence assumption
to rank hotel clusters     this method outperformed the random benchmark put
forward by kaggle  and serves as our benchmark going forward for the relative
success or failure of an algorithm 
another simple method used was training an svm  one issue we
faced in applying svms to our problem was adapting the typically discriminative svm algorithm to generate probabilities  which can then be
used to create a ranking of hotel clusters 
we ended up using a
method described by wu et al 
relying on pairwise coupling of single
class predictions  learning the class probabilities using cross validation      figure    the distribution
in selecting the kernel to use for our svm  we avoided extensive experiments of the target hotel clusters 
with linear kernels due to the obvious lack of linear separability in the data  indeed  initial explorations were found to perform poorly  with a    map  accuracy 
using a polynomial kernel was considered  but this was found to be prohibitively expensive in terms of compu  xx     

tational time required  the rbf kernel e    was favored instead  both because of the speed of pre existing
implementations  and because of the euclidean norms potential to be interpreted as a similarity measure  which
we thought might perform well due to the hypothesized importance of user similarity  we first normalized the
training matrix to ensure that this interpretation would be valid  parameters c and  were tuned using   fold
cross validation  selecting the parameters that gave the highest map  accuracy 
the best parameters were found to be c       and          the model
still underperformed  and it was found in practice that lower values of c and 
reduced the sensitivity of the model to the data  and made both the predictions
and map  accuracy converge towards predicting the top   most numerous targets 

   

weighted user similarity

   

gradient boosting

figure    mean average
our first success came from using a form of ensemble tree boosting algorithm  precision  using different
this was chosen due to boostings tendency to intelligently learn the non linear values of k  for our baseline
structure of data  our approach involved minimizing a softmax objective function methods 
to output a list of probabilities  one per class for a test user  which would then
be used to get the top k hotel cluster recommendations  we did this by using an
off the shelf xgboost package  which trains several classification and regression
trees  at each step fixing the previous trees and adding a new one 
we experimented with several regularization terms and stepsizes  but found
that they had no real impact on our results  for the number of rounds to train 
we chose   as a reasonable trade off between complexity of the model and time
needed to train it 
our most successful class of algorithms involve a novel combination of clustering  kernelized similarity and weighting distance  inspired by user similarity
algorithms commonly used in recommendation systems      first  we cluster all of
training data based on the src destination id feature  the idea was that people who
travel to similar destinations are more likely to book into the same hotel clusters 
originally we attempted to cluster based on hotel market  due to the relatively
high correlation with the target hotel clusters  see figure    but this resulted in
figure    pearson corre  poor results  as the number of hotel markets represented in the data was too small 
lation matrix for the processed features 
 

fiproducing clusters too big to be useful  the dataset contained       unique values
of destination ids  but only      unique values for hotel markets   once clustered
together  we created a dictionary with src destination id as a key and the users in
the training data that contain that id as a feature as the value  once the dictionary is set up  we make predictions
as follows  for each member of the test set  we hash into the group that share the same src destination id feature
using the created dictionary  once we have the group of training users  we then create a kernel matrix using
some kernel function  many were attempted  and we discuss the ones we used later  and find the top     users
in the group  once we have these top     users  we give each user a score based on the similarity  and for each
hotel cluster sum up the scores of the users in the top     who booked that cluster  the clusters with the ten
highest scores are recommended for the user 
in measuring user similarity  our first attempt with cosine similarity performed relatively poorly  as certain
features serve as numeric representations of qualitative information  for example  the continent ids for two users
may be close together numerically  but that does not imply the two continents should be considered similar 
thus  we instead opted to represent similarity through jaccard similarity  thus  to this end  we opted to utilize
the kernel function
m
  x
  xi   yi  
k   x  y   
m i  
which counts the number of features shared  using this kernel  we define the kernel matrix to be the vector
k where the ith element of k is the result of the kernel function defined above  we then sort k and extract the
top     users 
now that we have the sorted top     users and their similarity scores for each according to the kernel function 
we need to assign each hotel cluster a score based on the number of times it appears in these top     users and
how similar these users are  lets define v to be a vector with     elements containing the hotel clusters  in
order  of the top     users  the first function that was attempted is as follows 
score   cluster   

 
x

    vi   cluster exp 

i  

i 
 
   

where tau is some constant     worked best for us  and vi is the hotel cluster of the top ith user  the basis
of the equation above is to weigh users who are really close to the test user more than the users who are far
away  but still heavily weigh clusters that appear more often in those top users  it is based on the equation for
locally weighted linear regression and was meant to test the hypothesis that similarity could be used to effectively
make recommendations  as shown in figures   and    this method outperformed our benchmark by a noticeable
amount  and made us move towards exploring other similar algorithms and scoring functions 
one important flaw with the previous scoring algorithm is that it weighs based on placement in the list of top
    clusters  it does not account for what the difference between those elements are or how close they actually
are to the test user  the ith place in the list is always weighed the same  to remedy is  our second attempt with
kernel methods used the same kernel function  but the scoring function for each hotel cluster was redefined as
follows 
score   cluster   

 
x

 

    vi   cluster ki e

i  

where k   is defined to be the sorted k matrix  which  since we do this for each individual test example  can be
considered a vector   and e is some constant  the larger e is  the more we penalize farther values  as the defined
kernel function gives values strictly between   and     we tested several values  and found   to be the most effective  using this scoring method based on the raw similarity score instead of merely placement  we saw a very large
improvement in the accuracy of our predictions across all metrics  and this became the basis of our most successful
algorithm 
we briefly digressed from our algorithm to attempt a different kernel method 
for this kernel method  we attempted to split the feature vector into two vectors 
one comprised of discrete features corresponding to qualitative data  country  location  user ids  etc   and another vector comprised of more continuous and quantifiable data  length of stay  user distance from location  number of children  etc   
lets call these two vectors split from an arbitrary vector x x and x  we also
normalize x to have a norm of    using this notation  we defined the new kernel
figure    mean average
function to be 
  
precision  using different
k   x  y    c  k   x    y       c  x t y   
values of k  for our kernel
where c  and c  are some positive constants such that c    c       the methods  the algorithms
idea behind this kernel was to account for similarity in continuous features when are numbered in order of
discussion in this section 
 

fiweighing user similarity  the rest of the algorithm remained unchanged  to the
second iteration  but overall performed worse regardless of the parameters used  suggesting that similarity based
on jaccard distance is more representative  armed with this knowledge  we returned to the previous algorithm 
our most effective algorithm  based off of the second kernel algorithm  was very similar  with one main
difference  we took advantage of the fact that the hotel market and is package features are very highly correlated
with hotel cluster  as can be seen in the correlation matrix in figure     so we performed the same algorithm but
used a slightly modified kernel function 
m

  x
   xi   yi          xi   yi an di   hotel market f eature           xi   yi an di   is package  
m     i  
which weighed the hotel market feature twice as much as other features  and is package     times as much
when accounting for the jaccard distance  using this kernel method and the same algorithm as before  we attained
the highest total accuracy on our predictions

 

results and discussion

overall  the best methods were the methods that utilized user similarity and kernels to recommend hotel
clusters that other similar users booked  gradient
boosting was also effective  but mean average precision seemed to hit a hard cap at     regardless of the
parameters used or size of the dataset sampled  as
seen in figure    these methods also took longer to
converge given increasing values of k  suggesting the
predictions are more nuanced  the svm performed
very poorly  as did other basic machine learning methods attempted on the data 

 

conclusion and future work

this project provides an excellent case study for applying machine learning algorithms to large data sets figure    precision  recall  f  and map  scores for
lacking obvious structure  it also embodies the chal  each algorithm across the chosen subset of the data 
lenge of recommending items about which we have no  map  refers to mean average precision over   recomfeatures  in addressing these challenges  we demon  mendations 
strated that a creative combination of user similarity
matrices and jaccard similarity outperforms gradient
boostinga technique currently well known for winning kaggle competitions  for future work  we recommend using ensemble stacking methods to combine predictions from various algorithms 

references
    expedia hotel recommendations 
recommendations

   april       web  https   www kaggle com c expedia hotel 

    pedgregosa et al  scikit learn  machine learning in python  jmlr                         may      
web 
    jure leskovec  anand rajaraman  jeffrey d  ullman  mining massive datasets  cambridge  cambridge
university press        print 
    wu et al probability estimates for multi class classification by pairwise coupling  journal of machine
learning research                     web     may       cambridge  cambridge university press       
print 
    gao huming  li weili a hotel recommendation system based on collaborative filtering and rankboost
algorithm  second international conference on multimedia and information technology                 
web     may      
    introduction to boosted trees  xgboost  web     may      

 

fi