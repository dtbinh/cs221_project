cs           final  project  
  
shale  gas  production  decline  prediction  using  machine  learning  algorithms  
  
wentao  zhang  wentaoz stanford edu  
shaochuan  xu   scxu stanford edu  
  
in  petroleum  industry   oil  companies  sometimes  purchase  oil  and  gas  production  wells  from  
others   instead   of   drilling   a   new   well    the   shale   gas   production   decline   curve   is   critical   when  
assessing   how   much   more   natural   gas   can   be   produced   for   a   specific   well   in   the   future   
which   is   very   important   during   the   acquisition   between   the   oil   companies    as   a   small   under 
estimate  or  over estimate  of  the  future  production  may  result  in  significantly  undervaluing  
or  overvaluing  an  oilfield   in  this  project   we  use  the  locally  weighted  linear  regression  to  
predict   this   future   production   based   on   the   existing   decline   curves    then    we   apply   the   k 
means  to  group  the  decline  curves  into  two  categories   high  and  low  productivity   moreover   
principal   component   analysis   is   also   tried   to   calculate   the   eigenvectors   of   the   covariance  
matrix    based   on   which   we   also   predict   the   future   production   both   with   k means   as   a  
preprocess   and   without   k means    at   last    three   methods   are   compared   with   each   other   in  
terms  of  the  accuracy  defined  by  a  standardized  error   
  
l dataset  
the   data   we   used   are   monthly   production   rate   curves   of   thousands   of   shale   gas   wells   as  
below   in  order  to  deal  with  different  lengths  of  different  curves   and  the     production  rate  
data  points  of  some  curves   we  modify  these  data  a  little  bit   we  substitute     data  points  in  
any   curve   by   a   very   small   number             and   we   make   all   the   curves   the   same   length   by  
adding  zeros  to  the  end   for  the  sake  of  being  loaded  into  matlab  as  a  matrix   
shale gas production decline   all curves

     

     

production rate   mcf   day  

     

     

     

    

    

    

    

 

 

  

  

  

  

   

   

   

month

  
figure        decline curves used to learn to predict production in the future  
  
l locally  weighted  linear  regression   lwlr   
our   goal   is   to   predict   the   future   gas   production   of   a   new   well    given   its   historical  
production   data   and   information   from   other   wells   with   longer   history    suppose   that   we  
randomly  choose  a  decline  curve  r  with  n  months  in  total   we  want  to  use  the  first  l  month  to  

fipredict  the  rest   n l   months  of  the  curve   in  order  to  find  curves  from  the  training  set  that  
are  similar  to  r   we  define  the  distance  between  two  curves  by  squared  l    norm   before  
we   calculate   the   distance    we   need   to   filter   the   training   set   by   removing   curves   whose  
history  is  shorter  than  n   then  we  pick  k  wells  from  the  filtered  training  set  that  are  closest  
to  r   give  each  of  them  a  weight  wi  and  make  prediction  for  r  as   
  



f predicted  

ineighbk   f past  existing  

 i  
 i  
w d  f past
 f
  h  f future
 existing measured
 existing



ineighbk   f past  existing  

 

 i  
w d  f past
 f
  h 
 existing measured

  

where  h  is  the  longest  distance   
  
results   
  

  

  
figure  

high productivity  good fit  upper left   high productivity  bad fit  upper right  
low productivity  good fit  lower left   low productivity  bad fit  lower right    

  
we  restrict  the  number  of  neighbors  k  equal  to      in  figure      four  typical  predicted  curves  
are   shown    the   results   are   generally   consistent   with   the   real   values    comparatively    the  
predicted   curves   are   smoother   than   the   real   ones    because   the   predictions   are   the   sum   of  
multiple  training  wells     
  
well        lwlr

    
    
    
    
 

  

  

  

month

  

well        lwlr

    
known curve
predicted curve
test curve

   

   

production rate   mcf   day  

production rate   mcf   day  

    

known curve
predicted curve
test curve

    
    
    
    
 

  

  

  

month

  

   

   

  

fiwell        lwlr

    
    
    
    
 

well        lwlr

   
known curve
predicted curve
test curve

standarlized error

production rate   mcf   day  

    

 
   
   
   
   

  

  

  

  

   

 

   

  

  

month

figure  

  

  

   

   

  

month

predicted curves with l  known months  increasing for a good fitting  error vs  l  

  
well        lwlr

    
    
    
 

  

    

production rate   mcf   day  

production rate   mcf   day  

    

  

  

  

   

known curve
predicted curve
test curve

    
    
    
    
 

   

  

  

  

  

month

month

well        lwlr

well        lwlr

   
known curve
predicted curve
test curve

    
    
    
    
 

well        lwlr

    
known curve
predicted curve
test curve

standarlized error

production rate   mcf   day  

    

   

   

  

   

   

  

 
   
   
   
   

  

  

  

  

   

 

   

month

figure  

  

  

  

  

month

predicted curves with l  known months  increasing for a bad fitting  error vs  l  

  
in  figure     and  figure      we  change  the  known  curve  from  short  to  long   and  plot  the  error  
versus   the   known   months    the   error   does   not   decrease   when   we   know   longer   curve   and  
predict  shorter   the  reason  might  be  that  the  standardized  error  is  defined  as  the  average  
relative   error   of   predicted   months    for   this   reason    in   the   tail   of   the   curve    since   absolute  
values  are  small   relative  errors  are  easily  to  be  large   a  better  error  needs  to  be  defined  if  
we  really  want  to  tell  if  the  prediction  is  better  with  longer  known  curve   
  
l principal  component  analysis   pca   
since  each  well  has  a  history  as  many  as  tens  of  months   intuitively   we  want  to  reduce  
the   dimensions   of   time   and   keep   the   intrinsic   components   that   reflect   production   decline   
first   of   all    we   filter   the   training   set   by   removing   the   wells   whose   history   is   shorter   than   the  
total  months  n  of  a  test  curve   after  the  normalization  on  the  data   we  eigen decompose  the  
empirical  covariance  matrix  and  extract  the  first     eigenvectors  as  the  principal  components   
then   we  fit  the  known  part  of  the  test  curve  by  using  a  linear  combination  of     eigenvectors   
the  coefficients   of  the  linear  combination  are  calculated  from  linear  regression   

yknown   ul     
and  we  predict  the  future  decline  curve  as   

yestimate   u h     

fiwhere  yknownrl  is  the  normalized  known  history  of  the  test  well   ulrl    is  the  eigenvectors  with  the  
first  l  dimensions   our  estimation  is  therefore  yestimate   
  
results   
  

  

  
figure  

high productivity  good fit  upper left   high productivity  bad fit  upper right  
low productivity  good fit  lower left   low productivity  bad fit  lower right    

  
as  can  be  seen  from  figure      the  prediction  is  either  too  smooth  or  too  variant  compared  to  the  
real  data   this  is  because  at  the  fitting  step     is  either  underfitted   high  variance   or  overfitted  
 low   variance     another   problem   in   pca   is   that   all   the   training   wells   have   contribution   to   the  
estimation   which  makes  it  unprecise  for  very  high  or  low  production  prediction   
  
l pca  after  k means  
if   we   assume   high productivity   wells   are   similar   to   each   other   and   low productivity  
wells  are  similar  to  each  other   we  can  group  all  the  decline  curves  into  two  categories   we  
modify   the   k means   method   to   be   applied   into   this   real   situation   that   different   decline  
curves  have  different  dimensions   we  calculate  the  distance   between  a  centroid  and  a  curve  
by   using   the   dimension   of   the   shorter   one    as   comparing   two   figures   in   figure        this  
modified  k means  method  is  good  enough  to  distinguish  high productivity  wells  from  low 
productivity  wells   
  

  
  
  
figure   decline curves in the high productivity wells  left  and the low productivity wells  right   

fithen   we  run  pca  again  after  clustering  the  original  decline  curves  by  k means   
  

  

  
figure  

high productivity  good fit  upper left   high productivity  bad fit  upper right  
low productivity  good fit  lower left   low productivity  bad fit  lower right    

  
from   figure        we   can   see   that   although   the   underfitting overfitting   problem   still   exists    the  
results   are   better   than   the   original   pca    this   might   be   due   to   we   add   the   l     norm   distance  
information  into  the  pca   which  makes  it  an  integrated  method   
  
l discussion  
  

figure  

errors of three methods calculated from leave one out cross validation  

  

  
we  apply  leave one out  cross  validation  to  all  the  three  methods   compare  the  predictions  with  
real  production  data  and  calculate  the  average  relative  errors  as  in  figure     and  figure      we  also  
define  a  threshold  value  to  avoid  the  extremely  large  errors   the  reason  we  do  this  is  that  one  
extreme  value  can  make  the  average  of  all  relative  errors  really  huge   but  these  extreme  values  
are  due  to  the  shutting  down  periods  of  the  wells   when  the  production  is  nearly  zero    figure     
verifies  our  intuition  that  lwlr  is  the  best  among  the  three  methods  because  no  information  is  
lost   due   to   dimension   reduction    pca   has   the   largest   relative   error   among   three   methods  
because   higher   order   principal   characteristics    reflecting   the   details   of   decline   curves    are   not  
included    k means   helps   cluster   the   wells   into   high   and   low   productivity   classes    which   improves  
pca  with  the  availability  of  that  prior  information   

fi