human action recognition using cnn and bow methods
stanford university
cs    machine learning spring     

max wang

ting chun yeh

mwang   stanford edu

chun    stanford edu

i  introduction
recognizing human actions is a popular area of interest due to
its many potential applications  but it is still in its infancy 
successful human action recognition would directly benefit data
analysis for large scale image indexing  scene analysis for human
computer interactions and robotics  and object recognition and
detection  this is more difficult than object recognition due to
variability in real world environments  human poses  and
interactions with objects  since researches on human action
recognition in still images are relatively new  we rely on methods
for object recognition as basis of our approaches  in particular 
we were interested in seeing how convolutional neural networks
 cnn   perform in comparison with past feature selection
methods such as bag of words  bow   also  we experimented
with various supervised and unsupervised classifiers  examined
our methods properties and effects on our action data set  and
also pre processed our data set in order to better our results 

ii  related work
in past decades  many ideas proposed to solve this problem 
some people put interest on understanding human object
reaction  bourdev et  al proposed poselet to recognize human
body part and further research on human pose relation  although
those methods have very impressive result  hand crafted feature
method still can t be very generalized to all purpose  they all are
used for specific goal 
to conquer that  krizhevsky et al      first used convolutional
neural network cnn  for image classification in      
convolutional neural network is a powerful method because 
unlike handcrafted feature methods  it learns features from whole
image through forward and backward processes in deep layer
structure  in       ji  shuiwang et al      first apply
convolutional neural network to recognize human action in
video and popularized cnn methods  however  cnn is bad on
localization  to overcome the difficulties  girshick et al     
proposed r cnn which combine region proposal and cnn 
although cnn has promising result  its training is a huge task 
to reduce the cost  people used a pre trained model  such as by
chi geng et al      use pre trained cnn model to learn features
from images and classify images by svm  to reduce the overfitting problem of cnn  srivastava et al  gave dropout which
prevent neural units from co adapting too much to address overfitting problem 
to fully understand cnn  we looked into feature extracted by
cnn  we thought some preprocessing to image will be helpful
to human action recognition  our goal is to recognize human
action  we though background should be irrelevant noise  to
reduce training cost  we will use pre trained model  we will finetune it and change some hyperparameter to improve the
prediction 

 

motivation came from neither partner having prior cnn experiences

iii  methods
data and setup
we utilized caffe  python  and pycaffe   and matlab to create
and run our cnn and bow models  we rented a server with
  gb of harddrive space and  gb of nvidia gpu memory 
costing roughly      including usage time  due to hardware
limitations  we had to reduce our data set size  so we chose to
classify   actions out of the stanford   data set  using     
images for training  and validation  for cnn   and     images
for testing  with such small data set  we allocated more images
for training  which only had     images per action for training 
instead  we used a train val test ratio of            we were at
risks of overfitting  but we took precautions to prevent
overfitting 
as a default  we used the images as given in the data set  then 
we applied cropping to our images in two ways  one with a tight
bound to isolate our subject and nearby objects  and one that is
    larger than our tight bound to capture some background
information  lastly  we pre processed images to a color
segmentation process using k means 

bag of words
in general  objects in an image can be described by feature
descriptors  forming a histogram for the image  a collection of
histograms from different images form the bag of words
 bow  model  which can be learned by a classifier  during
training  we used a scale invariant feature transform  sift 
method to extract features  then we utilized spatial pyramid
matching  spm  to obtain histograms from increasingly small
sub regions  stacking these histograms together helps us
maintain spatial information  we then used k means method to
cluster the final histograms into k code words  during testing 
match the histogram of the input image with our bow model 
bow is unaffected by position and orientation of objects in the
image  and the spm method gives us more spatial information
to help us localize objects 

figure         caption

convolutional neural network
cnn is a different method of obtaining image features and
training on feature representations in high dimensional space  it
has been quite successful in recent years  since its introduction
in       alex krizhevsky       

fiwe used caffenet     architecture as the basis to our
experiments  it is similar to alexnet  but pooling is done before
normalization in caffenet  in brief  caffenet has   convolution
layers followed by   fully connected layers and a softmax layer 
we trained using pre trained weights  which have ran for
        iterations  to give better generalization and to prevent
overfitting our data  this is our control case 

images together in  d space  with t sne  we set similarities of
high dimensional points  distribution q  and low dimensional
points  distribution p  as two different joint probabilities  where
a higher probability indicate similarity  the cost function is then
a kullback leibler divergence of distribution q from p 

          





this leads to the minimization problem 

 
                             



coincidentally  since t sne is an unsupervised method to
cluster our data  we also tested to see how well it classifies our
data by applying a k means algorithm on top of t sne 
figure   caffenet architecture
then  we experimented with changing learning rates and
hyperparamters for each layer  which are  kernel size  padding
amount  stride  and number of outputs  hyperparameter tuning
involves changing the sizes of the cnn layers  creating a very
different cnn  despite having the same number of layers  to
study the effect of locality sizes on our results  we conducted
two tests with the first layers kernel size being    and   
respectively  and different amounts of paddings were used to
keep other layers the same  in a third test  we also changed the
first layers kernel size from    to     then decreased our kernel
sizes in the following layers until the  th layer matches the
original    x  x   dimension 
we also created cnns from scratch  using our customdefined layers and hyperparameters  below is a summary of our
three custom models  we only show kernel size  k  since we
only adjusted other parameters to suit our new k  
custom    conv k      relu  pool  norm 
conv k     relu  conv k     relu  fc  softmax
custom    conv k      relu  pool  conv k   
relu pool conv k    relu poolfc
fcsoftmax
custom    conv k      relu  pool  conv k    
relu  pool conv k     relu  pool  fc  dropout
 fc  softmax
our custom cnn   is a small cnn with   layers  the other
two are larger  the difference between our custom cnn   and  
is that custom cnn   has a dropout layer  this is to prevent our
network from overfitting by giving each neuron a    
probability that its activation will become zero in each iteration 
in other words  a dropout of data  this avoids co adaption of
units 
we also ran googlenet for comparison  which uses an
atypical architecture embedded with inception layers that
contain multiple convolutions  in terms of recognition 
googlenet is known to yield better results than caffenet  but it
is more difficult to fine tune so we kept caffenet as our basis 

t distributed stochastic neighbor embedding
we used the t sne algorithm to help us visualize the features
obtained from the last fc layer of the caffenet in relation to our
actual data  features from this layer is a high dimensional
histogram for each image  and t sne allows us to cluster these

cnn   classifier
similar to using the t sne algorithm  we extracted
activations from the last fully connected layer of our cnns as
features and put them through various classifiers  we are
interested in using features from cnn for image classification
problem  but skip the softmax layer that caffe uses 

figure   our pipeline  applying svm on extracted features

support vector machine
svm is to find a hyperplane that give the largest minimum
distance to training data  it is to optimize


  

 
      
 
  

                            
               
the second term  
    let us can have margin less than    c
control the two goal want to achieve  keep   small and
make margin less than   
   

to use this linear svm on our multiclassifier data set  we used
one vs one comparison  we first experimented with one vs
all method then used one vs one for better results 
multi class support vector machine
we used one versus one for our dataset 
for one versus one method  if we have n class  there will be
n n      classifier 
each classifier is for two classes from our dataset  we are
going to solve the following optimization problem
min               

    
 
         
 


  
 
                        
 
                        
 
                        

fieach classifier will vote to one class  and the most voted class will
be final result
additive chi square kernel
additive chi square kernel does normalization to the feature
histograms  so that spikes in the histograms will not be heavily
affect the result  we used the one vs one comparison 
       


trained weights and giving   learning rates to some of the
weights should provide enough generalization 
we examined caffenets first layers outputs and noticed that
while caffeent can capture large features correctly  it sometimes
recognizes background noise and irrelevant information as key
features 

     
       

k nearest neighbor algorithm
choose an integer k  knn classifier will find the nearest k
neighbors of x from training data  according to the class of
nearest k point  it give conditional probability for each class 
             

 
      


random forest
a random forest method is an ensemble method  it build a
series of simple trees which are used to vote for final class  for
classification  rf predict the class that predicted by most trees 
the predictions of the rf will be the average of the predictions
by simple trees


 
      

  

iv  experiments and results
default cnn
we first obtained data from running our data with pre trained
weights of caffenet and googlenet  we obtained these
accuracies 
model
caffenet
googlenet

top  accuracy
      
      

we then examined some properties of caffenet  we verified
that our model has converged by looking at the   st layer weights
to verify theres no noise 

figure   nicely converged  st layer weights  left  vs noisy weights
 right 

testing on training data yielded an accuracy of         this
may indicate some overfitting  but we believe it is mostly due to
the original models doing well  this is because using pre 

figure   first layer outputs   nd row shows main features and local
objects are captured   rd row shows some noise is captured 

for improvement  we believe it would be beneficial to filter
out noise and have larger locality of features 
it becomes difficult gauging the activations in later layers 
due to the locality of each neuron  so it was not used 

custom cnn
based on preliminary results  we wanted our cnn to capture
larger features and ignore smaller objects or noise  hence  we
created our custom cnns  as described in the methods section 
cnn
kernel size  
kernel size   
kernel size   
custom cnn  
custom cnn  
custom cnn  

top   accuracy
      
      
      
      
      
      

none of our custom cnns matched the default models
accuracy  this could be we did not have the time to train our
models for long enough because we could only run for       
iterations  which takes half a day  but we noticed that the   st
layers weights appear to converge nicely  so its also possible
that the default caffenet was designed to be the best cnn of its
kind  hyperparameter tuning is  we realized  an optimization
problem of its own 
we noticed that  unexpectedly  a larger kernel size at the first
convolution layer yielded lower accuracies  we compared the
 st layers outputs and noticed that  while a larger kernel size
does give us larger locality and capture bigger features in the
images  as intended  it is perhaps too broad for our cnn  the
smaller kernel size  on the other hand  captures too much detail 

fifigure   layer   outputs  same column is from the same model  from
left to right  k    k     k  

bag of words
from looking at bow code words  we also thought it would
be beneficial to filter out background noise 

svm
 chi square 

      

      

      

knn
rf

      
      

      
      

      
      

we saw that for the most part  k     performed better  but this
may not be optimized  since number of code words is heavily
related to the properties of images  so there is no best way to find
k but trial and error  like finding cnn hyper parameters 
according to our result  k     is better  so we can deduce that
sift doesnt use as many distinct features from our images  so
that we dont need too many words 
a more useful takeaway is looking at our results of our
cropping  after we cropped the image based on tight bounding
box  we saw that accuracy actually dropped  this is contradictory
to our expectation  we thought that removing background noise
would reduce error and improve our result  however  we realized
that contextual information is actually important for classification 
we then expanded our bounding box by     times to include
local background information  as predicted  we saw an
improvement in our result 

figure   bow features

we tried to filter out the background by changing our k size
for the k mean cluster  but its not inherently obvious how many
codewords to use  we tried k     and k     
figure   plot of our results

bow    layer k     
original
cropped
svm linear 
svm
 chi square 

      
      

      
      

cropped
    x 
      
      

knn
rf

      
      

      
      

      
      

bow    layer k     
original
cropped
svm linear 
svm
 chi square 

      
      

      
      

cropped
    x 
      
      

knn
rf

      
      

      
      

      
      

bow    layer k     
original
cropped
svm linear 

      

      

cropped
    x 
      

cnn   classifier
fine tuned caffenet
top   accuracy
original
caffenet
caffenet svm
 linear
caffenet svm
 chi square 
caffenet knn
caffenet rf

cropped

      
      

      
      

cropped
 larger 
      
      

      

   

   

      
      

      
      

      
      

as shown above  if we train svm and other classifiers on top
of features extracted by cnn  we achieve better results than
using cnn alone  this was surprising  since cnns own
accuracy was already high 
we again thought it may be due to the overfitting issue
described in previous section  so  when we use svm for
classification  we made svm resistant to overfitting by tuning the
parameter c 
although kernel trick perform better than linear svm in bow
model  we didnt use it on cnn feature because cnn feature is

fivery high dimension  using kernel on cnn will be time
consuming with not a better result  so  we simply use linear svm
here for cnn feature 
we observe that svm  knn  rf all perform well on our dataset
when using cnn features  even though in our bow model knn
and rf both did badly  even though cnn is not perfect at
extracting features  it is much better than bow model  which takes
in too much noise from the image 
we saw cnn knn have even higher accuracy after we
cropped the image  table below show some predictions using
cnn knn on different cropped images  we can see that the
background is a contributing factor  image   was classified as
climbing because of the wall background  so does the image   
after we cropped the image and put tight bounding box on action 
image   and image   became right but image  was missed 
without rock in image   it was classified as jumping  in our
expanded bounding box  predictions for images      and   became
correct  
we can see that the background is necessary when the action
relies on the environment  some action is highly related to the
background  like climbing  where as some do not  like jumping  if
we could recognize the relationship between the background and
the action  we can achieve better results 

un fine tuned caffenet
top   accuracy

caffenet
caffenet svm
 linear
caffenet knn
caffenet rf

original

cropped

      
      

      
      

cropped
 larger 
      
      

      
      

      
      

      
      

hand  actions that require environmental interactions  jumping 
climbing  are not as obvious  also  images taken from afar or
from unconventional angles would be harder to cluster  this
could be due to the introduction of background noise or
occlusion  it becomes obvious that pre filtering our data set
would be an important step prior to training 

conclusion
we experimented with and validated many methods and
techniques in our project  the most useful takeaways for future
work is that  for either supervised or unsupervised learning  it is
important to include sufficient but not excess background and
contextual information prior to training for human action
recognition  the key point is how to select the region from image 
we saw that cropping is a strong tool to use  but we cannot crop
too much or too little background 
then  we found that knn performs well with fine tuned
caffenet model on our dataset  knn is a very fast calculating
model  for future work  we will test and evaluate knn using the
whole    action dataset 
in general  cnn is a great tool at extracting features from
images  even though it lacks the ability to distinguish subject 
object  and background  similar to bow  even so  it significantly
outperforms bow model  as we expected from literature  for small
size dataset  using svm  knn on cnn feature gives even higher
accuracy than cnn itself  we thought that cnn could overfit such
small size dataset  in small size dataset  it may be more accurate to
combine svm knn with cnn feature 

v  references
   

we also tested the classifiers on non trained caffenet 
surprisingly  we found svm give a pretty good accuracy  it is
only a little lower than fine tuned feature  knn and rf are not
like svm  their accuracy is much lower than fine tuned caffenet
feature  this confirms that caffenets pretrained model does a very
good job at recognizing objects  such that when we insert our data
set we do not need to train much 

   

feature examination  t sne

   

after applying the t sne method  we noted that the accuracy
was only         much like our other classifiers  whats more
interesting  though  is visualizing our data  we see that images
with clearly distinct objects  holding an umbrella  riding a
horse  playing guitar  etc  are more distinguishable  on the other

   

   

   

alex krizhevsky  ilya sutskever  geoffrey e  hinton        imagenet
classification with deep convolutional  nips proceedings 
chi geng  jianxin song        human action recognition based on
convolutional neural networks with a convolutional auto encoder   th
international conference on computer sciences and automation
engineering 
girshick  ross  et al  proceedings of the ieee conference on computer
vision and pattern recognition   rich feature hierarchies for accurate object
detection and semantic segmentation        
ji  shuiwang  et al  n d   d convolutional neural networks for human action
recognition  pattern analysis and machine intelligence  ieee transactions
on                     
jia  yangqing and shelhamer  evan and donahue  jeff and karayev  sergey
and long  jonathan and girshick  ross and guadarrama  sergio and
darrell  trevor         caffe  convolutional architecture for fast feature
embedding   arxiv preprint arxiv           
krizhevsky  alex  ilya sutskever  and geoffrey e  hinton        imagenet
classification with deepconvolutional neural networks  advances in neural
information processing systems 

fi