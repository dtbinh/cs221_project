using convolutional neural networks to perform classification on state farm
insurance driver images
diveesh singh
stanford university
    serra mall  stanford  ca
diveesh stanford edu

abstract

   previous work and background
convolutional neural networks is where machine learning meets computer vision to tackle the problem of object
detection  every month  the state of the art in convolutional neural gets pushed further and further  currently  the
leading model is the inception v  model  that achieved a
      top error rate on the imagenet dataset  the inceptionv  model has    trainable layers  these kinds of models
take about   weeks to train  along with a substantial amount
of computing power 
another top framework in this field was proposed by girshick et al   this framework divides classification into two
steps  where the first step performs object detection and the
second step performs cnn recognition  along with the two
methods listed above  there are several other ways to modify
and leverage cnns in order to perform object recognition
tasks      because of their previous success  we decided to
explore their performance in our own visual detection task 

for the state farm photo classification kaggle challenge  we use two different convolutional neural network
models to classify pictures of drivers in their cars  the first
is a model trained from scratch on the provided dataset 
and the other is a model that was first pretrained on the
imagenet dataset and then underwent transfer learning on
the provided statefarm dataset  with the first approach  we
achieved a validation accuracy of        which is not much
better than random  however  with the second approach  we
achieved an accuracy of        finally  we explore ways to
make these models better based on past models and training
techniques 

   introduction

   techincal approach

this paper will discuss using convolutional neural networks  cnns  to perform deep learning on an image dataset
provided by state farm insurance  state farm has provided
a dataset that contains several pictures of drivers in their
cars performing specific actions  detailed later in the paper 
our task is to be able to classify these images based on the
action that the driver is performing  many of these images
show drivers performing actions that are unsafe while driving a car  the motivation behind this problem stems from
state farms need to understand what kinds of behaviors
drivers are engaging in so that they can set their insurance
policies accordingly  for example  if texting and driving is
a common cause for accidents  which it often is  insurance
companies can increase their premium for drivers that are
more likely to be texting and driving  by analyzing this
data  we will not only learn what trends drivers tend to exhibit  but also be able to automate the process of pinpointing
the actions that lead to accidents 

     dataset
the dataset contains about   gb of photos of drivers
in their cars  which translates to approximately        pictures  each driver has several images associated with them 
and the images themselves are divided into    classes 
which leads to about       images per class  the classes
are as follows 
   c   safe driving
   c   texting   right
   c   talking on the phone   right
   c   texting   left
   c   talking on the phone   left
   c   operating the radio
    

fibeing of category i

   c   drinking

                                          

   c   reaching behind

to generate this probability vector  we will be applying a
softmax activation to the final layer of the network  the
probabilistic interpretation of the softmax classifier is as follows

   c   hair and makeup
    c   talking to passenger

efyi
p  yi  xi   w     p fy
j
je

some examples from the dataset can be seen below

with convolutional neural networks  there are several different update rules one can use to update each of the weight
matrices  bias vectors and convolutional filters  the update
rule that has been known to work best in practice is known
as the adam update rule  which consists of the following 
m   beta  m      beta   dx
v   beta  v      beta    dx    
x    learning rate   m    np sqrt v    eps 
the hyperparameters that we set are the following
   learning rate         
   beta  decay      

for the purposes of this paper  we divide up our dataset
into a training set and a validation set      of our dataset
will make up our training set  and     will makeup the
validation set 

   beta  decay       
   weight initializer   gaussian
the purpose of using this architecture was to provide a baseline for how well a convolutional neural network can perform on this type of dataset 

     preprocessing
to ensure that the data is primed for usage in a convolutional neural network  we shrink each image to    x   
pixels  this size was selected based on the default values
that are used in a popular convolutional neural network
model known as vggnet     resizing of the images was
done by a simple python script 

     

this next architecture leverages an existing architecture
that has been proven successful in the past  known as
vggnet     vggnet    has been trained on the imagenet dataset  and therefore is good at recognizing images
in that dataset  however  we want our model to be able
to distinguish the images in our dataset  to do this  we
use a process called transfer learning  transfer learning
is the process of training only some of the layers in a
cnn on a new dataset  while leaving the majority of
the layers untouched  in our case  we can preload the
pretrained weights of the vggnet    model  found at
https   gist github com baraldilorenzo   d       aaad a  d   
then  we can train the top layers on our state farm dataset 
without affecting the rest of the layers  by doing this  we
get the advantage of using a pretrained model that can
perform basic feature extraction  i e edge detection  sift
descriptors etc   in the convolutional layers of the network 
and then the final  fully connected layers can use these
features to make classifications specific to our problem 
the hyperparameters that we set are the following

     classification
for performing actual classification  there are various
types of architectures and training techniques we can use 
we will evaluate each different architecture and compare
the results to see which architecture performs best  also 
within each architecture technique  there are various different hyperparameters we can adjust in order to fine tune the
model 

     

approach    transfer learning on vgg network

approach    small network

the first architecture involves using a small     layer cnn
that contains   convolutional layers and   fully connected
layers at the end  the output of the final layer is an individual probability vector for each image  where each entry i in
the vector indicates the predicted probability of that image
    

fi   learning rate         

the loss function above is essentially identical to the categorical cross entropy loss function  so it was a straightforward choice to minimize that loss function during gradient
descent 
however  intuitively  this loss function does not provide an
intuitive evaluation of the model  instead  we establish our
own criterion for evaluating the model  as mentioned earlier  every training example outputs a vector of the following form

   beta  decay      
   beta  decay       
   weight initializer   vggnet    weights
a

visual

of

the

model

is

provided

below

   

                                          
because each training example belongs to one class  we assign the training example to the class whose probability is
the highest and generate a vector of binary values  then 
we compare the class assigned by the classifier and its true
class to evaluate the accuracy  the official accuracy metric
used

     technical resources  limitations and solutions
due to the nature of convolutional neural networks  we
need a significant amount of computing power and time in
order to successfully train a model to the point where it can
be used in practice  for example  training the inceptionv  model took   weeks to train on   nvidia tesla k  s 
which are some of the best gpus currently in the market 
for the purposes of this paper  we created a gpu instance
on amazon aws which had a reasonable amount of processing power and memory to perform the robust calculations required to train a convolutional neural network  however  it was not robust enough to handle a large training set 
which is why we had to limit the number of training examples to         to get around this  we trained each model
with the first        pictures for    epochs and noted down
the results  then we took that model  and trained it on a
completely new set of        pictures for    epochs  then
finally  we took the remaining       images and trained the
model for    epochs  while this was not ideally how the
model would have been trained  given the computing resources we had  it provided us with a way to expose the
model to all of the images in order to make it more generalized 

acc  

     loss and accuracy results
we trained both of the networks described earlier  a network trained from scratch  and a pretrained network with
transfer learning performed on it   as detailed earlier  we
had to train our model   spurts  twice with approximately
       images and the final time with approximately      
images due to technical limitations  once we trained the
whole model on these images  we evaluated our model by
taking the     images that were not used in the training set
and computing the categorical cross entropy across all these
examples  the training loss after    epochs of training on
the first        examples was
losstrain         
and the validation loss was
lossval         
the

   results
     evaluation metric
we measure the quality of our approach based on a loss
function  the higher the loss  the lower the accuracy   the
loss function used for accuracy is as follows
logloss   

numcorrectp redictions
numexamples

n m
  xx
yij log pij  
n i   j  

where n is the number of images in the test set  m is the
number of image class labels  and yij is the true value of
example i for attribute j  as posted on the kaggle website 
    

graph

is

as

follows

fiwhile we only trained for    epochs  it seemed that the loss
still had not converged and required several more epochs
for it to fully converge  this makes sense because we are
training this model from scratch  and so it requires several
epochs to actually understand the trends in the data  after
exposing the model to the rest of the training set in chunks 
we achieved losses of

after exposing the model to all        images  we achieved
losses of
losstrain         
lossval         
after performing transfer learning on this model  we
achieved a training accuracy of       and a validation accuracy of      clearly  this is better than random guessing 
so our model was able to understand the trends in our data 

losstrain         
lossval         
now  with a trained model  we can evaluate the accuracy
on a dataset of     images  the accuracy was       
given that there are    possible classes for a training example  random guessing expects to achieve     accuracy 
we explore reasons as to why this was the case in section
   we can also observe how the both the training accuracy and validation accuracy change as we train the model 

we analyze the reason for this in section  

   analysis
     dataset
a significant challenge that comes with trying to design
an accurate model is overcoming issues with the dataset 
while there wasnt much noise in the dataset  i e images
that were not of a driver   a lot of the images were very similar because they were all of a driver sitting in a car  so it
was difficult for a model to learn appropriate distinctions 
specifically  the top layers of the network are responsible
for learning what features contribute to an images class 
some of the distinctions that were especially difficult for
the model to learn were c   hair and makeup   c  and c 
 talking on the phone   left  right   due to how similar they
were  while cnns are generally able to implicitly learn
subtle features like hand placement  facial recognition  and
light dark patches  these   categories were very similar in
the sense that all of them involved having the drivers hand
near their face  a potential solution could be to add more
fully connected layers  as adding more parameters gives the
model the ability to make more subtle distinctions  perhaps
allowing for more overfitting in the fully connected layers
of the network would help 
also  there is a general issue that comes up while doing
transfer learning on a pretrained model when using your
own dataset  our dataset was exclusively pictures of people
driving  whereas the pretrained model understood features

for the second architecture  vggnet    with transfer
learning   the losses after    epochs on        examples
was
losstrain         
lossval         
the

graph

of

the

loss

is

as

follows

    

fiof all      classes of the imagenet dataset  intuitively  it
does not make sense that our model need to understand aspects of all      classes  but should devote more of its attention to making the fine grain distinctions between drivers 

effects of different optimizing techniques  i e  adagrad 
adadelta  vanilla sgd etc   to see how the speed of convergence changes and how it affects the accuracy  also  we
can try different nonlinearities after each layer of the network  leaky relu as opposed to regular relu  and see
how that affects the above metrics 
one of the biggest drawbacks to using cnns is the amount
of time it takes to train one to the point where its usable 
therefore computational efficiency is of utmost importance 
specifically with the first approach  training that network
till convergence would have taken much more time and
computing resources  experimenting with fewer layers in
this model is a potential way to reduce the amount of time
it would take and also reduce the potential for overfitting 
however  since the model was nowhere close to convergence  it is difficult to evaluate whether the model would
have high bias or high variance given more time to train 
specifically for transfer learning on the second approach  it
would make sense to try training more or fewer layers and
see how that affects the rate of convergence and overall accuracy  most of the time in transfer learning  we only train
the fully connected layers becaue the convolutional layers
act as feature extractors  but there have been advantages in
also training the final convolutional layer  so this would be
an avenue worth exploring 
as mentioned earlier  the dataset was not only a set of individual pictures  but could actually be grouped into sequences of frames from a video  while convolutional neural networks do not normally work on videos by default 
there have been some techniques researched by researchers
here at stanford that make classification of videos possible  some of the techniques detailed in the paper are early
fusion  late fusion  and slow fusion     while these
techniques are distinct  they all share the same general approach  while looking at a frame  it takes into account the
previous frames and the following frames to incorporate the
movement going on in the video in the process of classification 
in future work  we hope to evaluate other types of models 
including ones that do not involve deep learning and compare the results  taking the predictions of several models
and averaging them to come up with a more robust prediction would also be a productive avenue  in addition  we
hope to design models that can take the temporal dimension
into account with the techniques mentioned above to extract
more information out of the dataset 
references

     results
with the first approach  the results were unfortunately
very close to that of random guessing  this can likely be
attributed to the fact that the neural network did not have
enough time to train on the existing dataset  as mentioned
earlier  most successfuly neural networks that are trained
from scratch require on the order of   weeks to train fully on
very powerful gpus  regardless  it provided a guideline for
how to design the second approach  as the main disadvantage with this first approach was the amount of computing
resources needed to train this model  given the availability of more resources  however  it would be worthwhile to
try training for much longer  and decaying the learning rate
over time  this is because when training begins  the learning rate needs to be large so that the updates on the weight
matrix can happen on a larger scale  however  as the model
gets closer and closer to convergence  a smaller learning
rate is required as that will allow the model to fine tune itself 
with the second approach  we were able to achieve an accuracy that was substantially better than random guessing 
which meant that our model was able to learn some trends
in our data  this is likely due to the fact that our pretrained
model could already perform basic object recognition  so
the entire training time could be devoted to learning the features of our dataset and adjusting the weight matrices accordingly  something interesting to note  however  is that in
most cases  the validation accuracy tends to be lower than
the training accuracy  as the model has seen the examples
that are used to calculate the training accuracy   generally 
when the validation accuracy is significantly lower than the
training accuracy  it is a result of overfitting  however  in
our case  the validation accuracy was higher than the training loss  the higher the validation accuracy  the more generalized the model is  the fact that the training accuracy
was lower than the validation accuracy indicates that the
model may have high variance  as its ability to recall images that had been seen before was weak  to fix this  it
would be worthwhile experimenting with different learning
rates and training the model for a much longer time  one of
the causes of overfitting is training the model for too long 
however  in this case  it was probably necessary to train the
model for longer if the validation accuracy was higher than
the training accuracy 

    r  girshick  j  donahue  t  darrell  and j  malik  regionbased convolutional networks for accurate object detection
and segmentation  ieee transactions on pattern analysis and
machine intelligence  tpami        
    a  karpathy et al  large scale video classification with
convolutional neural networks  cvpr       

   further improvements

    z  yan  h  zhang  r  piramuthu  v  jagadeesh  d  decoste 
w  di  y  yu  hd cnn  hierarchical deep convolutional neural
network for large scale visual recognition  iccv       

there are several improvements that can be made to the
above approaches  it would be interesting to explore the
    

fi