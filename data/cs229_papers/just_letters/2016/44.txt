cs    project
final report

topic retrieval and articles recommendation
yu wang  xinyu shen  jinzhi wang
abstract
how to search articles effectively is significantly important to researchers in academia  currently researchers use
search engines like google scholar and search by keywords  eg  machine learning  topic modeling  k means
etc    the typical search results are large amount of articles  which match the keywords exactly but are on many
different topics  it is very time and effort consuming to read through the papers manually and select out desirable
ones  we propose a solution to search papers by topic  we apply the vector space model and tf idf to vectorize
and model documents  then we use k means algorithm and latent dirichlet allocation  lda  method to train
on a large document set and analyze every paper in it to generate its distribution over topics  the algorithm
recommend papers to reader by analyzing whatever the reader has read and compare with the distribution of all
papers in the training set  the papers with most similar distributions are those recommended to the user  in
this way we realize searching in the database by topics rather than keywords  we use      papers from past
cs     course project reports as data source  without any title or keywords matching  experimental results
have demonstrated that our method can successfully extract the topics beneath the words of an article and
recommend closely related ones 
keywords
vector space model  text feature  tf idf  k means  lda  topic distribution  recommendation

contents
 

introduction

 

    motivation                                                          
    objectives and outline of our approach              
    project benefits                                                  
 

background and literature review

 

 

methodologies and algorithms

 

    data and preprocessing                                      
    training algorithms                                            
k means  latent dirichlet allocation  lda 

    recommendation algorithm                                
 

results and discussion

 

    k means clusters visualization                            
clusters  visualization

    recommendation                                                
acknowledgments

 

references

 

   introduction
    motivation
in academia  a proper method to effectively extract topics
from papers and search for papers with similar topics have
long been desired  currently  people are familiar with searching by keywords using tools like google scholar  however 
these searching results are merely based on the matching of

input keywords  but we want to search by the semantic contents  which is beyond keyword matching  even by skimming
condensed abstract section  substantial time will be consumed
to acquire the topics on literature review  especially considering the vast volume of literature on internet  it is almost
impossible to retrieve accurate topics and find best matching
papers manually 
    objectives and outline of our approach
the problem we are to address is that given a user input  a
paragraph of interest or several journal papers which we call
reading list  how to create a recommendation list of papers
to help the user decide a clear order of pursuing and provide
him her with abundant information as well 
as a rough depiction of the framework of our proposed
method  we will first need a collection with enough literatures  ideally across many different areas  it would be the
best if we have a literature database like sciencedirect  then
we process on the papers in the collection  including format
converting  word trimming  high and low frequency word filtering and non english word removal to obtain a training set
formed by word frequency statistics  then we will calculate
topic distribution for each document  given the number of topics arbitrarily selected  when making recommendations  the
algorithm analyze the reading list and compute its topic distribution  then the algorithm judge the similarity between this
distribution and every documents distribution in the training
set and recommend the most similar ones 

fitopic retrieval and articles recommendation     

    project benefits
our method can be used as an effective way of paper recommendation  first  by topic modeling  we will improve both
the efficiency and accuracy of paper searching towards a particular topic  second  by conducting topic search  it is easier
to find important literature bridging two different academic
fields 

   background and literature review
in our project  we use an advanced topic modeling method
to help researchers explore and browse large collection of
archives  historically  topic modeling can be used to improve
the accuracy of classification by their contextual information 
for example  guo et al     studied the geographical location to
classify different type of tours  ka wing ho    tried to solve
the problem of genres classification of moves by considering
it as a multi label classification problem  in their work  they
both used topic models to organize information 
in recent years  some research concerning topic modeling focused on how to train the dataset in the environment
of twitter     the authors demonstrated that topic models
training quality will be influenced by length of document  latent dirichlet allocation  lda  model and author topic at 
model  which is an extension of lda are used and compared
in this research  and lda had better performance in their
settings  one more interesting paper explores the topic and
role of author in a social network     this paper introduced
the author recipient topic  art  model  which is based on
lda and at models  considering the attribute of topic allocations with factors tuned from sender recipient perspective 
another paper introduces the relational topic model  rtm  
which is a hierarchical model with network structure and node
attributes     their research focus was on words in each documents and they realized summarization of document network 
with predicted links and words amongst them  the rtm
model is built upon mixed membership model  and it also
reuses the statistical assumptions behind lda 
previous research and other related works will be served
as reference and comparison of our research work  we incorporated some ideas  applications and structures from them as
a reference 

information lost or distorted using an online tool http   
pdftotext com   we then remove all numbers  signs 
symbols and non english letters and convert all english letters to lowercase  next porter stemming algorithm is applied
to remove the morphological endings of the words in the documents  for example make  makes  making are stemmed
to mak and surprise  surprises  surprising  surprised  surprisingly to surpris  the last step of text processing is to
remove trivial stop words like the  and  was  we etc  from the
data  after all these preprocessing steps  we finally have the
non trivial  and sequential  as in the original article  english
words written in plain txt files 
denote the document set by d    d    d      dm    the text
feature set bt w    w    w         wn   
we apply the vector space model on all the documents 
every unique stemmed word in a document is treated as a
dimension  the magnitude of vector component in that dimension is the total number of word appearance in that document 
repeat this process for all the documents and we generate the
text feature matrix x  the elements of matrix x is
xi  j  

  word   w j  

   

now every column in matrix x represents a feature for learning algorithms to learn later  we then apply threshold on every
element of x to filter out words like names  abbreviations  etc 
 
xi  j
xi  j  
 

xi  j  threshold
xi  j   threshold

   

by testing  we find   is a good choice for threshold  a thing
worth mentioning here is after the thresholding  many columns
in matrix x will become all zeros  which means the corresponding feature is considered indiscriminative  we remove
all these zeros columns for the sake of both code efficiency
and features meaningness 
before finally send x to learning algorithm  we further
process matrix x using the concept of inverse document frequency to account for the intuition that a word appearing in too
many incidents should be less discriminative that a word that
appears only in few documents  regardless of their actually
frequency in the individual documents 

   methodologies and algorithms
    data and preprocessing
data source currently used in the research is previous cs   
course project reports  we are using all the project reports
from year      to      and the number of articles is      
ideally online open source research papers are our best source
of data but lets restrict the data source to course reports for
testing purposes 
the first step is to convert formatted pdfs to plain txt
files  the articles are published on course website in pdf
format  as are most of the other research literatures  they
are converted to program readable plain txt files with no


word in di

xi  j   ln

 d 
  word   word j     
      d   w j  d   wordin di

 d 
the weight ln     d w
emphasized more the words appearj d  
ing in less documents    is added to avoid zero denominator 

    training algorithms
as stated before  the final training set is presented to the
training algorithm as the matrix x with rows representing
training samples and columns representing the text features 
in the training part  we applied both k means and latent
dirichlet allocation  lda  to perform unsupervised learning 

fitopic retrieval and articles recommendation     

unsupervised learning is the natural choice here because the
lack of label for each sample  i e  topic classification of each
course report 
at the end of training process  both k means and lda
will analyze all reports and calculate weight distributions over
topics for each of them  the weight distributions indicate
the likelihood of a document belonging to each topic  the
similarity between these distributions will be our criterion for
recommendation 
in the following discussion  topic and cluster essentially
mean the same thing  we set the topic cluster number k to be
   
      k means

in our case  the data sent to k means algorithm is x with
     rows and       columns  we first trained the data using
k means clustering method  we used cosine distance in our
algorithm 
denote the row vectors of x by   r    r         rn  
cosdis di   d j       

 ri  r j
   ri        r j   

   

where di   d j are ith and jth document 
cosine distance or cosine similarity measures the cosine
of angle between two vector  it can be easily applied to any
high dimension space  unlike euclidean distance  it measures
the difference in orientation rather than magnitude  cosine
distance is suitable for text vectors because scaling a text
vectors should not change the topic of this document 
after convergence  k means assigns each report to a cluster  the next step is to analyze the weight distribution over
all topics for each report  in our vector model for documents 
each vector in high dimension represents a document  the
intuitive idea is that if a vector is surrounded by many vectors
belonging to cluster i  then this vector should have a high
weight on cluster i  in practice  we artificially set a threshold
 and define hypercone   r  ri      to be the  vicinity of ri  
we admit there is some arbitrariness in choosing   we count
the number of vectors assigned to each topic in the vicinity of
 ri   the number count can be presented as n    n         nk   after
normalization  i e  divided by the total number n of vectors in
the vicinity  we have the weight distribution of document ri
over all topics  the distribution is nn    nn         nnk   denote the
distribution matrix for all vectors by tkmeans  
      latent dirichlet allocation  lda 

lda calculate the weight distribution using a different method 
we apply the matlab lda programs offered by http   
psiexp ss uci edu research programs data 
toolbox htm 
the lda algorithm has the exact same input as k means  i e 
the matrix x  m by n  mentioned above  each row of x is a
sample and each column is a word 
the algorithm outputs another matrix z  n by k   with
its rows represent words and columns represent topic  zi  j
is a number between     and it indicates how likely word

i  wi   belongs to topic j   j      k    with matrix x and z 
we can calculate the weight distribution over topics for all
documents  first we need to normalize x by row and get x 
then tlda   xz 
denote the rows in matrix tkmeans and tlda by wdi  
    recommendation algorithm
as mentioned in the motivation part  we want to extract reading interest and topic preference from the reports the reader
has already read and recommend according to similarity between weight distribution over topics 
in practice  given the reading list containing papers the
reader has read  we can calculate the word frequency vector r 
for k means algorithm  we put r in the high dimension space
and count the number of vectors in each topic in its  vicinity 
the procedure is the same as above  for lda algorithm  we
 then  rz
 is
normalize r by its total number of words and get  r 
the weight distribution over topics 
denote the weight distribution of this new reading list by
wd  by comparing the new distribution wd to row vectors wdi
in matrix tkmeans and tlda   we are able to rank the reports in
the training set by their similarity to wd and recommend those
with highest similarity 
to judge the similarity between wd and wdi   we use both
kl divergence and cosine distance 

   results and discussion
    k means clusters visualization
the actual algorithm used all      reports from          
and the cluster number was set to     the scale of this set is
too large to be clearly presented in the final report  so we run a
clustering on      reports  by doing so we can take advantage
of the classification available on the course website http   
cs    stanford edu projects     html  and compare them with clustering results  note the course website
only provide classification for      reports 
      clusters
table    sample number in clusters

cluster
finance   commerce
general machine learning
natural language
life sciences
computer vision
audio   music
physical sciences
athletics   sensing devices
theory   reinforcement
recommendation

website
number
  
  
  
  
  
  
  
  
  
n a

k means
number
  
cannot tell
  
  
  
  
  
  
cannot tell
  

fitopic retrieval and articles recommendation     

after all the preprocessing  matrix x for      reports
has     rows and       columns  we tried various cluster
numbers ranging from   to     experiments show that cluster
number between   to    give the best results  we judge the
clustering results by extracting the most frequency words in
each cluster that are not present in other clusters and manually
determining if they are closely related  it turned out that all
the      project reports are already classified into   groups
according to the course website  thus we clustered the reports
into   categories and compared the number of samples in
each cluster with results on the website  they are shown
in table    the most frequent words in each cluster are
shown in table    as one can see from both table   and
table    most frequent words in each cluster

finance  
commerce
natural
language
life
sciences
computer
vision
audio  
music
physical
sciences
atheletics
  sensing
devices
recommend
system

sale store price stock market trade
company strategy employee return
bill document token sentenc
answer tf idf question text
recip link code comment
gene patient tumor cancer
thyroid cell diseas diabet
surviv brain tissu
image color pixel cnn face
convolut layer descriptor object
recognit visual neural webcam
song music audio sound chord note
voic speaker genr melodi frequenc guitar
sensor activ packet jet traffic motion
damag simul quantum structur
memori seismic acceleromet
game player team season win
nba polici quarterabck hero
football borad defens statist injuri
user review busi rate movi restaurant
yelp recommend item star factor

table    k means algorithm clusters narrow topics well  which
usually comes with landmark words  like sale and stock
to finance and commerce  gene and cancer to life
sciences  song and music to audio   music  etc 
k means can not perform well for the general machine
learning and theory   reinforcement categories  this
is because all the documents are cs    course projects and
every report more or less mention the key words like svm 
learning  supervised  when we conduct clustering on
this document set  key words to machine learning area will
become completely indiscriminative  what we learn here is
that we have to provide a training set that contains different
examples in some aspects to cluster well 
in general  the training is satisfying and clustering is rather
obvious  all     samples are automatically labelled by k
means algorithm  labeling can be manually done in future
for higher precision 

 a  clusters in  d space
 b  clusters in  d space
figure    clusters
table    recommendation list

reading list
machine learning
supervised
applied to the
deeplearning
detection of
for multiclass
retinal blood vessels
image classification
top   recommendation list
k means
lda
implementing machine
learning algorithms
pedestrian detection
on gpus for real time
using structured svm
traffic sign classification
farmx  leaf based
disease identification
equation to latex
in farms
object classification
for autonomous vehicle
identifying gender
navigation of
from images of faces
stanford campus

      visualization

we also plotted the high dimension vectors in  d space after
dimension reduction using pca algorithm  the clusters in  d
space are shown in figure  a  where the coordinates are the
reduced dimension and each color marks a different cluster 
figure  b is a  d side view of figure  a  as one can see    of
the   clusters are separated well but the rest   are not as good 

    recommendation
we provide recommended papers based on the reading history
of users  two pieces of articles are randomly selected from
the dataset as users reading list  testing data   by using
methodology stated in methodology section  we generated
corresponding reading list papers compound distribution for
both k means model and lda model  figure   illustrates the
weight distribution over topics for the reading list  the largest
few sections relate to topics like image  classification  and
supervised machine learning  consistency between these two
method can be observed here 
the recommendation algorithm returned three papers for
both of the models  as shown in table   
we can expect  just from the title  that the two input papers
are related to image detection or classification using super 

fitopic retrieval and articles recommendation     

acknowledgments
the three of us would like to thank professor duchi for teaching this great class and offering the opportunity of working on
a project as a team  we also want to thank all the tas  who
have always been helpful in both the project and homeworks 
last but not least  we thank all our classmates who we have
sought help from 
 a  k means
 b  lda
figure    weight distribution of reading list

 a  k means

 b  lda

references
   

alan guo  chanh nguyen  and taesung park  building a
better tour experience with machine learning 

   

ka wing ho  movies genres classification by synopsis 

   

liangjie hong and brian d davison  empirical study
of topic modeling in twitter  in proceedings of the first
workshop on social media analytics  pages       acm 
     

   

andrew mccallum  andres corrada emmanuel  and
xuerui wang  topic and role discovery in social networks 
computer science department faculty publication series 
page         

   

jonathan chang and david m blei  hierarchical relational
models for document networks  the annals of applied
statistics  pages              

figure    weight distribution of reading list and recommend

reports

vised machine learning approaches  and the returned result
from both models share similar topics  indicated by keywords
in titles  like traffic sign classification  object classification  pedestrian detection  svm  identification  images of faces  etc  noticing that one of the recommended
papers is named equation to latex  this result demonstrated
the distinct advantage of our model over regular search engines   it discovers the topic of documents without replying
too much on keywords like image  machine learning or
detection  this paper actually describes how to detect equations in pdf documents and convert them to latex codes by
machine learning approaches  which is a good reference for
the user who intends to inquire topics like image processing
with machine learning methods 
figure   illustrates the comparison between the weight distribution over topics of reading list papers and recommended
paper  the red lines are the distribution of the reading list and
blue lines represent recommended reports  as shown in the
plots  documents recommended by k means method have a
very similar distribution with the reading lists distribution 
while distributions of documents recommended by lda deviate more  which may indicate more variance error  but this
does not necessarily mean k means perform better because
the way of counting paper numbers in the vicinity of reading
list has an averaging effect and might facilitate the similarity
between different documents distribution 
to conclude  k means and lda model respectively on
distribution over topics of documents and words  the recommendation algorithm following each of them recommends
different results but they all satisfy our expectations  our
algorithms successfully achieve our goal of topic extraction
and article recommendation 

fi