predicting medicare costs using non traditional metrics
john louie  and alex wells 
table i

i  introduction
in a      piece     in the new yorker  physician scientist
atul gawande documented the phenomenon of unwarranted
variation  differences in cost that cannot be explained by
socioeconomic factors or medical comorbidities alone  in
health care costs and delivery  in a particularly stark example 
gawande compares the very similar neighboring cities of
el paso and mcallen  texas  patients in mcallen have
medicare deductibles several times those of their neighbors
in el paso 
in this paper  we outline our approach to predicting
medicare costs on both hospital referral regions  hrr  and
hospital levels  more specifically  we aim to build models to
predict an individuals health care costs on the basis of nontraditional metrics by leveraging data from multiple opensource repositories  in doing so  we hope to both improve
the accuracy of cost predictions and gain insights into factors
responsible for fluctuations in health care costs 

r aw data f iles
source
dahc
dahc
dahc
dahc
dahc
dahc
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov
medicare gov

name
ta  demographics xls
dap hrr data      xls
dap hospital data      xls
dap hospital data      xls
dap hospital data      xls
dap hospital data      xls
readmissions complications and deaths   hospital csv
structural measures   hospital csv
timely and effective care   hospital csv
healthcare associated infections   hospital csv
outpatient imaging efficiency   hospital csv
readmission reduction csv
readmissions and deaths   hospital csv
complications   hospital csv
structural measures   hospital csv
timely and effective care   hospital csv
healthcare associated infections   hospital csv
outpatient imaging efficiency   hospital csv
readmission reduction csv

year
n a
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

ii  methods
a  choosing our data sets
for our project  we focused on using two different sources
for our medicare data sets  the dartmouth atlas of health
care  dahc  and medicare gov  the dahc provides information on chronically ill patient care  claims based medicare
spending  medicare population demographics  post discharge
events  and more  organized according to state  county 
hospital referral region  hrr  and individual hospital levels 
medicare gov has a dedicated website  data medicare gov 
that provides a diverse range of data sets from which we
chose the publicly available hospital comparison data sets 
which track individual hospital attributes and events such
as structural measures  complications  readmission rates 
payments  value of care  outpatient imaging efficiency  and
more  within both the dahc and the medicare gov data
sets  each of the metrics were associated with unique ids
 e g  a provider id for a specific hospital or a hrr id   the
data from both dahc and medicare gov contain detailed
information on approximately       hospitals  out of the
      hospitals in the united states       which amounts to
over     coverage  table   details each of the files that we
used for this project 
we elected to use the dap hospital data year xls files
from dahc for two reasons  first  this particular set of files
was one of the few that the dahc had on the hospital level
vs  on the hsa  hrr  county or state level  in contrast  the
  stanford
  stanford

university  computer science  jwlouie stanford edu 
university  biomedical informatics  awells  stanford edu 

medicare gov data sets were almost exclusively at the hospital level  second  these files represent information collected
from chronically ill patients during their end of life period
of care  for the medicare patient population in particular 
chronic illnesses account for   out of    patient deaths
and these patients last two years of life  i e  end of life 
alone account for     of all of medicares spending  the
chronically ill patient population with medicare coverage
significantly contributes to medicares overall costs and is
an important population to study and analyze 
b  preprocessing data sets
in order to utilize any of the aforementioned data sets  we
needed to perform an extensive amount of preprocessing 
first  we organized the different data sets by their respective
levels  for example  we grouped all the hospital level data
sets for dahc together because each file utilized the same
unique provider id  using a single raw data set  which
contains information for around       hospitals in a single
year  would provide poor learning opportunities  thus  when
creating our training sets  we combined data from multiple
years  we then selected features from our dahc data sets
 columns in the files correspond to our features  and from
our medicare gov data sets  measure ids correspond to
our features  and created training sets corresponding to the
dahc hrr data  one with demographic data to serve as a
baseline estimator and another with hrr data from       
dahc hospital data over multiple years and medicare gov

fihospital data over multiple years  each training example
within each training set is specified by a unique provider
or hrr id 
for our four training sets  we used the following labels
 y i  s          dahc baseline and hrr estimators  price 
age  sex   race adjusted total medicare reimbursements per
enrollee  parts a and b             dahc hospital level
estimator  total medicare spending for medicare spending
per decedent by site of care during the last two years of life
 deaths occurring over all four years   and    medicare gov
hospital level estimator  spending per hospital patient with
medicare  medicare spending per beneficiary  
once we created our training sets  the code to do so
is found on our github page   we found that many of the
training examples  especially those from medicare gov  were
missing information  in order to address this issue  we used
the following different strategies 
   missing feature thresholding  in our first preliminary
approach  we omitted any training examples that failed to
have a certain percentage of features  say     or      filled 
after this initial thresholding  for the remaining training
examples that had missing features  we replaced them with
the mean over that features column  this filling method was
used for both our dahc and medicare gov training sets 
   item item collaborative filtering  our subsequent
data filling approach was to use an item item collaborative
filtering recommender system  which we wrote with the guidance of cs    s recommender system lecture notes      for
this algorithm  we used the pearson correlation coefficient
and when filling in an entry  we considered at most    
nearest neighbors  if after the recommender system step we
still had missing data  i e  for a given entry the nearest
neighbors were neighbors with that entry missing   we again
filled the entry with the mean over the feature column  this
filling method was only used with our medicare gov training
set  in future we could apply it to the dahc hospital training
set which had little data missing  
in addition to the above strategies for addressing missing
data  we utilized variance thresholding in order to remove
features that yielded little to no variance to our data set 
after performing these preprocessing steps  our training sets
were ready to be used with various learning models 
c  learning algorithms
all of the learning algorithms we used were implementations found in pythons scikit learn package     
   supervised learning algorithms  for our datasets 
we used multiple different supervised algorithms for both
classification and regression 
a  classification  for classification  we used both logistic regression and linear discriminant analysis  lda  
logistic regression was used on our medicare gov training
set to predict whether or not the expected medicare cost for
an individual hospital was above or below the national average  we also used multi class lda to predict the expected
medicare cost quantile for an individual hospital 

b  regression  we decided to use the following three
algorithms to predict medicare costs      linear regression 
    kernelized support vector machine  and     gradient
boosting  each of these methods was used to predict expected medicare cost on both the hrr and hospital levels 
   unsupervised learning algorithms  in addition to
using supervised learning techniques like those mentioned
above  we decided to also use the following unsupervised
algorithms      k means clustering      principal component
analysis  pca  and     various manifold learning techniques 
a  k means clustering  we ran k means on our initial
dahc and medicare gov training sets in an attempt to try
and determine whether there were any overall patterns or
trends in our data 
b  principal component analysis  pca  and manifold
learning  our primary reason behind leveraging both pca
and manifold learning was to help us visualize our highdimensional data  both pca and manifold learning allowed
us to project our data onto two dimensional plots  however 
pca makes the assumption that there is inherent linearity
in the data while manifold learning attempts to reveal
non linear structures in the data  the following manifold
learning algorithms were used in our visualization     
locally linear embedding  lle       local tangent space
alignment  ltsa       hessian locally linear embedding 
    modified locally linear embedding      isomap     
multi dimensional scaling  mds       spectral embedding
and     t distributed stochastic neighbor embedding  tsne  
d  validation methods
for each of our supervised learning techniques we used
k fold cross validation  where k      in addition  we generated learning curves  which show the cross validation
and training scores for an estimator  to help us visualize
our models performance with varying sized training sets 
learning curves allow us to determine how much we benefit
from adding more training data and whether the estimator
suffers more from a variance error or a bias error  for our kmeans clustering  we evaluated the quality our clustering
by utilized the silhouette coefficient  which is a method for
evaluating clusters when the true cluster designations are
unknown 
iii  results
a  medicare gov hospital compare datasets
the training set constructed using medicare govs hospital
compare data sets generally performed poorly with all of the
supervised learning algorithms that were attempted  table  
contains the performance of the medicare gov training set
with linear regression  svm and gradient boosting with kfolds cross validation  k      
because the labels for this particular data set represented
how much a hospitals spending deviated from the national
average  the average being represented with a    with under spending being less than   and over spending being

figreater than     we leveraged logistic regression to determine
whether we could accurately classify an example as over or
under spending  with k folds cross validation  k       our
mean classification accuracy was            
unfortunately  because of the poor performance of our
regression and classification models on the medicare gov
training set  we elected to omit further results 
table ii
m edicare   gov e stimator r esults
model

linear regression
svm
gradient boosting

residual
sum
of
squares
          
          
          

r  score

          
          
          

using the same three metrics as mentioned above for our
baseline model  the results are shown in table   and figure
  below   note that in our plot of important features  we
limited the number of features displayed to only include the
top     
table iv
hrr r egression r esults
model

explained
variance
score
          
          
          

linear regression
svm
gradient boosting

residual
sum
of
squares
       
       
       

r  score

         
         
         

explained
variance
score
         
        
         

gradient boosting for hrr level data
b  dartmouth atlas of health care  hrr datasets
a  baseline models  projected medicare cost is commonly based on attributes such as age  gender  and ethnicity 
using demographic features  we created three supervised
baseline estimators of medicare cost on the hrr level  to
determine how well the baseline models performed on the
test set  we calculated the residual sum of squares  r  score 
and explained variance score for each model  we also plotted
the training and test set deviance as a function of boosting
iteration  as well as the most important features used for
regression  the results of this analysis are shown in table  
and figure   
table iii
hrr baseline e stimator r esults
model

linear regression
svm
gradient boosting

residual
sum
of
squares
          
         
         

r  score

            
           
         

explained
variance
score
           
           
         

gradient boosting for hrr level data baseline estimator

fig     training test set deviance and most important features for hrr
regression

c  dartmouth atlas of health care  hospital level datasets
using hospital level datasets from the dartmouth atlas
of health care  we created some additional models to both
predict the quantile rank and the raw expected medicare cost
of an individual hospital 
a  quantile rank  to predict how expensive an individual hospitals cost is relative to other hospitals in the
united states  we used multi class lda with k     fold cross
validation  each hospital in the training set was grouped by a
quantile  either quartile or decile  based its average medicare
cost  we then predicted which quantile group a hospital from
the test set belongs and kept track of the overall accuracy
and error of the model  the results are shown in table   
table v
l inear d iscriminant a nalysis r esults
quantile grouping
quartiles
deciles

fig     training test set deviance and most important features for hrr
baseline estimator regression

b  models using non traditional features  in order to
predict the expected medicare cost on the hrr level  we
utilized the same three supervised models and evaluated them

accuracy
    
    

error
    
    

the lda model was able to predict the quartile or decile
rank of an individual hospital with reasonable accuracy
and error  the model correctly predicted the quartile and
decile rank of a hospital with       and       accuracy
respectively  we also saw that when the model incorrectly
predicted the quartile or decile  it was usually only off by  
or   quantile groupings 

fipca with k means  k     

b  expected medicare cost models  next  we attempted to predict the expected medicare cost of an individual hospital based on non traditional features from the
dahc  again  we leveraged the same three models as before
with the same performance metrics with k     fold cross
validation  the results are shown in the table   and figure
   the boosting plot is again only for the top    features  
table vi
h ospital r egression r esults
model

linear regression
svm
gradient boosting

residual
sum
of
squares
          
          
          

r  score

        
        
        

fig    
explained
variance
score
         
         
         

hospital level data projected onto two dimensions using pca

manifold learning with        points and     neighbors

gradient boosting for hospital level data

fig     hospital level data projected onto two dimensions using various
manifold learning techniques

fig    
training test set deviance and most important features for
hospital level regression

c  k means  pca and manifold learning  for our
dahc hospital level data set  we performed k means with
cluster sizes of   to    as mentioned before  the silhouette
coefficient  which is on a scale of    to    was used to evaluate the quality of our clusters  the silhouette coefficients
for k     to k     are shown in table   

d  hospital level learning curves  because our training set for our dahc hospital level data was significantly
larger than any of our other training sets          vs  
        we also elected to produce learning curves for our
three regression models in figures      and   for linear
regression  svm and gradient boosting  respectively 
linear regression learning curve

table vii
dahc h ospital l evel k m eans c lusters
number of clusters
k  
k  
k  
k  
k  
k  

silhouette coefficient
        
        
        
        
        
        

fig     linear regression learning curve produced from hospital level data
with training sets of varying size

iv  analysis
in order to visualize our clusters  we decided to combine
our k means clustering labels with both pcas and multiple
manifold learning algorithms visualizations  by projecting
our data onto two dimensions  r    r     we were able to
roughly see how the data was clustered  the result of pca
and k means  k      clustering is shown in figure    while
the manifold learning techniques and k means  k      are
shown in figure   

a  hrr level data  our first baseline hrr estimator
using traditional metrics such as demographics and ethnicity
performed poorly and proved very ineffective at predicting
medicare costs when compared to the estimator based on
non traditional features  from our analysis  some of the most
important features for our non traditional estimator were
number of ambulatory cases  readmission rate  physicians
per         residents  and critical care physicians per

fisvm learning curve

fig     svm learning curve produced from hospital level data with training
sets of varying size

gradient boosting learning curve

fig     gradient boosting learning curve produced from hospital level data
with training sets of varying sizes

        residents  these results suggests that quality of
initial care and health care system complexity  e g  number of
physicians per resident  may play a larger role in determining
a regions medicare costs than individual demographics  as
a result  enforcing more thorough initial health screens to
reduce readmission rates  and possibly ambulatory cases 
may lead to a decrease in medicare costs 
b  hospital level data  we found that models trained
on our data set consisting of non traditional metrics at the
hospital level predicted expected medicare cost very well 
on the individual hospital level  some of the most predictive
features include  percent of deaths occurring in hospital 
medical and surgical unit days per patient  medical specialist
visits per patient  and number of beds  a higher percent
of deaths occurring in hospitals may be indicative of the
hospitals reception of more extreme cases of chronic illness 
 e g  treatment of eczema vs  renal dialysis   which may in
turn  require more medical and surgical unit days per patient
and medical specialist visits per patient  additionally  hospitals receive more extreme medical cases usually through a
referral process because they are larger  either in staff  which
usually translates to a higher patient capacity  and more
equipped to handle medical complications that may arise
from severe medical conditions  these patients eventually
end up with high medical and surgical costs before the end
of their lives  as with the hrr level finds  we see that a
systems complexity is correlated with medicare costs 
c  learning curves  the learning curve graph for our
linear regression model shows that the training and validation
scores are plateauing to a value of approximately      this

particular graph pattern shows that our model is currently
suffering from higher bias than desirable  meaning we are
underfitting the data  in order to address this issue  we could
introduce greater complexity to our estimator  however  this
visual phenomenon may be indicating that the data may not
have a strictly linear relationship  it is also worth noting that
increasing our training set size for linear regression would
provide little improvement on the training and validation
scores  with this in mind  understanding the visuals provided
by the manifold learning plots may be informative in
understanding the underlying non linear structure to our data
that may explain the observed  plateauing behavior 
the learning curves for our gradient boosting and svm
models show that we are achieving validation scores of
around      because the training scores are above our validation scores for both of these models  we can see that their
generalization and performance can be further improved with
additional training examples 
v  conclusions   future directions
our results indicate that medicare costs can be estimated
with reasonable accuracy using non traditional metrics associated with individual hospitals or hrrs  additionally 
models trained on non traditional features were significantly
better at predicting medicare costs than models trained on
demographic information alone  as a result  our analysis
suggests that medicare costs are more strongly influenced by
location of care  hrr region or hospital  than they are by
individual demographics  therefore  non traditional metrics 
such as those used in our project  should be included in order
to assure accurate and realistic medicare cost predictions for
patients 
looking forward  we hope to next construct a predictive
model for individual patients  the ability to predict the
expected cost of admission to a specific hospital based on an
individuals symptoms could be beneficial and more reflective
of cost than predictions at the hospital level  we also hope
to improve our current learning models by utilizing methods
like grid search to find optimal parameters for our models
and incorporating more data into our training sets 
code
all the code and datasets used are located at 
https   github com jlouie      cs    project 
acknowledgment
we would like to thank the entire cs     teaching staff
for this opportunity and especially professor john duchi for
his investment in providing an incredibly challenging and
rewarding offering this quarter 
r eferences
    gawande  atul  the cost conundrum  the new yorker     may
     
    fast facts on us hospitals  american hospital association  jan
      web 
    leskovec  jurij  anand rajaraman  and jeffrey d  ullman  recommendation systems  mining massive datasets  stanford university 
    scikit learn  machine learning in python  pedregosa et al   jmlr    
     

fi