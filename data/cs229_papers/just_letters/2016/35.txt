discovery of transcription factor binding sites with deep convolutional
neural networks
reesab pathak
dept  of computer science
stanford university
rpathak stanford edu

abstract
transcription factors are key gene regulators  responsible for modulating the conversion of genetic information from dna to rna 
though these factors can be discovered experimentally  computational biologists have become increasingly interested in learning transcription factor binding sites from sequence data computationally  though traditional machine learning architectures  including support
vector machines and regression trees have shown moderately successful results in both simulated and experimental data sets  these
models suffer from relatively low classification accuracy  typically measured by area under the receiver operating characteristic curve
 auroc   here we show that learning transcription factor binding sites from sequence data is feasible and can be done with high accuracy  we provide a sample cnn architecture that yields greater than     auroc  additionally  we discuss key questions that need to
be answered to improve hyperparamter search in this domain 

 

introduction

since the human genome project  which concluded in the early     s  computational biologists have become interested in learning
features of the genome from sequencing data  revolutionized by the rapid advancement of second and third generation sequencing
technologies  learning biologically relevant features from sequencing data has become possible  in the past   years  scientists have used
traditional machine learning techniques and probabilistic graphical models to impute haplotypes  learn epigenetic markers  and much
more  though we cannot provide a full review here  this is done well in     
   

problem and related work

this paper specifically considers the transcription factor binding site discovery problem  though we cannot provide a review of transcription factors here  we refer the reader to slattery et al  where the transcription factor binding site literature is well reviewed     
formally  our problem is a multi task classification problem  we are given  as input  a training set with pairs  x  i    y  i   ni     in our
problem  the input data  x  i  is a matrix  of dimension    n   where n is the length of a dna sequence  this matrix  which is referred
to as the positional frequency matrix  pwm  has four rows corresponding to each channel of genetic alphabet  namely  a  c  t  g   our
labels  y  i  are either scalar or vector  depending on the number of transcription factor binding sites that are being learned  nonetheless 
the dimension is equal to the number of classification tasks  and each element of y  i  is a binary label in the standard space          the
goal is then to accurately predict the labels from the training data  which is to accurately predict whether or not each of the transcription
factors binds in a given sequence 
recently  many groups have studied similar genome prediction problems  especially in the context of epigenetic features  leung et al
review the progress here in      for this specific problem  support vector machines with a k mer kernel has shown greater than    percent
auroc      additionally  recent groups have discussed the applicability of neural network architectures to this problem         
   

our contributions

here  we provide a sample convolutional neural network  cnn  convnet  architecture which provides greater than    percent accuracy
on a simulated data set  additionally  we use a recently published interpretation package to show that the features learned by the
cnn are roughly the transcription factor binding sites themselves  suggesting that cnns are good models for this problem  despite
the computational expense  finally  we provide a discussion of areas of key difficulty that need exploration to improve the accuracy of
cnns for larger scale problems and for experimental datasets 

 

fi 

methodology

our work uses a simulated training set of sequencing data and labels  we note  however  that this is a standard practice within this
domain  and many papers that evaluate model accuracy have taken similar approaches      this simulated training set is then provided
as input into our convolutional neural network  which requires hyperparamater tuning to get high accuracy 
   

simulated data

we sampled sequence of length      base pairs       symbols  from the genetic alphabet  a  c  t  g  with a fixed gc fraction  of
     which is a biologically motivated result  this means that the probability assigned to each letter was                     respectively 
with these probabilities we sampled a fixed number of reference sequences  depending on the problem  we then uniformly at random 
sampled an index from which to embed a transcription factor motif  these motifs  which come from the encylopedia for dna elements
 encode  project  are non deterministic  again  we are unable to review the results of the encode project here  but this is done well
in     
the motifs are accompanied by a position weight matrix  pwm   which is a n    representation of the odds of each symbol of the
genetic alphabet at each position over the background frequency of a given letter  we binomially sampled motifs from the position
weight matrix  and replaced our sequence data with the sequence of the motifs  for our studies  some experiments tested the number of
training examples  in which case  we typically kept balanced classes of negative and training examples  negative samples had randomly
embedded sequences  which provides a more biologically relevant negative class of data  then  from these reference sequences  we
sampled sequence reads  which are     base pair substrings  since sequencing data contains irreducible noise  we added noise by flipping
some symbols  with probability       at each index  based on the approximate error rate of a standard high throughput sequencing
technology  our labels were constructed by creating vectors with dimension equal to the number of transcription factors that were
embedded  we placed a   in the ith index of our vector if transcription factor i was embedded in the sequence  otherwise we placed
   array data is achieved through the one hot encoding of our reads  our sampling methodology  though not identical to any previous
study  was informed by previous machine learning studies in genomics     
   

convolutional neural network

convolutional neural networks have three features      convolutional layers      pooling or subsampling layers  and     fully connected
layers  the convolutional layer conducts an affine transformation over its input  multiple activation maps are created by multiple
convolutional lters  these pass to an activation function  which is a non linearity such as a rectified linear unit  relu   the output is
then passed to the pooling layer  which subsamples the convolutional output and takes the maximum entries within the domain of a
subsampling unit  called max pooling   this process  convolution to max pool  is repeated  unlike previous layers  the final layer is
fully connected and a matrix multiply is done to get output predictions  our architecture is cl mp cl mp cl mp fc  where cl is a
convolution layer  mp is a max pool layer  and fc is a fully connected layer  between a convolution and max pool layer  data always
passes through the relu  additionally  convolutional neural networks like other feedforward archictures use backpropogation to update
weights during each epoch during training  we cannot fully review cnns here  so again we refer the reader to     
we implemented our convolutional neural network using keras  a deep learning library that wraps around the deep learning software in
theano  we discuss hyperparamaters used in results and discussion  the input data was the training set as described previously  the
testing and validation data were similarly simulated sets of data 

 

results and discussion

we conducted two classes of experiments  single motif classification and multi motif classification  the first task is formulated as a
single task  binary classification problem  with scalar labels  the second task is formulated as a multi task  binary classification problem 
with vector labels 
   

single motif embedding task

in the single motif embedding task  we first embedded motif
tal   which encodes the transcription factor t cell acute lymphocytic leukemia protein    this transcription factor has the sequence  see right   based on the second known motif from the
encode project 
this figure is related to the position weight matrix  pwm  of each
motif  which is of dimension    l  l is the length of the motif  
the ith column of the matrix represents the probability of each
of the four states  a  c  t  g  at base pair i in the motif  the
sequence logo above is generated by calculating for each base 

figure    t al   encode  transcription factor motif  depicted as
sequence logo

 

fithe information content  which is log        hi   en    where hi
is the  shannon  entropy at each base pair  and en is the small sample collection factor  which is reviewed in     
     

data needs

we investigated how many training examples in the form of
position specific scoring matrices  pssms  were necessary to get
near optimal results  we measured an upper bound on testing accuracy by providing to a random forest the exact locations in the
pssm where the embedded motifs were  thus  this represents an
upper bound on performance  since we do not limit cnns from
learning these decision boundaries 
from figure    around      training examples is optimal to reach
near testing accuracy achieved by a random forest model  auroc           n         
     

hyperparameter tuning

figure    single motif embedding  data needs

the hyperparameters for the single motif task needed to be tuned
to get optimal results  basedo on figure    with a convolution

 a  convolution filter width

 b  pooling filter width

 c  number of filters per layer

figure    example hyperparameter tuning plots for single motif embedding
filter width of     pooling filter width of     and    convolution
filters per convolutional layer  we achieved auroc of        with n        training examples 
     

dropout

to prevent overfitting  we investigated whether dropout was a feasible strategy to improve training accuracy  to assess this  we
used a motif with greater heterogeneity than tal   the motif
we chose was ctcf  discovered motif    which displayed more
entropy in its position weight matrix  based on the figure    as
dropout increases  the test auroc substantially increases  suggesting that adding dropout to the model may improve its robustness 
   

multi motif embedding task

for the multi motif embedding task  we embedded three motifs
known to co bind      ctcf known motif   is responsible for
transcripition regulation  v d j recombination  and chromatin architecture regulation  znf     known motif    is a transcription
activator  and six   known motif    recruits many dna binding
proteins  the sequence logos are shown in figure   

figure    dropout probability versus test auroc

 

fi a  ctcf  known motif  

 b  six   known motif  

 c  znf     known motif  

figure    multi motif embedding  encode motif sequence logos
     

data needs

we again investigated the number of training examples to reach
the accuracy of the random forest model  which is given the optimal decision boundaries by location of motifs in the sampled sequence  based on figure    we needed around       training examples

 a  data size  overall task

 b  data size  motif  

 c  data size  motif  

figure    data size needs by task  multiple motif embedding
to get near the random forest accuracy  additionally  for single motifs  within the task  the same size of data also nearly achieved the
random forest models accuracy 
     

hyperparameter tuning

the hyperparameters for the multiple motif task needed to be tuned to get optimal results  based on figure    using a convolution filter

 a  convolution filter width

 b  pooling filter width

 c  number of filters per layer

figure    hyperparameter tuning plots  combined for all   tasks
width of     a pooling filter width of     and    convolutional filters per layer  we achieved auroc         with n         training
examples 
     

feature importance and model interpretation

recently  a group at stanford published a method called deeplift  deep linear importance feature tracker   which is able to identify
feature importance in the input to a cnn  cite    we ran the software for our multi task classification to see what features the cnn

 

fi a  ctcf 

 b  six 

 c  znf

figure    deeplift interpretation sequence logos  multi task classification problem
found most important to predicting the labels  these figures are encouraging because  if compared against figure    it is clear that the
features learned by the model correspond with parts of the encode motifs  thus  not only is the model learning the correct labels  but
it does so by identifying structure of motifs within the sequence data 

 

further questions and outlook

despite good accuracy on the learning tasks described above  a number of questions need to be answered before this model can be
extended to experimental data  this work shows that it is indeed possible to learn transcription factor binding sites based on sequencing
data  after sufficient data and hyperparameter tuning  nonetheless  it is still unclear how the optimal width of the convolution filter scales
with the entropy and structure of the position weight matrix  additionally  we were not able to investigate how density localization
impacts hyperparameters and how penalty coefficients for regularization and dropout probabilities may need alteration as more motifs
are embedded  though our methodology is similar to that adopted by previous computational biology papers  we acknowledge that
testing this model on experimental data is important 

 

conclusion

here  we have demonstrated the feasibility of learning transcription factor binding sites from sequencing data  we note that there are
an number of key questions  see further questions and outlook  that should be explore prior to using these models on experimental
data  nonetheless  deep learning through cnns provides high accuracy for learning genomic transcription factor binding loci 
acknowledgments
we thank stanford research computing for allowing access to the sherlock computing cluster for access to gpu compute nodes on
which to run theano and keras code  additionally  we thank anshul kundaje  johnny israeli  and avanti shrikumar  stanford  computer
science  for providing access to their recently published deeplift code  which provided interpretation figures for our cnns 

references
   
   
   
   
   
   
   
   
   
    

babak alipanahi et al  predicting the sequence specificities of dna  and rna binding proteins by deep learning  in  nat
biotechnol              pp         
roadmap epigenomics consortium et al  integrative analysis of     reference human epigenomes  in  nature                 
pp         
yann lecun  yoshua bengio  and geoffrey hinton  deep learning  in  nature             pp         
dongwon lee et al  a method to predict the impact of regulatory variants from dna sequence  in  nat  genet              
pp        
michael k  k  leung et al  machine learning in genomic medicine  a review of computational problems and data sets  in 
proc  ieee               pp         
daniel quang and xiaohui xie  danq  a hybrid convolutional and recurrent deep neural network for quantifying the function of
dna sequences  in  biorxiv         p         
thomas d schneider and r michael stephens  sequence logos  a new way to display consensus sequences  in               
pp           
matthew slattery et al  absence of a simple code  how transcription factors read the genome  in  trends biochem  sci      
        pp          arxiv  nihms       
jie wang et al  sequence features and chromatin structure around the genomic regions bound by     human transcription factors 
in  genome res               pp           
jian zhou and olga g troyanskaya  predicting effects of noncoding variants with deep learning based sequence model  in  nat 
methods               pp       

 

fi