initial steps toward automating legal document
editing
lauren blake  lblake stanford edu 
abstract

introduction
drafting successful legal documents requires experienced lawyers  careful research  and nuanced reasoning  nonetheless 
drafting also requires many mundane and time intensive tasks that delay the process and increase its cost  one consequence of
this is that law firms hire a large number of support employees to assist with these low level tasks  according to us bureau
of labor statistics data released in may       approximately     of workers in us legal services are in support roles  e g 
paralegals and legal assistants  and these workers are paid    bn annually 
starting by reducing time spent on editing  one of the most common low level tasks  machine learning has the potential
to significantly improve writing and reviewing legal documents  to reach this potential  many challenges need to be further
understood and subsequently addressed  including how machine learning can comprehend the key points in a legal document
and reflect lawyers negotiations on those key points in its recommended edits 

objectives
this paper initially evaluates how supervised learning algorithms could automate editing legal documents  the papers scope is
narrowed to analyzing a particular set of legal documents and edits 
the legal documents are non disclosure agreements  ndas   which are also called confidentiality agreements  ndas
are a type of private contract that determine how the participants can use sensitive information and are routinely exchanged
between companies before considering business development or m a transactions  for example  a company for sale will
require potential buyers to sign an nda before providing them with financial statements and other non public information 
edits are limited to the initial set of comments on a document  which consist of one participants requests for specific text
to be inserted into or deleted from an nda after reviewing the other participants original document  these initial requested
changes reflect the negotiation between the two parties on key terms  e g  duration of the agreement  non solicits of employees 
restrictions on buying stock  limitations on sharing information with others  and contain significantly more meaning than fixing
typos or making cosmetic changes to the wording 
the long term ideal model for automating editing would modify the original document provided by one party to reflect
another partys preferred legal terms and language while maintaining the documents integrity  e g  clear writing  proper
grammar  and consistent defined terms   this would significantly reduce or even eliminate the editing done by lawyers today 
to determine what may be feasible  the models in this paper take on the relatively simpler classification task of predicting
whether words in the edited nda text come from the original nda text provided by one party or were requested as insertions or
deletions to the text by the other party  the models in this paper reverse engineer the edits that would be made by the long term
ideal model by identifying changes that were previously made to the edited document  without the benefit of seeing the original
document   conversely  the long term ideal model would suggest changes that could be made to the original document in the
future 
by using similar legal document text data and models  the classification task discussed in this paper gauges the complexities
of building the long term ideal model  highlighting the difficulties with unbalanced and sparse data  and indicates next steps
forward 

related work
the intersection between machine learning  natural language processing  nlp   and law has been investigated by the
academic community and technology firms  their focus has been primarily placed on information retrieval used in e discovery
software and predictive models for case outcomes  although relatively less common  content analysis techniques are being
developed  which are used to interpret public documents  e g  court cases  laws  sec filings  in academia and support the

fitechnology services offered by a few legal software start ups that summarize and track private contracts  beyond these topics  a
literature review did not identify any relevant academic or industry research on specifically editing legal documents 
looking beyond the legal context  colgrove  tibshirani  and wong initiated research in machine learning for automating
editing by using naive bayes models to predict what section of a wikipedia article a user should edit based the users revision
history  these models were not particularly successful  providing more accurate predictions on the article instead of the article
section edited by a user   
despite limited directly relevant published work  there is an extensive body of research on nlp techniques to draw
from  given their shared goal of tagging tokens within text sequences  parts of speech  pos  tagging and named entity
recognition  ner  have many commonalities with identifying edited text and have informed numerous decisions in this
papers analysis  including the text processing and the models used 

data set
the data set contains     nda documents from a single investment firm  which include approximately       sentences       
unique tokens  and         tokens total  each nda was initially written by a company running a sale process and then edited
by the investment firm 
text processing was completed to create sentence and word tokens and remove word stems  as detailed below   most
importantly  the correct original  inserted  or deleted label for each tokens was identified based on comparing the original and
edited versions of each nda  across the entire data set      of tokens were original     were inserted  and    were deleted 
text processing steps
   identify the inserted or deleted portion of the text by running
an advanced version of the diff terminal command on the
original and edited version of each document  html tags
mark the start and end of these edited portions of the text 
   tokenize the text into sentences with the punkt tokenizer
provided by the natural language tool kit  nltk  python
package  the punkt tokenizer recognizes periods ending
sentences versus those within sentences using unsupervised
learning 
   tokenize the sentences into words using a customized regular
expressions tokenizer in the nltk package  which accommodates the html tags 
   normalize the tokens by removing suffixes with the porter
stemmer from the nltk package 

text processing example
original text     will have any legal effect  within ten days  after
being so requested   
text with html tags representing edits     will have any legal
effect   del  within ten days   del  ins  promptly   ins   
after being so requested   
sentence and word tokens after stemming       will    have  
 ani    legal    effect     within    ten   days    promptli  
 after    being    so    requested     
labeled tokens       will  o    have  o    ani  o    legal  o    effect  o    within  d    ten  d   days  d  
 promptli  i    after  o    being  o    so  o    requested  o      

   label each token as original  inserted  or deleted based
on its position relative to the html tags 

figure    text processing overview  o  i  and d represent original  inserted  and deleted labels  
adding ner to the text processing was seriously considered and tested because ner should be able to increase the
similarity between documents by replacing document specific names with generic ones  e g  replacing google with company  
however  ner was ultimately not included because it did not meaningfully increase edited token classification accuracy on the
training data  in part  this was likely caused by existing ner models mislabeling capitalized legal phrases  e g  evaluation
materials or contemplated transaction  as organizations or people and could be remedied by a ner model trained on a legal
document corpus 
in order to maintain at least some context from prior tokens and labels  each sentence was treated as one observation 
    of the sentences were randomly chosen as training examples and the remaining sentences were testing examples  within
a sentence  each token can potentially have a different label  consequently  classification results were calculated based on
identifying individual token labels correctly 
it is very difficult to gather ndas because these documents are private contracts and may only be obtained from one of the
signing companies  as a result  this papers data set has a small sample size  this leads to sparsity and a limited number of
inserted and deleted token examples  furthermore  due to the imbalance between original and edited tokens in the data  model
performance on the original tokens is more heavily weighted than performance on edited tokens in training unless additional
adjustments to the data set are made 
   

fimethodology
hidden markov models  structured averaged perceptrons  and conditional random fields were evaluated based on their
ability to successfully classify inserted and deleted tokens within each edited nda sentence  these models were chosen
because they are often relied on in nlp for similar tasks  e g  pos tagging and ner  that involve tagging text sequences 
hidden markov model
the hidden markov model  hmm  is a generative model that estimates the probability of an unobserved sequence of
inserted  deleted  or original label states when there is an observed sequence of word outputs  from the markov property  the
model assumes the current inserted  deleted  or original label state and word output only depend on the prior state  therefore 
no additional features beyond the likelihood of the current word output conditional on the prior state and the likelihood of
transitioning to the current state from the prior state are taken into account when the hmm makes predictions 
the supervised hmm from the nltk package determined the maximum likelihood output and transition probabilities
based on the frequencies observed for the word tokens and states in the training examples  the model subsequently tagged
tokens in the testing examples based on the maximum likelihood state sequences generated with these probabilities by the
viterbi algorithm   
structured averaged perceptron
the structured averaged perceptron  sp  uses online learning to continuously improve its original  inserted  or deleted label
predictions  this discriminative model updates the weights placed on different features after an incorrect prediction and leaves
the weights unchanged after a correct prediction  as commonly used in pos tagging  the features include a combination of
suffixes  prefixes  entire words  and labels drawn from the two prior tokens  current token  and the two following tokens as well
as a constant bias term 
the sp from the nltk package determined the weights with    iterations through all of the training examples  after
training was completed  average weights were calculated and tokens from the testing examples were tagged with the highest
weighted label  in which prior token features were based on the sps prior predictions   
conditional random fields
the conditional random field  crf  model can be considered a more expressive version of the multinomial logistic
regression model that makes predictions for sequences of original  inserted  or deleted label states  like other discriminative
models  the crf maximizes the probability of each label conditional on a rich feature set derived from the text  after testing
the abundant features available  a restricted feature set was chosen which included the same set of features from the sp model
along with extended sequences of words and labels 
the linear chain crf from stanford nlp group  primarily used for their ner classifier  finds the maximum likelihood conditional probabilities using stochastic gradient descent  the model tags tokens based on monte carlo inference approximations
of the highest probability state sequence  using gibbs sampling on possible state sequences   
adjustments for unbalanced data
most of this paper considers models trained on all training example sentence  in order to counteract the bias towards original
labels from the unbalanced data set  two additional training processes were considered  the first process trained on edited
sentences  discarding sentences with only original tokens from the training data 
the second process breaks the training sentences into bigrams and randomly undersamples the bigrams where the second
token has an original label  resulting in an even distribution between original  inserted  and deleted labels for the bigrams
second tokens  the first token serves as context for the second token and consequently its label is not factored into the
undersampling  resulting in more original labels appearing across both tokens in the bigrams  while     of the tokens were
original in the all sentences training data      and     were original in the edited sentences training data and the randomly
undersample bigrams training data respectively 

results
when trained on all sentences  all three models had over     accuracy on the testing examples driven by identifying over    
of the original tokens correctly  highlighting the clustering of accurately classified tokens seen in figure    accuracy for the
tokens within a particular sentence ranged widely from        for each of the models  in addition            and     of
sentences were classified      correctly by the hmm  sp  and crf models respectively 
although these high level results are strong  a better indicator for these models performance is their classification accuracy
on edited tokens only because these labels represent the editing processing and appear infrequently in the data  the crf is
revealed to be a significantly better classifier based on edited token accuracy  which reflects its more flexible feature set that
   

fioriginal
inserted
deleted
total

support
      
     
   
      

hidden markov model
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    
    
    
    
    
    
    

structured perceptron
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    
    
    
    
    
    
    

conditional random field
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    
    
    
    
    
    
    

table    summary of classification results for training on all sentences  edited token accuracy is defined as  number of true

actual

positives for inserted tokens   number of true positives for deleted tokens     total number of inserted tokens   total number of deleted
tokens   boundary errors are not taken into account when calculating precision and recall  

original
inserted
deleted

hidden markov model
predicted
original inserted deleted
      
   
   
   
   
  
   
  
   

structured perceptron
predicted
original inserted deleted
      
  
  
   
   
  
   
  
   

conditional random field
predicted
original inserted deleted
      
  
   
   
     
  
   
  
   

table    confusion matrices for training on all sentences
takes more sentence context into account when tagging  the crf correctly identified     of edited tokens while the hmm
and sp correctly identified     and     respectively 
among edited tokens  inserted tokens were classified more precisely and with higher recall than deleted tokens  across all
three models  precision ranged from        for inserted tokens and from        for deleted tokens  recall was above    
for inserted tokens  reaching as high as     for the crf  while recall was below     for deleted tokens  dropping as low as
    for the hmm   as seen in the confusion matrices in table    the weaker performance on deleted tokens is largely due to
the models being more likely to misclassify deleted tokens for original tokens than inserted tokens for original tokens 
classification improved by making adjustments for the unbalanced data set in training  particularly for the hmm and sp
models  as expected  hmm and sp benefited from a higher frequency of edited tokens  which placed more weight on correctly
classifying these tokens in training  this meaningfully increased recall at the cost of reduced precision and low overall accuracy
given a large number of original tokens were tagged as edited  the best example of improvement is the sp where edited
token accuracy increased by     to     for training on edited sentences only and by     to     for training on randomly
undersampled bigrams  compared to training on all sentences  
most notably for the other models  when trained with randomly undersampled bigrams  hmms edited token accuracy
improved by     to     and crfs dropped     to      the drop in classification accuracy can likely be explained by the
crfs reduced ability to decipher context when considering only two tokens at a time 
because this is a preliminary look at applying machine learning to legal editing  these results require several caveats 
 the ndas are sourced from a single firm and consequently the models identify that specific firms preferred edits  other
firms may have different editing preferences  which these models may not be able to recognize or may find more difficult
to learn in the training process  leading to poorer classification results than shown in this paper 
 because there are only     documents  the text reflects a limited subset of the edits made  the language used  or situations
addressed in an nda  additional data would be required to further generalize the three models and to further mitigate
unbalanced data sets 
 given that identifying edits is highly context dependent  treating sentences as observations may remove important context
available earlier in the paragraph or earlier in the document  models incorporating prior sequences into predictions  e g 
recurrent neural networks  and longer training observations  if additional ndas are available  should be evaluated 
 text processing  feature selection  and model training could be further customized for legal documents  hopefully leading
to improved classification results  this has already proven to be a successful strategy with the enhanced performance of
models trained on edited sentences or randomly undersampled bigrams 
 the crf can have a tendency to overfit the training data with complicated features that reflect nuances in one or two
training examples  the feature set was deliberately restricted for the model in this paper  narrowing a potentially large
gap between training and testing accuracy to         training accuracy vs      testing accuracy  
   

fiedited
sentences

rus
bigrams

inserted
deleted

hidden markov model
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    

structured perceptron
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    

conditional random field
overall accuracy        
edited token accuracy       
precision recall f  score
    
    
    
    
    
    

inserted
deleted

overall accuracy        
edited token accuracy       
    
    
    
    
    
    

overall accuracy        
edited token accuracy       
    
    
    
    
    
    

overall accuracy        
edited token accuracy       
    
    
    
    
    
    

table    summary of classification results after adjustments for unbalanced data
testing example tokens tagged by each model
hidden markov model 

structured averaged perceptron 

conditional random field 

example paragraph tagged by each model
hidden markov model 

 del within ten days   del   ins promptly   ins after being so requested
by the company or  name   del   name   del  ins  name  in writing   ins  except to the extent you are advised by legal counsel that complying with such request would be prohibited by law or regulatory authority  you
will return or destroy at your cost all evaluation  del  material at the option  del  ins material 

structured averaged perceptron 

 del within ten days  del  ins promptly   ins after being so requested
by the company or  name   del   name   del  ins  name  in writing   ins except to the extent you are advised by legal counsel that complying with such request would be prohibited by law or regulatory authority  you
will return or destroy at your cost all evaluation  del  material at the option  del  ins material 

conditional random field 

 del within ten days  del  ins promptly   ins after being so requested
by the company or  name   del   name   del  ins  name  in writing   ins except to the extent you are advised by legal counsel that complying with such request would be prohibited by law or regulatory authority  you
will return or destroy at your cost all evaluation  del  material at the option  del  ins material 

figure    visualized classification results for training on all sentences  green represents correct classifications while red
represents incorrect classification  names redacted for this paper are indicated with  name   

conclusion
despite caveats  this papers classification results should be treated as a positive sign that applying machine learning to
editing legal documents is feasible and worthy of additional research  approximately     overall accuracy and     edited
token accuracy in the best models for this simplified classification task show that supervised learning models can handle the
complexities of sparse  unbalanced  context rich legal document data as long as the right text and training processes are in place 
many additional steps need to be taken before machine learning lightens lawyers workloads  the long term ideal model
that automates editing original ndas will need to identify deleted tokens similar to the tagging done by the models in this
paper  yet  deleted tokens had the poorest classification performance and were at best identified correctly     of the time 
preliminary testing of models trained on edited documents to find words that should be deleted in original documents were
unsuccessful  in addition  the long term ideal model will have to generate text to insert into the original document  with more
options on how many and what particular words are inserted  this will be the largest and most rewarding challenge 

references
   colgrove  c   tibshirani  j    wong  r  predicting edit locations on wikipedia using revision history  stanford undergraduate
research journal             
   bird  s   loper  e    klein  e  natural language processing with python         
   finkel  j  r   grenager  t    manning  c  incorporating non local information into information extraction systems by gibbs
sampling  proceedings of the   nd annual meeting of the association for computational linguistics  acl             
       

   

fi