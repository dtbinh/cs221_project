optimised prediction of stock prices with newspaper articles
bryan cheong  stanford university  mathematical and computational science  bcheong stanford edu
christopher yuan  stanford university  computer science  cqyuan stanford edu
stephen ou  stanford university  computer science  sdou stanford edu

  

abstract

for this project  we investigated the predictive power of newspaper
articles on the stock prices of various companies  using supervised
learning  we were able to obtain a testing accuracy of up to     
we then performed reinforcement learning on this predictor  feeding its predictions into a markov decision process  mdp  which
bought and sold shares on a simulation that we programmed  we
compared the mdps performance on a years stock prices to that
of a random guesser  overall  we conclude that the mdp with our
predictor generally outperforms a random guesser 

  

introduction

we treated data from newspaper articles with machine learning
tools to predict up or down changes in the stock prices  i e  our response was binary  in particular  we examined four algorithms  the
naive bayes algorithm  svm  the perceptron  and boosting with
weak learners on a bag of words model of newspaper articles  in
addition  we optimised these algorithms by stemming the bag ofwords  introducing term frequency inverse document frequency
 tf idf  to improve our algorithms grasp of the relevance of
words  and finally with cross validation for the best parameters 
after obtaining a base predictor that predicts using newspaper articles whether the stock of a company would go up or go down  we
also wrote a markov decision process  mdp  learner that builds on
the base predictor using reinforcement learning that is able to buy
and sell shares of a company  in order to do so  we needed to make
the assumption that this approximates to a markov process  our final simulation tests if there may be credence in such an assumption 
despite its limitations 

  

previous works

our literature survey found five works that also used news articles
for classifying stock prices  however  almost all of these previous
works used a naive bayes classifier on a simple bag of words
model  which generally performed poorly 
gidfalvi     implemented a naive bayes text classifier on financial news to track very short term  on the scale of minutes  fluctuations in stock prices  and concluded that there is a strong correlation between news articles and the movement of stock prices in a
window between    minutes before and    minutes after the news
articles are published 
two such previous works were former cs    n projects from
stanford university  ma      and lee and timmons      both
projects used two approaches  a naive bayes classifier and a logistic
regression classifier  which both papers referred to as a maximum
entropy classifier   and both projects generally concluded that the
naive bayes classifier was relatively ineffective  and the logistic
regression classifier had slightly better accuracy 

chen et al      are an undergraduate group at uc berkeley that
used a naive bayes classifier and sense labelling of words in news
articles  whether they are positively or negatively connotated  to
categorise stock movements in the companies mentioned in these
articles  once again to test the efficient market hypothesis  however 
they concluded that the sense labelling of news articles have no
impact on the movement of stock prices 
the last work is a masters thesis from the norwegian university
of science and technology by aase      in his work  aase used
an svm text classifier with an rbf kernel and sense labelling of
words to make predictions on stock prices  but limited his scope
only to norwegian oil companies 
we observe that none of these previous works employed tf idf
or cross validation to improve their algorithms  which may explain
why they mostly concluded that newspaper articles have weak predictive power on changes in stock prices  in addition  none of these
previous works used reinforcement learning to improve on their
base predictor 

  

data

we needed to obtain articles that mention specific companies from
reputable newspapers in order to compile our data set and set up
our bag of words models  we decided to use reputable sources
instead of publications like twitter  buzzfeed  etc  because we hypothesize that reputable articles based on fundamental company
research and understanding will generally have more predictive
power than less reputable articles  unfortunately  the newspapers
that we wish to examine  such as the new york times and the wall
street journal  have paywalls that block their websites and make automatic scraping difficult  instead  we relied on the proquest newsstand database to obtain our data 

input

output

search terms

stemmed text

federated search
algorithm

stemming
algorithm

pqns xml tree

raw text

xml tree parsing
algorithm

article parsing
algorithm

pqns article urls
fig    webscraping pipeline

fi 



proquest newsstand archives all articles from these newspapers in a searchable database  by writing an algorithm to generate
federated search urls  we were able to automatically obtain the
pqms xml tree which contains the urls of the full texts of articles that mention the company in interest  from the newspapers
that we want the articles from  and the year for which we wish to
generate the data  we then wrote an xml tree parser in order to automatically compile a list of all the pqns urls wherein the article
full texts are contained 
unfortunately  proquest does not allow these articles to be accessed without a validating url  hence  we also had to write a
web scraper specifically tailored to work around the limitations of
the proquest database by generating curls so that our automatic
article gatherer behaved like a real users browser  we then used
regular expressions to parse only the full texts and the article publication dates from the webpages to finally obtain our raw text data 
which could then be treated with the python stemming library for
pre processing  the schematic in fig   summarize the webscrapping pipeline that we used in order to obtain our text data 
next  we modeled our input data  i e  news articles  as a bagof words model  for the input matrix x  each of the n columns
represents a word in our dictionary  and each of the m rows represents a news article  each xij entry contains the frequency for
which the word j appears in the news article i 
our response variables were the stock movements of individual
companies  we obtained the end of day stock prices for the last
twenty years of six companies 
    boeing
    general motors
    mcdonalds

therefore 
p x            xn  
 p x   y   p x   x    y        p xn  x            xn    y 
 p x   y   p x   y        p xn  y 
n
y
 
p xi  y 
i  

now  to figure out whether the stock price will go up  y      or go
down  y       we will calculate the probability
p y     x 
p x y     p y     
p x 
q
  n
i   p xi  y      p y     
q
  qn
p x
 y
 
   p y
       n
i
i  
i   p xi  y      p y     

 

then  we will calculate the same probability for stock price going
down  i e  p y     x   now  we can classify whether y     or
y     based on which one has a higher probability 
in addition  we also treated our data using support vector machines  svm  in the hope to capture the nonlinear relationship in
our data  the support vector machine model was initialised without smoothing on a gaussian kernel 
 
  x  z      
   
then  we used the perceptron algorithm  a simple linear algorithm that decides whether an input belongs to one class or another 
we first tried the algorithm with a bias term  so we made a prediction according to
k x  z    exp 

    tesla
    twitter

h  x    g t x   b 
where

    valeant
we chose these particular companies either because their stock
prices are volatile or mainstream news publications frequently
write about them 
the stock prices for these six companies were obtained from the
quandl database using their api  for our base predictor  we only
took into account whether the stock prices moved up or down to
obtain a binary response  our response  i e  stock prices  is a binary label where    indicates the stock price has gone up and   
indicates the stock price has gone down  we computed the label by
subtracting the closing price on the day the article came out from
the closing price before the day the article came out  if the article
published date falls on a weekend  when the markets are closed 
we used the difference between the monday closing price and the
friday closing price 
we separated our data into a training and testing set  where the
training set was twice the size of the testing set  in total  we processed       articles for our dataset over six companies 

  
   

methodology
supervised learning algorithms

we used four supervised learning algorithms to treat our data  since
previous literature mostly used the naive bayes algorithm  we included it in our investigation to compare with other algorithms  for
naive bayes  we made whats called the naive bayes assumption we assumed that all the xi s are conditionally independent given y 

 
g z   

 
if z   
 
  if z    

finally  we applied a boosting algorithm that automatically
chooses its feature representation  in this case  our boosting algorithm was initialized using simple decision stumps as weak learners 
our motivation for using these four algorithms was to compare
their relative performance in testing accuracy as we applied the various optimisations in subsequent steps 

   

optimizations

our first step in dealing with the raw text data was to pre treat it
with a stemming library  this is to ensure that reflects the various
forms of a word  e g  diversify  diversifying  diversified  as
the same token  our reasoning for this is that the various forms
of the word do not differ significantly in semantic content  and indicate roughly the same meaning when they occur in newspaper
prose  so the stemmed forms of the words is a better representation  we will compare the test accuracy of the various algorithms
using stemmed and unstemmed tokens 
our second step in optimising the four algorithms was to apply
term frequency   inverse document frequency  tf idf  weighting on the stemmed text data  tf idf places the following weights
on the tokens as they appear 
tfidf t  d  d    tf t  d   idf t  d  

fi

be   let the number of states we have be represented by n  
finally  let osa  s    be the number of observations from state
s to s  under the action a  then the transition probability is
given by the following 
 
   osa  s   
if p s   s    
n

psa  s    p    n     
      osa  s    otherwise
n

tf t  d        log tf t d  
size d 
 
df t

where t refers to the term index  d refers to the document index 
and d the corpus of all documents  additionally  tf t d refers to
the number of times the term t appears in the document d and df t
refers to the number of documents d the term t appears in  finally 
tfidf t  d  d  refers to the final weight placed on the term t  this
weighting places less weight on words that occur in more articles 
and that are therefore less able to distinguish between articles 
our motivation for applying tf idf was because we observed
that a naive weighting on token frequencies put heavy weights on
very common words such as the  for  etc  which dominate other
words in terms of absolute frequency  we hypothesized that while
such words are  on an absolute level  more common words  they
are less able to distinguish between articles than rarer words that
are able to distinguish more between articles  or better reflect an
articles semantic content 
finally  we applied parameter optimisation via cross validation
of these four algorithms  drawing on the training data for a crossvalidation set  the final optimised parameters are  naive bayes
with a laplace smoothing of   on the observations  svm with a
linear kernel without smoothing  perceptron without regularisation 
and boosting on decision trees with arbitrary depth on     estimators  we used the final parameter optimised algorithms to be used
on our mdp reinforcement learning simulation 

   

reinforcement learning

having obtained a base predictor for stock price movements based
on newspaper articles  we applied reinforcement by using it to build
a markov decision process  mdp  that simulated the buying and
selling of shares  the base predictor we used to train the mdp is
the optimised naive bayes predictor  we placed our mdp learner
in a simulation run using real data from the stock price of tesla
in the year       and using real newspaper articles from the new
york times and the wall street journal  all of which were not in
the training set used to train the base predictor  our design of the
mdp learner is as follows 
these are the parameters we used to write our markov decision
process 
    states  percent return from the starting amount
    rewards  same as the state  a higher percent return equates
to a higher reward
    discount factor       
    actions  for the most part  our algorithm was allowed to buy
or sell up to   stocks  or take no action  however  we placed
restrictions on the actions based on the following conditions 
if we had negative cash  the algorithm was only allowed to
sell stocks  up to   
if there was no article for a particular day  the only allowable
action was to do nothing    
    transition probabilities 
before discussing transition probabilities  we must first consider the observations we recorded  we observed every transition from one state to another under an action  for example  if we moved from    return to    return by buying  
shares  we would record it 
now  let the prediction we get from an article on a given day
be represented by p  and the test accuracy of that prediction

in order to implement mdp  however  we needed to fundamentally
assume that the day to day transitions of a stock buy and sell portfolio satisfy the markov property  such an assumption is  we acknowledge  possibly contentious  while changes in the price of a
stock on a day to day basis and the actions of buying and selling
a stock are arguably independent  because our mdp is a trader  it
necessarily needs to hold different amounts of stocks in its portfolio
from day to day  this different level of stocks being held or shorted
is not reflected in the state  which therefore dissatisfies the markov
property  hence  we implemented a penalty on holding or shorting
too much stock  such that the amount of stock held by our mdps
portfolio is usually within    to   shares  in that manner  it is able to
buy or get rid of stocks within a single action  and so the transitions
between states and actions approximates the markov property  in
other words  we are able to implement mdp by making the percent
changes in the value of the portfolio and the actions of buying and
selling stock a pseudo markov decision process 
then  we implemented the value iteration algorithm to compute
an optimal mdp policy and its value  for each day that the stock
market is open  we compute the maximum q values that a state
can obtain by taking all the possible actions  where the q value
equals the sum of qi for each of the new states s reachable from
the current state s  and
qi   transprob s  s      reward s      discount  value s     
please see the appendix a for a pseudocode summary of our
markov decision process and value iteration implementation 

  

results

generally  the four algorithms showed incremental improvement
with each optimisation that we applied  as demonstrated in the upward trend in test accuracy shown in fig     which plots the average
test accuracy of the algorithms across the six companies for which
we used in our dataset 

naive bayes

svm

boosting

perceptron

    
    
test accuracy

idf t  d    log

 

    
    
    
    

baseline

stemming

stemming
  tf idf

stemming
  tf idf  
best params

fig    test accuracy with various learning algorithms

fi

the baseline algorithms  without any optimisations  generally
were no better than chance  with test accuracy around      however  after applying all our optimisations  the algorithms generally
improved in test accuracy to around      except for the naive
bayes algorithm which was favoured by previous literature  which
was never able to obtain a test accuracy above       we summarise
the test accuracy of the incrementally optimised algorithms in
fig   

baseline
  stemming
  tf idf
  optimised params

naive bayes
     
     
     
     

svm
     
     
     
     

boosting
     
     
     
     

finally  for our results from the mdp simulation  we find that
the learner is generally able to outperform a random guesser when
buying and selling stock using the base predictor  fig   below illustrates the percentage of return of our learner  in red  v s  the percentage return of the random guesser  in blue   this figure is based
on tesla stock and articles from january         to december    
      after running simulation of     trading days  a full year has
only around     working days  excluding weekends and holidays 
when the stock markets are closed   our learner produces a return
of        while the random guesser produces a return of        

perceptron
     
     
     
     

simulation performance

 

random guesser
value iteration algorithm

 

 

percent return

 

fig     tabulated test accuracies from incrementally optimised
algorithms

  

  

in addition  we also observed that after tf idf  the weights that
the naive bayes algorithm placed on tokens seemed to be intuitively meaningful  we extract some of the top positive and negative
weighted terms in fig    below  seeing how the test accuracy for the
algorithms improved all around after the application of tf idf  it
bears out our hypothesis that such rarer words  which are better
able to distinguish between different articles  should be given more
weight  these rarer words  as listed fig      are what we would ordinarily expect to connote positivity or negativity surrounding
a description of a company in a newspaper article 
term
suggest
rumor
high
report
rise
express
staff
soar
diversified
reorganization

weight
    
    
    
    
    
    
    
    
    
    

term
wary
discourage
misinform
reprehensible
abusive
dispute
denounce
cheapen
rough
takeover

weight
     
     
     
     
     
     
     
     
     
     

fig     tokens with positive or negative weight from naive bayes
predictor  in unstemmed form for readability 

  

  

 

  

   

   

   

   

number of days

fig    mdp returns vs  random guesser for tesla  jan           to
dec           
there may be multiple sources of error for our models  in particular  we noted that the testing accuracy of our boosting algorithm
decreased when we used cross validation to optimise the parameters from using boosted decision stumps to boosted decision tree of
arbitrary depth  this is probably a case of over fitting 
in addition  since there were insufficient days in the year to allow
the learner to converge  the mdp is still not making optimal decisions  it is unclear  however  if extending the learning period and
increasing the amount of data the learner has access to will lead
to convergence  since the if we extend the period to more than one
year  the assumption of the markov property may begin to break
down  and the transition probabilities arrived at by the mdp for
one year may not be useful in the next year  or in the long run in
general  whether this is a fundamental limitation of using mdp for
such a use would require further experimentation 

  

conclusions

in summary  the four algorithms that we used  naive bayes  support vector machine  perceptron  and boosting  without any optimisations only produced results no better than random  as expected 
however  adding several optimisations were able to bring the test
accuracy up to     
stemming helps unify the words that are represented in different forms in our training and test data but have the same semantic
meanings  tf idf helps increase the weights of rare words that are
strong indicators of a particular class and decrease the weights of
common words that are weak indicators of a particular class  parameter tuning helps us to figure out what are the best parameters
to use in the four algorithms in order to produce the best results 
there are few interesting takeaways from our work in modeling the stock simulation as a markov decision process and run 

fi

ning the value iteration algorithm on it  first  we found that modeling the discretised percentage returns as states helps the value
iteration algorithm converge quickly  second  we found that using
past observations to determine the transitional probabilities gave us
an accurate measure of the magnitude with which the stock would
change in the future  third  adding a penalty for buying too much
stock helps prevent the case where it would take too long to sell
the stocks during a negative run of a particular stock  fourth  this
penalty also helped us approximate the markov process even when
some assumptions about the model have been violated  lastly  we
acknowledge that in order to conclusively determine whether or
not the mdp simulator consistently outperforms a random guesser 
more analysis must be performed 

   

  

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

future work

we tested the four algorithms of our base predictor on individual
companies stock prices separately  it remains an open question as
to whether such supervised learning methods would be effective
in predicting stock price movements in general  i e  if there is a
general model that is able to be tested on companies in general 
this would be especially interesting if there is significant predictive
power when testing on companies that were never included in the
training dataset in the first place 
it was clear that even after running a years worth of data on
our mdp algorithm  it still had not reached convergence after the
end of the year  while we may suggest that the mdp algorithm
be simulated over a longer time period than one year  there is an
open question as to whether a companys stock price  over a longer
period of time will still allow the mdp learner to approximate a
markov decision process  it is generally acknowledged that stock
prices are more variable in the long run  hence the short term forces
that underlie the mdp learners transition probabilities may change
over the long run  while investigating the long run future accuracy
of the base predictor  and the long run soundness of modelling stock
purchase and sale on an mdp are interesting  they are unfortunately
outside the scope of our project  wherefore we leave these questions
to future work 

  

references

    gidofalvi  gyozo  using news articles to predict stock price
movements  department of computer science and engineering  university of california  san diego        
    ma  qicheng  stock price prediction using news articles 
cs   n  final report        
    timmons  ryan  and kari lee  predicting the stock market
with news articles  cs   n  final report        
    chen  jerry  and aaron chai  madhav goel  donovan lieu 
faazilah mohamed  david nahm  bonnie wu  predicting
stock prices from news articles         
    aase  kim georg  text mining of news articles for stock
price predictions         

appendix

algorithm   reinforcement learning algorithm
   date  jan         
   cash      
   numstocks   
   observations     
   values     
   totalvalue      
   predictions  getlearningpredictions  
   prices  getstockprices  
   state  getstartstate  
    epsilon      
procedure s imulator
for all dates do
action  getactionwithmaxvalue  
newstate  getnewstate state  action 
observations state  action  newstate     
runvalueiteration  
state  newstate
procedure getaction w ith m ax value
values     
for actions reachable from state do
for all reachable newstate with transprob  reward do
values append getvalue newstate 
transprob  reward  
return argmax values 
procedure get m ax value
values     
for actions reachable from state do
for all reachable newstate with transprob  reward do
values append getvalue newstate 
transprob  reward  
return max values 
procedure get n ew s tate
cash  cash  prices date   action
numstocks  numstocks   action
date  getnextdate  
totalvalue  cash   numstocks  prices date 
state  getreturn totalvalue 
procedure run value i teration
while true do
oldvalues  values
for all states in mdp do
values state   getmaxvalue state 
if max oldvalues   values    epsilon then break

 

fi