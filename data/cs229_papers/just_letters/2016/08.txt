reinforcement learning for traffic optimization

matt stevens
christopher yeh

mslf   stanford   edu
chrisyeh   stanford   edu

abstract
in this paper we apply reinforcement learning
techniques to traffic light policies with the aim
of increasing traffic flow through intersections 
we model intersections with states  actions  and
rewards  then use an industry standard software
platform to simulate and evaluate different policies against them  we compare various policies including fixed cycles  longest queue first
 lqf   and the reinforcement learning technique
q learning  we evaluate these policies on a varying types of intersections as well as networks of
traffic lights  we find that q learning does better
than the fixed cycle policies and is able to perform on par with lqf  we also note reductions
in co  emissions in both lqf and q learning
relative to a fixed cycle baseline 

we can define our states to incorporate as much information about the system as we want and share it across many
traffic lights to allow them to coordinate 

   algorithm
     q learning
we use q learning with function approximation to learn
the best traffic signal actions  before we detail our specific
problem formulation  we first describe the q learning algorithm for a general markov decision process  mdp   under a mdp with fixed transition probabilities and rewards 
the bellman equation  equation    gives the optimal policy 
q s  a    r s       max
q s    a   
 
a

if the q function can be correctly estimated  then a greedy
policy becomes the optimal policy  and we can choose actions according to equation   

   motivation
according to a study by texas a m  americans waste
about   billion hours and   billion gallons of fuel in traffic each year  david schrank   bak         this makes up
roughly    of all gasoline consumed in the united states
 eia         this means that reducing traffic can have a
significant impact on peoples lives  as well as their carbon
footprint 
for this project  we aimed to reduce traffic by finding better traffic light policies at intersections  an ideal intelligent
traffic light systems can reduce traffic through several techniques  it can turn lights green for longer in directions with
more traffic  use sensors to dynamically respond to arriving cars and changing traffic conditions  and coordinate between lights to create runs of traffic that flow through many
lights 
reinforcement learning is a promising solution to this
problem because it can represent all of these techniques 
by using a general notion of actions  we can decide when
to turn lights on and for how long  it excels at dynamic
control and is designed to adapt to new conditions  lastly 

   

 s    arg max q s  a 
a

   

we use the function approximation abstraction from  mnih
et al          from our simulator we extract values for
the state features  action  reward  and next state features 
 s  a  r  s     note that the s variables are not states  but features of states  thus  q s  a  represents the approximate
q value for that state and action  each  s  a  r  s    tuple is
a training example for our q function  in order to satisfy
the bellman equation  equation     we minimize the loss
between our current q value and our target q value over
parameters  subject to regularization   equation    


   
min   r    max
q s
 
a
 
 

q s 
a 
   
 
    
   
 


a

     q functions
the heart of q learning is the q function used for estimation  in the naive case  the q function can simply be a
lookup table that maps states and actions to q values  in
which case q learning is essentially the same as value iteration  however  q learning lets us generalize this framework to function approximations where the table of states
and actions cannot be computed 

fireinforcement learning for traffic optimization

every part of equation   is differentiable  so if our q function is differentiable with respect to its parameters  we can
run stochastic gradient descent to minimize our loss 
for our implementation  we use stochastic gradient descent
on a linear regression function  we also performed sgd
with a simple one layer neural network 

   problem formulation
we approach this problem as a mdp with states  actions 
and rewards 
     simulator
to model an intersection as it would exist in the real world 
and to evaluate our policies  we followed other researchers
 covell et al         in using sumo  simulation of urban mobility   an open source industry standard software
package for modeling road networks and traffic flow  krajzewicz et al          in particular  we used sumo version
      sumo allowed us to build different types of road
networks  add cars to our simulation and determine their
routes  and add sensors for the traffic lights 
the sumo package also comes with traci  a python api
that allows a user to get information from a traffic simulation and modify the simulation as it runs in response to this
information  we built an architecture that made use of the
traci interface to get information about queue sizes  carbon emissions  sensor data  and traffic light states  in order
to successfully deploy our algorithms 
     setup
our road network featured   connected traffic lights in a
 x  square grid  spaced    m apart  each traffic light
was also connected to a    m long road  which we used
to mimic a highway on off ramp  thus  we had a total of
  source nodes and   destination nodes  each road contained   lanes with a speed limit of    mph  and we used
sumos default configuration for left turn and right turn
lanes  each traffic light was set to allow right turns on red 
yellow lights were set to be   seconds long  on the roads
in between the traffic lights  induction loop sensors were
placed in each lane about   m before each traffic light 
on the roads leading into the network  the induction loops
were placed about    m before each traffic light  we used
sumos default car configurations  every second  for each
possible  source  destination  combination  we generated
a car at with probability       figure   shows the road network that we built 

figure    a zoomed out view of the road network that we trained
and tested our q learning algorithm on  the small yellow dots
are cars traveling through the network  while the larger yellow
rectangles represent induction loop sensors

     objective and reward function
there are many ways to formulate an objective function
for traffic optimization  ideally  we would try to model the
lost utility of drivers time spent waiting in traffic  and the
cost and environmental effects of wasted gasoline  these
effects are all difficult to estimate  fortunately  however 
all of these effects are positively correlated with increasing traffic  so any metric the captures the general trend of
the amount of traffic will capture the general trend of these
effects 
we specifically chose the throughput of cars through intersections as our reward function  where throughput is defined as the number of cars that pass through the intersection per unit time  while there are many other reasonable
measures of the amount of traffic  e g  wait time  co 
emissions  and total distance traveled in a given time interval   we chose throughput as our reward function for two
reasons  first  this reward is more directly tied to the action taken  meaning that the learning algorithm has to do
less work separating the signal from the noise in the rewards  second  the reward varies more linearly with traffic
flow than other metrics  which makes it easier to fit 
sumo does not provide a direct way to calculate throughput  so instead  we calculated the sum of the speed of cars
through the intersection per time step  this sum approximates a discrete integral of speed over time  giving the total distance traveled by the cars through passing through
the intersection  then  the total distance traveled divided

fireinforcement learning for traffic optimization

by the width of the intersection is equal to the number of
cars that pass through the intersection  thus  our reward
function is proportional to throughput  and off by a constant factor of the length of the time step and the width of
the intersection 
     actions
we formalize each traffic light as an agent that can perform a set of actions  after each    second interval  the
traffic light chooses which direction to turn green  for example  a traffic light with no left turns may choose from
the actions  north south green  east west green   we designed the traffic lights in our network to allow left turns 
so they had two additional actions  while some approaches
use signal durations as actions  salkham et al          we
chose this parameterization because it offers more flexibility   arel et al         we abstract these actions away so
that the agent only knows that there are several distinct actions it can perform  and not which directions they correspond to 
     features
our goal is to have each agent  i e  traffic light  learn the
optimal policy  i e  which direction to turn green  based
on inputs that would be available in the real world  to this
end  we give our model three types of features 
   sensor data  sumo has the capacity to simulate
induction loop sensors  in the real world  these
loops are generally installed under streets to provide
nearby traffic lights with information about cars passing above them  in our simulation  we placed induction loops in each lane before every traffic lights  for a
total of    induction loops per traffic light  these induction loops inform their respective traffic lights with
the number of cars that have passed over them in the
last time step  along with the average speed of these
cars  we also provide each stoplight with the previous
five time steps worth of sensor information 
   stoplight history  we provide each stop light with
the previous five time steps worth of its phases  a
phase in this context refers to the specific permutation
of lights colors for each lane in the intersection  each
phase is represented by a number in the feature array 
   features from adjacent traffic lights  in order for
each traffic light agent to learn to coordinate with the
other agents  we provide each agent with the features
given to the adjacent stoplights  as in  salkham et al  
      
in total  we represent each state with     features 

due to our backpropagation algorithm  it was important to
normalize features to the same range  the gradient value
is multiplied by the feature value during backpropagation 
so features with high values get high weights during training  and since they had high values to begin with  their
influence is increased quadratically in the final regression
value 

   experiments
     parameters
to figure out the optimal hyperparameters for the qlearning algorithm  we used a combination of random and
manual search  we ran a smaller number of training and
testing iterations to achieve acceptable models and then extracted the average number of cars waiting as our metric for
comparison 
the discount factor  parameter in q learning had no significant effect on results  our learning rate  and regularization  were fairly standard  for linear regression we
used        and          for our neural network we
used        and         if we decreased regularization or increased learning rate by too much  our function
values exploded due to the recursive nature of the q function 
in q learning  it is important to balance the need to explore
the state space with the need to choose good actions  for
this reason  we used an  greedy algorithm which chose
the highest q action most of the time  but occasionally
chose a random action  we found that too many random
actions could cause disastrous results across the grid  including gridlock  making it very difficult for our algorithm
to learn well  to address this  we used the lqf algorithm
described below to choose a heuristically good action some
fraction of the time instead of the random action  this gave
our algorithm a warm start  and helped prevent gridlock 
we found that choosing the heuristic action instead of a
random action      of the time worked best 
we also realized that we might get better results if  decreased over time  reflecting the fact that the algorithm
needs to learn as it converges  to this end  we incorporated
a parameter denoting the half life of the epsilon parameter 
so that it would decay exponentially over time  we found
that setting the half life to     time steps worked best 
     baseline
we implemented three other algorithms for comparison
against our q learning approach to traffic optimization 
   short cycle  changes the phase every    seconds in
a round robin fashion 

fireinforcement learning for traffic optimization

   long cycle  change the phase every    seconds in a
round robin fashion 
   longest queue first  lqf   lqf chooses to let the
direction with the highest number of cars be green 
previous research  wunderlich et al         has shown
that lqf is robust algorithm even under high load  in
the real world  it is not possible to directly find queue
lengths  and we extracted this information directly
from the simulator  although lqf is a greedy algorithm  it is given more information than q learning 
which makes it a reasonable target for comparison 
     results
we trained q learning on an episode of      time steps
using the  greedy approach as described above  and then
used the parameters to test on a new episode  choosing the
action with the highest q value every time 

figure    average amount of co  emissions per distance traveled
by cars in the simulation 

tions is expressive  and tends to represent many common
control paradigms for traffic lights  cycles can be represented with the previous action features  for example  if
we want to switch from action   to action   after four time
steps  then action   can have a high weight for performed
action   four time steps ago  actuated systems that respond to arriving cars can be represented by high weights
on the sensor features  lastly  networks that coordinate
to let runs of cars through can be represented by our network as well by incorporating previous action features from
neighboring lights  looking at our feature weights and our
simulation results  we saw all of these kinds of learning
taking place together 

figure    average number of cars waiting at stoplights during the
simulation  all differences are highly significant 

as shown in figure    q learning with linear regression
performs as well as our lqf baseline  our neural network
implementation performs slightly worse than linear regression  when we look at the co  emissions data provided
by sumo  though  the performance is not directly correlated  we see that the two q learning algorithms are now
tied  as well as the two cycle algorithms  figure     we
believe that this is a result of how q learning coordinates
cars between the four intersections  reducing the amount
of start stop motion that creates the most amount of co 
emissions 

   conclusion
we believe that our linear regression algorithm performed
so well because the hypothesis class of our features and ac 

our linear hypothesis class was incapable of fully representing lqf  however  estimating the number of incoming
cars between the sensor and the light is simple  estimating
the number of cars beyond the sensor can only be done in
expectation using features for neighboring lights and sensors  the fact that q learning can learn the number of cars
arriving on average explains why q learning and lqf had
comparable performance for average waiting time  and the
fact that this estimation is noisy explains why we see more
emissions in the q learning scenario  meaning that the cars
stop and start more frequently as the light makes small errors  however  lqf should be representable as a general
function of our features  if not a linear one  for this reason 
we implemented a neural network model 
our neural network model performed worse despite having
a more expressive hypothesis class  in fact  because our
neural network used a relu activation function  the hypothesis class of linear regression was a subset of that of
our neural network  this means that under perfect training  our neural network can never do worse than a linear
regression  clearly  we were not training our neural net 

fireinforcement learning for traffic optimization

work perfectly  since our hypothesis class was larger  the
neural network had lower bias but higher variance  since
we are using a simulator  we have access to infinite training data  so in theory more iterations and a lower learning
rate would solve these issues  however  these techniques
had only modest gains for us  suggesting that the changes
needed to match the performance of linear regression are
drastic 

well documented traffic simulation software suite 

overall  our results show that reinforcement learning is effective at learning good traffic light policies  our algorithm
was able to perform as well as lqf  which had more information about the system than any real world algorithm
would have available  though not the optimal policy  it is
a high bar to meet  and suggests that q learning can be a
powerful technique 

covell  michele  baluja  shumeet  and sukthankar  rahul 
micro auction based traffic light control  responsive 
local decision making  in intelligent transportation systems  itsc        ieee   th international conference
on  pp          ieee       

one of the key takeaways of this work was that domain
knowledge can be used to confine the hypothesis class to
a set of more reasonable policies  we are not the first to
come to this realization  algorithms that are now standard for traffic control choose over a library of possible
light timings  jayakrishnan et al          this limits the
hypothesis class to a set of actions that are all reasonable 
but some are better than others  we did something similar in our model  in that our formulation of actions did not
allow the controller to control yellow lights  so all of our
actions were guaranteed to not cause a car crash  clearly
there is a tradeoff between expressiveness of actions and
the potential for bad results  future collaboration with traffic engineers to bound the problem and use domain knowledge to eliminate the worst actions may yield an algorithm
with real world applicability  skeptical users of the system
could be guaranteed that the algorithm would give reasonable results even in the worst case  which is something that
our system cannot guarantee 
q learning shows promising results  and model free systems like q learning can use the power of machine learning to discover trends that are overlooked by the heuristics
and approximations of explicit optimization algorithms  a
reinforcement learning system has the potential to provide
adaptive control and coordination  theoretically matching
the current state of the art  on top of this  multi agent reinforcement learning is a distributed technique  which gives
it fault tolerance as well as the potential to scale up to a
larger network  for all of these reasons  we believe that
reinforcement learning is a promising paradigm for traffic
control 

   acknowledgments
we thank our cs     classmate alex tamkin
 atamkin stanford edu  for his contributions
to this project  we also thank the sumo team for their

references
arel  itamar  liu  cong  urbanik  t  and kohls  ag  reinforcement learning based multi agent system for network traffic signal control  intelligent transport systems  iet                    

david schrank  bill eisele  tim lomax and bak  jim  urban mobility report  technical report  texas a m  college station  tx       
eia  mar      
url https   www eia gov 
tools faqs faq cfm id    
jayakrishnan  r  mattingly  stephen p  and mcnally 
michael g  performance study of scoot traffic control
system with non ideal detectorization  field operational
test in the city of anaheim  in   th annual meeting of
the transportation research board       
krajzewicz  daniel  erdmann  jakob  behrisch  michael 
and bieker  laura  recent development and applications
of sumo   simulation of urban mobility  international
journal on advances in systems and measurements   
              december      
mnih  volodymyr  kavukcuoglu  koray  silver  david 
rusu  andrei a  veness  joel  bellemare  marc g 
graves  alex  riedmiller  martin  fidjeland  andreas k 
ostrovski  georg  et al  human level control through
deep reinforcement learning  nature               
          
salkham  as ad  cunningham  raymond  garg  anurag 
and cahill  vinny  a collaborative reinforcement learning approach to urban traffic control optimization  in
proceedings of the      ieee wic acm international
conference on web intelligence and intelligent agent
technology volume     pp          ieee computer
society       
wunderlich  richard  liu  cuibi  elhanany  itamar  and urbanik  tom  a novel signal scheduling algorithm with
quality of service provisioning for an isolated intersection  intelligent transportation systems  ieee transactions on                    

fi