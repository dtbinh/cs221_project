have you met the    a machines approach to human relationships
jiayu lou  hang yang

introduction
we have all pondered about the same thing in a
relationship  is he she the right one for me 
countless articles  quizzes  suggestions  tips  and
consultants out there are trying to answer this
question for us  they bring out big words to make
it logical  they tell us to mind the differences  but
also note that some differences matter and some
do not  they place their own arbitrary weights on
terms such as ambition  core values  intelligence 
emotional intelligence  spiritual beliefs  etc  it
makes us wonder  if we are already talking about
logic in romance  why dont we take a step further 
what if  instead of reading these sources and still
remaining puzzled about the relationship  we can
tell you with a high confidence level exactly how
likely you are going to be with your significant
other for the rest of your life with the help of a
machine learning algorithm  thanks to how
couples meet and stay together dataset
 hcmst  by rosenfeld  michael j   reuben j 
thomas  and maja falcon  it is now possible 
based on your everyday habit  your usage of
internet  your marriage status  and other
miscellaneous information about your life  to
predict if you will stay with him her for at least
the next few years 

related work
the hcmst dataset has been utilized in various
ways concerning different social study fields  yet
most of the papers are based on a few arbitrarily
chosen measurements  and no machinelearning related work is found  early work
entails the internet related data to argue
increasing internet coverage has increased
chance of meeting partners for glb  gay 
lesbian  bisexual  individuals  and internet as a
social intermediary has partly replaced traditional
dating spaces      on a similar note  data on the
respondents sexualities and their relationship
longevity has been used to support the assertion
that same sex couples break up rates are
comparable to heterosexual couples     

more recent working papers discuss the topic of
relationship stability as a whole  following the
main motives behind the hcmst project 
however  restricted by the limits of human  these
papers tend to focus on selected areas of the
whole dataset  drawing conclusions from partial
observations and features  thus indirectly putting
arbitrary weights on the subject matters  one such
example chooses to investigate the relation
between relative earnings in the household and
the relationship stability      while the others
select subjects of gender and marital status in
order to explore the break up rates of
heterosexual couples     
potentially due to the novelty of the data and the
fact that it has only finished its budgeted five
waves very recently in       until now there
exists minimal efforts to attempt bringing all
parameters together under the roof of machine
learning algorithms  we are here to pioneer 

dataset and feature selections
i  

initial data

hcmst conducted   sequential rounds of
surveys in                        and      
respectively  the initial dataset consists of two
parts  the respondents answers to the original
questions  e g   recorded answers to question
how old are you    and features generated from
the collected raw data  e g   the categorization of
cases based upon age division   the dataset
includes      respondents with     features 
supplemented by additional    features from
wave   survey and    features from wave  
survey results 

ii  

data elimination

since this project aims at predicting the future
relationship status based on current info  we
believe that at this point only the data collected in
the first wave is relevant to our purpose  we have
hence dropped data collected from wave   to
wave    keeping only the survey results on their
relationship status at each milestone 

fithe survey assumed that some couples who
didnt respond to follow up surveys still stayed
together  for the sake of precision  however  we
only kept data of respondents with partners in the
beginning of the timespan who continuously
responded to the surveys in following periods
until wave   or breakup  based on the results
from the   follow up surveys  we generated
feature final relationship status  boolean  to
indicate the final status of couples after   years 
after deleting all the redundant features and
observations  we are left with      respondents
with     features 

iii  

preprocessing non standard data
values

the initial data contains many features with a
substantial amount of missing values  while
some bear minimal relevance to our goal  e g 
gender of the   th member in your family  and
can be dropped without significant impact  other
missing values are indicative of important
information and dropping them will result in high
bias  therefore  for those question answers that
have already been processed  we only kept their
corresponding feature  for example  for question
   how would you describe the quality of your
relationship   we dropped feature q   and
kept the corresponding processed feature
relationship quality  this will prevent
problems generated by singular matrix in future
prediction models  secondly  some of the missing
values are results of branching questions  for
example  if question   b is only required for
people who answered yes for question   a 
then feature q  b will contain lots of missing
values  for these features  we integrated them
into the main branch question by generating more
categorical classes in features like q  a 
thirdly  we dropped clearly unrelated features
with high level of missing values or nonnumerical and non categorical values  these
operations leave us with     features to work
with 

iv  

feature selection

considering that the size of our datasets after
processing stays around       we decided to
include fewer features to avoid potential
overfitting  therefore  we have implemented the
forward based sequential feature selection based

on logistic regression model with crossvalidation of    folds  features were selected
based on misclassification rate using logistic
regression model and feature selection
terminates after the misclassification error no
longer improves  this leaves us with    features 
with    most important features 
 

coresident  if the couple lives together

 

relationship quality  how the respondent
ranks relationship quality

 

s   the status of current
relationship  married  sexual or romantic
partner

 

age difference  the difference in age
between respondent and partner

 

how long ago first met  how long ago
the respondent meets his partner

 

distancemoved   mi  the distance
between hometown and current home

 

papevangelical  if the respondent identifies
as evangelical christian

 

children in hh  number of children in
household

 

how long ago first cohab  how long ago
the respondent starts to live with partner

  

ppincimp  the household income
categorized into   classes

prediction models
i  

logistic regression

we first applied logistic regression model trying
to minimize the loss function with l regularization 

 
     


 

   

 



   

 

 

   

      

   
   

 
       

  

fiany output of logistic regression is in the range
       where output smaller than     will be
categorized as   and the rest categorized as   
this method generates misclassification error of
      for train set and        for test set 

ii  

support vector machine

we implemented the the svm without kernel 
 
  b
     
 
 
   

       b            
               

then we integrated the gaussian kernel into
svm  which didnt improve the result  observe
that the train error using gaussian kernel is
substantially smaller than normal svm without
kernel  because the number of observations is
relatively small  we believe that using kernel
would further complicate the method and
therefore result in overfitting 

iii  

we generated the top down binary decision tree
by examining the optimal statistical improvement
brought about by each feature at each split  to
capture the optimal improvement  we ordered
both the categorical and the continuous attributes
from the smallest to the largest and measured the
improvement in misclassification error by
dividing at each consecutive pairs  when a
missing value is encountered  we used surrogate
split because many alternative features with high
variance can be found 
in actual prediction  we first generated the binary
decision tree with minimum branch size of   
observations and unlimited depth  and this results
in misclassification error of       for train set
and        for test set after    fold crossvalidation  as certain level of overfitting is
shown  we decided to pre prune the tree by
adding the maximum number of splits in our
decision tree  after trial and error     splits
generate the lowest test error and achieves a
relative good balance between the test set and the
train set  with misclassification error of       
and         respectively  for the convenience of
representation  we have pruned the tree to have at
most    splits  as shown below 

decision tree

many of the survey questions have sequential
correlation with each other and some features
existence are entirely based on others  i e   only
respondents who have answered yes to
question have your religion changed since    
will be asked to answer what is your religion at
      therefore we believe that the decision tree
model would be a proper representation of the set
of if then choices and would replicate the design
logic behind the survey 

despite decision tree helps to visualize the
primary features at each split  surrogate variables
may never be used in actual splitting  therefore

fiwe also calculated the variable importance trying
to capture all the highly important variables by
measuring the improvement attributable to each
variable either as a primary or a surrogate splitter 
below is a graph showing the importance of
features with non zero importance value  note
that the  nd to the  th variable doesnt appear in
splitting at all but might serve as surrogates for
coresident  also  despite that there are over   
features  the variable importance table shows that
only very few of them are decisive features and
actually have an effect in splitting 

iv  

results and analysis
at this step  we have graphed the test error and
train error regarding each method after crossvalidation  as the graph have shown  the errors
generated by all the methods range from     to
around     

naive bayes

our final dataset is left with fewer than ten
numeric values  the distance from the respondent
home to the current home  how long ago they first
met and how long ago the respondent first lives
together with partner  etc  in order to apply naive
bayes model  we have discretized the data by
converting these numeric features into several
categorical classes  because the assumption of
the naive bayes is that every conditional
probability is independent of each other  we also
calculated the covariance matrix between all the
features and dropped features with a covariance
over      we then applied the naive bayes model
trying to maximize
 

   

 u

 

            

 

as the graphs have shown  generally logistic
regression produces the best result while naive
bayes performs the worst  decision tree and
support vector machine have very close
performance in both test and train dataset 

 

            

       

because some of the survey questions have
relatively small amount of responses or
unbalanced results  we also added laplace
smoothing to ensure at least one data point per
feature per class  using the obtained probabilities
w x     w x y   x we then cross validated by
partitioning the data in    folds and obtained the
averaged misclassification error        for train
set and        for test set 

model

false
positive

false
negative

precision

logistic
regression

      

     

      

svm

      

     

      

naive bayes

      

      

      

decision tree

      

     

      

one possible reason that naive bayes doesnt
generate good precision is that some of the input
features are not completely mutually independent 
despite that we have used sequential feature

fiselection and later removed features with
correlation greater than     when using nb  some
features left are still vaguely related with each
other and this violates the basic independent
assumption of naive bayes  the correlation
between variables  however  helped to boost the
precision in decision tree model because the
missing values can be replaced with their
correlated alternatives using surrogate splits 
we believe that one explanation for the logistic
regression to generate better result than decision
tree is the high dimensionality of the dataset
compared to the number of observations  as trees
always tends to overfit in presence of high
dimensionality since it has high freedom degree 
we had to limit the number of splits to prevent
overfitting 
however 
some
important
information are lost in this process and bias is
sacrificed to obtain lower variance  logistic
regression  though very simple  does draw
information from the basis of the entire set of
features and therefore could perform better than
tree based model 
note that the false positive rate is almost about  
times as high as the false negative rate in logistic
regression  svm and decision tree  this is
partially because the initial dataset is imbalanced
with the ratio between positive data and negative
data being      we then decided to rebalance the
dataset by adding a weight vector to assign more
weights to negative classes  the table below
shows false positive and false negative results
generated using the re balanced dataset  notice
that the the precision rate doesnt stay stable for
all the four models and false positive and falsenegative rates are more balanced than before for
logistic regression  svm and decision tree 
however  the false negative rate from nb is still
substantially higher than the other three  meaning
that naive bayes model is very pessimistic about
the couples relationship  it tends the believe a
couple would break up even indeed they will very
likely not 

model

false
positive

false
negative

precision

logistic
regression

      

      

      

svm

      

      

      

naive bayes

      

      

      

decision tree

      

      

      

we also tried to apply the logistic regression and
decision model to predict the data points that have
been deleted from our dataset due to     missing
values      unknown labels  these observations
were first removed from our samples because the
respondents stopped to respond to the survey
from the second  third or fourth round 
interestingly  the pattern demonstrated by our
prediction shows that the earlier the couple stops
to respond to the survey  the more likely they will
get a   in prediction  in other words  for couples
that stop to respond to the survey since round   
our model predicts that very large portion of them
will break up in   years  this verifies our guess 
people dont just quit in the middle of the survey
for random reasons  the absence of a couples
voices in later surveys might already indicates a
deceased romance  and the pain and
embarrassment to admit this usually makes
people shun away 

conclusion
while our models express      test errors  it is
rather reasonable given that relationships are still
indeed based on one of the most complex systems
in the known universe  human mind  our results
provide insights of what otherwise remains
mysterious and unquantified  and will potentially
help sociologists and everyday individuals alike 
in process of fitting samples through the models 
we have also discovered that adding more
samples dont always result in higher accuracy 
this is likely rooted in the nature of relationships
and romance  its the surprise and unexpected
turn of events that highlight their beauty  when
consensus deems long distance relationships hard
to maintain  there are always outliers who prove
it wrong  and same goes for other difficulties in
love  for this exact reason  while increasing data
size from additional surveys will theoretically
improve our prediction  we believe that it may not
be necessary 

fireferences
    rosenfeld  michael j   reuben j  thomas 
and maja falcon         how couples meet
and stay together   stanford  ca  stanford
university libraries  waves       and  
version       wave   supplement version
      wave   supplement version     
http   data stanford edu hcmst 
    rosenfeld  michael j  and reuben j 
thomas        searching for a mate  the
rise of the
internet as a social intermediary  american
sociological review          
  

    rosenfeld  michael j        couple
longevity in the era of same sex marriage
in the united states  journal of marriage
and family            
    weisshaar  katherine        earnings
equality and relationship stability for samesex and heterosexual couples  working
paper 
    rosenfeld  michael j        who wants
the breakup  gender and breakup in
heterosexual couples  working paper 

fi