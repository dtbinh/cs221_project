 

playing chinese checkers with
reinforcement learning
cs     spring      project final report
sijun he wenjie hu hao yin
 sijunhe  huwenjie  yinh  stanford edu

abstractwe built an ai for chinese checkers
using reinforcement learning  the value of each board
state is determined via minimaxation of a tree of
depth k  while the value of each leaf is approximated
by weights and features extracted from the board 
weights are tuned via function approximation  the
performance of our modified minimax strategy with
tuned weights stands out among all the other strategies 

i  i ntroduction
chinese checkers is a game played on a
hexagram shaped board that can be played by two
to six players individually or as a team  the objective is to be the first to move all ten pieces across the
board into the opposite starting corners  as shown
in figure    the allowed moves include rolling and
hopping  rolling means simply moving one step in
any direction to an adjacent empty space  hopping
stands for jumping over an adjacent piece into a
vacant space  multiple continuous hops are allowed
in one move  a more detailed introduction of the
chinese checkers can be seen in wikipedia 
the reinforcement learning is an area of machine
learning typically formulated as markov decision
process  mdp   the model consists of states  actions  transitions  etc   which is suitable for decision
making in board game like chinese checkers  the
objective of our project is to use reinforcement
learning to build an ai agent for chinese checkers 
and to explore the effectiveness and efficiency of
the ai 
to simplify the problem  our ai only solves
the one vs one mode of chinese checkers  dif 

fig     chinese checkers rules  downloaded from website 

ferent from classic reinforcement learning where at
each state the player solves a simple maximization
problem  in our ai for chinese checkers  it is a
adversarial zero sum game  therefore  each player
needs to consider not only his her own strategy  but
also the opponents responding strategy  therefore 
a better way to depict such procedure is minimaxation  which is elaborated in section   
the biggest challenge in applying reinforcement
learning to our ai is how to learn the weights in
function approximation  we modified the standard
algorithm for function approximation so that it fits
our minimax setting  and we adopted a diminishing
learning rate to stabilize the update  our simulation
results showed that this learning procedure is effective  in that the difference between the weights

fi 

before and after an iteration is small  simulation
results also showede the robustness of our update 
in that our learning procedure with different initializations will end up with very close weights 
we tested the performance of different strategies
by playing against a random look ahead greedy
player  simulation results showed that the minimax
strategy with tuned weights significantly outperforms the minimax strategy with initial weights 
moreover  we further modified our strategy such
that it divides the game into three stages and applies different strategies thereon  simulation results
showed that this modified strategy outperforms the
basic minimax strategy 
the rest of this report is organized as the follows 
we first talk about how we implement the board
in section    then we introduce the basic methodology of our ai in section    and point out the
difficulties in implementation as well as our solution
in section    we cover our modified strategy in
section    simulation results are shown in section
  
ii  b oard r epresentation
figure   is the starting board where we worked
on in minimax searching  weights tuning  and simulations  each o stands for a vacant spot    stands
for a spot occupied by player  s piece  and   stands
for a spot occupied by player  s piece 

fig     board representation

note that this board is smaller than the original
board of chinese checkers  the state space com 

plexity of chinese checker      is high      thus
building and testing the ai for the full game board
is computationally intensive  thus we adopted a
smaller board of   pieces for each player during development and simulation  furthermore  the
hexagram shaped board was modified into a heuristic diamond shaped board  which is reasonable for
one vs one mode 
iii  m ethodology
we adopted the classical approach for game
playing ai  game search tree  which best mimics
the behavior of a human player while demonstrates
super human performance by taking advantage of
the computing power  with each node representing
a board state of the game  and each edge representing one possible move from one board state to a
subsequent board state  the game search tree can
emulate the thinking process of a human player 
chinese checkers is a two player zero sum game 
thus an objective value is needed to evaluate
the situation on the board  player  s goal is to
maximize the value  while player   minimizes it 
the logical approach is the minimax tree  which is
a decision tree that minimizes the possible loss for
a worst case scenario resulted from the opponents
optimal move 
due to the large state space complexity of chinese checker  it is unrealistic to build a top down
game search tree  instead  a shallow k depth minimax game tree that searches only the tip of the
tree is built  at each node  the value is taken
as the minimax score which is computed by the
minimax algorithm of depth k  when the search
has reached the bottom of the k depth search tree 
a k a  the leaves  the score is approximated by a raw
score  which is a linear evaluation function based
on the features of the board state  we exploited  
features that are based on the positions of pieces on
the board  which are described as the following 
 ai   the squared sum of the distances to the
destination corner for all pieces of player i 
 bi   the squared sum of distances to the vertical
central line for all pieces of player i 
 ci   the sum of maximum vertical advance for
all pieces of player i 

fi 

with i         note that we used the square of
the distances to penalize the trailing pieces  which
would motivate each player to make the pieces
cohesively and thus promote hopping  besides  the
  features are extracted from a larger amount of
possible features  so that the overfitting of the model
was avoided  the other features we have explored
and found unnecessary included 
 the horizontal variance  how scattered  of
pieces of each player 
 the vertical variance of pieces of each player 
 the maximum vertical advance for a single
piece of each player 
the evaluation function is the form of
ve   w   a   a      w   b   b      w   c   c   
where the weights w    w    w    w   t would be
trained via function approximation  which would be
described in the following section 
iv  c hallenges and solutions
a  weights tuning and function approximation
the performance of the ai is highly dependent
on how well the weights in the evaluation function
is tuned  the objective of weights tuning is to allow
the evaluation function to consistently approximate
the minimax value through the depth k tree search 
the challenge is to develop an algorithm to improve
and stabilize the weights within certain iterations 
the following algorithm   based on function
approximation is our solution to perform weights
tuning  an introduction of function approximation
can be seen in      the basic idea of the algorithm
is to conduct value iteration after each game played
by both ai players  a new weight is computed
by performing least squares on the recorded feature vectors at each turn and their corresponding
minimax scores  a diminishing learning rate  is
imposed at each iteration  in order to stabilize the
update  the iteration is repeated until the weights
value are stabilized 
b  run time complexity and alpha beta pruning
another challenge is the run time complexity of
the algorithm  due to the nature of chinese checkers  players usually have around         feasible

algorithm   weights tuning
   initialize a weights w 
   repeat
  
play one game with both player following
minimax rule using weights w  record the
feature vectors at each turn in a matrix 
and the corresponding minimax scores in a
vector v 
  
wnew  leastsquare   v  
  
w  w    wnew  w 
   until stabilized

moves at a typical turn  the worst case number of
board states evaluated in a minimax tree of search
depth   is on the scale of       the large branching
factor  combined with any non trivial search depth 
can easily result in impractical run time for realtime game play 
we address this problem by adopting alpha beta
pruning  a detailed description of this technique can
be seen in      furthermore  in order to expedite
the alpha beta pruning  we enqueued all the feasible
moves of the player in a priority queue where the
priority is in the order of the weight calculated score
of the resulting board if the corresponding move is
taken  the effectiveness of the alpha beta pruning is
shown below in table i  where we listed the time
used as well as the number of nodes visited for
the starting board to compute the minimax value  it
shows that the time used as well as the total number
of nodes visited is significantly reduced 
depths
 
 
 

w o
    
     
    

time  s 
w  pruning
    
    
     

nodes visited
w o
w  pruning
   
  
    
   
     
   

table i
t ime used and number of nodes visited in computing
the m inimax value

v  f urther d evelopment
in the strategy described above  we adopted minimax tree search at each turn of playing  note that
there is a waste of computing power in this strategy

fi 

at the beginning and end of game  at the beginning
of game  the pieces of two players have not interact
with each other  thus each players move does not
interfere the others  therefore  it is unnecessary to
consider the opponents strategy  thus the minimax
strategy can be simplified as a pure maximizing
strategy  this is also true at the endgame  when
the pieces of two players are split up  thus each
player only needs to consider how to end the
game as soon as possible  without considering the
opponents strategy 
in the light of this knowledge  we modified our
strategy above in the following way  in the start
game  the player only consider his her feasible
moves and chooses the one that gives the maximal
weights calculated value  in the midgame when two
players pieces intersect  we search our optimal
move via the minimax procedure  in the endgame 
the player would take a move that achieves maximal
vertical advance 
we will test the performance of this strategy and
compare it with the basic strategy in section   b    
vi  r esults
a  convergence of weight tuning
with algorithm    we were able to tune and
stabilize the weights  the criterion for stabilization
is wnew  w  as shown in the plot  the difference
of weights diminished as more training games were
played  while the algorithm didnt necessarily converge  it did improve the effectiveness of the ai
dramatically after weights were tuned  which will
be shown in section   b    
furthermore  the algorithm is also robust to
initializations  two different unit length initial
weights were attempted                       t and
                     t   and both of them converged
to the similar values after weight tuning  the tuned
weights from different search depth are shown
below 
w depth            

     

      

w depth            

     

      

fig     convergence of weight tuning

b  benchmarking
we measured the performance of our algorithms
by simulating     games against a benchmark strategy  the benchmark is a greedy random look ahead
algorithm that takes the move that gives the most
combined vertical advance in   steps  tiebreaker
is preference to trailing pieces and further ties
are broken by random selection  the result was
measured by winning steps  which is the number
of steps needed for the losing player to finish the
game 
   effects of weights  figure   demonstrates the
game results of ai players with tuned weights
and untuned weights  the untuned weights are the
initial weights                      t   the results
show that the ai with tuned weights performs
significantly better than one with untuned weights 
which is expected 
   effects of search depth  figure   shows the
game results of ai players with search depth  
and    as expected  the ai with search depth  
outperforms the same strategy with search depth
   it is worth noting that the worst case runtime
for depth   strategy is under   seconds  while the
worst case runtime for depth   strategy is over    
seconds 
   effects of modified strategy  figure   compares the game results of the basic minimax strategy
and the modified strategy  the modified minimax
strategy improves the performance tremendously for
depth    while theres no significant improvement
for depth    the reason is that the endgame algo 

fi 

rithm in the modified strategy is comparable to the
minimax algorithm with a search depth    though
theres no improvement in terms of winning steps 
the modified strategy is orders of magnitude faster
in terms of time complexity  reducing the runtime
for an average of     seconds to less than   
seconds 

fig     effects of weights

fig     effects of modified strategy

r eferences
    bell  george i        the shortest game
checkers and related problems  integers     
    liang  percy      a  lecture    mdps ii 
stanford edu class cs    lectures mdp  pdf 
    liang  percy      b  lecture    games i 
stanford edu class cs    lectures games  pdf 

fig     effects of search depth

of chinese
     
http   web 
http   web 

fi