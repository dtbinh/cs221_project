prediction of stock price movements using options data
charmaine chia
cchia stanford edu

abstract this study investigates the relationship
between time series data of a daily stock returns and
features describing the options market based upon the
underlying stock  linear regression was found to be a
poor model for predicting a given days return from
returns and options features of the past two days  an
alternative approach was attempted by smoothing the
returns data using a    day exponential moving average
 ema   pre processing selected options features  and
approaching the data as a classification problem  decision
stumps boosting with    fold cv was applied to different
sets of features to predict ema returns  with        being
the lowest average training error achieved  while the
ema return of the previous day was by far the most
predictive feature  useful signal was also found in the
options related features  finally  a new approach to the
original regression problem was attempted using the
boosting margin as the independent variable  this gave a
mse comparable to the best linear regression
performance  and a classification error rate slightly better
than that achieved through applying decision stumps
boosting to the raw returns data 

i  introduction
signal detection in finance remains a difficult topic in
machine learning  especially for practical applications
like price prediction  successive points in a time series
are not necessarily independent and identically
distributed  so predictions of a dependent variables
future value need to take into account past values as well
as independent variables  furthermore  financial asset
returns often non normal and display non ergodic
patterns  which can lead to overfitting when standard
assumptions are applied  the signals which are easier to
detect often are useless as markets drive most to
equilibrium price  such that trading on them becomes
unprofitable  often  signal is drawn from data about the
underlying assets  for mortgages  we look at
characteristics of borrowers  for companies  debt to
equity ratio  hundreds of analysts are paid to develop
theories about individual companies and trade on them 
with this in mind  i attempt to use options data to
predict stock returns  an option is a contract sold by one
party to another  offering the buyer the right to buy or
sell an underlying asset at an agreed upon price during a
certain period of time  the right to sell is known as a
call and the right to buy a put  the agreed upon price
is the strike  k   to be distinguished from the price of

the options contract itself  v   options can be thought of
as bets on the underlying stock price at a given point in
the future  the intuition behind this study is that certain
aspects of options market behavior could reflect
movements of  informed investors  relevant options
features relate to the puts and calls traded and implied
volatility  implied volatility is the perceived future
volatility of the underlyinga key input into options
pricing models  most famously  black scholes 
determining what contracts are worth 
ii  data
the data analyzed for this report consists of the stock
prices and options data of    healthcare companies  over
the period from          to            from the
stocks data  time series of returns was calculated for
each company using the formula 
  

      
  

the returns time series can be smoothed to capture
the broader trends in stock behavior  this can be done
by taking the simple moving average  sma  over a given
interval of days  the exponential moving average  ema 
where the later days in interval are given more weight 
and a gaussian moving average  gma  where the days
in the middle of the interval have highest weight 
the raw options data comprises    features relating
to the volume of call and put contracts traded each day
and various parameters derived from black scholes  the
data was further split into total  at the money  in themoney or on the money contracts  and the relative price
of puts to calls  characterized by the put call parity
deviation  pcpdev   it also includes variables associated
with the implied volatility  its spread and skew  adding
up to a total of    features  given the large number of
potential predictors  many of them highly correlated
with each other  the question we seek to answer is 
which features are most important for predicting the
future price of the underlying stock  or equivalently 
future returns   and what is the best machine learning
model for doing this 
iii  linear regression
as a prelude to building the learning model  the data
was studied through visual plots to get a sense for any
obvious correlations between the returns data  the
outcome variable y  and individual options features  xk 
k           both the returns and options data were

fismoothed by applying a    day simple moving average 
scatter plots of both raw and smoothed data were
generated  treating each day as a separate data point
 y t   xk t    this confirmed that the data is was not really
normal  with larger tails  however no strong correlation
was immediately observable with any of the features 
figure   shows the example of returns vs put volume 
for company aet 

   day sma for returns

 a 

 b 

smoothed put volume

fig      a  histogram    b  intensity map of sma returns vs put volume

to ascertain if highest cross correlation between the
outcome and independent variable time series occurred
at a lag other than tlag      cross correlation plots were
generated for raw and gma smoothed returns and
returns rolling variance versus all    options feature
time series  the idea is that if temporal trends in the
options data do indeed forecast trends in returns  the
greatest effect might only occur after a few days  and it
would be important to capture this in the regression
model  an example is shown in figure   

interpretation of the plots was not clear  nonetheless
based on the average result  we build a rudimentary
linear model that regresses y t  against y t     y t     x t    
x t   a total of    independent variables  this attempts
to capture the dependence on a given days return on
past days performance and options features  a good
result was not expected as far more factors determine
the returns of a stock  but this would potentially be
illuminating as to what variables can be dropped  elastic
net with        was used  with regularization helping to
root out potentially misleading predictors  note
however the difficulty of comparing the relative
importance of different features based solely on the
magnitude of coefficients found  as the independent
variables are not standardized  though that comes with
problems of its own   figure   shows the results of the
regression on one company  figure   is a scatter plot of
the results for all    companies  summarizing the
maximum coefficients  at min  for all    features and an
intercept term  for the outcome variable of  a  raw and
 b  smoothed returns  the green stripes highlight the
variables which do not have consistently zero  or close
to zero  coefficients  these include the previous days
returns  implied volatility related features  and pcpdevrelated features  however  a wide range of coefficient
values were found  sometimes of opposite polarity  if we
assume that a given feature should have a roughly
consistent effect on the returns  we would expect its
coefficients to have similar values even across different
companies  as such  it would seem like linear regression
on the unprocessed features is not particularly useful or
accurate for modeling returns 

 a 

fig     elastic net results for y t    y t     y t     x t     x t   

 b 

fig    cross correlation of returns and implied volatility spread  for  a 
raw    b  smoothed time series

in general the highest cross correlations were seen at
lags of   to   days  data smoothing was found to
significantly damp out correlation peaks and shift the
position of turning points  unfortunately  the exact

fig     scatter plot of regression coefficients of    independent
variables    intercept  for    companies

fiiv  classification
given the limited success of linear regression  it is
worth checking if the problem can be simplified to a
classification one  where we attempt to find signal in the
features to predict if returns on a given day will be
positive or negative  secondly  we ask if more predictive
features can be found by processing the features from
the raw data  finally  we look beyond linear hypotheses
for predicting outcomes 
one indicator that traders have used to gauge market
direction is the put to call ratio  or pcr  this is obtained
by dividing the volume of puts traded by calls traded on
a given day  typically  traders buy stock options to hedge
their underlying equity positions  lending credence to
the notion that pcr might indicate market sentiment 
which in turn might predict market performance  figure
  shows historical data from jan      to may      for
chicago board options exchange pcr  equity only 
values against the s p     closing prices  the dotted
lines indicate that an increase in pcr values was
followed by declines in the s p      and vice versa 

 d scatter plots of returns two other options features
on a given day were made  and the points color coded
according to whether the returns on the next day were
positive or negative  figure   shows scatter plots of the
returns  pcr fractional change and pcpdev  separated
according to the labels of  a  raw returns   b     day
ema returns  it is clear that the space occupied the
points with each label almost entirely overlap in the case
of raw returns due to their noisy nature  while there is
some separation  though still considerable overlap 
when the labels depend on the smoothed returns 
linear  quadratic and rbf kernels were used to separate
the labels using svm  with the number of iterations set
to        no convergence was found under the default
settings  but by allowing the kkt violation level to be
increased to     in the linear and quadratic kernels 
and     for the rbf kernel  classification boundaries as
shown in fig   bii were obtained  while this indicates
some utility of the method  svm with these kernels is still
not ideal for our data due to high overlap 
an approach based on thresholding both the pcr and
the fractional change in pcr was next attempted  this is
based on the idea that correlations in movement of the
market and pcr happen mainly when the pcr breaks
above or below certain levels that indicate whether the
market is bullish or bearish  this can be seen in fig    
where the  b  and  c  show different thresholds applied 
 a 

fig     cboe pcr and s p     time series

whether or not the pcr for a specific stock predicts its
performance is a slightly different case  to better
visualize changing trends in pcr and returns  a    day
exponential moving average  ema  was applied to both
time series  further  since market movement is signaled
by changes in pcr  the daily fractional change in pcr was
calculated using a similar formula as that for returns 

 b 

 c 

fig     time series of returns  red  and pcr fractional change  blue 

 a 

 b 

fig     scatter plots of  a  raw and  b  ema smoothed returns at day  t  vs returns  pcr fractional change   pcpdev at day  t   

fiv  decision stumps boosting
the use of thresholds suggests the non linear
classification by decision stumps  essentially a one level
decision tree which predicts an outcome based on 
             

     
   

individual stumps based on single features are
however unlikely to give much better results than
chance  the algorithm can be called a weak learner  and
we look for some way of combining multiple weak
hypotheses to build a much strong classifier 
the ensemble learning method we implement is
adaptive boosting  adaboost   for which its inventors
schapire and freund won the godel prize in      
adaboost takes as inputs a weak learner algorithm and
a distribution of probabilities p i  over the training data 
it iterates over the hypothesis space of the learner 
choosing the hypothesis j x  giving the lowest
prediction error on the weighted training data  with
each iteration  p i  are updated to emphasize examples
that were wrongly classified  the weights j on the t
learners chosen up till current iteration t are updated
via coordinate descent to minimize 
    


 
                

  

after t iterations  the model is based on the weighted
sum of predictions of the t learners chosen 
                    
in this case  the weak learner is decision stumps  and
the hypothesis space includes all possible features and
thresholds for each feature  the following features were
included in the feature space  taking into account the
observations from the previous sections 
a 
b 
c 
d 
e 

ema returns  days t    t  
ema put to call ratio  days t    t    t  
put call parity deviance  days t    t  
implied volatility  days t    t  
implied volatility spread   skew  days t    t  

the outcome variable predicted is the t    day ema
return  which can be obtained recursively after
initializing the very first interval ema   using 
   

 
 
       
 
  
  

note how we are able to recover a predicted raw value
of the return for each day  rt  once we predict emat    fold cross validation was performed  where data from  
out of the    companies was set aside as the test set each
time  the number of boosting iterations t was chosen to
be      this was repeated for several different
combinations of features  with the aim of finding out
how predictive different features are  the results and
error plots obtained from the experiments are
summarized in the results section 

vi  results
table   summarizes the average test error rate  over
   fold cv  after     iterations  from the experiments
for   different choices of feature space  the sets of
features included are indexed a to e  described at the
end of the last section  
returns
only

b  c

d  e

b  c  d  e

with a

     

     

     

     

w o a

     
 raw 

     

     

     

table    results for boosting with different feature sets

based on the frequency and priority with which
certain features were selected by adaboost  we can infer
how much useful signal for predicting the ema returns
they contain  the top few features appear to be 
  
  
  

ema returns
mpiv spread  civ spread
cpcpdev  mppcpdev

  and   both relate to differences between calls and
putsthe difference in implied volatility in the case of   
and contract price in the case of    as such its not
surprising that they contain information about the
directionality of the underlying  the prefix c and mp
refer to different methods of calculating the each feature 
from the table  we see that the including past ema
returns improves the prediction error dramatically to
        this is not surprising given that we would
expect a given days return to depend a lot on the most
recent trend  especially after random fluctuations have
been smoothed to some degree  as comparison  the
same boosting algorithm was also applied to predicting
raw returns  from both raw and smooth returns   and
the error rate averaged       basically not much
better than random  see bottom left cell in table    
interestingly  adding feature sets b  c  d   e did not
improve prediction performance once returns were
included as a variable  in fact  from the learning curves
in figure  a c  we see that not much learning goes on
after the first iteration  when ema returns  feature set
a  were not included in the hypothesis space  however 
b  c  d  e still give error rates significantly better than
chance performance  this indicates that these do contain
information that predicts the smoothed returns  figure
 a  f show the learning curves over     iterations for
the training and test sets for    fold cv  which were
summarized in table   
having shown some success in studying the data as a
classification problem  and ascertained which features
are most significant  we return to the original regression
problem of predicting the magnitude of the return  one
approach is to build a hierarchical model  where the first

fifig     adaboost learning curves for   different sets of features over     iterations     fold cv  blue  training error  black  test error

step predicts the direction of the return and the second
step predicts its magnitude  given that the first step was
reasonably accurate   regression trees and svm
regression are two methods that could be applied to this 
here  i briefly suggest another method that builds on
the results from adaboost  the idea is to use the margin
based on which the outcome prediction     was made
as the independent variable in a linear regression to
predict the magnitude of the returns  that is 


    
  

        

this would only work if a more positive boosting
margin  which we would interpret as a higher
probability of a    label  also correlates with larger
positive magnitude  and vice versa 
to see how feasible this is  the ema returns at time t
were plotted against the un normalized boosting
margin  as seen in fig     evidently  there is a lot of
variance about the mean  nevertheless at the extreme
ends of the plot  larger absolute boosting margins do
seem to predict larger ema returns  the mean y value
for each point along the x axis can approximately be
fitted with a  rd order polynomial as indicated by the red
regression line  this can be thought of as the expected
value of the ema return given that the boosting model
chosen is accurate  the distribution of points about each
value in x can also be further analyzed to obtain the
variance given x 

as a sanity check  we attempt to convert the predicted
ema return back into a raw return described at the end
of section v  the mean squared error of this prediction
over the whole dataset was then calculated by
comparing it to the actual return values  this came up to
    e     as a benchmark  an elastic net regression with
       has mse ranging from     e    to     e    as
the l    norm constraint is tightened  in other words 
this method of regressing on the boosting margin does
not seem to be significantly more inaccurate than the
most accurate elastic net regression  finally  these raw
predicted returns were converted to binary labels and
compared with the actual returns labels  the error rate
was      lower than the       obtained by
prediction using boosting directly on the raw data  while
the regression model clearly needs more work and
rigorous testing  this is a promising start 
vii  conclusion
future work could focus on refining the classification
model to improve performance  for example by
incorporating local weighting into the probability
distribution assigned to the data in adaboost  another
possibility is to use multi level decision trees as the base
weak learning algorithm  instead of just decision stumps 
going beyond methodology  the feature space could
also be expanded to combine the information present in
options data with other variables that are known to be
relevant  ema smoothing could be tried over different
intervals of time to find the optimal length  models could
be built attempting to forecast returns further into the
future than just one or two days ahead 
viii  acknowledgements

fig     ema returns vs adaboost margin  red   rd order regression line

i am very grateful to steven glinert for
proposing the original research question  patiently
explaining finance concepts that i was new to 
helping acquire the data set used  and providing
invaluable advice over the course of the project 

fi