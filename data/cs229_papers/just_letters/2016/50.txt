visual attention models of object counting
jack lindsey  stanford university  jacklindsey stanford edu
steven jiang  stanford university  syjiang stanford edu
abstract we develop a sequential learning model
using a recurrent neural network architecture and reinforcement learning to recognize and count objects in
images  simple feedforward neural networks perform
well on this task when trained using backpropagation 
however  convolutional neural networks are computationally expensive and results are less certain when
the image input has imperfect resolution outside of the
focus area  like input to the human visual system   we
build an architecture based on visual attention models
implemented in the literature  such as by mnih et  al  
applied to the task of counting objects in images  our
model scans an image for distinguishing features based
on models of the retina  layering blurriness on the image
to simulate focus on a particular area of the image  our
recurrent neural network sequentially incorporates the
data from inputs to produce an accurate count of the
objects in the entire image  and reinforcement learning
dictates glimpse locations for each successive iterative
step  we achieve accuracy of      percent  and we prove
that the glimpse model tracks discrete object figures in
images through the relationship between the number of
glimpses required to yield accurate estimates and the
number of objects present 

i  introduction and related work
feedforward convolutional neural networks have
achieved some success when applied to problems of image recognition and object discrimination  as it stands 
convolutional neural networks have made impressive
gains in traditional image recognition tasks  such as
telling objects apart and classifying images based on
familiar features from training data 
however  several problems persist with convolutional
neural networks on more general image recognition
tasks  firstly  convolutional neural networks are computationally expensive to train  and they require large
amounts of manually labeled data to train on  usually
involving tens of thousands of manually labeled images and several gpu cores to train to convergence 
however  models of human visual attention from neuroscience literature suggest that human eyes scan and
glimpse throughout an image     fixating on significant
 this paper is not sponsored by any organization 

features or objects that are distinguished from the
background or other less significant features  recurrent
learning algorithms based on this type of visual attention offer potentially significant optimization above
implementing vanilla recurrent neural networks  this
biologically inspired approach allows the algorithm to
integrate information obtained from processing smaller
and more salient focus points within a large image 
ideally making the training and prediction processes
more efficient 
recurrent neural networks can be used to implement
this attention model by iterating through the internal
network states in a fashion parallel to image processing in the brain when humans initially encounter
and recognize images  in particular  the problem of
object counting has mostly been approached from the
perspective of convolutional neural networks  and it
presents a different set of challenges than traditional
image recognition  we build upon previous recurrent
attention model  ram  efforts and apply the architecture  equipped with reinforcement learning  to object
counting  we believe that sequentially incorporating
data from multiple glimpses could outperform a feedforward approach in cases where objects are sparse or
clustered  or otherwise unevenly distributed  our goals
are to compare the efficacy of two different approaches
to the counting problem and analyze the performance
of the attention model in more detail  in particular  we
evaluate neural networks abilities to closely replicate
the pattern of retinal glimpses human eyes take upon
encountering images with multiple objects 
ii  methods
a  data and general design
training and testing images were generated using
simcep      a publicly available tool that synthesizes
realistic portrayals of arbitrary numbers of randomly
placed biological cells  the tool was developed
by lehmussola et  al  based upon real datasets of
fluorescent cell microscopy data  this particular
dataset presents several advantages for the task of
object counting  first  the cells produced are similar

fienough in appearance to allow decent performance
by a trained network  but they vary in shape enough
to prevent the network from simply deploying naive
methods to maximize the reward function  like
integrating the total mass of non background material
in an image  second  the images are vary significantly
in color and resolution  well representing many of the
core features of realistic images  we generated a total
of five thousand    x    pixel images  one thousand
in each count class for counts ranging from one to
five  the first eight hundred in each class for used for
training  and the rest for testing  the images in our
dataset are all uniformly sized  we choose to represent
the counting problem as a classification problem 
where probabilities are assigned to each of five classes
and a prediction is made by choosing the count class
with the highest probability in the distribution  we
train the network by comparing predictions with each
images true object count  we choose count classes
from one to five to optimize our evaluation of the
effectiveness of the glimpse model in the ram 
creating a clear distinction in how the glimpse network
performs on images with few objects and images with
a greater number of objects 
b  convolutional feedforward network

with     resolution  this simulates the reduced quality
of vision in the human retina away from a fixation
point  the last layer is the full image blurred to     
resolution  these three layers are combined into three  
x n x n inputs to the ram  where n is the patch size
 each pixel has dimension    corresponding to rgb
values  
the area of high focus is most helpful for counting 
while the low resolution windows allow the program
to only make judgments about where to place the
focus in the next time step of the network processing 
this replicates the parallel processing of input to the
retina while discerning and counting objects in a scene 
being vaguely aware of objects in the background while
sharply focused on discrete objects in a given location 

fig     a representation of the input to the recurrent network 
here  three windows of decreasing size and increasing resolution
are shown 

first  we implement a feedforward network with
convolution layers  the network has two spatial convolution layers with max pooling to reduce the dimensionality of the input  followed by three standard
linear hidden layers with nonlinear activation functions 
the network is representative of feedforward networks
commonly deployed for many applications 
c  recurrent attention model
second  we train a model that sequentially chooses
a predetermined number of glimpse locations within
each image and uses the images resulting from these
fixations as the sequential inputs to a recurrent neural
network 
   simulating visual focus  we model visual focus
by giving the network access to several small  concentric windows that progressively decrease in size and
increase in resolution  sequential inputs take the form
of   different    x    images layered on each other  see
fig     where these   images are placed side by side for
comparison   the first layer is a   x   window around
the glimpse location  the second layer is a larger   
x    window around the glimpse location  distorted

fig     the original image from which the above windows were
obtained 

fi   controlling focus trajectory  we implement an
algorithm that can choose along what path the algorithm glimpses to recognize discrete objects within
the attention window  previous approaches have included learning policies using reinforcement learning to
plot glimpse paths      our network uses essentially the
same approach  learning a policy for glimpse locations
based on a simple reward function for a given image 
described by
r     y  t    c 

where y  t  is the prediction for an image on iteration
t  and c is the true object count for an image  the
network outputs both the next location based on the
glimpse network and a guess to the object count at
every iteration of the network  the new location is used
in the next iteration to produce the new input to the
network 
   integrating information from sequential perspectives  we choose the recurrent neural network as the
base architecture because of its ability to accept arbitrarily long sequences of input  traditional feedforward
networks lack this capability  in our training model 
we input the images in certain sequences  the neural
network accepts these sequences and incorporates them
into a global visual understanding of the image from
multiple glimpses rather than taking in all the data at
once 

fig     a simplified depiction of the recurrent model used  at
each time step the network outputs an estimate of the object count
and an  x y  coordinate pair specifying the next glimpse location 
which is used along with the input image to produce the series of
focus windows described elsewhere in this paper 

reinforcement training on the feedforward network described above  it should be noted that only limited time
was spend optimizing the parameters of this model 

d  unifying the model

b  attention model

the aforementioned considerations   deciding where
to focus and how to integrate the information obtained
from these focus points   are integrated into a single network architecture  the model has two central
components  the processing component that mixes the
input image sequence and the glimpse location  and the
recurrent component  the recurrent component mixes
the image input and the internal network representation
at each iteration  updating the internal network representation at each iteration  the network sequentially
integrates the information from each iteration into a
final prediction for the object count of an image 

after training the attention model  using three concentric resolution windows and seven total glimpses 
for eight minutes  and obtained a final accuracy on the
test set of      percent  this value is a reflection of
numerous successful trials  however  it should be noted
that on some trials  the recurrent models accuracy
remained flat at    percent  no better than random  
this is a result of the ram glimpsing at locations from
which it can glean no substantial information 

iii  p erformance

an issue that has received relatively little treatment
in the literature is the extent to which the recurrent
attention model successfully learns to focus on the most
salient areas of the image  our decision to apply the
model to object counting arose in part because this

a  convolutional feedforward network
preliminary testing yielded a      percent accuracy
rate after eight minutes of stochastic gradient descent

c  comparison of models
iv  e valuating the f ocus m echanism

fifig     comparison of performance of the feedforward network
and attention model  controlling for training time 

task lends itself well to rigorous evaluation of the algorithms decisions about where to focus  we tested this
behavior as follows  first  we trained the network to
estimate counts after seven glimpses  during the testing
phase  however  we limited it to n glimpses  with n
ranging from   to    inclusive  as a hyperparameter for
the network  in doing so  we could analyze the models
intermediate estimates of the object count after only a
few glimpses and evaluate how each additional glimpse
affects predictive power for each count class 
after training and evaluation  the network produces
the clear result that images with fewer objects  e g 
one or two  were correctly classified even when the
model was restricted to fewer glimpses  images with
more objects required more glimpses to be classified
accurately  there exists a close correlation between
the number of glimpses a network is allowed and the
number of objects that it must count in the image  this
suggests that the network succeeds in choosing glimpse
locations that closely correspond to an object in the
image 
as a control to demonstrate that this effect was in
fact do to intelligent glimpse decisions  we repeated this
test  but restricting the input to the network to only the
smallest  highest resolution window  in other words  we
restricted the models field of view  removing its access
to low resolution information about the entire image  in
this implementation  performance even on low count
images improved with more glimpses  indicating that
the glimpse locations were being decided more or less
randomly and therefore the program benefited from
being allowed as many as possible  this result indicates
that the models access to a low resolution version
of parts of the image outside its focus window was
allowing it to choose salient points to focus on 

fig     demonstration of the efficacy of the glimpse location
decision mechanism  lower count images after only a few glimpses 
while higher count images could not  indicating that glimpses were
indeed targeted at the relevant areas of the image  i e  the locations
of the objects  after sufficient training

fig     repeated version of the previous experiment  but with
the models field of view restricted entirely to the focus window 
the effect observed in the original version was not replicated here 
indicated that the low resolution peripheral image data was indeed
enabling proper functioning of the attention mechanism

v  discussion
the ram was clearly outperformed by a more
traditional convolutional neural network when trained
for the same amount of time  and it was unclear
whether the performance of the recurrent network
would have improved further given additional training
time  that said  we have reasons to be optimistic
about the recurrent attention model  first  allowing
the model additional glimpses would likely  given the
trends we observed  result in significantly improved
accuracy  second  other parameter adjustments  e g  the
number or size of focus windows  or other aspects of
the network architecture  could optimize the algorithm
further  such models have not received nearly as much
study as feedforward networks  and thus parameter
optimization has not been explored as fully 
the ram has advantages that make it worth studying further  once trained  it can classify images more
rapidly since it processes less data  furthermore  it
more accurately models human perception  making it a

fivaluable model for cognitive scientists  such a model
has applications outside of object counting  as the
concept of integrating a series of incomplete snapshots
could in principle be applied to processing of text 
speech  or a number of other tasks 
even with our current model  however  we successfully demonstrated the viability of a recurrent model
for object counting  most significantly  we have shown
the effectiveness of the attention mechanism  proving
that the success of these attention models is in fact due
to correct glimpse behavior  not simply good predictive
capacity given limited information  we believe this is
an important result  as it allows one to aim for symbiotic training of the attention mechanism and prediction
mechanism  though the interrelated nature of these two
parts of the algorithm gives the model its power  it
also has pitfalls  we believe that the occasional nobetter than random accuracy of the model after training
was due to a chicken and egg effect  where the model
could not learn how to use glimpse information before
learning how to glimpse properly  and vice versa 
introducing some stochasticity into the process could
alleviate this issue  but more study is needed 
vi  d irections for f uture r esearch
we identified the largest source of inconsistency in
the networks output as dependent upon the choice of
glimpse hyperparameter  the current architecture of
the network depends upon a preset fixed number of
glimpses for the network to take before it stops trying to
improve predictive accuracy  our specific improvement
in the model is to implement a network that trains itself
on how many glimpses to take for each image 
first  we will insert a mechanism that if the ram
glimpses at a location with no information  then it
randomly glimpses somewhere else and restarts the
iterative process 
second  we would thus modify the reward function
in the reinforcement learning to not only reward the
network for correctly identifying object counts but
also continuing glimpsing until the network is able
to  within a threshold of accuracy  identify this correct
object count  this reward function would take the gradient of the probability given to the correct count class
as the network keeps glimpsing and stop the network
from further glimpsing once the gradient falls below a
certain threshold  meaning that the network has become
confident in the correct count for an image  this more
closely replicates the motion of the human retina   
the human eye does not stop glimpsing around a scene

after a fixed number of glimpses  but rather  it keeps
on scanning an image for prominent locations until it
is confident in the information it has collected about a
scene 
third  we would apply our network to non image
data to demonstrate the versatility of a recurrent attention approach to other sensory input  such as  potentially  sound  tactile input  natural language understanding and translation 
vii  acknowledgments
we would like to thank professor james mcclelland
and steven hansen in stanfords department of psychology for the initial inspiration behind the research
and their tremendous support and guidance in giving
us the resources and references for the work  we also
are grateful to nicholas leonard for providing open
source code that aided us in implementing the recurrent
attention architecture 
r eferences
    volodymyr mnih  nicolas heess  alex graves  koray
kavukcuoglu  recurrent models of visual attention nips
     
    a  lehmussola  p  ruusuvuori  j  selinummi  h  huttunen 
and o  yli harja  computational frameworkfor simulating
uorescence microscope images with cell populations  ieee
trans  med  imaging                       
    williams  ronald j   simple statistical gradient following algorithms for connectionist reinforcement learning machine
learning                       
    leonard  nicholas  recurrent model of visual attention 
torch documentation      
    bogdan alexe  nicolas heess  yee whye teh  and vittorio
ferrari  searching for objects driven by context  in nips 
     
    misha denil  loris bazzani  hugo larochelle  and nando de
freitas  learning where to attend with deep architectures for
image tracking  neural computation                       
    antonio torralba  aude oliva  monica s castelhano  and
john m henderson  contextual guidance of eye movements
and attention in real world scenes  the role of global features
in object search  psychol rev  pages              
    stefan mathe and cristian sminchisescu  action from still
image dataset and inverse optimal control to learn task specific
visual scanpaths  in nips       

fi