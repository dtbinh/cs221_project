predicting stock prices and analyst
recommendations
saumitra thakur

theo vadpey

sandeep ayyar

suid          
stanford university
sthakur  stanford edu

suid          
stanford university
tvadpey stanford edu

suid          
stanford university
ayyars stanford edu

i  i ntroduction  
since the mid     s  virtually all financial trades are executed via computers  much of that trading occurs algorithmically  with computers executing purchases and sales in
response to a market made heavily of other algorithmic traders 
 
about    percent of all equity transactions in the united
states now are done via high frequency  algorithmic  trading    as a consequence  market behavior has fundamentally
changed  in several occurrences in the past decade  markets
experienced flash crashes where one algorithm making a
big sale triggered a waterfall of other algorithms to respond
similarly and the market tumbled to a fraction of its value
within seconds   
the rise in computerized trading has been accompanied with
increasing efforts to use machine learning to predict future
market behavior  previous research has focused on forecasting
movements in price  either as a continuous price or in discrete
intervals  or forecasting decisions to buy or sell  these studies
have met with mixed results  for both practical and theoretical
reasons  the price of traded equities remains difficult to predict 
with the task described as difficult if not impossible by
some  this is due both to the large number of possible features
that could be used to train an algorithm as well as the lack
of consensus on what  theoretically  ought to underpin the
valuation of a security   
researchers have tried a range of techniques  including
supervised learning techniques  artificial neural nets  backpropagation networks  hybrid kohonen self organizing maps 
and other methods    several prior projects in this course have
examined financial trading as well 
although computerized trading has left its mark on equities
markets  much of equity transaction today still occurs based
on value judgments by humans  these judgments  in turn 
often reflect the sentiment of professional equity analysts
employed by financial institutions like goldman sachs  indeed 
many computer scientists have shifted their focus away from
forecasting prices or other stock technical properties and
instead focusing on forecasting perceptions  
a large number of traders conduct market research through
bloomberg terminals  bloomberg is the market leader in
distributing financial data   bloombergs market power is so
substantial that the uk government had to postpone a major

sovereign debt buyback when the bloomberg network went
offline briefly in        for public equities  bloomberg offers
a consensus analyst recommendation from      strong sell
to strong buy  that reflect the aggregate opinion of equity
analysts  prior research on how these recommendations impact
the market suggests that they may actually move markets in the
opposite direction  fueled  perhaps  by a perception that others
with the same information will move in the direction of the
recommendation   regardless of direction  researchers agree
that such recommendations have a major impact on market
perceptions and prices   
we take a novel approach to applying supervised learning
to financial modeling  in addition to forecasting the price of an
equity in the future  which we treat as a regression problem  we
also forecast bloomberg consensus analyst recommendations 
as a classification problem   analyst recommendation categories between       as discussed previously  these consensus
numbers have tremendous power to shape market perceptions
and are associated with detectable movements in the value
of the stock  prior studies have forecasted price  but little to
nothing has been published on forecasting analyst recommendations 
ii  data s ets
we collected over     data sets spanning about   years
 quarterly  for each company in the standard and poor    
 s p      and the russell       the s p     consists of
the     largest companies by market capitalization  while the
russell      consists of      small companies  we were
curious how our methods would perform between the two
datasets  however we ended up abandoning the russell     
due to the large amount of missing data relative to the s p
companies 
iii  m ethods  
a  data processing
the data took the form  x i   y i    where x i is a matrix
of features in r      for company i  the rows of x i
represented time  the columns represented the features  the
label for company i was given by y i in                     rows
corresponded to time in      q            q   

fiwe thinned the original dataset  including only features and
companies with an adequate number of observations  we prioritized keeping companies in the dataset over features  from
an initial     features  and     companies  we narrowed the
dataset to     companies and     predictors  which included
balance sheet items  like cash stock and goodwill  income
statement items like revenue  and several macroeconomic
indicators  e g   gdp growth     year treasury rate 
we added percentage change over the previous period as a
feature for each existing feature  we anticipated that in some
cases the percentage change from the previous quarter would
be more significant than the quanitity in a single quarter 
we made an important modeling decision to treat each
feature as a random variable  and assumed that this was
iid across companies  using this assumption  we restructured the data such that instead of a sequence of matrices 
 x     y       x     y            x       y        we operated on a large
matrix 
  

x
y 
 x 
y  


   
   
  
  
   
   
x
y
in order to attempt to preserve the time information  before
restructuring in this way we applied a simple autoregressive
model to each data series  on each company  to find the
number of significant lags  knowing that linearly dependent
features would be removed by our feature selection procedure
later on  we chose the maximum number of lags that a feature
demonstrated for any company  and included those lags as
features 
missing values were a significant problem in our dataset  we
chose to approach the problem with     dummy variables  we
replaced missing data with an arbitrary value     and included a
dummy feature where each element corresponding to a missing
value took a value of    and was   otherwise  we found
that this method conformed with our lack of an opinion with
respect to missing values 

fig     overall pipeline for predicting components of portfolios

b  model training and evaluation
we randomly divided our data samples into training      
and test sets        we trained several supervised learning
models on the training data using    fold cross validation to
evaluate model performance  we also performed parameter
tuning using    fold cv to select the optimum performing
metrics for each model algorithm  we then fit our model on
the entire training set and predicted responses on the test
set  we divide our learning tasks into two parts as seen in
fig    classification for predicting the analysts stock ratings
and regression for predicting stock prices  accordingly  we
use different metrics for evaluating the performance of our
models  for the classification problem  we use metrics such as
accuracy  specificity  sensitivity  positive predictive
value  ppv  and negative predictive value  npv  
accuracy  

tp   tn
tp   tn   fp   fn

   

sensitivity  

tp
tp   fn

   

specif icity  

tn
fp   tn

   

ppv  

tp
tp   fp

   

npv  

tn
tn   fn

   

where 
t p   true positive rate  t n   true negative rate
f p   false positive rate  f n   false negative rate
for regression  we use root mean squared error  rmse  to
evaluate models
v
u


n
u  x
yi  yi

rm se   t 
n i  
yi

we briefly describe some of the methods used 
   support vector machines  we used support vector machines for classification and support vector regression  an
extension of svm  where the response is continuous instead
of binary  for regression  svms are quite effective in highdimensional spaces and fairly versatile in terms of choosing
different kernels e g linear  polynomial  radial basis functions 
we select the optimum models by tuning parameters such as
cost  gamma and kernels 
for classification  since we have k     classes  we used
the one versus all classification approach  here we fit k
svms  and each time we compare one of the k classes to
the remaining k    classes  for example if  k    k         pk
are the parameters that result from fitting an svm comparing
the kth class  coded as    to the others  coded as     if x
is any test observation  then svm assigns this observation to
the class for which  k    k x     k x           pk xp is

fithe highest  as this corresponds to a higher likelihood that the
test observation belongs to the kth class rather than any of the
other classes 
in case of support vector regression  the method seeks
coefficients that minimize an epsilon loss function  where only
residuals  yi         xi         p xip   larger in absolute value
than some positive constant contribute to the loss function 
   k nearest neighbors  k nearest neighbor is a nonparametric method where given a positive integer k and a test
observation x    knn first identifies k points in the training
data that are closest to x    represented as n    knn then
estimates the conditional probability of class j as the fraction
of the number of points in n  whose response values equal
j  then knn classifies the test observation x  to the class
with the highest probability by applying bayes rule  knn is
useful as the cost of the learning process is not large and
no assumptions need to be made about the characteristics of
the concepts  however  in our case knn is computationally
expensive since number of features is large  we used    fold
cv to select number of neighbors  in knn regression  instead
of combining discrete predictions of k neighbors  we combine
continuous predictions  these predictions are combined by
averaging 
   lasso  ridge based methods  in ridge method  a linear
model is fit by penalizing the coefficients using l  norm
by virtue of a tuning parameter   as  approaches a large
value  the  nd term in the eqn   called the shrinkage penalty 
grows and the coefficient estimates approach    this forces
some of the coefficients towards zero  the ridge method will
therefore include all the p predictors in the model which is a
disadvantage if we have a large number of predictors  in lasso
method  the coefficients are penalized using l  norm by 
 eqn    as  becomes large enough  some of the coefficients
will actually shrink to zero  hence  lasso performs variable
selection and the models are easier to interpret as it will give
rise to a sparse model  we select the value of  using    fold
cv 
p
p
n
x
x
x
 yi    
j xij      
j 
i  

j  

p
p
n
x
x
x
 yi    
j xij      
 j  
i  

j  

   

j  

   

j  

   tree methods  tree based methods involve separating
the features into a number of regions  we then estimate the
mean of training samples in the region for which a data point
which we want to predict belongs to  basic trees are simplistic
and do not generalize very well  hence we evaluate approaches
such as boosting  bagging and random forests  bagging takes a
subset of samples of the training observations and fits several
trees  the predicted values on are then averaged which helps
to reduce the high variance in a basic tree  random forests
gives a slight improvement over bagging by decorrelating
the trees by randomly sampling m predictors from the full
set of p predictors as split candidates every time a split

is considered  boosting involves sequentially growing trees
where each tree is grown by using information from previous
trees  we optimize parameters of these tree based methods by
estimating the out of bag error estimates 
table i  parameter tuning for selected models
classification model
lasso  ridge based methods
support vector machine
decision trees
random forests
k nearest neighbors

tuning parameters
regularizing penalizing term
kernel function  cost  gamma
number of trees
number of estimators  number of features
number of neighbors

iv  r esults  
a  classification
as we are dealing with k     classes  multinomial   we
used grouped  lasso penalty on all k coefficients of particular
variables  this shrinks them to zero or non zero together  fig
  shows the cross validation curve along different values of  
the  which gives the minimum cv deviance is chosen and
corresponding coefficients at that  are selected by the model 

fig        fold cv showing multinomial deviance of lasso
fit
the nature of the label we tried to predict was such that in
most cases  the previous label  included as a feature  was the
same as the current label  consequently  for all classification
results  we felt an appropriate naive benchmark would be
the percent of labels perfectly predicted by the previous
periods label  this was       in our dataset  as seen
in table ii and fig    most of our results were close to
this in terms of accuracy  outperforming the naive estimate
slightly  by around     with the exception of svm   overall 
random forests was the best performing classifier  while most
classifiers had high accuracy  specificity  ppv and npv  they
suffered from low sensitivity 
b  regression
for regression  we started out by evaluating ordinary least
squares model for predicting stock prices which had a test
rmse of       to improve performance  we applied shrinkage

fitable ii  performance comparison of different classifiers
for analyst recommendations  naive benchmark accuracy
      
model
lasso   logistic reg 
svm
random forests
decision trees

accuracy
     
     
     
     

specificity
     
     
     
     

sensitivity
     
     
     
     

ppv
     
     
     
     

npv
     
     
     
     
 a 

 b 

fig     plots of  a  ridge coefficients as a function of   d 
lasso coefficients as function of 

fig     performance comparison of different classifiers for
analyst recommendations

methods such as lasso and ridge regression  fig   shows the
result of coefficients being shrunk towards zero  a in case of
ridge regression and completely to zero  b in case of lasso  fig
  shows the reduction in training mean squared error mse  as
a function of log  values in lasso where mse increases with
increase in the penalty term a dictated by the regularization
parameter  
as we had a large number of predictors  lasso works better
than ridge regression as it performs feature selection and
selects only non zero coefficients in the model for training 
accordingly  as we see in table iii  applying lasso results
in a test rmse of      while ridge regression results in test
rmse of       other methods such as svm  and tree based
methods perform slightly better than lasso regression  with
boosting giving us the lowest test rmse of       overall 
while we managed to significantly reduce the test error from
simple linear regression  there is still room for improvement 
table iii  results for regression  stock price prediction
regression model
linear regression
linear regression with ridge
linear regression with lasso
support vector regression
k nearest neighbors    
bagging
random forests
boosting

test root mean squared error
    
    
    
    
    
    
    
    

fig        fold cv showing mean squared error of lasso fit

v  d iscussion  
overall  with respect to classification  most of our models mildly improved over the naive benchmark  achieving
accuracy of around     compared with      the naive
benchmark   it would be interesting to see if the information
captured by this improvement would be valuable for security
selection  a simulation and related security selection procedure could give answers to this 
with respect to regression  we achieved great improvements
over simple linear regression by applying different approaches 
we managed to get rmse from      to      in the best
model  via boosting  
some ways in which we might improve our error or
extend analysis 
 we might include polynomial terms  nonlinear
transformations  or interaction terms in the models 
 it might be more interesting to predict analyst labels further
in the future because they are less related to the previous
label 
 it might be interesting to model the analyst recommendations
as markov processes  and calculate metrics such as average
holding time 

fin otes
 

mcgowan  m  j          rise of computerized high
frequency trading  use and controversy  the  duke l   
tech  rev   i 
 
patterson  scott          fast traders face off with big
investors over gaming     june       the wall street
journal 
 
kirilenko  a  a   kyle  a  s   samadi  m     tuzun  t 
        the flash crash  the impact of high frequency trading
on an electronic market  available at ssrn         
 
brabazon  tony  a connectivist approach to index modelling in financial markets  aug  sep          afolabi  m  o  
  olude  o         january   predicting stock prices using a
hybrid kohonen self organizing map  som   in system sciences        hicss         th annual hawaii international
conference on  pp          ieee  chicago
 
afolabi  m  o     olude  o         january   predicting
stock prices using a hybrid kohonen self organizing map
 som   in system sciences        hicss         th annual
hawaii international conference on  pp          ieee   zekic 
m         september   neural network applications in stock
market predictions a methodology analysis  in proceedings of
the  th international conference on information and intelligent
systems  vol      pp           
 
kaplan  c  a          collective intelligence  a new
approach to stock price forecasting  in systems  man  and
cybernetics       ieee international conference on  vol    
pp              ieee 
 
        bloomberg increases market share lead over
thomson reuters     march       the baron 
 
cox  josie          bloomberg terminals go down
globally     april       the wall street journal 
 
http   wealthmanagement com equities do analystinvestment recommendations really drive stock performance

table iv  sample data dictionary

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

code
ee   
px last
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   
bs   

definition
analyst recommendation      
last price
 bs  total assets
 bs  shares outstanding
 bs  cash and cash equivalents
 bs  short term debt
 bs  long term debt
 bs  total current liabilities
 bs  preferred equity
 bs  net pp e
 bs  total current assets
 bs  inventories
 bs  accounts and notes receivable
 bs  accounts payable
 bs  short term investments
 bs  long term investments and receivables
 bs  goodwill
 bs  total intangible assets
 bs  gross pp e
 bs  retained earnings and other equities
 bs  other long term
 bs  other current assets
 bs  other long term assets liabilities
 bs  other short term liabilities
 bs  share capital   apac
 bs  number of treasury shares
 bs  pension and post retirement obligations
 bs  accumulated depreciation
 bs  rental expense
 bs  future minimum operating lease obligations
 bs  intangible and other assets
 bs  total capital leases

fi