question answering with neural networks
ye tian
yetian  stanford edu

nicholas huang
nykh stanford edu

abstract
question answering  qa  is a highlyversatile and challenging task towards real
artificial intelligence  it requires machines
to understand context knowledge and the
question query  and provides an answer 
the recent achievements of neural networks  or deep learning  in encoding and
decoding very complicated information
encouraged us to apply them to qa  in this
paper  we design and implement a memory network model and compare its performance with lstm based models  our
experiments show the memory network
model outperforms lstm based models
by a comfortable margin in almost every
task 

 

introduction

question answering  qa  has enjoyed much academic interest due to its flexibility  but with this
flexibility also comes great challenge  in this
project we focused on questions based on finite
amount of information  the system will rely on
nlu and some small amount of reasoning to answer the questions  the tasks are discussed in section   
in this project we implemented a family of
lstm and a memory network model for qa task 
lstm has been applied successfully on the sequence modeling tasks  memory network  weston et al         sukhbaatar et al         is a new
model that incorporates an external memory representation  a technique to improve nn performance on long sequences is attention mechanism 
attention mechanism allows nns to attend to
different parts of the sequences  in our project
we implemented attention mechanism for lstm 
described in full details in section    finally  we
combine the memory network and lstm to form

tianlun li
tianlunl stanford edu

a new kind of model  which we call lstm endto end memory network  lstmmemn n   

 

related work

weston et al         designed the babi task set for
evaluating qa skills of a system  the babi task
set consisting of various simple tasks that focus on
one aspect of intelligence  and will be discussed in
details in section   
weston et al         presented the memory network which learns by combining a short term inference component and a long term memory component  this was later followed by sukhbaatar
et al          who proposed the end to end memory network memn n variant which is a differentiable version of what weston et al  designed 
this model can be trained automatically with optimization methods  sukhbaatar et al         evaluated memn n system on the babi task set  weston et al         and recorded performances surpassing lstm and weakly supervised version of
westons memory network by a wide margin 
attention mechanism mitigates the long termdependency problem in traditional lstm by enabling the system attend to different parts of internal representation  bahdanau et al         

 

data

we used the babi dataset by weston et al           
the dataset consists of context question paragraphs in    different tasks  each answer is designed to be a single word  the dataset can provide insights into different aspects of a qa system 
the vocabulary  however  only measures at    
words and prevents the dataset from being more
realistic 
 

available https   fb ai babi

fitask
dataset
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  

att lstm
en   k
        
        
        
        
       
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

pyr att lstm
en
        
       
        
        
        
        
       
        
        
        
        
       
        
        
        
        
       
        
        
        

memn n
en   k
 
        
        
 
        
        
        
        
        
        
        
 
        
        
 
        
        
        
       
 

memn n
en
        
        
        
        
        
      
        
        
        
        
        
        
        
        
 
        
        
        
        
        

memn n pe
en
        
        
      
        
        
        
        
       
        
        
        
        
        
        
       
        
        
        
        
        

lstmmemn n
en   k
 
        
        
 
        
        
       
        
        
        
 
 
        
       
 
        
        
        
        
 

mlpmemn n
en   k
        
        
       
        
        
        
       
        
        
        
        
       
        
        
       
        
        
        
        
        

table    accuracies of scores of each task for the models

 

models

in a general setting  we define a qa problem as a
collection of context question answer tuples  in
our project we are only concerned with questions
that base solely on the context  for each contextquestion tuple  we want the machine to supply an
answer 
to work with neural networks  we encode each
word in the context and question by a word index
in the vocabulary  therefore we encode contexts c
as integer matrix where cij is the word index of the
j th word in the i th sentence in the context  similarly  we encode question q as a vector of word
indices and answer is a single word index 
all of our models first summarize  c  q  to a
vector g c  q   and then compute the probability
distribution of a given  c  q  as
p a c  q    softmax w g c  q  
where w is a learned weight matrix  now we introduce our models 
   

end to end memory networks
 memn n  

a memn n contains a memory store of contexts  each word in a conetxt sentence is embedded with an embedding matrix a  we denote the
result of embedding as ca   then we combine word
vectors into a sentence representation with combination function fc   similarly we embed question q as with an embedding matrix b as fc  q b   

finally we embed the contexts with yet another
embedding matrix c to extract a candidate word
for answer  with all the embeddings  we evaluate
an attention score over each context sentence as
fs  ca   u   by default the scoring function is simply a cosine distance function  finally the output
word is chosen among the candidate words by a
softmax on the attention score 
in a k hops version  the output of previous hop
is fed into the next layer  this can be stacked for
multiple layers with little additional computation
complexity 
 sukhbaatar et al         also introdueced techniques called position encoding and temporal
encoding that takes the order of words and sentences into account during encoding
in addition  we also experimented with the idea
of using a whole lstm as the combinator function  we call this variant a lstm end to end
memory network or lstmmemn n   another
improvement we attempted was replacing cosine
distance function fs with a multilayer perceptron
architectures  we name this variant mlp end toend memory network or mlpmemn n 
   

lstm family

rnns  especially lstms  are very useful tools
to model sequence data  we can use lstms to
encode a g c  q  representation  attention is a
powerful mechanism for long sequence  such as
how contexts in our problem can be  in these
lstm models  we still embed context sentences

fimean validation accuracy 

   

i
ii
iii
iv

   
   
   
   
   

                                                  

task
figure    test accuracies of each model on the babi en   k task set  with        questions per task 
legend  i  lstm with attention  ii  pyramid lstm  iii  memn n   iv  lstmmemn n
and question with embedding matrix a and b and
feed the vector respresentation to lstms sequentially  denote the output of the lstm with respect to input q b as u   fu  q b    similarly we
denote the output with respect to the context as
r   fr  c  q   then g c  q  can be defined as a simple concatenation of the two
g c  q    ukr
the major difference between variants of lstm
models is the design of encoding function for contexts fr  
there are two problems with a traditional
lstm model  on one hand  lstm tends to forget
the data it receives a long time ago  one the other
hand  it doesnt take the question representation
into account when it encodes context sentences 
to mitigate the first problem  one solution is to
implement a pyramid structure  where we feed every the result vector after reading each sentence
into another lstm and get a more stable and longterm representation 
attention mechanism can address both problems  attention mechanism comes in many different flavors  the basic attention mechanism called
token level global attention  computes the attention scores with a multilayer perceptron  the

attention mechanism can also combine with the
pyramid structure 

 

experiment

we implemented our model with theano  and
trained on babi en and babi en   k task
sets  the results show memory networks perform well against the baseline lstm in almost
all the tasks  one exception task     size reasoning is hard for memory networks using bag ofword combination  likely because of many noncommutative relations in the text  but position encoding and lstmmemn n overcame this issue 
the results are shown in figure   and table   
we also trained memn n on    tasks jointly 
reaching a mean test accuracy of     

 

analysis

by comparing the results in table    we see that on
this relatively simple dataset  memn n performs
extremely well on most of the tasks  memn n is
computationally simple  which makes it easy to
train and mitigate overfitting  for a k hops
memn n   it only contains o k  matrix vector
 
available
at
neuralcraft

https   github com tyeah 

fimultiplications and o k  softmax nonlinearities 
secondly a memn n is statistically flexible  at
least for this qa task  which helps it fit the training data very well  but the original memn n encodes sentences in a relatively naive bag of words
fashion and can lose information about word and
sentence orders  position encoding and temporal encoding in terms have shown to significantly boost the performance in relevant tasks  for
comparison  lstmmemn n uses a lstm as
an even more flexible sentence encoder  and thus
beats a plain memn n on some tasks such as
task  and task    lstmmemn n is also able to
overfit some tasks which memn n cannot  such
as task   and task     so we can say more flexible
sentence encoder does help 
variants of lstm models dont perform as well
as memn n   there are larger gaps between
training accuracies and test accuracies  and they
are more computationally complex  which makes
them harder to train and generalize  also  all the
attention based lstm models only conduct single
hop reasoning  it may help by increase the number of hops  which will unfortunately worsens the
computational complexity 
some of the tasks are clearly hard no matter
which model we use  we think these factors contribute to the difficulty of a qa task  the number
of supporting facts involved  the complexity of the
relation itself  and the length of the context 
the number of supporting facts needed to formulate an answer is an important factor  for example  task         need one  two  and three supporting facts respectively  there is a clear decrease
in performance from task   to task   for each
model  there may be a number of reasons behind this  the straightforward one is the limited
expression power of a fixed length vector  since
in our training process  we apply the same hyperparameters to all    tasks for a model  some tasks
with more supporting facts becomes hard to fully
encode with the hyperparameter chosen 
the second factor is the complexity of the relation within and between the supporting facts 
for example  our experiments show no model
produces an accuracy higher than     for task
    path finding  where the system needs to encode sentences describing the relative positions
between two points  in the end  the question asks
about a path  more precisely  a general direction  going from some point a to another pont

b  the machine needs to be able to connect a
and b through a few intermediate steps  which requires more sophisticated representation than simple matching  another example is task     size
reasoning  which predominantly features relations
that are non commutative  a is smaller than b  
in these sentences  the order of word a and b
is very important  as we have discussed previously  this requires the use of position encoding
and other sophisticated encoding  in our experiments  lstmmemn n achieves a test accuracy
of        beating all the other models 
the third factor contributing to difficulty is the
length of the context  context length in task   
three supporting facts is unusually long compared
to other tasks  whereas other tasks commonly
have    to    context sentences  task   can have
up to a total of     such sentences  the performance on this task turns out very poor for all the
models  for lstms  large context length translates to very long term dependency  which can results in undesired phenomena like gradient vanishing or gradient exploding  which causes the
lstm to forget relevant facts  hochreiter et al  
       for memn n   larger context length
means a larger number of candidate in the memory to choose from  lstmmemn n is somehow
able to overcome this issue partially and outperforms all the other models in task   

 

future work

two main areas of future remains to be explored 
first  we can improve the model performance on
the babi task set  second  we can generalize the
current model to more complicated tasks 
currently  memn n achieves nearly perfect
training accuracies for most tasks  but among
these tasks some show a low test accuracy  showing the sign of overfitting  we should design
proper regularization to better generalize on the
test data and shrink the gap between training and
validation accuracies 
other task sets we can test our system on include the children book test  cbt  task set   hill
et al         and the cnn daily mail qa corpus 
 hermann et al          whereas the vocabulary
size of babi is limited to      the vocabulary of
 
available at http   www thespermwhale com 
jaseweston babi cbtest tgz
 
available at https   github com deepmind 
rc data 

ficbt task set has about        words  the vocabulary sizes of cnn part of the corpus and daily
mail part of corpus both measure in one to two
hundres of thousands 
for memn n   the complexity of the model
comes in the forms of number of hops  but because the necessary complexity of the model can
vary tremendously by tasks  it would be useful to
design a memn n model which can learn to control and regulate its own number of hops  moreover  a better sentence encoder may also help the
performance 
the main problem facing the lstm models is
long term dependency  explicit memory representation like those used in memn n may help
mitigate this issue  another possible approach
to tackle the long term dependency issue is an
idea called neural turing machine  graves et al  
      

 

conclusion

we experimented with memn n and lstms as
the two major qa solutions  in our experiments 
memn n outperform lstm models  but the
combination of the two models  lstmmemn n  
achieves even higher test accuracies  since language is a sequential data by nature  however  we
think lstm models also have a large potential to
be explored 

acknowledgement
professor chris potts and professor bill maccartney helped us defining our project 
jiwei li helped us debug the model when it
wouldnt converge 
the lisa lab deep learning tutorial  gave us
a great head start with the lstm model 

references
d  bahdanau  k  cho  and y  bengio  neural machine translation by jointly learning to align and
translate  corr  abs                 
a  graves  g  wayne  and i  danihelka  neural turing machines  corr  abs           
     
url http   arxiv org abs 
          
k  m  hermann  t  kocisky  e  grefenstette 
l  espeholt  w  kay  m  suleyman  and p  blun 
available
https   github com lisa lab 
deeplearningtutorials

som  teaching machines to read and comprehend  corr  abs                   url
http   arxiv org abs            
f  hill  a  bordes  s  chopra  and j  weston 
the goldilocks principle  reading childrens
books with explicit memory representations 
corr  abs                   url http 
  arxiv org abs            
s  hochreiter and j  schmidhuber  long shortterm memory  neural computation           
           
s  hochreiter  y  bengio  p  frasconi  and
j  schmidhuber  gradient flow in recurrent nets 
the difficulty of learning long term dependencies       
a  kumar  o  irsoy  j  su  j  bradbury  r  english  b  pierce  p  ondruska  i  gulrajani  and
r  socher  ask me anything  dynamic memory networks for natural language processing 
corr  abs                   url http 
  arxiv org abs            
d  e  rumelhart  p  smolensky  j  l  mcclelland 
and g  hinton  sequential thought processes in
pdp models  v              
s  sukhbaatar  a  szlam  j  weston  and r  fergus  weakly supervised memory networks 
corr  abs                   url http 
  arxiv org abs            
j  weston  s  chopra  and a  bordes  memory
networks  corr  abs                  url
http   arxiv org abs           
j 

weston  a  bordes  s  chopra  and
t  mikolov 
towards ai complete question answering  a set of prerequisite toy
tasks  corr  abs                   url
http   arxiv org abs            

fi