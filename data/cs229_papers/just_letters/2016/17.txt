 

classification of transcription start sites in the human genome
ann he  zandra ho  chloe siebach
  

introduction

the dramatic advancement in genome sequencing within the last decade has transformed the field of genetics  as our data
samples grow exponentially and biological techniques become infeasible  there have been new initiatives to use big data
to gain insight into gene regulation  the central dogma of biology states that the flow of genetic information goes from
dna to rna to protein  each gene has regions that control the level of transcription  or the level to which dna is being
transcribed to rna before it is then translated into functional proteins for our bodies  these regulatory regions fall into
two major categories  promoters and enhancers  promoter regions serve as the binding site for the transcriptional
machinery  rna polymerase   while enhancer regions bind molecules that help recruit rna polymerase to the promoter 
we use data from the roadmap epigenomics project  which used histone modification data to classify transcription start
site  tss  regions into functional categories in several dozen cell types  we hypothesize that the sequence of dna
surrounding each tss determines a tsss function  and thus propose to develop a machine learning method that is able to
predict a tsss functional class from its sequence context  we implemented four models  naive bayes  random forest 
gradient boosting  and convolutional neural networks  and trained them on tss and epigenetic data collected from
three cell lines  a     gm       and k      the ability to classify known tss regions on a large scale would open up
many doors for continued exploration of the mechanisms governing gene regulation 
  

related work

there is a lot of interest in classifying these tss regions as promoters or enhancers  but practice has proved difficult  in
particular  the ability to distinguish between promoters and enhancers is fundamental in understanding the mechanisms of
gene regulation  epigenetic marks are highly predictive of functional class cite chromhmm   but the required
experiments are expensive and require large amounts of input material 
in particular  histone chip seq  chromatin immunoprecipitation  followed by dna sequencing   is one of the leading
technologies available for visualizing information encoded in the epigenome  while this method is able to identify the
promoters and enhancers well  this technology is too expensive and impractical to utilize on a large scale 
despite the interest and demand for classification of human tsss  there have been no attempts to use machine learning
for annotations thus far  in addition to expediting the overall annotation process in a cost effective way  this novel
approach would surpass biological assays by generalizing to classify rare cell types as well  an area in which lab
techniques have failed due to their dependence on large amounts of biological material  the ability to predict functional
class from sequence and experimentally identified tsss would expand the catalog of known enhancers  and would
provide insight into the sequence features that differentiate enhancers from promoters 
  

data   features

our dataset consists of        tss regions spread across three cell linesa     gm       and k     this data comes
from the roadmap epigenomics project  a government funded effort to provide an epigenomic map to identify key
functional elements in the human genome for basic biology and disease research  having a complete annotation of the
promoter and enhancer regions of a cell line will provide crucial epigenomic insights  since these areas interact with
transcription factors to upregulate and downregulate protein expression 
it should be noted that although there are many more enhancers than there are promoters in a genome      of our training
examples are promoters  this imbalance can be attributed to the method of tss identification  which relies on the
collection of transcribed rna  these rna strands arise naturally from promoters  while they only surface due to the
accidental transcription of enhancers  the statistical power to detect promoter tss regions is reflected in the data set 

fi 

   
     

preprocessing
region size

a tss region is determined using information gathered from numerous assays of a cell line  these assays pinpoint the
exact base pair that is transcribed first  and this exact location has been found to vary  the slight variation leads us to
define a general regionthe transcription start site regionthat on average spans approximately    base pairs  we relied
on dr  nathan boleys peak calling pipeline to identify these regions  boley   the dna responsible for attracting various
proteins for transcription surrounds the tss region  to train our classifiers  we considered region sizes of     and     
base pairs with the tss at its center 
     

k mers

k mers were used to preprocess the data for gradient boosting and random forest  in computational genetics  the k mers
of a dna sequence refer to all its possible subsequences of length k  these k mers therefore keep track of consecutive
patterns of length k in the data  and though they do not track larger or non contiguous patterns  they are often used for
convenience and for where continuous patterns are enough to classify accurately  for each tss sequence in our data set 
we created a feature vector containing the counts of distinct   mer appearances of all consecutive   mers of that sequence 
we chose to use   mers since            distinct permutations gave us a large spectrum of distinguishing patterns to
classify along  without causing our dataset to be too large to work with  the observation of a specific sequence of k    
base pairs is long enough to be meaningful  as opposed to something as short as k    which would just return a count of
each base in the sequence   and yet short enough to appear frequently  the k mer vector is analogous to the vector of word
counts used for classifying spam and non spam emails 
     

one hot encoding and filters

one hot encoded sequences are used to pre process the data for neural
nets  because there are four options for which a base is at any location 
this process takes a sequence of length n and converts it into a matrix of
size   x n  with each row corresponding to a base  a  c  g  or t   the
values of the matrix m are indicator variables  such that m a j      if the
jth base is a  and   otherwise  an example encoding is pictured in figure
  

figure    an example one hot encoding
sequence  to be fed into a neural nets model 

the first layer in our neural nets architecture is a convolutional layer  which scans the one hot encoded sequences with
filters of size   x     over the course of training  these filters identify patterns in the encoded sequences that are important
for predicting labels  neural nets also iterates through several more hidden layers of data processing  but the first is the
most useful to understand because the trained filters can give us insights into base pair patterns that differentiate between
enhancers and promoters 
     

balanced data sets

as mentioned previously  our data set contains almost    times as many promoters as enhancers  we accounted for this
disparity by introducing balanced data sets while training our classifiers  drawing a subset of promoters at random to
create a new train set 
   methodology
    model selection
initially  we considered naive bayes  random forest  gradient boost  and convolutional neural networks as models 
we quickly found  however  that the naive bayes assumption of independence between k mers made for poor predictive
ability  and chose to focus only on the other three  within each of those models  we examined the impact of model
parameter choice on accuracy using cross validation techniques on balanced data sets  and then trained each model using
the optimal parameters on the entire data set 

fi 

      gradient boosting
for gradient boosting  we used the logistic regression loss function and primarily examined the impact of changing the
number of weak learners that it considered in its predictions  again using    fold cross validation  we found that the best
accuracy occurred with     weak learners  figure     here  accuracy was measured by area under the roc curve 

figure    cross validation results for gradient boosting with
varying numbers of estimators  with accuracy measured by
auroc 

figure    cross validation results for random forest with
varying numbers of estimators  with accuracy measured by both
auroc and auprc 

      random forest
random forest models are parametrized by the number of trees that are built during training and the depth of those trees
 among other parameters   we quickly found that changing the depth of the tree did not have a significant impact on our
model  and chose instead to focus on the number of trees  using    fold cross validation  we found that the best accuracy
occurred when we used    predictors  figure    with the logistic regression loss function  we began by evaluating model
performance based on the area under the receiver operating characteristic curve  in anticipation of an unbalanced test set 
we later considered the area under the precision recall curve as well 
      convolutional neural networks
convolutional neural networks have been shown to work well on dna data in the past  so we quickly found architectures
and parameters that performed decently well        auroc       auprc  on cross validation test data  the two
architectures can be found in figure    and the plot of their relative predictive accuracies can be seen in figure   
architecture   
convolution
architecture   
dropout
convolution
flatten
dropout
dense
maxpool
dropout
convolution
activation
dropout
dense
maxpool
dropout
flatten
activation
dense
dense
activation 
figure    the sequences of layers used in the two different
neural nets architectures that were considered 

figure    comparison of cross validation results for different
neural nets architectures  with accuracy measured by auroc 

as we trained the two differently architectured models  we used a callback function of keras neural nets software that
allowed us to stop training the model when the performance on a hold out data set began to worsen  this method ensures
that the model learns as much as possible from the training data without overfitting  to understand this method more

fi 

fully  we made a plot of loss  training and validation  vs epoch  training iteration  for both models  figure     the loss
function used was binary crossentropy  the plot shows that the training loss generally decreases  whereas the validationloss jumps around and eventually starts to increase  once this increase starts to occur  however  the keras package stops
training the model  preventing overfitting 

figure    change in the values of training and validation loss for neural
nets architectures as a function of the number of training iterations run 

figure    cross validation results for neural nets
models with varying numbers of filters  with accuracy
measured by auroc 

based on the information gathered  we chose to further tune and analyze model architecture    the boxplot in figure  
shows the impact of varying the number of filters in the first convolutional layer on the accuracy of the model  we wanted
to have as few filters as possible while still optimizing for accuracy  measured by auroc   after experimenting  we
chose to proceed with the model that used    filters  we were ultimately able to fine tune our model with great accuracy
       for both auroc and auprc on cross validated test data  thanks to the experience of our mentor with this kind of
data 
   results
after training our final gradient boosting  random forest  and convolutional neural networks models on the a    
gm       and k    cell lines  we tested on the hepg  cell line 
    gradient boosting and random forest
we discuss the results from the random forest and gradient boosting models together due to their similarity in weighting
the importance of all      possible   mers  figure   displays the top      mers identified by each model  although we
cannot conclude from the lists that a particular sequence pattern can be used to identify promoters  we do learn that the
guanine and cytosine nucleotides are highly indicative of promoter regions  this result is consistent with the literature  as
sequences of guanine and cytosine tend to form strong intermolecular forces that would promote the binding of rna
polymerase to the region  kudla et al   the top    lists generated by the two classifiers share four kmers in common 
which was an encouraging sanity check  both models did very well on area under the precision recall curve  gb        
rf         and less well on area under the receiving operator characteristic curve  gb         rf         

gcggcg
cgccgc
ccgcgc
cgcggc
ccgccg

random forest
gggcgg
ccgccc
cggccg
cgcgcg
ccgcgg

cgccgc
ccgccc
gggcgg
cggcgg
gcggcg

gradient boosting
ggcggc
gggccc
gcgccg
ggggcc
caggca

figure    top ten   mers for the random forest and gradient boosting models    mers found in both models are
highlighted in blue 

fi 

    convolutional neural networks
we chose to interpret the filters produced in the first layer of the architecture as it deals directly with the data  and is
trained to detect patterns that appear very frequently or infrequently in promoter regions  our    filters are initialized
 mostly  randomly and orthogonally to capture as much variety as possible  and the training process changes them to be
representative of significant patterns in the data  admittedly  many of the filters were noisy and difficult to analyze  and
we chose to display the two most representative filters in figure    consistent with findings from the gradient boosting
and random forest models  our neural nets classifier recognized cytosine and guanine nucleotides in close proximity 
this model also revealed an important motif consisting of adenine and thymine content  this finding is also consistent
with the literature  as high at content corresponds to flat regions of dna  ideal sites for the unwinding of the double
helix for transcription  rohs et al   the neural networks model turned out to be highly accurate  doing extremely well on
both auroc         and auprc         
convolutional neural networks classifier outperformed the gradient boosting and random forest classifiers  suggesting
that neural nets probably recognized some features that were not present in the k mer lists  this result exposes an
interesting limitation of the contiguous k mers used in gradient boosting and random forest  consider motif     the
nucleotides are not clustered together  but rather span a region of     base pairs  which gradient boosting and random
forest would understandably fail to recognize 
random forest  gradient boosting  and convolutional neural networks all have different underlying mechanisms of
learning from a data set  but they each reveal the importance of gc clusters for identifying promoter regions  this result is
consistent with what could have been expected  because gc content is related to how strongly enzymes are able to bind to
that region of dna  additionally  our convolutional neural networks model suggests that intermittently spaced adenine
and thymine nucleotides are also highly predictive of promoters in transcription start sites  a feature that is unrecognizable
by the   mers of gradient boosting and random forest  all three models performed very well on the test set  confirming
our initial hypothesis that models trained on certain cell lines may be generalized across many cell lines  our results
indicate that machine learning is a good  cost effective alternative to current biological techniques of transcription start
site classification 

motif  

motif  

figure    motifs of two of the filters used on the final neural nets model  before training and
after training 

fi 

references
boley  nathan          methods for the analysis of high throughput sequencing data  uc berkeley  biostatistics 
retrieved from  http   escholarship org uc item   b   fc
kudla  g   lipinski  l   caffin  f   helwak  a     zylicz  m          high guanine and cytosine content increases
mrna levels in mammalian cells  plos biology        e     http   doi org         journal pbio        
rohs r  west sm  sosinsky a  liu p  mann rs  honig b  the role of dna shape in protein dna recognition  nature 
                          doi         nature      

fi