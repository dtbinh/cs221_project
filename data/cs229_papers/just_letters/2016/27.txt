learn to rate fine food
jiacheng mo  lu bian and yixin tang
stanford university  us

abstract we investigate a food review dataset from amazon
with more than        instances  and utilize information from
the data set such as the text review  score  helpfulness  etc 
instead of the traditional word representation using frequency 
we use skip gram to train our own word vectors using the pretrained glove twitter word vector as the initialized value  we
also use recursive parsing tree to train the vector of the whole
sentence with the input of word vectors  after that  we use
neural network methods to classify our review text to different
scores  we mainly research and compare the performance
of gated recurrent unit network  gru  and convolutional
neural network  cnn  on our data  after tuning the hyper
parameters  we get our best classifier as a bi directional gru 
we also build a long short term memory model  lstm 
for text generation  which is able to generate text for each
score level  then we build a recommendation system based on
latent factor model  with stochastic gradient descent  and
recommend    items to selected users  finally  we use softmax
regression to visualize the most important words for a certain
score  and design a spam review binary classification based on
the helpfulness scores of the reviews 

i  introduction and related work
in recent years  food reviews have become increasingly
popular on social media  people are posting their reviews or
comments on facebook  yelp  twitter  etc  when selecting
local restaurants or food  people also tend to make their decisions based on these reviews  hence  it is important for both
restaurants and individuals to quickly get the information
and score of a food item or restaurant from thousands of
reviews  it is also beneficial for some platform to provide
different customers with their personal recommendations 
mcauley and leskovec     have done related work on
recommending beer  wine  food  and movies  where they
built a latent factor recommendation system that explicitly
accounts for each users level of experience  here  however 
we built a less sophisticated latent factor recommendation
system  with the intent to achieve reasonable accuracy 
marx and yellin flaherty     have done related work on
sentiment analysis of unstructured online reviews using gru
and lstm model  where they incorporated a relatively small
data set  here  we extend the discussion to a larger data set 
and we have used lstm model for review generation that
has never been done before 
ii  datasets and features
our analysis focus on an amazon food review database
consisting of          instances  the data set contains the
texts of reviews  scores and helpfulness  in more detail 
 product id  the product that this review is for  the
total number of items that are rated is          and
the average number of reviews for an item is     

fig     distribution of scores of reviews 

user id  the user who wrote this review  the total
number of users is           so the average number
of reviews that each user gave is     
 text  the main content of the review 
 helpfulness  number of people who find this review
helpful 
 score  the score this user gave to the food item 
the histogram in fig    shows that the number of positive
reviews is greater than the negative ones  also  the number
of reviews with score   is much greater than other reviews 
this shows that customers are more likely to give a review
when they feel either very satisfied or disappointed 


iii  methods
a  word to vectors
   skip gram  instead of using the traditional countbased representation of words  we train the word vectors
from our own data  first  we download the glove global
vectors for word representation which was trained on the
crawl data of twitter  b tweets     m vocab       we can
visualize the vector representation by finding the nearest
neighborhood of a certain word through l  measure  for
example  the   nearest words of frog are rana  litoria 
etc 
however  we do not directly use this representation for
our final representation of words  instead  we just use this
as an initializer of our self trained word to vector model 
we train our word vectors by skip gram model  which uses
the method of predicting the surrounding words in a window
of certain length  the objective function is to maximize the
log probability of any context word given the current center
word 
j    

t
 x
t t  

x
mjm j   

log p  w t   j  w t   

fiwhere  represents all variables we optimize and p wt j  wt  
is the probability 
exp ut  vc  

p o c    pw

w  

exp utw vc  

where o is the outside word vector and c is the inside word
vector  every word in this model has two vectors 
   phrase vectors  now we have got the vector
representation of each single word  but our aim is to
classify the entire review  in sentences   hence we need
to derive a method to vectorize the sentences  a natural
approach is to average or concatenate all word vectors in a
given sentence  it turns out that this works well and can be
very convenient to implement like the fig    
we are not satisfied so far since neither concatenation
nor averaging treats each word equally and neglects the
relationship between words and the structure of the sentence 
hence we use a recursive parsing tree to get our sentence
vector  concretely  for every representation of two candidate
children  we calculate the semantic representation if the two
nodes are merged  as well as a score showing how plausible
the new node would be  the score of a tree is computed by
the sum of the parsing decision scores at each node  we train
our tree by maximizing the max margin parsing     

b  neural network methods
   gated recurrent units  our first task is to group
reviews into   classes  the main method we use involve
models that are neural network based and are all
implemented in tensorflow  we use logistic regression
as our baseline model and try to add some layers and more
complicated structures to reach a higher accuracy  the
state of the art model we implemented for classification is
the bidirectional gated recurrent units  gru  
we know that the standard recurrent neural network
computes hidden layer at the next time step directly by 
ht   f  w hh ht    w hx xt  
in gru  we first compute an update gate according to the
current input word vector and the hidden state 
zt    w z xt   u z ht   
then we compute the reset gate with different weights 
rt    w r xt   u r ht   
and obtain the new memory content and the final memory 
ht   tanh w xt   rt  u ht   
ht   zt  ht        zt    ht
fig    shows the framework of our bidirectional gru  the
difference between the standard gru and our bidirectional
gru is that for each hidden layer  the information flows
not only from the left but also from the right  the concrete
formulas of our   layers bidirectional gru are listed below 

 i 
direction  ht  
 i 
   z  xi   u
   r  h i   
 zt    w
 i  t
 i  t 

right
fig     sentence representation by averaging

fig     sentence representation by parsing tree

j 

x
i

s xi   yi    max  s xi   y     y  yi   
ya xi  

fig    shows the illustration of this method on the same
sentence 

 i 
   r  xit   u
   r  h i   
 rt    w
 i 
 i  t 

 i 
 h   tanh w
   i  xt   rt  u
   i  ht   
t
 h i    z  i   h i        z  i     h i 
t
t
t
t
t 


 i 
lef t direction h t  

 z 

 r   i 
 i 


z t    w  i  xit   u  i  ht   

 r 

 r   i 
 i 


r t    w  i  xit   u  i  ht   




  i 
 h
t   tanh w  i  xt   rt  u  i  ht   

 i 
 i 
 i 
 i 
 i 
h t   zt  ht        zt    ht

   

output
 top 
 top  
yt   sof tmax u   ht   h t     c 
by feeding a review into each x  we allow each batch
review to influence each others prediction  an idea that

fimay seem counterintuitive since each review is independent 
however  different reviews may have a certain connection
with the same food or restaurant  only through this structure
can we explore the deeply connected information of reviews 
after we fetch yi from gru  we use cross entropy loss for
the network and train the weights 

given

a

f or

h 

                

hi     f unction
choose
of

beyond the   layers gru network  we also obtain good
results from a   layers gru    layers convolutional neural
network  and   layers convolutional neural network  we
summarize the accuracy later to compare these models with
different hyper parameters 

random
i

hi  

the
to

above hi   xi  

largest
be

probability

class

xi  
   

c  recommendation system
we use latent factor model to recommend food items
to a user  as shown in fig     the sparse user item utility
matrix r can be decomposed to a user factor matrix p and
a item factor matrix q  in our model  we choose the number
of factors to be      after decomposition  we use stochastic
gradient descent  sgd  to decrease the loss  and find the
final matrices p and q  whose product is closest to r 
denote the riu of the matrix r the rating given by user
u to item i  the total error is 
e 

x

 riu  qi ptu       

 i u israted

x

  pu       

u

x

  pi       

i

denote iu the derivative of the error e with respect to
riu   then
iu   riu  qi ptu
fig       layers gru

and the update equations for qi and pu in sgd are 
qi  qi    ui pu  qi  

   long short term memory  lstm   after the
classification  we want our model to automatically generate
some reviews  concretely  we first specify a score
representing the rank of the reviews  like   means the best
and   means the worst   and then feed this score to the
neural network  we want our neural network to generate the
review corresponding to the specified score 
the model we use is lstm  which is also a modified
version of recurrent neural network but with some reset and
forget state  this method selects the most probable word at
each time  depending on which class we are in  fig    gives
us a brief structure of lstm 

pu  pu    ui qi  pu  
after    iterations of sgd  we obtain the final p and q 
then  we predict the score the user will rate an unrated item 
and recommend    items of the highest scores to the user 

fig     latent factor model
d  features visualization and spam reviews

fig     lstm for generation

our algorithm for generation can be simplified as 

in this part  we use the traditional count based
word representation and compare the results with the
previous approach  we utilize count vectorizer followed
by term frequencyinverse document frequency  tf idf     
transformation to transform the text into word tokens  count
vectorizer converts a collection of text documents to a
matrix of token counts and produces a sparse representation
of the counts  the following tf idf transformation converts
the count matrix to a normalized tf idf representation 
in information retrieval  using tf idf  instead of the raw

fifrequencies of occurrence  can scale down the impact of
words that occur very frequently in a given corpus and
that are hence empirically less informative than features
that occur in a small fraction of the training corpus  for
example  words such as the  is  etc  can appear in any
text in general  and thus are offset by the frequency of the
word in the corpus in tf idf representation 
for finding the words that contribute the most to each
of the five scores of the food reviews  first we use feature
selection to select      best informative words out of the
      features  from tf idf   the next step is to build a
softmax regression model on these features and to fit between
words  x and scores  y  in softmax regression  we maximize
the log likelihood 
l    

m
x
i  

log

t  i 
k
y
 i 
e l x
   y  l 
  pk
jt x i 
l  
j   e

finally  we choose the largest    coefficients in i of
the probability of each class yi   and get the corresponding
words as the most important words for the score 
since the relationship between a word and the helpfulness
of the corresponding review is not as clear as the relationship
between a word and the score of its corresponding food item 
we define an helpful review to have a helpfulness score
greater or equal to    where as an useless review has   as
helpfulness score  thus  this becomes a binary classification
problem  with truncated data set that does not contain reviews
with       or   helpfulness score  we fit the data to softmax
regression model  quadratic discriminant analysis model     
as well as the decision tree model      where we varied
the regularization parameter and plotted training and crossvalidation accuracy versus regularization parameter 

fig     accuracy table

classes  then we use convolutional neural network and tune
the learning rate  from the table we find that the learning
rate      is more suitable to reach a better minimum 
finally  we use gru with two and three layers respectively
to train our data  we find   layers gru can reach a higher
accuracy and its top   classes  scores   and    accuracy is
      this is meaningful  because usually we only need a
broad idea of whether this food is good or not  instead of a
explicit score 
   lstm  here is an example text generated from our
lstm of score   food review  we show the process of
picking the highest probability from the last hidden layer
and fetching into the next input layer 

fig     level   generation
with the the number of iteration increases  the generated
sentence makes more sense to us  including the punctuation 
for each class  we select some reasonable generated sentence
to present here 

iv  results
a  neural network methods
   gru  the results of gru and cnn model and the
loss sequence of   layers gru are summarized in the table
below 

fig      generated review
b  recommendation system

fig       layers gru loss function
we list part of our experiments as above  we start with
the vanilla neural network of a single layer and its test
accuracy is      which is not bad when we have   different

in the sgd process  we have changed the learning rate  to
get the lowest convergence error after    iterations see figure
     we finally decide to use      to get the lowest error 
 another way is to decrease the learning rate as iteration
grows  
we can get the recommendation for an user by finding
the largest scores in the row corresponding to this user in
the utility matrix r  there is a sample recommendation for
this user shown in figure     however  the recommendation
result is not as good as we thought  there exists a set of
products being recommended to many different users  the
reason behind this problem may be the fact that our model
does not include an item bias  so the scores of some items
may be much greater than others for all the users  these

fifig      cross validation accuracy curve for qda and for
dt
fig      error decreases as iteration grows  different learning
rate

items  such as trident gums  are much more likely to be
recommended than others 

fig      recommendation result example

c  features visualization and spam reviews
after fitting the data to the softmax regression  we compute    words that have the largest coefficient and thus are
the most significant words for each score  we organize these
words into tag clouds fig       where the size of each word
is based on the relative value of the coefficient 

fig      tag cloud for scores from   to  
first we fit the data to softmax regression model  and
find that the training error as well as cross validation error
are      and      respectively  we notice that the softmax
regression model failed  possibly due to the non linearity of
the frequency of a word versus the score of the food item 
we obtain decent training and cross validation accuracy
when fitting our data to both qda and decision tree model 
we vary the regularization parameter and plotted training
and cross validation accuracy versus regularization parameter fig      
v  conclusions and future directions
we use multiple neural network models to classify our
text  our results show that bi directional gru has the best
performance with high efficiency  another advantage of our
gru is that it is immune to extremely unbalanced data 
during our experiments  we find that when the distribution
of data is not uniform  for example  when most food items

are in level    most classifiers will just assign any input
to the majority class to reach a high accuracy  however 
gru can still learn it well and not stick to the majority 
a natural generalization of our model is to apply it on
different review tasks  say  yelp  twitter  walmart  ikea  etc 
this requires our model to automatically choose the hyper
parameter and structure  in order to accept the input under
different context  this will be our new direction to design a
more robust classifier which can be used upon any kinds of
customer review 
our recommendation system is based on the basic latent
factor model  we have reached a relatively low error
by changing the learning rate in sgd process  but the
recommendation result is not very satisfying  this model
can be further improved by adding some terms  such
as  user item bias  implicit feedback  temporal dynamics 
user associated attributes  and confidence level      also  if
time is allowed  we can develop a better recommendation
system  a common way is to use hybrid method and build
a system that has many levels filtering      for instance 
we can add a global effect filtering before latent factor
model  and after that  add a collaborative filtering user user
or item item  
softmax regression succeeded in selecting words for a
specific score  but it failed in modeling the helpfulness of
a word  for the latter task  both qda and decision tree
model fit the data well  mainly due to the non linearity of
the frequency of a word versus the score of the food item 
r eferences
    mcauley  and leskovec from amateurs to connoisseurs  modeling the
evolution of user expertise through online reviews  proceedings of the
  nd international conference on world wide web  international world
wide web conferences steering committee       
    marx  and yellin flaherty aspect specific sentiment analysis of unstructured online reviews
    glove twitter crawl   http   nlp stanford edu projects glove 
    taskar et al       learning structured prediction models  a large
margin approach
    blei  david m   andrew y  ng  and michael i  jordan  latent dirichlet
allocation  the journal of machine learning research                   
    friedman et al  the elements of statistical learning springer  berlin 
springer series in statistics       
    breiman et al  classification and regression trees  crc press       
    jure leskovec et al  mining of massive datasets cambridge university
press       

fi