alex han
stat    
june        
modeling hla ligands for binding prediction of new peptides
over the course of human history  hla have evolved
to be able to recognize  bind  and present the wide variety
of proteins of pathogens  whereas pathogens have evolved
to escape detection  because of this human pathogen coevolution  hla is now one of the most hypervariable proteins in humans  with easily over thousands of different
known alleles  as a result  each hla has unique binding properties that allow it to present different peptides 
whether host or foreign 
developments in mass spectrometry have allowed biologists to sequence these peptide ligands that can bind
to a single hla allele in a healthy human cell line  older
assays could only offer a dissociation constant  or a metric
of how well a ligand is thought to an hla without offering direct proof of whether a ligand was actually bound
by hla  a newer method has been developed that definitively shows binding with an hla allele  however  it does
not come with dissociation constants  and we cannot sequence the ligands that did not bind 
earlier machine learning methods for peptides based
on these older experiments relied on a threshold for the
dissociation constant and made a two class problem by
labeling peptide ligands as good binders and poor
binders non binders  however  this new dataset has only
has good binders  unfortunately  these datasets cannot
complement each other as they relied on different experimental protocols 
my two goals in this project were to model the distribution of peptides that bind to each hla and compare
them  and use this data to predict whether untested peptides will bind or not 
data and preprocessing
my unpublished experimental dataset has been given
to me by curtis mcmurtrey and william hildebrand 
there are peptide sequences from   different hla alleles  in this experiment  i will be using just the peptides
that are   amino acids long as it has been shown that
these nonamers are the likely canonical peptides that
hla are best designed to fit  it is important to note that
all of these sequences were shown to bind  there is no experimental data for peptides that do not bind  a brief
summary of counts of sequences is shown in table   

introduction
the hla  human leukocyte antigen  is a family of receptor proteins found on the outside of nearly every cell
in the human body  they are one of the most important
components of the innate immune system  they bind randomly fragmented peptides from discarded proteins floating around the interior of the cell  then pull them out to
the cell surface to present them to the immune system 
generally  hla will bind peptides representing the repertoire of human proteins  but if a virus has infected a cell 
hla will present peptides from viral proteins to the surface of the cell for other cells to detect and trigger an
immune response  in this way  the hla serve as a window for the inner contents of a cell to be seen without
other cells having to invasively sample it from within  a
cartoon depicting the function of is shown in figure   

table    a short summary of the sequences used 
hla
  of nonamers
b      
    
b      
    
b      
    
b      
   
c      
   

figure    simple illustrations of hla function in both
healthy and infected cells 

for model testing  i randomly generated sequences as
 

fia negative set using the frequency of amino acids found
in the human proteome  however  the experimental data
has a low sensitivity rate  such that any randomly generated sequence may actually be a binding sequence  however  given the high dimensionality of the data      possible nonamers   i am using the assumption that these
randomly generate sequences will suffice as nonbinders 
for testing predictions  i will be using a set of sequences also obtained using the same experimental protocol  but with only hla b       in a cell with a strain of
influenza a virus h n   swine flu      a small number of
these sequences have been identified as binders from the
viral proteome  the remaining set of all possible nonamers from the viral proteome are used as nonbinders 
rather than use sparse encodings of these peptide sequences  i e  create    binary variables for every   positions of the amino acid for the presence of one of the   
amino acids at every position   i opted to convert the sequences into physiochemical properties  creating a numerical pseudo continuous range of values  the four properties i used were molecular weight  surface area  isoelectric
point  and hydrophobicity index  thus converting every
nonamer string into    features  figure   explains this
transformation  a two dimensional t sne on the transformed experimental data is shown in figure   

kernel density estimation
because the data is a one class set  i sought a generative approach by building a model that would best
explain the sequences  i settled on using multivariate
kernel density estimation for this purpose  it addresses
several quirks with the data including its irregular multivariate distribution  sparseness in its dimensions  and its
high covariation among not only just the   physiochemical properties assigned to each position  but also across
positions  i could then later fit new peptides into this
model the likelihood of binding with an hla 
however  the significant drawback of this approach is
its complexity  because creating a density in    dimensions is prohibitively costly  i decided to first reduce the
dimensionality of the data  i tried two approaches  principal components analysis and autoencoders 
pca   kde
pca is a simple linear transformation of the original
variables that also maximizes the total variance in its first
few components  so  the first few principal components
can be used as a substitute for the original dataset  this
methods downsides are that pca cannot capture nonlinear trends  and it misses out on the lesser components 
which while less important  represent unique aspects of
the data 
after pca  i chose the first five   principal components of the data use as my features for the multivariate
kde  which accounted for approximately        of the
total variance depending on the hla allele 
to verify the results of this method  i used    fold
cross validation for the dataset  in addition to using     
randomly generated peptide sequences  they were then
plugged back into the kde after conversion with the origfigure    the    features generated using the   positions inal loading matrix to obtain likelihood values  any conclusions drawn from their model fit should be taken cauand   physiochemical amino acid properties 
tiously as some of these random sequences may very possible bind to the hla in reality 
at each iteration of the cross validation  i used    
of the sequences pcs to construct the kde model  then
the reamining     and the fake peptides to test the
model  the bandwidth matrix of the kde was optimized using the smoothed asymptotic mean squared error
 samse     
autoencoder   kde
autoencoders are neural networks that aims to create
an output approximate to its input after passing through
a hidden layer with fewer nodes  the hidden layer encoding with fewer nodes can then be used to substitute the
original dataset  this methods downsides are that it relies on neural networks  which not only have many input
parameters  but also do not underly a probabilisitic foundation  which means optimizing and evaluating a neural
network  is difficult to do empirically 
instead of using a pca and choosing the top   principal components to use for the kde  an autoencoder with
one hidden layer with   nodes was first constructed using
figure    t sne of the experimental data 
the training set of     of the true peptides  the autoen 

ficoders were manually tried under a variety of parameters
to assure optimal information retention and feature diversity among hidden nodes  similar to the pca   kde
approach      of the sequences pcs to construct the
kde model  then the reamining     and the fake peptides to test the model 

kde performance
to visualize the overall predictive performance  i created roc curves to visualize and compare the accuracy
of these methods  the roc curves in addition to their
auc values for a single k folds iteration is shown in figure   
for each of the k folds iterations for both methods  an
auc score was calculated  these auc scores were then
averaged over the ten iterations  these values are plotted
in figure   
sampled kullback leibler divergences
the two sets of kde likelihood distributions were also
used to study the difference among hla probability distributions  rather than use the empirical distribution
of     possible sequences  i used randomly generated sequences to obtain sampled kullback leibler divergences 
the matrix of kl divergences is shown in table   for the
pca kde and table   for the autoencoder kde 
table    a matrix of kullback leibler divergences for
the pca   kde approach
hla
b       b       b       b       c      
b      
 
    
    
    
    
b      
    
 
    
    
    
b      
    
    
 
    
    
b      
    
    
    
 
    
c      
    
    
    
    
 

table    a matrix of kullback leibler divergences for
figure    example roc curves  for the same samples the auto   kde approach
shown in figure    top  and figure    bottom  
hla
b       b       b       b       c      
b      
 
    
    
    
    
b      
    
 
    
    
    
b      
    
    
 
    
    
b      
   
   
   
 
   
c      
    
    
    
    
 

figure    accuracy of svm methods by varying  
one  and two class  support vector machines
a simpler prediction model was created using svm 
figure    box plots of auc values for    fold cross val  the one class svm was trained on     of the experiidation results 
mental sequences  while the two class svm used both the
 

fithis dataset  pca tends to be more robust in this situation 
the svm models to predictably poorly as with the
experimental data and random sequences in the sine flu
virus peptide prediction  this may because of the high
dimensionality of the dataset and small sample size as
well 
pca kde in general appears to perform the best in
terms of prediction capability  in fact  it is very close to
the accuracy of predictions run through one of the leading peptide prediction algorithms  netmhc       however 
netmhc relies on a much larger database of peptide sequences of an older method  so it is not a completely fair
comparison 
future directions
i will try to use the set of sequences from the larger
databses of peptides to see if binding predictions can be
improved  these data however  as mentioned in the introduction  use a different method that cannot explicitly tell
whether a peptide had bound or not  at the very least 
recreating models with the larger database can shed light
on whether the newer deep ligand sequencing is on par
with the older information 
i am also looking into the accuracy of other simpler
baseline methods by using a combination of dimension
reduction and feature selection with svm or random
forests 
acknowledgements
i would like to thank curtis mcmurtrey and william
hildebrand for the sequencing data and information regarding the technicalities of deep ligand sequencing 
i also thank the parham and bustamante labs  in
particular  hugo hilton for regular discussions on this
project  and elena sorokin for suggestions on machine
learning approaches 
lastly  id like to thank john duchi and the tas of
stat      in particular  claire  for teaching this incredibly challenging and rewarding course 

    experimental sequences and     of the generated sequences  they were both tested with the unused     of
both sets of sequences  rather than test the overall accuracy of the results  i tested the sensitivity and specificity
rates of the test set  the results are plotted across various
values of  in figure   
prediction of viral proteome binding
to test the accuracy of all four predictions methods
on an actual dataset  i first chose the optimal likelihood
threshold using all available data  and chose optimal values of  for the svm models with all the data  to attain
the best overall accuracy  then  all possible  mers from
the swine flu virus strain were tested  the sensitivity and
accuracy of these predictions are shown in table   
table    prediction of h n  proteome binding
pca kde auto kde   svm   svm
sensitivity
 
 
 
 
specificity
     
                 
discussion
with regards to comparing the distributions of hla 
t sne appears to perform the best  even though it is
not an explicit clustering method  pca and autoencoding of the data do not show distinct clusters as well as
t sne  and an approach with k means in varying numbers of dimensions was unable to return any meaningful
classifications  this is likely because the data is sparse
and has correlations in higher dimensions  sampled kl
distances from the kde distributions does not necessarily
agree with the t sne interpretation of clusters  though it
is a very different metric to compare with  from t sne  it
would appear that there are three meaningful clusters of
the five hla   b        c          b        b        
and  b        
in my kde approaches  pca approach tends to fare
better than the autoencoder approach across the board 
this may largely be because of the small sample size of

references
    wahl a  et al         hla class i molecules consistently present internal influenza epitopes pnas          
       
    duong t  and hazelton m l         gapped sequence alignment using artificial neural networks  application to
the mhc class i system journal of nonparametric statistics           
    andreatta m  nielsen m        gapped sequence alignment using artificial neural networks  application to the
mhc class i system bioinformatics  feb               
    nielsen m  et al         reliable prediction of t cell epitopes using neural networks with novel sequence representations  protein science            

 

fi