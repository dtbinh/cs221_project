social network circle discovery using latent dirichlet allocation
jaimie xie
frank fan
stanford university
stanford university
department of
department of mathematical
computer science
and computational science
ffan  stanford edu jaimiex stanford edu

matthew kim
stanford university
department of
computer science
mdkim stanford edu

abstract
online social networks  such as facebook  provide a great interface for connecting with others  whether they are acquaintances or close friends  however 
there is no distinction made between different social circles  or clusters of friends
who share some common feature s   in
this paper  we explore ways to apply latent dirichlet allocation  lda   an unsupervised learning algorithm traditionally
used for topic detection in textual corpora  to automatically detect social circles
among a subjects friends  for each friend 
which we will consider as documents  we
take in account both the profile features
and users friends  comparable to word
tokens  finally  we will analyze our results by finding the cost minimizing assignment from our circles to the groundtruth circles  based on the balanced error
rate  ber  

 

introduction

as of august       facebook  an immensely popular online social networking service  had over
     billion monthly active users  the site allows
users to create a user profile and add other users as
friends  users can then categorize their friends
into lists  such as close friends  or people
from work  however  with the average facebook
user having about     friends  manually picking
out these friend circles becomes a laborious process  the purpose of our experiment is to explore
algorithms that can automatically detect these circles  so that users can more easily personalize to
whom they share their information  for example  a
user would most likely not want to share the same
information with acquaintances than with family
members 

figure    graphical interpretation of an ego network  the
large  red node signifies the ego node  the alters are colored
according to the ground truth circles they were placed in 

in order to detect these circles  we will consider
three sources of information  the users profile features  the users friends features  and the network
structure  in general  we want the friends in each
circle to share certain common features  such as
same school  same work  etc    and also have many
common friends within the circle   connectivity
within a circle can also provide information on
which groups are more casual and which are more
tight knit  
finally  we should also consider the possibility
of friends belonging in multiple circles  for example  someone who went to the users university
could also be a coworker  therefore  our problem at hand is not a traditional clustering problem 
where each example falls in one cluster  we will
address the multi cluster problem by using an algorithm that determines a multinomial distribution
of the circles for each of the users friends  we will
explain this algorithm in more detail in the latter
sections 

fi 

background

z

this section gives background information on the
methods that have been explored to solve the task
of social circle discovery  we will then discuss
latent dirichlet allocation  which is a method
from natural language processing that we will
apply to our task 
   

related work

mcauley and leskovec        developed a novel
method that builds a probabilistic model of an ego
graph based on connectivity between nodes  and
the circles that exist among the nodes  the circles
are treated as a latent variable in the optimization
of the likelihood of the graph  petkos et al        
used latent dirichlet allocation in social circle
discovery  but only used individual user features
and ids of neighbors in model training 
   

latent dirichlet allocation  lda 

for social circle discovery  we turn to latent
dirichlet allocation  lda   originally devised by
blei et al         for topic modeling in natural
language processing  this involves treating users
in a network as documents  and user features as
words  the primary hope is to produce a mixture model of social circles for each user  based on
their features  birthday  workplace  etc  not only
is this an intuitive extension of the field of linguistic topic modeling  lda proves to be a more timeefficient algorithm than traditional methods  while
achieving comparable results  petkos et al        
lda is a generative algorithm that views documents as mixtures of topics  with each topic being
a multinomial distribution of words  lda models
the production of each document in a corpus in the
following fashion 
   produce an n  poisson    the length of the
document 
   produce a   dirichlet     represents the
distribution of topics within a document 
   for each word wi in the document 
 a  produce a topic zi  multinomial   
 b  produce word wi  multinomial zn    
from this model  we can formulate the probability of a document  w 

p w       q

p

i i

i  i  



k
y


  n k v
yxy
j
i i   
 i ij  wn  d

i  

n   i   j  

since this formulation produces a loglikelihood maximization problem that is intractable  blei et al         propose an em
procedure for learning the model parameters 
furthermore  in line with work done by hoffman et al         and rahurek and sojka        
we utilize an online learning variant of lda for
this project 

 

experiment

in the previous section we discussed the lda
method  which we will now apply to the problem
of social circle discovery  this involves modeling
the users friends as documents  features as
words  and social circles as topics in the
documents  the features we used not only
included individual friends features  birthday 
workplace  etc    but also the ids of each users
friends to capture some sense of the connectivity
of the graph  we also added features which each
friend shares with the user ego node  concretely 
for each friend document  we have the feature
labels for each exhibited feature on his her profile 
the feature labels for each feature he she shares
with the user node  and the user ids of all the
friends that he she is connected to by an edge 
to evaluate the performance of our lda
algorithm  we also ran two variants of k means
clustering  using just the feature vectors of the
friends profiles  however  since in many cases 
the feature dimensions exceeded the number of
friends  we compressed the feature vectors using
tsvd  see section       
to summarize  we ran the following algorithms 
 lda using the network structure  users profile  and friends profiles  we set the number
of circles  k   the number of ground truth
circles  we will refer to this algorithm as
lda
 lda as above  except we set k using the
aicc selection algorithm described in section      this will be lda c
 k means clustering using only the compressed feature vectors of the users friends 
we set k   the number of ground truth

ficircles  we will refer to this algorithm as
kmeans
 k means as above  except we set k using the
aicc selection algorithm described below 
this will be kmeans c
   

parameter tuning

with the lda algorithm  it is vital to pick the
number of topics  which we will denote by k  to
model  we accomplish this with a stepwise procedure through a grid search of varying values of
k  to choose the model that minimizes the aicc
 petkos et al        

aicc    ll    p  

 p p     
n p 

where ll is the log likelihood of the model
with respect to the dataset  n is the total number of words across all documents  total number
of features summed across all users  and p  
k m       d k     is the effective number
of parameters  with k being the number of topics  circles   m the number of distinct words  features  and d the number of documents  users  
to obtain the log likelihood of a model  we
utilized the perplexity measure produced by the
online lda and used the following formula relating perplexity and log likelihood  blei et al 
      
ll c 
 
n
with c representing all the documents in a corpus
 i e   all users in a network  
the traditional aic criterion helps to choose
the model with the greatest likelihood  while penalizing models with large numbers of parameters
 which mitigates over fitting   the aicc criterion  however  also corrects for finite sample sizes
that are small with respect to the dimension of the
parameter space  hurvich and tsai        we see 
also  that as the number of words n in the corpus increases without bound  the third term of the
aicc drops out and we are left with the formula
for aic 
p erplexity c    exp 

   

baseline comparison

to set a baseline comparison  we decided to use
k means clustering  which is capable of assigning
each user to only one circle  so is expected to be
less robust than our lda algorithm 

figure    the number of circles predicted may not always
correlate with the ground truth number of circles 

to pre process the data before k means clustering  we used the truncated svd method  berry et
al         whereas most implementations of the
tsvd require one to specify the number of eigenvalues to keep  we use our own tsvd implementation  adapted with a rule proposed by leskovec
et al        which allows us to avoid humaninspection of the svd eigenvalues  for a drop off  
and to avoid hard coding the number of eigenvalues to keep 
 with our data matrix  with rows representing users and columns representing the feature values for each user  we formed the svd
 u v   
 by a rule of thumb  leskovec et al        
we truncated the maximum number of eigenvalues such that  the sum of squared remaining eigenvalues is at least    of the sum of
squared original eigenvalues  this allows us
to capture a good amount of the variation
in the data  while reducing uninformative dimensions 
 lets say we truncated the svd to the m
largest eigenvalues  our tsvd is then denoted by  um m vm   to form the dimensionreduced dataset  we simply take um m  
   

data

the data that we will use for training testing
is provided by the stanford network analysis
project  and all of our data comes from facebook 
the data is divided into ego networks  which
consists of the ego node  all of the nodes it is
connected to  called alters   and all of the edges
there may be among these nodes  within each ego
network  we have the following 

fifigure    the average ber scores across the four algorithms that were tested  ber scores are obtained by subtracting the average ber error from    so a higher score means
better performance 

 circles  these are the circles that the user
manually chose  the ground truth circle  the
circles are not necessarily disjoint  so one
user can be in multiple circles  we will compare our results with these sets of users 
 edges  this contains every edge in the ego
network  other than the implicit edges that
connect each alter to the ego node  an edge 
 n    n    signifies that alters n  and n  are
friends on facebook 
 features  for each alter  we are given a binary array  where a   in index i signifies that
feature i is satisfied  and   otherwise   the
features are constructed in a tree structure 
where example features include 
 education university stanford
 education university harvard
 education year       etc 
 feature names  this contains the names of the
features that correspond with the feature arrays  in general  we will just use the numerical labeling of the features 
   

results analysis

after running the lda algorithm  we get the
multinomial distributions of the circles for each of
the friends in the ego network  at this point  we
can choose a cut off probability to choose which
circles each user actually should be assigned to 
for example  if a user is assigned a probability
      of being in circle a  then this user is likely
not actually in circle a  in choosing this cut off

probability  we have to consider how many circles
we are actually predicting  let n be the number
of circles we predicted  then we will place user u
in circle c if p r u  c      n
in our k means algorithm  each user was automatically labeled into one circle 
once we have established these circles  we want
to be able to directly compare the automatically
produced circles with the ground truth circles 
which are the circles that the ego user manually
chose  to do this  we must determine an optimal
mapping from our circles to the circles which the
ego user hand picked  first  we need to determine
some error cost function which we would like to
minimize  for the purpose of our experiment  we
used the balanced error rate  ber   as did petkos
et al         if we let c    c    c         ck  
be the set of automatically produced circles  and
c    c    c         ck   be the set of ground truth
circles  then  we can define the ber as 
 
ber ci   ci    
 

c

 ci  ci    ci  cic  
 
c
 ci  
 ci  

 

the ber cost function equally weights the fraction false positives and false negatives  if we
compute the ber for every pair  ci   cj    we can
construct the cost matrix where the ij  th entry
is ber ci   cj     note that since the number of
circles which we predicted does not always match
the number of truth circles  our cost matrix is not
always a square matrix  the number of matchings
that we will get in this case will be min  c    c   
we want to find a circle matching f   c  c 
which gives us the least total error  if we were
to try every possible f and then compute the cost 
this would take o n    however  with the kuhnmunkres algorithm  we can solve the assignment
problem in o n    time 
for our final ber score  we take the average of
the ber rates from each circle assignment  then
subtract that from one 
x
 
berf  
    ber c  f  c   
 f   cdom f  
for each algorithm we average the berf values
from each of the    ego networks  we get the following results  kmeans reports a ber score of
      kmeans c obtains a score of       lda
a score of       and lda c a score of      
for both our k means and lda algorithms  we
achieved better results when we predict the number of circles using aicc   rather than just setting

fik   the number of ground truth circles  this is
because in the latter case  we are overfitting the
data   in one of the networks  for lda   using our
predicted number of circles  k     rather than
k       improved the ber score from      to
      many of the ground truth circles only contained     people  and by abstracting away these
circles  we actually got better results 
another surprising result was that the k means
algorithms  which used only the feature vectors of
the users friends  but did not consider the network structure or users own profile features  did
better than the lda algorithms  which considered
all three components  however  our implementation of the lda algorithm places larger weight on
the network structure because most of the users
friends have many more connections within the
ego network than  s in their feature vectors  since
we are treating each connection as a word in the
documents  the users friends   the documents will
largely be composed of network structure  this
implies that profile features  even using the compressed vectors  may tell us more about circle formations 

references
berry  m  w   dumais  s  t   and obrien  g  w       
using linear algebra for intelligent information retrieval siam review                 
c  hurvich and c  tsai        regression and time
series model selection in small samples  biometrika 
                 
david m  blei  andrew y  ng  michael i  jordan 
      latent dirichlet allocation  journal of machine learning research             
matthew d  hoffman  david m  blei  and francis
bach        online learning for latent dirichlet
allocation  advances in neural information processing systems     nips       
julian mcauley and jure leskovec  learning to discover social circles in ego networks  proc c of nips 
leskovec  j   rajaraman  a   and ullman  j  d  mining
of massive datasets  cambridge university press 
     
pedregosa et al  scikit learn  machine learning in
python jmlr     pp                  
georgios petkos  symeon papadopoulos  and yiannis kompatsiaris        social circle discovery in ego networks by mining the latent structure of user connections and profile attributes 
ieee acm international conference on advances
in social networks analysis and mining 
radim rahurek and petr sojka        software
framework for topic modelling with large corpora  elra 

figure    lda performs better for most trials when we predict the number of circles using aicc selection  rather than
using the number of ground truth circles 

 

future work

we hope to explore other combinations of features to include in our lda model  such as interaction terms and indicators of edge strength 
we also hope to devise ways to factor networkconnectivity into our model building without having it overwhelm the other features present  another area of work would be to explore parameter
tuning with bic and aic and compare results
with aicc selection to verify our theoretical decision to use aicc  

fi