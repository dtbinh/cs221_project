automated transcription of guitar music
carl fredrik arndt

lewis li

institute for computational   mathematical engineering
stanford university
stanford  california      
email  cfarndt stanford edu

electrical engineering
stanford university
stanford  california      
email  lewisli stanford edu

abstractin this report  we present our methodology of analyzing
music to extract and transcribe guitar notes  our approach is divided
into three steps  the first isolates the guitar track from the music using
independent subspance analysis  isa   meanwhile  the second step uses
both frequency and time domain based approaches to transcribe the
guitar track into the corresponding notes  a duration detection algorithm
is then used to isolate the length of each note  we will apply this
methodology to a series of test data ranging from diatonic scales to
segments of classic rock songs  one important distinction is that we do
not attempt transcription of guitar chords  focusing only on monophonic
scores 

i  i ntroduction
a  motivation
the concept of automated music transcription  amt  has been
extensively studied in recent years  the premise is the autonomous
translation of digital audio waveforms into a symbolic music representation akin to what would be on a score  this can be almost
viewed as a dis assembler routine for acoustic music  such a program
would have widespread applications  allowing the modification  and
re arrangement of music  in addition to providing an easier platform
for aspiring musicians to learn how to play specific songs  from a
different perspective  music transcription is presently performed by
trained humans  the development of an effective automated transcription algorithm could provide insight into complicated processes of
human perception of music  another application could be improved
identification of musical genres through the analysis of the notes rather
than just the frequency signature  consequently  the possibility of
automated music transcription has piqued the interest of individual
in a wide variety of fields 
b  prior and related work
with the increase in computing power  research into amt has
grown substantially over the past decade  a closely related field is
that of automated speech recognition  which has enjoyed considerably
greater research efforts  mainly due to the commercial implications
of such products  real time speech recognition and natural language
processing has now entered the mainstream with products such as siri 
while amt lags behind in research exertion  many of the ideas from
speech recognition can be borrowed      
music is separated into two main categories  monophonic and polyphonic  the former refers to music where no more than one note
is simultaneously being played  while latter describes music where
no such constraints are defined  transcribing monophonic music has
been studied previously in      the basic steps require disseminating
between silence and noise  determining a pitchs tone  and finally
mapping to an appropriate note  a variety of techniques such as zerocrossing      or auto correlation have been shown to provide accurate
results       alternatively  frequency domain methods such as harmonic product spectrum  hps        and cepstrum      have shown

comparably favorable results  conversely  automated transcription of
polymorphic music is a much more challenging problem as many
spectral peaks arise in the frequency domain corresponding to the
various harmonies and fundamentals being played       a number
of recent phd and masters thesis have been dedicated to addressing
this problem  the most recent general approach has centered upon
non negative matrix factorization       approaches such as     have
used prerecorded note samples to build a library of note templates 
which incoming notes are then mapped to  despite such efforts  no
general purpose system has been developed that can be on par with
a trained human musician  generally  constraints such as the drums
and percussive instruments are not allowed  or the number of other
instruments must be known at run time 
a related problem is that of source separation  commonly referred to
as the cocktail part problem  this is far from being a solved problem 
but approaches such as principal component analysis  pca  together
with independent component analysis  ica  have demonstrated
success on simplified problems       there have been attempts to
use pitch detection to aid in source separation       but very limited
proposals to combine the two to transcribe polyphonic music 
c  goals
this project intends on developing an implementation that combines
the two aforementioned techniques to provide an attempt at amt 
obviously  the development of a general purpose amt is not possible 
thus a secondary goal is identify the constraints under which such
a system would work  it is well known that chord recognition is a
very challenging problem       hence the scope of this project will
focus on single guitar notes  furthermore  it is known that delays and
echos makes source separation very difficult       consequently  this
will complicate separating tracks where electric guitar effects such as
flanger  reverb  and chorus are used  in the following sections  we will
present out methodology for track separation and music transcription 
describe the method of which we collected test data and present and
discuss the experimental results before delineating any future steps 
ii  m ethodology

fig    

overview of methodology

music is consisted of notes  each with characteristics such as
pitch  duration  loudness and timbre  pitch refers to the perception
of how high or low a note is  notes are classified into sets of

fi   semi tones  termed an octave  pitch is determined primarily by the
fundamental frequency of the note  table ii shows the fundamental
frequencies of the notes a guitar with standard tuning is capable of
producing  duration refers to the temporal length of each note which
is measured in beats  loudness is measured by the amplitude of the
waveform  finally  timber describes the color of a note  which
allows humans to desiminate between different instruments playing
the same note  in reality  notes are comprised of the fundamental
frequency f  as well as harmonics  frequencies manifesting at integer
multiples of f    the initial step of our methodology uses pca and ica
to isolate guitar tracks  the preceding phase demarcate the portions of
the track corresponding to rests  allowing us to evaluate the duration
of each note  this involves extracting features of the waveform and
applying an appropiate threshold  the penultimate step estimates the
pitch of each individual note using both time and frequency domain
techniques  finally  we match the estimated pitch to table ii  and
produce a musical instrument digital interface  midi  version of our
initial track to verify correctness 
octave
e
f
f 
g
g 
a
b 
b
c
c 
d
e 

 

 

 

 

 

     
     
     
     
     
     
     
     
 

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

    
    
    
    

table i
f undamental frequencies of notes that a guitar can produce with
standard tuning    

p



components used  this means that pni   ii    here i tea the singular
i  
values  needs to be greater then some given number  chosen to be    
in      
   independent feature creation  if we group all the  vectors
from previous step into a   n observation matrix v  we can now
use the standard case ica on this  this will create an independent set
of feature vectors zi      i   
   subspace identification  to create clusters containing separate
instruments  we will use the kullback leibler divergence  first calculate the so called ixegram matrix  defined in      as



d 




  
 

kl  z    z  
kl  z    z  
  
 

kl  z   z    kl  z   z      

kl  z   z  

kl  z    z   
kl  z    z   
  
 

kl  z    z   
kl  z    z   
  
 




 


here kl  zi   zj   denotes the kullback leibler divergence between
zi and zj   since the kullback leibler divergence is calculated using
a distribution function  and we only have access to a samples  these
has to be approximated numerically  next introduce the cost function

h m   d   


x
c  

 x

x

 
p

j  

mjc

mic mkc dik  

   

i   k  

here m is a    binary valued matrix where  is the number
of clusters and mi j denotes whether signal i is in cluster  or not 
note that the number of clusters is prespecified  so we have to run
this part of the algorithm multiple times to approximate right number
of clusters  to find m for a fixed k we minimize the cost function     
this is be done by using a deterministic annealing algorithm  basically
a fixed point algorithm  which solves

a  track separation
the track separation will be done by a method called independent
subspace analysis  isa  which was proposed in      isa uses ica
create number of different signals  where each signal is matched to a
source  instrument  
we here look at a setting where we assume that the number of
sources in the audio signal is stationary  but the extension to the
non stationary case is trivial  we now describe all the steps of isa
separately 
   divide signal  if we are given an audio signal x we split
this signal into m separate pieces of length w  here the first piece
corresponds to the first part of the song and etc  denote each of these
pieces xk      k  m and put them into a w  m matrix x 
   spectral transformation  since we are interested in the spectral
properties of the signal we apply a linear spectral transformation  in
out implementation we used the short term fourier transform  to the
signal  for each xk   this means that we are using a linear operator m
 an n  w matrix where n is the number of spectral components  and
find a representation sk such that xk   m sk   a n  k matrix s in
then created in the same way as x 
   feature extraction  to extract features from the one dimensional
signals we apply the svd to s  so that s   u v t   here the v vectors corresponds to our principal components features  to determine how many principal components to use  we put a lower threshold
on the information ratio  assuming  is the number of principals

p

j   mj  dj k
p
d

i k
k  
  n
m
j 
j   j  i
p
i   
 
    j   j  i mj 
 exp i   t  
mi    p
 
   exp i   t  

p



where t a variable parameter  this is described in      
   retrieve signals  in the last step we simply have to retrieve
the  audio signals of the different instruments  this is done by first
inverting the ica step by using the mixing matrix  then inverting
the svd and at last using the inverse transform of the spectral
transformation 
b  duration detection
silence detection is a commonly studied problem in voice activity
detection  vad        and we will borrow some of the commonly
used audio features to detect rest beats in music 
   short term energy  the short term energy is defined as 
xi  

m
x

x   m w   n  m 

m  

where w refers to a hamming window  the short term energy gives a
representation of the temporal amplitude variation  this is perhaps the
best feature for detecting periods of silence in a an audio sequence 

fi   zero crossing rate  the zero crossing rate is defined as 
zi  

m
x

     sgn x m    sgn x m      w n  m 

m  

zero crossing measures the noisiness of a signal  and has been used
to discern between music and other noises  background  vocals etc 
   

the original spectrum  this will eliminate high frequency harmonics 
while magnifying the fundamental frequency  figure   gives a visual
representation of this algorithm  a few iterations of the downsamplemultiply procedure will yield a spectrum that has a maxima at the
fundamental frequency 

   spectral flux  the spectral flux is defined as 
f li  

m
x

 exi  m   exi   m   

m  

where exi is the normalized dft coefficients of the ith window 
this measures the local spectral change which is notably higher due
to noise during silence 
   spectral centroid  the spectral centroid is defined as 
p
mx m 
cn   pm 

m  x m 
here x refers to the dft of x  the spectral centroid gives a metric
for the brightness of the sound      
   weiner entropy  the weiner entropy is defined as 

 p
m  
 
exp m
m   ln x n 
wi  
pm  
 
m   x n 
m
weiner entropy is used as a metric for determining if a sound is more
tone like or noise like     
   thresholding  with these features  it is possible to train a
support vector machine  svm  or another other classifier to label
portions as noise or music  however  it has been shown that much
simpler techniques can be employed to provide excellent separation
      we adapted the following steps from     
   compute histogram of feature values
   detect histogram local maximas
   m 
   compute threshold t   w m
  where w is a user defined
w   
weight  and m  and m  are the first and second maximas 

fig    

overview of harmonic product spectrum

   cepstrum  cepstrum is another fourier analysis technique
which involves evaluating the logarithmic amplitude spectrum of the
signal that is 
c    f log x   
assuming this log amplitude spectrum contains many regularly
space harmonics  the fourier analysis will yield peaks corresponding
to the fundamental frequency  in essense  the agorithm searches for
periodicity within the spectrum 
the cepstrum has units of quefrency  and peaks within the cepstrum
are known as rahmonics  these correspond to periodicies within the
spectrum  therefore  to obtain an estimate of f    we search for
peaks within the quefrency regions corresponding to fundamental
frequencies of notes a guitar can produce  quefrency and frequency
are inverses of one another 
   auto correlation  auto correlation is the most commonly used
time series pitch detection algorithm in speech recognition      the
idea relies on the fact that the correlation signal will have a peak at
the lag corresponding to the pitch period  the autocorrelation for a
discrete signal is given as 

r k   

n
x

k   s m s m   k 

m  

we will use a combination of the features defined above and a
smaller window size    ms to   ms   since rests in music are generally
shorter than that of human speech 
c  music transcription
once the tone portions of the track have been isolated  a pitch
detection algorithm can be deployed to compute the notes themselves 
pitch detection is also a common problem in voice recognition and a
variety of algorithms have been proposed      as there is no overall
consensus of an ideal pitch recognition algorithm  we have elected to
implement the a few of the most popular  this can offer us insight to
any causable relationships between dataset and an optimal algorithm 
   harmonic product spectrum  harmonic product spectrum
 hps  is a frequency domain algorithm       the basic idea is to take
the fourier transform of a windowed portion of the waveform  the
windowing function that was proposed was the hamming window 
ideally  this will yield a set of peaks at the fundamental frequency and
its associated harmonics  f  can be abstracted simply by finding the
greatest common denominator of peak frequencies  a computationally
efficient method of evaluating this is to down sample the spectrum by
an integer factor  and then computing the element wise product with

here n refers to the number of samples in the window  and k is the
lag index  the choice of n is important in the sense that it must cover
  or more periods such that periodicity can be seen  conversely  a
larger window will dimish the algorithms capability to detect temporal
variations in pitch  now to find the fundamental frequency we use the
following 
f   

 
kmax fs

  r kmax     arg max r k 
k

here fs is the sampling frequency  the key assumption is that if
s m  is periodic  r k  should exhibit peaks at integer multiples of
the period  the main peak in the autocorrelation function will be at
k      since a signal should always correlate to itself   the location
of the next peak gives an estimate of the period  the method usually
require a number of periods of data to form a reliable estimate  and
thus some averaging of the frequency signal is unavoidable 
d  post processing   midi synthesis
pitch detection and duration detection will both yield a vector
corresponding to the number of windows  the pitch detection vector

fiwill contain the estimated fundamental frequencies  by finding the
closest match for each frequency in table ii  we can obtain a vector
of semi tone notes  this vector is then multiplied element wise by the
boolean vector generated by duraction detection to yield the estimated
transcription  where rests correspond to   frequency  to generate an
audio waveform  we transformed the appropiate notes into midi notes 
allowing for playback 
iii  data c ollection
for the intial testing of pitch detection and duration detection 
we recorded monophonic guitar tracks using ijam and apples
garageband  the test cases were comprised of a e major scale 
the introduction to led zeppelins stairway to heaven  and guns
nroses sweet child o mine  to test the track seperation we used
red hot chilli peppers otherside  which features drums  guitar  bass
and vocals 
iv  e xperimental r esults
a  track separation
we ran the previously described algorithm on approximately   
seconds of the initial part of otherside  this part of the song only
contains a bass guitar and a regular guitar and thus we could set
the number of clusters    to be fixed to two  after doing this we
transformed the received clusters back to wav files and played them 
one could clearly here which part contained the bass note versus the
regular guitar  in figure   we show the initial wav file and then both
the first  regular guitar  and second  bass  clusters  the main problem
with the algorithm is that it is build to separate pitched instruments
and thus fail to separate the song from the instruments as noted in
    

fig     duration detection algorithm performed on stairway to heaven  a  short
time energy with median smoothing  b  spectral centroid with median smoothing  c 
duration detection result  d  waveform of the original track 

c  music transcription
we attempted music transcription using hps  cepstrum and autocorrelation on the various sets of test data  for our selected test cases 
hps appeared to yield the best results 
   e major scale  this was a simple test case where we played
  notes that are part of a e major scale  all three of the algorithms
were able to recognize all notes as shown in figure   

fig     time series plot of the original wav file top   separated guitar part middle 
and bass guitar part bottom  

b  duration detection
we attempted duration detection using permutations of the features
described in section ii b  it was found that short time energy and
spectral centroid provided acceptable results  while additional features
did not necessarily yield any improvements  figure   shows the
features after a median filter has been applied  the bottom two plot
indicates the periods during which a note is being played compared
to the actual waveform  as we can see  the duration detection is
reasonably successful at identifying the rests and notes 

fig     pitch detection result combined with duration detection for e major scale
using hps

   stairway to heaven  stairway to heaven is an slow tempo song
 approximately   bpm in     time   we recorded a version that was
strictly monophonic  all three algorithms generated small incorrect
spikes in frequency at the begining of each note  this can be rectified
by using a median filter of   elements 
   sweet child o mine  sweet child o mine is a fast tempo
song      bpm   that was originally played by guns n roses using an
overdriven electric guitar  for the purpose of this project  we recorded
a version played using a clean guitar  the output is shown in figure
  

fifig    
mine

pitch detection result combined with duration detection for sweet child o

d  summary

song
e major
stairway
schild
oside
sandman
fad blk

notes
 
  
  
  
  
  

hps
 
 
     
     
     
     
      
     

cepstrum
 
 
 
   
  
  
  
  
  
  
  
  
  
  

auto corr
 
 
 
   
  
  
  
  
  
  
  
  
  
  

in our test cases  we were able to demonstrate over     success
rates using hps  it should however be noted that we selected our test
cases in order to match the capabilities of our algorithms  that is 
monotonic tracks with no chords  in order to cater to all songs  this
restriction will have to be lifted 
v  f uture w ork
in this project  we have demonstrated that isa and hps can be
used to achieve music transcription at least on select test data  the
greatest challenge is the transcription of chords  bayesian approaches
such as       and generated success rates of over     and could be
adapted to serve an appropiate purpose here  furthermore  to generate
an actual score the beat of the music will have to be determined 
certain attempts such as      have also achieved around     success
rates  once the tempo is established  it is simple to transform the
durations into note lengths  finally  certain aspects of a score such
as dynamics should also be detected  the combination of all these
components would result in a complete automated music transcription
program 
acknowledgements
the authors would like to thank prof  andrew ng and the cs   
teaching staff for their machine learning course 
r eferences
   
    juan pablo bello  giuliano monti  mark sandler  and mark s  techniques
for automatic music transcription  in in international symposium on music
information retrieval  pages            
    m  a  casey and a  westner  separation of mixed audio sources by independent
subspace analysis  in proc  int  comput  music conf   july          
    patricio de la cuadra and aaron master  efficient pitch detection techniques for
interactive music  in in proceedings of the      international computer music
conference  la habana       
    s  dubnov  generalization of spectral flatness measure for non gaussian linear
processes  signal processing letters  ieee                  aug       
    theodoros giannakopoulos  silence removal in speech signals       

    fabien gouyon  francois pachet  and olivier delerue  on the use of zero crossing
rate for an application of classification of percussive sounds  in proceedings of
the cost g   conference on digital audio effects  dafx          
    john e  hartquist  real time musical analysis of polyphonic guitar audio  masters
thesis  california polytechnic state university       
    w j hess  pitch and voicing determination  advancesin speech signal processing       
     t  hofmann and j  m  buhmann  pairwise data clustering by deterministic
annealing  ieee t  pattern anal  pages           
     aapo hyvrinen and erkki oja  independent component analysis  algorithms and
applications  neural networks                  
     m  karjalainen and t  tolonen  multi pitch and periodicity analysis model for
sound separation and auditory scene analysis  in proceedings of the acoustics 
speech  and signal processing        on      ieee international conference
  volume     icassp     pages         washington  dc  usa        ieee
computer society 
     tomi kinnunen  evgenia chernenko  marko tuononen  pasi frnti  and haizhou
li  voice activity detection using mfcc features and support vector machine       
     anssi p  klapuri  automatic music transcription as we know it today  journal of
new music research  pages              
     gopala krishna koduri  joan serr  and xavier serra  characterization of
intonation in carnatic music by parametrizing pitch histograms  in proceedings
of the   th international society for music information retrieval conference 
porto  portugal  october            http   ismir     ismir net event papers 
    ismir      pdf 
     philip mcleod  damon simpson  robert visser  mike phillips  ignas kukenys 
yaoyao wang  arthur melissen  natalie zhao  and all the  fast  accurate pitch
detection tools for music analysis 
     m  h  moattar and m  m  homayounpour  a simple but efficient real time voice
activity detection algorithm       
     c  panagiotakis and g  tziritas  a speech music discriminator based on rms and
zero crossings  ieee transactions on multimedia                 
     barak pearlmutter and lucas parra  a context sensitive generalization of ica 
     
     l  rabiner and r  schafer  digital processing of speech signals  englewood
cliffs  prentice hall       
     matti ryynnen and anssi klapuri  transcription of the singing melody in
polyphonic music  in in proc   th international conference on music information
retrieval       
     eric d  scheirer  tempo and beat analysis of acoustic musical signals  journal
of the acoustical society of america                      
     emery schubert and joe wolfe  does timbral brightness scale with frequency
and spectral centroid 
     alexander sheh and daniel ellis  chord segmentation and recognition using
em trained hidden markov models       
     paris smaragdis and judith c  brown  non negative matrix factorization for
polyphonic music transcription  in in ieee workshop on applications of signal
processing to audio and acoustics  pages              
     christopher wendt and athina p  petropulu  pitch determination and speech
segmentation using the discrete wavelet transform  in in proceedings of the ieee
international conference on acoustics  speech  and signal processing  pages   
         

fi