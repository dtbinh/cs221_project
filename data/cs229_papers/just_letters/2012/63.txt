identifying low quality youtube comments
alex trytko and stephen young
cs    final project   fall     
youtube provides an unparalleled platform for sharing and viewing video content of every
imaginable nature  however  the user experience is tempered by the erratic quality of discourse
manifested in the comments posted in response to videos  unfortunately  the anonymity of the internet
seems to foster a preponderance of highly offensive or derogatory comments which serve little apparent
purpose other than to intentionally provoke and upset other users  youtube combats such comments via
a voting system whereby a comment is hidden from view once it has received a certain number of
downvotes  users can only see the contents of these comments by specifically requesting to do so  this
feedback mechanism allows the youtube community to collectively decide which comments are
offensive  inflammatory  or otherwise low quality  thus elevating  however marginally  the average level
of discourse 
a drawback to youtubes current comment hiding strategy is the unavoidable delay between the
time when a low quality comment is posted and when it accrues enough downvotes to be hidden  in this
project  we employ machine learning concepts to classify youtube comments without relying on human
involvement  we train and test three different algorithms toward this end  multinomial naive bayes  a
support vector machine with a linear kernel  and logistic regression  it is easy to see how classifiers like
the ones used in this project can help to reduce  or even eliminate  this latency  even if it is undesirable
to show or hide a comment based solely on the output of the classifier  the predictions can be used to
indicate how predisposed a comment is to being low quality  and adjust the downvote threshold for that
comment accordingly  if adopted  this approach has the potential to significantly improve user experience
at minimal cost to youtube 
data acquisition
data acquisition was a much greater hassle than we had originally anticipated  while google does
provide apis to access youtube data  indeed  we used these apis for some steps of the data acquisition
process   they do not provide information about the individual ratings of comments  therefore  our only
recourse was to scrape this information directly from youtube comment pages  however  the structure
of youtube comment pages made this task non straightforward  hidden comments  i e   the ones in
which we were interested  do not appear in the page until the user clicks a show the comment link 
which issues an ajax request to retrieve the comment and inserts it into the page  to circumvent this
obstacle  we used a nifty third party python package called selenium to automatically expand all hidden
comments before scraping all of the comments from the html document  a major downside of using
selenium is inefficiency   it takes on the order of minutes to extract comments from a single comment
page  unfortunately  selenium would fail  seemingly at random  to expand some comments in a manner
that prevented all subsequent hidden comments on the page from being expanded  after several failed
attempts at workarounds  we resigned ourselves to simply discarding all comments after the initial point
of failure while processing a comment web page 
a final  and particularly vexing  difficulty with data acquisition was youtubes inconsistency in
displaying hidden comments  comments can be displayed in two contexts  either as a top level comment
or as the parent of a reply to that comment  in some cases  a comment marked as hidden in the latter
context will not be marked as hidden in the former context  this behavior is obviously problematic

fibecause it implies that the same comment could exist twice in our dataset with different labels  to
address this issue  we added a post processing step wherein we eliminated all duplicate comments for a
particular video  keeping the positive label in the event of label disagreement  that is  if the same
comment appeared as both visible and hidden  we labeled it has hidden 
we performed some standard pre processing operations before feeding the data into our algorithm 
first  we strip out all hyperlinks from comments  we then convert all words to lowercase  remove
punctuation  and drop words that are one character long  we then apply the porter stemming algorithm to
each remaining word in order to reduce similar words to their common roots and avoid treating words
with the same effective meaning as separate features  we also selected a conservative set of stop words
  the    to    is    it    and    on    of    in   and discarded all occurrences of those words in our dataset  as well as
all numbers 
spell correction is another preprocessing step commonly applied in text classification problems 
we decided against performing spell correction because misspellings may themselves be indicative of
comment quality  and hence valuable to keep separate from the correct versions  running spell correction
on a poorly written comment may drastically alter the overall quality of the comment  and may make a
difference as to whether or not readers would decide to downvote the comment 
dataset summary statistics
videos 
   
comments  total         
comments  hidden      
feature selection
as with any text classification problem  not every word in our dataset is likely to be a useful
feature with respect to predicting comment quality  first  some words simply do not occur often enough
to provide any indication of one class or the other  because youtube comments are rife with
misspellings and colloquialisms  our dataset contains a high proportion of rare terms   of the       
unique words in our dataset  over        only occur once  therefore  our first feature pruning step was to
disregard all words that occurred below a certain threshold value  hereafter minimum word frequency 
or mwf  number of times 
another feature pruning technique focuses on words with similar conditional class probabilities 
i e   words that occur with similar frequency in visible and hidden comments  for algorithms that assume
feature independence  i e  naive bayes   eliminating such words could reduce noise and improve
classification performance  we do so by computing the probabilities of a word given the positive class
and given the negative class  dividing the larger by the smaller  and disregarding the word if the quotient
falls below a certain threshold value  hereafter minimum conditional probability difference factor  or
mcpdf  
note that throughout this paper  we regard mwf and mcpdf not as constants but as configurable
parameters  we tried to be conscientious about not over aggressively pruning the feature set by setting
these values too high  which could result in spuriously high performance  we discuss this problem more
thoroughly in the section entitled a caveat about feature selection 
a note about performance metrics

fibecause our dataset features a heavily skewed class distribution  only       of all comments
were hidden   raw classification accuracy is not a terribly useful indication of performance  for example 
consider a dumb classifier that blindly gives all comments the negative label  this classifier would have a
      success rate  despite being obviously useless  therefore  when discussing classifier performance 
we usually refer instead to auc  which denotes the area under the precision recall curve  auc is
bounded between   and    note that the aforementioned dumb classifier yields auc of        which
properly reflects its true performance 
furthermore  all performance values are calculated using k fold cross validation  usually with k  
   
multinomial naive bayes
an obvious first approach to any text classification problem is multinomial naive bayes  we
elected to implement the multinomial  as opposed to multivariate  variant  this variant takes term
frequency into account  without feature pruning  i e   with mwf and mcpdf both set to     naive
bayes performed very poorly with an auc of         not much better than the dumb classifier would
have done  the following table demonstrates the changes in performance  measured in auc  of our
naive bayes classifier as we adjust the value of the mwf and mcpdf parameters  the grayed out
entries correspond to parameter values that resulted in over aggressive feature pruning  which we define
as a feature set containing fewer than     words  recall that the full feature set has over        words  
mwf

 
 
 
 
 
 
 
 
 
  
  
  

mcpdf
 
        
        
        
        
        
        
        
        
        
        
        
        

 
        
        
        
        
        
        
       
        
        
        
        
        

 
        
        
        
        
       
        
        
        
        
        
        

 
        
        
       
        
        
        
        

 
        
        
        
        
        

 
        
        
        
        

 
        
        
        

note that performance generally increases with both parameters  indicating that mwf and mcpdf are
valuable techniques for selecting features 
a caveat about feature selection
unfortunately  we did not realize until very late in the project that we were performing feature
selection at the wrong stage of the training testing process  we prune the feature set with respect to the
entire dataset  then separate the dataset into a training and test set  instead  we should have have

fiseparated the dataset into training and test sets first  and pruned the feature set only with respect to the
train set  this ensures that we are testing on examples for which we have absolutely no prior knowledge 
the consequence of this oversight is that  for very high values of mcpdf  we are essentially
eliminating all terms except those which happen to only show up in one class in our dataset  this would
grant our classifier a large advantage because  when calculating conditional probabilities for each term in
a test example  it will be much more likely  and perhaps only have the choice  to classify the example
correctly  it is as if we are doing a certain learning over the entire data set with this pruning 
we acknowledge that  rather than feature pruning on the entire dataset and then doing k fold cross
validation  we should do the pruning within the k fold cross validation  over only the portion of the data
we are training on  so as not to implicitly learn anything unfairly about the testing data set  unfortunately 
this change would consist of a fair bit of restructuring of our entire process  as we would have to move the
k fold cross validation logic out into our python script  we tried putting the pruning logic in our matlab
code  but without data structures such as maps  our pruning step over such an enormous input matrix ran
oppressively slowly 
the purpose of including this discussion at such length in this report is that we want it to be
understood that we recognize the unfortunate implications of the way we implemented feature pruning 
and that we acknowledge that the steps should have been done in a different order  however  we do try to
avoid exploiting any significant artificial performance gains resulting from this error by not using
extremely large values for this mcpdf  in doing so  hopefully we reap the intended benefits of this
pruning technique  i e   reducing noise stemming from features with similar conditional class
probabilities  without reaping the unearned advantage described above  however  it is difficult to know
how much of the performance gains in our findings and results are due to calming of noise  and how
much are due to this mentioned implicit learning on the entire data set 
experiments with other algorithms
in addition to naive bayes  we applied a linear svm and a logistic regression classifier toward
our problem  unfortunately  neither yielded promising results  the svm exhibited poor performance for
low values of the mcpdf and mwf parameters  when we used very high values for mcpdf  we
achieved near perfect performance  however  these mcpdf values corresponded to extremely limited
feature sets  which lead to spurious performance results as discussed in the caveat above  in fact  these
were the results which first really made us aware of this issue  furthermore  unfortunately the svm is not
more honed  because we believed ourselves to be making forward progress with the nb classifier  but
didnt realize until too late that the pruning we were doing  and the gains we were seeing  may not have
been valid 
the following graph shows the performance of our three classifiers with values for mwf and
mcpdf that work toward optimizing performance without over aggressively pruning the feature set 
while none of the three exceed the level of performance corresponding to human classification  as
indicated by the magenta dot   naive bayes comes fairly close and all three perform better than random
guessing  which would be represented by a horizontal line with precision equal to the positive class prior 
or       

fidifficulty of the problem   conclusions
one very important thing to keep in mind is that classifying youtube comments is inherently a
very difficult problem  even a human  given an unlabeled comment  cannot consistently predict whether
it would be downvoted heavily  thus  it is unrealistic to expect this algorithm to perform as well as in
canonical text classification problems such as spam classification 
to illustrate the difficulty of the problem  we estimated human classification performance by
subjecting ourselves to the task of classification  we had a script give us a set of n random examples 
where a certain percent of the examples  between   and    percent  were positive examples  the reason
for making sure to give such a percentage of positive examples was that  as we are humans  we could not
go through all         comments looking for positive examples  and it was likely that a randomly chosen
    or     would not contain any positive examples otherwise  because of the skew of the data  note that
this meant that the probability of seeing a positive example was much higher for our human testing   if
anything this should give our human precision results an advantage  however  we still only saw low
numbers for precision and recall  an average of     precision and       recall  
another factor that contributes to the difficulty of this problem is that whether a comment will be
hidden is not merely dependent upon the content of the comment  after all  human voting behavior is
nondeterministic   a given person on a given day may or may not decide to downvote a comment 
regardless of how inflammatory the comment is  thus  the exact same comment posted in response to the
same video at two different times might be hidden in one case but not the other 
although our classifiers did not exhibit stellar performance  they could still serve a purpose in
real world application  youtube is clearly not likely to block comments based solely on an algorithmic
prediction  given the difficulty of accurately classifying comments  it is important to err on the side of
caution   i e   high precision is more important than high recall  thus  the service that the algorithm
provides is not necessarily to classify comments outright  but to provide a signal of comment quality that
could be employed in a more sophisticated vote based hiding mechanism  for example  rather than
specifying an absolute voting threshold below which a comment is hidden  youtube could maintain a
dynamic threshold for each comment based on the algorithms predicted label  this could result in truly
bad comments being hidden more quickly  thus reducing exposure to low quality comments and
improving overall user experience 

fi