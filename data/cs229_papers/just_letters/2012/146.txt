cancerous tissue classification using microarray gene expression
pei chun chen  victoria popic and yuling liu
stanford university

in this project  we apply machine learning techniques to perform tumor vs  normal tissue classification using gene expression microarray data  which was proven to be useful for early stage cancer diagnosis and
cancer subtype identification  we compare the results of both supervised learning  k nearest neighbors 
svms  boosting  and unsupervised learning  k means clustering  hierarchical clustering  routines on three
datasets  gse   renal clear cell carcinoma   gse      pancreatic cancer  and srbct  round blue cell
tumors   in order to eliminate the non informative genes from the data sets  we apply feature selection
using the t test  differential expression test  and the pairwise correlation coefficient between the class labels and each gene  which boosted the classification accuracy for most of the methods that we carried
out  we present the misclassification error rate for hold out          fold  and leave one out cross validation  in agreement with prior work  we found machine learning techniques to be very efficient with
this classification task  for binary classification  cancer vs  normal  the highest accuracy  close to    
for gse  and more than     for gse      was achieved with adaboost and a linear kernel svm  for
multi class classification  srbct tumor subtypes  we achieve an accuracy of      with a linear kernel
svm without feature selection and     after reducing the feature dimension by   using the correlation
coefficient feature selection technique 

introduction

chine learning techniques  e g   clustering  neural networks 
svms   in our paper  we also investigate different machine
as the central dogma of biology suggests  dna makes rna  learning methods and feature selection techniques and are also
which makes protein  since gene expression  i e the proteins able to achieve high precision in classifying cancer and noncreated in the cell  controls the differentiation of tissues  decod  cancer tissues  as well as identify cancer subtypes 
ing gene expression has become an important active research
area in molecular biology and bioinformatics  the first step tomethods and results
wards this goal is to provide an accurate measurement of gene
expression levels in different cells  powerful techniques  such
data sets
as the microarray technology  have been developed to measure
the abundance of mrna sequences  which is the intermedi  we evaluate the performance of the classification techniques
ate product in the gene expression process  this state of the  on two gene expression data sets profiled using the microarray
art method is able to simultaneously measure the transcribed technology  the first data set  gse   contains gene expression
mrna of more than        genes in the whole human genome  levels of        genes reported in the renal clear cell carcithese expression datasets are obtained by quantitatively mea  noma study by boer et al   and consists of    cancer tissue
suring the hybridization  fluorescence  silverence  and so on  of samples and    normal control samples  the second data set
 gse      comes from the allele specific expression study of
sample mrna to the immobilized cdna on the microarray 
if there is a major difference in cell phenotypes  we can ex  pancreatic cancer published by tan et al   and contains the expect a very different gene expression profile  therefore  gene pression levels of     genes in     cancer tissue samples and
expression has been widely used to classify different types     normal tissue samples  we use the expression level of each
of tissues  in particular  cancer  non cancer  and cancer sub  gene as a separate input feature  therefore  our input data can
type tissue classification is very important since a reliable be described as an m x n matrix  m     tissue samples  n    
early stage cancer detection is able to significantly improve the features   where entry  i  j  represents the expression level of
chances of surviving cancer and precisely identifying different gene j in tissue i  each sample  i e  matrix row  is associated
with a    or    label indicating whether the sample tissue is
stages of cancer can help plan a better therapy strategy 
there are two major challenges associated with tissue clas  cancerous or normal  respectively 
sification based on gene expression  the dimension of the input
features and the noise in the data set  mainly due to sample
gene selection
contamination   usually  the microarray data set is composed
of more than        gene expression measurements  however  in order to find genes with the most predictive power  we fildue to the high cost of the microarray experiments  the num  tered ranked the genes according to the following criteria     
ber of tissue samples is very limited  several hundred at most   t test  with fdr       significant differential expression  using
thus the classification model is prone to suffer from over  the matlab volcano plot  figure   shows the volcano plots
fitting  the second challenge is due to the fact that the can  obtained for the two data sets  the top two quadrants show the
cer tissues extracted are unavoidably contaminated with normal genes that were significantly differentially expressed  signifitissues  which introduces significant noise  since only a small cantly up or down regulated   and     pairwise correlation coefnumber of genes show significant difference in expression lev  ficient between the class labels and each gene  after sorting feaels between cancer and normal tissues  it is possible for this tures according to their absolute correlation coefficient values 
differences to be overwhelmed by to the noise and the high di  we run learning algorithms using different sizes of feature sets
mensionality of the feature space 
to find the optimum set of features as plotted in figure     due
significant prior work has been done in this field    this to the high dimensionality of the feature space  techniques such
work achieves high classification accuracy using various ma  as forward or backward search are too prohibitive  the data set
 

fitable   

svm accuracy with linear  quadratic  and radial basis
kernels  best results are seen for the linear kernel 
dataset

validation

linear

polynomial

radial

gse 

holdout

      

      

      

gse    

fig    

feature selection using matlab volcano plots   a 
gse      b  gse  

feature selection using correlation
  

  
  
  
  
error rate

error rate

  

  

      

      

      

      

      

      

holdout

      

      

      

  fold

      

      

      

loocv

      

      

      

the following techniques  k nearest neighbors  svms  boosting  k means clustering  and hierarchical clustering  our data
analysis pipeline is summarized in figure    the performance
of the learning algorithms is evaluated using hold out    fold 
and leave one out cross validation 

feature selection using correlation

  

  fold
loocv

  

  

k nearest neighbors  knn 

 
  

 

 

   

 

   

 
   
feature size

 

   

 
 

x   

 

 

   

   

   

   
   
   
feature size

   

   

   

the k nearest neighbor classifier labels a test example with the
label of the closest  most similar  example to it in the training set  when k      or with the label of the majority of its
closest k training examples  the k nearest neighbor method is
non parametric and simple to implement  we have varied the
parameter k           as well as tried several metrics for the
similarity measure  available as matlab options   in particular  the euclidean distance  l  distance  sum of absolute differences   and correlation    sample correlation between points  
this simple algorithm performs quite well on our data sets  see
figure     achieving highest accuracies for smaller values of k 
it performed best on the data set gse      which had fewest
 and more relevant  features  the misclassification error rate of
the classifier also decreased significantly on the gse  data set
after feature selection was applied 

    

fig    

 a  gse  linear kernel svm error rate vs  feature label
correlation  the lowest error of      was achieved for feature
size of      around       of the original feature size   with a
small number of features  the error rate is around      which
is expected due to high bias  as the feature size grows  the minimum error rate         is achieved with feature size      the
error rate then increases again and levels out around     without improvement from using more features   b  gse     linear kernel svm error rate vs  correlation  the lowest error of
    was achieved for feature size      around     of the original
feature size  

support vector machines  svm 
we used the libsvm tool with various options to train and test
the datasets   we tried three different kernels types      linear
kernel      polynomial kernel with dimension two  quadratic  
and     radial basis kernel  table   summarizes the classification accuracy of each kernel type  the linear kernel has the best
performance  since the feature vector is high dimensional comparing to the number of samples in the dataset  mapping features to an even higher dimension increases the variance and
thus lowers the testing accuracy  by exploring various feature
selection techniques  see table     we acquire better accuracy
with a much smaller size of the feature set  around       for
the gse  dataset   the computation time and memory requirements are also greatly reduced  more specifically  using the    
fig     classification pipeline 
features selected using the volcano plot  out of the       features   the accuracy of gse  goes up from        to        
feature dimensionality was reduced as follows  gse         the accuracy of gse     stays high using both the t test and
     by          by         by      gse             the correlation method  with     and     features out of the
by          by          by      the p value cutoff used original     features  respectively 
for     and     was set to       we evaluated the learning algorithms on the original and each of the reduced data sets 
boosting  with adaboost 
adaptive boosting algorithms  such as adaboost  build one
strong classifier by iteratively adding weak learners that can
we explored both supervised and unsupervised learning algo  help classify the examples misclassified so far  this is achieved
rithms for the classification task  in particular  we considered by maintaining a weight associated with each example s t  a
classification pipeline

 

fifig     knn misclassification rate  distance metric  euclidean  gse    a     d   gse       e     h    a  and  e  are obtained
with original dataset containing all genes   b  and  f  with t test gene selection   c  and  g  with t test and showing differential
expression and  d  and  h  with correlation gene selection 

fig    

boosting misclassification rate  adaboost with tree weak learners  gse    a     d   gse       e     h    a  and  e  are
obtained with original dataset containing all genes   b  and  f  with t test gene selection   c  and  g  with t test and showing
differential expression and  d  and  h  with correlation gene selection 

very well on our data  the error goes down to   on gse    
and      on gse      

table   

linear kernel svm accuracy with gene selection  best
accuracy improvements are seen for the t test based selection 
dataset

validation

t test

volcano plot

correlation

gse 

holdout

      

      

      

  fold

      

      

      

loocv

      

      

       

holdout

      

      

      

  fold

      

      

      

loocv

      

      

      

gse    

k means clustering
we applied the unsupervised k means clustering approach to
partition the samples into tumor and normal cells  partitioning the samples into only   clusters  cancer normal  can be expected not to perform very well because a particular tumor class
can consist of significantly different sub types and we can expect different gene expression levels to correspond to different
cancer stages  we tested several values for the cluster count k
       to assign a label to a test data point  we first assigned a
label to each cluster center based on the majority label of the
training examples assigned to this center and then used the label
of the cluster center closest to the test data point to determine its
label  figure   shows the misclassification rate on the original
data set and the filtered data set that resulted in the lowest error

larger weight is given to the harder to classify data points   we
used adaboost for our classification problem with the default
tree weak learner  figure   shows the misclassification rate
vs  the number of weak learners added to the classification ensemble  as can be seen  this classification algorithm performs
 

fiwith the rest of the samples  which can be the cause of the lower
classification accuracy values as compared to gse     
multiple class classification extension
after running the learning routines for binary classification  we
tackle the multi class classification problem and test the efficiently of gene selection techniques using the data set from the
small round blue cell tumor study  that contains the expression levels of      genes corresponding to   different cancer
subtypes  neuroblastoma  nb c  nb t   rhabdomyosarcoma
 rms c and rms t   non hodgkin lymphoma  nhl   and the
ewing family of tumors  ews t and ews c  
from the previous experiments  we observed that linearkernel svm and boosting do well on the classification task  we
thus use these two algorithms to train and test on the multiclass
dataset with different sets of features  as shown in figure   a  
after using correlation based feature selection for svm  we get
     error rate using only     out of      features  and   
with      features  for boosting  we get        error rate
with     features and        with      features  although the
boosting error rate is quite high  comparing to the        error rate using all      features  feature selection technique still
helps lowering the error rate  in order to gain graphical insight
on the clustering result  we employ the hierarchical clustering
algorithm and show the results in figure   b   with the rows
and columns corresponding to genes and samples  respectively 
we can see the samples are correctly classified into seven clusters 

fig    

k means misclassification rate   a  gse  all features 
 b  gse  gene selection using correlation  best error rate    c 
gse     all features  d  gse     gene selection using the ttest  best error rate  

rates for this problem  better results can be seen for higher values of k  however  the classification accuracy of this technique
is significantly lower than that of other examined algorithms 
feature selection does improve the performance slightly 

hierarchical clustering
hierarchical clustering is a technique that builds a binary tree
by iteratively merging groups of points  individual data points
or cluster centers  that are similar to each other by some given
distance metric  therefore  it can efficiently cluster the dataset
into a desired number of clusters and  at the same time  provide a visually interpretable distance measure between the data
points or cluster centers  which makes it a good choice to visualize and analyze the structure of our data  as some cancer
types can contain an arbitrary number of subtypes and usually
it is unknown how many or what subtypes a specific cancer has 
thus it is possible to discover or validate the structure of specific cancer types based on hierarchical clustering  we clustered
both datasets using this method and the dendrogram of the gene
expression levels is shown in figure    the genes correspond
to the columns and the samples correspond to the rows   the
green  black  and red colors in the heat maps indicate a low 
medium  and high expression of the corresponding gene in this
sample  respectively 
informative patterns can be observed in the graphs  for
gse      we can clearly see the two distinct classes  corresponding to cancer and normal tissues  for which the expression of the genes flips  namely  the same gene is down regulated
 green  in samples of one class and up regulated in the samples of the other class  red   the samples are split into roughly
    and      which corresponds to the number of the cancerous and normal samples in the set  respectively  comparing the
graphs before and after feature selection  both according to the
t test and the pairwise correlation coefficient   we can see fewer
genes that have similar expression across the two classes since
these genes will be filtered out according to our criteria for feature selection  the dendrogram for gse  seems much more
complicated  indicating possibly many subtypes in the cancer
tissues  we can also find a lot of outliers that dont cluster well

conclusion
gene selection was effective  especially on the gse  data set 
in improving the accuracy  as well as the speed and memory consumption  of the investigated algorithms  for example 
the accuracy of the linear kernel svm using holdout increased
from     to      on the gse     data set  the t test and correlation methods give better results than the differential expression test  this is because only    genes are both statistically
significant and differentially expressed  making the test too selective 
highest classification accuracy was achieved using the linear
kernel svm      on gse     and     on gse    boosting
     on both data sets with    weak learners   and knn     
on gse     and     on gse  with small values for k   lowest
accuracy was obtained using k means clustering  the structure
of the microarray data itself may be complex so that without
enough domain knowledge  it is difficult to decide the number
and the initial values of clusters that can give us higher accuracy
while utilizing k means 
we compared the genes selected using the three different
techniques for both gse  and gse      for gse      genes
were selected in both the t test and the correlation methods 
and    in both the differential expression and the correlation
methods  this shows that the gene selection methods agree with
each other quite well on the significance of genes   note  genes
selected by the differential expression test are selected by the ttest as well  since it basically adds expression level constraints
on the t test   for gse          genes were selected in both
the t test and the correlation methods  and   in both the differential expression and the correlation methods  in order to know
if the selected genes make biological sense  we examined the  
genes selected by all three methods in gse      dapk   one
 

fifig    

hierachical clustering  gse    a     c   gse       d     g    d  are obtained with original dataset containing all genes   a 
and  e  with t test gene selection   b  and  f  with t test and showing differential expression and  c  and  g  with correlation gene
selection 

fig    

multi class classification results   a  linear kernel svm error rate with correlation based feature selection   b  hierarchical clustering into   classes  the binary tree is colored according to the class of the branch 

 

of the selected genes  is found to be very related to cancer  it
is called death associated protein kinase   and is a tumor suppressor candidate  we also examined the    genes selected by
the differential expression test in gse     and found tgfa 
transforming growth factor alpha  tgf    which is highly
related to cancer in its upregulation in some human cancers 
these validate the results of our gene selection implementation 

li  t   zhang  c   and ogihara  m  bioinformatics                 
       

 

boer  j   huber  w   sultmann  h   wilmer  f   von heydebreck  a  
haas  s   korn  b   gunawan  b   vente  a   fuzesi  l   et al  genome
research                         

 

tan  a   fan  j   karikari  c   bibikova  m   garcia  e   zhou  l  
barker  d   serre  d   feldmann  g   hruban  r   et al  cancer biology   therapy                     

 

chang  c  and lin  c  acm transactions on intelligent systems and
technology  tist                  

 

ben dor  a   shamir  r   and yakhini  z  journal of computational
biology                       

 

ben dor  a   bruhn  l   friedman  n   nachman  i   schummer  m  
and yakhini  z  journal of computational biology               
       

 

khan  j   wei  j   ringner  m   saal  l   ladanyi  m   westermann  f  
berthold  f   schwab  m   antonescu  c   peterson  c   et al  nature
medicine                     

 

fi