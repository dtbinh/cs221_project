predicting the   th academy awards
stephen barber  kasey le  sean odonnell
december         
as the oldest awards ceremony in the media  the academy awards is responsible for bestowing the penultimate
achievement in cinema  the oscar  with which comes distinction and prestige  this immense potential for
positive exposure is a marketers dream and so every year  studios will spend millions to promote their films
during oscar season  enough to draw accusations that perhaps the academy awards is more a promotional
scheme than recognizing superior quality film  using a training set composed of every film to have ever been
nominated for an oscar in four target categories  we aim to develop an effective learning algorithm to reproduce
the results of past oscars  predict the results of the upcoming   th academy awards  to be held february
       and attempt to determine the features that contribute the most to an academy award victory 

 

data and features

ingly  and we use extensive binary labeling to indicate
the presence of an actor actress in a movie  as well as
we wished to use data that could easily be collected labeling our known academy award victories for each
for any movie within the last century  which limited us film  resulting in a sparse set of      features 
to more conventional and standardized film attributes 
since we needed qualitative critique that would be
available for any movie that could have been nomi   
testing methods
nated for an academy award  we decided to focus on
aggregated ratings  such as from audience and critics for testing  we began using hold out cross validation
on rotten tomatoes and imdb  our choice of training where we trained on movies released before      and
data is determined by a master list of all movies that tested on movies released after       for our specific
have ever earned at least a nomination in the best film  problem  this division of data made sense as the realbest director  best actor  or best actress categories life application of our algorithm would be to predict
of the academy awards 
future academy award nominations  we also wanted
to take in account for any kind of growth or evolution
in the judging process from year to year  furthermore 
    data set generation
since nominations are selected from a pool of movies all
imdb and rotten tomatoes  rt  served as our pri  released in the same year  it makes sense that year is
mary sources of feature information  first we aggre  not just a feature of the movie but also a way to group
gated a list of every film nominated for an oscar in movies that are in competition with each other 
for robustness  we also begin using k fold cross valthe  colloquial  categories of best film  best director 
best actor  and best actress  and then we used these idation with    subsets  in general  we found that our
film titles and their respective release years to search models had the same success under either validation
for and acquire respective film features  using the unof  method  however  since we were often testing multiple
ficial rt api and our own custom web scraping script models with multiple variations  we found k fold cross
for imdb  we have aggregated  in all  the imdb and validation too computationally expensive to be of much
rt  both audience and critic  ratings  release month  use  thus  all stated results will be from hold out cross
writer  director  cast  mpaa rating  year  run time  validation 
and genre feature data from our master list of collected
to better evaluate our data  we also opted to use
film titles  which is nearly      films in total  all col  matthews correlation coefficient as our measure of perlected data has been discretized and binned accord  formance since we were performing binary classifica 

fitions  additionally  given that our data is skewed with
a small fraction of positive examples  we wanted to put
less emphasis on accuracy and instead use a balanced
measure of precision and recall  the coefficient returns
a value between    and    with    indicating a perfect prediction    indicating a random prediction  and
   indicating a completely opposite prediction 

 

idence  gaussian naive bayes performed the worst out
of all our models and significantly poorer than multinomial naive bayes  we ended up using a polynomial
kernel for our svm and optimized the degree for each
category 

   

feature and model selection

after discovering that each of the categories had varied
success with each of the different classifiers  we decided
to separately optimize the features and models used for
each category  motivated by the fact that our feature
set was over four times the size of our training set  we
implemented feature selection using backwards search 
we counted the binary vectors for cast  writers  and
directors as one feature each  resulting in a total of
   features  we wrapped our feature selection around
all our models including logistic regression  svm with
polynomial kernels of degree   and    gaussian naive
bayes  and multinomial naive bayes  although computationally expensive  we ran the process over all
models  enabling us to simultaneously select the best
features and model for each category  we used hold out
cross validation and matthews correlation coefficient
as our score to measure the performance of each iteration  this resulted in the following optimal model and
features for each category 

learning algorithms

we began by using quick and dirty implementations of
the gaussian naive bayes classifier  for simplicity and
speed  and a svm linear and polynomial classifier     degrees  for robustness and flexibility of implementation  since we had no prior knowledge or assumptions
about the distribution of our data  we wanted to cast
a wide net and work with generic models to start 
we had some initial success using polynomial svms
and thus we decided to also implement logistic regression as an alternative objective function optimization
classifier  the gaussian naive bayes did not perform
as well as the svm  so we began to use the multinomial
naive bayes model instead which assumes less about
the distribution of the data 
the results of these initial tests can be seen in the
first four columns of each category graph in the results
section     

best actress  svm with a polynomial kernel of
degree   without audience rating
    svm exploration
best actor  multinomial naive bayes without direcin the first runs through our data  we found that poly  tor  cast  critic rating  and runtime
nomial svms performed the best out of all the basic best director  logistic regression without audience
models  thus  we decided to explore svms further and rating  release month  and runtime
try to optimize their performance over our data  al  best film  logistic regression without audience ratthough we tried to reduce the range of values of each ing  cast  and genre
feature during our data set generation  we had to allow
enough values to maintain distinct years and ratings 
with feature and model selection  we found imthus  our feature vector was uneven with the union of provement in the matthews correlation coefficient in
these features and our many binary vectors  as a re  all four categories  it was interesting to see that the
sult  we tried to normalize our feature vectors to assure best model changed from svm to either logistic rethat no feature was intrinsically weighted more based gression or multinomial naive bayes in three of the
solely on its wider range of values  however  this did four categories as we reduced the dimension of our feanot improve performance  so we reverted back to our ture vectors 
original vectors 
for curiositys sake  we also did an ablative analysis
next  we experimented with different types of ker  of the features to identify the ones with the most prenels to try to better model the distribution of our data  dictive power  the two most predictive features were
we applied the gaussian radial basis function as a ker  mpaa rating and the rotten tomatoes critic ratings
nel to see how normal our data was  this resulted for best film  imbd rating and cast for best director 
in much worse performance of the svm  definitively director and genre for best actress  and release month
concluding that our data is not normal  as further ev  and genre for best actor  genre was overwhelmingly
 

fithe most significant factor for best actress and actor  does not require much dataset tuning  we trained our
semi supervised classifier on a small fraction of our la    semi supervised learning and beled dataset  labeled a larger fraction of an unlabeled
dataset  then trained our classifier on the union of
random forests
our pre labeled and classifier labeled dataset  repeating
in an attempt to potentially better model the underly  these two steps until convergence  however  it preliming distribution of data  which has not be well modeled inary results showed that it performed no better than
by the gaussian distribution   we attempted a semi  our multinomial naive bayes classifier  so we disconsupervised naive bayes classifier using em  nigam tinued its use   perhaps our dataset was too fundamenet  al         and random forests  a favorite of kag  tally different from the text dataset used by nigam  to
gle contestants looking for an effective algorithm that have benefitted 

 

results

 

fired outline name indicates best of   basic models  which is enhanced through feature selection  compared
against  red  random forest model to judge optimality of feature selection  considering random forests strong
feature handling 

 

fi   

conclusions and discussion
film title
life of pi
les miserables
zero dark thirty
wreck it ralph
skyfall
silver linings playbook
the sessions
moonrise kingdom
mirror mirror
looper
the amazing spider man
argo
the dark knight rises
marvels the avengers
safety not guaranteed
beasts of the southern wild
playing for keeps
rock of ages
men in black iii
wrath of the titans
lincoln
red dawn
amour

we were not able to produce as strong of a classifier
as we would have liked  but the results of our models and feature selections still hold various implications
for the academy awards nomination and selection process  excluding different features for each academy
awards category overall produced better results  yielding numerous insights  some exclusions were intuitive
in hindsight  such as runtime became a confounding
feature for   of   categories  some exclusions were surprising  audience rating confounded   of   categories 
implying perhaps that audiences are maybe too selective  or maybe too undiscriminating  or even that
judges makes selections differently from the average
movie reviewer  either way  it seems assuring that
both cast list and genre hold little importance in nominating a best film   an oscar worthy film seems to
still need something more  our difficulty in selecting
an accurate model built upon a supposed dataset distribution could be explained by the sheer difficulty in
winning an academy award in the first place  considering that the most oscar worthy films tend to walk
away with the most awards  and  for example  with
    movies released in      and only   best film nominations for      movies  oscar nominations likely follow some kind of power law distribution  which is more
difficult to model with most well established machine
learning algorithms  at least compared to gaussian and
multinomial distributions  future work in improving
an academy awards classifier would likely require even
more careful  in depth feature selection  with a welldesigned neural network to more closely model any underlying distribution 

   

 

likelihood of
nomination
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
    
    
    

references

kamal nigam  andrew mccallum  and tom m 
mitchell  semi supervised text classification using em          pal sri com calofiles cstore palpublications calo      semisup em pdf
mpaa       theatrical market statistics       
http   www mpaa org resources  bec ac  a  e   b    b bff fb    a  pdf

predictions for the   th academy
mathieu blondel 
awards

semi supervised naive bayes
in python  mathieus log  june          
http   www mblondel org journal            semisupervised naive bayes in python 

as a fun end to our project  we wanted to make predictions for the   th academy awards being held on
february       using the model and features we selected for the category of best film  we evaluated the
likelihood of some major films from      being nominated for or winning in the category  the following
table lists some of the top films most likely to be nominated 

pedregosa et al  scikit learn  machine learning in
python        http   jmlr csail mit edu papers v   
pedregosa  a html
made extensive usage of  http   www imdb com  oscars nominations 

 

fi