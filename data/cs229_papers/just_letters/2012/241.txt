predicting the daily liquidity of corporate bonds
louis ben arous
december         

introduction

methods

let us define the liquidity of a financial asset as
its volume  or the number of transactions of that
asset during a particular time window  due to the
lack of a major electronic market for fixed income
instruments  many liquidity issues may arise when
trading corporate bonds  where one security might
become highly illiquid and create huge price impacts for investors  as they find themselves unable
to close their positions 
this research will set up  test and compare statistical predictors from different fields  time series analysis and machine learning  to accurately forecast
the daily volume of us investment grade corporate
bonds  the goal is to find a global machine learning
algorithm that outperforms univariate time series
models out of sample 

i use as measure of performance the average absolute error  aae  to compare models  the aae of
a given model is defined as 
n

aae  

 x
 yi  yi  
n
i  

where yi is the prediction of the model for the ith
observation  the goal of this study is to find a
machine learning learning algorithm with a lower
aae than the time series aae 
as mentioned earlier  the first part of this study
is to find the best possible univariate time series
model  and to fit this model to each one of the   
bonds in my dataset  as a result  i will have   
sets of parameters  one for each bond   and i will
consider my general aae for the time series case
as the average of the aae of each model 
then i will fit different machine learning models to
the complete dataset  all the bonds at once   and
see how the error of those models compare to the
time series error 

data
to implement our study  we need daily data for
investment grade us corporate bonds  the question that arises is which bonds should we look at to
create an appropriate predictor that could be applied to other bonds  the approach taken in this
study is to look at the composition of the barclays
us aggregate index  agg    and to pick the top
   corporate bonds that have the highest weight in
this index  the agg is the most commonly followed index for us fixed income products  which
justifies its use to pick the most relevant bonds 
i have grabbed and merged the volume and price
data for each of those bonds from bloomberg into
one dataset for a time window from november    
     to november           since some of those assets were created well after       special care must
be taken to handle gaps and missing values in the
data when building our predictors  i normalize my
data such that the liquidity values for each bond
have mean   and variance   

arma egarch
let lt k be the volume of the kth bond at time t  we
have    time series lk for k               there is no
reason for the volume of a corporate bond to be stationary  since the number of transactions a day of
an asset depends highly on the overall market capitalization of the underlying company  which may
grow or shrink over time  since time series model
assume stationary inputs  i difference the lk series
to get the rk series  where rt k   lt k  lt  k  
a quick look at the autocorrelation function  acf 
plots and partial autocorrelation  pacf  function
plots for each rk tells us that the rk are indeed stationary  and that the acf cuts off at lag    and the
pacf cuts off between lag   and lag    example 
 

fiibm             

  
 

    

  

   

acf

sample quantiles

  

   

  

   

standardized resid qqplot

 

 

  

  

  

  

  
   

lag

  

  

 

 

 

theoretical quantiles

ibm             

    
    

lt   k   l  k  

    

partial acf

    

   

once the model is fitted  i test the time series
model out of sample by fitting the models only on
previous data  and then making the forecast  we
can easily make new prediction rt   k   and compute
lt   k as
t 
x

rti k   rt   k

i  
 

 

  

  

  

  

  

assuming we have data from t   to t t and that
we want to fit our model on at least    values  we
compute the aae of the kth bond as

lag

this hints that we should fit an arma p  q 
model  with p between   and    and q    for simplicity  instead of fitting a different arma model
to each series  i decided to fit an arma       to
all of them  to take into account volatility clustering and asymmetric volatility responses due to
movements in rk   i also add a egarch component to the model  i therefore fit by mle the following arma      egarch      model to each
series by using the matlab garchfit function 
rt k     k  

 
x

i k rti k   ut k  

i  

 
x

t
  x 
aaek  
 lt   k  lt   k  
t
t   

the results are shown at the end of this paper 

ols and pca
now that i have fitted our time series models  i
want to fit global learning algorithms to the data
to see whether they fare better  restricting ourselves to the case where we only use previous values as features  as in time series models   the first
approach to take is to try to fit an ordinary least
square regression to fit the previous daily volume
values to the current one  let h the autocorrelation lag  the number of past values i will look at  
and let 
h
x
lt k  
i lti k   t

j k utj k

j  

ut k   t k t k
 
log t k
 

  k  

 
x
i  


fj  t k    

 
i k log ti k
 

 

 
x

fi  tj k  

j  

 j k   j k  t k  j k e t k  
 j k   j k  t k  j k e t k  

t k   
t k    

i  

where t has mean   and variance      notice that
i seek to get the unique values i for all k  to
do that  i set up my data in a panel data format 
where i set up a response variable y and a matrix
x such that 

looking at qqplots  we see that the normal assumption t  n        is violated  hence  i assume t is
a standardized student t random variable  mean   
variance    
 

filt  

inclusion of my lagged variables using a bayesian
information criterion  bic   and find that my best
model is with h      for h     my sample has 
      observations after removing missing values 
with the test set having      observations and the
training set having       observations  xtraining is
therefore a       by    matrix 
secondly  i do pca on the covariance matrix of
xtraining   to see whether i can reduce the dimension of this problem  the next figure shows that
the first   components  and mainly the first one 
account for most of the variance 



lt    


    
   

y 
 lt   


lt    


  
 

lt   
lt   

   
  
x 
lt   

lt   

  
 

lt   
lt   

lt   
lt   




lt   
lt   

lt   
lt   




lth  



lth    




lth   

lth    


pca

   

variances

   

   

   

to make my analysis  i remove any row with missing
values  i create a training sample and a test sample by
randomly selecting     of the total number of row indices from x and y and by making the predictor matrix
xtest and test response ytest from those selected rows 
the rest of x and y become xtraining and ytraining   i

   



   

find the parameter vector  by

   

   

t
t
    xtraining
xtraining    xtraining
ytraining

for a lag of     we see that our parameters i for
i           look as such 

comp  

comp  

comp  

comp  

comp  

    

as such  let u    u  and u  be the top eigenvectors 
then i can represent the regression as 
ytraining      

i xtraining ui   t

    

i  

    

i can then test the ols and the ols with pca
factors models i get on xtest and ys et to get the
aae of those two models  the results are listed in
the results section 

    

model coeff

 
x

kernel regression
 

  

  

  

  

  

due to the nature of our kernel  i try to implement kernel regression with an exponential kernel
e tti    to see whether it can filter out the noise 
instead of fitting  by cross validation  a better
technique seems to be to do a lasso regression of
different kernel fits with different    i take    values of    where        i   i       
p h   j   
x
xtraining i j
j   e
ytraining i      
i
p h    j     
i  
j   e

time

this plot shows that the most relevant variables
seem to be the first   lagged values in decreasing
order  the previous day is the most important  followed by the second one        given we are looking
at a time series  this is fairly intuitive  this  coupled with the fact that only the first   lags seem to
have a pvalue lower than       tells us that h    is
too large  and that we are overfitting  clearly  our
parameters have a large variance  i take two approaches to solve this issue  first  i do a stepwise

where i implement the lasso using the modified
least angle regression algorithm from the r lars
 

fi using neural networks and seeing whether using additional depth can make us perform
better 

package  which find the best fit by k fold crossvalidation  this fit sets to   all of the  except for
                                           
and           
we can see in the results section that the ols regression  the ols regression on pca factors and
the kernel regression yield similar out of sample
aae  which is higher than the average out ofsample aae we get from our arma egarch
models  this is a perfectly understandable result 
considering arma egarch models do filter and
regress the data on its past values  as does kernel
regression  but do it one bond at a time  and by
modeling the variance as well  however  we can
ask ourselves whether there is another way to do
better than arma egarch using xtraining as
our predictor matrix 

 adding features to the model 

will deep learning help 
i fit a neural network with only   hidden layer composed of     hidden units on the xtraining   with
a learning rate of      a momentum of     and a
weight decay of       i find that this model performs better than mart but not as well as svr
and the time series model 
i then fit a   hidden layers neural network with
    hidden units in each layer  i train this network as a deep belief net  which means seeing the
network  minus the output node  as stacked restricted boltzmann machines  those are trained
by stochastic gradient descent with contrastive divergence of   iteration  the resulting parameters
are then used as initial values to train the neural
network via backpropagation 
i used the deeplearntoolbox in matlab to do this
fit  i find that given the numbers of parameters  i
need to increase my weight decay to     for better results  adding  or subtracting  hidden layers
increases the aae  as such    hidden layers is my
best dbn model  we see that this model fares better than the   hidden layer network  but not significantly better than svr of the arma egarch
models  as such  adding depth did not significantly
improve my accuracy in this setting 

boosted trees  svr
the first step is to use boosted trees using the
mart  multiple additive regression trees  algorithm on our predictor matrix xtraining   mart
computes a sum of regression trees of fixed size  in
this case  only   leaves  which are added to the
model via a stagewise forward procedure  this implies that each new regression tree is fitted on the
residual of the current model  a regression tree has
the form 
t  x     

j
x

j i x  rj  

i  

where     rj   j  j    and j is the number of leaves 
another technique to try is using support vector
for regression  svr   with a radial basis kernel 
which is a modification of support vector machines
adapted to regression  i fit mart using the r
mart package and svr using the r e     package 
we see in the result section that mart performs better than ols and kernel regression  but
still does not match the arma egarch models 
however  the difference between the svr model
and the arma egarch models is almost insignificant  as such  we have managed to match
the time series model using svr  however  we
do not outperform  and training the svr model
takes a much longer time than training the armaegarch models  even though there are    of
them  as such  i try two other approaches 

adding features
the only remaining way to improve our model is
to add more features  the first feature i added was
the industry label  split between    financials  jp
morgan  goldman sachs      utilities  the dow
chemical company  at t     industrials  ibm  
consumers  walmart   out of the    bonds in my
dataset     are financials    are utilities    are industrials and   are consumers 
additionally  i grabbed the price data for each
bond during the same time period  november   
     to november           and used returns and
volatility  where the returns are computed as
rt k  
 

p ricet k  p ricet  k
p ricet  k

fimodel
ols   pca
kernel
ols
mart
neural net    layer
svr
arma      garch       mean 
neural network   layers
mart   new features

and the volatility is defined as the standard deviation of the past    returns  since a month has
around    trading days  
for each volume lt k   i then define its predictors to
be  as before  the previous    volume values for
this bond  lti k where i            but also its past
  returns values and   standard deviation values 
its industry label  as well as the past   volume 
returns and standard deviation values of all of the
other bonds in the sample  as such  i try to capture
whether returns and volatility can influence volume  whether bonds from different industries have
different behavior  as well as whether the volume
of a bond can be influenced by the volume  price
and volatility of other bonds 
there are a few issues with this model  first of
all  the number of missing values increases drastically  if i were to remove rows with no missing
values  i would lose most of my sample  moreover 
there are now                      features in
the model  many of which may be redundant  as
such  i need a model that can perform well with
missing values  a mix of categorical and numerical
values and redundant variables  neural networks
and svr are of not much help here 
the best candidate in this case is mart  which
does enough regularization to remove unneeded
variables  and uses surrogate splits to accommodate for missing values  however  i realized that
after too many iterations  the mart algorithm is
overfitting the data  i therefore restricted the number of iterations to      and the number of leaves
in each tree to   instead of   
i find that mart with this new extra set of
features outperforms on average the arimaegarch models  with a reasonable training time 

we see that arma egarch models fare really
well against other learning algorithms  the error
of arma egarch models is on average half the
standard deviation of data  we find that using
mart with additional features such as an industry label  past returns and volatility get us to an
error rate that is     of the standard deviation 
we see that this final model gives us for any bond
in the sample approximately the minimum aae we
can get with an arma egarch model  with the
advantage of fitting only one model to the data 

going forward
i have restricted myself to study the behavior of
a limited number of investment grade corporate
bonds  this research could be extended to junk
bonds or other types of fixed income products 
similarly  we could add other features in the analysis  such as credit rating and maturity  finally  i
have also restricted myself in this study to univariate time series model  it could be valuable to test
how multivariate volatility models perform against
mart 

references
 trevor hastie  robert tibshirani  jerome
friedman  the elements of statistical learning  data mining  inference  and prediction 
second edition  february      

results
here are the summary statistics for the aae values for the    arma      egarch       models 

aae

min
      

max
      

mean
      

median
      

aae
      
      
      
      
      
      
      
      
      

 tze leung lai  haipeng xing  statistical
models and methods for financial markets 
     

std
      

 rasmus berg palm prediction as a candidate for learning deep hierarchical models of
data  http   www  imm dtu dk pubdb views 
publication details php id           

and here are the sorted aae values for the different models 

 

fi