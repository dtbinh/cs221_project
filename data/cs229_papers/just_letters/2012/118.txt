analyzing cs     projects
michael chang  mchang   
ethan saeta  esaeta 
cs    

 

introduction

the goal of this project is to study the characteristics of cs     projects  we explore whether past projects
have certain traits that distinguish them from other machine learning papers and whether projects can
generally be clustered by topic  in doing so  we hope to determine whether it would be possible to predict
the kinds of projects we will see this year 
in this paper  we first describe the data we use and how we process it  then  we explore whether
these questions can be answerwed using techniques for classification  finally  we look at using unsupervised
learning techniques to get more information about project topics 

   

data set

our main data set is the cs     project reports from      and       we convert each pdf to plain text using
the standard unix  pdftotext  utility and tokenize the text files  ignoring all nonalphabetic characters   in
particular      is not a valid token in our model   we run the porter  stemming algorithm  on each token 
which improves results in all experiments  for conciseness  we have omitted the results of our experiments
without stemming 
in our first classification experiment  we compare     projects to other works in machine learning  for
this  we use papers published at nips in      and         these papers are processed in the same way as
the projects 
cs         
cs         
nips     
nips     

  documents
   
   
   
   

  unique tokens
  stemmed tokens

cs    
      
      

cs       nips
      
      

table    statistics about our data set
table   summarizes some statistics about the data set we used  the right table shows that stemming
reduces the size of document vectors by        

 

classification experiments

for these experiments  we use a multinomial event model to construct vectors for each document  that is 
the j th entry of x i  represents the number of times word j in our vocabulary occurs in document i  we
train an svm with a linear kernel using liblinear    we use      projects and papers as our training set and
     projects and papers as our test set 
  http   snowball tartarus org algorithms english stemmer html
  http   nips cc conferences 

  http   www csie ntu edu tw  cjlin liblinear 

 

fi   

cs     project vs  nips paper

for this experiment  we consider all     projects positive examples and all nips papers negative examples 
the results of this experiment are shown in table   

predicted

cs    
nips

actual
cs     nips
   
  
 
   

precision 
recall 
accuracy 

     
     
     

table    results of cs     vs  nips classification
these results suggest that cs     papers are in fact very distinctive  even among work in machine
learning  looking at the data that was misclassified  we found that a lot of the     projects that were
classified as nips papers either introduced new learning algorithms or used more advanced techniques  such
as neural networks   whereas most     projects directly apply the techniques we learned in class to various
fields 
     

limiting the number of pages

next  we explore how much the accuracy of our classifier depends on the fraction of the document we consider 
our goal is to see whether predicting if a document is a     project or a nips paper can be done eectively
using only the first couple of pages  or whether the classifier needs to consider the entire document  since
    projects and nips papers are generally dierent lengths  we run the clasisfier using a percentage of each
document  starting from the beginning of the document   the results of these experiments are shown in
table    and a graph of precision  recall  and accuracy is shown in figure   
  of doc
   
   
   
   
    

positive
true false
   
  
   
  
   
  
   
  
   
  

negative
true false
   
 
   
  
   
 
   
 
   
 

precision
     
     
     
     
     

recall
     
     
     
     
     

accuracy
     
     
     
     
     

table    results of classifier on limited number of pages

figure    graph of classifier accuracy
since using just     of each document already yields precision  recall  and accuracy results above     
the first page or two of each document seems to be a very strong indicator of whether the document is a
    project or nips paper  the lack of significant improvement at           and     suggests that the
middle pages of the document do not add substantial information that helps classify the document  while the
 

fispike in accuracy at      suggests that the last page is also a good indicator of document class  thus  we
conclude that the first and last page of each document are the best indicators of whether it is a     project
or nips paper 

   

cs     project topic classification

next  we turn our attention to clustering     projects by topic  we pose this problem as a classification
problem by manually labeling projects  based on their titles  according to their general field  such as vision
or robotics  for this experiment  we choose to focus on vision  as we found a significant number of
projects related to vision 
using the same approach as above  we now label vision projects as positive examples  and all other
projects as negative examples  our training set       projects  contains    positive examples  out of      
and our test set        contains     out of       results are shown in table  

predicted

vision
other

actual
vision other
  
 
 
   

precision 
recall 
accuracy 

     
     
     

table    results of classifier on vision projects
these results suggests that it is generally easy to identify a non vision project  but some vision projects
are dicult to identify  much of this diculty likely comes from the fact that the field of  vision  is ss large
that vision projects from one year may not adequately represent the field  for example  there is a project in
     about digital image forensics  however  since there are no projects about image forensics in       this
project was misclassified 

 

automated clustering

the results above show a significant limitation in using classification to identify project topics  doing so
assumes that the training set contains enough examples to accurately represent the topic  another problem
is that though many general topics  such as vision and robotics  show up each year  there are some topics
that are only receive attention in one particular year  for example  in       there are many projects about
predicting the stock market using twitter  whereas there is only one project in      that mentions stocks 
using the method above would not work here  since there are no training examples to use to identify the
topic 
thus  we turn our attention to unsupervised techniques for identifying clusters within a single year 
without depending on data from previous years  running the standard k means algorithm  often results in
all projects being placed in a single cluster  the biggest problem we identified came from in the minimization
of x j  i  
 i 
to solve this problem  we first let xj     if word j appears in project i  regardless of frequency  then 
we make the following modification to k means  set the cluster of project i to
d
e
c i    arg max x i    j
j

where hx  yi is the inner product of x and y 
this formula rewards projects for having words in common with the cluster centroid  without penalizing
them for words they do not have or words that are irrelevant to the centroid  this makes sense intuitively 
since projects will generally contain many words that do not indicate their topic  and not all words that
indicate topic will appear in every project on that topic 
running this algorithm with k     often produces a small       project  cluster  however  whether such
a cluster is produced and what topic it potentially represents are highly sensitive to the starting position of
the cluster centroids  therefore  we use the following approach 
  http   cs    stanford edu notes cs    notes a pdf

 

fi initialize mij     for all pairs of projects  i  j  
 repeat n times  we used n        
 run the above k means like algorithm  with k     and cluster centroids initialized to random
projects 
 for each cluster whose size is   c  we used c        for each pair of projects  i  j  in the cluster 
increment mij  
this gives us a matrix containing the number of times each pair of projects appears in the same  relatively
small cluster  for each project i  we can rank the projects it is most likely related to by sorting the projects
j according to the values of mij  
to visualize the clusters  we represent each project as a node in a graph  we draw an edge between node
i and node j if j appears in the top   ranking of i  and vice versa   requiring a mutually high ranking
accounts for projects which are unrelated to many other projects and thus rank projects almost arbitrarily  
the result is shown in figure    we have omitted nodes with no connected edges and highlighted interesting
parts of the graph  a list of some of the projects in the graph is presented in table   

figure    graph of clustering results

 

fitable    selected project titles
from the graph  we can see that our algorithm is able to detect a number of topic clusters  in particular 
many projects about using twitter to predict the stock market  labeled  twitter   are connected to one
another  we observed that mij for these projects tended to be in the   s   also  projects about vision were
also connected  though mij values were slightly lower  around        
another interesting feature of this graph is the ring  labeled  interesting non cluster   although each
project in the ring ranked its neighbors highly  the lack of edges that cross the ring suggests that these
projects are not pairwise related in the same way as the other clusters  there was no noticeable topic
relationship between projects in the ring 

 

conclusion

the techniques we use are able to point out some interesting characteristics of cs     projects  we are able
to distinguish between     projects and nips papers with very high accuracy  suggesting that     projects
have a distinct place in work on machine learning  as they largely emphasize applications rather than new
algorithms  also  both our supervised and unsupervised techniques show that  while it is fairly easy to find
similar projects  it is much more dicult to identify a significant fraction of the projects corresponding to
a particular topic  this suggests that  although it may seem like many projects fall into the same general
field  projects vary quite widely in how they apply machine learning  even within a single field  this fact 
combined with the fact that some topics do not appear year after year  suggests that it would be rather hard
to predict what sorts of projects students will come up with next year  for example  this one  

 

fi