 

predicting popular xbox games based on search
queries of users
chinmoy mandayam and saahil shenoy

i  i ntroduction
this project is based on a completed kaggle
competition  our goal is to predict which xbox
game a user on the best buy site will be most
interested in based on their search query  essentially
this will be a recommender system type problem 
which we are looking to solve by building a multilabel classifier 
ii  data d escription
each training
information 

example

has

the

following

user

sku

search

click

query

token

clicked

query

time

time

we have about        training examples  the
come from over        users over almost   months
from august october       in addition  we are
given a separate list of about     xbox     related
skus and some meta information   including the
product description  price history  etc    about these
items  this list has multiple duplications  and after
culling these  we find that this list contains    
unique entries the test data has the same information as the training data except for the sku  which
needs to be predicted 

matches the actual product that the user clicked
on  and zero if the actual product was not recommended  the mean of the average precision values
for all the queries gives us our map score 
in our our midterm report and our poster  our
reported scores on kaggle were significantly lower
than our actual performance giving us an impression
that our models had a high variance  or that the test
data had a different distribution from the training
data  however  this discrepancy was only because
we had misinterpreted the file format required of our
submissions  so only a portion of our predictions
was actually being tested  we have corrected this
error in this report
iii  c lassifier models

since we are expected to give more than one
choice  a simple classifier which gives us one recommendation without a good confidence measure
would be a bad choice  we need a way to rank
our recommendations  so  we must immediately
rule out vanilla svms  a bayesian model   such a
naive bayes classifier  would quite naturally have
a probability interpretation that could be used to
compare the likelihood that a particular product will
be selected  we could also use logistic regression 
which inherently attaches a sigmoid confidence
measure to each classification  g t x   the closely
related soft max regression would have a similar
p erformance evaluation
interpretation  we decided to try out naive bayes
we decided to perform our testing using the same
and logistic regression based classifiers 
metric that the kaggle challenge uses  map  
 mean average precision at rank     the challenge
iv  c lassification f eatures
expects a list of the top   recommendations for
the each search query  then  the precision of the
since the number of search queries per user is
recommendation is computed  using the actual click so low  we decided to ignore the user information
information  for the set of the top n choices   for   which we could have run collaborative filtering
each n from   through    this is then averaged to on  and instead concentrate purely on the search
obtain the average precision at rank   ap    score queries and the time information  text classifiers
for the query  effectively  the score for a particular generally use a bag of words based model  where
recommendation is   k if our k th recommendation the number of occurrences of a each word in a

fi 
   

training set
x validation

map   score

    

   

    
 

   

 

   

 
dataset size

   

 

   

 
x   

 

figure    cross validation performance of multinomial naive bayes

vocabulary is the feature vector for each sample 
however  since our search queries are we decided
to first try models based on a bag of letters
model  where the feature vector is the number of
time each character  including numerals  appears 
based on this simple feature vector  we tried using a
naive bayes classifier and a logistic regression based
classifier  this feature vector was extracted using a
python pre processing routine 
v  m ultinomial naive bayes  
we trained a multinomial naive bayes classifier
in matlab  similar to the one we had in homework    we then retrieve the   skus with highest
probabilities  we performed    fold cross validation
on data sets of up to       training examples  as
we can see from the graph  the results were quite
disappointing  both in the training error  as well as
in the performance over the test data set  which gave
us a score of only       compared to the top score
of       therefore  although we seem to have a low
variance  we have a problem with severe bias 
vi  l ogistic regression and s oft  max
regression  
using the same feature vectors  we trained a
one versus rest logistic regression classifier with l 
regularization for each sku that appears in the
training data in matlab  we can then predict the
classification probabilities g it x  for each sku 
we then retrieve the   skus with the highest
it x for each x  which is our recommendation 

unfortunately this algorithm is quite slow   each run
with the full training data takes about    minutes  
which prevented us from doing a full k fold crossvalidation analysis  when we used a small subset
of the data  about      samples  to train the classifier  we obtained a cross validation error of      
the performance on the kaggle competition data
was also a much more reasonable       however 
training with a much larger data set of       
samples did not improve the performance too much 
cross validation gave us a score of        and the
performance on the kaggle competition test data
was actually slightly better         clearly  we still
have a problem  albeit smaller than before  with
model bias  
we also tried performing logistic regression using
the number of occurrences of ordered pairs of letters
 e g  aa ab ac      as well  this increased the
feature vector size greatly  which is usually a good
way of removing bias  however  this also increased
the computational complexity of the problem  we
tested a classifier trained on a relatively small
sample set of about      samples  we found that
the improvement compared to using single letter
frequencies was statistically insignificant  especially
considering the enormous increase in computational
requirements 
we also tried to add some features corresponding
to the date of the search query  the data was
collected over approximately three months  new
video games are usually released on tuesdays  with
major titles being reasonably well spaced  similar to
movie releases  therefore the time of a query can
also be helpful in predicting user interest in a game 
the dates were first binned into one of   groups  
about two weeks each  and assigned a feature vector
with   components based on their position in the
bin  using linear interpolation  this gave us a small
improvement in the score to       
we also implemented a soft max regression
solver for the same feature set  a batch gradient
decent based optimizer was built in matlab  this
solver was computationally slow  requiring over   
minutes to converge  the additional computation
had no benefit however  with soft max regression
giving us almost the same performance as oneversus rest logistic regression  with a score of       
we also attempted build svms for this classification problem  to build a multi class svm  we
trained a one vs many classifiers for each product 

fi 

as before  we used the functional margin as a
heuristic measure of the classifiers confidence level 
unfortunately  this technique does work very well 
and we could only get as score of       with a linear
svm and       using a gaussian kernel 
vii  w ord f requencies
after attempting supervised learning techniques
on the training and test set with the single character
 includes letter and numbers  frequencies  the next
step was to change the feature space of all words
used in all the queries  in other words creating
a binary vector that is of the same size as the
dictionary of words used in the queries from both
the training and test set  the methods applied to
this test set were the logistic regression and svm 
this turned out to perform poorer than in the same
algorithms as was used before for the training and
test sets that use the single character frequencies 
a suspected reason that this didnt get a higher
success rate is because the size of the feature space
was much bigger which resulted in over fitting 
one interesting thing that was found is that in both
cases logistic regression did better than the svm
classifier  results for each algorithm summarized in
the table below 
algorithm
test success
logistic regression
       
svm  with linear kernel 
      
viii  c lustering p roducts
part of the data given is the product name with
relevant information  such as date released  price 
sales rank  etc    the goal of clustering is to figure
out which products are similar to one another  the
end goal of doing this is to use this information in
order to help with the classification aspect of this
problem  by giving it slightly more accuracy about
which class it should be looking at  a few preprocessing steps were taken before clustering was
implemented  as a first step towards getting something to run columns were eliminated if they were
either not filled in for each product  corresponded
to a id number  or if all the values in the columns
took on the same value for each product  which
arose in the some of the boolean valued columns
where each entry for each product was labeled as
false   name of the product wasnt included since
it varied a lot  there were a total of     products

total and     features originally  after this preprocessing of removing irrelevant features  where
irrelevant is defined by the criteria listed above  
there are now a total of    features  all text features
were converted into bit vectors where each feature
of the bit vector was a word or sentence depending
on the text feature  once the pre processing is
done all rows  corresponding to each product  were
standardized via subtracting the mean and dividing
by the standard deviation for each row  standardizing each row makes all the numbers of similar
scale  therefore a euclidean distance norm can
be used  to heuristically determine the number of
clusters that should be used a spectral graph theory
technique was used  in which the laplacian matrix
was diagonalized and the greatest difference in the
each of consecutive eigenvalues was used      then
normal k means was used to cluster the objects  this
clustering technique didnt not yield meaningful
results as it did not group similar product names
together  e g  call of duty and call of duty ii  
so one modification was made in which products
with similar names were automatically clustered
together  which shall be denoted as pre clustering 
pre clustering now yielded     pre clustered objects together  now each row is the average of
each product in the pre clustered objects  the same
spectral graph theory technique was used to figure
out the number of clusters and kmeans should use
on these     pre clusters  the purpose of all this
clustering is to cluster the product names together
for the purpose of making the classification part of
this project more accurate  there will be two levels
of training happening  the classification algorithm
 svm or logistic regression  will train on knowing
which cluster each example belongs to  once that
is done another svm or logistic regression will be
implemented within that cluster to know which of
the products the user is interested in  in the end
this didnt not make the over all algorithms perform
better by doing this two step supervised learning
process via clustering 
ix  l anguage p rocessing techniques
the aforementioned techniques are generic tool 
and do not make use of the structure inherent in
search queries for games  there is a surprisingly
large degree of similarity between search queries 
as compared to even generic text  for instance  we

fi 

figure    a graph of the eigenvalues of the laplacian matrix plotted
in descending order  circles denote each of the eigenvalues  using
the index labeling given on the horizontal axis  the top   greatest
differences occur between consecutive eigenvalues of   and      and
     and      and    and   and   

found that after performing some additional preprocessing  like separating words from numbers and
eliminating spaces and other such non informative
characters  we found that there were only     
distinct word strings in the entire training data   even
without accounting for word order   this naturally
leads to the following idea  if each search query
corresponded to a particular game  wed only have
to look in our training set for the same query string
and thus find the corresponding product  of course 
this is not entirely true  since the same query could
lead to different products   especially if they are
just minor variants    so  we have to consider the
probability that a particular query corresponds to a
certain product  thus we can use an approach like
naive bayes  the multivariable version is especially
suitable   in particular  each search query is characterized by just two features 
   a concatenation of the words 
   any specific numbers that appear in the string
 this helps distinguish between items like
battlefield   and battlefield    

upon investigation of the cross validation examples that the algorithm misclassified  we found some
particularly interesting modes of failure 
   a change in just     words in the search query
caused the query to not match any training
example
   a search query for a game not searched for
in the training set  but the full name of the
product is present in the meta data file 
to tackle both these problems we use the following
heuristic  if a query has no matches  we separate out
the words in the query and compare this set of words
with the same set for all the other queries in our
training examples  we then replace this query with
the query with highest number of matching words 
we also add a small weighting factor to the match
based on the frequency the query  we developed
a scheme to efficiently compute the best matching
word for each query  essentially  this implements
a k nearest neighbor algorithm  we used k   for
simplicity  for the queries in word frequency space 
equipped with this function  we can now easily
handle the second case as well  by just creating a
dummy query based on the full name of the product
for each product in the meta data file 
in addition we also added the date based feature
discussed in the section on logistic regression 
using all these features together in our naive bayes
framework  we obtained our best score of      
on the kaggle private leaderboard  the whole algorithm takes less than    seconds to complete 
a graph of k fold cross validation performance for
this algorithm is shown in figure    we see that
the cross validation error does not converge to the
self validation error  a sign of lingering variance 
while this can often lead to over fitting  in this
case  a fundamental reason for the difference is that
during self validation  we have already observed all
the queries that we are testing  when we perform
cross validation  new variants of search queries can
and do appear   since no model exists for these
new variants  we are not over fitting in the classical
sense 

we built a naive bayes classifier based on this feax  c onclusions
ture set in python  to our surprise this gave a score
of       almost approaching the performance of our
we have implemented several algorithms and
naive  this is even more remarkable considering that heuristics to try and tame this massively multi class
the computation for this classifier runs in less than classification problem  we found that using general
   seconds 
statistical learning tools  such as logistic regression

fi 

 

    scikit learn  machine learning in python  pedregosa et al  
jmlr     pp                  

training set
xvalidation

    
   

map   score

    
   
    
   
    
   
    

 

   

 

   

 
dataset size

   

 

   

 
 

x   

figure    cross validation performance of multivariable naive bayes

and soft max regression can perform very well in
multi class classification problems  however  using the margin as a measure of confidence for
svms is a bad idea  and it is advisable to perform
some probability fitting as suggested in     before
performing multi class classification with on oneverses rest svm classifiers  we also observed that
some very simple and computationally fast learning
techniques  when coupled with a knowledge of the
structure of the data  can actually outperform theses
generic algorithms 
our final score of        the highest score was
       on the map   metric would have ranked
us   th among the    teams that participated in
the original kaggle challenge  we believe  further
improvements  such as adding a spelling correction
module can help us improve the performance of our
scheme further 
xi  acknowledgments
would like to give a big thanks to nick hahner 
allison carlisle  and paola castro for providing
a program written in python using scikit learn to
run some of these supervised learning algorithms
including svms  
r eferences
    data mining hackathon on     mb  best buy mobile web site
  acm sf bay area chapter http   www kaggle com c acm sfchapter hackathon small 
    mean average precision https   www kaggle com wiki meanaverageprecision
cs    lecture notes http   cs    stanford edu materials 
    soft max regression http   ufldl stanford edu wiki index php softmax regression
    cs    lecture notes  discovering clusters in graphs  spectral clustering http   www stanford edu class cs    slides   spectral ml pdf
    platt  john   probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods   advances
in large margin classifiers                    

fi