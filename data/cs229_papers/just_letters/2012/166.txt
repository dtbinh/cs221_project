smart music rating predictor
scott chao  huang song  liqun yang
introduction
product recommendation systems have been used to improve various user services  both consumers
and vender can benefit greatly from such system  our project focuses on improving users music listening
experience and presents a method for automatic music ratings prediction  for the purpose of this paper 
data was publicly gathered available music data base from yahoo music  yahoo music is one of the most
popular music website on the internet  providing extensive information about music and allowing users to
rate many music item  this rating predictor could be valuable to the music listeners by predicting a rating
for a music track that has not yet rated by the potential listeners and can be used to recommend a music
piece according to each persons music preferences  we had four different approaches to implement the
predictor  and some of them incorporate several algorithms  given a music track id  user id  the predictor
would automatically produce a rating  based on the rating  the predictor then recommends the music
piece if the rating prediction is above     k nn  k nearest neighbors  algorithm had best rmse root
mean square error            while the combined of k means  linear regression  and softmax
regression had the best correct recommendation rate           when the predicted rating and its actual
rating are both above below     it results a correct recommendation  
data selection
a data set containing text based music rating information from yahoo  music was obtained from
http   webscope sandbox yahoo com catalog php datatype c  this dataset contains ratings from
          users to         items including four different categories  tracks  artists  albums  and genres 
the total number of ratings is              this dataset is directly released by yahoo  music  all the
users and items are meaningless anonymous non repeatable numbers so that no identifying information
is revealed  the ratings are continuous numbers from   to      an item has at least    ratings and each
user has at least    ratings in this dataset 
data processing
considering this large scale of dataset and the limitation of time we have on this course project  we
decided not to use the complete dataset for this project and to focus on learning how to predict with
information of users and information of tracks only  excluding the artists  albums  and genres   we picked
       users and        tracks randomly from the original dataset  while we tried to keep the same
requirement on the ratings of users and tracks to ensure the quality of training  a music item should have
at least    ratings  and each user should have at least    ratings in this dataset  with this processing  we
were able to obtain a dataset with realistic information and reasonable size for this project  in the dataset 
each rating has four kinds of information associated with it  user id  track id  and date of ratings  all user
id s and track id s are consecutive integers  both starting at zero 
preliminary feature selection and model
the preliminary features were selected to be the two features coming with the dataset  user id  and track
id  the preliminary model was selected to be linear regression model due to the continuous prediction
nature of this problem  the root mean square error  rmse  of this model is         the predicted ratings
were all among rating of    to     which generated a relatively high rmse value  we then ran our
algorithm on training data set and found the similar result  therefore  we believed we had an under fitted
model in this case  the main problem of the preliminary is that the user id and track id are all
meaningless numbers  so training the computer to learn to fit this is meaningless as well  for example  if
user    rates    for track     it doesnt mean and usually wont be the case that user    will rate    for
track     thus it is necessary to modify the feature selection for better performance 
features selection
as the result described in the previous section  we made a dramatic change in our features selection
process  the data set was further processed to expand our features  we believed the features below had
higher correlations to the ratings 

fiuser features
   number of ratings the user made
   users rating mean
   users rating variance
   users rating standard deviation
   users number of zero ratings
   ratio of zero rating to all ratings 

music track features
   number of ratings assigned to the track
   tracks rating mean
   tracks rating variance
    tracks rating standard deviation
    tracks number of zero rating
    ratio of zero rating to all ratings

k means and linear regression
the linear regression algorithm under fitted the data set as described in the previous section  given the
characteristics of our data set  we then decided to incorporate k means algorithm into our linear
regression model with expanded features  we first used k means to separate the data set into   clusters
using features                            for each cluster  we trained our linear regression model using all
twelve features  but only the data set within the cluster  as a result  we had   linear regression models 
one for each cluster  for each input  we then first decided which cluster it belonged to and used the
corresponding linear function to predict the ratings  the resulting rmsd was         and the correct
recommendation rate was         the result was better than the pure linear regression model  however 
when we ran the algorithm on training
data  we still obtained a similar result
 rmsd was       and
recommendation correct rate was
         which indicated the model
was still under fitted  therefore  in
order to fix the problem  we then tried
to separate the data set into   clusters
using all twelve features  the result
was slightly better  resulting rmsd on
test data set was         and the
recommendation correct rate was
         figure   below shows the error
figure    error distribution using linear regression and kdistribution resulting from linear
means 
regression and k means model  the
red columns represent the model that
used   features for k means
classification  while green columns
represent the model that used   
features for k means classification 
the x axis stands for prediction error
ranges
 
 
and y axis represents the percentage
corresponding to each prediction
error range respectively  we can see
that the overall results of k means
classification with    features is slightly
better that of   features 

figure   score distribution using linear regression and k means

fik means  linear regression and naive bayes
figure   above shows the overall score distribution of the test set  x axis represents data indices and y
axis represents the score value  red dots in the graph indicate the actual rating in the test set  and blue
dots indicates our predicted scores  each pair of blue dot and red dot of the same index corresponds to
one pair of predicted score and actual rating  the order of dots in the graph is the result of score sorting
for easy indication  note that the prediction score bears a linear distribution in all regions  particularly  the
predicted scores that should be zero are mostly far from zero  this is the main reason for the high rmse 
therefore  we decided to incorporate naive bayes algorithm to our existing algorithm  naive bayes was
used to predict to see if the output rating should be zero or nonzero  the algorithm was trained on the
same training data with twelve features  if it predicts the output rating to be zero  then the output rating
should be zero disregarding the output from k means and linear regression model  on the other hand  if
it predicts the output rating to be non zero  then it should use the resulting rating from k means and linear
regression model  with naive bayes  the rmsd was       and the recommendation correct rate was
        the worse rmsd was caused by the miss predictions by naive bayes algorithm  the correct
prediction rate was only        and most of them were coming from correctly predicting non zero cases 
when it miss predicted  either assigned a non zero rating to zero or zero rating to non zero   the errors
were usually large  therefore  resulting a larger rmsd  to improve the prediction rate  we attempted to
implement svm  however  due to large training set         by      the run time for svm was too long 
theoretically  if the prediction rate
could reach       the rmse
would drop to       

figure    error distribution using linear regression  k means and classification
constraints

figure    score distribution using linear regression  k means and   class softmax
constraint

k means  linear regression
and softmax regression
besides using naive bayes
algorithm  we also tried to
incorporate k means and linear
regression with softmax
regression 
  classes 
the data set was divided into
three categories  greater than or
equal to     greater than   and
less than     and equal to   
softmax regression was first
trained on the training set with
twelve features and then
assigned a class to each input 
the output rating of k means and
linear regression is constrained
according to each inputs
category  the resulting rmsd is
         and recommendation
correct rate is        
   classes 
the data set was divided into ten
categories                    
                      and

fietc   softmax regression was trained on the training set with twelve features and then assigned a class to
each input  the output rating of k means and linear regression was constrained according to one of the
ten classes  the resulting rmsd is          and recommendation correct rate is        
figure   shows the error distribution of all three constraint models  the yellow  purple and green columns
represent the model that used naive bayes constraint    classes softmax constraint and    class
softmax constraint respectively  the x axis stands for prediction error ranges and y axis represents the
percentage corresponding to each prediction error range  note that although the    class softmax
constraint achieved the best rmse among the three  the detailed error distribution shows that it may not
be the best model we want to use  since the percentage for small errors  ranging from      is much lower
than the other two  the   classes softmax constraint achieved a better result in respect of the detailed
error distribution shown in figure    as a result  we believe the   classes softmax constraint should be
the best model so far  also note that in figure   the prediction for zero ratings are much better than that
without constraint  and this was exactly what we were expecting 
k nn  k nearest neighbors 
unsatisfying with the results  we
continued to search for a better
algorithm that could improve the
performance  we implemented userbased and track based k nearest
neighbors in our work  during the
training process  the algorithm will be
trained to find the top    most similar
users for each user and the top    most
similar tracks for each track  the
similarity here is defined as the
correlation between the ratings  two
users will be more similar to each other
if they have rated more tracks in
common and those ratings are less
deviated  two tracks will be more similar
figure    prediction difference distribution of k nn model
to each other if they have been rated more
by the same users and have received closer ratings  with the information  a new rating can be predicted
by taking the mean of the ratings made by the similar users on the similar tracks  however  with this
approach  there is a possibility that these particular tracks have not been rated by these users before 
should this happen  a rating of zero will be used as prediction for simplicity 
k nn gives relatively promising predictions  the performance of this algorithm relies on the static user
preferences and track popularity  the algorithm will not give good predictions if the user develops a new
taste  however  by updating the training set frequently  it is possible to adjust this algorithm dynamically 
the implementation of this algorithm can be improved by using the time  date of the rating and the extent
of similarity as weighting parameters when making the prediction  due to the overall good performance of
k nn  it takes a long time to finish the training  moreover  the complexity of the algorithm increases
dramatically with larger dataset 
overall comparison

fifigure    rmse and correct prediction rate comparison
figure   shows the final results of all the models we have used  the blue bar is the rmse  and red bar is
correct prediction rate  from left to right are the results of models that used linear regression only  linear
regression     feature k means  linear regression      feature k means  linear regression      feature kmeans   naive bayes classification constraints  linear regression      feature k means     class
softmax classification constraints  linear regression      feature k means      class softmax
classification constraints  and k nn  to find out the best model  we need to balance the results of rmse
and correct prediction rate  from figure   we found that linear regression      feature k means     class
softmax classification constraints and k nn should be the two best candidates  the former has the best
correct prediction rate with an acceptable rmse  and the latter has the best rmse with an acceptable
correct prediction rate  while its hard to tell which is better in terms of their results  we note that the
former model is much more efficient than k nn  since k nn takes much more time for computing than the
former model  so the linear regression      feature k means     class softmax classification constraints
model should be the best model that well pick for implementing actual application 
conclusion
the combination of k means  linear regression  and softmax regress produced the best correct
recommendation rate          and k nn had the best rmse         however  theres still room for
improvements  if the classification can correctly predict the output to be a zero rating  rmse can reach as
low as       and the recommendation provided by the predictor can be appreciated by the users       
of the time  however  we do believe the data set inherently possesses some degree of randomness 
since not every user would seriously rate every music pieces  therefore  its challenging to achieve higher
correct prediction rate 

reference
    a linear ensemble of individual and blended models for music rating prediction  by po lung chen 
chen tse tsai  yao nan chen  ku chun chou  chun liang li 
cheng hao tsai  kuan wei wu  yu cheng chou  chung yi li  wei shih lin 
shu hao yu  rong bing chiu  chieh yen lin  chien chih wang  po wei wang 
wei lun su  chen hung wu  tsung ting kuo  todd g  mckenzie  ya hsuan chang 
chun sung ferng  chia mau ni  hsuan tien lin  chih jen lin and shou de lin 
department of computer science and information engineering  national taiwan university 
     informative ensemble of multiresolution dynamic factorization models  by tianqi chen
  zhao zheng  qiuxia lu  xiao jiang  yuqiang chen  weinan zhang  kailong chen and yong yu 
shanghai jiao tong university 

fi