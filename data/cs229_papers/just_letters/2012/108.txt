cs       final project report
speech   noise separation
ceyhun baris akcay
stanford university
department of electrical engineering
stanford ca
cbakcay stanford edu
abstractin this course project i investigated machine learning
approaches on separating speech signals from background noise 
keywordsmfcc  svm  noise separation  source separation 
spectrogram

i 

introduction

speech processing is a highly popular research subject  as
it is a common problem in all signal processing tasks  speech
processing is also adversely affected by noise in the
environment 
one of the applications that is highly susceptible to noise is
indubitably speech recognition  intuitively thinking about it 
the task is to identify words from a speech signal  if this system
is going to be useful it has to be invariant to different speakers
 different pitch  timbre  etc   so for example if there are
multiple people speaking  i e  the noise is a superposition of
other speech signals  this should obviously deteriorate
performance  this in fact is the truth as described in     
there is a great deal of literature on this topic  various
methods are used  such as blind source separation      utilizing
neural networks      using probabilistic models      using
multiple microphones to utilize the spatial difference of the
sources  etc  here i will focus on machine learning approaches 
speech signals are quasi periodic signals  whose periods
are called pitch       this is largely due to our vocal cords
generating a periodic sound wave which is modulated in our
mouth  speech formation can be considered as a periodic signal
 the periodic sound waves  being passed through an lti filter
 the mouth       this nature of speech signals give them unique
properties that can be exploited for detecting  isolating even
separating them  the fact that different peoples voices have
different pitch is one major aspect used in separation  in fact
such harmonic characteristics of the signal can be very easily
seen in a spectrogram     an example is in figure   
the rest of the report will first present some of the literature
on the topic  followed by a short summary of speech signals
that is relevant to the project and finally my method and results
will be presented 

figure    an example spectrogram of me saying out my name  the
red parts are the dominant frequencies

ii 

literature review

a  spectral learning for blind seperation
bach and jordan  in     investigate a method for blind
speech separation using a single microphone  their idea is to
segment the spectrogram of the given signal into multiple
disjoint segments such that each segment will correspond to the
speech of one speaker 
firstly they construct feature vectors for speech signals that
can be used for separation  they use non harmonic cues such
as continuity and common fate  as well as harmonic cues such
as pitch and timbre  for timbre they use the spectral envelope
and for the pitch cues they have developed a novel algorithm
that can identify multiple pitches in a windowed portion of a
speech signal and their corresponding dominance within the
window  roughly corresponding to amplitude of the pitch  
obviously for the continuity determination  entire spectrogram
needs to be used  as a result the feature space used is very high
dimensional  they use an efficient implementation of
parameterized affinity matrices to ultimately perform the
segmentation 

fib  an approximation to the em algorithm
in      frey et  al  propose a different approach for learning
noise from a single microphone recording of noisy speech 
they model the speech and noise parts of the signal as
independent signals consisting of a mixture of gaussian
distributions  gmm   by using a mel frequency scale  book 
and assuming that the noise and speech have little phase
correlation  they approximate the magnitude squared of the
signal to be                      where s denotes
speech and n denotes noise 

the resulting coefficients exhibit great difference between
speech and non speech  for speech signals it has a certain
shape in the low quefrencies  corresponding to the vocal tract
filter characteristic  and a few impulse like features in the
higher quefrencies which correspond to the pitch in the
speech 

they use algonquin to obtain the posterior probability
distribution using a vector taylor series approximation  they
use this approximated probabilistic inference method as the estep in the em algorithm  i will not go into every detail here  
finally by approximating a lower bound on the data  and
refining it  they do the m step of the em algorithm 

c  neural networks
although there are interesting techniques in the literature
using neural networks as well  i found that these techniques
are not very familiar to me and the aforementioned ideas were
more interesting given that we are learning the basic theories
behind them in class 
figure    the mfcc pipeline  figure taken from     

iii 

preliminaries

before i begin with my results i would like to give a brief
overview of the speech concepts considered
a  the logarithmic frequency phenomenon
the human auditory system is not uniform in frequencies 
the perception of one frequency is usually very different than
another  less or more sensitive  and usually is completely
insensitive to any frequency below    hz or above    khz     
it turns out that the human sensitivity to sound is an
approximately logarithmic function of the frequency as well as
the amplitude  we are not interested in the amplitude in this
project   this is why it is a commonly employed practice to
represent the frequency logarithmically      the mel frequency
scale is one such logarithmic frequency scale commonly used
in a variety of applications from estimating the vocal tract
frequency response to pitch determination     
b  cepstrum and the mfcc
cepstrum or cepstral analysis is a simple transformation of
the frequency spectrum of a signal such that multiplications in
the frequency domain  which are convolutions in the time
domain  become summations in the quefrency cepstrum 
which basically means the logarithm of the frequency
spectrum 
mfcc  mel frequency cepstral coefficients  is especially
useful in determining characteristics of a speech signal  e g 
pitch can be very easily determined from it   mfccs are
calculated by first warping the magnitude spectrum of the
signal to the mel scale  which is done by a mel filter bank 
followed by taking the logarithm of the warped magnitude
spectrum and ultimately followed by a dct     

iv 

the system

a  identifying parts in the signal that contain speech
the first step is to determine which frames of the signal
contain speech  as described above the mfccs for speech and
non speech signals differ in nature  what i did was to take   
different speech signals from split them into    ms windows
and to label these portions of the signal as speech or nonspeech  obviously such a short duration audio signal is not
enough for humans to perceive it as speech or non speech even
less when it is not a rectangular but a hann window  so i
labeled these windows as speech or non speech by looking at
the histogram 
after the labeling  using the liblinear package for
matlab i trained an svm classifier on them  ultimately this
classifier was used on the speech windows for classification as
speech and non speech  in total there was     features for the
svm  note that even though    ms equals to     samples at   
khz  for efficient dft computation i preferred a     point
dft of the     point window 
this classifier achieves approximately     testing error
and    training error  the main cause for the error is my
coarse labeling of the training data  unfortunately looking at the
histogram it cannot be done much finer  a second cause is
speech is not always periodic pulses modulated in the vocal
tract  for example sss of shh kind of sounds are more like
white noise passing through the vocal tract so do not have a
very distinct mfcc signature as do the voiced syllables     
and a second source of error is some windows capture both
silence and speech together  overall these effects caused a
more than desired error rate but  still not unreasonably high  so

fii decided to continue with this method instead of trying more
sophisticated pitch tracking kind of algorithms for this
classification 

                             



          

   
            
         


b  the denoising process
for the denoising procedure i preferred to use the
approximate em algorithm described above  now i will
provide some further details on this algorithm called
algonquin     

where    and    are the approximated means of the
speech and noise for the aforementioned classes  the s are
the covariances between the speech and noise according to
their superscript and all of them are diagonal  this follows from
the assumed priors 

first of all the energy spectrum of the signal plays an
important role  their approximation which follows from the
assumption that the noise and speech are uncorrelated is given
below 

so the approximated posterior for one window is given as
below 

                    

   

let y  s and n be the vector of logarithms of each of the
terms  for every value of f  in equation      then we can write 
                     

the multiplication is element wise  taking the logarithm of
the above expression we end up with the function below 

                   
   

assuming the errors in equation     to have a guassian
distribution  the observation probability is given by the
expression below 

                   
   

where the  is assumed to be a diagonal covariance matrix for
ease of implementation and simplicity 
the prior distributions of the speech and noise are assumed
to be independent mixture of gaussians  which makes the prior
probabilities of both as below 
                           
               

                            
               

where cn and cs are the noise and clean speech classes
respectively and the covariance matrices are both assumed to
be diagonal  combining the priors their independence and
equation     for the conditional probabilities they arrive at the
joint distribution below 
                

                            

as we can see with this joint distribution                
is not a mixture of gaussians and hence to use an em
algorithm needs some further manipulation or simplification of
the distribution 
this is done by approximating the posterior probability for
the parameters of the current noisy speech windows as below 

                                    

where          is the mixing proportion for the speech and
noise classes 

so the goal is to minimize the relative entropy between the
p and q distributions aforementioned  since the relative
entropy is in effect a measure of the difference of two
probability distributions  also note that ln p y   does not
depend on any of the variational parameters defined before and
minimizing the relative entropy  rl  corresponds to
maximizing ln p y    rl which is a lower bound on the loglikelihood 
as a result this method called variational inference can be
used to approximate an e step in the well known em algorithm
    
also note that the relative entropy introduces nongaussianness into assumed gaussian distributions  this is
prevented by using a second order taylor series approximation
to the relative entropy computation  the full update equations
are pretty lengthy and wont fit into a double column formatted
paper in a readable manner so interested reader is encouraged
to check out     for the full equations as well as certain
derivational details that i omitted here for brevity 
v 

results   discussion

my data was taken from the pascal chime challenge  i
corrupted the data by additive white gaussian noise with
varying snr values  the overall system was able to achieve
between   db to    db improvement in the snr  compared
against the pure speech  the improvement does not depend
much on the input snr  different speech signals with same
snr sometimes produced      db and       db  the one
thing that has to be really worked on is a higher accuracy
speech
and
non speech
classifier 
because
this
misclassification ends up pretty audible at times  you here
sudden onset of high white noise   i am not very satisfied with
the results  i thought    db would have been possible  in    
they do not provide a snr analysis  they just provide the
improvement achieved in a sample speech recognition task 
vi 

conclusion

this project firstly gave me the chance to explore a very
interesting field in signal processing which is speech
processing  such an extensive literature survey provided a
great depth in usages of machine learning as a noise
cancellation technique  also to implement an em algorithm

fifor the final system gave the opportunity to solidify my
understanding of the em method  all in all i have benefited
from the project greatly 
references
   
   

x  huang  a  acero  h w  hon  spoken language processing  prentice
hall       
b  frey  t  t  kristjansson  l  deng and a  acero  algonquin 
learning dynamic noise models from noisy speech for robust speech
recognition  advances in neural information processing system       
pp            

   
   

   
   

f r  bach and m  i  jordan  blind one microphone speech separation 
a spectral learning approach  frbmi        pp        
j  tchorz  m  kleinschmidt and b  kollmeier  noise suppression based
on neurophysiologically motivated snr estimation for robust speech
recognition  advances in neural information processing systems 
     pp          
http   www sciencedirect com science article pii s                 
last visited on            
r  m neal and g  e  hinton  a view of the em algorithm that
justifies incremental sparse and other variants  learning in graphical
models  pp           kluwer academic publishers  norwell ma       

fi