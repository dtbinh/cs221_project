optimal vtol of
spacexs grasshopper
        

brian mahlstedt

ficontents
motivation                                                                                                                          
results                                                                                                                                
reinforcement learning theory                                                                                        
 dof inverted pendulum                                                                                                  
rl algorithm construction                                                                                               
 dof grasshopper problem statement                                                                             
 dof plots                                                                                                                        
future work                                                                                                                       
appendix    dynamics                                                                                                      
notes assumptions                                                                                                        
derivation of the equations of motion                                                                          
appendix    source code                                                                                                 

motivation
grasshopper is spacexs prototypical first stage  a testbed for vertical takeoff and landing
 vtol  to be implemented on future generations of reusable falcon   launch vehicles 
traditional optimal control algorithms such as lqr and lqg could achieve comparable
results  but reinforcement learning provides the infrastructure to adapt to unmodeled
scenarios  this capability to update and learn a new optimal policy makes rl an
attractive solution for the unpredictable conditions of landing a rocket based launch
vehicle on mars  i have already developed lqr and lqg controllers through prior
experience  so direct comparisons between the robustness of the controller and the
disturbance rejection of the closed loop plant will be observed 

results
i successfully derived a model for  simulated  trained  animated  and produced a fully
functional and stable controller for a virtual grasshopper vehicle  the controller performs
wonderfully in the simulator  but was not tested on hardware  a processor rated at
   gflops might need around   hrs to confidently learn an optimal policy from
scratch  such is not ideal for realtime architectures  but an mcu may deploy a prelearned algorithm with reasonably power budgeted adaptability  building upon a
previously optimal policy to accommodate disturbances is significantly less of a
computationally intensive task  and may be performed onboard in near realtime 

fireinforcement learning theory
a comprehensive theoretical discussion is not appropriate here  but ill provide my staple
equations 
  

       max                   


  
  

       argmax                   


  

a markov decision process  mdp  tuple is logged at every timestep of the dynamical
situation  updating its state transition probabilities  and reward function  at every
failure  it then employs synchronous value iteration by solving bellmans equations to
find the optimal value function   and policy   for this new mdp model 

 dof inverted pendulum
before embarking upon the dynamical  computational  and theoretical complexity of the
  degrees of freedom rocket  the logical initial testbed was a standard inverted pendulum 
in machine learning  preliminary optimization before practical application often leads to
convoluted code  so simplistic beginnings are always preferred  all innovate ideas 
peripheral script calls  animation tweaks  parameter manipulations  all were vetted in this
simple springboard before being implemented upon the full system  this led to
streamlined debugging  better state space discretization  more efficient code structure 
and many more benefits  because of the parallelization of both simulators  the only
difference being the equations of motion that propagate the dynamics   the following
discussion of  dof conclusions is identically applicable to the  dof launch vehicle 
fig    shows the simulator in
action  the command window
updates to display the number of
failures  value iterations  and
erection time  fig    shows the
binary output of the optimal
policy  which is updated at each
failure  figures   and   show
convergence of the algorithm 
with the number of value
iterations decreasing as the
policy learns  and the percentage
of actions that are randomly
chosen decreasing as the policy
figure     dof inverted pendulum simulator
becomes more deterministic 
videos of various sequences of
trials may be viewed at http   www youtube com watch v vj dppqibae and
http   www youtube com watch v sku ihwjgn   these animations are extremely useful
for building intuition for the strategies and capabilities of reinforcement learning 

fifigures      optimal policy  random action selection    value iterations per failure

rl algorithm construction
the following  fig     shows the ultimate output of the simulation  a learning curve of
success time vs  number of failures  it exhibits initial learning and then plateaus 
appearing to converge upon a local
optimum because intuitively  the global
optimum is a policy that gets the
inverted pendulum to remain erect
forever  this is a product of multiple
sources of nonconvexity      the
simulator gets initial rewards for not
failing  then associates those state action
pairs with reward  then repeats those
patterns in a routine that encompasses
only a small fraction of the state space 
this can be mitigated with the
introduction of randomness into the
simulation  making it less greedy  i
figure    learning curve
have manifested this solution in my
dynamics by adding noise to the forcing
function  and providing a small probability of an action failing or the opposite of the
intended action occurring  i have also expanded the range of state reinitialization  forcing
the simulator to start in very poor states it might never find itself in  both solutions
provide a much more comprehensive final policy      the second source of local optima
is poor coarse discretization of the state space  this can be mollified by logical evaluation
of where accuracy is needed  for this simulation  the angle from vertical is more
significant of a metric than the lateral position  so that should be discretized more finely 
this  along with manipulation of the limits defining failure  provide the simulator
encouragement to explore the entire state space intelligently with precision in the regions
that need it  using these techniques  i was able to tweak the simulation and get the
pendulum to remain inverted forever 
the following plots illustrate the difference in selection of initial conditions for the state 

fifigures      r  v  and learning curve for uniform state reinitialization

figures       r  v  and learning curve for gaussian state reinitialization
uniform state reinitialization entails picking a new state  upon failure  at random from
the entire state space  gaussian state reinitialization draws the state from a normal
distribution  centered around each state element being zero  good conditions     
and       figures       vs          showing the optimal reward and value function 
reveal the expected result that the uniform reinitialization forces the algorithm to explore
more of the state space  yielding a better result less subject to local optima  but fig    vs 
fig     shows the downside of the uniform distribution  the gaussian state
reinitialization yields a much better learning curve  because it generally starts in better
conditions and can thusly remain erect for longer  in practice  gaussian state
reinitialization is a good default  and uniform state reinitialization should be used when
urgency of learning is a factor  if modularity is an option  starting with uniform for initial
learning and continuing with gaussian once it has gained robustness is the best
procedure 
note also that the reward and value functions make logical sense  there are three main
peaks representing the   range discretization of the angular velocity  each of those has  
peaks for the   range discretization of the angular position  and so on for the   linear
velocity and   linear position subsequent peaks 
here is an extensive list of the parameterized variables in my simulation  all of which i
implemented  and the conclusions gleaned from their options 
   synchronous value iteration  found to be slightly faster than asynchronous 
   asynchronous value iteration  converges to same optimum as synchronous  all
of these iteration schemes initialize from the previous value to encourage speed 
   policy iteration  wrote a separate function to solve bellmans equations for the
value functions   it often took fewer iterations to converge  but ran a little more
slowly 
   state   action rewards  slowed the simulation down  but provided great results
and flexibility 
   do nothing actions  expanded the action space  have to be careful this isnt

firewarded too highly or it will always fail  should weight state more than action  
   ode solver  using a higher order intelligent ode solver to propagate the state
increases accuracy and convergence  but is not worth the extra computational
time complexity if the step size is small enough  which     s is  
   dynamic parameters  earths gravity is around  x stronger than mars  the
pendulum remains inverted for shorter periods of time 
   tolerance for value iteration       is usually a good combination of
time accuracy  the lower this is  the longer the simulation will take to converge
on a new value function after each failure 
   threshold for single convergence     is usually appropriate  increase to force
more learning stability 
    state failure limits  relax them a bit to allow the controller to move into a new
regime of action state pairs 
    discount factor       has been working well  decrease this to make the
algorithm weight quicker rewards more heavily 
    dynamics timestep      s is fine for quick use  but use      s for rigor  the
finer the time step  the more accurate the physical simulation 
    convergence criteria  assert that value iteration has converged after the mean
of all value function states has differed below some tolerance  rather than waiting
for the worst case to converge  this accounts for those future trials where the
number of iterations jumps very high after a seeming run of cumulative value
iteration success 
the final parameter  which deserves its own section  is the reward function  simulation
behavior varies significantly with selection of different rewards  initially  i employed the
basic reward function     for failure  and   otherwise  this kept the pendulum inverted 
but did nothing for regulation of the state  i e  all states that werent the failure state were
considered equal  a better reward function weighted both  where a higher reward was
given for  or  being zero  and at the limits of the acceptable ranges  the reward was
zero  an even better reward function considered the action as well  rewarding for  or 
or  being zero  the reward function i ultimately used implemented all of these  where
there were weights for both individual states against each other  as well as total state vs 
action  ala lqr controller weighting   the reward function also didnt give   for the
limits of the state space  it instead gave    to ensure failure was duly avoided  this
reward function behaved spectacularly 

fi dof grasshopper problem statement
im defining the altitude and lateral
position of grasshoppers center of
mass with        and the
rotational position with euler
angles         which are
respectively yaw  pitch and roll 
the full derivation is in the
appendix  but im not assuming
small angles  im restricting my
euler angles to avoid singularities 
im neglecting planetary rotation 
gravity differentials  drag  mass
loss  and im approximating the
rocket as a cylinder  these are all
extremely reasonable assumptions
figure     configuration  credit  nasa glenn 
for a free rigid body in
unconstrained space  note that  is
the altitude  and therefore the gravitational force is in the   direction 
my state is therefore
                



and the resulting equations of motion are

                      
 

                       


  
           

  



    
                   

 

                       


  
                   
 
  

my state space limits yaw and pitch to  o  roll is unlimited  after intelligent
discretization of these ranges  my state vector ultimately has      elements     for the
failure state  

firegarding force capability  ive modeled grasshopper with an axial merlin  d engine 
theoretically capable of        lbs of thrust  ive enforced that this main rocket is always
firing  there are effectively   cold gas thrusters on grasshopper for lateral stability  one
in each quadrant  capable of firing in one direction  this yields a   dimensional action
space   basically two orthogonal thrusters that can fire         the axial m d has
thrust vectoring capability of around   o  which simply manifests itself by increasing the
thrust of the lateral cold gas while decreasing the axial force  ive incorporated this 
an integral part of coding any simulator from scratch is the development of an accurate
model  since i have appropriate kinematic expertise  i chose to derive this explicit model
by hand  coupled with the deterministic action space  the simulator proved to be very
robust 

 dof plots
here are some plots from the output of
my  dof grasshopper
simulations  ive had to
tweak a few functions to
get everything to run 
but the inherent
difference is almost
literally reduced to
substitution of the
equations of motion 
therefore  my extensive
analysis of the inverted
pendulum rl applies
equally here  so turn to that section for
rigor 

figure     learning curve

figure     state transition probability matrix

fig     shows  for a specific station transition probability matrix  it remains nearly
diagonal  as expected  you are most likely to transition to a similar state  it is extremely
sparse  which expresses the relative benefit of other reinforcement learning schemes like
q learning  it order to get intuitively useful information from these plots  wed have to
visualize the data in higher dimensions because of the discretization of the state space 

fifigure     grasshopper simulator

future work
here is a short list of the subsequent endeavors id pursue next with the simulator   given
the fact that i single handedly took an rl problem from scratch  derived a model by
hand  simulated it  learned it  and implemented it in an animation within two months  im
fairly satisfied with the length of this remaining work section 
   continuous state mdp  discretizing the state space while still accurately
describing the pose well enough for a decent policy has been hard  ive had to be
very crafty in distributing the state space  matlab runs out of memory while
trying to store  when it gets up to around                which is
extremely easy to surpass 
   vertical landing  id have to remove the singularities by introducing a second 
redundant set of euler angles  then invert gravity upon stabilization 
   time varying dynamics   finite horizon mdp  inclusion of orbital rate 
changing gravity  etc  the finite horizon doesnt have to be something
chronological  id likely choose to step forward with respect to altitude  such that
when       times up 
   solidworks animation  ive worked with importing  d solid models and
rendering them within matlab before  which would have resulted in nice
animations  albeit much longer computations  
id like to acknowledge dr  andrew ng and the entire teaching staff of cs    for their
dedicated commitments in profession of this course 

fiappendix    dynamics
notes assumptions









im not assuming small deviations from the nominal vertical position  which
would result in   decoupled  dof inverted pendulums 
my restoring forces are not in the inertial frame  as a pendulum on a cart  mine
are in the body frame  and must be projected back into the inertial frame 
i am neglecting mass loss   working with the rockets principle coordinate frame 
which removes the   and   terms from newtons and eulers
equations 
using euler angles with a set sequence           and set ranges of yaw
         pitch          and roll          there is only one singularity where
multiple euler angle sets yield the same attitude  this is the singularity parallel to
the z axis  to circumvent this  i can 
a  restrict pitch from ever fully reaching     or     then orient the body
frame so that this singularity is lateral  not along the important rocket axial
dimension  this is the workaround that i have implemented 
b  define two regions of my state space  one using euler angles with respect
to one frame  the other using another set of euler angles with respect to
another frame  depending upon the configuration  pick the euler angles
with no singularity 
im neglecting the earths rotation  although that would be very easy to introduce 
ill approximate the principal moments of inertia with a cylinder 
angular momentum  linear momentum  and energy are all quantities that are not
conserved 

derivation of the equations of motion 
my state is 
                



so the goal of eom derivation is to get 

  
                 

which can subsequently be propagated to obtain the next state with a simple  st   order
euler algorithm or a more complicated higher order solver native to matlab 
kinematic relations

      

fi



 
    
 

 



  
     
 


      
   

 
     

 

 
 
 



 


   
   

 
   
 

  


the inverse of this matrix yields a passive rotation matrix that represents a vector with
body coordinates in the inertial frame  because rotation matrices are orthogonal  the
inverse is equal to the transpose 


    


   
   


         

  

   
      




  
 
 

         

 
 


                           

 
 
  
 
 



 


                 
 
  
   

  

     
  

 






 
      
 

 
     
 


notice that there is an overhead singularity at         where a perfectly vertical pitch
would eliminate a degree of determinism from the configuration  in such an orientation 
there is an infinite quantity of euler angle sets that would describe that attitude of the
rocket 
aside        euler angle development would yield 
      

    
 



 

   
 
 
 
    
    


 
 
 
 

 



 

 
   
 

fi

  
     


   
   


  

       

         


  


  
   


         


  


         

 
 


                            


 
   



                  
 

   




 
      
  
 









 

  
       
  

  
       
  

notice that again there is a singularity along the z axis at        where an upright pose
would eliminate a degree of determinism from the configuration  in such an orientation 
there is an infinite quantity of euler angle sets that would describe that attitude of the
rocket 

force balance

newtons equations 



             
 
 



  
      
      
 
 

the thrusters exert forces in the body frame  so we must express these in inertial
coordinates 




 



      

                     
  


 




fi


       
 
 

   
   


    

               
 

 

eulers equations 

in body frame 





             


  





        
 



 

 

where 

 
  


the rocket is approximated as an axisymmetric prolate cylinder  and the moments of
inertia are with respect to a body fixed frame with coordinate axes aligned with the
principle axes of the body 
 
        
  



      
 

 

                
 
  





 

 

               
 
         



taking derivatives of our body rate   euler angle rate relations 
       
                
               

    
 

       
                                
                        

which will be solved for and propagated in matlab 
lagrangian formulation
gravitational potential energy  referenced from earths surface 



fi   
translational kinetic energy of center of mass  and rotational kinetic energy about the
center of mass 
 
  
                  
   

 
 
 
 
 
                                    
 
 
 

 

  
 

 
 
 
                                                       
 
 
 

taking derivatives with respect to our generalized coordinates 
  

   
  

  

   
  

  

   
  

  

   
  

  

   
  

  

   
  

where the generalized forces simply must be transformed as before 

 

   

                       
  
  



but the new generalized moments must be explicitly calculated 




         








         








         




evaluating these derivatives yields our desired equations of motion for all six generalized
coordinates 


         



fi       

                
       
 

                     
which can be formulated into the same result derived via force balance  isolating the
state accelerations  we obtain our final update formulas for the simulator  all forces and
moments are in the body frame  

                      
 

                       


  
           

  



    
                   

 

                       


  
                   
 
  

fiappendix    source code
main m
   main control script
  cs    final project
  brian mahlstedt
  introduction                                                             this simulator employs reinforcement learning to optimize the
thruster
  policy for vtol of the spacex grasshopper vehicle 
     determine best action for current state
     run the dynamics to get the next state
     count the rewards and transitions
     if it hasn t failed yet  return to step   and continue simulation
     if it fails  use the total rewards transitions to update the
markov
 
decision process  mdp  model by calculating a new reward
function r
 
and station transition probability matrix psa
     reinitilize state and repeat     until learning has converged
cd  c  users brian dropbox graduate work cs    final project code  
clear all close all clc
tic   begin timing
  parameter definition
max fail           max failures allowed  should converge before
this 
disp start        start display after this   of failures  can be
used to
  rush through initial trials and only display after the mdp
model is
  reasonably accurate  or set equal to max fail to never display
 fastest
  simulation   or set to   to visualize from the beginning 
gamma           discount factor  emphasizes earlier rewards 
tol           convergence criteria for value iteration 
no learning threshold         consecutive runs where value iteration
  converges in one iteration  near perfect mdp model 
dt          time step resolution for the dynamics of the simulation 
success time            if the sim balances for this many seconds  it ends
  and considers itself a success  no matter the state of the rl
algorithm 
ns                    number of states  the last state denotes
failure 
na        number of actions 
  initialization
consecutive no learning        number of trials with near perfect
mdp model
nfail        number of failures 
iterations        counts simulation iterations 
iter start        number of sim iterations at start of current
trial 
success        boolean to indicate the success condition 
numrandact        counter for actions that were selected at random 
  starting state
x      y      z     
x dot      y dot      z dot     
psi     pi       randn  psi dot       pi       randn 
theta     pi       randn  theta dot       pi       randn 
phi       pi          randn     phi dot       pi       randn 
s   state psi psi dot theta theta dot phi phi dot     discretized
state 
  preallocation
iters fail   zeros max fail     sim iterations until failure 
randact   zeros max fail     percentage of random actions per
failure 
vp iter   zeros max fail     number of value or policy iterations
until
  convergence 
psa counts   zeros ns ns na     these count the transitions that
happen
  over multiple timesteps between failures  old state  new state 
action  
psa   ones ns ns na  ns    each layer is a matrix  where element
 old state 
  new state  action  shows the probability that taking action k
in state i
  will result in state j  this is updated at every failure  initialize
  asa uniform distribution  same likelihood of reaching any
state  
r counts   zeros ns       counts the reward over multiple
timesteps between

  failures  the first column is the new state and the rewards observed
  there  the second column counts each time you went to that new
state 
r   zeros ns       current reward function associated with each
state 
  only updated at every failure 
v   zeros ns       total expected discounted future reward for
each state 
p   zeros ns       optimal policy  this is an array that simply
contains
  the best action to take in each state to maximize v 
  animation
if disp start   max fail
warning  off   matlab delete filenotfound   
delete  animation mp   
vidobj   videowriter  animation mp    mpeg     
vidobj quality       
vidobj framerate     dt    real time
open vidobj  
end
  main simulation loop                                                   while  nfail   max fail        
 consecutive no learning   no learning threshold 
  determine the best action for the current state based on psa
and v
  s is just passed to action and reward to check for failure
state 
 a coin    action psa v s psi theta  
if coin
numrandact   numrandact   
end
  get the next state by simulating the dynamics
 x y z x dot y dot z dot psi theta phi psi dot theta dot phi dot 
     
dynamics a dt x y z x dot y dot z dot    
psi theta phi psi dot theta dot phi dot  
new s   state psi psi dot theta theta dot phi phi dot     discretize 
if nfail     disp start
simdisplay x y z psi theta phi  iterations iter start  dt    
iters fail nfail  dt nfail vp iter nfail  a  
writevideo vidobj getframe gcf   
end
iterations   iterations     
  reward function
rew   reward new s psi theta a  
  update transition and reward counters
psa counts s new s a    psa counts s new s a      
r counts new s      r counts new s       rew    
  if simulation fails
if  new s    ns 
  compute new mdp model  update psa and r 
for k     na   loop over action layers 
for i     ns   loop over old states 
total   sum psa counts i   k   
if  total        does nothing if state action hasn t been
tried 
psa i   k    psa counts i   k  total 
  calculates probabilies of all next states from this
old state 
end
end
end
for i     ns
if  r counts i           checks if we ever got to that new
state 
r i    r counts i    r counts i    
  reward associated with each state  updated every failure  it is
  the sum of all the rewards noticed when you got to that
state
  divided by the number of times you went to that state 
end
end
  value iteration to optimize v for this new mdp model
iter        loop counter 
change        just set higher than tol to start while loop 
new v   zeros ns       preallocate v array to compare to previous
allvs   zeros   na     preallocate array of  v s for each action

fiwhile change   tol   change   all differences in the value
vector   tol
iter   iter     
for i     ns
 psi   theta          state x i  
for k     na
  synchronous   loops through states  then updates
v s  all
  at once
allvs k    reward i psi theta k   
gamma psa i   k  v 
 
  asynchronous   updates each v s  as it changes
 
allvs k    reward i x theta k   
psa i   k  new v 
end
new v i    max allvs     update using a 
end
change   max abs v   new v      can change to mean and make
tol tight
v   new v 
end
 
  policy iteration to optimize v and pi for this new mdp
model
 
iter        loop counter 
 
change        just set higher than tol to start while loop 
 
new p   zeros ns       preallocate policy array to compare
to previous
 
for i     ns
 
p i    action psa v i psi theta     determine policy 
just once before loop
 
end
 
while change   tol   change   all differences in the value
vector   tol
 
iter   iter     
 
v   bellman psa p r gamma     solve bellman s equations
for v
 
for i     ns
 
new p i    action psa v i psi theta     determine
best action
 
end
 
new v   bellman psa new p r gamma  
 
change   max abs v   new v      can change to mean and
make tol tight
 
p   new p 
 
end
  check if convergence occurred in one iteration  no learning 
if  iter      
consecutive no learning   consecutive no learning     
else
consecutive no learning     
end
  update counters
nfail   nfail     
vp iter nfail    iter 
iters fail nfail    iterations   iter start 
iter start   iterations    start time for next trial 
randact nfail    numrandact iters fail nfail       of random
actions 
numrandact        restart counter 
  calculate policy pi  from v   redundant if using policy iteration 
for i     ns
 psi   theta          state x i  
 p i       action psa v i psi theta  
end
  print useful info to command window 
fprintf  nfails   i
viters   i
time     f s n     
 nfail iter iters fail nfail  dt  
  visualization of mdp model  the animation is figure   
  r  figure   
figure    set gcf  units   normalized   position                     
bar r  axis tight grid on
xlabel  state   ylabel  average reward associated with state 
r s   
title  visualization of reward function  
  v   figure   
figure    set gcf  units   normalized   position                     
bar v  axis tight grid on
xlabel  state  
ylabel  total expected future rewards for being in state and
acting optimally  v  s    
title  visualization of the value function  v   
  pi   policy   figure   
figure    set gcf  units   normalized   position                     
bar p  axis tight grid on
xlabel  state   ylabel  optimal action to take in state  
set gca  ytick                       yticklabel     
   y  z    y  z    y  z    y  z    y  z    y  z    y
 z    y  z    y  z   
title  visualization of optimal policy   pi    

 
 
 
 
 
 
 
 
 

  psa  figures   to   na 
for i     na
figure   i  stem  psa     i   markersize    
xlabel  old state   ylabel  new state  
zlabel  probability of transition  
title   psa for action   num str i   
set gcf  units   normalized   position     
       i         na         na      
end

  reinitialize state 
   initial conditions map the entire space to force rl to explore 
 
 
 
 
 
 
 

  uniform
x      y      z      x dot      y dot      z dot     
psi      pi        rand    
psi dot       pi        rand    
theta      pi        rand    
theta dot       pi        rand    
phi      pi        rand    
phi dot       pi        rand    
  normal
x      y      z      x dot        y dot      z dot     
psi     pi       randn 
psi dot       pi       randn 
theta     pi       randn 
theta dot       pi       randn 
phi       pi          randn    
phi dot       pi       randn 
s   state psi psi dot theta theta dot phi phi dot  
else   if the simulation didn t fail  simply continue
s   new s 
end

  if the inverted pendulum balances for xxxs  end simulation as
success
if  iterations iter start  dt   success time
iters fail nfail      iterations   iter start    last element   success 
success     
break
end
end
  simulation end  visualization   analysis                                 truncate unused elements of preallocated arrays
iters fail iters fail            
randact nfail   end       
vp iter vp iter            
  plot learning curve
figure hold on semilogy iters fail dt  k   
  compute simple moving average
window      
i     window 
w   ones   window     window 
weights   filter w   iters fail dt  
x    window   size iters fail dt     window    
h   plot x  weights window size iters fail dt       r     
set h   linewidth      
title  learning curve  
xlabel  trial number  
ylabel  time to failure  sec   
warning  off   matlab legend ignoringextraentries   
  legend  time to failure   moving average   location   southeast  
if success
plot nfail   iters fail end  dt  og   markersize       
 markerfacecolor   g     plots a green dot if ends in
success 
end
  calculate total computation time
t   toc 
h   floor t       
m   floor  t h           
s   t h      m    
fprintf  time elapsed   i hour s    i minute s      f second s  n   h m s  
  plot value policy iterations vs  number of failures
figure plot   nfail vp iter 
xlabel  number of failures  
ylabel  value policy function iterations to convergence  
title  value policy iterations vs  number of failures  
  plot the percentage of random actions taken per failure
figure plot   nfail randact     
xlabel  number of failures  
ylabel  percentage of random actions taken  
title  percentage of random actions vs  number of failures  
  animation output
if disp start   max fail
close vidobj  
winopen  animation mp   
end

fidynamics m
function  new x new y new z new x dot new y dot new z dot    
new psi new theta new phi new psi dot new theta dot new phi dot 
     
dynamics action dt x y z x dot y dot z dot    
psi theta phi psi dot theta dot phi dot 
  simulates the dynamics of the rocket via the equations of motion 
 
     
 
 
 
  state vector    x y z x y z psi theta phi psi theta phi 
 
  states are propagated using a variable order rungakutta method 
  input and output angular values are radians 
  psi theta phi   yaw pitch roll        euler 
  parameters for simulation dynamics                                       atmosphere
g           gravitational parameter   m s    earth         mars  
    
  grasshopper
h         height   m 
w        width   m 
r   w      radius   m 
v   pi r   h    volume   m   
p          adjusted homogenous density   rp           kg m   
m   v p    mass   kg 
ia   m r        axial moment of inertia   kg m   
it   m       r   h       transverse moment of inertia   kg m   
  engines
f m d      e     full force of m d     kip   n 
f lat     e     full force of lateral cold gas thrusters  n 
  introduction of randomness                                             action flip prob           probability action is not as intended
if rand   action flip prob
actions       
actions action          remove intended action from consideration
action   actions randi        pick new unintended action at
random        na 
end
  determine action                                                        
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 

m d
m d
m d
m d
m d
m d
m d
m d
m d

fires
fires
fires
fires
fires
fires
fires
fires
fires

  x  
  x  
  x  
  x  
  x  
  x  
  x  
  x  
  x  

if action     
fx   f m d  fy
elseif action     
fx   f m d  fy
elseif action     
fx   f m d  fy
elseif action     
fx   f m d  fy
elseif action     
fx   f m d  fy
elseif action     
fx   f m d  fy
elseif action     

lat
lat
lat
lat
lat
lat
lat
lat
lat

fires
fires
fires
off
off
off
fires
fires
fires

  y  
  y  
  y  
     
     
     
  y  
  y  
  y  

lat
lat
lat
lat
lat
lat
lat
lat
lat

  f lat 

fz   f lat 

  f lat 

fz     

  f lat 

fz    f lat 

    

fz   f lat 

    

fz     

    

fz    f lat 

fires
off
fires
fires
off
fires
fires
off
fires

  z 
    
  z 
  z 
    
  z 
  z 
    
  z 

fx
elseif
fx
elseif
fx
end

  f m d  fy    f lat  fz   f lat 
action     
  f m d  fy    f lat  fz     
action     
  f m d  fy    f lat  fz    f lat 

  more introduction of randomness                                        force noise factor          multiplied by between      and     
no force prob           force is   with this probability
fx   fx    force noise factor      rand   
fy   fy    force noise factor      rand   
fz   fz    force noise factor      rand   
if rand   no force prob
fx      fy      fz     
end
  calculate moments from forces                                          mx        no thruster can exert a moment about the axial direction 
   no direct roll actuator  
my   fz h       n m 
mz    fy h       n m 
  equations of motion                                                    x acc  
  m  fx cos psi  cos theta  fy  cos psi  sin theta  sin phi     
sin psi  cos phi   fz  sin psi  cos phi     
cos psi  sin theta  cos phi    g 
y acc     m  fx sin psi  cos theta  fy  cos psi  cos theta     
sin psi  sin theta  sin phi   fz  sin psi  sin theta  cos phi    
 
cos psi  sin phi    
z acc     m  fx sin theta  fy cos theta  sin phi  fz cos theta  cos phi   
psi acc  
  it cos theta   mz cos phi  my sin phi  ia  phi dot theta dot   
psi dot theta dot sin theta     it psi dot theta dot sin theta   
theta acc     it      ia it  psi dot   sin   theta ia phi dot psi dot   
 cos theta  my cos phi  mz sin phi   
phi acc  
mx ia psi dot theta dot cos theta  sin theta  it cos theta     
 mz cos phi  my sin phi  ia  phi dot theta dotpsi dot theta dot   
 sin theta     it psi dot theta dot sin theta   
  return new state variables  using euler s method                       new x   x   dt x dot 
new y   y   dt y dot 
new z   z   dt z dot 
new x dot   x dot   dt x acc 
new y dot   y dot   dt y acc 
new z dot   z dot   dt z acc 
new psi   psi   dt psi dot 
new theta   theta   dt theta dot 
new phi   mod phi   dt phi dot   pi     modulus    
new psi dot   psi dot   dt psi acc 
new theta dot   theta dot   dt theta acc 
new phi dot   phi dot   dt phi acc 

fiaction m
function  a coin    action psa v s psi theta 
  determines the best action for the current state based on psa and v
    find pi s    argmax a    sum over all states s   psa s  v s     
  if there is a tie amongst best actions  one is selected randomly from the
  top choices 
na   size psa    
a   zeros na    
  the first column is the expected value of total future discounted
  rewards for taking that action in the current state  e  s    psa   v s   
  the second column is the original index of that expected value  i will
  sort later  so these are for information retention 
for i     na
a i       reward s psi theta i  psa s   i  v i  
end
a   flipud sortrows a      place best actions first 
a a       a                 truncate actions that aren t  tied for  best 
num tied   size a       number of actions tied for best 
a   a  floor num tied rand           picks an action randomly from those
  that are tied  even if there is only    
if num tied    
coin        to indicate a random action was chosen
else
coin     
end

reward m
function r   reward s psi theta a 
  this function receives the state and action and calculates the reward 
sa weight        to weight state regulation over thrust minimization
if any a                 two lateral thrusters fire
ease     
elseif any a                 one lateral thruster fires
ease      
elseif a       neither lateral thruster fires
ease     
end
if s        
r      
else
r   sa weight     
      abs psi      pi               abs theta      pi          
    sa weight     
ease 
end
 
 
 
 

rewards more for being vertical not firing 
all have between    failure  and    psi     or theta     or f      reward 
which is weighted based on a cost function ala lqr control  for a total
reward between   and    except for the failure state r      

fistate m
function state   state psi psi dot theta theta dot phi phi dot 
  this function returns a discretized value for a continuous state vector 
  there are   angular position ranges for psi  yaw     for theta  pitch  
  and   for phi  roll   this yields the discrete state output as one of
                     permutations plus   for the  failure  state  a finer
  discretization produces a better policy  but a larger state space
  requiring longer computations 
  define limits for discrete state regions
   outside of these   failure  except for phi  roll  which is cyclic 
psi lim   pi                   
psi dot lim   pi                       
theta lim   pi                   
theta dot lim   pi                       
phi lim   pi                        
phi dot lim   pi                       
  calculate number of states based upon number of discrete ranges
psi tot   length psi lim    
psi dot tot   length psi dot lim    
theta tot   length theta lim    
theta dot tot   length theta dot lim    
phi tot   length phi lim    
phi dot tot   length phi dot lim    
numst   psi tot   psi dot tot   theta tot   theta dot tot      
phi tot   phi dot tot    not including failure state
  calculate multiplier of permutations in subdivisions
phi dot mult   numst phi dot tot 
phi mult   numst phi dot tot phi tot 
theta dot mult   numst phi dot tot phi tot theta dot tot 
theta mult   numst phi dot tot phi tot theta dot tot theta tot 
psi dot mult   numst phi dot tot phi tot theta dot tot theta tot psi dot tot 
psi mult   numst phi dot tot phi tot theta dot tot theta tot psi dot tot psi tot 
  find ranges that the current state elements fall within
psi intvl   interval psi psi lim  
psi dot intvl   interval psi dot psi dot lim  
theta intvl   interval theta theta lim  
theta dot intvl   interval theta dot theta dot lim  
phi intvl   interval phi phi lim  
phi dot intvl   interval phi dot phi dot lim  
  determine state by summing subsequent contributions from state elements
if any  psi intvl psi dot intvl theta intvl theta dot intvl    
phi intvl phi dot intvl     
state   numst   
else
state   phi dot mult  phi dot intvl       
phi mult  phi intvl       
theta dot mult  theta dot intvl       
theta mult  theta intvl       
psi dot mult  psi dot intvl       
psi mult  psi intvl       
      to shift states back up by one
end

fistate x
function  psi psi dot theta theta dot phi phi dot    state x s 
  this function takes the discretized value for a continuous state vector
  and returns the  average  of the corresposding state elements 
  define limits for discrete state regions
   outside of these   failure  except for phi  roll  which is cyclic 
psi lim   pi                   
psi dot lim   pi                       
theta lim   pi                   
theta dot lim   pi                       
phi lim   pi                        
phi dot lim   pi                       
  calculate number of states based upon number of discrete ranges
psi tot   length psi lim    
psi dot tot   length psi dot lim    
theta tot   length theta lim    
theta dot tot   length theta dot lim    
phi tot   length phi lim    
phi dot tot   length phi dot lim    
numst   psi tot   psi dot tot   theta tot   theta dot tot      
phi tot   phi dot tot    not including failure state
  calculate multiplier of permutations in subdivisions
phi dot mult   numst phi dot tot 
phi mult   numst phi dot tot phi tot 
theta dot mult   numst phi dot tot phi tot theta dot tot 
theta mult   numst phi dot tot phi tot theta dot tot theta tot 
psi dot mult   numst phi dot tot phi tot theta dot tot theta tot psi dot tot 
psi mult   numst phi dot tot phi tot theta dot tot theta tot psi dot tot psi tot 
if s           failure
psi   psi lim    
psi dot   psi dot lim    
theta   theta lim    
theta dot   theta dot lim    
phi   phi lim    
phi dot   phi dot lim    
  reward will then be zero from the state
else
phi dot intvl   ceil s phi dot mult  
phi intvl   ceil  s phi dot mult  phi dot intvl        
phi mult  
theta dot intvl   ceil  s phi dot mult  phi dot intvl       
phi mult  phi intvl     theta dot mult  
theta intvl   ceil  s phi dot mult  phi dot intvl       
phi mult  phi intvl    theta dot mult  theta dot intvl     theta mult  
psi dot intvl   ceil  s phi dot mult  phi dot intvl       
phi mult  phi intvl    theta dot mult  theta dot intvl       
theta mult  theta intvl     psi dot mult  
psi intvl   ceil  s phi dot mult  phi dot intvl       
phi mult  phi intvl    theta dot mult  theta dot intvl       
theta mult  theta intvl    psi dot mult  psi dot intvl     psi mult  
  find ranges that the current state elements fall within
psi   interval x psi intvl psi lim  
psi dot   interval x psi dot intvl psi dot lim  
theta   interval x theta intvl theta lim  
theta dot   interval x theta dot intvl theta dot lim  
phi   interval x phi intvl phi lim  
phi dot   interval x phi dot intvl phi dot lim  
end

fisimdisplay m
function simdisplay x y z psi theta phi time elapsed tlast nfail val iter a 
   animates  the simulation by updating the rocket position at each
  timestep  visualizing the movement within a figure 
figure      figure   always denotes the simulation display
set gcf  doublebuffer   on  
set gca  nextplot   replacechildren   
cla
  plot grasshopper   m d
h       r         m 
 xc yc zc    cylinder 
 xs ys zs    sphere 
gray              
orange               
origin    x y z     center of mass to rotate about
hold on
p    surf xc r y yc r z zc h h   x  facecolor  gray  edgecolor  gray     cylinder
rocket rotate p  psi theta phi origin  
p    surf xs r y ys r z zs r h   x h  facecolor  gray  edgecolor  gray     nose
rocket rotate p  psi theta phi origin  
p    surf xs r y ys r z zs r h   x  facecolor   r   edgecolor   r      m d
rocket rotate p  psi theta phi origin  
p    surf xs     r y ys     r z zs     r r h   x  facecolor  orange  edgecolor  orange     m d
rocket rotate p  psi theta phi origin  
p    surf xs     r y ys     r z zs     r      r h   x  facecolor   y   edgecolor   y      m d
rocket rotate p  psi theta phi origin  
  conditionally plot lateral thrusters
if any a            
p    surf xs r   r y ys r   z zs r   r h   x  facecolor   r   edgecolor   r      pushes my  y
rocket rotate p  psi theta phi origin  
p    surf xs r        r y ys r   z zs r   r h   x  facecolor   y   edgecolor   y      pushes my  y
rocket rotate p  psi theta phi origin  
end
if any a            
p    surf xs r   r y ys r   z zs r   r h   x  facecolor   r   edgecolor   r      pushes my  y
rocket rotate p  psi theta phi origin  
p    surf xs r        r y ys r   z zs r   r h   x  facecolor   y   edgecolor   y      pushes my  y
rocket rotate p  psi theta phi origin  
end
if any a            
p     surf xs r   y ys r   r z zs r   r h   x  facecolor   r   edgecolor   r      pushes my  z
rocket rotate p   psi theta phi origin  
p     surf xs r   y ys r        r z zs r   r h   x  facecolor   y   edgecolor   y      pushes my  z
rocket rotate p   psi theta phi origin  
end
if any a            
p     surf xs r   y ys r   r z zs r   r h   x  facecolor   r   edgecolor   r      pushes my  z
rocket rotate p   psi theta phi origin  
p     surf xs r   y ys r        r z zs r   r h   x  facecolor   y   edgecolor   y      pushes my  z
rocket rotate p   psi theta phi origin  
end
hold off
view    axis equal
xlabel  x  m    ylabel  y  m    zlabel  z  m   
  labels
  axis                pbaspect             
set gca  position                 
title  animation   xlabel  y  m    ylabel  z  m    zlabel  x  m   
  add real time to plot  slows down simulation 
set    showhiddenhandles   on  
delete findobj  horizontalalignment   center      delete previous
annotation  textbox                       string     
  time elapsed    num str time elapsed     f     sec      
 horizontalalignment   center   backgroundcolor   w   
annotation  textbox                       string     
  time elapsed  last    num str tlast     f     sec      
 horizontalalignment   center   backgroundcolor   w   
annotation  textbox                      string     
  number of failures    num str nfail   i       
 horizontalalignment   center   backgroundcolor   w   
annotation  textbox                      string     
  value iterations  last    num str val iter   i       
 horizontalalignment   center   backgroundcolor   w   
drawnow

firocket rotate m
function rocket rotate h psi theta phi origin 
  inputs the euler angles        yaw pitch roll  in radians and rotates
the
   d graphic given by handle h 
  define passive rotation matrices
r psi    cos psi  sin psi       sin psi  cos psi             
r theta    cos theta     sin theta            sin theta    cos theta   
rotate h           psi     pi origin    matlab s  y is my z
rotate h r psi            theta     pi origin    matlab s  x is my y
 rotated once 
rotate h r theta  r psi          phi     pi origin    matlab s z is my
x  rotated twice 

bellman m
function v   bellman psa p r gamma 
  this function solves bellman s equations for the value function according
  to some policy 
ns   length r     number of states
a   zeros ns ns     preallocating the array that will be av   r
for i     ns
a i      psa i   p i   
end
a   a diag ones   ns  gamma  
v   a  r  gamma  

fiinterval m
function intvl   interval val limits 
  this function takes a value and a vector of limits for the value  and
  then outputs the interval that the value falls between  the output is
 
  if the value is outside of the range difined by the limits  limits
must
  be in ascending order 
 
  for example
  interval                           
  interval                         
  interval                             
  interval                          
  interval                           
for j     length limits 
if val   limits j 
intvl   j   
return
end
end
intvl        if return is never invoked  the value is higher than the
high
  limit and we return   to indicate failure 

interval x m
function val   interval x intvl limits 
  this function takes an interval and a vector of limits for intervals 
and
  then outputs the mean of the respective interval  limits must be in
  ascending order 
 
  for example
  interval x                       
  interval x                     
  interval x                       
  interval x                     
val   mean  limits intvl  limits intvl      

firotations m
force balance
clear all close all clc

                                                            
psi double dot  

kinematic relations
syms p t s   phi         theta          psi        

mz cos p    my sin p    ia pd td   ia sd td sin t      it sd td
sin t 
                                                                     

rp              cos p  sin p       sin p  cos p   
rt    cos t     sin t            sin t    cos t   
rs    cos s  sin s       sin s  cos s             

it cos t 

theta double dot  
i b   rp rt rs    inertial frame to body frame
b i   i b     body frame to inertial frame
e b         sin t      cos p  cos t  sin p       sin p 
cos t  cos p   
  euler angle derivatives to body rates  p q r  e b  pd td sd 
b e   simplify inv e b      singularity at theta     deg
  body rates to euler angle derivatives  pd td sd  b e  p q r 

force balance
  newton s equations
syms m g   mass gravity
syms fx fy fz   forces in the body frame

 
ia sin   t  sd
                  ia pd cos t  sd   my cos p    mz sin p 
 

 
sd

sin   t 
                                                                      it
 

phi double dot  
mx
     sd td cos t     sin t   mz cos p    my sin p    ia pd td ia
ia sd td sin t      it sd td sin t       it cos t  

dummy     m  b i  fx fy fz   m g       
xdd fb   dummy     ydd fb   dummy     zdd fb   dummy    
  euler s equations

lagrangian formulation

syms sd sdd pd pdd td tdd   derivatives of euler angles
syms ia it   axial and tranverse moments of inertia

  since matlab cannot take generic derivatives with respect to

pp   pd sd sin t  
qq   sd sin p  cos t  td cos p  
rr   sd cos t  cos p  td sin p  
  double letters are body rates to not overload p with phi  the
euler angle
ppdot   pdd sdd sin t  sd td cos t  
qqdot   tdd cos p  td pd sin p  sdd cos t  sin p     
sd  pd cos p  cos t  td sin p  sin t   
rrdot    tdd sin p  td pd cos p  sdd cos t  cos p     
sd   td sin t  cos p  pd sin p  cos t   
syms mx my mz   moments in body frame
sdd fb   solve my  it qqdot pp rr  it ia   sdd  
tdd fb   simplify solve subs mz  it rrdot pp qq  itia   sdd sdd fb  tdd   
sdd fb   simplify subs sdd fb tdd tdd fb   
pdd fb   simplify solve subs mx ia ppdot sdd sdd fb  pdd   
  dd   double dot
fprintf                                                          
     n n  
fprintf  force balance  n n  
fprintf                                                          
     n n  
fprintf  psi double dot    n  
pretty sdd fb  
fprintf   n ntheta double dot    n  
pretty tdd fb 
fprintf   n nphi double dot    n  

time  i ve
  derived  and double checked  the following formulas  this
section simply
  performs the form manipulation 
m    mx  sin t    my cos t  sin p    mz cos t  cos p  
m    my cos p    mz sin p  
m    mx 
tdd l   simplify solve    
m   it  tdd sd   cos t  sin t   ia  pd sd cos t sd   sin t  cos t      
 tdd   
pdd l   solve m  ia  pdd sdd sin t  sd td cos t   pdd  
sdd l   simplify solve subs    
m   ia   pdd sin t pd td cos t  sdd sin t      sd td sin t  cos t      
it  sdd cos t      sd td cos t  sin t    pdd pdd l  sdd   
pdd l   simplify subs pdd l sdd sdd l   
fprintf                                                          
     n n  
fprintf  lagrangian formulation  n n  
fprintf                                                          
     n n  
fprintf  psi double dot    n  
pretty sdd l  
fprintf   n ntheta double dot    n  
pretty tdd l 
fprintf   n nphi double dot    na  
pretty pdd l 

pretty pdd fb 
                                                            

                                                            

lagrangian formulation

fi                                                            
ans  
psi double dot  
       
mz cos p    my sin p    ia pd td   ia sd td sin t      it sd td
sin t 
                                                                    it cos t 

ans  

theta double dot  

ans  

 
ia sin   t  sd
                  ia pd cos t  sd   my cos p    mz sin p 
 
 
sd
sin   t 
                                                                      it
 

phi double dot  
a
mx
     sd td cos t     sin t   mz cos p    my sin p    ia pd td ia
ia sd td sin t      it sd td sin t       it cos t  

verification
  specify random values
p     pi rand 
t    pi   pi rand 
s     pi rand 
pd        rand 
td        rand 
sd        rand 
ia        rand 
it         rand 
mx      rand 
my      rand 
mz      rand 
fprintf                                                          
     n n  
fprintf  tests   subsequent answers should be identical  n n  
fprintf                                                          
     n n  
subs sdd fb 
subs sdd l 
subs tdd fb 
subs tdd l 
subs pdd fb 
subs pdd l 

                                                            
tests   subsequent answers should be identical 
                                                            

ans  
      

ans  
      

ans  
       

       

       

fi