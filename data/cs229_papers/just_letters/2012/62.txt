predicting the effectiveness of bike classifieds
lawrence xing
stanford university
lxing stanford edu

abstract
we apply text classification techniques to the problem
of predicting the eventual sale of classified bike
advertisements  a combination of nave bayes  support
vector machines  and support vector regression performs
reasonably well on bag of words and tf idf feature
representations of the data  the use of information gain
thresholds to prune out less relevant term features yields a
significant improvement in accuracy for the two vector
approaches  finally  we demonstrate that support vector
regression is a novel and effective classification method
for text categorization 

   introduction
online advertising is a major source of monetization on
the internet  it is the primary source of revenue for
google  and numerous other websites host ads alongside
their content  with increasing amounts of money being
devoted to advertisement  the topic is a prime candidate
for optimization 
one common line of research is personalized
advertising  which matches of specific products to the
customers most likely to buy them      while this can
certainly be effective  it does not reveal what makes an ad
inherently appealing or unappealing  in this paper  we
investigate methods for predicting the appeal of an
advertisement broadcast to a set audience 
to narrow the scope of investigation  we focused on a
specific set of advertisements  classified bike listings from
the website craigslist org  we chose this data set for
multiple reasons  first  it was very simple to identify the
sale of an advertisement by continuously polling the
listing until it was taken down  secondly  we hoped to
extract a rich feature set from bike classifieds based on
price  devaluation  number of images  componentry  etc 
finally  the author enjoys cycling 

     background work
there has been little research into machine learning based
on the rich feature sets described above  part of this lies in
the specificity of the problem domain  and another part
stems from the absurdity of trying to optimize an ad with

respect to its price and devaluation from msrp 
however  there is a substantial foundation for
approaching advertisement success prediction as a text
classification problem  sahami et al  describe a bayesian
approach to document classification using basic
probabilistic methods      additionally  the field of
information retrieval  ir  has developed techniques for
featurization of documents  including term frequency 
document frequency  and term frequency inverse
document frequency      using the last of these feature
representations  it has been shown that support vector
machines  svms  can be used to effectively categorize
text     
smola and vapnik also separately describe a regression
technique whose derivation is similar to that of svms
called support vector regression  svr           we attempt
to apply this to text classification in what appears to be a
novel approach 
finally  to combat the high dimensionality of traditional
ir document representations  we adopt a thresholding
technique based on the information gain metric as
recommended by joachims     and described by yang and
pedersen     

   methodology and results
     dataset
craigslist offers an enormous variety of listings  ranging
from appliances to housing  for this investigation  we
limited the range of posting data to road bikes with a list
price of over      in the san francisco bay area  section
  describes our rationale for this subset  additionally  the
price bound was chosen to observe a relatively expensive
market that would warrant buyer scrutiny 
the conventional barrier between an online ad and the
corresponding purchase is spatial  the two are hosted at
different locations  requiring server side knowledge to
associate the two  on craigslist  the separation is
temporal  it is possible to identify the sale of a bike listing
when the listing is taken down from the website  this in
turn requires tracking of the posting throughout its
lifetime  from initial submission to removal 
to collect advertisement data  we polled the first two
pages of craigslists bike listing section for new postings

fiat a frequency of once every two days  once identified  we
accessed each postings individual listing page to extract
the title  body text  price  posting date  and images  in
addition to these initialization scrapes  we also polled
stored ads with the same frequency to check of they had
been removed  and sold   because we could not recover
the exact sale time associated with a removed ad  we
assigned each a coarse sale time bucketed within two days
of when it was first discovered missing 
during ad initialization  we excluded advertisements
older than one week at the time of scraping  the rationale
for this was that these ads were inherently biased by virtue
of their age  other  already sold listings within the same
time cohort would have been erroneously excluded from
data collection because they would have been taken down
at the time of scraping 
one complication was that craigslist does not have a
public api for its data  and in fact actively discourages
high volume scraping of its website due to bandwidth
considerations     offenders can be ip blocked from the
website  to combat this  we conservatively inserted a
delay in between the fetching and polling of each
individual posting  additionally  we limited the total
number of queries per day  this limited the overall size of
the data set  we collected     classified ads 
in practice  we observed that a posting that does not sell
within four weeks will not sell at all because it becomes
buried from view by newer postings  thus  we assigned a
binary class label to our dataset based on whether or not it
sold within one month 

     data processing and feature selection
initially  we intended to construct a rich feature set for
each advertisement based on the price  the presence of
images in the ad  the devaluation from msrp  post time 
and any mention of specialized componentry  initial
experiments on this featurization yielded poor results  so
we discarded it in favor of an ir grounded approach 
for each ad title and body  we performed the following
transformations 
 

strip html tags using the web framework ruby
on rails actionview module
remove stop words using the nltk corpus
porter stem remaining words
tag and replace numeric tokens

we stripped html data to sanitize useless metadata
 although ad formatting could be a potential avenue of
exploration for another time   stop words were removed to
prune non informative words like the and a  and the
remaining words were porter stemmed to regularize
 
http   stackoverflow com questions        how do craigslistmashups get data

variations of certain root forms  numeric tokens were
rewritten for the same reason  to regularize information
like   cm and   cm into the common root
 num cm  for the last preprocessing step  we used a
custom regular expression based tagger for numeric
expressions only  we deliberately eschewed the nltk
tagger due to its large set of tags  which we felt were too
specific for the task at hand 
finally  we computed the tf idf vector representation of
each document  suppose the vocabulary of the entire
corpus is   then each document can be represented based
on term frequencies in a    dimensional vector  where the
  element is the number of occurrences of the   term in
the document  the resulting vector contains a bag ofwords representation for the document  however  this
approach tends to overweight common terms which do not
provide much discriminatory information between
documents  thus it is desirable to weight each term
element inversely to the number of documents in which
the term appears  the document frequency   the final term
frequency inverse document frequency  tf idf  for term t
in document d from a dataset of documents d is then
    f t  d   log

  
          

once the tf idf scores were computed  we additionally
normalized each document vector to unit length to
decrease the weight of longer documents 

     nave bayes
one of the simplest approaches to text classification is
nave bayes  this technique generatively models the
probability of a document conditioned on a class  then
selects the class which produces the highest maximum a
posteriori  map  estimate  the generative model is
augmented by an assumption of conditional independence
between individual terms in the document  if an
advertisement is represented as a series of terms     
            then the models approximation for the
likelihood of a classification is


                       
  

the parameters    and      are maximum
likelihood estimates based on the training data  the
estimate we used above is a slight variation of the original
nave bayes model called the multinomial event model 
this model considers the likelihood of each term in the
document  as opposed to the likelihood of each distinct
term 
in addition to implementing the nave bayes model  we

fialso applied laplace smoothing with a smoothing
parameter of   to place a prior on unseen terms 
to test nave bayes  we used leave one out cross
validation  we ran it separately on both advertisement
titles and body texts 

     svm
using the tf idf vector representation of each
advertisement  the problem reduces to binary
categorization of at high dimensional data set  svms are
appropriate for this  and it has been demonstrated that they
work on tf idf features     
we trained an svm to classify both advertisement titles
and body texts  we found that l  regularized svm with a
linear kernel performed best  the gaussian radial basis
function  poly    and poly   kernels were tried did not
perform as well  the loss parameter c was experimentally
determined by iterating over the logspace from       to
      and reached an optimal value at     
for computational reasons  we only performed    fold
cross validation to test svm 

     support vector regression
a popular approach to text classification is logistic
regression  commonly used with a bayesian prior on the
class distributions  we were interested in experimenting
with an alternative method called support vector
regression  svr   which has not to our knowledge been
used before for text categorization 
as a variant of regression  svr attempts to fit a
function that directly maps input vectors to output labels 
with a linear kernel  this function f x  takes the form
          

where w and b are the parameters of the svr model 
instead of optimizing an objective function of total meansquared error like logistic regression  svr attempts to
confine all error within the smallest possible threshold  in
this sense it is the opposite of svm  which tries to confine
functional margins outside of the largest possible
threshold in order to separate data 
svr is typically solved by posing the equivalent
problem of minimizing w while holding the error threshold
constant at some   with regularization to allow for soft
margins  the problem is to minimize


subject to

 
      
 
  

              
   

this problem statement is remarkably similar to that of
svm  indeed  the resulting solution can be represented as
the inner product of the input with a subset of the training
data  hence the term support vector regression 
we trained an l  regularized svr on both
advertisement titles and body texts using only a linear
kernel  as with svm  we experimentally set the loss
parameter c to be      to test the data  we performed
leave one out cross validation 

figure    graphical representation of svr  the regression bounds all
points within the smallest possible margin  penalizing those that fall
outside     

     results
we adopted the ir metrics of precision and recall to
quantify the accuracy of each technique  precision is
defined as the number of true positives over total
positives  and measures how much of the retrieved data is
appropriate  recall is defined as the number of true
positives over true positives over false negatives  and
measures how much of the data that should have been
retrieved was actually positively categorized  in addition 
the f score of a technique is the harmonic mean of the
precision and recall 
in this case  we chose sold ads to be positives and
unsold ads to be negatives  however  there was no
particular reason to fixate on identifying sold ads  so we
used overall prediction accuracy as the final metric  with
precision and recall as additional descriptors 

random
nb title
nb body
svm title
svm body
svr title
svr body

precision
     
     
     
     
     
     
     

recall
     
     
     
     
     
     
     

f score
     
     
     
     
     
     
     

accuracy
     
     
     
     
     
     
     

table    test accuracy for baseline  nave bayes  svm  and svr on
both advertisement titles and body texts 

fifigure    training and test accuracy of all three methods with respect to fraction of data set used 

the test accuracy of all three methods was around      a
reasonable improvement over a baseline of random
guessing  of the three techniques  svr performed the best
in all four categories of precision  recall  f score  and
accuracy  this appears to validate the use of support
vector regression in text classification  training and
prediction using ad titles consistently outperformed
learning using advertisement body text  one explanation
for this is that the title feature vectors were considerably
shorter due to a smaller vocabulary      terms as
opposed to      
while the models performed acceptably on the test data 
they all exhibited an exceptionally high accuracy rate on
the training data  to determine if the test and training
errors would converge over time  we repeated the
classification experiments using reduced data sets
consisting of           and     of the full data set  this
performed no better  figure     training error consistently
hovered near zero  indicating a severe overfit 
this phenomenon was obvious in hindsight  our data
set size was only     advertisements  while the bag ofwords feature representations for ad titles and bodies used
    and      dimensions respectively  given the low
amount of training data relative to the features  we should
have expected a high amount of variance 
to address this problem  we chose to reduce the number
of features in the tf idf space  the alternative of gathering
more data was risky due to craigslists ip blocking of
high volume scrapers  unlike rich feature sets where each
feature has a semantic interpretation  there is no obvious
method for discarding elements from tf idf vectors 
instead we turned to a mathematical approach 

     information gain for feature pruning
the entropy of a distribution p over class labels    
        is defined as



            log    
  

intuitively  a dataset with balanced classes will produce
the highest entropy because both terms of the summation
will be moderate  while a skewed dataset will have low
entropy because one term will be close to zero  suppose a
binary classification is made based on some term t  then
define     to be average entropy of the two sets
classified by t  weighted by the size of each set  if
classification based on t alone separates classes well  then
the resulting entropy     should be lower than the
original      the information gain of t is thus
defined as         
we used this metric to prune the training feature sets by
only retaining elements of the tf idf vectors with the
highest information gain  that is  those terms that would
separate sold and unsold classifieds the best  joachims
recommends using a fixed threshold      but with no
baseline of information gain we instead varied the number
of retained features from   to the original feature set size 
with experimentation  we determined the optimal
feature set size to be       out of     original terms for
title analysis  and          out of      for body text
analysis for svr svm respectively  we used these
pruned feature sets to perform svm and svr again with
the same parameters 

     results using pruned feature sets
svm title
svm body
svr title
svr body

precision
     
     
     
     

recall
     
     
     
     

f score
     
     
     
     

accuracy
     
     
     
     

table    test accuracy for svm  and svr on both advertisement titles
and body texts using pruned tf idf vectors 

fifigure    test accuracy of svr and svm on title and body tf idf representations 
with respect to number of features included in the pruned vector 

the results of svm and svr with pruned feature sets
were significantly improved over use of the full feature
sets  notably  both methods performed better operating on
the body text than the title text  these two observations
are consistent with the hypothesis that overfitting was
caused by a high dimensionality  in the latter case  we
would expect that as the body tf idf vector was reduced to
a similar size as the original title tf idf vector  the
comparative advantage of title analysis low
dimensionality would be removed 
we also observed that feature pruning seemed to follow
a general trend of reduced accuracy with an extremely low
or high number of features  and the highest accuracy
somewhere in the middle  this observation held true for
all four combinations of svm svr and title body  figure
    this illustrates the traditional bias variance tradeoff in
model feature selection  with very few features  our
classifiers tended to underfit  while the full feature set
induced an overfit 
information gain weighting allowed us to recover the
most entropy reducing terms in the tf idf weightings  out
of interest  we present them here 
 

title terms  sst redlin almost japan clean
body terms  bent item bicycl benicia durabl

   conclusion
our results demonstrate that the appeal of classified
advertisements can largely be predicted by their raw
content in addition to their presentation  we also show that
with the best classifiers  the body text is generally a
stronger predictor of success than the title 

joachims use of information gain thresholding for
feature selection proved to be an effective method for
reducing dimensionality in a conservative fashion  and we
effectively used it to compensate for a high
dimensionality to dataset ratio  finally  we have shown
that support vector regression is a valid method for text
classification and can in some cases outperform svm and
multinomial nave bayes 
we would like to acknowledge andrew ng and the cs   
course staff for their teaching  and libsvm 
liblinear  ruby on rails  and the nltk as invaluable
tools for this study  we wish to disacknowledge craigslist
for their anti data mining practices 

references
    j  shanahan  digital advertising  going from broadcast to
personalized advertising  nips      workshop  machine
learning in online advertising       
    m  sahami  s  dumais  d  heckerman  and e  horvitz  a
bayesian aproach to filtering junk e mail  learning for text
categorization  papers from the aaai workshop       
    c  manning  introduction to information retrieval       
    t  joachims  text categorization with support vector
machines  learning with many relevant features  proc  of
the european conference on machine learning  ecml  
     
    a  smola  a tutorial on support vector regression 
neurocolt  technical report series  nc  tr          
     
    h  drucker  c  burges  l  kaufman  a smola  and v 
vapnik  support vector regression machines  advances in
neural information processing systems    nips       
    y  yang and j  pedersen  a comparative study on feature
selection in text categorization  international conference on
machine learning  icml        

fi