predicting nsf award money from abstracts
kyle brogle

sean ma
stanford university

laura stelzner

december         
abstract
the nsf receives tens of thousands of proposals requesting funding each year 
this leads to a significant amount of time being spent by humans reviewing these
proposals and deciding what amount to award  in order to help reduce this time
frame  we explored different supervised machine learning techniques to see if any could
accurately classify the amount of money that should be awarded based on past abstracts
and amount award  with an accurate classifier  the process for human reviewers could
be streamlined to find the promising projects first and prioritize giving them funding 
our results showed that multinomial naive bayes was the best model to use out of
those tested  with an accuracy of    percent 

 

introduction

the national science foundation is the gatekeeper of billions of dollars used for scientific research in the united states  this funding is critical to making advancements
in science  mathematics  and other related fields such as medicine  in      the nsf
received       billion dollars of funding      most of which will be allocated to distribution of research awards to researchers  as mentioned from the nsf funding description
page  nsf receives approximately        proposals each year for research  education
and training projects  of which approximately        are funded      we used machine
learning techniques to classify the amount of funding a nsf proposal would receive
based on past funding allocations  reviewing        proposals by hand takes a lot
of time  initial data from machine learning could help speed the process by helping
determine which proposals are most promising given what theyve awarded in the past 
and the classifier can continually be adjusted over the years as more proposals are
available as training data 

 

data

for our data we have         proposals submitted to the nsf between      and
     courtesy of university of california  irvines machine learning repository     we
began by giving each proposal a unique identifier  the proposals were all parsed into
a bag of words format in order to transform them into feature vectors to be classified 

 

fifigure    a histogram detailing an approximate distribution of the data
we removed stop words from our vocabulary  as they are too common in occurrence to
aid in classification  our dictionary of words produced was of length         we also
extracted the awarded amounts from the proposals themselves  an initial histogram
of the training data  as shown in figure    suggests that    class labels would nicely
divide the proposals  twenty buckets in increments of         up to    million  and
one bucket for awards greater than    million 

 

method and results

after our data was processed into a usable format  we decided to start by training a
multi class svm with linear kernel to classify our data  although we have approximately         training examples  we trained on smaller subsets  in order to determine
whether the size of the training set impacted our classification error  we decided to train
the svm with training set sizes                   and        these sets were built
by randomly choosing documents from our dataset  to try to avoid training sets that
are similar in topic or year requested  in each training scenario  we used   fold crossvalidation in order to test the accuracy of our model 
when analyzing the results  we found that the linear svm suffers from extremely
high variance  as our training error is near zero  refer to the first subplot in figure
    in order to try and remedy this  we decided to simplify our model by reducing the
number of features  in order to do this  we reduced the vocabulary size by using the
porter  stemming algorithm to stem all the words in our vocabulary  this reduced
vocabulary size from        to         this resulted in less overfitting of our model 
but the high variance persisted  refer to the second subplot of figure     before spending time training on larger training sets  we decided to investigate the performance of
some other classifiers 

 

fifigure    training vs  test error for linear svm
the additional classifiers we chose to test were multinomial naive bayes and stochastic gradient descent  we then ran these classifiers on both the stemmed and nonstemmed training sets of sizes                   and        the naive bayes classifier
seemed to perform the best of all our classifiers  exhibiting classification accuracy of
   percent  with    classes  the trivial classifier that uniformly selects a class for
each document would achieve     percent accuracy if the distribution of classes over
documents was uniform  it would actually achieve less in this application  since the
documents are concentrated in categories with lower award amounts  we found that
the training error was also high  around    percent   indicating high bias  because
of the high bias  the reduction of the feature space provided by the stemmed dataset
caused naive bayes to perform extremely poorly  refer to figure   for the results  
on the other hand  when using a stochastic gradient descent classifier  we found
classification accuracy to be around    percent  refer to figure     like the linear
svm  this classifier had very small training error  due to overfitting the model to the
training set  when training on the stemmed dataset  the reduced feature set had a
more significant impact here than with the linear svm  with the reduced feature set 
the classifier no longer overfit the model to the training data  and started to exhibit

 

fifigure    training vs  test error for naive bayes and stochastic gradient descent
signs of high bias  the accuracy of our classifications decreased when running on the
stemmed data in this scenario 

 

conclusions

from our analysis  we see that the best performance was realized with the naive bayes
classifier on the un stemmed dataset  unlike the linear svm and stochastic gradient
descent classifiers  naive bayes did not overfit our model to the training set  the high
training error in naive bayes indicates high bias  which suggests that classification
error can be lowered with the addition of more features to the model 

 

future work

in order to further improve the accuracy of the naive bayes classifier  we plan to address the high bias by adding more features  one specific feature that we feel will
improve the classification accuracy greatly is the year of submission  this should really be considered in our model  as popular research areas  especially in the field of
computer science  tend to change over time  with new topics frequently emerging  this
feature will be weighed much higher than the bag of words features  as we suspect it

 

fiplays a significant role in funding decisions  other features that were suggested to us
during poster presentations were the sponsoring university and the department  which
we could extract from either the principal investigators mailing address or the nsf
program id 
it is also possible to improve upon the model itself by changing the buckets to better
fit the distribution outlined in the histogram from figure    if we were to choose buckets so that there are approximately equal number of proposals in each bucket  it could
make our results more accurate  since there will be an even distribution of samples per
class  one unfortunate problem with this change is that the actual range of how much
a paper could earn is no longer a fixed range  so the information may not be as useful 
it is also possible change our model from a multi class classification problem to a regression problem  so that instead of discretizing the amount of money into buckets  we
attempt to give a best guess as to exactly what the paper will earn  however  this may
not be as feasible with such a large feature set  as our vocabulary exceeds        words 
we also wish to extend the dataset by contributing proposals up to the current
year  we are currently writing a script to parse the award format provided by the
nsf awards website in order to further look into how well this problem can actually
be solved with machine learning 

 

citations

    frank  a    asuncion  a          uci machine learning repository  irvine  ca 
university of california  school of information and computer science  retrieved oct
          http   archive ics uci edu ml 
    national science foundation  about funding  in national science foundation
where discoveries begin  retrieved nov           from http   www nsf gov funding aboutfunding jsp
    national science foundation  fy      appropriations signed into lawnsf to
receive        billion  in national science foundation where discoveries begin  retrieved nov           http   www nsf gov about congress     highlights cu        js

 

fi