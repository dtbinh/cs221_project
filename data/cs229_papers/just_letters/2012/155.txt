a risky proposal  designing a
risk game playing agent
juan lozano  jlozano stanford edu
dane bratz  dbratz stanford edu
abstract
monte carlo tree search methods provide a general framework for modeling decision problems
by randomly sampling the decision space and constructing a search tree according to the sampling results 
artificial intelligences employing these methods in games with massive decision spaces such as go and
settlers of cataan have recently demonstrated far superior results compared to the previous classic game
theory approaches  browne et al      we apply monte carlo tree search methods  particularly the uct
variant  to an online version of the popular board game risk 
  introduction
    risk and lux
risk is a popular game where players compete to control and conquer countries using armies that
their countries generate  the game is classically played using a map of the globe  and players sequentially
act individually on a turn  a turn consists of four phases  the cards phase  during which the player can
exchange a set of cards that he holds for additional armies to add to this turns army income  the place
armies phase  during which the player decides how to distribute his income armies amongst his countries 
the attack phase  during which the player conducts any desired attacks  and subsequent army moves into
conquered countries   and the fortify phase  during which the player can choose to redistribute armies
between adjacent countries  players can obtain cards either by conquering at least one country a turn or by
eliminating an opponent and taking the cards they held  the initial countries for each player are chosen
randomly or through a draft among the players 
lux by sillysoft is a popular online platform for risk  the platform notably offers a development
package with which one can develop both maps and ai agents to be used by the platform for gameplay 
the development package offers example ai agents that are deterministic in nature as well as the helper
methods that the ai agents use to quantify higher level aspects of the game state  several of the agents
provided  from our personal experience playing against them  offer a respectable level of competition 
    motivation
the game of risk presents an interesting opportunity to study the boundary between classic
games and modern video games  the former games usually have a discrete state space and deterministic
outcomes  while the latter normally have a continuous state space and indeterministic outcomes  the
game of risk contains an interesting blend of the two  on one hand the board state is discrete and at any
given turn is easily represented on a modern computer  but the number of possible board states that could
result from the current board state is massive  in fact it can be shown that the state space complexity of
risk is infinite and the game tree of risk has an exponential branching factor  wolf      additionally 
the current reward for a given board state is easily calculated  and deterministic  unfortunately  the
future reward for a current board state is uncertain  due to the chance elements involved in attacks 
even calculating the expected outcome of a battle is computationally quite complex  wolf      these
conclusions indicate that the classic approaches to modeling and learning games  such as minimax  would

fibe computationally infeasible for the game of risk 
the above concerns led us to a general class of algorithms which has performed exceptionally
well in infinite search spaces  monte carlo methods and in particular monte carlo tree search  these
methods have been successful in other computationally complex board games such as go and settlers of
cataan  where traditional game theoretic methods had previously struggled  chaslot et al     additionally 
monte carlo methods have recently been used in video game development for generating an evaluation
function to accurately rate the quality of a video game ai  chaslot et al     the common thread between
these diverse applications is that the state space to explore is very large and the reward function is usually
uncertain and highly variable 
    data set
we modified    of the existing ais to write to a file various aspects of the board state  i e 
number of countries owned  number of opponent armies in a given country  etc   before and following
the phases of the agents turn  overall  we wanted to be able to correlate certain features of gameplay
with winning for a particular agent  we then played four of the better agents against each other for    
game simulations  we gathered data in this way for two general reasons  we wanted a robust data set
while also ensuring that our data would be relevant  by playing the harder ais against each other  we
hoped to improve our datas quality by forcing the winning agent to have played at least somewhat well to
have won  playing a game between only ais can also be done quite quickly   within a matter of seconds 
which allowed us to gather information for a large number of games 
  approach
    risk strategy
risk strategy varies heavily with the specific rule set that is being used  which is especially
apparent in the lux platform   it depends on how what the value progression is for turning in cards  the
map structure  and whether continents increase in bonus value over time  because of this  we decided to
focus on a particular variant of risk  in particular  we used the classic map  with the initial countries for
each player chosen randomly  with a card bonus that increased by   armies per cash in  starting at four  
and with a    continent increase per round  these are the default lux settings  
on these settings  we noticed a few general things  first  that maximizing our troop income for
the next turn seemed to be an incredibly effective strategy  this was best done by conquering and holding
continents  conquering and holding more countries  and getting cards by either taking at least one country
per turn or eliminating an opponent and taking their cards 
    monte carlo tree search and uct algorithm
monte carlo tree search methods are characterized by two key assumptions  random simulation
may be used to approximate the true value of an action  and the subsequent approximate value can be
efficiently incorporated to update the search of the game tree toward a best first strategy  browne et al 
    based on these two assumptions  a monte carlo tree search method iteratively constructs a partial
game tree using the results of previous searches of the tree to guide the current construction of the tree 
when searching a game tree  the algorithm is parameterized by a game simulator  a default strategy  a tree
policy  a game state evaluation function  and an input game state  browne et al      the input game state
provides the root of the partial game  the tree policy dictates the starting node for new searches and the
construction of new game tree nodes based on the results of previous searches  searches are conducted

fifrom a given starting node through simulation of the default strategy for a specified number of turns 
ideally until completion of the game  the evaluation function is then used to evaluate the final simulated
game state  the results of this evaluation are then back propagated up the game tree 
the uct algorithm is a specific variant of monte carlo tree search that has been applied to
other games like go  browne et al      the uct algorithm specifies a tree policy that constructs and
searches the partial game tree in such a way that there is always a nonzero probability that we will explore
every possible strategy  browne et al      but  the strategies that yield higher rewards have a higher
probability of being explored which implies that on average they will be explored earlier in the search
 browne et al     
    application to risk
in order to apply a monte carlo tree search method to the game of risk we made some
simplifying assumptions  first  we decided to limit the search space by employing heuristic strategies
provided by the lux api to choose our initial troop placement  our turn by turn troop placement  our
fortification moves  and our card turn in strategy  these are drastic simplifying assumptions  since the
strategy employed in each phase is dependent on the strategy employed in the other phases  however 
each phase also deserves independent treatment  since the structure of the rewards  and the strategies
employed to optimize those rewards  vary drastically from phase to phase  gibson et al  for example 
applied monte carlo methods to the specific strategy of choosing which continents to occupy  however 
we observed that for average to relatively skilled humans  the troop placement and fortification strategies
all consisted of a combination of heuristics provided by the lux api  since these heuristics already
seemed to be working well for the provided agents  and providing a comprehensive strategy that
incorporates all of the game phases is outside the scope of a single paper  we decided to focus solely on
the attack phase 
the attack phase presents an interesting challenge because it drives the dynamics of the entire
game  unfortunately  even though the number of game states that can result from a single attack is
only two  the number of possible game states that can result from a series of attacks  a battle  grows
very quickly  since we can choose to stop attacking after each roll of the dice  a more simplistic model
assumes all battles are carried out until completion  meaning that a series of attacks on a single country
ends when the attacking country no longer has enough troops to attack  or the defending country has been
conquered  we made this assumption in order to ease the computational burden and we also observed that
in risk  the most important factor is the final outcome of the battle  wolf      also  if we assume all our
attacks are carried out to completion  then losing an attack corresponds to the worst possible scenario  and
we would like our agents general strategy to be robust enough to endure these scenarios  our application
of the uct algorithm to a risk game with players and countries consisted of the following
components 
i 

a game tree node 


  where 
the game state matrix  where

is the number of armies for player

on country from the rules of risk all countries must have at least one army
on them during every turn 


are the child game tree nodes of

fi

is the number of times the game tree node has been visited by a
search 



is the total value of all the game states that resulted from searches that
passed through node

ii  a tree policy

which takes as input a game tree node and returns the child node

that maximizes the following function  browne et al     




where is the number of times the current parent node
has been visited  and the subscripted values refer to the child node  and is a
parameter  the first term incentives exploitation of high reward choices  while
the second term ensures that we explore choices that have fewer visits  and
the exploration constant dictates how likely we are to explore other options
 browne et al      we tried various values of and settled on
in the context of the attack phase of risk  the child nodes of a game state node
are all the possible game states that could result from any of the possible attacks
the agent could make in the game state described by

iii  a game state evaluation function
  which takes as input a game state matrix and
outputs a value for the game state  the value  for a given game state  corresponds to
our agents expected troop income after subtracting off the expected troop income of the
opponent with the largest troop income  where the troop income for a player is a function
of the number of countries held by the player  the continents controlled by the player  and
the number of cards the player has 
iv  a default strategy 

which takes as input a game state matrix and outputs an attack

to simulate 
 we actually had several default strategies that corresponded to high level goals
such as taking a continent  eliminating an opponent to gain their cards  or
ensuring opponents do not secure a continent 
v  a battle simulator 
which takes as input a game state matrix and an initial
attack to simulate and outputs the game state matrix which results from simulating the
series of attacks  beginning with the input attack and going until completion  as explained
above 
a typical attack phase for our agent consisted of using the uct version of monte carlo tree
search to build separate partial game trees for each possible default strategy in parallel  each default
strategy space was searched for the same amount of time  after which time the agent executed the strategy
specified by the game tree with the highest value at its root  the strategy chosen was executed until the
agent lost a battle  at which point the above process was repeated with the current game state serving as
the root node for the partial trees  the agent stopped attacking when none of the strategies yielded an
expected positive value 
    results and future considerations

fiusing the above strategy  our agent was able to defeat the best provided ais    percent of the
time  to put this number in perspective  the provided ai with the highest win percentage against the same
competition was only able to win    percent of the time  in the future we would like to expand our
implementation to include the other game phases in the decision process  once this has been completed
we will be able to reverse train our agent in order to determine better forms for the game state evaluation
function as well as a better value for the exploration constant  additionally  we plan on implementing a
basic form of pattern recognition which will allow the agent to begin implementing combinations of
strategies based on general families of board types 











references
gibson  r   desai  n   and zhao  r        an automated technique for drafting territories
in the board game risk  in proceedings of the sixth aaai conference on artificial intelligence
and interactive digital entertainment  http   www aaai org ocs index php aiide 
aiide   paper view     
wolf  m          an intelligent artificial player for the game of risk   unpublished doctoral
dissertation   tu darmstadt  knowledge engineering group  darmstadt germany 
http   www ke tu darmstadt de bibtex topics single   
guillaume chaslot  sander bakkes  istvn szita  and pieter spronck         montecarlo tree search  a new framework for game ai  in proceedings of the fourth artificial
intelligence and interactive digital entertainment conference  eds  michael mateas and chris
darken   pp           aaai press  menlo park  ca   presented at the aiide    
http   sander landofsand com index php research
browne et al  a survey of monte carlo tree search methods  ieee transactions on
computational intelligence and ai in games  vol     no     march      
http   ieeexplore ieee org stamp stamp jsp tp  arnumber         tag  
sillysoft  lux delux   the best risk game there is  http   sillysoft net lux  

fi