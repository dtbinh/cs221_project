predicting wine prices from wine labels
jamie ray     svetlana lyalina  and ruby lee 
 

department of engineering physics  stanford university
department of computer science  stanford university

 

submitted on december         

abstract
motivation  the wine label is an icon that unites
powerful branding and fine design under a single
image  with u s  wine consumption predicted to
increase to       billion by       wine labels are used
not only to convey a given wines type  producer  and
origin  but also its emotional and cultural appeal  thus 
a wine label is carefully designed to convey the proper
messaging of a particular bottle  whether it is for the
casual consumer or the choosy collector  we have
applied machine learning to the art of wine label
design by learning the design features which most
influence a bottles popularity and selling price  we
have collected over        wine labels and associated
data on price  type  origin  and other notes from the
web  from this data  we have developed an extensive
training set by extracting particular graphical features
from the labels and textual features from the metadata 
by implementing a support vector machine  svm  and
a nave bayes classifier  we have worked to develop a
preliminary model for classifying wines  based on their
labels  into specific price bins  however  our results
demonstrate that the features weve chosen have little
power in predicting the price  bin  of a wine bottle 
especially at granularities that might be useful 
 

introduction
the wine label is an icon that unites powerful branding
and fine design under a single image  wine labels are used
not only to convey a given wines type  producer  and
origin  but also its emotional and cultural appeal  thus  a
wine label is carefully designed to convey the proper
messaging of a particular bottle  whether it is for the casual
consumer or the choosy collector      during the california
wine renaissance beginning in       the evolution of the
wine label reflected the shift from traditional  old world
bottles to fresh  artistic wine packaging     
by       wine consumption in the united states alone is
predicted to increase to       billion      today  wine
producers hire designers and consultants to craft wine labels
that construct a powerful message to their target
demographic  which will purchase wines within a specific
price range  in order to better understand and potentially
improve this design process  we have worked to develop a
wine label classification model  which will predict wine
prices based on both graphical features from the wine label
 

to whom correspondence should be addressed 

and textual features from the wine notes  type  and origin 
we hope that future wine producers will be able to use such
a model to inform their design decisions 
 

methods   results
data collection  there are a number of online wine
shopping websites that have thousands of wines associated
with price  origin  and producer of the bottle  these include
wine com and winechateau com  however  the images on
winechateau com contain the entire wine bottle  while
wine com has images of the label only  thus  we decided to
scrape wine com for all of its        wine labels and wine
name  price  type  varietal  appelation  region  and additional
notes  we used the wine com api to obtain a printerfriendly version of each wines page  we then wrote a
scraper with python and the beautifulsoup library that
locally saves the wine label and the associated data in xml
format 
feature generation  graphical features  we use
matlabs computer vision and image processing
toolboxes to generate features from jpeg images of the
wine labels  usually in the range of    x    pixels  we
found that matlab is sufficient to compute a large
number of features in reasonable time 
image features      seconds for a    x    image  
 height
 width
 aspect ratio  height   width 
 background color  average over pixels in flat
regions  may switch to k means on these pixels 
 grayscale otsu threshold  after clahe   and
number of distinct regions generated by
thresholding
 centrality of edges and corners  weighted mean
with pixels towards center given higher gaussian
weights 
we also analyzed subsections of each image  these
subsections are 
 red pixel values
 green
 blue
 hue
 saturation
 value
 fourier transform
 grayscale
 smaller square  quarter of full image  in center of
grayscale

 

fipredicting wine prices from wine labels





edge map  canny edge detector 
corner strength map
distance map  where distances are computed from
nearest edge 

for each subsection  the image features are 
 mean
 variance
 quantiles                                    
 entropy
 regular otsu threshold without clahe and
effectiveness metric
 quantiles of gradient magnitude
 symmetry score  mse across four axes of
symmetry 
 number of blocks in quadtree decomposition of
square cropped image
the background color is computed as a mean over pixels
in flat areas of the image  we use the edge map to define
flat regions as those that are greater than a certain distance
away from any edges  after examination of wine labels  we
find that there is often a single background color upon
which text and graphics are set  suggesting that this simple
method can be successful in a majority of cases  the
thresholding step begins with the grayscale image  then
performs an adaptive contrast equalization step to adjust for
large scale intensity gradients across the image  this allows
us to binarize the adjusted grayscale image using an
intensity cutoff  determined using otsus method  which
minimizes the inter class variance between the light and
dark groups   we also analyze the performance of this
binarization in terms of the number and size of segments it
creates  finally  we consider the possibility that image
features in the center of the label may be more predictive
than those towards the edges  so we calculate some features
such as edge and corner strengths with greater weights for
features in the center  these weights fall off as a gaussian in
x and y pixels from the center   edge and corner values give
a measure of how much structure and change is present note that we expect many edges and corners when text is
present  so there may be a correlation to the amount of text 
by subsections of each image  we mean that the original
image is the rgb color representation of the wine label 
this   color channel image is then processed into
components  each of the same size  such as    x    pixels 
which more closely correspond to features of interest  for
example  the hsv  hue  saturation  and value  color space
may contain structure that isnt easily determined from rgb
analysis  like gradients in hue that cannot be recreated from
processing individual color channels   we also consider
features involving the frequency components of the
grayscale image computed via the  d dft  in keeping with
our hypothesis about the importance of positional
information in the image  lost in calculating mean  variance 
etc  we also crop the label to a small central square whose
image wide features can be compared to the original  the
edge map is a binary image combining strong and weak
edges found with a canny edge detector  essentially looking
for large connected  oriented gradients in the grayscale
image  the corner map actually assigns values based on the
gradient magnitude in different directions  strong corners

should have large gradients in multiple directions   finally 
the distance map computes  euclidean  distance from the
nearest edge  and can help distinguish between images with
large flat regions and those with lots of interspersed
structure 
for each of these processed images  we compute a
standard set of image statistics  which consider them as
either  d or  d signals  we use mean  variance  quantiles at
various percentages  entropy  measure of texture in  d
signal  related to shannon information   an otsu threshold
without contrast adjustment  and the quantiles of gradient
values  calculated at first order as the difference between
adjacent pixels   we also compute mean standard error
between two halves of the image  comparing top bottom 
left right  and the diagonal symmetry of a square cropped
version  finally  the same cropped square is decomposed
into smaller squares recursively until a threshold of
uniformity is met  quadtree decomposition   so a large
number of resulting squares suggests lots of texture in the
image  we note that for some of the processed images  like
the fourier transform   these statistical features may not
contain useful information  for example the symmetry or
quadtree decomposition in frequency space doesn t have a
clear aesthetic interpretation  however  it doesn t take much
extra time to calculate them  keeps the code simpler  may
yield some surprisingly useful features  and they can always
be easily discarded 
feature generation  textual features  after binning
the wine notes in    increments  we calculated the tf idf
 term frequency   inverse document frequency  of all terms 
considering all the notes from a single bin to be a
document  tf idf is high for frequent words  but is not
skewed towards common low information words  of 
and  the  etc    the top ten words for each bin were
collected to make up a set of words to be used as features 
this was done to shrink the number of words that would be
counted as features for an algorithm combining both text
and image data  as opposed to blindly including all words
ever encountered  the wine type  varietal  appellation  and
region will also be used in the model  but these features did
not require further parsing after data collection 
model creation   analysis  graphical features 
features were generated in several  separate  runs through
the list of almost        wines  including processing of the
label images      continuous valued features   extraction of
classifiers like region and varietal     binary categorical
features   and text processing on the description      
binary features   we first needed to combine all the features
      total per wine  into a final dataset in the appropriate
form for learning  certain wines didn t have images or
adequate descriptions  so these were discarded from our list 
in addition  several of the wines were in     ml  half size 
bottles  which would confound the learning process in that
they have the same label and description as a regular size
bottle of the same wine  but roughly half the price  finally 
after plotting the distribution of prices we realized that there
might not be enough statistics for wines valued in the long
upper tail  as a result  we decided to discard those above the
  th percentile  and restrict our problem to learning wine
pricing in the range below      to preserve sufficient data
for training  this left us with a learning dataset of      
wines with      features per wine 

 

fipredicting wine prices from wine labels

to make features more comparable  we first normalized
each feature to have zero mean and unit variance  this
allowed us to easily perform principal component analysis
on the data  which we hoped would be useful to reduce the
dimension of the problem  fig    shows a cumulative sum of
the eigenvalues from the covariance matrix  demonstrating
that much of the covariance can be replicated using a small
fraction of the principal components  we stored the
eigenvectors and eigenvalues so that we could test learning
using different combinations and numbers of principal
components  the principal components also suggest that no
single feature or set of few features is effective in preserving
covariance  as they contain non negligible components
along several tens of features  which is easily visualized by
plotting  as originally formulated  the question of
 predicting wine prices  is truly a regression problem  but to
begin and determine whether a solution might be feasible 
we decided to start by discretizing the prices into bins 
using the coarsest possible binning  we could label wines as
either high  or low priced  using the median price as the
dividing point  another more interesting option is to use   
bins  which we choose based on the percentiles of prices
 first bin corresponding to the lowest     of prices  for
example   for the latest classifier  data was also scaled to be
in the range        after application of pca  this was
performed both to make values easily comparable and based
on the suggestion of the libsvm algorithm s writers 
finally  in case there was some structure of the wine id s
that could invalidate the assumption that the training and
testing sets were equally distributed  the order of wines was
randomly scrambled before assigning individual samples to
be used for training or testing 
finally  before ultimately training the desired classifier 
we tested many combinations of model parameters and
input features in an attempt to understand and optimize the
learning process  models included naive bayes  using the
binary word features only  and support vector machines
 on all features  with a variety of objectives  kernels  and
parameters  the latter used the free libsvm compiled c
code  cite   we tested using different numbers of principal
components  with and without the        scaling  and also
tried training with the full set of unprocessed data  although
in many cases the differences in performance werent large 
we converged on an svm with the nu objective and radial
basis function  gaussian  kernel  using      principal
components and scaling the resultant features  a set of grid
searches over the model parameters  coarse then fine 
suggested that we should set gamma to     and c to    for
optimal performance  fig      the grid search was
performed with   fold cross validation  and the learning
curves generated using a training set of specified size and a
default testing set of    of the data  or      samples  fig 
    other models tested included a matlab
implementation of an svm  and a regression svm built
into the libsvm software  which performed poorly  with its
predictions limited to the range          
the results of training are disappointing to describe  as the
classifiers performance leaves significant room for
improvement  training with the model selected as described
yields a support vector machine with an accuracy of       
when binary labels are generated by binning the prices into
two bins  if we train with    labels  the accuracy is only

        both of these accuracies were determined using
five fold cross validation on the full dataset  and the latter
case used a set of one vs one svm models to choose the
label with the most votes  according to libsvms output
the optimization converged  but included a large number of
the training samples as support vectors  with a large fraction
of those as bounded support vectors  alpha inequalities
satisfied with equality  
model creation   analysis  textual features  for the
text descriptions  we built a nave bayes classifier  using
the set of top scoring tf idf words as features  training was
done on a randomly selected     of the data  and then the
classifier was tested on the remaining      text data did
not prove to be an informative source of class information 
as the accuracy of the classifier generally ranged between
      matlab implementation  and       python nltk
implementation  when trying different sized bins  started
with    bins  later settled on    equally populated bins with
custom bounds  and different sets of indicator words
 initially best by tf idf  then highest mutual information
score words  and later tried full set of all words  
we tried to apply a more advanced clustering algorithm
based on dimensionality reduction  t sne       but this
showed an equally bleak picture  an amorphous cloud of
points with no visible unified clusters  fig      in a final
attempt to observe some form of coarse grain structure in
the text  we calculated the mutual information of each word
with a particular bin  the top    words by mutual
information for each price segment are shown in fig   
 

discussion
there are multiple reasons why the naives bayes
classifier demonstrated low accuracy  first  each wine
producer allocates a wildly different amount of effort into
writing the description for their product  leaving us with
some wines that had essentially only the year  type and
maybe location in the text  while other wines had a full
paragraph describing the flavors and smells in extensive
detail  in fact  when we tried clustering the wines using k
nearest neighbors based on a vector of word frequencies  the
only clusters that made sense were the ones where producers
put in minimal effort into the description  this was not
necessarily the cheapest wines  just the ones with laconic
texts  laplace smoothing was applied in an attempt to
ameliorate the problem of sparse word vectors  but had little
effect on accuracy 
in the list of top    words by mutual information for each
price segment  fig      little difference can be seen in the
words deemed important  with the possible exception of the
bin             which has a more diverse set of words  this
seems to be a cusp in the pricing landscape  with producers
trying to separate themselves from their lower priced peers 
interestingly  nothing about the most expensive price
segment is particularly distinctive  in fact  it shares three
words with the cheapest segment 
in accordance with the large variance in wine prices and
the idea that their labels and descriptions hold significant
influence over the buyers decision  it seems reasonable that
with a combination of features derived from them and a
large number of wines one might train a classifier to predict
those wines that are more successful  and should therefore

 

fipredicting wine prices from wine labels

be priced higher   the reasons that our approach was only
marginally successful are unclear  even after significant
diagnosis  it is possible that the features used dont
effectively capture the qualities that make a wine appealing 
furthermore  even if they do  the directions in feature space
along which such qualities occur may exhibit less variance
than other less predictive directions  which would result in
predictive power being lost during pca  the model selected
may not in fact be optimal for solving this problem  as the
high accuracy on training sets seen in the learning curve
may be a sign of overtraining  however  the data processing
and model selection described are intended to ameliorate
overtraining and further fixes arent obvious  while sanity
checks were performed at various stages in the process  it is
still possible that during the process of scraping prices 
images  and descriptions   or while combining features from
these different sources   some of the data was corrupted or
mixed incorrectly  which would randomize the correlations
to some extent and confound the training  finally  one might
imagine that the original problem wasnt well formed 
there are two senses in which this may be the case  first 
perhaps the label and description arent significant
predictors of wine price  more optimistically  the
connection between success and price might be too naive 
our features were chosen to separate appealing wines from
less appealing  and therefore less successful  bottles  but it
doesnt logically follow that the most aesthetically pleasing

label and enticing description will be found on the most
expensive wine  rather  a better metric than price may have
been the volume sold or the profit  total or per bottle   these
considerations could all be useful in refining the learning
process  nevertheless  this work has demonstrated that the
features weve chosen have little power in predicting the
price  bin  of a wine bottle  especially at granularities that
might be useful 
acknowledgements
the wine label data was obtained from wine com  additionally  we
are very grateful to the cs     course staff  especially professor
andrew ng  for their inspiration and guidance 

references
    thomas  d  a   and pickering  d  g          the importance of
wine label information  international journal of wine marketing 
            
    caldewey  jeffrey  icon  the art of the wine label  south san
francisco  the wine appreciation guild       
              state of the wine industry  silicon valley bank 
web     dec        www svb com      wine report pdf  
    van der maaten  l p j  and g e  hinton         visualizing
data using t sne  journal of machine learning
research                 

figures

figure    left   cumulative sum of the eigenvalues from the covariance matrix for pca  demonstrating that much of the
covariance can be replicated using a small fraction of the principal components 
figure    right   a set of grid searches over the model parameters  coarse then fine  suggested that we should set gamma to
    and c to    for optimal svm performance 

 

fipredicting wine prices from wine labels

figure    left   svm learning curves  generated using a training set of specified size and a default testing set of    of the
data  or      samples  learning curves generated with model parameters that should be less prone to overfitting show
reduction of both testing and training accuracy 
figure    right   plot produced by t sne  demonstrating lack of structure in the text data  labels are price bins as defined
earlier  randomly selected      text descriptions for plotting 

figure    top    words from wine description by mutual information for each price segment 

 

fi