how well do people learn  classifying the quality of
learning based on gaze data
bertrand schneider

yuanyuan pao

stanford university

stanford university

schneibe stanford edu

ypao stanford edu

abstract
in this paper  we describe how eye tracking data can
be used to predict students learning scores  in a
previous study  the first author collected eye tracking
data such as gaze position and pupil size while
subjects either collaborated or worked independently
on a problem  in this paper  we seek patterns in the
eye tracking data gathered during this experiment to
accurately predict students learning outcomes  we
iteratively tried various machine learning algorithms
and found that a support vector machine  svm  with
a quadratic kernel was able to correctly classify
       of our test data using only aggregated eyetracking counts  we then repeated this approach under
a generalized setting where we extracted features after
applying k means clustering to the gaze data  the
accuracy improved to         these results show
how machine learning techniques can be applied to
make qualitative sense of educational datasets 
  

researchers have been trying to use artificial
intelligence techniques to assess human learning 
   
eye tracking and gaze analysis
knowing the location of a user s gaze can provide
insight into the user s visual attention and his eyemovement control mechanism  in education  this kind
of measure can provide information on what students
perceive and what they miss  moreover  knowing
where students focus their attention provides clues on
their conceptual understanding of a phenomenon 
there has been some preliminary work on using
machine learning techniques on eye tracking data 
however  to our knowledge very few of them tried to
directly predict learning outcomes 
  
experimental setup
in this section we briefly describe the study that the
first author conducted in order to gather our dataset 

introduction

with the recent shift in education from the classroom
to the web  there have been new questions arising as
to how best to analyze the quality of learning 
learning analytics  la  is defined as the
measurement  collection  analysis and reporting of
data about learners and their contexts  for purposes of
understanding and optimizing learning and the
environments in which it occurs  our project deals
with small learning analytics  that is  using a large
set of features over a small number of examples  our
goal is to predict students learning gains when using
this data as input features  from here  we can refine
the range of measures and apply the resulting
algorithm to larger sets of examples 
more generally  current work has been focusing on
different levels of learning analytics  for instance  in
text analytics  researchers have been trying to
automatically classify the quality of an essay     and
to create algorithms to help solve the problem of
creating fast  effective and affordable solutions for
automated grading of written work  lastly 

   
methods
the experiment had three distinct steps  first  students
were assigned to different rooms  they could
collaborate via a microphone when working on a set
of contrasting cases  in one condition  subjects saw
the gaze of their partner on the screen  in a control
group  they did not  they spent    minutes trying to
predict how different lesions would affect the visual
field of a human brain given a document describing
how lesions affect the brain as well as the diagrams
shown in figure     note that we do not use the
speech data and have treated all of the subjects as
independent  
   
measures
at the end of the learning period  each subject took a
learning test to assess their understanding of the topic 
our test measured learning on   distinct categories 
memory  conceptual understanding and transfer
question  we rated collaboration with meier  spada
and rummels     rating scheme  for this first
attempt  we only tried to predict learning scores  in

fifuture work we will also try to predict the quality of
collaboration based on the eye tracking data 

figure    contrasting cases used in this study  subjects had
the answer of two cases  top left and top right  and had to
predict the results of a lesion on the three remaining cases 

   
eye tracking data
for each subject  we have the complete eye tracking
data for the first part of the study  this means that the
users gaze was captured    times per second  this
resulted in approximately        data points for each
participant  a data point can describe the location of
either a fixation or a transition or the size of the
participants pupil at certain time point   in total  we
have    subjects          measures             data
points that we can exploit as input features for our
machine learning algorithm 
   
manual feature extraction
we organize our data in the following way  first  we
divided the screen into   areas of interest based on the
diagram shown in figure    this grid was defined to
separate semantic regions on the screen  we then
computed the counts of fixation on each area   
features  and the transitions between those regions    
features   finally we computed the minimum 
maximum and average pupil size for each example 
in summary  we aggregated the raw gaze data into   
features to feed into our learning algorithm 
  
classification via machine
learning
our goal is to find the best model to classify good and
bad learners based on our gaze data  since our entire
feature set is significantly larger than our training set 
we are very likely to over fit our data and therefore
perform poorly on new and unseen data  our method 
then  is to perform model selection and feature
selection by trying various algorithms and
combinations of the features  and because we want to
maximize the number of training examples  we use

leave one out validation to test our models and
features 
   
model selection
each of our features represents the counts of gazes per
area of interest  the different transitions between
areas  or the cognitive load  pupil size  of the subject 
we assume that these features can be treated as
independent  additionally  we choose to normalize it
so that the relative magnitudes among different
features cannot affect the model parameters  then 
given the normalized data and our independence
assumption  we decided to apply three classification
techniques  nave bayes  logistic regression  and
support vector machines  svm  
   
cross validation
we first used our three algorithms by splitting the data
in half and randomly labeling those two groups as
test and training data  this simplistic approach
led to poor results due to the small number of training
and test examples  we used the leave one out
approach to obtain more reliable test and training
errors  we iteratively trained our algorithms on the
entire dataset  minus one row  and predicted the
category on this example  this process was repeated
m times  where m   number of rows in our dataset  
one advantage of svms over other techniques is the
ability to work in a high dimensional feature space by
using kernels  during model selection  we also varied
our svm algorithm by using different kinds of
kernels  linear  quadratic  gaussian and polynomial  
   

feature selection

to solve our over fitting problem  we tried to select
the best combination of features to improve our
performance  unfortunately  our dataset had too many
features for too few data points  for good accuracy  we
actually only needed the features that are the most
indicative of the actual category  here  we iteratively
ran our best svm model and added in features one at
a time until we achieved our highest test accuracy 
   

results

table   shows the results that we obtained from
running the three models on the gaze data set with and
without feature selection  we describe how we
performed feature selection in the following section 

fitable    accuracy from applying the three classification
algorithms to our data  for svm  a linear kernel was used by
default  when not specified otherwise   training accuracy is
reported only when feature selection was not used 

gaussian
kernel
polynomial
kernel
quadratic
kernel
quadratic
kernel using kmeans
segmentation

training
test
without
feature
selection
test
with f s 
test
with f s 
test
with f s 
test
with f s

naives
bayes

logistic
regression

      

      

support
vector
machine
 svm 
       

      

      

      

n a

n a

      

n a

n a

      

n a

n a

      

n a

n a

      

both nave bayes and logistic regression performed
poorly because they could not even perfectly fit the
data that it was trained on  even though the test
accuracy from logistic regression seems to be better
than svm s test accuracy  if the model cannot even fit
the training data with    error  then it is not capturing
the right information from the training set 
for naive bayes and logistic regression  there were
not enough training examples to cover the entirety of
our high dimensional feature space  nave bayes did
not succeed because it was trying to fit conditional
probabilities on more features than training examples 
so  in many cases  there were at most one example per
feature value and  as a result during testing  would
either not have seen the values or over fit for them 
logistic regression suffers from a similar problem
except  here  the algorithm was using a few examples
to identify the parameters for many more dimensions 
our support vector machine model  when using a
default linear kernel  proved to have the best
performance on our data  since it does not try to
explain each data point  like logistic regression and
nave bayes  but instead tries to maximize the margin
between the two classes in the training examples  we
achieved a      training accuracy but only a       
test accuracy  which led us to believe that our process
was suffering from over fitting and required feature
selection 

after performing feature selection  we were able to
achieve a test accuracy of        with a linear kernel
and        with a quadratic kernel  table     which
is a substantial improvement from the     test error
when using a linear kernel without feature selection 
  

generalization with k means

for our particular data set  it is easy to reduce an
individual s gaze data points into counts per region
since we knew exactly what was displayed on the
screen and which regions were important towards
learning  now  we want to see how our classifiers
perform if we assume only the raw gaze data and no
extra information about the problem 
because each individual subject has a lot of raw gaze
data points  we cannot avoid the task of aggregating
counts over regions of the screen  just like in section
     this is where we apply k means over all of the
available gaze data points to identify the k clusters in
our problem  we treat these clusters as the regions
and feed the counts data as the features for our
classifier  just as before 
   

choosing a value for k

in preparing the data  we first needed to decide on the
value for k  again  we applied svm to the resulting
data for different values of k and chose the value that
yielded the highest leave one out cross validation
accuracy 
we tested for values of k from   to     if the training
accuracy was not       we did not even look at its
test accuracy  also note that  for certain values of k 
k means would not converge  a plot of the resulting
accuracy versus k is shown in figure   

fifigure    parameter tuning for k  test accuracy  using
loocv  versus the value of k as it is varied from   to    

as expected  the accuracy is much lower for smaller
values of k  for the values of k where k means did
not converge  the accuracy values were not consistent
and  therefore  unreliable  even though they are
depicted in figure    therefore  the best value of k
that converged was k    with an accuracy of        
and the centroids are marked on the diagram in figure
  

figure    results from feature selection  our classifier
improved greatly with the first three features added  and our
best result was reached with just ten features 

with just one feature  our performance was already
better than when we used all of the data  and with
just three features  we already achieved an accuracy
of         which is better than that of this exact
classifier with the manually defined clusters described
in section      our highest accuracy was        
which first occurred with ten features 
the top ten features identified through feature
selection and yielded the best performance are listed
in table     in order of output 
table    feature selection results  the top ten features that
are sufficient for a        loocv accuracy on our data set 
output
feature name
output
feature
order  
order  
name

figure    cluster centroids  the    centroids identified
through k means are marked on the screenshot of the problem
that our subjects were working on 

   

 

     

 

     

 

     

 

     

 

     

 

     

 

     

 

     

 

      

  

  

feature selection

with the counts of gazes per    clusters and
transitions between those clusters  we have the input
data for our svm classifier  but we want to be able to
do much better than the        loocv accuracy
achieved when we use every feature  as before  we
want to perform feature selection to find the best
combination of features for classifying subjects as
good or bad learners  the results of running feature
selection are shown in figure   

it is also interesting to note that gaze counts for each
cluster is not among the top ten features nor the top
thirty  the only features that were most useful were
the number of transitions between certain clusters 
the particular transitions in table   are technically
the most indicative of understanding  so we can
possibly infer which pairs of locations on the screen
are good or bad connections  if we cross reference
these features with the centroids shown in figure   
we can see that some of these features involve

fitransitions from a related answer choice on the right
of the screen to a position on a diagram with a related
lesion depicted  for other features  we can see
transitions between unrelated points  which could be a
feature that helps classify bad learners  these top
features provide insight into good and bad approaches
to finding the correct answer to the problems 
   

multiple values of k

we also tried consolidating the count data from
multiple values of k as the input features to our svm
classifier to see if we could improve our accuracy  but
when we ran feature selection  we noticed that we
were picking out only the features associated with the
highest value of k  that converged  to get the best
performance  therefore  we resorted to using only one
k value at a time 
  
conclusion
our preliminary results show that very rudimentary
eye tracking counts can accurately predict complex
outcomes such a students learning or the features that
most indicate good or bad learning  these findings
provide exciting perspectives for online and in situ
education  analyzing gaze movements can provide a
better assessment of understanding  even under
generalized settings where we know nothing about the
problem that is being solved  we can use the gaze data
to identify regions and then aggregate count data for
our classifiers  the performance in this general case
was         a       increase from when we
manually defined those regions 
in addition  the top features identified via feature
selection were extremely helpful in providing insight
into the problem s characteristics  a correct answer
and its supporting diagram  uninformative regions 
etc  we can see that the features chosen  together
with the locations of the centroids from k means  can
help indicate what places were more confusing or
helpful to the subjects 
more data will allow us to make even more finedgrained predictions like predicting whether a
particular misconception is likely to arise among
particular students  from another perspective  we can
also train machine learning algorithms to separate
students who answer a test by using rote
memorization or critical thinking  this is a
particularly valuable approach considering the
difficulty of assessing students thinking skills and the
focus that is currently put on students   th century

competencies 
  

further work

our classification performance was remarkably high 
and this is partly a result of having very few data
points  we would like to gather more data for
different types of problems and different types of
learning styles to be able to evaluate where our
methods are weaker and require more improvement 
since the results from section   demonstrate that we
can find connections between clusters via feature
selection  another next step would be to evaluate the
accuracy of this method  we would like to separate
these features by which category the support falls
under  knowing which features are associated with a
good learning approach can help struggling students
establish the right connections  for example  if their
gaze is at a particular point  then we can suggest the
next location they should be looking at and see if this
scaffolding helps  overall  our results lead to more
questions and more insights into education that help
tailor studying to each student s learning style 
more generally  we envision our results being applied
to a multitude of learning situations as eye tracking
devices become cheaper and widely available  not
only would this approach provides more precise
means of assessing students learning  but also give
teachers a direct and potentially real time feedback on
the kind of concepts that students struggle with 
  

references

    meier  a  et al        a rating scheme for assessing the
quality of computer supported collaboration processes 
international journal of computer supported collaborative
learning        feb              
    pellegrino  j w  and hilton  m l        education for life
and work  developing transferable knowledge and skills in
the   st century  national research council  washington 
dc         

fi