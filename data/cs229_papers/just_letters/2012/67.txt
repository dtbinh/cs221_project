autumn     

cs    project   separating speech from noise challenge

separating speech from noise challenge
we have used the data from the pascal chime challenge with the goal of training a support vector
machine  svm  to estimate a noise mask that labels time frames frequency bins of the audio as reliable or
unreliable  this noise mask could be used by another block in the signal processing pipeline to treat the
unreliable data as missing and then replace the missing data with an estimate of the clean audio by searching a
corpus of clean audio samples for the most probable match using the features of the unreliable data and the
surrounding audio  for this project  we have focused on the noise mask estimation using an svm and not on the
data imputation portion of the problem  it has been demonstrated by kallasjoki et al      that given a good noise
mask  it is possible to achieve significant improvements in automated speech recognition accuracy rates by
replacing unreliable portions of the audio with estimates of the clean audio 
in order to judge the svm classification accuracy in generating a noise mask  we needed an oracle
mask which gave the correct answer for the mask  the oracle mask generates a label of reliable or unreliable for
time frame frequency pairs  using the mel filterbank energies of the clean signal and the noisy signal and labeling
time frequency pairs as unreliable if the snr is less than    db 
using the oracle mask  we estimated the best case performance of noise mask estimation and data
imputation by replacing unreliable time frequency segments  as labeled by the oracle mask  with the known clean
speech audio  as expected  this achieves very good recognition rates that approach the accuracy of the using the
clean speech audio  as previously discussed  previous work     has demonstrated significant performance
improvements using an oracle mask along with sparse imputation methods  estimating the noise mask was a weak
point in the paper cited above  our goal has been to improve on the noise mask generation using machine learning
methods 
since the automatic speech recognition system used in the chime project  htk  uses the mel filterbank
energies as features for a hidden markov model  we started by using these same filterbank energies as features
for the support vector machine to estimate a noise mask  we have used the freely available tools liblinear
    and libsvm     for training and prediction rather than writing an svm from scratch  allowing us to focus on
feature selection  kernel selection  etc  since the noise mask generates a label of reliable unreliable for each
frequency bin  time frame pair  we needed to train a separate svm for each frequency bin 
the full data set consisted of hundreds of thousands of training examples  where each training example
consists of a time frame of mel filterbank energies for each frequency bin   this large data set had the potential
to result in long training and prediction times  twenty separate svms needed to be trained  one for each
frequency bin of interest  we experimented and found that the training accuracies for the svms for different
frequency bins were relatively similar  so to keep the problem tractable  we focused on a single frequency bin  one
svm  while learning about the svm 
we started by using the log filterbank energies for all frequency bins in a given time frame as features for
all svms  all    svms had the same features  just different labels   we used a linear kernel  liblinear  and
found that the classification accuracy was unacceptably low  using the middle frequency bin  k     and        
examples  out of roughly         total training examples  split with        training examples and        test
examples   the linear kernel resulted in a classification accuracy of        scaling the features to a range of
page  

fiautumn     

cs    project   separating speech from noise challenge

       improved the accuracy to       while reducing the runtime by an order of magnitude 
since the training set was quite large  training and prediction with a nonlinear kernel  using libsvm  was
very slow  so we pursued improving the accuracy of the linear kernel  by observing the training and testing
accuracy for various training set sizes  we determined that the svm using a linear kernel was underfitting the data 
so we considered what additional features could be added that would be relevant to generating an accurate noise
mask 
temporal information about what the spectrum of the audio is before and after the time frame of interest
could be used to provide additional information to the classifier  adding features for the frame before and the
frame after for all frequency bins improved the accuracy relative to only scaling the features from       to      
for our test setup         training examples         test examples  frequency bin     linear kernel  
we thought that there was likely to be some correlation between the features that was not accounted for
with the linear kernel  so we investigated adding features that consisted of product terms of the features  this
improved accuracy relative to only scaling the features from       to        however  adding the squared
features meant that the number of features went from    to      we were limited by the virtual address space
available in matlab     bit student edition  in addition to the runtime for svm training and classification  so we
were limited in the number of additional features we could add 
in the chime setup  the speaker was in a fixed location while the noise could be at any location  and the
audio was recorded via two microphones  up to this point we had been using the average of the two audio
channels to find the log filterbank energies to be used as features  we added additional log filterbank energy
features for the difference between the audio channels  using scaling  difference and repeated time frames
improved the accuracy slightly from       to       
at this point our classification accuracy on the training set was still unacceptably low and the linear kernel
was still underfitting the data  we were running into issues adding additional features because we were hitting the
maximum array size in    bit matlab  despite the high runtime of the svm using a nonlinear kernel  libsvm  we
knew we likely needed a nonlinear kernel in order to have the model complexity necessary to fit the data 
similar to the linear kernel  we recorded accuracy vs model complexity for several possible feature sets 
using the original unscaled log filterbank energy features  a radial basis function kernel  the same         training
examples but split using   way cross validation  vs       without cross validation for the linear kernel   the svm
achieved an accuracy of       on the middle frequency bin  k      this is similar to what the linear kernel
achieved  we performed a search over the svm parameters  using cross validation to select the optimal
parameters  unfortunately we did not record a before after accuracy for this step   using scaled features
increased the accuracy to       while decreasing the runtime  adding the difference features that were
previously described along with scaling increased the accuracy from       to       adding the features for the
time frame before and after the frame of interest along with scaling and using difference features increased the
cross validation accuracy from       to        at this point the accuracy when testing on the same data as the
svm was trained resulted in an accuracy of      vs       using cross validation  so the test accuracy was
approaching the training accuracy  it was difficult to tell whether the training accuracy would converge to the test
accuracy given enough data or not  however we knew that the training accuracy represented an upper bound on
the accuracy for the current feature set  we added one more set of features representing the speaker identify
page  

fiautumn     

cs    project   separating speech from noise challenge

 which is known for the chime dataset   where a   in a particular position in the feature vector indicates that
that speaker is talking  after proper scaling and removal of empty rows in the feature matrix  the test accuracy
using the same         training example subset improved from       to       
with this feature set we began looking into using the full dataset as well as feature selection and
parameter selection  using scaled features  difference features  the optimal parameters found for a smaller
dataset  and all         training examples  with     used for training and     used for testing   the svm
achieved a classification accuracy of        similar to the test accuracy of       with the same features and a
        training example subset of the data  adding the speaker identity features previously described improved
the test accuracy of        indicating that the increased model complexity that resulted from adding the speaker
identity features increased the variance when using a subset of the data 
training and testing with the full dataset required several days of runtime  feature selection and
parameter selection both require cross validation for multiple permutations of parameters and features  we
initially performed feature selection and parameter selection using a        training example subset of the dataset 
cross validation accuracy degraded when using a subset of the features  so feature selection using this subset of
the data did not indicate that it was possible to reduce the feature set  we also found the optimal features through
cross validation on the        example subset of the data  however  intuition and experimentation indicated that
the optimal parameters were not constant across training set size  since the model parameters impacted the bias
versus variance tradeoff  we chose to perform feature selection and parameter selection again using a larger
subset of the data          training examples  and   way cross validation for each setting  figure     the feature
selection indicated that reducing the feature set size would degrade the svm classification accuracy significantly
so we did not pursue this further  as expected  the optimal parameters for the larger training set size were not the
same as for a smaller training set size  we retrained the svm with the new estimated optimal parameters  but
unfortunately the svm training and prediction took several days and did not quite complete in time  in parallel  we
used the same full dataset          examples      training      testing  with the new estimated optimal
parameters to correlate with another frequency bin  we chose bin k   as a bin with a high concentration of
speech energy  this run did complete in time  and we found a classification accuracy on the test set of       
this concluded our optimization of the svm  given the time required for training and testing the svm with the full
dataset  we did not extend training and testing to svms for other frequency bins apart from the correlation run
described above  although our experimentation with smaller dataset sizes indicates that the training and testing
accuracy is similar for other frequencies 
figure   below shows the oracle noise mask  the svm estimated noise mask  and the difference between
the masks for a subset of the test dataset  for runtime reasons  a subset of the training dataset was used to train
the svm when generating this plot  so the training accuracy was lower        than what the svm achieved for
the full training and test datasets  table   summarizes the classification accuracy versus settings using a non linear
kernel in libsvm 
in this paper we have described the design of an svm classifier which takes audio clips and generates a
noise mask with labels of reliable or unreliable for time frame and frequency bin pairs  the noise mask can be
used by a data imputation system to replace unreliable audio with an estimate of the clean audio to improve
automated speech recognition accuracy  the features used by the svm are the log mel filterbank energies for
the average and difference of the audio channels along with the speaker identity  the resulting svm achieved a
page  

fiautumn     

cs    project   separating speech from noise challenge

classification accuracy of       for a speech dominated frequency bin  which is representative of the accuracy
that can be achieved for the svms for other frequencies 
table    comparison of classification accuracy vs feature set and settings
features  setup

accuracy

notes

unscaled fbe  libsvm

     

   k examples    way cv  freq k   

scaled fbe  libsvm

     

same as above

scaled fbe  difference features  libsvm

     

same as above

scaled fbe  difference features  before after
features  libsvm

     

same as above

scaled fbe  difference features  before after
features    speaker identity  libsvm

     

   k examples      train      test  k   

same as above

     

   k examples      train      test  k   

same as above with optimized svm parameters

didnt finish in
time

same as above

same as above with optimized svm parameters

     

same examples as above  freq k  

figure    classification accuracy versus svm parameters using         examples

page  

fiautumn     

cs    project   separating speech from noise challenge

figure    svm estimated noise mask versus oracle mask

references
    kallasjoki  keronen  gemmmeke  remes  palomaki  mask estimation and sparse imputation for missing data
speech recognition in multisource reverberant environments  in chime      workshop on machine listening
in multisource environments
    liblinear    a library for large linear classification  machine learning group at national taiwan
university          dec       http   www csie ntu edu tw  cjlin liblinear 
    libsvm    a library for support vector machines  chih chung chang and chih jeh lin          dec 
     http   www csie ntu edu tw  cjlin libsvm 

page  

fi