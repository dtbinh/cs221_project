improvement of an automatic speech recognition toolkit
christopher edmonds  shi hu  david mandle
december         
abstract
the kaldi toolkit provides a library of modules designed to expedite the creation of
automatic speech recognition systems for research purposes  for purposes of acoustic modelling  the toolkit provides a framework for training neural networks by stochastic gradient
descent  recent research  however  indicates that other standard nonlinear optimization
techniques offer speed improvements and simplification of the training process  in this paper we build and test modules compatible with the toolkit to implement alternative training
methods 

 

the automatic speech recognition pipeline

the entire automatic speech recognition pipeline serves to convert a segment of spoken word
audio  known as an utterance  to a corresponding accurate transcript  for our purposes we
can break the automatic speech to text transcription process into several key functional blocks 
feature extraction  acoustic modelling  and decoding 
figure    asr pipeline

   

feature extraction

during feature extraction  windows of raw pcm audio samples are transformed into features
that better represent the speech content of the signal within that window  this is most often
accomplished by use of mel frequency cepstral coefficients  mfccs   these coefficients are
derived from the spectrum of the frequency domain  representing the change in frequency content
of the speech signal  the frequency domain is mapped nonlinearly so as to approximate human
sensitivity to differences in pitch 
consequently  each window of n audio samples comprises a frame that can be described
by a feature vector of its n coefficients  given an utterance m frames in duration  calculation of
mfcc features will result in a matrix of features of dimension m  n 
other methods  such as pca whitening  may then be used to transform and reduce this
feature space 

   

acoustic modelling

an acoustic model serves to map the extracted features to a sequence of likely spoken sounds 
known as phonemes  in particular  this is accomplished by using a sliding window over the
frames of feature data  and then employing a statistical model to predict the probability that
the central frame of the window is a result of any given phoneme being spoken  as such  the
acoustic model ultimately associates each frame of audio with a distribution over the phonemes 

 

fi   

decoding

given a distribution over all possible phonemes for each individual frame  the decoding process
constructs a probable sequence of phonemes  this process must account for the probability of
that spoken sequence generating the given audio signal  but also the prior probability of that
sequence of phonemes being sensical 
this first probability is of course determined from the output of the acoustic model  this
second probability  however  is determined by use of a language model which represents language
usage via a hidden markov model  in this language model  the states are phonemes  for a
monophone model  or sequences of phonemes  the transition probabilities then represent the
probability that one phoneme follows another in standard speech 
as the decoder passes over the data  the possible sequences of states are stored within a
graph known as a lattice  which can be pruned to reject the least likely sequences  once the
most likely sequence is determined  the phonemes can be mapped to complete words  creating a
transcription of the original audio 

 

the kaldi toolkit

kaldi is a research oriented open source toolkit providing a number of utilities which serve to
simplify the creation of an automatic speech recognition system      the individual utilities are
written in c   as stand alone executables  the toolkit specifies a number of file formats for use
in exchanging data between these executables  this separation of utilities and the specification
of file formats allows these utilities be easily pieced together within shell scripts to constitute
end to end speech recognition systems 
for the purposes of feature extraction  kaldi includes utilities to compute mfccs  among
other common speech audio features  utilities are also included for computing feature space
transformations 
for use in acoustic modelling  kaldi includes utilities for training and applying gaussian
mixture models and neural networks  these models may be used individually or employed
serially together in a tandem fashion     
additionally  kaldi offers several utilities for decoding the output of the acoustic model 
with utilities optimized for decoding with large language models  utilities which discard unlikely
outputs  and utilities which make use of lattices to maintain the most likely outputs 

 

some experimentation

to familiarize ourselves with the features of the toolkit  we ran a number of speech transcription
tests with a variety of configurations  training and testing data was acquired from the wall street
journal continuous speech recognition  wsj csr  corpus      which consists of a variety of sets
of english natural language speech utterances with corresponding ground truth transcriptions 
our experiments used data from the si          utterances  and dev        utterances  subsets
of the wsj corpus  we experimented with different sized training sets  simple mfccs  delta
coefficients  the time derivative of mfccs   and kaldis selection of feature space transforms 
the results of these experiments are documented in table   

 

a proposed modification

among its utilities  kaldi provides tools for implementing deep neural networks as part of the
acoustic model  these tools include a framework for training neural networks by the method of
stochastic gradient descent  sgd   however  recent research by le et  al     suggests that the
deep network training process can be simplified and accelerated by employing other standard
nonlinear optimization techniques  namely limited memory bgfs  l bfgs  or the conjugate
gradient method with line search 
while stochastic gradient descent is valuable for optimizing over a large training set  the sgd
method is sensitive to the choice of learning rate and the convergence criteria  consequently 
stochastic gradient descent must often be wrapped within a cross validation framework to obtain
an optimal result on unfamiliar data 
 

fitable    kaldi transcription results
system configuration

word error rate  wer 

features  mfcc coefficients
acoustic model  monophone  gmm
training set       shortest utterances of si   
test set  dev  

      

features  mfcc delta coefficients
acoustic model  triphone  gmm
training set       shortest utterances of si   
test set  dev  

      

features  mfcc delta and delta delta coefficients
acoustic model  triphone  gmm
training set       shortest utterances of si   
test set  dev  

      

features 
mfcc with linear discriminant analysis  lda  and maximum likelihood linear transform
 mllt  feature space transformations for feature space reduction
acoustic model  triphone  gmm
training set  si   
test set  dev  

      

by comparison  l bfgs is a stable algorithm whose convergence can be simply checked by
the norm of the gradient  however  unlike stochastic gradient descent  l bfgs necessitates the
calculation of the gradient over the entire training set  as a result  l bfgs does not directly
scale well to training sets with large numbers of examples  however  by training over minibatches
of the data  this weakness is mitigated 
given the potential improvement to the training process that alternative optimization methods can deliver  as documented by      we develop additional utilities compatible with the kaldi
pipeline that integrate off the shelf implementations of these alternative methods 

 

our approach

we worked off of an existing codebase provided by andrew maas that implements deep neural
network training in matlab using minibatching and integration with the minfunc optimization
method library      we aimed to incorporate this code with the kaldi pipeline to construct a
tandem deep neural network and gmm acoustic model 
carrying out this integration required the creation of new recipes for both the training and
decoding steps in kaldi to support a tandem architecture acoustic model that incorporates the
given l bfgs neural network training codebase 

   

training

at a high level  the training procedure is similar to other tandem asr systems  first  posterior
probability distributions are generated over the phonemes via kaldis monophone gmm training
procedure on the si    dataset  these distributions are then used as labels to train the network
on the windows of raw mfcc feature vectors from the si    training set  we used a    sample
wide window of features as the input features to the neural network  training in this way is very
similar to hybrid asr systems 
in order to integrate the matlab neural network implementation into the kaldi toolkit we
had to perform several steps  first the features and labels needed to be transformed from kaldis
table format into a matlab matrices  similarly we had to create file format transformations
to take matlab output and create kaldi tables  we wrapped this work into a set of shell
script commands that allow us to run matlab as a part of the kaldi workflow 

 

fithe output of the neural network is a probability distribution label indicating the probability
that the central frame of the window corresponds to a given phoneme  once the network has
been trained  features from all of the datasets were fed through the network to create a set of
transformed features  these transformed features are then used to train a gmm using labels
from a previously trained triphone gmm system 

 

results

we compared the performance of the neural network in kaldis tandem recipe to our own
neural network  apart from differences in training algorithms there are a couple of notable
differences between the kaldi neural network and that which we configured  the kaldi neural
network contains one more layer of hidden nodes that is used to create a bottleneck configuration 
additionally  the kaldi network operates on spliced features from    frames at a time while our
network uses a window of    frames as the input features 
our neural network training results are summarized in table    the training error represents
neural network labelling accuracy against the si    training set with the monophone gmm
labels while the testing error represents the networks accuracy against the dev   test set with
monophone gmm labels 
we also capture the amount of time necessary to train the neural networks  making direct
comparisons here is difficult since the kaldi implementation uses a high degree of parallelization
and is written in c   versus the single threaded matlab code used in our implementation 
as can be seen from table   our neural network achieved better accuracies than the kaldi
bottleneck neural network  interestingly our sgd training achieved better results than l bfgs 
from the graphs though it is possible to see that l bfgs improves more monotonically while
sgd tends to have higher variation in accuracy rates 
table    neural network results
neural network
kaldi tandem defaults
our network   sgd              
our network   l bfgs 

si     training 
accuracy
      
   
   

dev    testing 
accuracy
      
      
      

training
time
  hrs
     hrs
   hrs

figure    comparison of test error vs  training time
   
   

test accuracy

   
   
   
   
   
our network  trained by sgd
our network  trained by lbfgs
kaldi defaults

   
 

 

   

   

   

   
    
    
training time  mins 

    

    

    

we took the network trained by sgd and used it to create transformed features to feed
into the existing kaldi gmm training scripts  results from the entire tandem system compared
to kaldis current tandem system are captured in table    we were unable to achieve results
equivalent to the kaldi tandem system  we attempted pca whitening to improve wer but did
not achieve improved results  it is possible that removing some layers from our network when
 

fitransforming features would result in better features for gmm training  this is supported by the
fact that kaldi tandem implementation removes   layers and uses the output of the bottleneck
nodes as transformed features 
table    end to end tandem results
neural network
wer  dev   wer  eval  
kaldi tandem defaults
      
      
our network   sgd 
      
      

 

conclusions

our alternative system achieves neural network output comparable to the results attained by
kaldis default neural network utilities  however  our resulting word error rates are significantly
worse  this suggests that the features being fed from our neural network to the gmm are
not well approximated by the gaussian mixture model  although we attempted to integrate
pca to reduce the dimensionality of our neural network  we were unsuccessful in achieving a
reasonable word error rate  while further research into the implementation of kaldis gmm is
likely necessary to troubleshoot this discrepancy  we suspect that acquiring features from earlier
in the neural network may improve gmm training 

 

acknowledgements

we would like to thank daniel povey for his help in navigating the kaldi toolset  we are also
very thankful for all of the guidance  support and resources that andrew maas provided us
through this process 

references
    h  hermansky  d p w  ellis  and s  sharma  tandem connectionist feature extraction for
conventional hmm systems  in acoustics  speech  and signal processing        icassp   
proceedings       ieee international conference on  volume    pages           ieee 
     
    q v  le  j  ngiam  a  coates  a  lahiri  b  prochnow  and a y  ng  on optimization
methods for deep learning  in proc  of icml       
    d b  paul and j m  baker  the design for the wall street journal based csr corpus  in
proceedings of the workshop on speech and natural language  pages         association
for computational linguistics       
    daniel povey  arnab ghoshal  gilles boulianne  lukas burget  ondrej glembek  nagendra
goel  mirko hannemann  petr motlicek  yanmin qian  petr schwarz  jan silovsky  georg
stemmer  and karel vesely  the kaldi speech recognition toolkit  in ieee      workshop on automatic speech recognition and understanding  ieee signal processing society 
december       ieee catalog no   cfp  srw usb 
    m  schmidt  minfunc   unconstrained differentiable multibariate optimization in matlab 
http   www di ens fr  mschmidt software minfunc html       

 

fi