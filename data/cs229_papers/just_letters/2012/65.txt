marland sitt  tony wu

cs      machine learning

evaluation of credit risk
marland sitt  tony wu
abstract
credit ratings are of large interest to bond investors and debt issuers  machine learning techniques have emerged as prominent ways for corporate credit ratings analysis by achieving better
performance than traditional statistical ones  we report a svmbased credit rating classifier with
    classification accuracy when compared to standard credit ratings  our technique uses a  fold
cross validation to find optimal parameters for the linear kernel  the results indicate possible over
or under rating of companies 

 

introduction

credit ratings are used by bond investors and debt issuers as a measure of riskiness of the companies
and their bonds  they are important determinants of risk premiums and the marketability of bonds 
credit ratings in essence are a measure of bankruptcy risk  the prediction of credit ratings and
thus default risk can have a significant impact on profitability  this was recently brought into light
by the      credit crunch  during this turbulent period  many companies declared bankruptcy  the
traditional approach for credit risk prediction takes into account various quantitative as well subjective
factors such as leverage  earnings  and reputation through a scoring system  some banks  however 
use ratings issued by standard credit rating agencies such as moodys and standard   poors  these
ratings tend to be reactive rather than predictive  therefore  there is a need to develop an accurate
quantitative prediction model for corporate defaults  we demonstrate a support vector mechine based
bankruptcy prediction model  the relationship between the characteristics of the company will be
learned instead of explicitly modeling the underlying dynamics of the company 
we see a few immediate uses of such a classifier  the first would be to obtain credit ratings for a
new company which may not have been rated by a credit rating agency  another would be to obtain
credit ratings for established companies which have recently undergone a large change  for which their
current credit ratings may be out of date  lastly  we can find fundamental misratings of companies 
for example  advanced micro devices  amd  is currently regarded as a b company by moodys  if
our machinery predicts amd as a or c  then this suggests the bonds issued by amd are overpriced
or underpriced  respectively 

 

data collection

data was aggregated from a variety of sources  credit rating data for each company was collected
from morningstar  morningstar com   fundamental data from income statements  balance sheets  and
statements of cashflows for each company was collected from gurufocus  gurufocus com   historical
daily stock open  high  low  and closing prices as well as volume information were downloaded from
yahoo  finance  finance yahoo com  

 

data preprocessing

our collected data falls into one of the following categories  continuous  discrete categorical  or discrete
ordinal 
continuous data does not need any sort of preprocessing  and can be used directly 

 

fimarland sitt  tony wu

cs      machine learning

discrete categorical data needs to be converted into a format a standard
machine learning algorithm can use  assuming we have k classes  we map
into k binary variables  with each element indicating whether that feature
belongs to that class  for example  if we have a discrete variable taking
values in  a  b  c  d   this will get mapped into the   dimensional vector


  class a    class b    class c    class d 

class

credit rating

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

 aaa  aa  
 aa 
 aa  
 a  
 a 
 a  
 bbb  
 bbb 
 bbb  
 bb  
 bb 
 bb  
 b  
 b 
 b  
 ccc  cc 

lastly  we must handle ordinal data  for example   no  low  high  gives
an ordinal ranking  one possibility is to transform the ordinal scale into
a numerical scale  and treat the data as continuous  generally  we cant
quantify the distance between two ordinal rankings  thus  the numerical
scale must be picked rather arbitrarily  which is quite undesirable  another
possibility is to treat the data as categorical data  but this is undesirable
as it loses the ordering information  because the svm implementations
we use do not handle ordinal data for regressor variables  we use one of
table    class mapping
the above approaches  depending on which seems more appropriate for the
data 
since very few companies are given the best and worst ratings  we group the top two and bottom
two ratings into one class  the mapping we use is given in table   

 

methods

as discussed in the data preprocessing section  ordinal rankings have some of the properties of both
numerical and categorical values  both classification and regression methods can be directly applied
to ordinal data  but neither is particularly well suited for the task  we apply two methods to deal
with the ordinal rankings  the first method makes use of an algorithm designed for ranking items 
the second method uses binary svm methods applied to the ordinal classification problem 

   

svm rank

the svm rank method uses a cutting plane algorithm for training linear svms      this algorithm
is not only designed for ordinal classification problems but can be trained in linear time  o sn log n   
for linear kernels  the cutting plane algorithm  as outlined in      iteratively constructs a sufficient
subset w of the set of constraints in the ordinal regression svm 
minimize
w ij  

subject to

c x
  t
w w 
ij
 
 p 
 i j p

t

 i 

 w x     wt x j         ij  

  i  j   p

where p     i  j    y  i    y  j   
in each iteration  it first computes the optimum over the current working set w  it then finds
the most violated constraint and adds it to the working set w  the algorithm then continues by
optimizing over the new working set  unless the most violated constraint is not violated by more than
the desired precision  
to pick the regularization parameter c  we perform a    fold cross validation on the training set
and pick the value that minimizes the misclassification rate  the misclassification rate is defined as
the number of incorrectly classified companies over the total number of companies  a linear kernel is
used in our experiments to utilize the time efficient training of the cutting plane algorithm 

 

fimarland sitt  tony wu

cs      machine learning

output class

confusion matrix
 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

nan 
nan 

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

    
    

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

    
    

     
     

    
    

     
     

    
    

     
     

    
    

     
     

 

 

 

 

 

 

 

 

 

  

  

  

  

  

  

  

target class

figure    confusion matrix for svm rank

round robin learning
 
 
 
 
 
 
 
 
  
  
  
  
  
  

actual class

figure    confusion matrix for svm rank

 

  

  

  

  

  

  

  

 

 

 

 

 

 

 

 

  
 

one simple idea about how to apply binary svms
to the multi class problem is called round robin
learning      assuming we have l classes numbered   to l  we train classifiers mij for    i  
j  l  with this method we obtain l l      
classifiers  in our dataset  we have l       which
results in     binary classifiers 
the classifier mij is meant to distinguish between class i and class j  each svm gives a
vote for a particular class  after collecting all
the votes  the class with the majority of the
votes is selected 
to pick the regularization parameter c  we
perform  fold cross validation on the training
set  and pick the value that minimizes mse 
where mse is defined by letting the classes take
the integer values   through l  further  we use
the linear kernel because of the computational
complexity of other kernels 

confusion matrix
 

predicted class

   

fimarland sitt  tony wu

cs      machine learning

output class

confusion matrix
 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

 

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

  
     

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

  
     

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

 

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

  
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

nan 
nan 

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

     
     

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

  

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

 
    

    
    

    
    

     
     

     
     

     
     

     
     

     
     

     
    

     
     

     
     

     
     

     
     

     
     

    
    

     
     

     
     

     
     

     
     

 

 

 

 

 

 

 

 

 

  

  

  

  

  

  

  

target class

figure    confusion matrix for round robin learning
confusion matrix

 

results

 
 

in section     we present results when applying
svm rank to our ordinal classification problem 
in section     we present results with round robin
learning 

 
 
 

svm rank

we achieve       classification accuracy with the
test data using svm rank  this is at     improvement over the baseline probability of correct classification of        if we expand the tolerance for error to   classes  the classification
accuracy increases to      a confusion matrix
for the results is shown in figure    as shown 
we see most of the predicted classes fall within a
class of distance   of the target class 

 
 
 
  
  
  
  
  
  

actual class

figure   
learning

 

confusion matrix for round robin

  

  

  

  

  

  

 

  

 

 

 

 

 

 

 

  
 

   

predicted class

 

fimarland sitt  tony wu

   

cs      machine learning

round robin learning

performing round robin learning  we can achieve       classification accuracy on the test set  an   
improvement over the baseline probability  if we expand the tolerance for error to   classes  the
classification accuracy increases to      see figure   for the detailed confusion matrix 

 

discussion

figures   and   show color coded confusion matrices for both methods  data points lying outside of the
tolerance band around the diagonal indicate companies of possible concern  companies located above
or below the diagonal indicate a possible under or over rating of credit  respectively  consequently 
this could indicate that the bond prices are over or under valued 

 

summary

we showed that classification of corporate credit ratings based on fundamental and technical financial
data can be done using svm based methods  ordinal regression svms can achieve up to      
classification accuracy while binary svms using round robin learning with a linear kernel can achieve
      accuracy  our model provides insight to predicting credit ratings for a new company which may
not have been rated by a credit rating agency  moreover  a high error of classification could indicate
a fundamental misrating of a company by the credit rating agency 

references
    t  joachims  training linear svms in linear time  proceedings of the acm conference on
knowledge discovery and data mining  kdd        
    furnkranz  j   round robin classification  journal of machine learning research                  
    cardoso  j   da costa  j p   learning to classify ordinal data  the data replication method  jmlr
                   
    t  joachims  making large scale svm learning practical  advances in kernel methods  support
vector learning  b  scholkopf and c  burges and a  smola  ed    mit press       
    o  chapelle  support vector machines in the primal           nov        http   olivier 
chapelle cc primal  
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large
linear classification  journal of machine learning research                     
    chang  chih chung   lin  chih jen         libsvm  a library for support vector machines 
acm transactions on intelligent systems and technology                

 

fi