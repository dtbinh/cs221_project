classifying the subjective  determining genre of music from lyrics
ahmed bou rabee

keegan go

karanveer mohan

december         

abstract

sify songs into ten genres using bag of words lyrics
features  achieving     accuracy  in this paper  we
focus on improving lyrics only classification in hopes
that this eort can help improve music classifications by translating into even more accurate predictions when other features are considered 

in this paper we address the area of multi class classification of single label music genres using lyrics 
previous work     has achieved a maximum of    
accuracy  which we improve upon  to do so  we
implement and compare a collection of supervised
learning algorithms and an unsupervised algorithm
on a uniformly distributed random dataset  processed from an online lyrics database to include
about       songs from   genres  we first analyze
the performance of each algorithm after preprocessing the data in various ways  we then discuss why
clustering algorithms such as k means and knn do
not perform very well in this setting  overall we
found that the ensemble classifier and multi class
naive bayes provide the best accuracies 

 

 

gathering data

our first attempt at this problem used song lyrics
from the recently released        million song
dataset      the accuracy rates after running naive
bayes were quite low  similar to those observed by
liang  gu et al            these authors noted that
the lyrics for the songs in the database were incomplete  we believed that a complete set of lyrics  unlike what was used in other text classifications  could
cause important changes in the performance of algorithms on lyrics and hence proceeded to search for
a dataset of complete lyrics 
we then used a publicly available training set    
of complete music lyrics  with about       songs
and a total vocabulary of about        words  we
randomly chose     th of the data for our training
set and used the rest for our test set  on which we
performed naive bayes  multi class svm  and knn 
our results were unexpected  with about     accuracy for l  regularized multi class svm and naive
bayes  and about     accuracy for k nearest neighbors  knn  k        upon further inspection  we
noticed that our dataset was highly skewed     
of the songs were from the genre classic pop and
rock  in other words  if all of our test data had been
assigned to this category  we would have achieved a
prediction accuracy of above      in an attempt
to establish uniformity among the training data  we
tried to manually reconstruct our data to be uniform  training     random songs from each genre
and testing    dierent random songs from each
genre  unfortunately  our newly uniformed data was

introduction

classifying music into genres based on lyrics is an
interesting problem in the field of music information retrieval that presents several challenges  in
this paper we chose to use individual datasets with
hard labels  but in reality  classifications are subjective  dierent people may assign dierent genres  or
even multiple genres  to the same song  although
we chose individual datasets with hard labels  the
presence of such debate among classifications speaks
to a certain degree of ambiguity in the classification
process  songs in many genres may not have lyrics
as well 
despite these diculties  however  there is still
much potential for song lyric analysis  focusing on
improving the prediction value of song lyrics alone
has allowed us to use music calibrated state of the
art text classification algorithms to improve upon
previously established benchmarks  liang  gu et
al             had used the musixmatch dataset associated with the million song dataset      to clas 

fitoo limited to provide any reliable results  to allow
a more accurate analysis of data  we decided to collect our own data via web crawling that has built in
checks to ensure a certain degree of uniformity  we
wrote a python web crawler to obtain new data from
lyrics wikia com     
we took into account many factors when pooling
and processing our data  while parsing the data  we
ensured that only unique  english songs are considered  the genres we chose were the top   most popular music genres according to the recording industry association of americas  riaa  consumer profile      using the crawler  we pooled     songs from
each of the following categories  blues  country  hiphop  pop  rap  r b  and rock  we then parsed all
the lyrics to produce the vocabulary of the dataset 
each songs lyrics were converted into a multinomial event model representation that consists of a
list of indices of words in the vocabulary and a list
of counts for each index  in addition  several papers mentioned the use of stemming in both reducing the size of the data and helping with accuracy
        in light of this  we created both unstemmed
and stemmed versions of our data sets and compared
the results of our algorithms in each case  in order
to stem our data  we used the porter stemming algorithm      which coalesced words with common
stems and removed a standard list of stop words 
our accuracy rates were similar for stemmed and
unstemmed datasets  with stemming helping only
to reduce the data size  each document was represented by a matrix of        features corresponding
to counts of english words 

 

figure    naive bayes categorical accuracy

parameters for each class  thus for l           
m ni
 i 
 i    l     
i  
j     xj   k  y
m
k y l  
 i    l n    v  
i
i     y
and

y l  

i     y

 i 

  l 

m
after having our probabilities and parameters
trained  we calculated the probability for the test
data and outputted each genre based on maximum
likelihood estimates of the training data belonging
to the genre 

   

random forests

results
figure    random forests categorical accuracy

in this section we describe the results of the algorithms used and briefly summarize algorithms not
covered in class  each bar graph displays the accuracies of the algorithm for categories    rock   r b
  country   hip hop   blues   rap   pop 

   

m

we ran the random forests algorithm with    
trees over the entire dataset and computed the error
with an oob error estimator  random forests classifies using multiple decision trees  each of which uses
a portion of the overall data  the algorithm works
by growing a given number of trees  each of which
is passed a randomly sampled portion of the overall training set and some features o which to base
the decision  each node in the decision tree also
randomly chooses what features to use for that particular split  to obtain a classification on test data 

naive bayes

we implemented the multinomial event version of
naive bayes in which we calculated the probability
of a given training example given each genre  since
we had seven classes  we had to introduce a set of
 

fian example is passed to each tree  which pushes the
example through it and outputs a classification  the
majority classification over all trees is the resulting
classification of the algorithm  among the noted
disadvantages of random forests is that it tends to
overfit when the data contains a lot of noise     

   



m
k
k
n
  
   
 
  y  i    j log p y  i    jx i       
ij
m i   j  
  i   j  

where


 i 

ej x
p y  i    j x i        k
l x i 
l   e

multi class svm

which gives the gradient j j    
m



i  

this approach has been found to work well in the
area of text classification       adding the weight
decay term allows for the overparameterized system
to be solved without resorting to removing one of
the probability vectors  our algorithm made use
of l bfgs provided by      to compute the minimum of the cost function  we found that softmax
initially performed very poorly with roughly     accuracy on both the stemmed and unstemmed data 
we made improvements to the algorithm by normalizing the word frequencies per song based on the
length of that song  intuitively  this helps because
longer songs would tend to have a higher word count
of common words even though the higher count is
not indicative of the genre  by doing this  we found
that accuracy rose to about      following the idea
above  we modified the original data by eliminating words that appeared in too many categories and
ran the test using a vocabulary that focused on the
words that were dierent in each set  when this was
done  the accuracy rose to around      comparable
to the other tests 

figure    multi class svm categorical accuracy

we used an l   regularized l  loss svm classification      by implementing the one v s  one
method instead of one against rest since it is much
faster and  as observed by c w  hsu and c j  lin 
is comparable in performance      as we were in a
multi class setting  we solved   binary svms  we
determined the optimal value of the relative weighting factor  c  by trying various values  finding that
the best performance was observed with c     

   

    i 
 
x    y  i    j   p y  i    j x i          j
m

softmax regression

   

k nearest neighbors

figure    softmax confusion categorical accuracy

as previous existing implementations were too
slow  we implemented our own version of softmax
regression with regularization  in our implementation  we used the modified cost function j    

figure    knn categorical accuracy

 

fithe k nearest neighbors algorithm  knn  works
by polling the points which surround a training example  more specifically  the algorithm keeps the
entire training set it is given  and when a test example is given to it  it finds the k nearest points to the
test example and outputs the most common class
among these k neighbors  we used dierent metrics
for finding the nearest neighbor  including euclidean
distance and correlation  in general  we found that
using the metric of euclidean distance gave better
performance than the correlation metric  we quantitatively measured the accuracies choosing dierent
values for k  and found that k       gave us the best
results over several rounds of testing 

   

figure    ensemble confusion matrix  this confusion matrix represents the output of our ensemble algorithm  a bluer color indicates that more classifications were made in that square and a
whiter color indicates the opposite  category    hip hop  has the
least confusion while category    pop  has the most confusion 

ran the ensemble with only our top   performing algorithms  in case of a tie  preference was given in the
order of nb   svm   random forests   softmax 
the updated ensemble consistently performed better than our best algorithms  reaching     accuracy 
the reason the accuracy only improved by      in
comparison to the best standalone algorithm  naive
bayes  for any test data was because most of our
algorithms were largely correctly classifying or misclassifying the same songs  again implying that certain songs cannot be classified well using lyrics 

k means

we decided to use an implementation of k means
     to test unsupervised learning on this problem 
we ran k means with   clusters for         iterations  since these clusters didnt have an a priori
class associated with them  we iterated over all permutations of categories and picked our accuracy for
this algorithm to be from the best performing permutation  this algorithm yielded an accuracy of   
   another approach we tried was running k means
with     clusters and replacing each data point that
k means is associated with with that cluster  using
that as our new training matrix  we found that
this provided comparable results with naive bayes
and knn  gayathri  k   et al             suggested
that this implementation would increase accuracy 
however  we did not see significant increases  we
speculate that k means may cluster the songs into
subcategories which are not correlated with genre 

   

 

analysis

ensemble

our initial ensemble of classifiers used all our individual learning methods to see which genre most
algorithms classified the lyrics into  the rationale
behind this was that as the number of hypotheses
increases  the chances that all of them will misclassify the lyrics decreases  we observed that running
this ensemble in some cases did worse than our best
algorithm  this became clear since knn and kmeans were giving accuracies of around     and
    whereas all the other algorithms had accuracies
of at least     and hence  knn and k means were
increasing the number of misclassifications  we then

figure    each bar represents the overall accuracy of one of our
algorithms over all   categories 

naive bayes consistently provided the highest accuracy rates with an average of about      followed
by accuracy rates of svm  random forests and
softmax  we found that clustering algorithms such
as k means and knn did not provide as high accuracies  which we believe is largely due to the overlap
of the data  when we examined our dataset  we
found that most music genres share at least     of
 

fitheir words  and furthermore  these words tend to
be common words in the english language  categories which contained unique words were classified
more accurately than those which did not  the genre
of hip hop shared the least amount of words with
other categories whereas pop songs shared the most 
overall  words that are unique to a category tend to
be limited to a few songs in that category and therefore do not provide much guidance in dierentiating
between genres  though this aects all our categorizations in general  our clustering algorithms are in
particular aected because the neighbors nearest to
a particular example are often not in the same song
genre 
our best results came from the ensemble learning algorithm when it did not include knn  since
the accuracy increase was only around       this
suggests that our models had large overlap in which
songs it classified correctly and incorrectly  leading
to little gain when we combined them  for instance 
some songs can be remixed from one genre to others  without any change in lyrics  this means that
certain songs have a certain ambiguity that cannot
be learned  while others are more indicative 

 

        http   www cs cmu edu  music dawenl 
files final pdf
    thierry bertin mahieux  daniel p w  ellis  brian
whitman  and paul lamere  the million song
dataset  in proceedings of the   th international
conference on music information http   labrosa 
ee columbia edu millionsong  
    publicly available  processed music lyrics  http 
 alliance seas upenn edu  cis    wiki 
    free unprocessed lyrics  lyrics wikia com 
    top ten most popular genres      http            
         e    b e  f   da      b  edce f 
pdf
    gaustad  tanja  accurate stemming of dutch for
text classification  alfa informatica rijksuniversiteit groningen       
    jensen  lee s   and tony r  martinez  improving
text classification by using conceptual and contextual features  computer science department 
brigham young university       
    porter stemming algorithm included in the python
library  http   nltk org 
    segal  mark r  machine learning benchmarks and
random forest regression  division of biostatistics 
university of california        

conclusion

     r  e  fan  k  w  chang  c  j  hsieh  x  r  wang 
and c  j  lin  liblinear  a library for large linear classification  journal of machine learning research                    

we have observed how using complete song lyrics
along with an ensemble of the best text classification
algorithms can help improve accuracy of music classification  while clustering algorithms have often
performed well in other text classification settings 
they seem unsuited for lyrics classification  we have
also observed how lyrics alone cannot provide a complete picture of a songs genre  adding more features such as length  rhyme scheme  and multi word
context might allow for better classification  other
possibilities of future work include experimentation
with these algorithms using non standard distance
metrics  dierent ways of examining the data  for
example with multi label classification  looking at
the top two choices of genre and seeing if either of
those match to the actual genre  may also provide
novel insight 

     hsu  chih wei  and chih jen lin  a comparison of methods for multi class support vector machines  department of computer science and information engineering  national taiwan university
      
     stanford site explaining softmax regression
http   ufldl stanford edu wiki index php 
softmax regression
     an implementation of k means by adam
coates from  https   sites google com site 
k meanslearning 
     an implementation of minfunc  http   www di 
ens fr mschmidt software minfunc html
     gayathri  k   and a  marimuthu  an improved
knn text classification algorithm by using kmean clustering  international journal of computing technology and information security            
      

references
    liang  dawen  haijie gu  and brendan oconnor 
music genre classication with the million song
dataset  machine learning department  cmu

 

fi