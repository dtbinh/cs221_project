cs    final report
reinforcement learning to play mario
yizheng liao

kun yi

zhe yang

department of electrical engineering
stanford university
yzliao stanford edu

department of electrical engineering
stanford university
kunyi stanford edu

google inc 
elenayang google com

abstractin this paper  we study applying reinforcement
learning to design a automatic agent to play the game super
mario bros  one of the challenge is how to handle the complex
game environment  by abstracting the game environment into
a state vector and using q learning  an algorithm oblivious
to transitional probabilities  we achieve tractable computation
time and fast convergence  after training for      iterations  our
agent is able to win about     of the time  we also compare
and analyze the choice of different learning rate  and discount
factor  

i 

i ntroduction

using artificial intelligence  ai  and machine learning
 ml  algorithms to play computer games has been widely
discussed and investigated  because valuable observations can
be made on the ml play pattern vs  that of a human player  and
such observations provide knowledge on how to improve the
algorithms  mario ai competition     provides the framework
    to play the classic title super mario bros  and we are
interested in using ml techniques to play this game 
reinforcement learning  rl      is one widely studied
and promising ml method for implementing agents that can
simulate the behavior of a player      in this project  we
study how to construct an rl mario controller agent  which
can learn from the game environment  one of the difficulties
of using rl is how to define state  action  and reward  in
addition  playing the game within the framework requires realtime response  therefore the state space cannot be too large 
we use a state representation similar to      that abstracts
the whole environment description into several discrete valued
key attributes  we use the q learning algorithm to evolve
the decision strategy that aims to maximize the reward  our
controller agent is trained and tested by the      mario ai
competition evaluation system 
the rest of this report is organized as follows  section  
provides a brief overview of the mario ai interface and the
q learning algorithm  section   explains how we define the
state  action  reward to be used in rl  section   provides
evaluation results  section   concludes and give some possible
future work 
ii 

background

in this section  we briefly introduce the mario ai frameword interface and the q learning algorithm we used 

a  game mechanics and the mario ai interface
the goal of the game is to control mario to pass the finish
line  and gain a high score one the way by collecting items and
beating enemies  mario is controlled by six keys  up  not used
in this interface   down  left  right  jump  and speed 
in the game  mario has three modes  small  big  and fire 
he can upgrade by eating certain items mushroom and flower  
but he will lose the current mode if attacked by enemies  other
than mario  the world consists of different monsters  goomba 
koopa  spiky  bullet bill  and piranha plant   platforms  hill 
gap  cannon  and pipe  and items  coin  mushroom  bricks 
flower   the game is over once small mario is attacked  or
when mario falls into a gap  for more specific descriptions 
please see     and     
when performing each step  the mario ai framework interface call could return the complete observation of    x    grids
of the current scene  as shown in figure    that is  an array
containing the positions and types of enemies items platforms
within this range  this is the whole available information for
our agent 
the benchmark runs the game in    frames per second 
the environment checking functions are called every frame 
therefore  while training we have to train and update the
qtable within    milliseconds 
b   greedy q learning
q learning treats the learning environment as a state machine  and performs value iteration to find the optimal policy  it
maintains a value of expected total  current and future  reward 
denoted by q  for each pair of  state  action  in a table  for
each action in a particular state  a reward will be given and
the q value is updated by the following rule 
q st   at      s a  q st   at   s a  r  max q st     at      
   
in the q learning algorithm  there are four main factors  current
state  chosen action  reward and future state  in      q st   at  
denotes the q value of current state and q st     at     denotes
the q value of future state           is the learning rate   
       is the discount rate  and r is the reward      shows that for
each current state  we update the q value as a combination of
current value  current rewards and max possible future value 
we chose q learning for two reasons 
  

although we model the mario game as approximately
markov model  the specific transitional probabilities

fibetween the states is not known  had we used the
normal reinforcement learning value iteration  we will
have to train the state transitional probabilities as
well  on the other hand  q learning can converge
without using state transitional probabilities  model
free   therefore q learning suits our need well 
when updating value  normal value iteration needs
to calculate the expected future state value  which
requires reading the entire state table  in comparison 
q learning only needs fetching two rows  values for
st and st     in the q table  with the dimension of
the q table in thousands  q learning update is a lot
faster  which also means given the computation time
and memory constraint  using q table allows a larger
state space design 



if collided with creatures in current frame    or   



nearby enemies  denoting whether there is an enemy
in   certain directions in  x  window  or  x  window
in large fire mario mode  



midrange enemies  denoting whether there is an enemy in   certain directions in  x  window  or  x 
window in large fire mario mode  



far enemies  denoting whether there is an enemy in  
certain directions in   x   window  or   x   window
in large fire mario mode   note the nearby  midrange 
and far enemies attributes are exclusive 



if enemies killed by stomp    or    set true if enemy
killed by stop in current frame 

the learning rate  affects how fast learning converges  we
use a decreasing learning rate s a     different for different
 s  a  pairs  specifically 
 
st  at  
   
  of times action at performed in st



if enemies killed by fire    or    set true if enemy
killed by fire in current frame 



obstacles    bit boolean indicating whether there exist
obstacles in front of mario  see figure   

  

the equation is chosen based the criteria of proposed by
watkins original q learning paper  he shows the following
properties of  is sufficient for the q values to converge 
  
  
  

 st   at      as t   
 st   at   monotonically decreases with t 
p

t    st   at      

one can easily verify the series     satisfy all the properties 
the discount factor  denotes how much future state is
taken into account during optimization  we evaluate under
several  values and chooses     as the final value  we will
show that non optimal learning parameters lead to highly
degenerated performance in the evaluation section 
when training our agent  we actually used  greedy qlearning to explore more states  the algorithm is a small variation of q learning  each step the algorithm chooses random
action with probability   or the best action according to the q
table with probability      after performing the action  the
q table is updated as in   
iii 

a mario state thus needs    bits to encode  meaning the
number of possible states is       however  in actual scenes no
every state will appear  as we use a hashtable to store the q
values  only states visited will be stored and tracked  at the
end of training  we found there are only about       states in
the final table  and the number of states visited often       
times  is even less  meaning the state space is very sparse 
the stuck state variable is added to handle the situations
where the action taken doesnt work as expected due to the
position rounding problem  for example  we notice when
mario is close to a tube  the jump right action is chosen
in order to jump up the tube  which is correct  however  when
mario is too close to the tube  jump key will not work and
mario wont move  as a result mario will be performing the
same action over and over  stucked   by adding a stuck state
variable  we observe mario is able to go into a new state and
choose different actions in order to break the loop 

m ario c ontroller d esign u sing q l earning

in this section  we describe how we design the state  action 
and reward to be used in the q learning algorithm  we also
briefly comment on why we design the state as it is 
a  mario state
we discretize the environment description into the following integer valued attributes 


mario mode      small      big    fire 



direction  the direction of marios velocity in current
frame    directions   stay  total of   possible values 



if stuck    or    set true if mario doesnt make
movement over several frames 



if on ground    or   



if can jump    or   

fig    

mario scene

the figure   shows a typical scene of the mario environment consisting of the platform and enemies  the area within

fidark blue  lighter blue  purple boxes indicates the range of
nearby  midrange  and far enemies respectively  for instance 
in the figure we have both mid range and far enemies  in
the easy level we coped  we dont distinguish different typed
enemies  the four red circles denote the obstacles array  if
there is obstacle the corresponding bit is set to   
b  actions
the mario agent performs one of    actions from the key
combination  left  right  stay  x  jump  notjump 
x  speed fire   nospeed  
c  rewards
our reward function is a combination of weighted state
attribute values and the delta distance elevation it performs
from the last frame  basically  moving forward  jumping
onto higher platforms and killing enemies will be positively
rewarded  whereas moving backward  colliding with enemies
and being stuck will be negatively rewarded  we also let the
reward of moving forward decrease when nearby enemies are
present  note we carefully design our state such that the reward
r s  is only a function of the state s  not based on what actions
are taken to reach s 
iv 

e valuation r esults

for training the q learning algorithm  we firstly initialize
the q table entries with a uniform distribution  i e  q 
u             then we trained the agent by       iterations
on a fixed level for the three mario modes  the order is  train
the level episode    times for small mario     times for large
mario     times for fire mario  repeat  etc  we believe in this
order the fire mario will be able to use the q table information
from previous runs even when he is attacked and downgraded 
during training the learning parameters are
 
 st   at    
  of times at performed in st
       
      
      
for every    training episodes of each mode  we evaluate
the performance by running     episodes on the current q table and use the average metrics as the performance indicators 
the evaluation episodes are run with                so the
learning is turned off and random exploration is minimal  it is
purely a test of how good the policy is 
we use   metrics to evaluate performance
  
  
  
  

a composite score combining weighted win status 
kills total  distance passed and time spent 
the probability the agent beats the level 
the percentage of monster killed 
the time spent on the level 

figure    shows the learning curves of evaluation score
using the previously described optimized parameters  the

fig    

evaluation score

discount factor    is      meaning that the q learning algorithm tried to maximize the long term reward  obviously  our
algorithm demonstrates a learning curve and quickly converges
to the optimal solution after about      training iterations 
at the end of training cycles  our average evaluation score is
around       for some evaluation cycles  we can even achieve
     points  which is nearly the highest score human can
achieve in one episode 
in order to show the generalization  we also tested the
trained q learning algorithm with random episode seeds  the
results show that for most random seeds  our trained algorithm
performs reasonably good  the reason that one test performs
bad is that there always be some unknown situations  to which
mario is not trained 
in addition  we plot the learning curves with fixed learning
rate          throughout training  and low discount factor
          as discussed above  in our training algorithm  we
keep decreasing the learning rate  the figure indicates that with
a fixed learning rate  when it converges  the converged solution
is not optimal and the variance in scores is larger  a low
discount factor means that the learning algorithm maximizes
the short term reward  in our learning algorithm  we gave
positive reward for right movement and negative reward for
left movement  if the algorithm tries to maximize short term
reward  mario will always move the right  however  in some
situations  mario should stay or go left to avoid monsters 
in this case  the short term reward maximization is not the
optimal solution 
figure    shows the winning probability learning curve  as
the early stage of learning process  the winning probability is
as low as      with the increase of training cycles  the winning
probability increases and converges to around      for the low
 learning  even the average probability is increasing but the
variance is very high  for the fixed  learning  the learning
curve converges but the converged value is not optimal  the
curve demonstrates that our trained agent is consistently good
on beating the level 
figure    shows the percentage of monster killed  since
we generated the training episode by the same seed and setup 
the total number of monsters within a episode is the same

fifig    

winning probability

fig    

time spent in frames

v 

c onclusion

in this project  we designed an automatic agent using
q learning to play the mario game  our learning algorithm
demonstrates fast convergence to the optimal q value with
high successful rate  the optimal policy has high killing rate
and consistently beat the level  in addition  our results show
that the state description is general enough  that our optimal
policy can tackle different  random environments  further  we
observe that long term reward maximization overperforms
short term reward maximization  finally  we show that using
decaying learning rate converges to a better policy than using
fixed learning rate 

fig    

percentage of monster killed

over training  at the beginning  the q values are generated
randomly  therefore  the killing percentage is very low  the
learning curve shows fast convergence of the killing probability
within a few training episodes  due to the high reward we gave 
there are two reasons that we give high reward for killing 
firstly  the killing action is given high score in evaluation and
therefore  we can achieve a high score in evaluation  secondly 
the mario is safer when he kills more monsters 
in this figure  the learning curve with fixed  shows a similar performance to our optimal learning curve  the learning
curve with low  has very low mean and high variance 
figure    shows the time spent for the mario to pass a
episodes successfully  the optimal learning curve shows a fast
convergence within     iterations  the learning curve with low
 has a similar performance as the optimal learning curve  the
reason is that the short term reward maximization forces the
mario to keep moving rightward  however  as we discussed
above  it is not optimal since the mario will collide creatures
with high probability  the learning algorithm with fixed 
needs more time to win because the converged policy is not
the best 

our rl approach could be extended in a number of ways 
for example  in our learning algorithm  we did not design
the state for the mario to grab mushroom and flowers  in
addition  our algorithm focuses on optimizing the successful
rate  possible future work may include  but is not limited to 


extend our state to allow grabbing coins and upgrading 



vary the reward function to realize different objectives
 e g  killer type mario 



make the state design more precise to cope with the
position rounding problem 



explore other rl approaches such as sarsa    and
fuzzy sarsa    to reduce state space and increase
robustness 

we believe that our work provides a good introduction to this
problem and will benefit the people with interests in using
reinforcement learning to play computer games 
vi 

a ppendix



source code  http   code google com p cs   mario



demo    http   youtu be gtwr zehonq



demo    http   youtu be qapthnd js 

fir eferences
   

   
   
   

   
   
   

j  togelius  s  karakovskiy  and r  baumgarten  the      mario ai
competition  in proceedings of the ieee congress on evolutionary
computation  citeseer       
the mario ai competition        http   www marioai org  
r  sutton and a  barto  reinforcement learning  an introduction  vol    
cambridge univ press       
j  tsay  c  chen  and j  hsu  evolving intelligent mario controller by
reinforcement learning  in technologies and applications of artificial
intelligence  taai        international conference on  pp         
ieee       
watkins and dayan  q learning machine learning       
g  a  rummery and m  niranjan  on line q learning using connectionist
systems  tech  rep        
l  jouffe  fuzzy inference system learning by reinforcement methods 
systems  man  and cybernetics  part c  applications and reviews  ieee
transactions on  vol      pp           aug      

fi