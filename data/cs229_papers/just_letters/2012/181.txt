parallel  learning  of  content  
recommendations  using  map reduce  
michael  percy   mpercy cs stanford edu   
stanford  university  
  

abstract  

in   this   paper    machine   learning   within   the   map reduce   paradigm   for  
ranking   online   advertisements   is   explored    online   logistic   regression  
 olr    is   used   to   predict   click through rates   on   advertising   data    a  
parallelized   stochastic   gradient   descent   method   is   used   in   order   to  
split   the   dataset   across   multiple   machines    and   the   resulting  
predictions  are  evaluated   
  

introduction  

the   content   ranking   and   recommendation   problem   is   a   common   one   in   industry   
particularly  in  the  realm  of  advertising   as  the  desired  number  of  features  to  be  used  
for   ranking   grows    and   the   number   of   items   needing   to   be   learned   also   increases   
larger   and   larger   numbers   of   training   examples   are   relied   upon   in   order   to  
accurately  predict  appropriate  contextual  rankings  of  them   at  the  scale  commonly  
seen  in  production  systems  today   parallelism  is  required  in  order  to  process  such  
large   amounts   of   data   in   a   reasonable   timeframe    learning   at   such   a   large   scale  
requires   that   the   learning   process   be   split   across   multiple   machines   and  
computation   parallelized    a   very   common   approach   to   parallelization   is   the   map 
reduce  paradigm   which  has  an  open  source  implementation  as  part  of  the  hadoop  
project    the   goal   of   this   project   is   to   implement   a   parallel   stochastic   gradient  
descent    sgd    algorithm   on   top   of   map reduce   and   apply   that   algorithm   to   learning  
a  contextual  content  ranking   

data     features  
the  data  set  being  used  is  the  kdd  cup        track      predict  the  click through  rate  
of   ads   given   the   query   and   user   information    dataset          this   data   set   consists   of  
online  advertisement  click through  data  consisting  of  the  ad  id   user  id   number  of  
impressions    number   of   clicks    and   various   other   features   including   contextual  
information   such   as   query   and   keyword   data    the   data   set   is   not   huge   by   big   data  
standards    but   still   sufficiently   large   as   to   benefit   from   parallelization    at     gb  
uncompressed   csv   at     m  records  in  the  training  set   
  
the   following   features   from   the   data   set   were   used   for   modeling   the   data    user  
gender    user   age   group        groups   plus   unknown     and   ad   position    cross   product  
feature   of   depth   x   location     these   features   were   chosen   because   they   are   very  

ficommon   and   widely   available   ad   targeting   features   in   industry    required   less   pre 
processing   to   expose   as   feature   vectors    and   seemed   like   obvious   choices   for  
predicting  who  might  click  on  a  given  advertisement   

building  blocks  
the  software  written  as  part  of  this  project  was  built  using  the  apache  crunch       
map reduce   framework    crunch   is   an   implementation   of   the   flumejava        
framework   which  came  out  of  google   it  provides  a  straightforward  programming  
model   that   abstracts   the   mapreduce   paradigm    providing   a   programming   model  
that   allows   for   defining   pipelines   that   allow   for   shuffling    sorting    grouping    and  
parallel   execution   of   mapper   and   reducer   tasks    all   within   a   hybrid   functional      
object oriented  paradigm  in  the  java  language   
  
another   open   source   software   project    which   aims   to   provide   building   blocks   for  
implementing  scalable  machine  learning  algorithms   is  apache  mahout        mahout  
provides  several  types  of  out of the box  machine  learning  algorithms   one  of  which  
is   a   general   online   logistic   regression    olr    implementation    mahouts  
implementation  of  olr  is  a  single machine  in memory  implementation   as  such   no  
features   are   provided   to   attempt   to   split   its   stochastic   gradient   descent   method  
across  multiple  machines   

parallel  stochastic  gradient  descent  
in  order  for  a  machine  learning  algorithm  to  fully  reap  the  benefits  of  running  on  a  
map reduce   cluster    a   method   for   parallelizing   the   optimization   step   is   desired    one  
method  for  parallelizing  stochastic  gradient  descent  is  described  in  zinkevich   et  al  
paper  from  nips              the  method  is   as  the  authors  say   strikingly  simple   
  
first    a   number   of   splits   k   is   determined    the   dataset   is   randomized   and   each  
machine   is   given   an   equal   portion   of   the   data   to   process    each   machine   runs  
stochastic   gradient   descent    sgd    in   parallel    using   a   fixed   learning   rate    each  
machine   returns   the   learned   weight   vector   to   the   master   routine   which   collects  
those   weights   and   then   simple   averages   them   to   get   the   resulting   weight   vector  
which  is  used  for  classification   

ranking  advertisements  

one  way  of  addressing  the  online  advertising  problem  is  to  view  it  as  a  problem  of  
predicting   the   expected   click through rate    ctr    of   each   advertisement    based   on  
what   is   known   about   the   user   viewing   it    the   context   of   the   request    and   the  
historical   performance   of   the   ad   itself    while   multiple   ads   may   be   shown   on   a   single  
page  at  one  time   because  the  inventory  is  so  large    in  the  case  of  the  kdd  cup  data  
there   is   an   inventory   of             ads      an   assumption   of   independence   seems  
reasonable   therefore   one  way  to  approach  the  problem  is  to  model  the  problem  as  
solving   for            where   we   are   trying   to   find   the   probably   of   the  
user  clicking  on  a  given  ad   assuming  it  was  shown  to  them   
  

fiimplementation  
in   order   to   implement   this   ctr prediction   model    parallel   stochastic   gradient  
descent  was  used  in  tandem  with  online  logistic  regression  to  train  models  against  
advertisements  of  varying  numbers  of  examples   
  
this   histogram   gives   a   sense   of   how   widely   the   data   set   varied    in   terms   of  
impressions   there  is  a  clear  long tail  pattern   where  a  handful  of  ads  get  millions  of  
impressions    a   significant   portion   get   hundreds   of   thousands    and   many   get   only   a  
handful  of  impressions   
  

  
  
several   different   data   sizes   were   selected   for   modeling   in   order   to   get   a   sense   of  
how  the  parallelization  and  splitting  affected  prediction  accuracy   in  each  case   the  
total   data   set   was   split   into   a   training   set           nd   a   test   set            and   the   root  
mean  squared  error   rmse   was  used  to  evaluate  the  effectiveness  of  the  algorithm   
in   the   case   of   the   training   data    the   data   was   processed   so   that   sgd   would   train  
against  discrete  click     no click  examples   however  when  evaluating  on  the  test  set   
the  logistic  model  prediction  was  compared  to  the  session  ctr  of  each  user   in  that  
way   we  are  able  to  treat  the  logistic  regression  output  as  predicting  the  ctr  of  each  
ad  directly   
  
below   is   a   table   showing   how   various   split   sizes   affected   the   prediction   error   of   the  
algorithm   based  on  evaluating  a  wide  range  learning  rates  from     to        a  learning  
rate  of      was  seen  to  be  generally  most  effective  across  all  data  set  sizes   therefore   
only   the   numbers   corresponding   to   a   learning   rate   of        are   presented   here    we  
show  how  the  train  and  test  error  rate  varies  as  the  number  of  splits  are  varied   
  

fiadid  
          
          
          
          
          
          
          
          
          
          
          
          

num   splits  
   
   
    
   
   
    
   
    
   
   
    
   

train  rmse  
           
          
          
           
           
           
           
           
           
          
           
           

test  rmse  
           
           
           
           
           
           
           
           
          
           
          
           

  
looking   at   a   graph   of   the   test   error   data    we   can   see   that   as   the   data   sets   grow    it  
becomes  more  difficult  to  accurately  predict  the  ctr   it  seems  likely  that  that  has  to  
do   with   ads   getting   more   exposure   having   more   general   appeal    whereas   ads   that  
get   less   exposure   may   be   more   likely   to   have   niche   appeal    this   seems   indicative  
that   the   chosen   features   are   not   sufficient   to   model   the   data    as   we   can   see   from   not  
only  the  test  error  going  up   as  larger  numbers  of  impressions  are  used  for  training  
and  testing   but  also  the  training  error  rising  as  well   it  seems  likely  that  additional  
content   or  context   based  features  could  achieve  better  results  on  the  more  popular  
ads   note  below   in  this  graph   that  the  rough  total  number  of  impressions  is  listed  
next   to   each   ad   in   parenthesis    that   is   the   total   number   of   impressions   in   the   entire  
data  set   before  splitting  into  train     test  sets   
  

  

  

ficonclusions  
as  can  be  seen  from  looking  at  these  graphs   the  parallelization  method  proposed  in  
     appears  to  be  extremely  effective   at  least   it  does  not  appear  to  hurt  the  rmse  
on   the   test   set   very   much    in   the   case   of   the               impression   ad    taking   the  
parallelization  factor  from      regular  olr   to      only  increased  the  rmse  by         
on   the   smaller    niche    ads    where   our   features   modeled   the   data   admirably    there  
was   also   a   negligible   difference    and   in   one   case   the   error   rate   was   even   reduced   by  
the   parallelization    it   seems   likely   that   that   can   be   attributed   to   some   kind   of  
regularization  effect   
  
from   a   practical   standpoint    its   also   worth   nothing   that   writing   and   debugging  
distributed   programs   is   harder   and   takes   longer   than   writing   single machine  
software  in  a  language  like  matlab   big  data  also  takes  a  lot  longer  to  munge  and  
pre process   and  requires  clever  tricks  to  handle  effectively   

future  work  
while  this  parallelization  technique  appears  to  work  well  on  this  data  set   additional  
investigation  would  be  helpful  in  order  to  establish  a  baseline  across  a  wider  range  
of   data   sets   and   algorithms    in   addition    modeling   content   and   context   features    in  
order   to   get   more   accuracy   on   more   general interest   advertising    would   help   to  
validate  that  this  technique  scales  to  larger  feature  sets   in  addition   providing  this  
functionality  out  of  the  box  in  crunch  would  be  incredibly  useful  to  the  community  
at  large   

references  
          kdd   cup   track        predict   the   click through   rate   of   ads   given   the   query  
and  user  information   http   www kddcup     org c kddcup     track   
   zinkevich    m     weimer    m     smola    a         li    l              parallelized   stochastic  
gradient  descent   advances  in  neural  information  processing  systems            
      
   apache  crunch   http   incubator apache org crunch   
   chambers   c    raniwala   a    perry   f    adams   s    henry   r   r    bradshaw   r       
weizenbaum    n             june     flumejava    easy    efficient   data parallel  
pipelines   in  acm  sigplan  notices   vol        no       pp              acm   
   apache  mahout   http   mahout apache org   
  
  

fi