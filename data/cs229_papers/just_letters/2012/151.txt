cs    project report
automated stock trading using machine learning algorithms
tianxin dai

arpan shah

hongxia zhong

tianxind stanford edu

ashah   stanford edu

hongxia zhong stanford edu

   introduction
the use of algorithms to make trading decisions has
become a prevalent practice in major stock exchanges
of the world  algorithmic trading  sometimes called
high frequency trading  is the use of automated systems to
identify true signals among massive amounts of data that
capture the underlying stock market dynamics  machine
learning has therefore been central to the process of
algorithmic trading because it provides powerful tools to
extract patterns from the seemingly chaotic market trends 
this project  in particular  learns models from bloomberg
stock data to predict stock price changes and aims to make
profit over time 
in this project  we examine two separate algorithms and
methodologies utilized to investigate stock market trends
and then iteratively improve the model to achieve higher
profitability as well as accuracy via the predictions 

   methods
     stock selection
stock ticker data  relating to prices  volumes  quotes are
available to academic institutions through the bloomberg
terminal and stanford has a easily accessible one in its
engineering library 
when collecting stock data for this project we attempted
to have a conservative universe selection to ensure that we
mined a good universe a priori and avoided stocks that were
likely to be outliers to our algorithm to confuse the results 
the criteria we shortlisted by were the following 

according to the listed criteria  we obtained a universe
of    stocks for this project   
the data we focussed on was the price and volume movements for each stock throughout the day on a tick by tick
basis  this data was then further preprocessed to enable interfacing with matlab and integrate into the machine learning algorithms 

     preprocessing
before using the data in the learning algorithms  the following preprocessing steps were taken 
     

discretization

since the tick by tick entries retrieved from bloomberg
happen in non deterministic timestamps  we attempted to
standardize the stock data by discretizing the continuous
time domain  from      am to      pm when the market
closes  specifically  the time domain was separated into
  minute buckets and we discarded all granularities within
each bucket and treated the buckets as the basic units in our
learning algorithms 
     

bucket description

for each   minute bucket  we attempted to extract   identifiers to describe the price and volume change of that minute
heuristically  we discussed the identifier selection with experienced veteran in algorithmic trading industry  footnote 
keith   based on his suggestions  we chose the following  
identifiers to describe the price change 

   price between       dollars

   open price  price at the beginning of each   minute
bucket

   membership in the last     of sp   

   close price  price at the end of each   minute bucket

   average daily volume  adv  in the middle    percentile

   high price  highest price within each   minute bucket

   variety of stock sectors

   low price  lowest price within each   minute bucket
  see

appendix

fisimilarly  we chose open volume  close volume  high
volume and low volume to describe the volume change 
with this set of identifiers  we can formulate the algorithms to predict the change in the closing price of each
  minute bucket given information of the remaining seven
identifiers  volume and price  prior to that minute    the
identifiers help capture the trend of the data of a given
minute 

     metrics
to evaluate the learning algorithms  we simulate a
real time trading process  on one single day  using the
models obtained from each algorithm  again  we discretize
the continuous time domain into   minute buckets  for
each bucket at time t  each model attempts to invest  
share in each stock if it predicts an uptrend in price  i e 
 t 
 t 
pclose   popen   if a model invested in a stock at time t  it
always sells that stock at the end of that minute t   to esti t 
 t 
mate profit  we calculate the price difference pclose popen
to update the rolling profit  if  on the other hand  it predicts
a downtrend it does nothing  this rolling profit  denoted
concisely as just profit in this report  is one of our metrics
in evaluating the algorithms performance 
in addition to profit  we also utilize the standard evaluation metrics  accuracy  precision and recall  to judge the
performance of our models  specifically 
accuracy  
precision  
recall  

  correct predictions
  total predictions
  accurate uptick predictions
  uptick predictions
  accurate uptick predictions
  actual upticks

to conclude  each time we evaluate a specific model or
algorithm  we take the average precision  average recall and
average accuracy and average profit over all    stocks in our
universe  these are the metrics used for performance in this
report 

   models   results
     logistic regression
     

feature optimization and dimensionality constraint

to predict the stock price trends  our goal was to predict
 t 

 t 
  pclose   popen
 
  open

price volume  high price volume  low price volume  end volume

based on the discussion above 
the first model we tried was logistic regression 
initially  we attempted to fit logistic regression with the
following six features     percentage change in open price 
   percentage change in high price     percentage change
in low price     percentage change in open volume    
percentage change in high volume  and    percentage
change in low volume 
note that although change in open variables are between the current and previous   minute bucket  since high
and low variables for the current   minute bucket are unobserved so far  we can only consider the change between the
previous two buckets as an indicator of the trend  formally 
these features can be expressed using the formula below   


 t 
 t   
 t   
popen popen  popen
   


 t   
 t   
 t   
phigh
phigh  phigh
   


 t   
 t   
 t   
plow
plow
 plow
   


 t 
 t   
 t   
vopen vopen  vopen
   


 t   
 t   
 t  
vhigh
vhigh  vhigh
   


 t   
 t   
 t   
vlow
vlow
 vlow
   

the results  however  showed that a logistic regression
model could not be applied well to this set of highdimensional features  intuitively this behavior can be
explained if we consider the significant noise introduced by
the high dimensional features  which makes it difficult to
fit weights for our model  more specifically  this behavior
could be due to certain features obscuring patterns obtained
by other features 
in an attempt to reduce the dimensionality of our feature
space  we use cross validation to eliminate less effective
features  we realized that logistic regression model on
stock data can fit at most two dimensional feature space
with reliability  the results of the cross validation suggested that feature    and feature    provide optimal results 
in addition to optimizing the feature set  we also use
cross validation to obtain an optimal training set  which is
defined as the training duration in our application  figure
  plots the variation of the metrics over training durations
from    minute period to     minute period  the heuristic assumption is training begins at      am  and testing
  our

implementation utilizes the mnrfit library in matlab 
will denote features using
 the numbering ofequations for the rest
 t 
 t   
 t   
of this report  e g  feature     is popen popen  popen  
  we

filasts for    minutes right after training finishes   we observe that logistic regression model achieves maximal performance when training duration is set to    minutes 
figure    performance over different training durations

hence  we train the logistic regression model with feature     and feature      starting from      am to      
am  and the obtained model obtains precision         recall         accuracy         and profit        when testing for the rest of the day 
     

improvements based on time locality

while logistic regression was able to achieve a reasonable performance with the two dimensional feature set
including     and     and made a profit of          we
attempted to further improve our results  based on earlier
discussion  our logistic regression model is constrained
to a low dimensional feature space  as a result  we must
either select more descriptive features in low dimensional
feature space or use a different model that would learn
from a higher dimensional feature space for our application 

new features based on the  minute high low model      
professionals in the algorithmic trading field recommended
the heuristic choice of
      the  minute high low
model tracks the high price  low price  high volume  low
volume across all the ticks in any  minute span  for the
most recent  minute span w r t  any   minute bucket of
 t 
 t 
 t 
 t 
time t  we define p h   p l   v h   v l as follows 
ph

 t 

pl

 t 

vh

 t 

vl

 t 

 
 
 
 

t
t
t
t

max

phigh

 i 

   

min

plow

 i 

   

max

phigh

 i 

   

min

plow

 i 

    

it  
it  
it  
it  

under the  minute high low model  we choose our features to be the following 


 t 
 t   
popen popen
    
 t 
 t 
ph
pl


 t 
 t   
vopen vopen
    
 t 
 t 
vh
vl
specifically  they are the ratio of open price and open
volume change to the most recent   minute high low
spread  respectively 
considering that our stock universe may be different  we
use cross validation to determine the optimal value of  
figure   suggests that     leads to maximal precision
while      guarantees maximal profit and recall  for
the purpose of this project  we chose     because higher
precision leads to a more conservative strategy 
figure    performance over different

we started by constructing more descriptive features 
we hypothesized that the stock market exhibits significant
time locality of price trends based on the fact that it is often
influenced by group decision making and other time bound
events that occur in the marketplace  the signals of these
events are usually visible over a time frame longer than
a minute since in the very short term  these trends are
masked by the inherent volatility of the stock prices in
the market  for example  if the market enters a mode of
general rise with high fluctuation at a certain time  then
large   minute percentage changes in price or volume
become less significant in comparison to the general trend 
we attempted to address these concerns by formulating

  inspired
  keith

by cs                winter  hw   problem   
siilats  a former cs     ta

fialso  we set training duration to    minutes based another cross validation analysis with      our  minute
high low logistic regression model finally achieves precision         recall         accuracy        and profit
       
table    comparison between two logistic regression models
model
baseline
 hl

profit
      
      

precision
      
      

recall
      
      

accuracy
      
      

by compare the performance of the two logistic regression models in table    we clearly see that  minute highlow model provides a superior model than baseline model 
this result validates our hypothesis on the time locality
characteristic of stock data and suggests that time locality
lasts around   minutes 

     support vector machine
as we discussed earlier  further improvement of results
may still be possible by exploring a new machine learning
model  the previous model we explored contained us to a
low dimensional feature space  and to overcome this constraint  we attempted to experiment with svm using    
regularization with c     
     

feature   parameter selection

we tried different combinations of the   features defined by
equation     to      equation       and equation      since
there are a large number of feature combinations to consider  we used forward search to continuously add features
to our existing feature set and choose the best set based on
our   metrics 
table    performance over different feature sets
features
        
          
               
    
               
              
               
               
        

profit
      
      
      

precision
      
      
      

recall
      
      
      

accuracy
      
      
      

      

      

      

      

      

      

      

      

we chose the last feature set since it leads to the highest
precision and also very high profit  recall  and accuracy 
in addition  we set training duration to    minutes using

cross validation  similarly  we choose optimal      and
c       using cross validation  we also compared linear
kernel with gaussian kernel  and linear kernel tends to give
better results 
the svm model trained with the chosen training duration  and c finally achieves precision         recall
        accuracy        and profit         by comparing  minute high low regression model with svm model 
we see that svm model significantly improves recall  by
almost       by only sacrificing a small percentage of precision  around     
     

time locality revisited

recall that the  min high low model is based on our
hypothesis that there exists a minute rolling correlation
in between trades within a certain period of time  and by
cross validation  we choose      for the svm model 
to further substantiate this hypothesis  we conducted an
experiment in which we train an svm using the optimal
parameters from the previous section  and then we evaluate
the accuracy of the model by testing it on different periods
of time 
specifically  the performance statistics of an svm
model  trained from      am to       am  are listed in
table    a close inspection shows that there exists a downtrend in performance as delay between testing period and
training period becomes larger  in fact  it wouldnt be surprising to see even better performance of this model within
   minutes after training completes as we chose        
table    performance over periods of time
period
           
am
           
am
           
am
           
am
           
am

profit
      

precision
      

recall
      

accuracy
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

   conclusion and furtherwork
predicting stock market trends using machine learning
algorithms is a challenging task due to the trends being
  the result is precision          recall          accuracy         
which tops all other results in table   

fimasked by various factors such as noise and volatility  in
addition  the market operates in various local modes that
change from time to time making it necessary to capture
those changes in order to be profitable while trading 
although our algorithms and models were simplified  we
were able to meet our expectation of reaching modest profitability  as per our sequential analysis it became clear that
factoring in time locality and capturing the features after
smoothing  to reduce volatility improves profitability and
precision substantially 
factoring in features of high dimensionality after careful
selection can also be significant to improving the results and
our analysis of the svm compared to logistic regression
was able to capture this  we expect that this is the case
because of higher dimensionality increasing the likelihood
of linear separation of the dataset 
finally  iterative improvements achieved through sequential optimizations in the form of discretization  realization of time locality  smoothing improved results significantly  cross validation and forward search were also powerful tools in making the algorithm perform better 
in conclusion  our experience in this project suggests
that machine learning has great potential in this field and
we hope to continue working on this project further to explore more nuances in improving performance via better algorithms as well as optimizations 
a few interesting questions that we think would be worth
investigating would be exploring other international stock
markets to find locations where algorithmic trading is able
to perform better  in addition  it would be interesting to
investigate other algorithms such as reinforcement learning
to compare with the models discussed in this report  feature
selection has been key and more work in discovering more
descriptive features would prove to be promising in terms
of making the results even better 

   acknowledgements
we would like to thank professor andrew ng and the
tas of the class for their feedback and input on the project 
we would also like to thank keith sillats for generous help
in the form of advice as well as valuable personal experience in the field that helped inform our decisions 

references
    jure leskovec  ta  keith sillats hw  

a  appendix
stock ticker
apol
cbg us equity
cma
cms
cvs
gci
gme
gt
jbl
kim
lnc
nfx
ni
nwl
nyx
pwr
qep
see
ter
thc
tie
txt
zion

origin
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity
us equity

fi