singer recognition and modeling singer error
johan ismael
stanford university

nicholas mcgee
stanford university

jismael stanford edu

ndmcgee stanford edu

   abstract
we propose a system for recognizing a singer based on
past observations of the singers voice qualities and their
ability to sing a given song  such a system could be useful to improve predictions in query by humming systems 
or as biometric identity data in consumer applications  as
well describe in more details in the following sections 
we managed to obtain a fairly high accuracy on individual voice recognition by training a multi class svm using
mel frequency cepstral coefficients  mfccs  as our main
features  we then tried to improve our accuracy by using insight from qbh systems  that is  characterizing singers also
on how well they can perform specific notes  in the process we developed a note segmentation system which shows
promising results using an unsupervised learning technique 
finally  we made the problem harder by attempting to recognize singers with background music 

particularly sensitive to this step because with a few notes
wrongly recognized  the whole comparison could be misaligned and therefore unusable  in addition  thanks to further research we realized that the most successful qbh systems     today are not trying to directly match hums with
ground truth songs  they are instead matching hums with
labeled hums  which seems to make a lot of sense since a
hum and a ground truth are so far apart in terms of spectrum
that its almost like comparing carrots and tomatoes  and
here is our second roadblock  it seems necessary to have a
fairly big database of labeled hums  after unsuccessfully
asking the interactive audio lab     to use their data  we finally decided to solve a different but related problem since
our qbh system seemed to be built on too many weak links 
we realized that with our dataset  we could solve another
problem  singer recognition  would it be possible to recognize someone singing a song  after training on them singing
other songs  how accurate can we be  would this be possible with famous singers supported by instruments 

   introduction
our original idea was to implement a query by humming  qbh  system which goal would be to recognize the
songs people are humming  using a dataset of  wav files of
people humming songs corresponding to some ground truth
monophonic midi files     ideally  we would try to match
hums with mp  files instead of midi but we quickly realized
that the melody extraction tools required to work with real
music simply do not exist yet 
to implement this system  our first goal was to extract
features  most precisely pitch and rythmic intervals  pis and
lirs   as suggested by b  pardo et al s paper     the idea
was to learn  for each person  what we call the singers error  knowing this error we could  from a hum  reconstruct
the ground truth and then try to find a best match with our
database of midi files 
as we started to implement our first feature extractor 
we ran into the first roadblock  how to segment the original hum into different notes  which is required to perform a
note by note comparison to the ground truth and build our
model  we tried to use two tools freely available       
but those did not give very good results  our system is

   early prototypes
     first qbh prototype
for our initial  exploratory prototype  we built a quick
system in python which used the vamp melodia plugin      
to process input and target songs  and calculated the edit
distance between the input and each target song  for determining the cost of mismatches during edit distance calculation  we used a cost function which deemphasized common
errors such as octave and half step misses note misses  but
without a more complex singer error model  on a set of
   target songs and   user inputs  this prototype achieved  
correct match  for an unimpressive       accuracy  at this
point we began to realize that there were many more unsolved problems in processing the input for the system than
in the learning algorithm itself 

     eigensongs
eigenfaces uses pca to reduce the feature space of an
image by calculating a set of eigenvectors which capture
the systematic variances in a set of facial images  essen 

fitially  determining the most important patterns in the image       because songs can similarly be interpreted as a
linear feature vector of amplitudes  we wanted to see if the
patterns in a song could be identified by performing pca
on several aligned recording of the same song  we ran into
two roadblocks which caused us to abandon the idea  first 
we would need a very large set of samples per song to do
the initial pca  and this was not practical given our dataset 
second  and more importantly  it is very hard to evaluate
whether pca is truly extracting useful information about
the song  or simply extracting noise  our initial prototype
performed poorly  leading us to believe the technique was
probably not reliable enough to develop further 
given these unsuccessful attempts and what we learned
from them  we decided to reformulate our problem and focus on singer recognition 

figure    noticeable differences appear in the mfccs of different
singers

   singer recognition  description and results
     overview of the problem solved
our final system performs matching between unique individuals and audio clips of their voices  given a recording of a person singing  who is known to have appeared in
our training set   the system identifies which of the training
singers is singing the input song  this recognition is useful
for training and applying an individually tuned singer error
model for qbh  as well as for general voice fingerprinting 

     dataset
we drew    singers from the mir qbsh database     
each of which had sung between    and    songs from a
pool of    possible songs  we used    of these songs as
training data to identify each singer  and tested our classifier
using the remaining data 

     features and training
our algorithm trains a multi class svm  using liblinear     based on the feature set extracted from each song
in our training data  we tried several different feature sets
during evaluation  but found that the most effective feature
set was a simple one  figure   shows the first    mfccs
for two different singers side by side  as is apparent in the
figure  the cepstral breakdown of different singers voices
have perceivably different fingerprints  we found that the
first    mfccs provide the most useful information  with
the addition of more coefficients providing little benefit after that  we discuss other possible good features in section
    

figure    accuracy of our system using various feature sets

in figure    on small numbers of unique singers  we were
able to identify around     of test recordings as the correct
singer  as the singer space increases  accuracy naturally
decreases  some singers have very similar voices  so matching voice characteristics alone is not sufficient to distinguish
them   but this system could be useful in many cases  for
example  any device with multiple user accounts could use
audio input to detect users and log into the correct account
with a high degree of accuracy 
with these satisfying results  we decided to move forward and experiment in two directions  first  what features could we add to improve our accuracy  second 
how would our system perform on harder problems such
as singer recognition with background music 

     results

   first experiment  improving accuracy

our results using this model are surprisingly good  considering the small size of the feature set  as can be seen

our first experiment aimed at improving the accuracy of
our system  to do so  the idea has been to add new features

fifigure    accuracy of our system in different singer spaces

in our model that are able to capture something that is characteristic of a given singer and is not captured by mfccs 

figure    variances in note accuracy for different singers

     using a singer error model
     

the idea

mfccs are great at capturing the essence of someones
voice  whether they are singing or even just talking  so if
we could add a feature that leverages an attribute specific to
singing  it would probably be something that is not captured
by mfccs  moreover  as we were reading about qbh systems earlier in our project  we were excited about one approach  modeling singer error  but this time  we wouldnt
model singer error in order to reconstruct the ground truth
of a song  but to characterize a particular singer  so how
can we do that 
we can see this error as some gaussian noise  so well
represent it by a normal distribution  centered on the ground
truth note  and well consider that the variance represents
this error  because we all have difficulties singing different
notes  it seems relevant to have one gaussian per type of
note  indeed  maybe singer a is really good a singing the
middle c  but is terrible when it comes to high as  since
the original files that well do the comparison with are using
the midi format  well use midi notations to represent notes
everywhere  they range from    to      lets formalize
these ideas 
xi  n  i  i    i           
where the xi s represent the note sung by a singer when
trying to render the actual note numbered i 
to estimate these variances  we use the maximum likelihood estimate on our training set  for each singer and each
note  we gather all the instances of what that singer actually
sang when trying to sing the given note  the variance is
then given by 
m
  x
 xi  i  
m i  
where the xi s represent the notes sung by our singers 

at the end of this process  we have a vector of variances
per singer  the big issue is that to perform this estimation 
we need to be able to segment the hums into separate notes 
as stated in the introduction  note segmentation tools are
not quite perfect  however  to get an idea of how discriminative our model was  we manually performed the note segmentation for two singers  five songs each  after this  long 
process  we obtained the results shown in figure  
though it is hard to draw solid conclusions out of two
singers but it seems the two we used would be fairly separable with this characterization 
one observation with this model is that we are sensitive
to a change of octave  if a singer transcribes perfectly a song
one octave down  well understand this as a pretty large error  but this is what we want  if youre always singing a 
instead of a   we want to capture it even though the rendition might be nice to listen to 
     

plugging into our current algorithm

our learning algorithm uses mfccs classifies singer with
one data point per singer song pair whereas the model weve
just mentioned outputs one data point per singer  to solve
this problem we could calculate the maximum likelihood
estimate for our gaussians once per song  but that means
we would probably have very few samples per note for each
data point  to alleviate  we could use binning and divide
our range of notes into three separate bins  low range  midrange and high range  we would then end up three additional features for each data point  the error on the low range
notes  the error on the mid range notes and the error on the
high range notes 
given the issues we had with note segmentation  we
havent had the opportunity to test the accuracy of our system with these additional features  however  we tried to
improve note segmentation tools with unsupervised learning techniques 

fifigure    our ideal singer recognition system

figure    k means clusters denoting the notes sung by the singer

figure    a comparison of chromatic composition

for labelling large datasets where the target song is known
for each sample to be labelled  while it is often of primary
concern in clustering algorithms to avoid locally maximal
clusters  in our case we want the algorithm to settle into a
local maximum given by the closest fit to the actual  target
song  to achieve this  we seed the algorithm by choosing k
to be the number of notes in the target song  and setting the
means at the center of each target note  notes which do not
occur in the users input naturally drop out because they do
not become associated with any sample points  our algorithm uses  time  f requency  tuples as coordinates for the
clustering algorithm  and the results on a sample song are
shown in figure    results are very good for songs where
the ground truth song approximately matches the input in
tempo and key  but not when the two are misaligned  to
solve key alignment  we simply align the medians of the  
inputs  choosing the optimal tempo alignment is a much
more complicated problem which we believe leaves room
for future work 
another problem we observed which causes clustering
to go awry is that clusters are often too close together  and
there are not enough samples to differentiate them  this can
be addressed by extracting more points to cluster on  but
how  figure   shows the evolution of a chromagram over
time for the ground truth song  as well as the noisier input
from a user  notice that despite the noise  dominant frequencies remain roughly consistent with the ground truth
song  including non dominant chroma as points and discounting their distance based on intensity would provide
more sample points and hopefully improve the clustering
algorithm 

     segmenting notes for use in the model
in developing the singer error model  one of the largest
problems we ran into is that available audio tools cannot accurately identify notes being sung or hummed  thus making
it difficult to train based on singer error  while we performed some manual labelling  this is not practical for large
datasets  k means clustering provides a practical method

     the ideal system
an effective note segmentation system would allow us
to automate the singer error calculation  figure   illustrates
what our ideal system would look like 

ficited to use unsupervised learning for note segmentation  as
opposed to pure signal processing approaches  getting this
problem solved would hopefully improve our singer recognition system  and even make qbh more addressable 

references

figure    performance of our system recognizing famous singers

   second experiment  tackling more difficult
problems
in addition to attempting to improve the accuracy of our
system with more features  we were curious to see how well
our svm would perform on harder problems  so we decided to see what would happen if  after training our svm
on two famous singers talking in interviews  we tested on
actual songs which featured both singing and instruments
in the background  we picked thom yorke  singer of the
british rock band radiohead  and barry manilow  a famous
american singer  for our experiment  we gathered interviews of them on youtube  for test data  we picked some of
their songs 
we obtained the results shown in figure    our first observation is that mfccs dont work anymore where there
is noise in the background  it would probably be helpful to
separate voice from background instruments  but thats another hard problem  interestingly  it seems that it is possible
to train on spoken words as in interviews and obtain a decent accuracy on acapella singing  the accuracy reaching
     on acapella singing has one caveat  we are training
on different chunks of the same recording that we are using
for testing so the results are probably good because of an
overfitting problem 

   conclusions and next steps
we end this project with a better understanding of what
machine learning can do for music today  on the one hand 
spectrum analysis makes available very strong features like
mfccs that are very useful for identifying specific audio
signals  on the other hand  when it comes to actually understanding the content  current solutions are ineffective or inconsistent  this makes it very hard to automatically extract
features such as melody  or even notes  for use with learning
algorithms  the music information retrieval  mir  community has been very active and working on these different
problems  hopefully  challenges such as source separation 
note segmentation and melody extraction will be overcome
in the near future and enable a lot of very exciting musicrelated applications 
when it comes to future work  we are particularly ex 

    the interactive audio lab  http   music cs northwestern edu  
    mirtoolbox 
https   www jyu fi hum laitokset musiikki 
en research coe materials mirtoolbox 
    liblinear  a library for large linear classification
http   www csie ntu edu tw  cjlin liblinear 
    the vamp audio analysis plugin system 
http   vampplugins org  
    sonic annotator  http   omras  org sonicannotator 
    b  p  arefin huq  mark cartwright 
crowdsourcing
a
real world
on line
humming
system 
sound and music computing 
     
http   smcnetwork org files proceedings         pdf 
    d  l  et al  a query by humming system that learns from
experience       
    l  myers  an exploration of voice biometrics       
    j  s  r  jang  mir qbsh corpus  available at the mir qbsh
corpus link at http   www cs nthu edu tw  jang 
     j  salamon  melodia   melody extraction vamp plug in 
http   mtg upf edu technologies melodia 
     j  salamon et al 
tonal representations for music retrieval 
from version identication to queryby humming
http   www justinsalamon com uploads 
                salamon mmir pdf
     p  b  et al  eigenfaces vs  fisherfaces  recognition using
class specific linear projection       
     r  kline et al 
approximate matching algorithms
for music information retrieval using vocal input
http   lyle smu edu  mhd     sp   kline pdf

fi