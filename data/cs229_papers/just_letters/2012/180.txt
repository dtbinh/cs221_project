learning  d human models
cs    final project report
fuad al amin

david shuang liu

systems engineer  intel
stanford university  electrical engineering masters

stanford university
electrical engineering masters

katherine chen

yoo hsiu yeh

stanford university
computer science masters

stanford university
electrical engineering masters

abstractwe attempt a variety of machine learning
algorithms to learn the human body shape  the objective is to
accurately reconstruct  d point clouds of bodies based on a
number of  d images  within  cm tolerance   generated from    
standing pose body scans from the civilian american and
european surface anthropometry resource  caesar      our
naive bayes with point neighbor voting filter preprocessing
algorithm gave the best result  selecting      points from      
potential points with mse      cm and precision        in
addition  categorical linear regression was used to predict body
dimensions based on personal characteristics  finally  svms and
random conditional fields  rcf  were applied to the dataset
with various parameters to distinguish accurate points from
inaccurate points as generated by the computer vision algorithm 
keywords d reconstruction 
clouds filtering  key words 

i 

human

modeling 

point

introduction

in online apparel shopping  the buyer cannot try on the
clothing and thus finds it difficult to determine the fit and look
of an item on their own body  our objective is to generate an
accurate  d model from a sequence of images or video stills 
captured by a mobile device  of a person rotating     degrees
 or equivalently  a camera rotating   a computer vision
algorithm uses those images to create a noisy  d point cloud 
to further refine and generate a realistic and accurate  d
model  we apply machine learning algorithms  learned from a
dataset of real human body models with metadata including
age  weight  gender  and ethnicity  this report details our
progress towards that goal  using images taken by a virtual
camera of rendered  d mesh scans of real people 
ii 

in recent developments  researchers have tried to use new
approaches like pde based mesh optimization and local
subdivision based mesh refinement for efficient and accurate
surface distortion approximation      most of the multi view
reconstruction algorithms focus on minimizing reconstruction
distortion  i e  reconstructing a  d model as close to the real
object as possible  in all of these research projects  an
algorithm pipeline works together to provide the final result 
data
acquisition 
image
preprocessing 
geometric
reconstruction and model construction     
iii 

the computer vision algorithm

in the computer vision portion of this project  we
developed a robust way to reconstruct the  d model from  d
images based on stereo vision  first  the algorithm utilized
surf and orb feature descriptors to establish
correspondence between  d projective images of known
camera positions  the epi polar geometry gave the location of
the feature point in  d space for each correspondence pairs 
upon achieving good results with the virtual models in
opengl  we applied the same reconstruction technique to real
images taken in a noisy environment  after camera calibration
and illumination adjustments  the computer vision algorithm
generated fairly accurate results  plus noticeable noise  even
with background clutter  imprecise body poses and movement 

related work

work on  d reconstruction from multiple  d images can
be categorized into two main categories  small baseline
reconstruction and wide baseline reconstruction  in small
baseline reconstruction  the cameras are relatively close to each
other  therefore stereo information is available  in contrast  in
wide baseline reconstruction  the stereo information is not
available because of the large separation between the cameras 
the flip side of small baseline reconstruction is that it requires
many camera shots while wide baseline can be accomplished
with few camera shots  in this work  we only consider small
baseline reconstruction 

figure     d images from       degrees  left  middle   computer
vision  d point clouds output after basic filtering  right 

fiiv 

data collection

a number of specialized  d body scan meshes were used
in this project      standing pose body scans of an assortment
of volunteers were acquired from the civilian american and
european surface anthropometry resource  caesar      
each scan was accompanied by detailed metadata of
corresponding standard anthropometric measurements and
demographic information such as weight  gender and race 
fifteen scans were done in person at the stanford biomotion
laboratory  one scan came from the shape completion and
animation of people  scape  dataset  which was also taken in
the stanford biomotion laboratory  and thus was the same
format 
the caesar meshes contained around                  vertices each  while the biomotion lab meshes were
each around                   vertices after merging  both
sets of meshes were given in stanford  ply file format 
v 

data preprocessing

to work with memory constraints in matlab  all meshes
were subsampled by about   x using the free mesh processing
meshlab software merge vertices filter  meshlab was also
used to batch remove duplicate faces and unreferenced vertices
before processing  and convert the filtered  ply files into
wavefront  obj files 
the number of estimated points generated by the computer
vision algorithm was dependent on the number of
correspondence features  in particular  corners are detected as
features  the same checkerboard pattern image was mapped
onto each model to control the number
of points generated by the cv
algorithm  by parsing and scaling the
 obj file texture vertex entries to match
the scaled x and z vertex information in
matlab  the computer vision algorithm
was able to generate             
estimated cloud points for each mapped
model  the image on the right
illustrates a sample of the body
rendered in opengl 
the height and distance of the
virtual camera from the model were set
to be   and    respectively  the scaling
from true distance to distance in the cv
algorithm was     cm   rendered unit 
the average height of the standing
models was       cm 
vi 

figure   preprocessed
human model in
opengl

technical appraoch and results

in order to obtain the best results  numerous machine
learning algorithms were attempted to investigate which one
would produce the best result  the evaluation metric for all of
the algorithms were average euclidean distance between the
 d point clouds and the true models surface  following sub
sections describe each approach and the observed result 

a  categorical regression
the caesar data set we obtained contained metadata for
     models  including gender  race  age  occupation 
education  fitness  and    anthropometric measurements  we
hoped to use insights from this data to improve our pruned
point cloud models  we allocated      data entries for our
training set and     for our test set  the predictors  regressors 
were gender  race  age  weight  and height  we then used
categorical multi linear regression to generate models to
predict the    anthropometric measurements  exhaustively
generating akaike s information criterion  aic       bayesian
information criterion  bic   mean square error  mse  through
cross validation with the     entries  and mean error  for all
repressor combinations  we selected the model with lowest
aic  bic  and mse as the top performing model for each y 
prioritizing smallest model when the model with the lowest
aic  bic  and mse differed  this is because bic tends to
penalize more on the model size than aic  see table   below
for evaluation metrics of our top performing models 
table i 

ten best anthropometric model

y  mm 

aic

bic

mse

knee height  sitting

     

     

      

vertical trunk circum 

     

     

      

sitting height

     

     

      

waist height  preferred

     

     

      

buttock knee length

     

     

      

head circumference

     

     

      

eye height  sitting

     

     

      

crotch height

     

     

      

arm length

     

     

      

foot length

     

     

      

b  point neighbor voting filter
the raw output from the computer vision algorithm
contained many scattered outliers  see figure below   note that
the points are concentrated about true model surface  this
observation gave inspiration to the pnv filter algorithm  in this
algorithm  each point votes for all neighboring points within a
threshold euclidean norm    points with votes less than a
threshold    are eliminated  the filter was run over ranges of  
and    and the optimal results were obtained at          and
        
the original pnv filter took more than    minutes to run
on a single body with        points given its       
complexity  therefore  instead of calculating the exact norm
between all points  all of the points were blocked into cubes
with side length equal to   this blocking method resulted in an
approximately   x speedup in the filter run time  the opengl
rendering below displays the difference between the original
and filtered point clouds 

fifigure   original point clouds  left   after pnv filtered point
clouds  right 

to   cm  even though our target is to filter out any points with
expected error greater than  cm 
note that our first task on the training set was to find the
closest point on the true surface corresponding to each point
from the cv model  this gave us a measure of error
           by which we could compute the above probability 
after computing the probabilities  we experimented with
various allowable probability tolerances by which to accept
points  we quickly settled on requiring a probability of     to
accept a point as larger values resulted in very few selected
points and small values reduced the accuracy of the training
set  as any point selected is     likely to be outside our
acceptable tolerance   thereafter  we wished to evaluate the
effects of various divisions of   and   on the performance of
the classifier on our test set of    samples  which provides the
results as shown below  note the test set on average has      
starting points  of which     are within  cm of the true model
 mean mse of      cm  
naive bayes performance for various angular divisions
   

c  modified navie bayes
the more complex algorithms provided insufficient
performance  detailed below  so we turned to the
fundamentals  the nave bayes classifier  given the large
training set      models with   k  points each   we hoped to
use simple frequency analysis to compute the probability of
any point being accurate to some tolerance  to parameterize
the model  it is clear that the two cameras  variables    and
     involved in generating a computer vision point  out of a
total of    cameras  are the most significant factors  each
camera pair is expected to have advantages and disadvantages
when computing points in various parts of its visible field  for
example  perhaps cameras can see better at the center of their
range  at the edges objects  or some other combination 
the other parameter of interest is the location on the body
corresponding to the generated point  this was represented by
a    angle in     planes  and    angle with respect to   axis  
rather than the raw coordinates because the surface location of
the point is expected to be more relevant than the specific
        box it belongs in  i e  angles are more relevant to what a
camera sees   furthermore  we needed to discretize the   and
  into segments to facilitate calculation of probabilities 
given the above parameters  we wished to compute the
following probabilities 
            
                                                 
                                                                     
 
                                                     

the above probability is the chance that the error of a
point will be less than   for the given camera configuration
and the location of the point  we computed these probabilities
for angular divisions of                                         
                 and    degrees as an experiment  similarly 
we computed probabilities for all integers   in the range of  

   
   
   
 
   
   
   
   
 

 

 

  
  
  
  
  
  
angular discretization increment  degrees 

  samples kept

mse

precision

true neg

  

  
  runtime  

in order to maximize the number of samples  reduce mse 
and increase the true positive rate     degree divisions are
found to be a reasonable balance  this provides an average of
    points with mse      and true positive rate       which
is notable in that it is higher than the     confidence cutoff
we demand  true negative         the runtime is also a
manageable    s for all    test samples  vs      for   degrees 
to which runtime data in the graph above is normalized  
note  we are interested primarily in reducing mse 
increasing the number of points  so as to get better coverage  
and increasing the true positive rate  i e  the proportion of
points we choose to keep that are actually within our  cm
tolerance   we are not particularly interested in increasing true
negatives  or equivalently decreasing false negatives  except
in as much as it affects the earlier factors  this is because our
algorithm chooses points based on confidence probability  it
is not possible for it to know every good point in an unknown
test set so it should only select the ones it has highest
confidence in  probability        the following precisionrecall curve summarizes this tradeoff and shows we choose a
confidence level of     to maximize precision in favor of
recall 

fi       of the original data set of which     are within
tolerance  with mse      cm and true positive rate of       
to determine whether our training set size of     samples
was sufficient  we test the classifier on the training set with the
result being very close to the test sample mentioned above
indicating we have low variance and a good sample size  the
results for the training set  tr  and test set  ts  

precision recall curve
    
   
    

precision

   
    
   

table ii 

nave bayes  low variance  test set adequate

    
   
    

 

   

   

   

   

   
recall

pass  

   

   

pass  

   

   

 

pass  

   modification    dynamic clustering
to improve performance further  we decided to take a
second pass through the remaining rejected points  this time 
we chose to accept those points which had a     probability
of being within  cm of the true surface  but only if the points
distance to any of the points in the already chosen data set is
less than                             these numbers are
based on empirical experimentation on a small subset of test
samples then generalized to the full test set 
we similarly implement a third pass  this time only
accepting those points with more than     probability of
being within  cm of the true image  but only if the point is
within                         of the true surface 
the primary objective of these steps was to increase the
number of high confidence points without significantly
reducing the accuracy  the results are summarized below for
our test set with    degree increments on the probabilities 

samples    kept 
precision    
mse  cm 
true neg     

pass  
tr
ts
    
    
    
    
    
    
    
    

pass  
tr
ts
    
    
    
    
    
    
    
    

pass  
tr
ts
    
    
    
    
    
    
    
    

   modification    point neighbor filtering
in order to further improve performance  we used the
point neighbor filter algorithm method to pre filter the input
into our nave bayes classifier  which is still trained on the
full data set   the pre filtering reduces the data set to      
points on average with mean mse      cm and       of the
points already being within tolerance  the graph below shows
how the nave bayes classifier further refines this 
naive bayes with point neighbor prefiltering
 
   
   
   
   
   
   
   
   
   

naive bayes with multiple pass modifications
   

 

 

   

   

   

   

  samples kept

   

 
pass  
mse

   

   
precision

   

   

 

true neg

 

as a result  we select  on average       points with mse
     cm and precision       

   
   
   
   
 

 

   

   

   

  samples kept

   

 
pass  
mse

   

   
precision

   

   

 

true neg

the iterative process dynamic clustering process shows
decreasing returns so we leave it at   passes  this also limits
computational complexity   which results in      points

d  conditional random fields  crf        
crf was used to classify points that are within  cm of true
model from points that are outside the bound  in our problem 
the computer vision generates  d point from fixed known
camera pairs  therefore  we can group all the points from same
camera pairs as a probabilistic graphical model  effectively  we
are assuming that each camera pair set has a distinctive
likelihood to produce accurate points at different region  and
thus we can use crf to generate this likelihood  we chose
crf for our problem because hidden markov models
 hmms  require strict independence assumptions on the
observations in order to obtain computational tractability and

fimaximum entropy markov models  memms  have the label
bias problem  the transitions leaving a given state compete only
against each other  rather than against all other transitions in
the model      using the hidden state conditional random
field library     with crf and latent dynamic crf  ldcrf   training was conducted with training size between    to
    bodies  however  the testing results for true positive and
true negative were both       which is unacceptable  ld crf
always performed better than crf  but ld crf took    x
times longer to compute  furthermore  the results did not
improve with a larger training set  one reason for the poor
performance is because the generated  d points sequence had
no structured ordering  thus  crf did not provide promising
outcome and no further investigation was done 
e  principal component analysis  pca 
fifteen different  d models of the same person were
generated by running the computer vision algorithm initialized
at   degree apart initial angles  the point cloud sets were
randomly down sampled to the size of the minimum set  then
the           vectors for each were concatenated into a vector 
and the vectors were concatenated into a matrix  no
discernible human like point cloud emerged from any of the
principal components  the main reason is pca requires all of
the  d models to be aligned and scaled exactly in order to
generate eigen bodies as the principal components 
f  support vector machine  svm 
a variety of svms within the matlab toolbox were used to
train on the           features of the cloud           points for
the camera pair       on a subset of      bodies  points were
labeled    if the cloud point error was         and   
otherwise  number of features  number of data points  svm
kernel  and c bounding box constraint were varied  however 
most of the svms tested did not converge  and only svm with
mlp kernel converge with almost     classification error rate 
vii  conclusion
in conclusion  naive bayes with pnv filter provided the
best results for the     caesar datasets  selecting on average
     points        of the original data set  with mse      cm
and precision        this is a very good result  there are
enough high quality points for important features to be
extracted such as measurements  shapes  and ratios  in fact  the
high confidence data can be used to filter  divide  or otherwise
provide reference to the entire data set  allowing us to have
enough prior knowledge to conduct further regression and
pattern fitting on even the noisy data  knowledge of high
confidence points can provide a reference mean for the noisy
data  from which other statistics can be computed  
our method of registering the cloud points to the true
points was based on an exhaustive search of minimum
distance  a variety of more sophisticated methods for
registering point clouds to surfaces exist         which may
have improved the accuracy of some of the algorithms tested 
additional work could include combining the categorical
regression body size model prediction with the naive bayes
with pnv filter algorithm to further enhance the  d
reconstruction accuracy 

viii  acknowledgements
special thanks to gregory zehner  senior physical
anthropologist at wright patterson afb  and jessica l  asay 
research engineer at the stanford biomotion laboratory 
greg was amazingly generous in his time and willingness to
help us get access to the models in the caesar dataset that
we needed  jessica spent a lot of her time helping us sort
through paperwork  and then graciously spent an early morning
with us helping us collect new data and explaining how the  d
scanner worked  we would also like to thank andrew maas 
head ta for cs     for suggestions on directions we could
take our data  and dr  dragomir anguelov  principal
investigator in the scape project  for suggesting different
avenues for acquiring needed data 
references
    caesar anthropomorphic database 
http   store sae org caesar 
    akaike  hirotugu          a new look at the statistical
model identification   ieee transactions on automatic
control                 doi         tac              
mr        
    morency  louis philippe  hidden state conditional
random fields library version    a  january   th       
http   sourceforge net projects hcrf 
    lafferty  j   mccallum  a   pereira  f  conditional random
fields  probabilistic models for segmenting and labeling
sequence data  in  proc    th international conf  on
machine learning  morgan kaufmann  san francisco  ca
              
    l  p  morency  a  quattoni and t  darrell  latentdynamic discriminative models for continuous gesture
recognition  proceedings ieee conference on computer
vision and pattern recognition  june      
    anguelov  d   et al  scape  shape completion and
animation of people  acm transactions on graphics
 tog    proceedings of acm siggraph       volume
   issue    july       pages         
    allen  b   curless  b   and popovic  z  the space of human
body shapes  reconstruction and parameterization from
range scans  acm transactions on graphics  tog  proceedings of acm siggraph       volume    issue
   july       pages         
    viet nam nghiem  jianfei cai  jianmin zheng     ratedistortion optimized progressive  d reconstruction from
multi view images   computer graphics and applications
 pg          th pacific conference on   vol   no   pp             sept      
    gao  p   xiaozhong guo  datao yang  d y   ming shen 
m s   zhao  y   xu  z   zhongwei fan  jin yu  yunfeng
ma      d model reconstruction from video recorded with a
compact camera   fuzzy systems and knowledge
discovery  fskd         th international conference on  
vol   no   pp                  may     

fi