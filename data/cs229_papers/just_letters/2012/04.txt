classifying chess positions
christopher de sa
december         

chess was one of the first problems studied by the ai community  while currently  chessplaying programs perform very well using primarily search based algorithms to decide the best
move to make  in this project i apply machine learning algorithms to this problem  specifically 
instead of choosing which move is the best to make  i want to produce an function that attempts
to determine the probability that a player is likely to win in a given chess position  note that
while searching chess engines also produce a score factor for each position  this score represents
the engines own belief  in a bayesian sense  that it will win the game given the position  whereas
our goal is to classify the actual probability of a win given human players 
the main possible application of such a classifying function would be as a heuristic in an a like search based chess engine  additionally  the structure of the classifier could shed insights
on the nature of the game as a whole 
i have acquired training examples from actual games played by humans  i decided to use
the fics games database  which contains over     million games played over the internet over
a period of years  this dataset consists of games in pgn  portable game notation  format 
which encodes the game as a whole rather than as a sequence of positions  since the goal of
this project is to classify positions  i needed to convert these pgn games to position sequences 
and used a python script to do so  this presented a technical challenge due to the fact that a
sequence of positions is several orders of magnitude larger  in terms of memory consumption 
than the pgn encoded games  by using specialized solvers  such as the stochastic subgradient
method  i was able to avoid storing all the positions in memory at once 
since these positions are played by humans  and humans have a wide distribution of skill levels
and play styles  any results from these data will depend on how the data are filtered  for this
project  i am only pre filtering these data by excluding     fast games in which the amount of
time remaining for each player would be a spoiler factor for the classier      games in which either
player forfeited on time      games in which either player forfeited due to network disconnection 
    extremely short games  and     games resulting in a draw  this last exclusion is done in
order to use a binary classifier for this problem  however  my approach could be extended to
include drawn games 
formally  we can express this problem as a ml problem as follows  our content x i  is a legal
chess position from a game played by humans  and our annotation y  i  is the outcome of that
game  a win or loss by the player  to move   we are trying to predict the expected value of
the outcome of the game given the position  note here that  due to the fact that humans are
playing these games  the result of the game is not a mathematical function of the board state 
furthermore  the nature of chess is such that the vast majority of positions encountered by
the algorithm will not necessarily favor either color  so they will both not be useful as training
examples for the classifier  and also raise the error rate when the classifier is tested  because
of these factors  it will be impossible for any classifier for this problem to produce a near zero
error rate over this dataset 
the approach will necessitate representing the board state as some multidimensional vector
of features  after investigating several possible feature sets  i settled on representing the board

 

fistate as a sparse vector  where each entry represents the presence of a particular piece on a
particular square 
lets look in more depth at this representation  a chess position consists of    squares  each
of which may contain a piece  there are   pieces  the pawn  the knight  the bishop  the rook 
the queen  and the king  additionally  each piece is one of two colors  white or black  this
creates a total of    distinct pieces  we can therefore  almost  fully represent a board state as
a partial function in 
f  s                                             p  n  b  r  q  k    white  black  
if this partial function describes our game state  it follows  from the definition of a partial
function  that we can equivalently describe the game state as a subset of 
                                          p  n  b  r  q  k    white  black  
since this set has magnitude                   it follows that we can represent any subset of
it as a vector in r      where the i th entry is either   or   to represent the presence or absense
of the i th element of this set in the subset 
there are some caveats with this representation  the partial function does not include certain
non position based aspects on the game state  namely  whose turn it is  whether castling is still
possible  and whether en passant is possible  i have assumed that the latter two things are
insignificant enough that they can be safely omitted as features  the former is more difficult to
deal with  but it can be resolved by considering the symmetry of the chess board  because of
this symmetry  if we reverse      the board      the colors of all the pieces  and     the result 
our classifier should produce the same result  therefore  without loss of generality  we can limit
ourselves to only positions in which white is to move  and for these positions  this representation
works well   for all the analysis below  we will use white to refer to the player who is to move
in the position  and black to refer to the player who is not to move  
we assume that  at any given position  the probability that a human player to move will win
the game is a function of the position  this is a reasonable assumption based on the structure
of the chess game  if we further constrain this function to be a logistic function of the above
board representation with some parameter   then the maximum likelihood estimate of this
parameter corresponds to logistic regression  therefore  i trained a logistic classifier using this
representation   i also tried a few other classifiers  such as a linear classifier and a support
vector machine  however  these classifiers did not perform well on the given dataset  this is to
be expected since the basic properties that we want to have for applying these methods do not
hold for this dataset  
i ran logistic regression over a dataset of about          training samples and          test
samples  my algorithm was written in python using numpy and scipy  and computed the
regression using the newton raphson method  it dealt with the relatively large amount of data
both by using sparse matrices whenever possible  in particular  the training matrix x  whose
columns are the sparse position vectors  was represented with a sparse matrix   and by avoiding
the hessian matrix inverse step by instead using the conjugate gradient method on the linear
system hs   g  where s is the computed step  g is the logistic gradient  and h is the logistic
hessian   this solver  figure    converged in about   iterations for most of the data sets tested 
the resulting classifier had 

train           

test           

the similarity between these numbers suggests that the model is not overfitting the data 
while these error rates are not great  the fact that they are significantly lower than     for a

 

fi 

 

 

 

 

d e f t r a i n l o g i s t i c r e g r e s s i o n  x  y    
t h e t a   numpy   a s m a t r i x          x  shape           t  i n i t i a l i z e t h e t a
f o r i in range   itermax    
l g r a d   l o g i s t i c g r a d i e n t  x  y   t h e t a    compute g r a d i e n t
i f numpy   l i n a l g   norm   l g r a d     t o l e r a n c e  
break
h   l o g i s t i c h e s s i a n  x  y   t h e t a    compute h e s s i a n
s t e p   numpy   l i n a l g   l s t s q   numpy   a s a r r a y  h    l g r a d          newton s t e p
t h e t a   t h e t a  numpy   a s m a t r i x   s t e p    perform update
return theta

figure    logistic regression solver code
large dataset of positions suggests that the logistic classifier produced a result that would be a
good heuristic for position value 
one advantage that our sparse representation offers us is that it allows us to average the
calculated weight parameters for a given piece  this  in turn  allows us to determine how much
the logistic classifier values a given piece 
when we look at the weights associated with a given piece  averaged over all squares  and
normalized to have the weight of the white knight be    the linear regression resulted in 

wwhitepawn           

wblackpawn           

wwhiteknight           

wblackknight           

wwhitebishop           

wblackbishop           

wwhiterook           

wblackrook           

wwhitequeen           

wblackqueen           

figure    piece valuations according to the logistic classifier

 

fithis is very similar to the well known system of valuing pieces in chess  which values pawns
at    knights at    bishops at    rooks at    and queens at    figure     the fact that the linear
regression independantly reproduced a valuation that was very similar to this system suggests
that it is performing some useful classification 
a pawn is a special piece in chess  once it moves forward  it cannot move backward  and
it is generally  bar capturing  restricted to moving within a single file  since it doesnt move
around a lot  it is reasonable to use pawns to study the value of holding a particular square on
the board  here  we cut by both rank and file and look at the resulting valuations of pawns
 figure    

figure    pawn valuations by rank and file according to the logistic classifier
notice that  for pawns  their value increases greatly as the move forward on the board  but
does not vary much as a function of their file  this makes sense since the further a pawn is
along the board  the more likely it is to either be threatening the opponent  or to be promoted
to a more valuable piece  it is also interesting that the classifier seems to value b  and gpawns higher than c  and f  pawns  this is counter to the expected notion that pawns are
more valuable the closer they are to the center of the board  one possible reason for this result
could be that the absense of a b  or g  pawn indicates damage to the local pawn structure  and
suggests the future loss of surrounding pawns  another possibility is that since c  and f  pawns
are routinely sacrificed for other forms of compensation in the opening stages of the game  their
loss is valued less by the classifier than pawns in other files 
now  we still have a relatively large error rate on this classifier  while much of this error
is probably explained by player error in the individual games  we might want to ask to what
degree this is true  if we assume that player error happens relatively uniformly over time  it
follows that the closer a particular position is to the end of the game  the less likely it is that a
blunder occured in the time between the position and when the game ended  such a blunder will
cause the position to be mislabeled  in the sense that it will be labeled with the outcome that
was actually less likely to occur  this mislabeling will increase the error rate of the classifier 
therefore  if we filter the original dataset to only include positions from the last n moves of a
game  we would expect  as n decreases  for the classifier error to also decrease  below  we filter
the positions in this way  and plot the classifier error as a function of move filtering threshold
 figure    

 

fifigure    classifier error as move filtering threshold increases
since the error rate drops to about     within a single move of the end of the game  this
suggests that about half of our classifier error in the general case was due to player blunder
causing a mislabeling of the test data   of course  even this figure does not completely rule
out the presence of blunders in the test data  since a human error can still occur in the move
preceding the end of the game  in fact  this is a relatively common case  where a player who
made a bad move will immediately resign as a result of this move   the fact that our classifier
produces this curve is an indication that it is performing well  and not overfitting individual
subsections of the data 
we can also intrepret this curve as showing the rate at which players make blunders over
time  as a function of distance from the end of the game  one possibly interesting avenue of
future research would be to look at how this curve varies for players of different strengths  one
would expect stronger players to blunder less frequently  and thus for the curve to be flatter 
but in fact this may not be the case 
even for the general case of positions  this classifier performs relatively well  managing to
predict the winner in two thirds of positions  considering the large number of drawn or unclear
positions  this seems like an impressive feat  from our analysis of human error  it seems like
about     of the classifier error is due to human error in the test set  the rest is likely due to
unclear or sharp positions that arent easily understood by a position based classifier 
my algorithm managed to learn  to a high degree of accuracy  a system of piece weights that
has been known to players for centuries  its results for pawn placement values also seem to be
interesting  especially in regards to variance by file  where the classifier results run somewhat
counter to established wisdom on the subject 
in the future  it could be interesting to try to find novel features for this problem  although i
was unable to find anything that produced good results  it also might be interesting to look at
the performance of this function as a heuristic in a chess engine  and to then have that engine
play and somehow use reinforcement learning to feedback and modify the heuristic based on
the results of this play 

 

fi