automated essay grading using machine
learning
manvi mahana  mishel johns  ashwin apte
cs    machine learning   autumn     
stanford university
final report submitted on     dec     

abstract

the project aims to build an automated essay scoring system using a data set of       essays from kaggle com  these essays were divided into   dierent sets based on context  we extracted features such as total
word count per essay  sentence count  number of long words  part of speech counts etc from the training set
essays  we used a linear regression model to learn from these features and generate parameters for testing and
validation  we used   fold cross validation to train and test our model rigorously  further  we used a forward
feature selection algorithm to arrive at a combination of features that gives the best score prediction  quadratic
weighted kappa  which measures agreement between predicted scores and human scores  was used as an error
metric  our nal model was able to achieve a kappa score of      across all   essay sets  we also got a good
insight into what kind of features could improve our model  for example n grams and content testing features 

 

introduction

essays are crucial testing tools for assessing academic achievement  integration of ideas and ability to recall  but
are expensive and time consuming to grade manually  manual grading of essays takes up a signicant amount of
instructors  valuable time  and hence is an expensive process  automated grading  if proven to match or exceed the
reliability of human graders  will signicantly reduce costs  the purpose of this project is to implement and train
machine learning algorithms to automatically assess and grade essay responses  these grades from the automatic
grading system should match the human grades consistently  currently  automated grading is used instead of second
graders in some high stakes applications  and as the only grading scheme in low stakes evaluation 

 

data

we used a data set from a competition on kaggle com  by the william and flora hewlett foundation  the data
consists of   sets of essays in ascii text  written by students from grade   to grade     essays in each of the   sets
have unique characteristics that are used for grading  this ensures that the automated grader is trained eectively
across dierent types of essays  each essay has one or more human scores and a nal resolved score  each essay
set has a dierent grading rubric  holistic in most cases  trait based in one set  each essay is approximately     to
    words in length  some essays are more dependent upon source materials than others 

 

methodology

a basic block diagram level implementation methodology is shown in figure    we extracted a set of features from
each essay  we chose features that may serve as proxies for what a human grader might look for while grading the
essay  linear regression was then used as our learning model to learn parameters based on the features  scores
were predicted for a distinct set of test essays  these scores were compared against human graded scores to arrive
at an error metric 

fifigure    implementation methodology 

    hypothesis
we hypothesize that a good prediction for an essay score would involve a range of feature types such as language
uency and dexterity  diction and vocabulary  structure and organization  orthography and content  a good model
would incorporate features from each of these areas to arrive at a good prediction  unfortunately  we do not have
source content available for all the essay sets  and hence will be unable to test for content 

    software tools   packages
we chose to implement our model in python     x  since there exist a diverse set of libraries for working with natural
language processing  we have used the natural language toolkit  nltk  and textmining for most nlp tasks  we
also used scikit learn to implement regularized linear regression  in addition to our own implementation   other
libraries  numpy  scipy  xlrd  xlwt  re   have been used for various tasks 

 

feature extraction

we used textmining and nltk libraries for text preprocessing and feature extraction  this mainly involved
removing placeholders for proper nouns  preparing the text  tokenizing and stripping essays of punctuation  intrinsic
variables in an essay like the style and uency cannot be directly measured   they have to be approximated with
measurable quantities like the sentence and word length  length of essay etc 
we extracted features across following categories to train our model  
   bag of words  bow    we used this as a basis to select features  words  that are good predictors of the
essay score  the textmining library has a utility called termdocumentmatrix for creating a bag of words  this is
a measure of the presence of specic content and concepts in each essay set that the examiner looks for 
for each essay set  the top words were collected after removing stop words like the  of  is  at  then compared to
the frequency of words in common language  we collected words that are far more frequent than in average writing 
and assigned a weight to them according to how frequent they were in the essay set  each essay was assigned a
value based on the number of these top words present and the word weights 
   numerical features   features such as total word count per essay  average word length per essay  sentence
count indicate language uency and dexterity  for feature extraction  the essays were tokenized and split using
python utilities  the individual tokens were then used to compute word count  sentences count and character
count values 
   parts of speech count  various parts of speech such as nouns  adjectives adverbs and verbs are good proxies
to test vocabulary  this feature can also be taken as a rudimentary proxy for diction  these features were extracted
using python nltk part of speech tagger  essays were tokenized into sentences before the tagging process 

fifigure    single feature kappa values 

   orthography   correct word spelling indicates command over language and facility of use  we extracted the
number of spelling errors per essay to test for these characteristics  the essays were stripped of punctuation and
tokenized  we ran the pyenchant       spell checker with the aspell dictionary to obtain the count of misspelt
words per essay 
   structure and organization   punctuation is a good indicator of a well structured and organized essay  punctuation based features were extracted using regular expressions matching from individual essays after tokenizing 

 

learning model

an overview of related prior work  see references  indicates that linear regression works well for essay grading
applications  we chose linear regression as our learning model  we did not have a separate validation test essay
set  so we used   fold cross validation to train and test our learning model across the whole range of essays at our
disposal  this was done to guard against overtting 

    evaluation
the error metric for the performance of our learning system is the quadratic weighted kappa  this is a robust
error metric  since it takes into account as the baseline the possibility of agreement occurring by chance  this
metric typically varies from    only random agreement  to    complete agreement   in the event that there is less
agreement between the raters than expected by chance  this metric may go below    the quadratic weighted kappa
is calculated between the automated scores for the essays and the resolved score for human raters on each set of
essays  the mean of the quadratic weighted kappa is then taken across all sets of essays  this mean is calculated
after applying the fisher transformation to the kappa values 

 

feature selection

after implementing a set of features  we proceeded to forward feature selection  to decide the order in which to
add features  we regressed each feature individually against the human graded scores  and calculated kappa values 
we repeated this across all   essay sets and averaged the kappa scores  the results were as follows 
we then proceeded to add one feature at a time to our learning model in the descending order of kappa values
calculated above  we discarded features which did not improve resulting model s kappa value  we repeated this step
across all essay sets and averaged the resulting kappa values  the resulting nal list of features and the resulting
model kappa is as below  this was the nal set of features to train our learning model  

fifigure    forward feature selection

note  one feature added at each step  numbering from gure above 

 

results and analysis

we used the learning model as described above to train and test across each essay set    fold cross validation was
carried out within each essay set  the results are as follows 

essay set

average kappa 

 

    

 

    

 

    

 

    

 

    

 

    

 

    

 

    

quadratic
weighted
kappa

    

  average over   fold cross validation  using fisher transformation
also  we normalized scores across all   essay sets  and used our learning system to train and test using the entire
set of       essays  we again used   fold cross validation  the average kappa value for this was       thus  we
do not see a signicant improvement in training and testing all essay sets over training and testing individual essay
sets 
the essay prompt description gives us some insight into why our model works better on some test sets than
on others  sets   and   are persuasive essays  and hence relatively free from contextual text  sets   to   expect a
critical response after reading a piece of text  essay  story  book   and is therefore expected to have more context
specic content  sets   and   are narratives based on personal experience or imagination  and should thus feature
rich lexical content and complex sentence structure 

fias expected  our model performs relatively well on the persuasive essays  it suers on sets where context is
central to the essay  this suggests that counting the presence of top words of the essay set in the essay is not a
suciently complete measure of content  surprisingly  it does well on set    which is expected to have context based
content  our model does not work well with narrative essays  this indicates we may need to incorporate features
for testing complex sentence structure  like n grams 

 

conclusions

as we hypothesized  features from each category contribute towards a good prediction  our nal model contains
features across all the categories from section     we tried to test for  our model works relatively better on noncontext specic essays  performance on content specic and richer essays can be improved by incorporating content
and advanced nlp features 

    future work
even though linear regression worked as a good predictor for essay scores we are did not test if this is the best model
for text assessment machine learning problems  there is scope for further exploration and evaluation of alternative
models in this area  for example logistic model trees   we could also further improve the model by using more
complex features  this would be particularly constructive for context specic essays in the data set  we believe
that more advanced nlp features  n grams  k nearest neighbors in bag of words  and features that are grammar
and usage specic can further enhance the prediction model 

    references

    valenti  s   neri  f     cucchiarelli  a          an overview of current research on automated essay
grading    
    attali  y     burstein  j          automated essay scoring with e rater v     the journal of technology 
learning and               retrieved from https   escholarship bc edu ojs index php jtla article view     
    kaggle         the hewlett foundation  automated essay scoring  retrieved    october      from kaggle 
http   www kaggle com c asap aes
    ng  andrew     cs    machine learning autumn      lecture notes  retrieved    november      from
stanford university  http   cs    stanford edu materials html
    lukic  a     acuna  v   n d    automated essay scoring  rice university
    preston  d     goodman  d          automated essay scoring and the repair of electronics      
    bird  steven  edward loper and ewan klein         natural language processing with python  o reilly
media inc 
    peccei  christian   textmining     http   www christianpeccei com projects textmining  
    machin  john   xlrd           http   pypi python org pypi xlrd  
     machin  john   xlwt            http   pypi python org pypi xlrd  
     jones  eric  travis oliphant  pearu peterson  and et al   scipy  open source scientic tools for python    
 http   www scipy org   
     kelly  ryan   pyenchant    http   packages python org pyenchant   
     pedregosa  f   weiss  r     brucher  m          scikit learn   machine learning in python               

fi