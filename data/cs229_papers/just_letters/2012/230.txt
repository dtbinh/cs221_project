detecting peer to peer insults
bertrand decoster



fayadhoi ibrahima



jiyan yang



abstract
we are mainly using tools in supervised learning to detect insulting remarks in social discussion such
as forums or tweets  in the feature selection  we extract features such as the frequencies of words and
components of the sentence  we also show that the subject of the sentence does not provide useful
information here  a new approach  cnl  combining the idea of naive bayes and logistic regression
which gives high accuracy of    percents  is provided  our main approach is applying ada boost to a
set of predictors obtained from naive bayes  support vector machine  logistic regression  cnl with an
outside parameter which is the size of vocabulary  the best accuracy is about    percents on the test
data with size over      

 

background

nowadays  more and more people are using blogs or social networks to express their opinions or share
stories  people from different cultures and of different ages may have discussions together in an online
forum  sometimes people may say something that is considered inappropriate by other participants  insults
and other defamatory language may cause hurt feelings and digress from the main topic of conversation 
however  it is impossible to have human moderators monitoring every online conversation  hence we need
to use machine learning to detect insulting automatically 
our aim is to focus on detecting comments that are intended to be obviously insulting to another
participant in the blog forum conversation  we are not looking for insults directed to non participants  such
as celebrities  public figures etc    insults could contain profanity  racial slurs  or other offensive language 
the insulting nature of the comment should be obvious  and not subtle  comments which contain profanity
or racial slurs  but are not necessarily insulting to another person will not be labeled as such 
we consider this as a binary classification problem  the data we use comes from kaggle website  since it
is involved with natural language  we use natural language toolkit  nltk  in python to extract necessary
information from our training data  furthermore  this problem tends to strongly overfit  we also try to
develop techniques or strategies that can be used to guard against overfitting 
we are using knowledge acquired during the quarter in machine learning  as well as some ideas that can
be found in papers on the subject written by researchers in computer and information sciences  such as in
 rium    and  czzx    
this paper will be organized in the following order  in section    we are presenting the methodology we
have followed to build a strategy to get a prediction  then we are exposing some results that we obtained
using different methods in section    and finally  we give a conclusion on the work that has been done in
section   

 

methodology

   

philosophy

here is a basic procedure of our approach  we will explore more details for each step in the following
subsections 
 equal

contributors
stanford university  stanford  ca       

 icme 

 

fifigure    general strategy for classification

   

cleaning the discussion

we clean the discussion to first get rid of some encoding parts  that may put a bias on results   gather similar
words with stemming and discard the less frequent words  to avoid working with too big data   the raw
data look like the following data     xc   xa if you take out the fags and booze and such like 
leaving say  xc   xa    c   xa     per week say and then recognise that they are feeding  
people then that doesnt seem too extreme    xc   xa   or so per day per person   after cleaning with symbols due to encoding  the new cleaned version of data look like the following 
if you take
out the fags and booze and such like  leaving say     per week say and then recognise that
they are feeding   people then that doesnt seem too extreme     or so per day per person  
then we do the stemming for the tokens 

   
     

extracting the features
finding the subject of the sentence

we tried to use the subject as a feature  while analyzing the data  we realized that a seemingly important
part of the results were of the form you are an xxx where xxx would be a derogatory word  typically
one that appears in the naive bayes as the most informative ones  so we had the idea to look deeper into
it  if we could detect such sentences  perhaps would they yield a high percentage of insults  we decided
to use a natural language processing  namely corenlp  from stanford  to determine what the subject of
each sentence is  this statistical tool performs pretty well on standard sentences  and even though a decent
portion of our samples were interjections that could not be properly tagged  we thought that we could still
go on with this approach  the results were less than impressive     tagging the subject of a sentence is still
not an exact science  roughly one third of the sentences are very wrongly mistagged     this approach does
not detect if the derogatory comment actually targets you  only the presence of such a term  finding such
a target is even more difficult than finding a subject of a sentence     people are very imaginative when
it comes to insult  the sheer variety of ways to insult  interjections  sarcasm  etc   renders a grammatical
approach very hard  we decided not to use this feature alone  and only incorporated it to naive bayes as a
plus 

 

fi     

finding other features

similar to the spam classification problem  bag of words is always a good idea to start with  we select
the      words with highest frequency among all the words in our training set  denote the vocabulary as
v   for each sentence  we compute the frequency of each word in v that appeared in that sentence and store
these information in a matrix with size m by  v   where m is the size of our training set 
also  we compute some basic features that could be extracted simply  these features include the length
of the sentence and the ratio of upper case letters in the sentence  also  to differentiate between an insult
directed to a public figure and one towards another participant to the conversation  we can use the fact that
the insult should have a target  that means that if the sentence contains the token you  along with a word
considered as insulting  then the sentence will have a higher probability to be insulting  besides these  we
are also interested in the component and structure of the sentence  for example  after tagging each word in
the sentence  we compute the frequency of nouns  verbs  adjectives and etc  also  we compute the number
of the most     positive informative words appeared in each sentence  after getting this  by a simple feature
selection algorithm  we use the following   features additional to the frequencies of the words in the bag 
feature name
frequencies
n prop
v prop
p prop
upper prop
length
numposwords
superfeature

illustration or condition
frequencies of each word in the bag
proportion of nouns
proportion of verbs
proportion of propositions
proportion of upper case letters
length of the sentence
number of the most     positive words appeared
binary  is   if you is in the sentence and numposwords    

in the following context  we will call the features other than frequencies as additional features 

   
     

building weak classifiers
trying a simple naive bayes approach

the first thing we tried was to use a naive bayes approach on the preprocessed raw data  as in the problem
set    we will only use the words in each sentence as features and suppose they are distributed in multinomial 
the frequencies of each word in the bag appeared in each sentence might be useful in the computation of
maximum likelihood estimator for parameters j y   and j y    
     

adding features and using logistic regression and svm

as discussed above  the features we have are not only the frequencies  but also some other useful quantities 
however  the issue is that naive bayes has the weakness of attributing independence of the features  for
these reasons  we turn to some other method  we use the logistic regression and support vector machine
here  for logistic regression  since it is sensitive to be over fitting  we will only use the   additional features
to train it  for svm  since it is more robust  we add every information to it 
     

combining naive bayes and logistic regression

since we have two sets of features  and in practice naive bayes and logistic regression could only use one
set of them  we are interested in exploring a new approach to use all the features 
suppose x and w are two sets of random variables and x and w are independent  meaning that any xi
and wj are independent  we have the following 
pr y  x  w 
pr x  w y  pr y 
pr x y  pr w y  pr y 
 
 
pr x  w 
pr x  pr w 
pr x pr w 
y
pr x y  pr w y  pr y 
pr x y 
 
 
pr y w   pr x y  pr y w   
pr xi  y  pr y w 
pr x 
pr w 
pr c 
i

pr y x  w   

 

fihence  this results in a combination of naive bayes and logistic regression  to see why  suppose that xi is
the feature corresponding to the i th word of the sentence and w is the additional features  we may get
pr xi  y  by using the traditional mle for naive bayes  meanwhile  pr y w  is the outcome of logistic
regression  when comparing this to the traditional naive bayes formula  it only replaces the term pr y  by
pr y w   generally  if the accuracy of logistic regression is good enough  we should expect a high accuracy
of this method since the mle of pr y  is just the proportion of positive labels  we are not worried about
putting too many features to it  although the x and w we have are not mutually independent  in practice
this approach works well  see the results in the next section   we call this new method cnl 
     

specifying the outside parameter

in naive bayes  we assume that each word in the sentence is distributed from a multinomial with size      
but sometimes some words in the vocabulary are useless  also  sometimes naive bayes is vulnerable to the
size of v   if we specify the size of the vocabulary as   the value of  will also affect the performance of
naive bayes  see the plot below  
we call such parameter  an outside parameter since it cannot be optimized for a certain way  what we
can do instead is to take some values of  and treat naive bayes as different models when it is associated
with different   then the only thing to do is to find the optimal predictors amongst these naive bayes with
different   the analysis can be made for svm and cnl as well 

   

obtaining strong classifiers by ada boost

now we may get a set of weak predictors s  for example  naive bayes with different outside parameters 
to get a strong classifier  we turn to ada boost which can be used in conjunction with many other
classifiers to improve their performance  in each iteration ada boost could find the optimized predictors in
the sense that it can classify the cases correctly where they were misclassified in the previous iterations  to
compute this weighted error  we use the k fold validation approach  in each iteration  the algorithm outputs
a classifier and its weight  at the end  we might get a weighted classifier  its prediction can be made by

hsp  x   

x

i hi  x  

   

i

where i   hi are the weight and predictor from the output of each iteration  it is not hard to show this is a
linear combination of all the predictors in s  the final prediction could be made by setting some threshold 

   

overall strategy

our main strategy is that firstly we should pre process the data and extract the features and labels for each
sentence we have  then we will use the following algorithm for training  basically it contains all the steps
we discussed above 
algorithm   building super predictor via ada boost
input  feature matrix x and label vector y  outside parameter set u  
output  predictor   
   let m be the size of training data  di     m for i              m  t        s    n b   sv m   cn l   lr 
for   u  l     
   while t        do
  
for i               s  do
pm
  
compute the k fold cross validation weighted error by i   j   dj  hi  xj   yi  
  
end for
  
t   argmini   i              m 
i yj ht  xj  
t
 z  for j              m and z is the normalization
  
predictorl   t  l      log   
t    dj   dj e
factor 
   end while

 

fiafter we get the parameter list and weights   we can make predictions for any single data according to
    with a threshold 

 

results

the results for using the four methods by using a vocabulary of size      are summarized in the following
table obtained from a training set with size      and a test set with size      
figure    errors for each predictor
method
features
training error
naive bayes
frequencies
        
logistic regression
additional features         
cnl
everything
       
support machine vector everything
        

test error
        
        
        
        

as we can see  our new approach cnl outperforms naive bayes  part of the reason should be the high
performance of logistic regression  for svm  its accuracy is smaller than naive bayes 
also  we apply algorithm     to build the super predictor  basically  the predictors adaptive boosting
selected are mainly logistic regression and cnl and the accuracy for super predictor is around    percents
by setting the threshold to zero  it is comparable to the existed predictors  by juggling with the threshold
  we get the following roc curve for the super predictor 
figure    roc curve of super predictor

by looking into the details  we conclude that when       it gives the most reasonable tpr and fpr 
this meets the expectation 

 

conclusion and future work

we have tried to investigate how to detect insults  as define in the introduction  that appear in social
discussions  to deal with it  we used some knowledge acquired during the quarter in machine learning 
indeed  we started with cleaning the data  then finding relevant features and finally trying different classifiers
and designing new predictor against over fitting  eventually  we used our super predictor by combining our
predictors via adaboost  however  it is still difficult to detect some false negative results such as this book
is fxxxing good or new words  wordplays  such as yuck fou  for these cases  we will need to find more
elaborated and complex techniques to tackle the issue 

 

fi