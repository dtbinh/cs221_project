predicting reddit post
popularity
by jordan segall and alex zamoshchin
   introduction
reddit is a social news website where users create
posts containing either links or text  which other
uses can  upvote  or  downvote   reddit has
become increasingly popular in the past year having
over two billion pageviews a month  with much of
the traffic and discussion driven by its voting
system  this projects purpose is to predict a posts
future popularity it will receive at the time the post
is created  more formally  this investigation hopes
to predict the maximum score  number of upvotes
minus number of downvotes  a post will receive in
its lifetime 
while a posts popularity is usually related to the
posts content  there are often other factors that
determine how successful a post becomes  the
focus of the investigation was to analyze how these
factors  primarily  how a post is presented  play a
role in predicting how popular a post will
become  this included features such as the title of
the post  the subreddit of the post  and the time of
day the post was created  while the problem of
classifying a reddit post without its content is quite
difficult  an improvement in predictive ability
would provide insight into the role of such features
in determining a posts popularity 
   approach
a file consisting of a random sampling of          
posts on reddit was used to collect the data  using
a python script and the reddit api to connect to
web pages corresponding to these posts  the features
for these posts were captured in a separate output
file  the following are features used in this
investigation 
 title  for each word w in the title of a post 
the number of occurrences of w 
 domain  the website for the url that is linked
to in the post 
 subreddit  the subreddit  or category  that
the post is posted to on reddit 
 over     a boolean indicating if the post is
intended for people over the age of    







self  a boolean indicating if the content is on
reddit  or is a link to an external website 
author  the user who created the post 
created  the time of day  in    minute
intervals  that the post was created 
thumbnail  the type of thumbnail  default 
nsfw  custom  of the post 
self text  if the post is not a link  ie  self
feature above is true   then for each word w
in the content of the text  the number of
occurrences of w 

   evaluation metrics
the algorithms were analyzed based on two
evaluation metrics  for multi class classification 
the first was the classification error  for a post
                 where     and     
where k is the number of classes       is the
prediction  and     is the actual label  then
     

     

where m is

the number of posts 
the second metric was the root mean squared
error  rmse   where
 

 

  

         regression

algorithms were evaluated solely on the rmse 
since classification error was not applicable 
lastly  all implementations were compared to a
baseline algorithm to compare relative
improvements among algorithms  for the baseline
algorithm  posts scores were predicted as the
median score of all posts within the dataset  three  
   multi class nave bayes
a multi class nave bayes classifier  mcnb  was
implemented  in which each classification
corresponded to a different range of scores 
version a  full feature set  nb 
the first version of multi class nave bayes
implemented utilized all of the features gathered in
the data collection phase  refer to section  
above   on a dataset of      posts  the algorithm
showed slight improvement over the baseline
classification error  however  the classification
errors decreased significantly when testing on the
training set  indicating high variance and overfitting

fiversion b  reduced feature set  nb 
one method to reduce overfitting was to increase
the size of the training set  while some attempts
were successful in increasing the datasize  matlab
quickly ran out of memory when training on
datasets greater than a certain theshold  as a result 
the overfitting problem remained 
another method of combating overfitting was to
decrease the number of features  not only would
this help with classification error  but it would also
enable use of larger datasets  by using less memory
in matlab   one way in which the number of
features was decreased was by eliminating stop
words  which are short  commonly used words
that are filtered out in processing natural language
text  though this decreased the number of features 
it did not provide substantial help in reducing
overfitting  nevertheless  all results shown are
without stop words  due to the small improvement 
ablative analysis  a process by which components
are removed one at a time  was used to determine
which features were the least helpful  the algorithm
arrived at an optimal feature set consisting solely of
subreddit  domain  and author  figure   contains
the error rates of this classifier  more detailed
results can be found in table   of the appendix 

classification error rate

figure    multi class nb
generalization
 version b 
validation

   
   
   
   
 
 

     

     

     

     

training set size

while the validation error rates were closer to the
generalization values  overfitting still existed in the
mcnb classifier  however  little more can be done
in this case  larger datasets are impractical due to
limitations in matlab  and decreased feature sets

only hurt our generalization error 
version b  reduced feature set by category  nb 
since overfitting still remained in the dataset  the
investigation shifted to determining if limiting the
classification to a specific subreddit would yield
improved results  by running version bs decreased
feature sets on the atheism subreddit  now
effectively   features   the algorithm was able to
achieve a decrease in classification error by
        and a decrease in rmse by          the
results of testing on various subreddits can be found
in figure   

decrease in error over baseline

in the mcnb algorithm  while the algorithm fit the
training data very well  it was not extendable to data
outside the training set  as a result  version b was
implemented to counteract the overfitting problem 

figure    multi class nb by
subreddit
      
      

     

     

politics

funny

      
pics

atheism

nsfw

figure   indicates that the success of the algorithm
depends on the subreddit  this may be because
reddit users that post consistently within a
particular subreddit follow trends that are easier to
classify as popular  while other subreddit categories
are harder to predict  this is clearly evidenced by
the fact that the algorithm performed the worst on
the pics subreddit  where popularity relies primarily
on the quality of the image  a feature inaccessible to
our algorithm  on the other hand  subreddits like
atheism or nsfw  in which very clear trends exist 
were easier for the algorithm to classify and showed
more substantial improvement over the baseline 
   multi class support vector
machine
while svms are typically utilized in binary
classification problems  they can be implemented as
multi class algorithms using a one vs the rest
strategy  the idea behind the one vs the rest
strategy is to convert the multi class classification
to a series of binary classification problems  where
each sub problem can be solved using existing
methods  for example  suppose there are k
possible classifications  the one vs the rest

fistrategy would create k such classification
problems  where for all i k problems  a classifier
separates the ith class from the remaining k 
classes  therefore  the overall classification for a
new x i  test example is based on which of the k
sub classifications returned the largest output value
when testing on x i  
a multi class svm was implemented using the
above method to classify posts into a range of
buckets  with each bucket representing a different
range of scores  performing a process of ablative
analysis similar to that done above resulted in the
same feature set of nave bayes version b  the
classification errors of this svm can be found in
table   of the appendix and are also shown below 

classification error rate

figure    multi class
svm

generalization
validation

   
   
   
   
 
 

     

      

      

training set size

from above  the fact that the validation tests
perform significantly better than the generalization
tests indicate that the algorithm is still overfitting 
despite the huge amount of data provided 
   linear regression
in contrast to the classification algorithms above 
linear regression allowed a post to be predicted
with a specific score 
version a  full feature set  lr 
linear regression was implemented with a squared
loss objective function  for consistency  all
training runs used a learning rate of     and ran for
    iterations  which were necessary for the weights
in the largest training set to converge  the
generalization errors produced using all features can
be found in table   of the appendix 
it is evident that although at first the algorithm
overfits the data  adding sufficient training
examples is able to overcome this such that the
validation and generalization errors are roughly the

same  this is an example of high bias  meaning that
the algorithm underfits the data  to provide more
expressive power to the algorithm  a greater number
of useful features would be required  however 
since all possible features accessible by the reddit
api were already utilized  adding new features
would require substantial work in scraping externalcontent links  in fact  it is reasonable that without
access to post content  the main factor in a posts
popularity  it would be very difficult to predict
vote count with a high level of accuracy 
version b  expanded feature set  lr 
however  one of the primary problems of the
existing model was that the existence of singular
words  in the title or elsewhere  were not extremely
indicative of popularity  the intuition being that
humor  idioms  or complicated descriptions all
involve combinations of words or phrases  
therefore  a new feature was created using a kernel
involving the combination of pairs of words in the
title  meaning  a new feature w  w  would be added
for every w  and w  in the title  the corresponding
permutation w  w  would not be added   therefore 
it would be possible for the algorithm to detect
trends using word combinations  however  only
mixed results were achieved in exchange for an
almost quadratic increase in time complexity  since
such a feature was deemed not successful  further
improvement would require substantial effort to
procure features outside of reddit 
   tf idf
one attempt to improve the results of the supervised
learning algorithms above was to utilize tf idf  or
term frequency inverse document frequency  tf idf
assigns a weight to each word in a post  indicating
the words relative importance to the post 
normally  the tf idf for a word w in a post p of a
collection of posts p would be 
 

tf idfw p p
 

 
 

 

 

   where

 

 

 

 

   

 

however  alternative tf idf approaches are more
helpful for multinomial implementations such as the
ones used in this investigation  as a result  an
alternative tf was used  where 

fi 

 

 

 

 

   rener       

the idea behind applying tf idf was to differentiate
between the relative importance of words in the title
and self text features of posts  and thus better be
able to differentiate posts based on their important
words  however  with the inclusion of tf idf  the
algorithms showed only mixed results  indicating
the substantial amount of noise present in the data 
   stemming
stemming is a technique used in natural language
processing to reduce a set of words to a smaller
subset of words consisting of base components of
the larger set  for example  while the word tree 
trees  trees  and trees are all different forms of the
word tree  the differences between the forms are
relatively insignificant in the process of predicting
reddit post popularity  therefore  stemming would
reduce the set of words that consist of tree to the
single word tree 
the hypothesis behind implementing stemming was
to reduce the feature sets for mcnb and svm 
since their validation error rates revealed that they
were overfitting the training data  the nltk
 natural language tool kit  framework was
utilized to help implement the stemming algorithm 
stemming decreased the feature sets for the
algorithms significantly  for svm on a      size
training set and      size testing set  the number of
features decreased from       to
       additionally  improvement from the
baseline classification error increased from       
to        indicating that stemming helped
considerably in the classification problem 
   evaluation and conclusion
version b of the multi class nave bayes
classifier achieved a        decrease in the
classification error from the baseline metric 
indicating that with large amounts of data simple
predictive algorithms are preferable  moreover  the
fact that a decrease of        was even possible
conveys how a considerable portion of a posts
success is dictated by factors outside of its content
 such as who posted it and where it was posted  
furthermore  when restricted to only the nsfw
subreddit  version b of the multi class nave bayes
classifier was able to achieve a         decrease
in the classification error from the baseline metric 

indicating that certain subreddits are much more
predictable than others  however  the fact the
mcnb classifier still overfits the data means more
improvement in this algorithm might be possible 
comparing the two classification algorithms reveals
that the multi class svm results are very similar to
those of multi class nave bayes  however 
mcnb appears to learn more quickly than svm on
smaller data sets  with the classification errors for
nave bayes being superior for training sets of size
    and        nevertheless  the two algorithms
appear to converge near a size of         and the
svm approaches an asymptotically lower error than
the mcnb  however  when comparing the
algorithms validation errors  svms errors are
much lower  indicating that svm is overfitting the
training data more so than the mcnb 
while the svm was marginally superior in
reducing the classification error for large datasets 
the mcnb algorithms rmse was slightly lower
than that of the svm for all training sizes  this
indicates that mcnb may be marginally better than
the svm at making predictions that are closer to the
actual score of the post  despite the prediction being
correct or not  however  such a small increase may
be attributed to just random sampling  and no strong
conclusions can be made 
when comparing classifier and regression
algorithms  a number of advantages and
disadvantages must be considered  although the
advantage of a regression algorithm is that it is able
to produce a specific numeric prediction  rather than
a categorical bucket  for this investigation such an
advantage was not evident  in fact  since the
problem of reddit classification is so difficult  the
linear regression algorithm was able to produce
results only with a rmse of          even after a
        improvement  an exact prediction with
such a deviation is not very useful  conveying the
necessity of score ranges  or buckets  in fact  after
a        improvement  the nave bayes classifier
was able to achieve a rmse of        buckets 
meaning that most posts were classified within two
buckets of the correct score range  this information
may however be less useful  indicating a
disadvantage of the nave bayes classifier 

firmse
baseline
     

   

     

classification
error learning
   
   

   
   
  

   
 

 
nb

svm

lr

overall  it was found that version b of the mcnb
and svm classifiers were slightly more desirable
than version a of the linear regression algorithm 
with more useful results  and substantially faster
runtime  the classification algorithms were able to
achieve a better result with a much simpler feature
set  this may convey the relative lack of indicative
power of features such as the title in predicting a
posts popularity  likely due to the substantial
noise   moreover  perhaps with large datasets 
simple algorithms are better at providing more
useful and accurate predictions 
    suggestions for improvement
as alluded to earlier  possible suggestions for
improvement include larger datasets  runnable in
environments with more memory  as well as adding

features outside of the reddit api  possible features
include determining the content of the link  or
applying computer vision to categorize images 
however  it is important to note that there are
intrinsic characteristics of the reddit community
that make the problem of classifying a post difficult 
as described in reference    the popularity of a post
is sometimes not even attributed to the content of
the post  but instead the discussions that occur
within the comments section of that post  however 
since posts begin with no comments upon creation 
this would not be possible 
    references
   http   www quora com reddit how do youget to the front page of reddit
   http   lingpipe blog com            rennieshih teevan and karger      tackling poorassumptions naive bayes text classifiers 
   http   cs    stanford edu proj     poonwu
zhang redditrecommendationsystem pdf
   the random sample of posts 
http   www infochimps com datasets csvdump of reddit voting data
   the list of stop words removed 
http   www translatum gr forum index php t
opic   

    appendix
table    multi class nave bayes version b results  ce   classification error  rmse   root mean squared error in buckets 
train generalization baseline decrease in
generalization
baseline
decrease in
validation
validation
size
ce
ce
ce
rmse
rmse
rmse
ce
rmse
   
      
      
      
      
      
       
      
      
    
      
      
      
      
      
       
      
      
     
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
  larger data sets resulted in an out of memory error in matlab
table    multi class svm results  ce   classification error  rmse   root mean squared error in buckets 
train
generalization baseline decrease in
generalization
baseline
decrease in
validation
size
ce
ce
ce
rmse
rmse
rmse
ce
   
      
      
      
      
      
      
      
    
      
      
     
      
      
      
      
     
      
      
     
      
      
     
      
     
      
      
     
      
      
     
      
     
      
      
     
      
      
     
      
      
      
      
      
      
      
     
      
table    linear regression version a  rmse   root mean squared error 
train size rmse baseline rmse decrease in rmse validation rmse
   
        
        
     
       
    
        
        
     
       
  larger data sets resulted in an out of memory error in matlab 

validation
rmse
      
      
      
      
      
      

combined project with cs     only basic
versions of multi class nave bayes and linear
regression were implemented for cs    

fi