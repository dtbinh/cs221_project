detecting wikipedia vandalism
tony jin  lynnelle ye  hanzhi zhu

 

introduction

since its inception in       wikipedia has become the largest encyclopedia ever created in human history 
with over   million articles in the english edition alone  it has become the highest traffic educational website
on the internet  it receives over         edits per day  which can be daunting for human editors to monitor
for vandalism  spam  or other inappropriate content  while there are existing vandalism reversion bots 
they are generally hard coded and may not be efficient enough at detecting vandalism  types of vandalism
include insertion of obscenities or personal attacks  deletion of valid content  and intentional introduction of
incorrect facts  which can be difficult even for a human to detect  
we will experiment with using machine learning techniques to create a vandalism detection bot  we will
consider features such as character frequencies  word attributes  attributes of the comment associated with
the revision  and the history and attributes of the editor  we will attempt to perform logistic regression and
naive bayes on these features  and we will also consider training an svm with them 

 

literature review

automatic detection of wikipedia vandalism has attracted significant attention in the literature  though
generally at a fairly preliminary level  naive bayes and svms have been applied with some success in     
          and      potthast  stein  and gerling in     and mola velasco in     have compiled lists of features
that are likely to be useful  we have combined ideas from their lists below 
 character frequencies  character distributions which deviate from expectation may indicate the
insertion of nonsense  long sequences of identical characters and character strings with unusual compressibility are similarly indicative  other potential features include the proportion of upper case
characters  vandals are more likely to use all capitals or all lowercase   the proportion of digits or
non alphanumeric characters  character diversity in general  and so on 
 word attributes  unusually long words may signal nonsense  or unusual word lengths in general 
the frequency of obscene words in the edit is also an obvious choice of feature  though it may be
necessary here to distinguish between frequency and impact  the latter being the percent by which the
edit increases the proportion of obscenity in the entire article  this is just because some wikipedia
articles  for example the ones about the words themselves  may legitimately require use of obscenity
in edits 
in a similar vein  one may wish to count first and second pronouns  colloquialisms  misspellings  and
wiki formatting elements  which vandals are unlikely to use  
more generally  comparing the words in the revision to those in the old version and to those in the
article overall is likely to be useful  since the insertion of a large number of words which do not appear
elsewhere in a long  well established article suggests that the new content is irrelevant 
 overall edit attributes  this includes  say  the change in size between the old and new texts  for
example  large deletions are often malicious ones 
 comment attributes  long comments tend to be associated with regular editing  though short or
empty comments are also common practice with well intentioned edits  it is also possible to apply the
same process to comments as to edit text  looking for nonsense or obscenity  though these are probably
much weaker features in this context  since there is no real incentive for vandals to insert junk text
into comments that are not part of the page 

 

fi editor attributes  anonymity is a strong indicator of vandalism  one may also be interested in
counting the number of past edits by the same user  and perhaps measuring the quality of past edits
by how often they were quickly reverted 
we will not attempt to use all of these features  since some should fall out of our algorithms  some are
highly correlated to the point of redundancy  and some are difficult to obtain or analyze 

 
   

the data
source

we draw our data from pan wvc     created by potthast        it consists of       revisions on      
wikipedia articles  each flagged as well intentioned or malicious by a majority of voters on amazons mechanical turk  the malicious edits make up      of the revisions  this corpus is slightly too large for
us and its very unbalanced distribution also makes evaluating accuracy more difficult  so we pull out two
sub datasets for some smaller experiments  the first sub dataset consists of     vandalizing and     nonvandalizing edits which are scrambled randomly before being split into training and testing sets  the second
sub dataset consists of all      vandalizing edits together with a matching      non vandalizing edits which
are scrambled randomly before being split into training and testing sets  we will refer to these as edits     
and edits      
while the fact that edits      and edits      contain a much different distribution of vandalism from the
real life situation could harm the performance of our classifier  in practice our algorithms seem to behave
similarly no matter what the distribution is  as long as it is the same between the training and testing sets 

   

processing

the corpus records the wikipedia page from before and after each revision  for each revision  we convert all
words into lowercase letters and store for each word the difference between the number of times it appears
in the new version from the number of times it appears in the old version  we will sometimes refer to this as
a frequency count of the word in the document  but it is important to remember that it can be negative
if the number of appearances of the word is reduced by the edit 
we also compute the following four non word features  the length of the edit comment  the amount by
which the edit changed the length of the article  this can be positive or negative   whether the editor was
anonymous  and the fraction of those words added by the edit which already appeared in the original article 

 

the experiments

we now describe the algorithms we implemented and their results  it is important to note that all our
experiments on edits      were performed under seven fold hold out cross validation  with     examples
per fold  all our experiments on edits      and the full dataset were performed under eight fold hold out
cross validation  with     examples per fold in the case of edits      and roughly      in the case of the full
dataset 

   

feature selection

although we could use the entire vocabulary for the word features  this results in too much data and the
words which are very infrequent do not improve the classifier  we thus wish to only look at words which
appear with some moderately high frequency  we ended up with three ways of ranking the words  first 
we simply took the most frequent words in all of the edits  both vandalizing and constructive  and removed
stopwords  the top ten words   we call this feature group a  second  we took the top      most frequent
words in each class  and only used as features those which appear in only one of the top thousand lists 
we call this feature group b  third  we took those words with the highest positive difference in frequency
between vandalizing and constructive edits  we call this feature group c 

 

fi   

naive bayes implementation and results

we performed a standard implementation of naive bayes on our training set  to prepare the data for naive
bayes  we extracted feature group b and replaced all negative frequency counts with a count of    though
this is of course a significant convenience  we believe it is also theoretically justified  this is simply because
we do not generally expect the content of the old version of a document to inform our judgment about the
legitimacy of a particular edit on its own  independently from how it compares to the new version  there is no
particular reason a certain word should be removed far more often by vandals than non vandals  while there
are many reasons it might be added far more often by vandals than non vandals   actually  this is probably
not strictly true  it is likely that the removal of some words corresponds to politically  or ideologicallymotivated vandalism  but we will assume that this is a fairly rare or at least localized occurrence which can
be addressed through other means  
tabel   gives the performance of naive bayes under eight fold cross validation using various numbers of
the top word features  note that as the number of features increases  accuracy and recall increase somewhat
while precision essentially doesnt change  of course  the most important point to note here is that these
results are all much better than chance 
number of features
   
   
   
   

overall accuracy
      
      
      
      

precision
      
      
      
      

recall
      
      
      
      

table    performance of naive bayes with various levels of feature selection 

   

support vector machine implementation and results

we ran liblinear on several different versions of our training set  with wildly varying and sometimes
rather strange results  on edits       with no preprocessing to remove negative frequencies  since it is no
difficulty for the svm to handle them and they cannot hurt   we get very reasonable accuracies in the
        range  increasing somewhat as the number of features increases  as shown in figure    the features
are drawn from feature group c 

figure    accuracy vs  number of features for liblinear  the horizontal axis is in hundreds of features
used 
the precision recall curve for varying numbers of features is shown in figure    note that in general 
 

filiblinear precision is much higher than recall  with numbers around     and     respectively  this is
generally good news in our context  because as part of an army of vandal fighting bots and humans  it is far
more important for a particular bot to avoid deleting legitimate edits than for it to find all the illegitimate
ones 

figure    precision recall curve for liblinear  with the number of features ranging from     to      
unfortunately  we were not able to replicate these results on edits       table   shows the behavior of
liblinear on edits      
number of features
   
   
   
   
    

overall accuracy
    
    
      
      
    

precision
      
      
     
      
      

recall
     
      
      
      
      

table    performance of liblinear with various levels of feature selection 
we see that while liblinear continues to do significantly better than chance in all cases  its specific
performance as the number of features change is a little mysterious 
we also tried attaching the four non word features described in section     to the training data for
liblinear  together with our      word features  the result on edits      was an accuracy of       
with        precision and        recall  this is a little better than the numbers in tabel    but not very
much 

   

logistic regression implementation and results

finally  we implemented logistic regression using solely the four non word features described in section     
reasoning that the dimensional blow up caused by including the words themselves would slow down the
algorithm to impractical levels  surprisingly  this gave us our most consistently good results  on edits      
the regression had an accuracy rate of        with a precision of        and a recall of         the success
of logistic regression can be attributed to the usefulness of the features in predicting vandalism  in particular
whether the user is anonymous  logged in users are highly unlikely to be vandals   the length of the comment
 many vandals do not both filling in a comment   and the change in document size  removal of large portions
of text is probably vandalism  

 

fi   

tests on the entire corpus

we then ran naive bayes and svm on the entire corpus with eight fold cross validation  naive bayes was
run on the non negative frequency counts only  the svm was run on four different matrices  depending
on whether the frequency counts included negative numbers or not  and whether the matrix included the
extracted features or was limited to just the frequency counts  the results are summarized below 
algorithm
naive bayes
svm  w o negative  w o features 
svm  w  negative  w o features 
svm  w o negative  w  features 
svm  w  negative  w  features 

accuracy
      
      
      
      
      

precision
      
      
      
      
      

recall
      
      
      
      
      

table    performance of various algorithms on the entire corpus 
accuracy  precision  recall are averaged over the   folds  it appears that naive bayes performs better
than the svm  a possible reason for this is that only    of the corpus is vandalism  so the prior in naive
bayes makes false positives less likely  additionally  running the svm over large amounts of data may result
in overfitting  as the decision boundary attempts to accomodate every single point 

 

discussion

we found that naive bayes  liblinear  and logistic regression all classify edits as well intentioned or
malicious with success probability much better than chance on the various subsets of the pan wvc   
corpus we considered  logistic regression on our four non word features had the highest overall accuracy
rate  but was impractically slow even with a stripped down dataset and could probably not be used in
practice  whether naive bayes or liblinear did better depended on the specific makeup of the training
and testing data  liblinear did well on a small balanced dataset of      edits  with a predictable small
loss in precision and noticeable gain in recall as the number of features went up  but behaved more strangely
 though still much better than chance  on larger datasets  perhaps because of overfitting  naive bayes
remained reasonably well behaved on all scales of data 

references
    belani  amit  vandalism detection in wikipedia  a bag of words classifier approach  arxiv preprint
arxiv                  
    chin  si chi  w  nick street  padmini srinivasan  and david eichmann  detecting wikipedia vandalism with active learning and statistical language models  proceedings of the  th workshop on information credibility  acm       
    mola velasco  santiago m  wikipedia vandalism detection  proceedings of the   th international
conference companion on world wide web  acm       
    potthast  martin  crowdsourcing a wikipedia vandalism corpus  in hsin hsi chen  efthimis n 
efthimiadis  jaques savoy  fabio crestani  and stephane marchand maillet  editors    rd international
acm conference on research and development in information retrieval  sigir      pages         
july       acm  isbn                   
    potthast  martin  benno stein  and robert gerling  automatic vandalism detection in wikipedia 
advances in information retrieval                 
    velasco  santiago m  mola  wikipedia vandalism detection through machine learning  feature review
and new proposals  lab report for pan clef             

 

fi