signal denoising via learning of non linear
manifolds
alex mihlin
abstract
a signal denoising method based on non linear manifold learning
was implemented  this method is applicable for a wide range of noise
types  it improves with training and may be optimized for different
signal types  experiments were performed on images with gaussian
noise  fig       and with a superimposed image  fig       

figure    denoised images      gaussian noise and     superimposed image 
the figures illustrate   a  the original image   b  denoised image   c  noisy
test image and  d  noisy training image  another  clean  version of the
training image was used  

introduction
the method at hand assumes that the noise preserves the local geometry of
the signal in feature space  thus  denoising a signal amounts to inverting
the global transformation induced by the noise  this inverse transformation
is learned from a set of clean and noisy training signals  the learning
process requires the embedding of one feature space into another  such
embedding may be done via dimensionality reduction methods 

 

fitwo canonical dimensionality reduction methods are principal component analysis  pca      and multidimensional scaling  mds       these
methods are appealing since their optimization is well understood and since
they are not prone to local minima  however  the pca and mds methods
are unable to embed the feature space into non linear manifolds  fig     
illustrates this shortcoming  if the feature manifold is non linear  far away
points in feature space may be embedded into close locations 
a more powerful  non linear  dimensionality reduction method  called
locally linear embedding  lle  was recently proposed      this method
attempts to find a low dimensional embedding  which preserves the local
geometry of the feature manifold  fig      illustrates the advantage of this
method over pca and mds 

figure    reduction from   d into   d feature space by     pca and mds
methods and     lle method  the left figure illustrates a shortcoming of
the pca and mds methods  distant points are embedded into close locations  the right figure demonstrates that lle overcomes this shortcoming 
the denoising method at hand uses locally linear embedding in order
to embed the feature manifold of the noisy test image into the feature
manifold of the noisy training image  c f  fig      the denoised image is
then obtained by embedding this manifold into the feature manifold of the
clean training image  this procedure is described in the next section 

method description
the method at hand is based on a recent work by r  shi  i f  shen and
w  chen      as an example  fig    illustrates   dimensional training and
test feature spaces  a patch  solid square  corresponding to a specific data
point  ii   is defined by the k nearest neighbours   ij  j       k   of ii   so that
x
ii  
wij ij
j

p

where j wij      the denoised test point  di   corresponding to each
noisy test point  ii   is determined by the following three steps  correspond 

fifigure    the denoising method  the denoising step  left arrow  assumes
that the noise preserves the local geometry in feature space 
ing to the three arrows in fig     
step    the test patch corresponding to data point ii is embedded into the
noisy training manifold  top arrow   this is done by locating the appropriate k nearest noisy training neighbours   tj    to this end  the training
and test noise types should be similar  a direct implementation of the
nearest neighbours search has a large complexity of
  ntrain  nnearest neighbours  

   

where ntrain and nnearest neighbours are the numbers of training data points
and nearest neighbours respectively  in order to address this issue  the
training data points were arranged into a tree structure  fig      the tree
brunches corresponded to points nearest to appropriate central points  cyan
circles   these central points were found via k means unsupervised learning  this method yielded an improved nearest neighbour search complexity
of


 lognbins  ntrain    nnearest neighbors
   
the nearest neighbours of a specific data point were thus found by  i 
locating the closest central point and  ii  searching for nearest neighbours
only within the corresponding brunch  in order to account for points near
brunch boundaries  neighbouring brunches were appended with overlapping
regions  c f  bottom of fig     
step    calculate the weights  wij   for the reconstruction of point ii from
its nearest neighbours   tj    namely  solve the following optimization prob 

fifigure    a tree structure for efficient nearest neighbours search 
lem 
min
wij

s t 

kii 

nx
train

wij tj k

   

j  

wij     if ii   tj are not nearest neighbours
nx
train
wij    
j  

this optimization problem is equivalent to the following system of linear
equations 
x
gijl wil    
   
l

gijl   

where
if tj   tl are not nearest neighbours of ii and gi is the local
gram matrix 
gijl    ii  tj  t  ii  tl  

p
the resultant weights are normalized so that j wij      if the number of nearest neighbours is larger than the feature space dimension  the
rank of gi is smaller than the number of nearest neighbours and equation
    is ill defined  this issue was addressed via l  regularization  which
resulted in an addition of a small constant to the diagonal elements of gi  
optimization problem     may be made convex  by requiring that wij    
this requirement forces each data point to lie within a convex hull of its
nearest neighbours 

 

fistep    assuming that the noise preserves the local geometry  the appropriate denoised patch is defined by the k clean training data points   cj   
corresponding to the k noisy training nearest neighbours   tj    left arrow
in fig      the denoised data point  di   is thus given  bottom arrow in
fig     by 
x
di  
wij cj
j

where the weights  wij   were calculated in step   
this method was tested on images with two types of noise   i  gaussian
noise with amplitude of     of the maximal feature value  fig       and
 ii  superimposed image  fig        the images were divided into patches 
each n pixel patch represented a  n  dimensional point  corresponding to
the red  green and blue colour values of each pixel  the patches were chosen
with some overlap  in order to insure the smoothness of the denoised image 
the images contained about             dimensional   x  pixel patches 
data points 
to account for the incompleteness of the training set  an iterative method
was proposed  this method required at least two pairs of training signals 
first  one of the training signals was denoised using the other  this denoised signal was then used as a training signal to further denoise an already
denoised test signal  this procedure could be repeated several times  continuously refining the denoised image  the denoised image from fig     
was obtained using two such iterations 

conclusion
a signal denoising method  fig     was implemented based on the locally
linear embedding manifold learning method  fig      an efficient nearest
neighbour search method was implemented via k means unsupervised learning  fig      the use of a large number of nearest neighbours was enabled
via l  regularization  an improved iterative denoising method was proposed and implemented  the method was successfully tested by denoising
an image of gaussian noise and of a superimposed image  fig     

references
   

i  t  jolliffe  principal component analysis  springer verlag  new york       

   

t  cox  m  cox  multidimensional scaling  chapman   hall  london       

   

l  k  saul  s  t  roweis  think globally  fit locally  unsupervised learning of
low dimensional manifolds  journal of machine learning research            
      

   

r  shi  i f  shen  w  chen  image denoising through locally linear embedding  proceedings of the international conference on computer graphics  imaging and visualization                

 

fi