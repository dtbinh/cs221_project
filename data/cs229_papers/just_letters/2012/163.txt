a facebook profile based tv recommender system

jeff david
applied materials
jdavid stanford edu

samir bajaj
apple  inc 
samirb apple com

cherif jazra
apple  inc 
jazracherif gmail com

abstract
we implement and evaluate several algorithms in the context of developing a recommender system based on
data gathered from facebook user profiles  in particular  we look at a matrix factorization technique  svd  
a clustering algorithm  k means   two collaborative filtering algorithms  a content filtering approach 
latent semantic analysis  lsa   link prediction  and nave bayes  and compare their performance in terms
of standard measures  the algorithms draw from principles and techniques in machine learning  natural
language processing  information retrieval  as well as graph theory 

 

introduction

 

recommendation systems are one of the most prominent applications of machine learning  and part of everyday life 
they empower users to sift through enormous amounts of
data and make informed choices  the field of recommender
systems has seen a lot of innovation  and research is actively
moving in the direction of leveraging social content  in that
spirit  our project centers around building a recommendation
system for tv shows based on data collected from facebook
profiles of several users 

traditionally  recommender systems have been evaluated on
the basis of rmse  the root mean squared error  this is
also the metric that was chosen for the famous netflix prize 
however  this works well in a system where users rate items
 tv shows  movies  books  etc   in a certain discrete range 
e g   on a scale from one to five stars  a la amazon com 
in the case of facebook profile data  ratings are unary  either
a user likes an item  thereby making a positive association  
or notand in fact  the absence of a like can be interpreted
either as a dislike or ignorance about the item  we therefore
chose to use the precisionrecallf  framework instead  because the unary rating scale makes the task of recommending
tv shows more like a classification problem  we define precision and recall as follows  where ri denotes a tv show
recommended to the user and l denotes the set of tv shows
liked by the user 
p
  ri   ri  l 
p recision   i p
i   ri  
p
  ri   ri  l 
recall   i
 l 

we start with a brief description of the data set used in the
development of the system and the metric we used to measure
performance  followed by each of the various approaches and
algorithms we implemented 

 

evaluation metric

data set

the data set used in training and testing the recommendation
system was a collection of        anonymized facebook profiles contributed by volunteers in the cs    class  as well as
by our friends and family members  of these        users
had expressed a liking for at least one tv show  and      
users had liked two or more tv shows in their profiles  there
are       different tv shows across all profiles in the data
set  right away  we can see that the user tv show matrix is   singular value decomposition
sparse  the ramifications of this fact will be reflected in our
svd is a matrix factorization technique that attempts to reduce
findings 
the ratings space and identify some small number k of topics
additionally  facebook profile data has a unary rating sys  so that the user preferences can be expressed as a combination
tem for movies and tv showsyou either like something  or of the users interest in a topic and the degree to which the
you dont express any opinion about it  so we used a   to topic is relevant  this is captured succinctly by the following
indicate a liking for a show  and a    or    to denote absence equation 
of any association 
m  u vt
 

fiin terms of the data set  m  r            and we chose to     item based collaborative filtering
factor out k      topicsthis value of k was chosen based
on examination of a sample of the facebook profile dataso this algorithm is similar in spirit to the user based version 
with the users and the items  tv shows  switching roles 
  r      
rather than using similarities between users likes to predict
testing was done by randomly selecting     of the users preferences  item based collaborative filtering uses similarifrom the population that had liked at least two tv shows  and ties between rating patterns of different tv shows  if two
then removing half   their ratingsi e   setting to  again  tv shows have the same set of users like or dislike them  then
in a random fashion  the matrix was then factorized and the they are similar  users are expected to have similar preferences
predicted ratings were computed by multiplying the users for similar items  our implementation used the same cosine
 modified  vector with v t and selecting the top positive similarity measure  and much of the same code to evaluate this
entries  the result was compared with the original ratings to approach 
evaluate the performance 
we performed    fold cross validation on each of the above
approaches in order to evaluate performance 

k means clustering

 

 

our next approach was running the k means algorithm to
identify clusters of similar users in the data  we followed an
evaluation procedure similar to the one used in the previous
experiment  randomly selecting     of the users from the
population that had liked at least two tv shows  and removed
half their ratings  using the standard iterative implementation
of the algorithm  we were able to partition the data into eight
clusters 

content based filtering

having examined collaborative filtering techniques to build a
recommender system  we next turned to content based methods  incorporating techniques from information retrieval  ir 
as well as natural language processing  nlp  

content analyzer

recommendations made to a test user were those that others in the same cluster had collectively liked the most 

facebook
data
collector

parser
tokenizer
stop word
eliminator

 

external
information
sources
plot text
imdb

collaborative filtering

stemmer
normalizer

the theme of this category of algorithms revolves around establishing similarities between different users based on the tv
shows listed in their facebook profiles  and between the items
 the tv shows  based on user interest  this allowed us to recommend tv shows to users on the basis of the viewing preferences of other similar users  likewise  using an item based
similarity  we recommend shows based on their preferences
for certain other shows 

profile
generator
tf idf

feedback
similarity
measure
cosine

   

user based collaborative filtering

evaluation

in this implementation    we used the classic knn algorithm to
define a neighborhood of users similar to the user who will be
recommended tv shows of interest  the neighborhood is defined on the basis of the cosine similarity measure  which is
defined as follows 

filtering
component
ranking

as a first step  the following transformations were applied to
all input datathe facebook profiles as well as the metadata
on tv showsin order to be able to treat all data uniformly 




u
  u 
  

cos 
u
  u      

k
ku k k
u
 

 

   parsing and tokenization  all text was split into
tokens with whitespace as the delimiting set of characters  further  punctuation and other non alphabetic
characters were dropped 

 and 
 represent the users likes and dislikes in
the vectors 
u
u
 
 
the unary based system  a higher value of similarity indicates
that the two users are closer in their tastes for tv shows 
 
 

recommendations

admittedly  withholding less than half the ratings would deliver better precision and recall  so the decision was rather arbitrary 
this part of the project was coded in python  for the complete source code  see     

 

fi   stop word elimination  a set of     most common this algorithm performed fairly well in comparison to others
english words were used as the stopword dictionary  that also operated only in the domain of users and tv shows 
all occurrences of these words were eliminated 
   stemmer  a standard porter stemmer was applied 
   nave bayes i
   normalizer  all text was converted to lowercase as
part of the preprocessing task  and encoded as utf    the first method aims at learning what genre of tv shows each
user likes based on attributes such as comedy  drama  reality 
subsequent to the aforementioned preprocessing  user profiles documentary etc  facebook profiles with more than   liked
were generated based on their facebook data as well as the tv shows that are listed in our database are considered 
tv programming metadata available to us  next  tf idf scores
   training phase  we determine  for each user  the
were computed for the various features and cosine similarity
probability distribution of each attribute given a tv
was used in the ranking function to compute users most simshow is liked  each user has his own separate preilar to a test user  recommendations were based on ratings of
diction model  the list of liked shows is taken from
users in the test subjects neighborhood 
facebook profile and is divided into a training set
      and a positive test set        an additional
in order to generate accurate results in this setup  we used
negative set of the same size as the positive set is
only those users who had liked at least one show for which
used to test tv shows for negative cases since negawe had some metadata  collected from an external source like
tive true labels from the users are not available  the
screaming velocity     or imdb      a      split of this
negative set is taken from the overall tv database
reduced set of users was used to carry out cross validation 
and doesnt intersect with the positive set 
the results surpassed those we had seen thus far 
   prediction phase  after training our user models  we
rank all tv shows from our database by computing
  latent semantic analysis
a score for each show  the score is the sum of probabilities of the users attribute  denoted by a below 
the userfeature matrix generated as part of developing the
distribution for only the attributes that are found in
content based filtering system was next used to investigate
each show 
x
lsa by finding a low rank approximation to the large feature
score user i  t v show j   
 pi  a   a  t v show j  
          
space  the userfeature matrix m  r
was factora
t
    
ized using svd to u v   with   r
  the test users
k cross validation is performed by repeating the
neighborhoods were subsequently computed in this semantic
training testing phases and shuffling the       ratio
subspace and recommendations were generated accordingly 
of the training and positive set in order to better gauge
how the learning algorithm is performing 
note that the matrix being factorized in lsa is the user
feature matrix  which is different from the one used in the
very first approach where every user was represented as a vector in the  also very high dimensional  space defined by the
various tv shows  but without any facebook metadata 

 

link prediction

from the perspective of graph theory  we can look at the set
of users u   tv shows t   and the associations between
them 
t
l  as a bipartite graph g    u  t  l  where u t is empty 
and all edges e  l connect vertices in u with those in t  
the edges are unweighted and the graph represents the complete information of the input data  we can now apply graph
algorithms to gain insight into this network  concretely  we
transform the problem to that of link prediction  where a recommendation is a link that is likely to appear as the network
evolves over time 
there are several linkage measures that can be employed
to predict connections  we used dijkstras shortest path algorithm on the bipartite graph  which  for this analysis  includes users as well as tv shows  and hence has dimensions
r             recommended shows were ranked by distance 
those that were closer to a user appeared at the top of the list 

if the score of a tv show from the positive set is in the top
   scores  it is considered a good recommendation and thus
a successful prediction  since the show belongs to the users
facebook tv list   if the score of a tv show from the negative
set is outside the top    scores  it is considered a successful
 not liked  prediction since we assume it is not liked 
 

fi  

nave bayes ii

bayes function  it was found that multinomial distribution mn worked the best  a         partition
was used for training testing the dataset  to weight
it fairly  the number of tv shows recommended for
each profile was equivalent to the number of tv
shows liked in each profile sample  this is important
when calculating precision and recall for this case  as
the precision when calculated like this is found to increase as we only consider cases where a minimum
number of likes are observed  e g     likes   results
for this implementation were as follows  precision  
       and recall          

the following data preparation and implementation steps were
taken for this approach to the problem 
   two types of dictionaries were created for each of the
   features  the first type of dictionary was a bag ofwords approach  this type of dictionary is more appropriate for the about feature   the second type of
dictionary tokenized each feature entry found  more
appropriate for the teams feature  
   the label for the classification models was tv shows 
a tv show dictionary was created by compiling a
list of all tv shows that were liked in all facebook
samples  the top      tv shows were used for initial data processing  and the top     tv shows were
used for the final classification model      tv shows 
including n a  occurred    or more times in all facebook samples  
   mutual information was performed to determine the
features with the highest relevance  based on these
results    features were selected to use for classification  each feature dictionary was truncated to include
only tokens words that occurred   or more times in
all samples combined  a table showing the selected
features  mi score  and feature size for each of the
feature categories is given below 

   intuition suggests that if a facebook profile is sparse 
then it becomes difficult to make a tv show recommendation regardless of algorithm robustness  to
quantify this intuition  the best trained classification
model   nave bayes multinomial distribution   was
tested on data that did not contain any n as in the selected features  about  interests  athletes  activities 
teams   there were a total of      records out of
      that contained non n a values for all of these
features  the data was again partitioned into    
training and     testing  results improved significantly to precision           and recall          
     correct predictions were made out of      tv
show likes for facebook profiles that contained no
n a values for any of the   features  it is worth noting
that facebook users that had all five of the features
completed averaged     tv show likes each  this
data validates the intuition that if a facebook profile
contains lots of information in the relevant features 
then the probability of making a correct tv show recommendation increases significantly  on the other
end of the spectrum  if a user has no profile information  then it is virtually impossible to make a tv
show recommendation  future work might include
determining the probability of a correct tv show recommendation based on the sparsity of the data in a
facebook profile 

   for each facebook profile sample  a record was created for each tv show liked in that facebook sample 
the label for each of those records was the liked tv
show  so if a facebook user liked    tv shows    
records were created for that sample  the binary feature vector for each record was of size       which
identified which features were present in the record 
      records were generated  thus the feature matrix was      x      with a corresponding label matrix of      x  
   using this feature and label matrix  svm was implemented using the publicly available libsvm     
results were poor     accuracy  a closer inspection
of the recommendation vector revealed that the most
popular tv show was always recommended  a grid
approach for trying different values of c  cost  and
gamma was attempted  as explained in the libsvm
guide  prediction accuracy did not improve  liblinear was also implemented but this did not work as
well 
   nave bayes was implemented with success  various
parameters were tried with matlabs     built in nave

   we now analyze results when only a minimum number of likes are reported in the facebook profile  results are shown below 

it can be seen that precision increases when the minimum number of likes increases  this tells us that our
nave bayes recommender is very good at matching
the top    likes with recommendations  but may not
 

finecessarily be in the same sequence  for example  if
only one tv show is recommended  in order for precision to count that case as true  that tv show must
be the only one mentioned  and thus also the favorite 
tv show of the user  even if that tv show was the
second favorite tv show  that recommendation will
be counted as false  when considering    tv shows
per user  the criteria becomes less stringent and the  
of true matches of recommendations to likes goes up
significantly 

  

we also spent a substantial amount of time investigating a solution using an svm based classifier  the idea was to treat
the recommendation of a tv show to a user as a classification problem because of its similarity to the facebook like
feature  we predict    for a tv show that we think the user
would like  and   otherwise  unfortunately  the results from
this approach were poor  indicating that it wasnt a viable technique 

  

summary

acknowledgments

we thank andrew maas for his help in getting started  and for
guidance along the way 

we covered a lot of territory in the course of working on this
project  and gained useful insight in the process of implement  we would also like to acknowledge the support and data proing the different algorithms 
vided by graham darcey and wayne yurtin of screaming
velocity  inc   for our project 
first  a real recommender system is a complex structure  and
takes a lot of effort to build and fine tune  even for a student research project like the one we built  it took an immense    references
amount of labor to collect  process  filter  and organize the
   bajaj  samir  source code available at github 
data before we could run any algorithms  second  there is
samirbajaj cs    project
real value in metadata  if it is available  the algorithms that
   lops  p   de gemmis  m     semeraro  g        
used external data and attributes from a users facebook procontent based recommender systems  state of the art
file performed consistently better than those that didnt avail
and trends  in f  ricci et al   eds    recommender sysany metadata  in discussions with engineers who have built
tems handbook  springer science business media 
large scale commercial recommendation engines  we learned
that production systems typically use a blend of several dozen
   ng  andrew  cs    lecture notes        stanford unialgorithms  combining the best of what each one has to offer 
versity 
   turney  peter d    pantel  patrick        from frequency to meaning  vector space models of semantics  in journal of artificial intelligence research    
pp          

the following table lists the precision recall results for the
various algorithms we implemented 

   chih chung chang and chih jen lin  libsvm 
a library for support vector machines 
acm
transactions on intelligent systems and technology                    
software available at
http   www csie ntu edu tw cjlin libsvm
   http   www mathworks com help stats naivebayesclass html
   screaming velocity  inc   http   www screamingvelocity com
   imdb  the internet movie database  http   www imdb com

 

fi