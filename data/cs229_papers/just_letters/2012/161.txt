s tanford u niversity  cs      m achine l earning

dont get kicked   machine learning predictions for car buying

albert ho  robert romano  xin alice wu
december         

  introduction

led us to update our data processing strategy and
determine a better way to evaluate and compare
when you go to an auto dealership with the in  different learning algorithms  finally  we impletent to buy a used car  you want a good selection to mented boosting and tailored our final algorithm
choose from and you want to be able to trust the selection based on initial successes 
condition of the car that you buy  auto dealerships
purchase many of their used cars through auto
auctions with the same goals that you have  they
we obtained our data set from the kaggle com
want to buy as many cars as they can in the best challenge  dont get kicked  hosted by carvana 
condition possible  the problem that these dealer  the data set contained    unique features with
ships often face is the risk of buying used cars that        samples along with a labeling of   for good
have serious issues  preventing them from being car purchases and   for  kicks   some key feasold to customers  these bad purchases are called tures included odometer readings  selling prices 
 kicks   and they can be hard to spot for a variety vehicle age  and vehicle model  one thing that
of reasons  many kicked cars are purchased due we immediately noticed was that good cars were
to tampered odometers or mechanical issues that heavily overrepresented in the data set  representcould not be predicted ahead of time  for these ing       of samples  the consequences of this
reasons  car dealerships can benefit greatly from became more apparent once we began comparing
the predictive powers of machine learning  if there machine learning algorithms across different metis a way to determine if a car would be kicked a pri  rics 
ori  car dealerships can not only save themselves
money  but also provide their customers with the     word bins
best inventory selection possible 
our first major challenge was the preprocess 

  initial data preprocessing

the following paper is split up into   main
sections describing our approach to solve this
problem  initial data preprocessing  early algorithm selection  data normalization and balancing  performance evaluation  and boosting  first
we identified the key characteristics of our data and
formed strategies for preprocessing  next  we ran
several simple machine learning algorithms  this

ing of data  for data such as the name of the vehicles model  manufacturer  and color  we had to
assign unique identifiers to specific strings in the
feature space  this was straightforward for a feature like transmission since we could assign   for
auto and   for manual  the process became more
involved with multivariate features such as the car
submodel  we decided that even though there were

 

fimany different submodels  categorizing them with
unique identifiers rather than grouping them was
the more conservative option 
    missing features

some of the samples had missing features  we
had the option of throwing out the sample completely  but we believed that it would be a waste 
we decided to implement the following rules  if the
feature was represented with a continuous value 
we would replace the missing value with the average of the feature over the other samples and if
the feature was represented with a discrete value 
we would create a new value specifically to identify
missing data 

 a  ratio of scaled vehicleage

    data visualization

before running any algorithms  we visualized
the data with plots to gain some intuition about the
features  the training data was separated into good
and bad datasets and compared  looking for trends 
histograms were plotted over each feature with the
frequency normalized so that good and bad cars
were equally represented  this allowed comparison of the relative frequency over a feature  an
example is figure  a  showing that bad cars were
generally older  to get an idea of how discriminating a feature was  the ratio of the relative frequency
of bad to good was plotted  figure  b shows that
current auction average price was a strong feature 
however this needed to be taken with a grain of
salt because the areas where the features were most
discriminating were generally in small tail regions
that applied to a very small subset of cars 

 b  ratio of currauctnavgprice

figure    histogram plots depicting ratio of scaled
vehicle age and current auction average
price

on the remaining      initial runs yielded about
    generalization error  which on first glance was
very good 
    logistic regression

since the feature we were trying to predict
was binary  we decided to try a logistic regression
model as a first pass  logistic regression via newtons method was implemented in matlab with
the same method of cross validation as that in svm 
we found that the algorithm converged after   iterations  yielding a generalization error of about
with the data parsed and some initial in      
sights to guide us  we applied some basic machine
    observations
learning algorithms that would identify where we
using generalization error as a metric  both loneeded improvement and what strategy would be
most effective  at this point  we chose generaliza  gistic regression and svm seemed to have yielded
tion error as a metric to evaluate our algorithms promising results  upon further investigation 
however  these runs would nearly always predict
performances 
the null hypothesis  i e  a good car prediction for
    support vector machine
every testing sample  this was where we started to
first  we tested our data with an svm  we used question the use of generalization error as a perliblinear v       and the method of cross valida  formance metric in favor of performance metrics
tion by training on     of our data set and testing that took into account false positives and false neg 

  early algorithm selection

 

fiatives  we also conducted a literature review in what car dealers care about  in general  you want
hopes of finding alternative algorithms more suit  a balance between precision and recall  so we used
able for skewed data sets 
auc and f   which are derived from fp and fn  to
find that balance 
through our literature search  we found that
when studying problems with imbalanced data 
using the classifiers produced by standard machine
    feature normalization
learning algorithms without adjusting the output
after evaluating the performance of our early threshold may cause poor performance      in this
attempts  we made several changes to the data pre  respect  auc is a good metric since it takes into
processing procedure in hopes of achieving better account sensitivity  recall  and specificity over the
results  through our literature search  we found entire range of possible output threshold values 
that data normalization increases the performance auc is a good indicator of one classifiers ability
of many classification algorithms     as a result  we for correct prediction over another  in addition  we
normalized our numeric features over the range   also used the f  score as a performance metric to
account for the inverse relationship between preto   
cision and recall      we define f  as the harmonic
    data balancing
mean between precision and recall 
in addition to data normalization  we also dis   pr eci si on  r ec al l
f   
   
covered that  up sampling  the data from the mipr eci si on   r ec al l
nority class is an effective way of solving the class
imbalance problem                   to do this if precision and recall has been traded off  the f 
we again split our data in a       cross validation score will not change  that way we can identify a
scheme  from the data split intended for training  superior algorithm as one that increases both prewe created a balanced training data set by over  cision and recall 

  data normalization and
balancing

sampling the bad cars  both balanced and unbalanced data sets were used for the algorithms we
tested from this point forward to observe the efafter applying data normalization and balancfects of artificial data balancing 
ing  we returned to our initial approaches using
svm and logistic regression  we found that by
using these algorithms with normalized and balas mentioned earlier  we found that using gen  anced data sets  we were able to achieve better auc
eralization error alone as a performance metric and f  scores  and therefore better results than bewas misleading due to the bias of our data towards fore  we also tried tuning the c parameter in ligood cars  a prediction of all good cars  for exam  blinear to little effect  from our own research
ple  would yield       accuracy  in the context of and discussion with the tas  we found that boostour problem  it is more relevant to evaluate an al  ing might be a promising approach for our learning
gorithms performance based on precision and re  problem  the idea behind boosting is to combine
many weak learners into a strong learner            
call
to implement boosting  along with a slew of other
tp
learning algorithms  we used weka  waikato envipr eci si on  
tp  fp
    ronment for knowledge analysis  v        
tp
weka made it easy to try many different learnr ecal l  
tp  fn
ing algorithms quickly  due to the nature of our
rather than predictive accuracy  since the number data  we were very interested in comparing the perof false positive  fp  and false negatives  fn  pre  formance of traditional classification algorithms
dicted by an algorithm is more directly related to with meta classifiers such as boosting and ensemprofit and opportunity cost  which is ultimately ble learning  however  weka is also very mem 

  boosting

  performance evaluation

 

fiory intensive  the program could not run logistic regression without crashing even with    gb of
memory allocated  as a result  logistic regression
was still implemented in matlab  while all others
were implemented in weka 

  results

model bags were used as well as sort initialization  the ensemble selection algorithm with most
promise was one that incorporated many different
classifiers  including naive bayes  j    and reptree
classifiers  this resulted in an auc of      along
with an f  of       just shy of logitboost 
it was found that contrary to literature  balancing the data did not generally improve classifier performance  in fact  classifiers generally
performed worse when trained on the balanced
data set  while balancing the data yielded reduced
number of false negatives  it also dramatically increased the number of false positives 

we used weka to implement several metaclassifiers  specifically adaboostm   realadaboost 
logitboost  and ensemble selection  the weak
classifiers we used were decision stump  decision
table  reptree  j    and naive bayes  decision
stump is a one level decision tree  decision table
is a simple majority classifier  reptree is a fast decision tree learner  based on information gain and
we found through our investigation that logpruning using reduced error pruning with backfitting  j   is an implementation of the c    deci  itboost was the best at predicting whether or not a
sion tree  which is based on maximizing informa  car would be a kick  it produced a prediction with
the highest auc value of       and an f  of       
tion gain 
adaboostm  is a general nominal classifier the f  value was not as high as we would have
boosting algorithm  using decision stump as its liked  but depending on the relationship between
classifier  it performed reasonably well with an gross profit and loss in the total profit equation 
auc of        we tried using more sophisticated f  may not even be a great metric to maximize the
classifiers such as j    random forest  and rep  parameter of interest 

  discussion

tree  however they all performed worse  realadaboost is an implementation of adaboost that is
optimized for binary classification  using decision
stump as its classifier  it performed well with an
auc of        similarly  other more sophisticated
classifiers did worse  perhaps due to overfitting 
logitboost using decision stump performed better
than adaboostm   with an auc of        logitboost using decision table performed slightly better  with an auc of       because of this we decided
to stick with logitboost as our boosting algorithm of
choice 
ensemble selection can use any combination
of weak classifiers to make a strong classifier  so it is
very flexible  one implementation is to additively
build a strong classifier by selecting the strongest
weak classifier  and then one by one adding the
next strongest weak classifier  we chose to use auc
as the metric for evaluating classifier strength  because ensemble selection uses a greedy optimization algorithm  it is prone to overfitting  to overcome this  strategies such as model bagging  replacement  and sort initialization were used  ten

tot al  p r o f i t   t n  gr oss p r o f i t   f n  loss
oppor t uni t y c ost   f p  gr oss p r o f i t
   
total profit represents the profit that a car
dealership will make if they follow the predictions
of an algorithm  all cars that are classified as
good and are actually good will make the dealership some gross profit per car  at the same time 
all cars that are classified as good  but are actually
not will cause the dealership to incur some loss 
the opportunity cost represents the gross profit
lost from any car classified as bad that actually was
not  what these formulas boil down to is a tradeoff between false negatives  false positives  and true
negatives through gross profit and loss  if loss is
higher for the end user  they would tailor the algorithm to produce less fn  while if gross profit is
higher  they would want less fp 
of all the procedures and algorithms we used 
the most useful were data normalization  boosting 
and using auc and f  as performance metrics 

 

fitable    algorithm comparison  a  decision stump  b  decision stump     iterations  c  decision table 
d  j   decision tree  e  maximize for roc  f  assortment

  future work
there are several strategies we would pursue in order to further improve prediction performance  one would be to evaluate our algorithms
on a separated data set created by the removal of
overlapping data via pca      literature suggested
that if a data set is overlapped  one could run algorithms on the portion of the data that is not overlapping to get better results  the reason we did not
pursue this in the beginning is that doing so would
create a high variance classfier may overfit the data 
another strategy that we did not get working would
be to use rusboost  which has been shown to improve performance on imbalanced datasets  such
as our own      finally  we would want to use libsvm with a nonlinear kernel such as gaussian to
compare with our other algorithms  due to computational performance limitations  we were unable to implement this method 

   acknowledgements
we would like to thank professor andrew ng
and the tas  especially andrew maas  sonal gupta 
and chris lengerich  for all their help on this
project along with kaggle and carvana for providing data 

    menardi g  torelli n         training and assessing classifcation rules with unbalanced data  working paper series
    provost  f         learning with imbalanced data sets
     invited paper for the aaai     workshop on imbalanced data sets 
    japkowicz  n          the class imbalance problem 
signifcance and strategies  in proceedings of the     
international conference on artifcial intelligence  icai       special track on inductive learning las vegas 
nevada 
    forman  g   scholz  m          apples to apples in crossvalidation studies  pitfalls in classifer performance measurement  acm sigkdd explorations           u   
    hastie 
t 
       
boosting 
retrieved
from
stanford
university
web
site 
http   www stanford edu  hastie talks boost pdf
    friedman  j   hastie  t   tibshirani  r          additive logistic regression  a statistical view of boosting  with discussion and a rejoinder by the authors   the annals of
statistics                 
    das  b   krishnan  n  c   cook  d  j         handling imbalanced and overlapping classes in smart environments
prompting dataset 
    seiffert  c   khoshgoftaar  t  m   van hulse  j   napolitano  a         december  rusboost  improving classification performance when training data is skewed  in
pattern recognition        icpr         th international
conference on  pp        ieee 

references
    graf  a   borer  s          normalization in support vector
machines  pattern recognition          

 

fi