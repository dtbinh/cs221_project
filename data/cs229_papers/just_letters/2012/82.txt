reconstructing
broadcasts on trees

u      as either white or black  then  for
each of the  d     children of the root  independently assign the child the color of the
parent with probability     p  and the opposite color with probability p  repeat this
process recursively to color the entire tree 

douglas stanford
cs    
december         

u  
u  

abstract
u  

given a branching markov process on a tree 
the goal of the reconstruction  or broadcast  problem is to guess the initial conu  
dition at the root  given the states of the
leaves  in this essay  we evaluate the performance of some algorithms  new and old  for figure    the time variable u in a p    
tree 
solving the reconstruction problem 

 

the reconstruction problem     asks the
following question  given the colors of the
leaves  with what probability can the initial
state of the root be inferred  the answer
depends on the value of u  the degree d and
the probability of color flip p  it is known
    that  in the limit of large u  the reconstruction probablem is unsolvable if

introduction

branching markov processes have a wide relevance across disciplines  in phylogenetics 
they arise as a description of the mutation
dynamics of evolution  in mathematics and
physics  they are simple examples of statistical mechanics systems with phase transitions  in theoretical cosmology  they are
used as a model for the fluctuation dynamics
of inflation 
in all three of these contexts  a natural and important question is the following 
how well can one recover the initial state of
the branching markov process from the final
state on the leaves of the tree  in this essay 
we will apply machine learning algorithms
to this question 
first  a definition  in this essay we will
work with two state branching markov systems  this is defined on a regular rooted
tree of degre d as follows  begin by specifying the state of the root  at generation

 
 
    p  
d 

   

however  if the reverse inequality holds 
then the leaves contain at least some information about the state of root  even in the
u   limit        
in this project  we will fix u and d and
vary p  this gives us an environment
in which the difficulty of the classification
problem can be controlled precisely  we will
study the performance of four different algorithms as a function of p  naive bayes 
not so naive bayes  parsimony and an svm 
 

fi 

the dataset

four children of the root  these principal
components have an approximate interprewe generated training and testing sets by tation as labelling whether one of the first
running the branching markov process for children had a color change  compared to the
different values of p  sample configurations root 
of the leaves for a degree five tree are shown
in figure    these configurations all had
white initial conditions at the root  the
value of p increases from the left panel to
the right panel to the bottom panel  and it
is intuitively obvious that the reconstruction
becomes more difficult as p increases 
  matrix

output txt matrix

  matrix

output txt matrix

  matrix

output txt matrix

figure    the second  third and fourth principal components for the d     system  the
first principal component is approximately
spatially homogeneous 

figure    sample configurations of the
leaves in the d     two state system after
six generations  for different values of p 

although the data shown here is for the
d     tree  it is convenient to run the algorithms  particularly parsimony  on a tree
of even degree  so we chose to work with
d      we generated       training and
testing examples for each of twelve values of
p in the range      to       with randomly
chosen white black initial conditions  for
reference  the critical value beyond which
reconstruction is asymptotically  u   
impossible is approximately p        for a
degree four tree 

to get a feel for the important variables 
we ran pca on a set of       training examples  the first principal component ends
up being roughly constant on all leaves 
naively  this encodes most of the information about whether the root was white or
black  the second  third and fourth principal components are shown in figure    again
for a degree five tree  the four obvious
blocks correspond to the descendants of the
 

fi 

naive ish bayes

the optimal  maximum likelihood  reconstruction of the root would proceed as follows  we would assign a uniform prior for
p  root  and then compute
figure    simplified tree graphs for the naive
bayes model  left  and the nsnb model
 right 

p  leaves root p  root 
p  root leaves   
 
p  leaves 
   
as usual p  leaves  cancels out when comparing probabilities for classification  all we
need is p  leaves root   in principle  explicit
knowledge of the markov matrix and the
tree graph make is possible to compute this
exactly  however  it appears to be computationally intractable 
one alternative is to use a maximum likelihood algorithm on a related but simpler
graph  for example  choosing the graph on
the left of figure   is equivalent to making
an ansatz
y
p n b  leaves root   
p  leafi  root      

this is much like the naive bayes graph on
the left  but it models the first generation
after the root explicitly as latent variables 
p n sn b  leaves root   
   
x
y
p   z  root 
p  leafi   z   
 z 

i

here   z  are latent varibles labeling the colors of first generation  p   z  root  is equal
to pm     p n   where n is the number of vertices in the first generation with the same
color as the root  and m is the number with
i
the opposite color  finally  p  leafi   z   is
as in eq      but with the color of the root rehere  p  leafi  root  can be computed simply
placed by the color of the relevant first genby iterating the markov matrix  explicitly 
eration vertex  and u replaced by u    
it is
in the text below  we refer to this model as
   
u
the
not so naive bayes  nsnb  algorithm 
   
p  leafi  root          p   
   
with the upper sign if the leaf is the same
color as the root  and the lower sign if the
leaf and root are opposite  after taking the
product over leaves  it becomes clear that
maximizing p n b over choices of the root
color reduces to majority vote  if most of
the leaves are white  guess that the root was
also white  otherwise  guess that the root
was black 
a slightly more complicated graph for
which maximum likelihood is nevertheless
tractable is shown on the right in figure   

 

parsimony

the parsimony reconstruction algorithm 
popular in the phylogenetic literature  recursively applies majority vote   more specifically  the leaves at generation u are grouped
into families of size  d      each having a
single parent at generation  u      the
 

on a tree of odd degree  the algorithm is more
complicated  here  we will focus on degree four  so
the above description is correct 

 

ficolor of each such parent is assigned as the
majority vote of the children  this algorithm is repeated until it assigns a color to
the root at generation u      parsimony
has the nice property that it identifies a coloring of the tree that minimizes the number of mutations  or parent child differences 
note  however  that this is not necessarily
the maximum likelihood reconstruction  indeed  well see that parsimony is a suboptimal algorithm for large p 

 

svm learning curve

  

probability of correct classification

  
  
  
  
  
  
    
  

   

   
training set size

   

   

figure    learning curve for the svm  p  
     

svm

 

the final algorithm we consider is an svm 
we used a gaussian kernel  with an    soft
margin  implemented using libsvm      we
found that feeding the data of the leaves
directly into the svm leads to overfitting
and relatively poor performance  instead 
inspired by the principal components discussed above  we used as features the majority vote of the descendants of differnt vertices  starting with the root and working upwards  the first feature is just the total census nwhite  nblack of the leaves  the second
feature is the census of the first    d    
leaves  the third is the census of the next
   d     and so forth  we selected features
using cross validation  and found rather robustly that the optimal number of features
for the d     tree was four  we also selected
the parameters  of the gaussian kernel and
c of the soft margin using cross validation 
the results were rather insensitive to the
choices of  and c  for       and c     
we trained on       samples  this appears
to be sufficient  judging from the learning
curve shown in figure   

comparison
methods

of

the

finally  we tested all four of the algorithms
on       samples for each of the values of p 
the performance is shown in the figure and
table below 
as expected  the performance of all algorithms decreases monotonically as p increases  for small values of p  the parsimony
algorithm does best  followed closely by the
svm  however  as p increases  parsimony is
less effective  indeed  for p        parsimony is the worst of the four  the naive
bayes algorithm is the worst for small values of p  but it ends up more or less tied
with the svm at larger p  the svm does
best overall  coming close to the accuracy of
parsimony for small p  but tracking the naive
bayes performance for larger p  nsnb has
performance very similar to the svm  but it
is slightly worse for all p 
we conclude that the svm emerges  once
again  as having very competitive performance across the board 
 

fiperformance comparison

    
    
probability of correct classification

table    probability of correct reconstrucnb
tion for the five reconstruction algorithms 
parsimony
as a function of p  the probability of changsvm

    

ing color in a single generation

    
    
    
    
    
    
    

    

    
    
probably of flip per generation

    

figure    performance of different reconstruction algorithms for various values of p 
the degree is fixed at four  and simulation
is run for six generations  the nsnb curve
is omitted for clarity  see the table for more
details 

p
    
    
    
    
    
    
    
    
    
    
    
    
    

nb
parsimony nsnb svm
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             
      
      
             

nals of applied probability           
          

references

    chih chung chang and chih jen lin 
libsvm  a library for support vector machines 
acm transactions
on intelligent systems and technology 
                  
software available at http   www csie ntu edu tw 
 cjlin libsvm 

    t  moore and j  l  snell  a branching process showing a phase transition 
journal of applied probability        pp 
             
    p  bleher  j  ruiz  and v  zagrebnov 
on the purity of the limiting gibbs state
for the ising model on the bethe lattice 
journal of statistical physics        
                   bf         
    h  kesten and b p  stigum  additional
limit theorems for indecomposable multidimensional galton watson processes 
ann  math  statist                     
    e  mossel  reconstruction on trees 
beating the second eigenvalue  the an 

fi