financial market time series prediction with recurrent neural
networks
armando bernal  sam fok  rohit pidaparthi
december         
abstract
we used echo state networks  a subclass of recurrent neural networks  to predict stock prices of the
s p      our network outperformed a kalman filter  predicting more of the higher frequency fluctuations
in stock price 

the challenge of time series prediction
learning from past history is a fudamentality ill posed  a model may fit past data well but not perform
well when presented with new inputs  with recurrent neural networks  rnns   we leverage the modeling
abilities of neural networks  nns  for time series forecastings  feedforward nns have done well in classification tasks such as handwriting recognition  however in dynamical environments  we need techniques
that account for history  in rnns  signals passing through recurrent connections constitute an effective
memory for the network  which can then use information in memory to better predict future time series
values 
unfortunately  rnns are difficult to train  traditional techniques used with feedforward nns such
as backpropagation fail to yield acceptable performance  however  subsets of rnns that are more
amenable to training have been developed in the emerging field known as reservoir computing  in
reservoir computing  the recurrent connections of the network are viewed as a fixed reservoir used to
map inputs into a high dimensional  dynamical spacea similar idea to the support vector machine 
with a sufficiently high dimensional space  a simple linear decode can be used to approximate any
function varying with time 
two reservoir networks known as echo state networks  esns  and liquid state machines  lsms 
have met with success in modeling nonlinear dynamical systems         we focus on the former  esn 
in this project and use it to predict stock prices and compare its performance to a kalman filter  in an
esn  only the output weights are trained  see figure    

echo state network implementation
the state vector  x t   of the network is governed by
x t     

 

f w in u t    w x t    w f b y t   



   

where f      tanh    w in describes the weights connecting the inputs to the network  u t  is the
input vector  w describes the recurrent weights  w f b describes the feedback weights connecting the
outputs back to the network  and y t  are the outputs  the output y t  is governed by
y t 

 

w out z t  

where z t     x t   u t   is the extended state  by including the input vector  the extended state
allows the network to use a linear combination of the inputs in addition to the state to form the output 
esn creation follows the procedure outlined in      briefly 
   initialize network of n reservoir units with random w in   w   and w f b  

 

fireservoir
x t 
win
wout
input
u t 

w

output
y t 

wfb

figure    the esn consists of inputs units  dark blue   reservoir units  orange   and output units  red  
the input weights w in   recurrent weights w   and feedback weights w f b are fixed  only the output weights
w out are trained  through recurrent connections and nonlinear activation functions of the reservoir units 
the esn can model arbitrary  dynamical systems with a simple linear decode 
   sparsify w by setting sp fraction of the entries to   
   set w   


w
max  w  

so spectral radius of w is  where        

   train network 
 a  input training sequence and assign output y t  to take on the value of the training output 
 b  record extended state z over training period 
 c  find w out to minimize kytrain  w out zk   over the training period 
setting the spectral radius to less than   ensures that any state of the network eventually decays to   
which preserves the stability of the network 

data  feature  and parameter selection
we sourced our data from yahoo  finance  we collected daily stock prices dating back from late     
to early       our feature vector included the current and   day history of the stock price  the           
and    day moving averages  volume  and the s p    index  these features are typically used in graph
analysis to buy  hold  or sell a stock  we only used features updated daily so did not include indicators
such as the price to earnings  which is released quarterly  the s p    index was chosen as a feature
since the index represents the     biggest companies in the largest economy in the world  the stock
volume was used as an indicator of the activity of the stock 
in our esn implementation  n                and sp         we wanted to gauge how many
training samples we need to train  we found that flushing out the initial network state by driving the
network with training input for a period before recording the extended network state for training the
output weights improved the network performance  see figure    
we examined how the testing error behaved with respect to the number of training examples  see
figure     error did not decrease monotonically with number of training examples  a possible explanation for this could be that during the high test error training periods  the stock was subject to different
perturbations than during testing 
to compare the contribution of each feature to the model  we performed a leave one out crossvalidation analysis to check which features result in the lowest error  see figure     the esn performed
worst when the price was not included as a feature  and the esn network performed its best when the
feature set included the current price  trading volume  and the s p    price 

 

fi   

      

     

      

    

  
  

flush time

  
  
  
  
  
  
 
 
 

 

 

 

 

 

 

log   density w  

figure    flush time for the kxk  to decay to less than      from initial random state as a function of
density  fraction of nonzero entries  of w   and spectral radius  legend   each data point averaged from
simulation of     random initial states  for the parameters of our esn  we found that about    time steps
was necessary to flush out the initial state 
all features
   
   

testing error

   
   
   
   
  
 
 

  

  

  
  
  of training examples

  

  

figure    performance of esn using all    features on a test set of     days as duration of training is varied 
 yy
  
test error is defined as   target  

variance normed mse

variance normed mse

y
target

   

   

 
 

  

  

  

  
  
  
  
  of training examples

  

  

  

  

   

   

 

  

 

  

   

   

   
   
   
   
  of training examples

   

figure    top  leave one out analysis for all features exluding volume  bottom  analysis for volume  note
the magnitude of error introduced when leaving out volume 

 

fikalman filter performance comparison
as a baseline to compare esn performance  we implemented a kalman filter  the kalman filter uses
an underlying linear dynamical model recursively estimates the state of a process by reducing the mean
squared error  we used a simple model to predict the next days price  we modeled the system by the
following 
xk     xk   

   

yk   xk   

   

where xk represents the price at time k and yk represents the observation made at time k  the process
and measurement noise  and  are assumed to be zero mean gaussian random variables where the
variance were measured using the historical data  figure   compares the filter with the esn 

results
esn was tested on googles stock price in comparison to the kalman filter  see figure     the esn
captures quick changes in the stock price whereas the simple kalman filter cannot 

stock price  usd 

   
actual
kalman predicted
esn predicted

   

   

   

   
 

  

  

  

  
  
  
days after training

  

  

  

   

figure    google stock price prediction for esn and kalman filter  esn predicts rapid changes in stock
 yy
  
price wheras kalman filter tends to smooth rapid changes  test error    target   for the esn is         in
y
target

contrast  test error for the kalman filter is        
the esn was also tested using    randomly selected stocks from the s p    to see if it generalize
well to other stocks      it is interesting to note that we dont see an increase in error as we forecast
more and more into the future  however  it makes sense that the error does not increase over time so
long as the underlying dynamics of the stock during the testing period is the same as during the training
period  if there were a major variation in our testing period not accounted for in our training period
than we would expect big errors 

 

fivariance normalized squared error

   

    

    

    

    

 
 

  

  

  

  
  
  
days after training

  

  

  

   

figure    esn performance on    randomly selected stocks from the s p      red marks indicate median
and blue bars extend between the       percentiles of the data 

discussion
time series analysis gained attention when george box and gwilym jenkins published their arma and
arima models for time series forecasting      esns provide a powerful black box method for modeling
time dependent phenomena  black box methods are appealing because they require no prior assumptions
or knowledge of the underlying dynamics of the time series data as with the kalman filter  the kalman
filter does not have enough features to predict prices and capture rapid movement in the stock price 
one would have to build a more complex model  but in that case one needs to estimate the state matrix 
i e  one needs to understand the dynamics between future prices and past historical data 
the power of reservoir computing and the relative dearth of theory and understanding indicates that
reservoir computing is a rich field opportunity for study  when we predicted the future prices of   
stocks with our esn we also observed higher error in periods with more volatility shown by a higher
volume of trading  this was likely exacerbated by the fact that we trained and tested on the same time
periods for all the stocks which means that our training set may not have captured the phenomena that
occurred in the market during the time of our testing set  in order to mitigate this  we would like to
train and test the    stocks at different time periods so that all of them do not suffer from volatility at
the same time which makes our predictions inaccurate 
due to time contstraints  we were only able to play with a few of the parameters of the esn  in
our future research we would like to explore what the highest frequency signal we can match with our
esn  according to              this involves a complex interplay between the spectral radius of w and
the number of reservoir neurons in our network  additionally we would like to explore how changing the
feedback weights w f b and the input weights w in affects the network 

references
    g  e  p  box  g  m  jenkins  and g  c  reinsel  time series analysis  forecasting and control  volume
     wiley       
    h  jaeger  the  echo state  approach to analysing and training recurrent neural networks with an
erratum note  tecnical report gmd report            
    h  jaeger  tutorial on training recurrent neural networks  covering bppt  rtrl  ekf and the  echo
state network  approach  gmd forschungszentrum informationstechnik       
    wolfgang maass  thomas natschlager  and henry markram  real time computing without stable states 
a new framework for neural computation based on perturbations  neural computation             
      november      

 

fi