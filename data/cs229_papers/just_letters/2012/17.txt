 

simultaneous co clustering and learning with
dimensionality reduction
harish ganapathy
cs     project report
e mail  harishg utexas edu

abstractin this project  we are interested in solving predictions problems involving dyadic data  the netflix problem 
where data may be organized into a matrix with the rows
representing users  the columns representing movies and  some
of  the matrix elements containing ratings  is an example of
a setting where the data is naturally dyadic  the prediction
problem here is essentially to fill in the missing entries of
the matrix with predicted ratings  recently  a technique called
simultaneous co clustering and learning  scoal  was proposed
that was demonstrated to be effective in solving such dyadic
prediction problems  scoal is essentially a divide and conquer
approach that seeks to form clusters of feature vectors and build
a separate local model for each one of these clusters  such an
approach inherently suffers from the risk of overfitting  especially
when the number of clusters is large  thus  to alleviate the effects
of over fitting  while at the same time  benefitting from the use of
multiple models  we propose the use of dimensionality reduction
in conjunction with scoal  we investigate two dimensionality
reduction techniques  principle components regression and the
least absolute shrinkage selection operator  lasso   we show that
both these techniques provide significant reduction in test error
 up to     in some settings  when linear predictive models are
employed within each cluster 

i  i ntroduction
in dyadic prediction problems  the training data consists of a
pair of objects called dyads  with associated labels  the goal
is to predict labels for the dyads that have missing labels 
apart from the popular netflix problem  another example of
a dyadic prediction problem arises in marketing where we are
interested in predicting whether a person will purchase an item
given a history of purchases  in such settings  it is of interest
to determine clusters of similar users and or similar movies 
the technique of co clustering  which simultaneously clusters
both axes  naturally lends itself to solving such problems 
co clustering has been used in other problems such as text
clustering and micro array data analysis that involve dyadic
data  once the clusters  are determined  one may build a
predictive model on a per cluster basis  such an approach finds
its roots in the general wisdom that partitioning the feature
space into multiple homogeneous segments and building more
accurate models for each segment  often works well  it follows
that such a technique would be most effective when the
underlying generative model for the data resembles a mixture
of gaussians model for instance 
a recent paper by deodhar and ghosh     proposes a novel
extension to the above sequential approach of co clustering followed by learning  they propose a simultaneous co clustering
and learning algorithm that essentially recognizes that the
ratings or labels inherently contain information pertaining to
  we

use the terms clusters and co clusters interchangeably in this report 

the similarity amongst the users and must hence be used
in the formation of co clusters  since the optimal joint coclustering and learning algorithm is of course np hard  the
proposed greedy algorithm alternates between  i  learning the
best models given a co clustering and  ii  learning the best coclustering given a set of local models  this algorithm is implicitly made possible by the fact that the model that predicts
a given matrix element  operates on the corresponding row
 user  and column  movie  attributes  therefore  the divideand conquer based prediction algorithm proposed by deodhar
and ghosh     is applicable to settings where such additional
attribute information is indeed available 
while divide and conquer is often an effective approach to
improving prediction performance  one has to guard against
over fitting the training data as the number of co clusters
increases  equiv  as the cluster size decreases   this is especially critical for high dimensional data  attributes  where it
would be quite easy to exactly fit a small number of labels
 located inside a small co cluster   to address this challenge 
in this paper  we propose to extend the algorithm introduced by
deodhar and ghosh     to include dimensionality reduction 
this means that when building local models for each cocluster or sub matrix  we include an additional dimensionality
reduction step  which reduces the size or number of parameters that constitute the model  we consider two methods of
dimensionality reduction that overlay the scoal algorithm 
 i  principal component regression  pcr    given a cocluster with a set of user movie attribute vectors  we retain
only the strongest eigenmodes  through singular value decomposition  of this set of user movie attribute vectors and build a
model from this set of eigenmodes to the labels  the number of
eigenmodes that is retained is determined by an upper bound
on the eigenmass  which is an additional input to the algorithm 
thus  this approach exploits any low dimensional structure
that may be present in the feature vectors belonging to the
co cluster 
 ii  lasso regularization   one may more directly
eliminate attributes that are not useful  i e   not correlated
to the labels  by using a more standard technique such as
regularization  while the scoal framework in general
accommodates any type of model within a co cluster  lasso
is applicable specifically when linear regression classification
is used within each co cluster 
we demonstrate the gains of dimensionality reduction in
the context of a recommender system application  the main
results in the report are as follows 
 we consider the well known movielens dataset that was
made available by the grouplens research project at the

fi 





university of minnesota      we show that as the number
of clusters increases  both lasso and pcr decrease the
mean squared test error when overlaid on plain scoal 
in particular  pcr provides gains of up to     in as the
number of clusters increases 
we consider the erim data set  widely used in the
marketing community  consisting of household panel data
collected by a c  nielsen      we show that scoalpcr with linear local models does not offer any gains
over plain scoal in this setting 
we study the use of pcr in conjunction with non linear
local models  in particular  we apply regression trees
within each co cluster  as with the erim data set  we
show that scoal pcr does not offer any gains here 
ii  p roblem d efinition

we describe the problem formulation in this section  there
are a total of m customers users and n items  matrix z
contains the ratings that the users have given the items 
zmn  r  denotes the rating given to item n by user m 
it is also possible that a user has not rated an item  this gives
rise to a natural matrix prediction problem  where given a set
of ratings  we are interested in predicting in the missing cells 
let matrix w with
 
   cell  i  j  has a missing entry
wij  
   
   otherwise
encode the locations of the missing ratings  let pi  rd    i  
              m   resp  qj  rd    j                 n   be a d  dimensional  resp  d   dimensional  feature attribute vector that
is associated with customer i  resp  item j   thus  associated
with each user item   dyad  we
a cumulative
 t can dassemble
 r    d       the regression
feature vector xij     pti qtj
problem of interest is to find a map from the set of features xij
to the positive reals such that the missing ratings are predicted
accurately  in the sense of mean squared error 
in the next section  we describe the scoal algorithm that
breaks up the data into smaller co clusters or sub matrices and
builds a model within each co cluster 
iii  s imultaneous co   clustering and learning
with dimensionality reduction

we introduce the following notation to facilitate coclustering  let k and l be the total number of row and column
clusters respectively  in particular  let ck   k                 k
denote the k th column cluster and rl   l                 l denote
the l th row cluster  for convenience  let r                  m  
               k  and c                  n                  l  denote
mappings from the customer  resp  item  index to its associated
row  resp  column  cluster  k and l are treated as inputs to
the problem in our formulation  making the presented results
applicable to settings where model selection is not feasible 
for a given k and l  the regression problem that scoal
seeks to solve can be written as
m n
t
 
min
i  
j   wij  zij   c i r j  xij  
   
w r t ci   rj    ij   i              k  j                 l 
the above problem is np hard and hence  scoal is essentially a greedy approximation algorithm  note that while    
assumes the use of linear prediction   i e    tc i r j  xij   within

each co cluster for notational simplicity  we may in general use
any predictive model here  in algorithm   below  we reproduce
the pseudocode for the algorithm from      as motivated in
algorithm   scoal
   input  convergence error threshold   z  w   random co clustering

ci   rj   i              k  j                 l 

   while true do
  
build models 
  
for k              k do
  
for l              l do
  
train linear regression model  kl given data points
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   

 xij   zij    i  ck   j  rl   wij     
end for
end for
update row cluster assignment r    assign each row to cluster
that minimizes row error 
for i              m do


r i    arg mink       k l
j  
s c s  j wis  zis 
 tkj xis   
end for
update column cluster assignment c    assign each row to
cluster that minimizes row error 
for j              n do


c j 
 
arg minl       l k
i  
t r t  i wjt  zjt 
 tli xjt   
end for
measure error at iteration n and store in error n  
if  error n   error n        then
break while loop 
end if
end while

the introduction  the original approach in algorithm   may
tend to overfit the data especially as the size of the co cluster
decreases  equiv  as k and l increase   in order to address
this issue  we propose two variants or overlays to the above
approach 
the first variant  called principle components regression
 pcr       essentially seeks to exploits any low dimensional
structure that may be present in the set of features attached to
the particular co cluster  the technique projects the features
onto a lower dimensional space and performs the regression
using these latent features  here  each latent feature is a
linear combination of all the features 
the second variant is a more standard regularization approach called lasso      here  an    penalty term is added
to the objective function in     in order to encourage sparsity 
whereas pcr tries to succinctly summarize all the features in
the form of a few latent features  lasso attempts to eliminate
features that are not useful 
the two proposed scoal overlays are presented below in
more detail 
a  principal components regression
we describe the pcr algorithm for a single co cluster 
consider an arbitrary co cluster  ci   rj   and let s  
               ci   rj   denote a serialized index obtained by counting top down through each column of the co cluster  similarly 
given a matrix a  rmn   let vec a   rmn  denote
the natural vectorization of this matrix by counting top down
through each column of the matrix  for convenience  let a s 
denote a sub matrix a formed by retaining rows indexed by
set s  next  let x   x  x        x ci   rj    t be a matrix that is

fi 

formed by collecting together all feature vectors belonging to
a co cluster  let skl    i   wi      i                  ci   rj   
index the set of rated items in co cluster  k  l   the basic idea
of pcr is to use the svd to form a set of latent features
in a lower dimension  to that effect we decompose x skl  
as x skl     u v t where u t u   i  v t v   i and 
contains the singular values                d    d    
arranged in descending order  the columns of t   u  are
often referred to as the principle components  we then form
a rank p approximation
x skl    tp vpt  

   

where tp and vp retain the first p columns of t of v
respectively 
in the context of our algorithm  we are interested in controlling the rank of the approximation through the eigenvalue
mass  in other words  given an upper bound           we
choose rank p such that
 
 
p
 

c   c
p   min p   d   d
  
    
c 
c  
 represents the fraction of eigenvalue mass that should be
retained  an advantage of working with eigenvalue masses
instead of direct rank values is that this allows the model to
retain a fixed amount of information since eigenvalue masses
may be treated as proxies for the amount of information in a
particular direction  with rank p chosen as described above 
we may form the low rank approximation tp    to x skl  
and then solve the following pcr regression problem
min
w r t

  y  skl    tp   t    
 

   

within co cluster  k  l   where y   vec z   the closed form
solution to     is well known and is given by
        tp   t tp      tp   t y  skl   

linear models  the lasso regression problem within each
co cluster can be written as
m n
t
 
min
i  
j   wij  zij   xij            
   
w r t 
for some appropriately chosen       where         denotes
the    norm  as  increases  the amount of sparsity increases
at the cost of training accuracy  the scoal lasso variant
may be obtained by replacing step   in algorithm   by     
note that  in contrast to pcr  lasso is only applicable when
using linear models inside each co cluster 
in the next section  we present some preliminary results
that quantify the performance of the proposed dimensionality
reduction techniques 
iv  r esults   r ecommender systems
we first apply the algorithms to the movielens dataset 
the dataset consists of          ratings from     users and
     movies  each user has rated at least    movies  in the
interest of expediting the run time  we reduce the size of the
data set  we choose the top     users that have rated the
most number of movies  following this initial pruning step 
we prune further by choosing the top     movies that have
the most number of ratings  this amounts to retaining     of
the initial data set  of the user attributes that are available  we
consider age and occupation  the occupation is represented
as a binary vector on a pre specified set of occupations  e g  
administrator  artiste  doctor  etc    the movie attributes that
we use are release date and genre  the genre is again a binary
vector on a pre specified set of genres  e g   action  adventure 
animation  etc    the feature dimensions are d       and
d       for this application  in addition  we pre process each
feature vector by subtracting away the mean and dividing by
the standard deviation 

   

note that while the svd does indeed place an added computational burden  the model in     can be computed efficiently
since the columns of tp are orthogonal  thus  the scoalpcr variant to scoal may be obtained by replacing all
features in algorithm   by the corresponding latent features 
once the models are constructed  we can now turn our
attention to the prediction problem  in order to predict the
missing entries  i e   those indexed by   i  j    wij      
we first project the feature matrix xtest onto the lowerdimensional space ttest      xtest vp    and then apply the
regression model to the transformed features to obtain the
predictions vec z    ttest         the error within cocluster  k  l  is then measured as
 
  vec z   vec z      
 skl  
having described the pcr overlay algorithm  we now move on
to the lasso approach that we review briefly as it is already
well studied in the literature 
b  lasso regularization
as mentioned earlier  while pcr seeks to retain  in part 
the information contained in all features  lasso on the other
hand tried to discard features by promoting sparsity in the

a  linear local models
in our first experiment  we apply linear regression within
each co cluster and evaluate the performance with and without
dimensionality reduction through pcr  the experiment is
conducted as follows 
 step    we choose some  k  l  
 step    we partition the data into training and test data 
this is accomplished by designating     of the available
user movie pairs as training and the remaining available
data as test  denote this partition by p   
 step    we vary the eigenvalue mass  across set
                           and for each value  we record the
training error train   p     p    under scoal pcr 

 step    we choose the value   p    with the minimum
training error 
 step    once chosen  we apply scoal pcr with eigenvalue mass  to the test data and record the test error
e   p     p    
 step    we then repeat steps       and record
e   pi    pi   under different train test partitions pi   i  
                 
 step    we finally compute the average test error
  
 

test  k  l      
k   e   pk    pk   and the associated standard deviation 

fi 

step    we repeat the above experiment under scoallasso and record the corresponding average test error 
 step    finally  we repeat the above experiment with plain
scoal in algorithm   and record the corresponding
average test error 
in figs   a    f   we compare the test error test  k  l  under
scoal  scoal pcr and scoal pcr for different configurations  k  l                                           we can
clearly make the following observations from the plots 
for the scoal algorithm  for fixed small values of k such
as k         in figs   a  and   b   as l increases  we first
see a decrease in test error due to a better model fit  however 
as we approach large l  i e   employ a large number of local
models  we see the effects of over fitting  for larger values of
k such as k            since we are already in the regime
of over fitting  as l increases  we only see further over fitting
and an increase in error as shown in figs   e  and   f  
the story is quite different with the scoal pcr algorithm however  from figs   a    f   we see that scoalpcr effectively combats over fitting  in particular  scoalpcr significantly outperforms scoal as l increases  the
performance gain are quite pronounced as  k  l  increases
and is plotted in fig   at l       we see that for large k 
the decrease in test error is roughly      thus  by using the
pcr overlap  we are able to derive the dual benefits of using
multiple local models while guarding against over fitting 


  

percentage gain    

  

    

k      scoalpcr

    

k      scoal

 

 

 

 

  

  

k

fig     decrease in test error between scoal pcr and scoal as a function
of k at l      

performance lies between that of scoal and scoal pcr 
note that the lasso solution is computationally less demanding than scoal pcr since the latter involves calculating the
svd of the feature matrix  nevertheless  while  some of the
gains may be attributed to the additional computation  the
remainder is due to the fact that pcr summarizes features
and retains as much information as possible while lasso
eliminates features  before we move on to the next section 
    
scoalpcr
lasso
scoal

    

 k l 

    

train

   



    

    

    

    

    

    

 

 

 

   

 
k

 

  

  

test

test k l 

 

 

k      lasso
k      scoalpcr
k      scoal

    

    

 

 

    

    

 

    

    
    

fig    

    

    

training error versus number of rows clusters l with k     

   
    
    
    

    
 

 

 

 
l

 

  

  

 

 

 a  k    

 

 
l

 

  

  

 b  k    
    

    
    

k      scoal

    

k      lasso

    
    

test

   

    



test k l 

   

    

    

    

    

    

    

    
    

    
    

k      lasso
k      scoalpcr
k      scoal

    

k      scoalpcr

    
 

 

 

 
l

 

  

  

 

 

 c  k    

 

 
l

 

  

  

b  results on erim dataset

 d  k    
    
k       lasso
k       scoal
k       scoalpcr

    
k       scoalpcr
    

    

k       scoal
    

k       lasso
    

test

test k l 

   
   

    
    
    

    

    

    
    

    
 

 

 

 
l

 

 e  k     

  

  

 

 

 

 
l

 

we plot the training error under scoal  scoal lasso and
scoal pcr for the cases l     and k                       
the training error under scoal pcr is seen to be the highest
while the test error in figs   a    f  at l     is also seen to be
the largest  this could indicate high bias due to the fact that
compressing the feature dimension simplifies the model  that
said  more investigation is needed here  an analysis of the
eigenvalue mass that is retained in each  k  l  configuration
would shed light on the precise amount of compression that
is being applied at each stage 

  

  

 f  k     

fig     average test error versus number of column clusters l for the three
algorithms on the movielens data set 

finally  we have also plotted the test error due to the lasso
algorithm in figs   a    f   we see that as  k  l  increases  its

next  we compare the performance of scoal vs  scoalpcr  i e   step   step    on the erim data set  consisting
of household panel data collected by a c  nielsen      in
particular  the data set contains past information about     
products that were purchased by a set of     households  if
one were to ignore households that have made less than two
purchases   there are three product attributes   market share 
price and the number of times the product was advertised  the
household attributes are income  number of residents  male
head employed  female head employed  total visits and total
expense  thus  the feature dimensions are d      and d     
for this application  as was done with movielens  we preprocess each feature vector by subtracting away the mean and
dividing by the standard deviation  element  i  j  of the data
matrix contains the number of units of product j that was

fi 

purchased by household i  the data matrix is sparse with
around     of the values being zero  the data matrix also
contains outliers   while        of the values are below    
some values are very large and range up to     
the results are provided in figs    a    d   we see that
the pcr overlay is not as effective in this setting  more
specifically  for lower values of k such as k         
scoal pcr underperforms scoal while for larger values
k          the test error is roughly the same  there might be
multiple reasons for this trend  firstly  the feature dimension
is already quite small  nine features  in the case of this data
set and furthermore  the sparsity in data might lead to poor
pcr models in the latent  compressed  feature space  further
investigation is needed here to determine the root cause for
the lukewarm performance of pcr 
   

   
k      scoalpcr
k      scoal

k      scoalpcr
k      scoal

    

    

    

    

scoal pcr
algorithm
scoal
scoal
scoal
scoal
scoal pcr
scoal pcr
scoal pcr
scoal pcr
scoal pcr
scoal pcr

versus

table i
scoal with regression

 k  l  configuration
     
     
     
     
     
     
     
     
     
     

trees  

average test error
      
      
      
      
      
      
      
      
      
      



test

    

test

    

we repeat steps   step   for an assortment of cluster sizes 
the results are presented in table i  as with the earlier results
with linear models on the erim dataset  we do not see any
significant gains due to pcr  a more comprehensive analysis
is necessary at this point but was not possible due to lack of
time 

    

    

    

    

    

    

v  c oncluding remarks

    

 

 

 

 
l

 

  

    

  

 

 

 a  k    

 

 
l

 

  

  

 b  k    

   

   
k      scoalpcr
k      scoal

k      scoalpcr
k      scoal

    

    

    

    


test

    

test

    

    

    

    

    

    

    

    

 

 

 

 
l

 

  

  

 c  k    

    

 

 

 

 
l

 

  

  

 d  k    

fig     average test error versus number of column clusters l for the three
algorithms on the erim data set 

in the next section  we revert back to the movielens data
set but this time however  we brief study the use of non linear
local models 
c  non linear local models
while the scoal algorithm as presented in algorithm  
uses linear regression as the predictive model within each cocluster  in its most general form      it is a framework that
accommodates any model within each co cluster 
we briefly study the impact of employing pcr dimension
reduction in conjunction with regression trees inside each cocluster  in other words  we first reduce the dimension of the
feature matrix x as per     and then construct a regression
tree  the prediction that is made at any given leaf consists of
the centroid of all the training feature vectors that percolate
down to that leaf  we control the size of the tree  and hence
the tendency to overfit  by enforcing a split criterion   a
node is allowed to split and add another level to the tree only
if the number of training feature vectors that percolate down
to that node exceeds     of the total size of the data in the
co cluster  the number of rows of feature matrix x  

in this project  we have shown that dimensionality reduction
may be used effectively in conjunction with the scoal
divide and conquer approach  in particular  pcr based dimensionality reduction performs increasingly better as the number of co clusters increases  scoal pcr outperforms plain
scoal by almost     as we approach     co clusters when
using linear models inside each co cluster  thus  in large data
applications where it might be too expensive to perform model
selection and select a suitable  k  l  configuration  it might be
a safe bet to employ the scoal pcr algorithm with a large
number of co clusters  lasso on the other hand provides
a low complexity alternative to pcr while paying a price
in terms of test error  lasso based feature selection does
still outperform plain scoal as the number of co clusters
increases 
further research is needed to systematically determine and
quantify the reasons for the gains  or lack thereof  due to pcr
and or lasso  as mentioned earlier  a detailed analysis of the
eigenvalue mass that is retained for each  k  l  configuration
would give us some more insight  finally  it is worth considering alternate dimensionality reduction techniques such as
partial least squares  which projects the features onto a basis
that captures most variance across both the features and the
labels  principle components regression ignores the labels and
finds directions that maximize the variance amongst only the
feature vectors 
r eferences
    m  deodhar and j  ghosh  simultaneous co clustering and learning
from complex data  proc    th acm sigkdd international conference on knowledge discovery and data mining  august      
    i t  jolliffe  a note on the use of principal components in regression 
journal of the royal statistical society  series c  applied statistics   vol 
    pp                
    movielens data set  http   www grouplens org system files ml data tar
  gz  university of minnesota 
    r  tibshirani  regression shrinkage and selection via the lasso  j  roy 
statist  soc  ser  b  pp                
    kilts center for marketing  erim database  http   research 
chicagobooth edu marketing databases erim  

fi