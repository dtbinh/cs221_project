learning to detect information outbreaks in social
networks
jiayuan ma
jiayuanm stanford edu

xincheng zhang
xinchen  stanford edu

stanford university

stanford university

   introduction
this is the information age  everyday everyone in the world is receiving tons of information on an unparalleled scale  and no one
could afford to consume all the incoming information  however 
almost everyone is eager to know every ongoing hot topic in the
world in order not to be left behind in the age of information explosion  one simple way to resolve this dilemma is to keep ones
eyes on some trusted influential nodes in the information diffusion
networks  so that one can keep himself herself updated with the information from these sources  the general problem of detecting
outbreaks in networks asks how to select a set of nodes to detect
some dynamic diffusion spreading process over a network 
many real world problems can be modeled under this setting  in
the domain of weblogs  such as twitter   bloggers publish  tweet 
posts and use hyper links  retweet  to refer other bloggers posts
and contents on the web  under this circumstances  we want to
select a set of blogs to read so that we can keep ourselves updated
with most of the stories  whether it can be linked or direct posts 
that propagate over the bloggers network  this is also an example
of a outbreak detection 
previous researches focuses on modeling the information cascades
directly on the network structure  see section       in this family of models  higher degree nodes are more likely to participate in
a cascade  while lower degree nodes are less likely to do so  yet
in reality  content information also matters  the probability of nodes participating in a cascade is relevant to the type of information they receives  in the bloggers example  an individual blogger
may be a professional sports fan who only cares about sports stories news  in this case  he she may prefer to reading and spreading
 re tweeting  sports news  and these people are only likely to participate in sports related information cascades  and this makes them
unlikely to be involved in a politics related information outbreak 
this project presents learning to detect outbreaks  which studies
how to incorporate content related information into normal outbreak detection  we propose a learning framework that allows for
modeling outbreak detection with content information  and evaluate our model on real world collaboration network and twitter network  furthermore  because we take into consideration the content
information flowing on the network  we can use some standardized
machine learning techniques to predict if a given specific piece of
content information will cause an information outbreak  we will also show some quantitative results on this outbreak prediction task
as well 
this paper is organized as follows  in section      we give the definitions of basic diffusion models where the rest of paper depends

upon  in section    we give the formulation of our problems  we
provide our approach to solving the problem in section    we report our empirical results in section   and wrap up with section  
on what we plan to do in future  due to the page limit  we omit the
related work section in our final report  we refer interested readers
to our milestone report for related work 

   

basic diffusion models

in kempe et al     s highly cited paper  two basic information diffusion models have been generalized  independent cascades  ic 
model and linear threshold  lt  model  both of them are the most
widely studied diffusion models 
given a graph g    v  e  w   where v is a set of n nodes  e 
v  v is a set of m directed edges  and w   v  v         is a
weight function such that wu v     if and only if  u  v  
  e  we
start the diffusion process with an initial set of active nodes s    and
the process cascades in discrete steps i                    let si denote
the set of vertices activated at step i  the whole process stops at t
when st     ic and lt only differ in how every individual node
is activated  and we will explain their differences 
   independent cascades  ic   if a node u first become active
in step i  it is given a single chance to activate each of its inactive neighbor v  with the probability of success being
wu v   each successfully activated node v will become active
in step i      notice that this process is unrepeatable  we
cannot make any further attempts on the same edge 
   linear threshold
 lt   an lt influence graph further as
sumes that vv wu v    for every u  the dynamics of
lt proceed as follows 
each node u has a threshold u which is uniformly distributed in the interval         which models the uncertainty of individuals conversion threshold 
the node u is activated when the weighted sum of its activated neighbors v is no less than the threshold u   mathematically  node u will become active if

wu v  u
   
v ji  sj

  

problem formulation

given a social network structure g    v  e   we want to select a
subset a  p v   of nodes in the network such that a is solution
for
max  

av

subject to  

r a 
c a   b

   

figiven in figure   
comparison to previous approaches  in the previous researches 
the probability of pu v is only relevant to the degree information
of node u and v  for example  in independent cascade models 
the whole graph g    v  e  is parameterized by a single uniform
probability p to each edge of the graph  usually from    to     
in separate trials  in linear threshold models  each edge from u to
v is assigned probability as pu v     dv where dv is the degree
of node v  unlike these traditional approaches  we bias the assignment of pu v based on the content correlation of node u and v  in
our project  pu v s are no longer simply determined by the network
structure  and they are related to the proximity in the content space 

  

figure    an illustration of blogger network  each blogger is denoted by a rectangle  and each post tweeted posted by bloggers are
represented by colored circles  all the circles enclosed by the blogger rectangle are the tweets posts published referred by this blogger  the links between circles denotes the reference relationships 
the color information on each post circle denotes the type of content topic the post belongs to  the color of the bounding rectangles
in this picture means the latent topic distribution for each individual
bloggers  the topic distribution of each bloggers are also displayed
in this diagram 

model and algorithm

in this section  we describe our approach of incorporating content information in the task of outbreak detection  section     introduces two unsupervised approaches of modeling content space 
which are used to aggregate information content into different highlevel topics  section     describes submodular optimization  which
is used in our project as the infrastructure of outbreak detection 
section     give a description of our approach of predicting the
outbreak of a given specific piece of information 

   

     
where r is a submodular set function defined on r   p v    r
according to some measure  in terms of the number of nodes affected or the reduction of time spent on detecting outbreaks  and c x 
is a non negative cost function defined
 on each vertex c   v  r 
c a  is further defined as c a    sa c s   and b is a budget
we can spend for selecting the nodes  in this project  we use the cardinality of the seed node set to define of c   such that c a     a  
the above is the original outbreak detection formulation  and we
incorporate content information c into the formulation below  given a social network structure g    v  e  where each node v  v
is attached with content information cv  c   the probability of
information will propagate from node u to node v through edge uv
is given by
pu v   sim cu   cv  

   

where sim is defined as a function sim   c  c         which
measures the similarity of content information cu and cv corresponding to node u and v  and normalizes into a valid probability
value  as we can see from the definition  the more similar contents
cu and cv are  the more likely information will spread between u
and v 
the problem boils down to finding a reasonable representation of
cu for each u  v quantitatively  in our project  we use the content
history of a node for quantization  in the case of collaboration network  the content history of a node is all the papers the author node
has published before  in the case of twitter network  the content
history of a node is all the posts the node has published retweeted
in the past  the rationale of this formulation is based on the assumption that nodes that share a similar content history are more
likely to propagate similar types of information  an illustration is
 
in section    we will see that we characterize the content space c
using n dimensional vector space rn  

content modeling

in our project  we used two popular unsupervised techniques in
natural language processing to model the property of information
content that propagates over the network 

latent semantic analysis

for the papers in collaboration network  we use latent semantic
analysis  lsa      to carry out content modeling on the this network data  each node in this network represents an author in the
research community  and the content attached to this node is the
paper information the author published in the past  to simplify the
process  we only use the title of the published papers as the content
information in our project 
we filter out a list of stop words  like a  the  of etc   from the
titles of those papers and treat the rest words in the titles as terms 
from above  we know we are just using the titles of those papers as the content information  and we will just build corpus from
each document  title  and its terms  before we actually using the
latent semantic analysis  we did the term frequency times inverse
document frequency  tf idf   weighting on corpus to decrease the
influence of common words appearing in every documents  the
term frequency tf  t  d  of a term t with regard to document d is
simply defined as the raw frequency of t in document d  the inverse document frequency is obtained by dividing the total number
of documents by the number of documents containing the term t 
and then taking the logarithm of that quotient  therefore  the idf of
term t with regard to the set of all the documents d is defined as
idf  t  d    log

 d 
  d  d   t  d  

   

the next step of lsa is to perform singular vector decomposition
 svd  on the term document matrix x  where xtd is given by
tf  t  d   idf  t  d   by using svd  we have
x   u v t

   

by only retaining the k largest singular values and their corresponding columns in the u and v matrices  we have a low rank approximation of the matrix x
x  x   uk k vkt

   

fiby doing this  we are mapping features in high dimensional space
into a much lower dimension space  k in this example  while preserving as much content information as possible  when projecting
to the lower dimension space  we can represent each title with vectors in rk space  in practice  we choose the lower dimension k
as    to balance between the computational feasibility and information preserving quality  after acquiring the content vector  we
employed spectral clustering on those vectors to label the latent topic for each paper title  therefore  we can quantize each node
 author  with a distribution on a latent topic space which we mined
from the content information 

      latent dirichlet allocation
though lsa is powerful and informative  but it requires much effort and computing resources to run and experiment with  when
very large and noisy data come in  we need a more robust model
for modeling the content information  latent dirichlet allocation
 lda      is such a probabilistic model for uncovering the underlying semantic structure of a document collection based on hierarchical bayesian analysis  the original idea of lda is to model
documents as if they arise from multiple topics  where each topic is defined to be a distribution over a fixed vocabulary of terms 
let us take twitter network as an example  in twitter network
where each node represents a twitter user  the content information
attached to each node is the tweets  microblogs  the user published
or retweeted before  the tweets may have different latent topics  such as sports  finance  politics and entertainment  we want to
model twitter users and their microblogs as a distribution over the
latent topics 
lda can be naturally applied as an enhancement of lsa  and it assigns a document with a distribution of multiple topics  the basic
pipeline of lda is linearly scanning through each word in the document  and initializes by randomly assigning some topics to each
word  afterwards  lda goes through a iterative improvement process  like expectation maximization  until it converges  for each
word  it computes the probability of topic given document and word
given topic  then we reassign each word to a new topic based on
the maximum likelihood estimation from the previous iteration  finally  this process converges and each document is assigned a distribution over the latent topic space 
for twitter data  we follow the same step as in the collaboration
network  first  we filter out stop words and http hyperlinks    secondly  we build corpus using only term frequency of the documents 
we utilize a parallel version of lda     implementation from plda package to speed up the computation latency 

    submodular optimization
following the same approach in      we approximate the submodular optimization problem     using a greedy hill climbing algorithm  in outbreak detection  the main operation when deciding
whether or not to add a node to the outbreaker set is to retrieve a
random cascade emanating from that node  for ic models  we toss
the biased random coins and remove all edges not for propagation
from g to obtain a new graph g in advance  with this approach 
the random cascade from a node v is simply the set of nodes that are
reachable from v in graph g   denoted as rg   v    this can be
achieved with a linear scan of the graph g by depth first search
 dfs  where we can obtain rg   v   for all vertices v  v   in
our implementation  we use the lazy forwarding techniques to select remaining seeds  the reason why we can use a priority queue
 
in this project  we regard the hyperlinks in the tweets as irrelevant
information 

algorithm   submodular optimization g  k  p 
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   t is the number of simulation iterations 
initialize s     t         
   build graphs in advance and initialize t heaps 
for i     to t do
compute gi by removing each edge from g with probability    p 
run dfs on gi and compute rgi   v   for all v  v  
hi   makeheap v   rgi   v     for all v  v  
for every v  v do
curvi  false
end for
end for
   lazy forwarding
for i     to k do
set sv     for all v  v  s 
for iter     to t do
while true do
v  deletemin hi  
if curvi then
break
else
decreasekey hi   v   rgi   v    rgi  s   
curvi  true
end if
end while
sv     rgi   v    rgi  s  
end for
s   s  argmaxvv  s  sv  
end for
output s 

for lazy forwarding is due to the diminishing return properties of
submodular functions  for all sensor placements a  b  v and
node s  v  b  we have
r a   s    r a   r b   s    r b 

   

therefore  the r s  computed in the previous iteration can be used
to bound r s   u   in the current iteration  for more details 
we refer the readers to the pseudocode in algorithm    the time
complexity of the submodular optimization is o  e  v     where
 e  is the number of edges and  v   is the number of nodes in g 

    outbreak prediction
with the content information being modeled  we can predict if a
specific given tweet will become popular in the information diffusion network  after running submodular optimization  we have
chosen a set of seed nodes  or sensor nodes  as the potential candidate of supernodes  because information emanating from these
seed nodes will result in a large scale cascade  these nodes are the
key to understanding the information diffusion process 
after we finished modeling the content in section      we have obtained the latent topic distribution for each of the node in the graph 
therefore  for each of these seed nodes we know exactly their content distribution  consider a scenario where one piece of specific
information  e g  a piece of tweet  come into the network  how
likely is it going to trigger a cascade of information diffusions so
that this information can reach a large portion of the nodes in the
network  we try to answer this question by evaluating the probability of these seed nodes accepting this piece of twitter information 
because we have modeled the tweets in the same latent topic s 

fi  

  

  

  

  

  

  

  

   

 

  
degree

 

  

   

    
    
    

  

  

  

 

 

 

  

  

  
  

 a  collaboration network dataset

  

seed set size

  

  

  

 a  ic p       
 

 

  

  

 

 

  
  
degree

 

  

celf algorithm
random approach
high degree approach
offline bound

   
 

independent cascade model with probability   
celf algorithm
random approach
high degree approach
offline bound

   

    

   

  

 

  

independent cascade model with probability    

    

    

target set size

   

target set size

proportion of nodes

portion of nodes

  

  

independent cascade model with probability   
celf algorithm
random approach
high degree approach
offline bound

   

degree distribution of twitter network
  

  

target set size

degree distribution of collaboration network
 

  

 

 

  

  

seed set size

  

  

   
   
   
  
 

  

 b  ic p      

 

 

  

  

seed set size

  

  

  

 c  topic sensitive

 

  

figure    sensor node placement on the collaboration network

 b  twitter dataset

figure    degree distribution of two datasets
distribution of different topics

    

user topic distribution

   

dom
crystal
biz
jack
rabble
ev

    

   empirical results
in this section  we showcase some of the experimental results we
obtained on two datasets we used in our project  we also summarize the major findings when running various experiments  furthermore  we will show the prediction accuracies for outbreak prediction tasks 

    dataset
      collaboration network data
by using the citation network in kddcup       we wrote a python
crawler that requests the arxiv api and downloads all the paper
titles  abstract and authors into our local database  part of these
information is later used as the content information  we treat all
the names with the same first letter of given name and same last
name as the same author  as the paper    pointed out  this approach
wont affect the result  we show a plot of degree distribution for
this collaboration network in figure   a   there are        nodes
and         edges in this network 

      twitter network
we are using the twitter tweets data from snap and the twitter social graph data from      instead of using the whole twitter social
graph  which is prohibitively large for computation  we downsample a subset of the original network for our experiments  the sampling process is simply a   step breadth first search  bfs  starting
from a list of celebrity nodes provided in      we use the subgraph
induced by all the traversed node in the bfs  there are         nodes and             edges in this network  and the corresponding
degree distribution is in figure   b  

    results on the collaboration network
in this section  we are showing results on the collaboration networks  figure   shows the results of running our implementation
of submodular optimization on the collaboration network  here we
use the information cascade  ic  model with probability p       
and     respectively  we run the simulation      times to report

topic distribution

number of documents

pace as the nodes in the network  we can measure the proximity
between the content of tweets and the topic distribution of nodes
in the graph  we can use these distances  like anchor distances  to
predict if this piece of tweet will be accepted by the influential seed
nodes and later forwarded to other nodes in the network 
with the help of machine learning approaches  we use the distances
between influential seed nodes and the topic distribution of tweets
to train classifiers which predict if a specifically given tweets will
trigger a large cascade of impact  or outbreaks  in the information
diffusion network 

   

    

    
    
    
    

   
   
   

   
 

t 

t 

t 

t 

t 

t 

topic no 

t 

t 

t 

t  

 a  distribution of    topics

   

 

  

  

latent topic number

  

  

 b    different users profiles

figure    topic distribution

the results  and we compare the results with the strategies of selecting random nodes  random approach  and high degree nodes
 high degree approach   we also included in our plot the curve of
the maximum possible theoretical upper bounds  offline bound  
which is simply  r s         e   as can be shown in the plots 
our results are very similar to the findings in      in figure   a  
we show the topic clustering results on the collaboration network
on high energy physics theory  the topic distribution is quite wellbalanced through all the papers we crawled from the arxiv  in table
   we also demonstrate some interesting clustering results on the
paper titles  the key words part are extracted manually from the
titles that are aggregated under the same topic label  in figure   c  
we give the result after adding content information into outbreak
detection  interestingly  we observe that compared to figure    the
outbreak detection is more efficient in a sense that we have more
activated nodes with the same size of seed nodes 

   

results on the twitter network

we preprocessed the tweets data from snap with lda  which assigns each documents  tweets  a topic distribution  after that  we
built user profile base on the document they authored  we chose the
social sensors  users  based on the submodular optimization framework  but with an enhanced by topic similarity weighted probabilkeywords
gravity
string
geometry
symmetry

paper titles
the shape of gravity
gravity in the randall sundrum brane world
four dimensional string triality
enhanced gauge symmetry in string theory
matter from toric geometry
compactification  geometry and duality  n  
second quantized mirror symmetry
mirror symmetry is t duality
table    examples of topic modeling

fi     
     

linear threshold model on the twitter data

     

celf algorithm
random approach
high degree approach
offline bound

     

     

target set size

target set size

     

     
     
     
     

     
     
     
     

    
 
 

linear threshold model on the twitter data
celf algorithm
high degree approach

  

  

  

seed set size

  

     

   

  

  

  

seed set size

  

independent cascade model on the twitter data with    probability

independent cascade model on the twitter data with    probability
celf algorithm
high degree approach

     
     

     

target set size

target set size

     
     
     
     
    
 
 

  

  

  

seed set size

  

     
     

     

   

content based outbreak detection on the twitter data

  

  

  

seed set size

  

   

     

content based outbreak detection on the twitter data

     
     

target set size

target set size

     
     

celf plain
celf topic  
celf topic  
celf topic  
celf topic  
celf topic  

     
     
     
 

  

  

  

seed set size

  

  
     
     
     
     
     
     

table    accuracy of predicting the outbreak of individual tweets

  

     

     

  
     
     
     
     
     
     

  
     
     
     
     
     
     

     

 c  ic model on twitter
     

seed node number
plain placement  baseline 
content based placement  
content based placement  
content based placement  
content based placement  
content based placement  

linear kernel
  
  
           
           
           
           
           
           
rbf kernel
  
  
           
           
           
           
           
           

     
     

celf algorithm
random approach
high degree approach
offline bound

  
     
     
     
     
     
     

   

 a  lt model on twitter
     

seed node number
plain placement  baseline 
content based placement  
content based placement  
content based placement  
content based placement  
content based placement  

     
     
     
     
     

celf topic  
celf topic  
celf topic  
celf topic  
celf topic  

     
   

     
 

  

  

  

seed set size

  

future work

in future  we are going to run experiments on the larger dataset 
which is much harder to tackle in our current framework  also 
wed like to research on an automatic way of tracking the popular
tweets  since twitter data does not contain the retweet amount and
comment amount information  it is hard to determine if a tweet is
trended  finally  wed like to extend our current model to incorporate more interesting features  like diffusion time  user preferences 
etc  

   

 e  topic sensitive results
figure    sensor node placement on the twitter data

ity graph and a plain linear threshold model graph  we fetched
tweets which are retweeted by tweetmeme account  which is outbreak as positive example and manually select some tweets which
are not trending as negative example  finally  we built a discriminative svm classifier by using those sensors as feature vector  we
trained the classifier and tested if the classifier can predict which
tweets are to outbreak  in figure    we plot the results for running
outbreak detection on the twitter network using independent cascade model and linear threshold model  we enhance the edge probability of the graph and give it as an input to the submodular optimization algorithm  we calculate the intersections between users
under one topic and we reward that overlap  the user pairs without interest similarities will have a default edge probability      if
they have a follow relation  for each topic  we generate a different
weighted graph and feed it as input to optimize  we noticed that
we actually choose different sets of users for different topics since
each topic will enhance the edge probability of different user pairs 
the plot of this set of results can be found in figure    we then
train an svm classifier based on the distance of user profile to a
tweets to see if we can distinguish those tweets with higher probability to outbreak  we achieve an approximately     successful
rate and we didnt observe a big improvement when we add content information to choose the different set of sensors  the results
are summarized in table    an example of topic distribution for
different users is shown in figure   b  

  

references

    d  blei and j  lafferty  a correlated topic model of science 
the annals of applied statistics  pages            
    w  chen  y  wang  and s  yang  efficient influence maximization in social networks  in proceedings of the   th acm
sigkdd international conference on knowledge discovery
and data mining  pages         acm       
    s  dumais  g  furnas  t  landauer  s  deerwester  s  deerwester  et al  latent semantic indexing  in proceedings of the
text retrieval conference       
    d  kempe  j  kleinberg  and e  tardos  maximizing the spread
of influence through a social network  in proceedings of the
ninth acm sigkdd international conference on knowledge
discovery and data mining  pages         acm       
    h  kwak  c  lee  h  park  and s  moon  what is twitter  a
social network or a news media  in proceedings of the   th
international conference on world wide web  pages        
acm       
    z  liu  y  zhang  e  y  chang  and m  sun  plda   parallel latent dirichlet allocation with data placement and pipeline processing  acm transactions on intelligent systems and technology  special issue on large scale machine learning       

fi