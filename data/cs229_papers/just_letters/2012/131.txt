user review rating prediction
sammy leong  ashish bhatia
dec         

objective

reviews are interleaved 

we had to throw out

a lot of positive reviews but we re still left with
given a set of user review data each of which is

abundant training examples 

associated with a user rating  the objective is to
build a model that is able to predict numerical
rating from textual data 

overview

success metric

to solve this problem        rst we evaluated
a number of learning algorithms such as logis 

our goal is to nd a learning algorithm along

tic regression  naive bayes  and svm on the

with data processing techniques that give us the

original raw data as is to get a sense of their

highest generalization accuracy  to do this  we

feasibility  capability  and speed  then we devel 

divided our data in half  trained with the rst

oped our own pre processing pipeline to trans 

half  and then tested with the second half  the

form the original data to better emphasize senti 

former gives us the training accuracy while the

mental features  which we evaluated with one of

latter gives us the generalization accuracy 

the learning algorithms 
based on our analysis  we removed several redundant and noisy features to further improve
the accuracy of our pipeline 
project

code

distribution

 excluding

is

yelp

prohibited 

data
is

whose

first attempt  baseline 

available

https   github com ashishb farsight
for our rst attempt we focused on trying out

data source

various learning algorithms on the original unprocessed data as is to get a sense of their capa 

we gathered our user review data from the yelp

bility for our given problem  another goal is to

academic data set    which contains user reviews

choose the fastest algorithm which gives reason 

of local businesses near stanford university 

ably good results and use it to iterate while we

each review contains a text sequence repre 

work on our pre processing pipeline 

senting the user review along with a numerical
rating that ranges from

  to   

the rst thing we

did was relabel each review as positive or nega 

     are labeled as
     are labeled as
results
positive  and reviews with rating   are randomly
tive 

reviews with ratings

negative  reviews with ratings

labeled as positive or negative 

    of the reviews are positive

here is a table that shows the training and gener 

which means the data is highly skewed to begin

alization accuracies for each learning algorithm 

with 

along with the time it took to run  the experi 

as it turns out 

to address this  we reshued the data

such that we have equal number of positive and

ments were done using

negative reviews  and that positive and negative

and

 

     

     

testing examples 

training examples

fitrain

test

time

     

    

     
     

    
    

     
     
     
     
     
     

   
   
   
   
   
   

misspelled words  one positive review may con 

matlab
naive bayes  mn 

     

tain the word good while another may contain
gooodi  we want to treat both signals as rep 

liblinear
logistic regression
l  reg svm  linear 

     
     

resenting positive reviews 

stemming

libsvm
c svm  linear 
c svm  radial 
c svm  sigmoid 
nu svm   linear 
nu svm   radial 
nu svm   sigmoid 
 

     
     
     
     
     
     

after spell correction  we further consolidated
words to their canonical forms by applying stemming  for example  one positive review may contain the word perfect while another may contain perfection  again  we want to treat both
signals as representing positive reviews 

nu      

per 

fect 

discussion

stopword removal

firstly  it s quite clear that liblinear runs signicantly faster than libsvm 

stem 

ming converts both words to their root 

in this step we removed stop words that are be 

for this reason 

lieved to add little relevence to reviews  words

we will iterate using liblinear when we evaluate

like the  i  was  etc  are stop words and thus

the performance of various pre processing tech 

removed  one caveat is that we did not remove

niques  in particular  we will use logistic regres 

the words no and not which will be explained

sion because it has higher generalization accu 

in the bigram generation section that follows 

racy than l  regularized svm with linear kernel 
secondly  it appears that nu svm with

   

nu   bi gram generation

in this case improved both the training and

generalization accuracies signicantly  as com 

after spell correction  stemming  and stopword

pared to the counterpart c svm results  

in

removal  we moved on to bi gram generation 

fact  the we were able to achieve the highest gen 

this step is very important because it allows us

eralization accuracy using nu svm with the ra 

to capture semantics that are not captured in

dial basis kernel  the only issue is that it takes

the uni grams or worst have opposite meaning

a very long time to run  for this reason we will

altogether  for example really good is a much

only revisit it after we ve settled on a good pre 

stronger signal for positive reviews than pretty

processing pipeline 

good or just good 

similarly  not good  if

captured inidividually contains the word good
which falsely indicates a positive review  we can

second attempt

x that by adding the bigram not good but

for our second attempt  we focused on data pro 

tains contradictory signals 

cessing and feature selection with the goal of re 

move the word good and keep only not good

ducing the noise in the data as much as possi 

which is a clear indication of a negative review 

now we have a problem where the review conto x that  we re 

ble  here we only made use of logistic regression
with

     

training examples and

     

testing

results

examples  what follows are the data processing
techniques that we used 

here is a table that shows the training and
generalization accuracies 

the individual sec 

spell correction

tion shows experiment results where we used

the rst thing we did was to apply spell check 

incremental

ing on the reviews and replace mispelled words

where we incrementally combined data process 

with suggested corrections  the idea here is that

ing techniques 

user reviews on the internet are often lled with

shows experiment results where we combined

each data processing technique individually  the

 

section shows experiment results
finally  the optimal

section

fionly data processing techniques that we believe

two and indeed we achieved training and general 

to give us optimal results 

ization accuracies that were superior to the rest 
looking at the training error vs generalization
train

error  it is as expected that by adding more train 

test

ing examples  the training error  bias  goes up

individual

       
       
       
       
       

baseline
spell correction
stemming
stopword rem
bi grams

where as the generalization error  variance  goes

       
       
       
       
       

down 

continue to go up where as the generalization
error will likely at o 

  spell correction
  stemming
  stopword rem
  bi grams

       
       
       
       
       

techniques  other learning algorithms  or other

       
       
       
       
       

tuning techniques 

further enhancements
to further rene our results  we focused on data

optimal
spell   bi grams

judging from this we

will have to either explore other pre processing

incremental
baseline

however the trend suggests that as we

add more training examples  training error will

       

processing and feature selection with the goal of

       

reducing redundant features in the data 

just

like in second attempt  we only made use of logis 

here is a graph that shows the number of

tic regression with

training examples vs training generalization er 

     

ror for the case of using only spell correction and

     

testing examples 

training examples and
what follows are the

extra data processing techniques that we tried 

bi grams 

using tri grams  and beyond 
training example vs error

we tried using tri grams  quad grams and even

    
training
generalization

penta grams  the assumption here was that the

   

it will catch phrases like not so good  was
hardly any better but it turns out that these

    

error    

phrases have really sparse density 
   

some trials  the accuracy even took a dip 

    

respecting natural phrase boundaries for bi grams

   

    

 

therefore 

we did not get any signicant gains out of it  in

earlier  we were forming bigrams across bound 

    

     

     

aries like full stop  question marks and semi 

training examples

colons  this produced features where the rst
word was a part of previous phrase and second

discussion

part of another 

individually  every data processing technique im 

ing a lot of noise and hence  we added checks to

proved the generalization accuracy but all of

ensure that we do create bi grams across phrase

them with the exception of bi grams  also re 

boundaries 

during the analysis of mis 

classied results  we realized that this was creat 

duced training accuracy  when combined incrementally  however  it appears that stemming and

importance of parts of speech

stopword removal actually reduces generalization
accuracy 

a detailed analysis of results showed that proper

from the individual and incremental experi 

nouns like pizza  pasta as well as determin 

ment results  we gathered that the most promis 

ers like which were too frequent and therefore 

ing data processing techniques are spell correc 

ended up in features 

tion and bi grams  thus we combined only those

cided to consider only following parts of speech 

 

to clean this up  we de 

fithis cleanup  though  made the feature extrac 

only data processing techniques that we believe

tion phase really slow  gave us signcant boost in

to give us optimal results 

accuracy  we used nltk    for parts of speech

train

test

baseline spell   bigrams 

       

       

tri grams

       

       

quad grams

       

       

phase boundaries

       

       

pos selection

       

       

baseline  spell   bi grams 

       

       

  tri grams

       

       

  quad grams

       

       

  phase boundaries  pb 

       

       

  pos selection

       

       

       

       

tagging 

individual

pos tag

description

example

cc
cd
ex
jj
jjr
jjs
rb
rbr
rbs
vb
vbd
vbg
vbn
vbp
vbz
wp
wp 
wrb

conjunction
cardinal number
existential there
adjective
adj  comparative
adj   superlative
adverb
adverb  comparative
adverb  superlative
verb  base
verb  past tense
verb  present participle
verb  past participle
verb  singular present
verb    rd person
wh pronoun
wh possessive
wh adverb

and
    
there is
green
greener
greenest
good
better
best
take
took
taking
taken
take
takes
who
whose
when

incremental

optimal
baseline bigrams pb pos

here is a graph that shows the number of
training examples vs training generalization error for the case of spell correction   bi grams  
phase boundaries   pos  parts of speech  selection 
training example vs error
  

less and were ignored

  

pos tag

description

example

fw

foreign word

d hoevre

in

preposition conjunction

in  of

ls

list marker

  

md

modal

will

nn

noun  singular

chair

 
 

error    

  
  
 
 

nns

noun  plural

chairs

nnp

proper noun

ashish

pdt

predeterminer

both
friend s

pos

possessive ending

prp

personal pronoun

he

rp

particle

give

to

to

to him

uh

interjection

hmm

wdt

wh determiner

which

 

 

    

    

    
    
training examples

     

     

discussion
clearly  using k grams for k   is too noisy  ignoring bi grams across phrase boundaries  like
full stop  improved the results and using natural language tool kit to tag parts of speech and

results

then removing extraneous parts of speech gave a
huge boost to accuracy of our pipeline 

here is a table that shows the training and
generalization accuracies 

the individual sec 

references

tion shows experiment results where we used
each data processing technique individually  the

incremental

training
generalization

  

following parts of speech were found to be use 

section shows experiment results

    yelp s academic dataset  on request       

where we incrementally combined data processing techniques 

finally  the optimal

section

    steven bird 

shows experiment results where we combined

toolkit       

 

nltk 

the natural language

fi    shoushan li  sophia y  lee  ying chen 
chu ren huang  and guodong zhou 

sen 

timent classication and polarity shifting 
in proceedings of the   rd international con 

ference on computational linguistics  coling      pages         stroudsburg  pa 
usa        association for computational
linguistics 
    lizhen qu  georgiana ifrim  and gerhard
weikum  the bag of opinions method for review rating prediction from sparse text patterns 

in proceedings of the   rd interna 

tional conference on computational linguistics  coling      pages         stroudsburg  pa  usa        association for computational linguistics 

 

fi