cs    project
predicting the us presidential election using twitter data
by
swathi chandrasekar  emmanuel charon  alexandre ginet
  
introduction
twitter is becoming one of the major online platforms
for expressing opinions and thoughts  as people share
more and more opinion online  we believe that the way
to collect data and point of view is going to
dramatically change in the coming years  as an
example  we would like to tackle the problem of
opinion polls in the particular context of a political
election  how can we use publically available data to
estimate elections results  how accurate is our method
in comparison to traditional opinion polls 
the fundamental goals of the project can be listed as
follows 
 analyze the sentiments from twitter feeds
containing words like  obama    romney  
 democrat  or  republican  
 identify whether a tweet expresses a positive or a
negative sentiment about a particular candidate
and achieve good accuracy on this 
 predict the overall voting intentions using large
sets of tweets 

i   building a training testing set
   collecting data
the twitter api has restricted data pulls to      tweets
at a time  we can specify hash tags and words that must
be contained within the tweets we pull  for this
particular project  we used the following keywords 
 obama    romney    democrat    republican    mitt  
 barrack  
 michelle  
  obama      
  romneyryan        mitt      to pull      tweets
every    minutes starting   th october  we used a
software called discovertext which gives a free
enterprise license to stanford students to implement
this  the database within this software has been
building up tweets and very quickly  more than a
million tweets from the week before november  th  the
day of the election was available for analysis 
   representing  processing and formatting the
data collected

for every tweet  we only analyzed the words it
contains  we didnt analyze grammar or the order in
which the words appear  so our representation of a
tweet is the vector of occurrences of particular words in
the tweet  in the rest of this paper  we will call
features the words we choose to take into account 
all tweets  both for training purposes or actual
prediction  were pre processed using r and the datamining package  tm  within it  the following preprocessing was made 





all words in tweets were first converted to
lowercase 
all punctuation marks were removed 
commonly used words like  a    an    the  which
don t contribute any meaning to the tweet were
removed 
we identified the root of each word and
modified each word with the same root to have
the same text  e g  attitude and attitud clearly
refer to the same word and our algorithm
represents these two words as attitud 
   building the label list

ultimately we want to build a training testing set   as
large as possible   where for each tweet we know with
great accuracy if it is pro obama  label     or pro
romney  label      several methods were envisioned to
automate the categorization of the training tweets but
we finally decided to manually label each of them to
achieve maximum accuracy and reliability  each
member of the team was assigned a sample of tweets
from various days before the election 
in order to simplify the problem of building the
training testing set  we decided to focus on tweets that
    mention only one candidate or party and     have a
clear positive or negative sentiment about it  hence  we
eliminated the tweets that mention no candidate or both
candidates  the tweets that have a neutral sentiment  the
tweets that are neither clearly pro obama nor pro
romney  the tweets that have a clear position for one of
the candidates but with a misleading combination of
candidate mention and sentiment 

fiwe have labeled a training set of       tweets
manually  out of which       passed the filter
described above 

naive bayes
   
mean accuracy
one random order

   

   building the training matrix

finally the size of our training testing matrix was      
x       

ii     training  with  different  algorithms  

   

accuracy

we then needed to build the document term matrix like
in problem set    for training and testing purposes  we
used two separate datasets  one set of        tweets
just to build the feature list  feature set  and our
previous set of       tweets for the actual matrix
 training set   the point was to have a feature list as
general as possible  that would generalize well to all the
tweets for prediction and that would not be limited by
the features in our training set  for both those datasets 
we chose tweets from different days so that contextual
tweeting does not bias our analysis  from the feature
set  we obtained a list of        features  we decided
to only retain the features that would appear at least  
times and ended up with a final list of       features 

   

   

   

   

 

 

  

  
  
  
  
  
  
  
percentage of labeled set used for training  for different sets for training and testing  

  

   

naive bayes model
we can see that for nave bayes  the accuracy is quite
bad  about      i e  random   this was expected  since
the nave bayes assumption is wrong in our problem 
take the following example  given that a tweet is proobama  the correlation of the words obama and
good is not zero 
   svm
support vector machines

in the following plots  the x axis is the percentage of
the labeled set used for training  using all the rest of
labeled tweets for testing   and the y axis is the
accuracy     
   naives bayes

   
mean accuracy
one random order

   

   

   

accuracy

we implemented several algorithms on our current
training testing set  the goal was to determine which
algorithm will perform the best and then apply it to our
entire dataset in part iii  we first implemented nave
bayes and a regular svm  we computed the accuracy
for different sizes of training and testing sets  for a
given percentage of training data used  we computed
the accuracy several times  one time with the given set
of tweets  chronological order  a few from each day 
and   times by randomly shuffling the order of the
tweets 

   

   

   

   

 

 

  

  

  
  
  
  
  
percentage of labeled set used for training  for different sets for training and testing  

  

  

   

svm model
first  the svm performs much better than nave bayes 
reaching accuracies of around     when we use    
of the labeled set for training and the     left over for
testing 
furthermore  we see that the set taken to be the training
set leads to different accuracies in both models  in
particular  we notice in each graph one extraneous
curve  it corresponds to when we do not shuffle the
tweets  in these curves  the accuracy is globally
smaller  this is due to the temporal correlation between
tweets  we predict better when the training and testing
sets are mixed in time  and not separated like in the
original set of labeled tweets   in other words  the

fitraining set must contain tweets of any time for better
overall accuracy  hence  we implemented the random
shuffling of tweets before all what follows  in the next
steps  we tried to improve the accuracy of svm using
feature selection 
   feature selection for svm
we used three different feature selection techniques
described in class  best mutual information  pca
 principal components analysis  and forward search 
 a  mutual information 
we implemented the filter feature selection described
in the lecture notes  based on the      selected tweets 
we ranked the      features in function of their mutual
information with the labels  then  we computed the
accuracy resulting form a svm  using     of the
labeled tweets for training and     for testing  using
the k best features  for k from   to      

 b  pca

svm with filter feature selection
 

    

   

    

accuracy

   

    

   

    

   

    

 

   

   

   

   
    
    
    
number of features kept after mutual information filter 

sticker  obama      bitch  seal  christian 
hurricanesandy 
thedailyedge 
attack 
administratrtion  security 
among them  we can distinguish different categories 
words in relation to the candidates  like
romneyryan       words that have a clear positive or
negative meaning  like bitch   and words that are
related to the particular context of this election  like
hurricanesandy   most of the significant words
actually belong to the third category  which confirms
our global approach on the problem  we did not try to
determine if a tweet had a general positive of negative
implication  but directly if it expressed an opinion on a
candidate  it happens that only mentioning benghazi
is strongly in favour of mitt romney  hence  we
abandoned the idea of using the dictionary of affects in
language  that gives an opinion score of individual
words      to classify tweets  since very informative
words like benghazi would have been given a neutral
score and thus wouldnt have helped classify the
tweets 

    

    

    

filter feature selection for svm
in the previous figure  we can see an increase and then
a decrease in the accuracy in function of k  when there
are too few features  the accuracy is low because the
meaning of many tweets is not captured  when there
are too many features  lots of the features can be seen
as noise because they are not informative on the
opinion of a tweet  finally  we reach the maximum
accuracy of     at about     features  this is an
increase of    points compared to no feature selection
at all  note that the     accuracy is a mean  the
accuracy varied between     and     for different
runs  because of the random shuffling of tweets  but
consistently occurred around     features selected 
furthermore  it is interesting to see what the most
informative features are  here are the first     in order  
benghazi  obama  romney  tcot  mitt  libya 
romneyryan      female  field  impeach 

we implemented and tested pca with svm  but for  
principal components and more  the svm does not
converge  ends at      iterations and gives about    
accuracy i e  random predictions   it seems that
combinations of features do not help  more
importantly  it is an indication that unsupervised
learning has little chances to give good results in our
problem  the distances between all tweets are small and
only few dimensions actually separate the data  like the
value of a tweet with features obama or romney  
thus  separating the data based only on the occurrence
of words didnt work  thats why we did not try to
implement other unsupervised learning techniques like
k means clustering or the em algorithm  algorithms
like svd could also have helped us identify clusters of
tweets  but since we had only a limited number of
clusters of interest  we expected the accuracy to be low
and did not pursue this method 
 c  forward search  wrapper feature selection 
we followed the same procedure as in  a  but with
forward search using svm to select meaningful
features  for a small amount of features  artifacts of
features present in too few tweets led to poor
performance  hence  we decided to begin with the   
features having the best mutual information  and then
we improved the accuracy step by step using forward
search  we reach about     accuracy with only   
features  so just a bit less than in  a   unfortunately 
time limitation and the high computational complexity

fiof forward search prevented us from performing it on
more features  the first ten words it choose  except for
the top    words of  a  are 
president 
vote 
win 
my 
endorse 
breitbartnew  play  me        get 
surprisingly enough  all the words are different from
the words    to    of  a   denote the appearance of
possessive words like my  which express a connection
to the candidates 
   online learning
last but not least  we had good reasons to believe
online learning would yield good results in our
problem  first  the svm  with feature selection  gives
about     accuracy  we argue that this good svm
result implies the samples are almost separable  with
margin  gamma   which is the condition for blocks
theorem to hold  second  the d max norm xi   is quite
small in our problem  a tweet has about    to    words
so we roughly have d     now using blocks theorem 
the number of errors in online learning should be
bounded by  d gamma     this bounded number of
errors implies that with a big training set  the accuracy
can become very high 

iii    predicting  the  elections    
we selected the svm with the top     features from
mutual information selection to analyze our entire
dataset  this algorithm proved to be the best
performing with about     of accuracy on the testing
set  although forward search achieved almost similar
accuracy with only    features  about      and would
have probably achieved better results with more
features  it was too time consuming to compute it 
   daily reports
first  we decided to apply our svm to tweets from a
given day to obtain the overall sentiment of voters on
twitter that day  we used our data from the   days
preceding the elections as well as the election day
itself  each day we analyzed a total of        tweets
out of the         collected for that day 
we obtain the following results  expressed as a
percentage of analyzed tweets 
tweets for each candidates each day 
  

hence  we tried online learning  it yielded about    
accuracy compared to     for svm  we then used
filter feature selection with online learning and it gave
the following accuracies  in function on the number of
features used  
online learning with filter feature selection

tweets in favor of each candidate

  
  
  
  
  
  
  
 

 

 

 

 

 

 

 

 

evolution day per day 

percentage of electoral tweets pro obama  blue  and
pro romney  red 

 

there are two interesting things to note here 

   

    the tweets are highly favorable to obama

accuracy

   

   

   

   

   

 

   

   

   

   
    
    
number of features kept after mutual information filter 

    

    

    

    

filter feature selection with online learning
we see some improvement  up to an accuracy of    
for     features  but the increase and decrease are not
as well delimited as with the svm  some features
introduce a fast decrease in accuracy at     and     
features 
finally  our best accuracy was reached with the svm
using filter feature selection      with     features 
and this is what we used to train on the whole labeled
set and then make our predictions in part iii 

by comparing our output to actual election results and
polls during the week preceding the election  we
understand that our tool was able to identify a strong
bias pro obama among twitter users  twitter is
definitely not an objective representation of the entire
population sentiment  it is already a very interesting
conclusion in itself  yet  as twitter keeps expanding its
user base  we can imagine that its data will become
more and more representative of the actual sentiment 
    the trend before the week of the election was in
favor of obama
this is a very interesting result and we understand the
power of our tool  we are actually capable of
identifying trends and shifts in opinion at a very
granular level  we can see from just one day to another

fihow the overall sentiment of twitter users has evolved 
no other existing opinion poll solution is able to come
up so rapidly with precise results on such a large user
base 
   overall results of the elections
in order to compute the overall result of the elections 
two different strategies could be applied here  we
could either simply use the tweets from the election
day or compute an average over data from the last
week  we have decided to adopt the second approach
to account for the fact that      people make a decision
based on their overall impression during the campaign 
not just their last word     since users dont necessarily
tweet everyday  the data from the whole week might be
a representation of more users 
in the end  our tool predicted that obama would collect
    of votes  our accuracy being roughly      in the
worse case the percentage of tweeters supporting
obama is at least      comparing to the actual result
        confirms our intuition that the twitter user
base is not representative of the actual nations
sentiment 

iv    conclusions    
our study leads to multiple conclusions  first  we
believe our tool can be used to complement the
traditional polls  on the one hand  our major
disadvantage is the bias in the tweeter users  there are  
times more people following barrack obama than mitt
romney   finding a way to balance the tweeter bias
would be a great improvement to our algorithm  but
polls also have arguably representative samples of the
population  who are the people who take the time to
answer to a pool    on the other hand  our major
advantage is the large number of tweets used in the
predictions  way bigger than what pool institutes are
able to do  this gives us a much more precise interval
of confidence  even taking accuracy into account  in
the final prediction 
then  we also discovered what were the most important
words in an election  it turns out they are the very
context specific ones  goods news is that our tool finds

those topics automatically  among the different events
and debates that occurred during the campaign  the
incident in benghazi and hurricane sandy were the
most indicative of an opinion  based on this
observation  using constraint weight svm would yet be
another way of improving our accuracy  along with
labeling more tweets and running the forward search
feature selection on a supercomputer  
lastly  the final result of the election is not based on
the nationwide opinion but on the state by state opinion
and the number of electoral college votes won in each
state  we could improve our algorithm by running it on
different states and giving a state by state prediction 
especially in the swing states 

bonus  
as a reward to the reader  here are a few tweets we
encountered during labeling 
mitt s thoughts and prayers go out to the people of ohio as
they watch tv coverage of hurricane sandy 
i don t support  obama because i need a job or healthcare  i
support him because he shines the light on the future i want
for my children 
bands will make her dance  food stamps will make her
twerk  but if romney becomes president  yall hoes will have
to work 
wtf is obamas last name  does anyone know 

references    
    cs    lecture notes  andrew ng  stanford
university
    sentiment strength detection in short informal text 
mike thelwall   al        
    sentiment analysis of twitter data  apoorv
agarwal  boyi xie  ilia vovsha  owen rambow and
rebecca passonneau      

fi