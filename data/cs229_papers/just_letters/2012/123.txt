type   diabetes mellitus classification
ayush sood  steven diamond  shizhi wang
department of computer science  stanford university
 dated  december          
we consider the problem of predicting whether a given patient has type   diabetes mellitus
using his or her electronic health records  ehr   which often lack common indicators of diabetes 
effective heuristics such as findrisc exist for detecting undiagnosed diabetes  but require that
patients be screened to collect all necessary information  we compare findrisc  restricted to
the information in our data set  with models of varying complexity  we note that an ensemble of
random forest and gradient boosting machine models vastly outperforms findrisc  furthermore 
the ensembles performance on our data is competitive with findriscs performance in the field 
we conclude that diabetes can be effectively detected using ehr alone  computer diagnosis can 
thus  complement more expensive screening 

i 

introduction

type   diabetes mellitus is a metabolic disorder that
is defined by high blood glucose levels due to insulin resistance and a relative insulin deficiency  diabetes affects
    million people worldwide      if current trends continue  the cdc estimates   in   u s  adults could have
diabetes by           type   diabetes is responsible for
    of all diabetes cases      hereafter we will abbreviate
type   diabetes as diabetes 
despite the dangers of leaving diabetes untreated  the
who does not recommend universal screening because
of the expense      diagnosing diabetes costs up to     
in lab tests      many countries screen for diabetes using
cheaper heuristics and only do lab tests on likely cases
     findrisc is one of the most widely used heuristics
     though collecting the data needed by findrisc is
simple  it still requires a screening session with patients 
this not only involves patient cooperation but also the
cost of a doctor visit 
we investigate whether undiagnosed diabetes can be
detected reliably from ehr  without screening patients
for diabetes specific risk factors  if this problem were
solved  medical providers could automatically search for
likely diabetes cases in their existing records  such computer diagnosis would cheaply and effectively improve
programs to find and treat diabetics 
ii 

data

our data set was provided by kaggle as part of the
practice fusion diabetes classification challenge     
the data set contains anonymized medical records for
over        patients  with labels indicating whether or
not each patient was diagnosed with diabetes  we divided the data set into test and training sets through
stratified sampling 
all obvious indicators of diabetes besides the diagnosis 
such as blood glucose levels  were removed from the data 
the data set also lacks many features used by findrisc
and other common screening heuristics  such as patient
diet  physical activity  and family medical history  the
data set thus simulates the ehr of a medical provider
that has not screened patients specifically for diabetes 

a  icd  codes
the data set included a diagnosis history for each patient  these diagnoses were represented using the icd 
encoding scheme      the scheme has a four level hierarchical structure  where specific diagnoses fall under
broader classes of conditions      for example  the diagnosis essential hypertension falls under hypertension
and diseases of the circulatory system  we generated
features not only for each icd  code that appeared in
our data set but also for the higher level conditions that
the code fell under 
b  ndc codes
the data set had records of all the medications each
patient was taking  these medications were identified by
their ndc codes  which are unique product identifiers
for drugs      to get useful data out of the ndc codes  it
was important to map them to the conditions that they
treated  that is because there are many drugs that treat
the same underlying condition   thus we must cluster
them together 
to tackle this issue we mapped each one of the ndc
codes to the active principle ingredient in the drug and
then mapped each one of those active principle ingredients to the underlying condition it treated  for example 
the ndc code           refers to a drug mevacor 
whose active principle ingredient is lovastatin  which
is a chemical used to treat high cholesterol  thus we
were able to map          ndc codes into a condensed
   dimensional feature set 
c  general health
for each patient  we were given a full record of every single doctor visit they had between the years     
and       each doctor visit provided us with basic information such as weight  height  bmi  systolic blood pressure  and diastolic blood pressure  the provided data
was very noisy  therefore  each visit was sanity checked
and cleaned  more specifically  all outliers were removed
and the bmi was recalculated 
lastly  we created features to capture the frequency of
doctor visits and the types of medical specialists visited 
lab tests  and general information such as age and sex 

fi 
d 

pre processing

table i  logistic regression

because the data set was highly skewed  with nondiabetic patients outnumbering diabetics by almost   to
   we duplicated each diabetic patients records   times
in the training set  we also normalized all continuous
features 
iii 

performance metric

accuracy is a natural metric to use for evaluating models in a classification problem  but since our data was
skewed  optimizing for accuracy tended to produce models that classified all patients as non diabetic  we instead
used area under the receiver operating characteristic
 roc  curve  or auc  as our performance metric  auc
is superior to other metrics that balance precision and
recall  such as f score  because it does not commit to
a particular probability threshold for classifying patients
as diabetic 
iv 

methods

we used findrisc  restricted to our feature set  as a
baseline model  we compared the baseline with a range
of increasingly complex models  we first trained our
models on the full feature set described in ii  we then
constructed a condensed feature set with the top    features  these were the    features most correlated with diabetes  skipping several hypertension features that were
near duplicates of the most correlated hypertension feature  we trained all the models on the top    features 
for each model  after selecting hyper parameters 
we determined a     confidence interval for the auc 
specifically  we selected     of the original skewed training data as a hold out set  balanced the rest and trained
on it  and calculated the auc on the hold out set  we
repeated this procedure n times  n       and found the
population mean  and standard deviation   we then
modeled the mean ofour auc scores as a gaussian with
standard deviation    which yields the     confidence
n

interval          
n
v 

a 

results

findrisc and logistic regression

findrisc is a logistic regression model on seven features  age  bmi  waist circumference  use of antihypertensive agents  history of hyperglycemia  physical activity  and dietary patterns       only age  bmi  and the
use of hypertensive agents were present in our data set 
we ran logistic regression using only these three features
to get the performance of findrisc on our data set  we
also ran logistic regression on our full feature set  excluding binary features where one category only appeared a
few times  the results can be found in table i 

feature set
findrisc
full

cv auc
    ci
test auc
                        
                        

we note that in the original findrisc study where
patients were screened to collect all required data  the
auc was           
b 

naive bayes

we used the e     r package for naive bayes classification  the package models continuous features as
gaussians with a different mean and variance for each
class to be predicted  we used laplacian smoothing with
a smoothing parameter of   
c 

k nearest neighbor

we used matlabs classificationknn library to apply
the k nn algorithm with different distance functions  for
each distance function  we tried all odd k between   and

number of training samples       as suggested by literature       the auc values of the different distance
formulas can be seen in fig    

fig     auc as a function of k  each plot represents a
different distance function for knn 

d 

random forest

we used the implementation of breimans random
forest algorithm in the randomforest r package  using grid search with    fold cross validation  we chose
the number of trees in the forest and the number of features to randomly select at each node as candidates for
splitting 
e 

gbm

we fit a gradient boosting machine  gbm  model using the gbm r package  we estimated the optimal number of trees through cross validation using the dismo
package 

fi 
f 

   both the number of instances and features are large
 both in       

svm
  

choice of primal problem

research suggested that for skewed data we should use
different regularization hyper parameters for positive and
negative classes       we followed this approach instead
of balancing the training data through duplication  we
thus solved the following two class primal problem  using
libsvm 
min

w b 

x
x
  t
w w   c 
i   c 
i
 
y   
y   
i

   the number of instances is much greater than the
number of features 
with     features and      training instances  using the
rbf was justified  we nonetheless verified that rbf was
superior to other common kernels  table iii shows the
best    fold cv auc for each kernel 

i

t

subject to yi  w  xi     b      i
i     i              m

table iii  best auc for kernels
kernel
best cv auc

we used the radial basis function  rbf  as a kernel 
for reasons discussed in    

linear polynomial rbf sigmoid
     
     
           

 

k xi   xj     e  xi xj   

we assigned the hyper parameter c  as the weight for
diabetic patients and c  as the weight for non diabetic
patients  we experimented with different weights by doing a grid search for the best value for c  and   keeping
c  pegged to c    the    fold cross validation auc can
be seen in table ii  note that these calculations were
done on a limited feature set composed of only the icd 
and ndc codes for the sake of time 
table ii  best auc for weight ratios ratio  

c 
c 

best cv auc

   

 

 

 

  

c 
c 

   

                                   

 
the auc value peaks for the ratio c
c       we noted
that this is almost the ratio of non diabetic patients to
diabetic patients in our data set  this makes sense  as by
setting the regularization parameter for diabetic patients
  times that of non diabetic patients  we are effectively
counting diabetic patients   times in the objective function  thus  it has the same effect as training with a
single class primal problem  but on a modified dataset
where each diabetic patient record is replicated   times 
c 
for all future runs  the weight ratio c
was set to   
 

  

choice of kernel

research suggested using the rbf kernel as a default
first choice  given that none of the following conditions
hold true      
   the number of instances is much smaller than the
number of features 

  

implementation challenges

when we first ran our svm on the test set  the test
auc was much lower than the cv auc  to investigate
whether the difference was due to the particular way our
data was divided into training and test sets  we created
new test and training sets from the training data and
repeated the process of selecting hyper parameters and
evaluating the final svm  again the test auc was much
lower than the cv auc 
we ultimately fixed the problem by using    fold crossvalidation instead of   fold  this solution worked because
the best choice of hyper parameters depends on the size
of the training set       the hyper parameters we found
to be the best for     of the training set were not the
best for the full training set  ideally  to fix the problem
we would use leave one out cross validation  but that was
not computationally feasible 
g 

ensemble methods

we constructed ensemble models by taking the mean
and median of the predictions from the top k models for
each patient  we excluded svm for lack of time  we
found that the mean of the top two models  i e  random
forest and gbm  performed best  including more models decreased performance  though the difference between
k     and k     was not statistically significant 

table iv  ensemble cv auc with     ci for different
methods and k

mean
median

k  
k  
k  
                                      
                                      

fi 
h 

summary

table v  model performance on full feature set
model
findrisc
logistic regression
naive bayes
k nearest neighbor
random forest
gbm
rbf kernel svm
ensemble

cv auc
     
     
     
     
     
     
     
     

    ci
test auc
         
     
 
        
     
         
     
         
     
         
     
         
     
 
        
     
         
     

the auc results for all models discussed in this paper
can be seen in table v  the ensemble of random forest
and gbm performed best on both the training and test
sets 

ehr are sufficient to detect undiagnosed diabetes  automated diabetes detection can not only complement but
even compete with more expensive screening programs 
in order to use our model to identify undiagnosed diabetes  we must select a threshold probability at which
to classify a patient as diabetic  the optimal threshold
depends on how we value precision relative to recall  a
medical provider could determine the threshold by comparing the cost of testing a non diabetic patient for diabetes with the cost of failing to diagnose a diabetic 
table vii shows the thresholds that maximize the f
score  where higher  favors recall over precision  fig   
shows the models performance over all thresholds 
table vii  optimal thresholds
criteria
f 
f 

score threshold precision recall
     
     
     
     
     
     
     
     

table vi  model performance on top    features
model
logistic regression
naive bayes
k nearest neighbor
random forest
gbm
rbf kernel svm

cv auc
     
     
     
     
     
     

    ci
test auc
         
     
         
     
 
        
     
         
     
         
     
         
     

table vi shows the models results on the top   
features  the logistic regression and gbm models performed best on both the test and training sets  the difference between the two models cross validation aucs
was not statistically significant 
note that the test auc was outside the     confidence
interval for many models  this happened because the ci
was for the models auc scores on data from the training
set  which is a slightly different distribution than auc
scores on a random data set  such as the test set  thus
neither cv nor test auc give us a perfect measure of
model quality 
vi 

a 

conclusion

best model

our most successful model was the ensemble of random forest and gbm trained on the full feature set 
our ensemble model achieved an auc of       on the
test set  which vastly outperforms logistic regression with
the findrisc features  our model is competitive with
findrisc screening in the field  where studies have
found an auc of            this shows that general

fig     precision vs  recall as the classification threshold
decreases 

b  comparing models and feature sets
the complex models  i e  random forest  svm and
gbm  performed better on the full feature set than on
the top    features  by contrast  the simple models  i e 
logistic regression  naive bayes  and k nearest neighbor 
performed equally on both feature sets  these results
accord with the fact that the simpler models have higher
bias than the more complex models  random forest and
svm performed worse than the simpler models on the
top    features  which suggests that the models overfit
the data due to their higher variance 
logistic regression on the top    features did not perform as well as findrisc in the field  thus we would
not recommend using our top    features as a screen 

fi 
ing heuristic  if medical providers ehr lack important
predictors of diabetes  as our data set did  they will get
better results by generating a large feature set and fitting
complex models 
vii 

future work

we could improve our results by generating more features  all our best models were around      cv auc 
which suggests that we hit the limit on how well we can
predict diabetes with our feature set  we suspected that
our feature set was our main limitation after developing random forest and svm models  but we continued
to apply new models rather than mine for more features
because those results were more interesting and could be
generalized to other data sets 
by contrast  any further features we created would
have been highly specific to our data set  for example 
kaggle removed blood glucose lab tests from the data
set because they reveal whether patients have diabetes 
we could thus have searched for features that indicate
patients had lab tests removed  since people who take a
blood glucose test are more likely to have diabetes 
collecting more data would also improve our models 
fig    shows a learning curve for random forest and gbm 
at      of the training set  the slope of the plotted test
auc is small but positive for both models  we did not include training auc for random forest because the model
predicted the training set perfectly 

fig     learning curve for random forest and gbm 

further exploration of ensemble methods could yield
better models  we could construct many random forest and gbm models with different hyper parameters or
trained on different subsets of the training data and combine them in more sophisticated ways  in addition  we
could improve our gbm models by investigating ways of
customizing them besides changing the number of trees 
the next step for our work is to try our approach in
the field  we could collaborate with a medical provider
to build a predictive model from their ehr  this would
teach us how well our methods generalize across data
sets  further  we could experiment with different ways

of assembling a training set  for instance  could we get
better results if we only trained on patients who were
tested for diabetes  our data set had patients who were
not tested  and some of them may have had undiagnosed
diabetes  we could also tackle the problem of estimating
how likely patients are to develop diabetes 
acknowledgements

we acknowledge the winners of the practice fusion
diabetes classification challenge for inspiration for features and models      we thank andrew ng  andrew
maas  all cs     tas  and dr  ajay sood for their advice 
references
    diabetes  who  n p   n d  web     dec       
 http   www who int mediacentre factsheets fs    en   
    centers for disease control and prevention  centers
for disease control and prevention  n d  web     dec       
 http   www cdc gov media pressrel      r       html  
    world health organization  guidelines for the prevention  management and care of diabetes mellitus  in  khatib
om  editor  vol      emro technical publications series 
     
    ada  diabetes screening cost effective 
diabetes in control  n p   n d  web     dec       
 http   www diabetesincontrol com articles diabetesnews      ada diabetes screening cost effective  
    schwarz  p   j  li  j  lindstrom  and j  tuomilehto 
tools for predicting the risk of type   diabetes in daily
practice  hormone and metabolic research              
       print 
    practice fusion diabetes classification  data  n p  
n d  web     dec       
    international classification of diseases  ninth revision  centers for disease control and prevention  centers
for disease control and prevention     sept        web    
dec         http   www cdc gov nchs icd icd  htm  
    clinical classifications software  ccs  for icd  cm  hcup us tools   software page 
n p  
n d 
web 
   dec 
     
 http   www hcupus ahrq gov toolssoftware ccs ccs jsp  
   
national
drug
code
directory 
fda  n p   n d 
web 
   dec 
     
 http   www fda gov drugs informationondrugs ucm       htm  
     herman wh  smith pj  thompson tj  engelgau mm 
aubert re  a new and simple questionnaire to identify people
at increased risk for undiagnosed diabetes  diabetes care
                 
     qi h   feature selection and knn fusion in molecular
classification of multiple tumor types  international conference on mathematics and engineering techniques in medicine
and biological sciences  june      
     osuna  r  freund  and f  girosi  support vector
machines  training and applications  ai memo       massachusetts institute of technology       
     maas  andrew  office hours      
     hsu  chih wei  chih chung chang  and chihjen lin 
a practical guide to support vector
classification 
n p   n d 
web 
   dec 
     
 http   www csie ntu edu tw  cjlin papers guide guide pdf  

fi