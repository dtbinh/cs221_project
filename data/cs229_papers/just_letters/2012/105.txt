spoken language classification
cs      machine learning
julien de mori

misrab faizullah khan

cameron holt

shahriyar pruisken

autumn     
abstract
we attempt to identify the spoken language of brief audio samples of speech  we employ
several machine learning based algorithms to approach this problem  using the mel frequency
cepstral coefficients of the audio signals as primary features  we discuss the results and examine
the advantages and disadvantages of each algorithm 

 

introduction

there has been much literature related to speech recognition in the machine learning community 
the most common problem in this area is to translate spoken words into text  however  this
approach is generally restricted to the case where the speakers language is already known  in many
situations  such as automated telephone systems  it would be useful for an algorithm to first recognize
the speakers language so that it can translate the speech from the appropriate language  as there
has not been much research on this problem  we examine various machine learning algorithms to
identify the spoken language from an audio clip  an svm  neural networks  and a gaussian mixture
model  other algorithms investigated include nave bayes  softmax regression  k means  and knearest neighbor  however the initial results from these algorithms were not as good as those from
our main models  we focused on binary classification among various pairs of languages  although
our algorithms can be easily extended to sets of more than two languages 

 

data processing

we obtained our training and testing data from voxforge      a website providing a collection of
audio recordings of speech in various languages  we wrote code to scrape a large number of   khz
   bit recordings in six languages  english  french  german  dutch  italian  and portuguese 
most of the clips are between four and seven seconds in length  as the computation time for
our algorithms is a large concern  we strip out all but the middle     seconds from each clip  we
then divide the remaining     seconds of speech into   ms frames  each overlapping by   ms  and
multiply each frame by the hamming window in order to smooth discontinuities in the signal  from
each of these frames  we extract features known as mel frequency cepstral coefficients 
the mel frequency cepstrum is a representation of an audio signal on the mel scale  a nonlinear
mapping of frequencies that down samples higher frequencies to imitate the human ears ability
to process sound  in our implementations  we used the first    cepstral coefficients as our primary
features  as is common in similar applications  a visualization of these features can be seen in figure
  
besides the cepstral coefficients  we also investigated using delta features  delta features are
the first and second time derivatives of the cepstral coefficients  capturing the change of the cepstral
features over time  which we hypothesize will be useful in classifying language  since pace is an

 

fifigure    the original sound wave and its cepstral coefficients

figure    the feature extraction process

important factor in language recognition by humans  we can calculate these features as the central
finite difference approximation of these derivatives
t i  

ct   i  ct  i
 

 t i  

and

ct   i   ct i   ct  i
 
 

   

where ct i denotes the i th coefficient of the t th frame of the clip 
we also experimented using filterbank energies  defined as the energies of the signal in various
regions of the cepstrum  however  adding these features to our model did not change the accuracy
and significantly increased the computation time  so we did not include them in our final model 

 

support vector machine

our baseline algorithm is an svm implementation  training the svm is done by first computing
the cepstral and delta features for each frame  each frame is then treated as an individual training
example  and the feature values are inserted into the training matrix  the svm is trained and tested
with a linear kernel  as the computation time with nonlinear kernels was prohibitively expensive 
testing was done by making predictions on the individual frames  and then making a prediction
on the clips language by either taking a majority vote  or by multiplying the probabilities of each
language across the frames  we found that both methods produced similar results 

 

gaussian mixture model

we attempt to model each language as a weighted sum of m multivariate gaussian distributions in
   dimensions  the size of our feature space   each language model is a probability distribution in
the space of cepstral features that returns the likelihood that a set of features belongs to its label 
 

fim     corresponds to the case that we assume each language to be distributed as a multivariate
gaussian  and compute its mean vector  and diagonal covariance matrix   making the implicit
assumption of uncorrelated features   for larger values of m   we initialized the em algorithm using
centroids returned by a k means algorithm  and computed the m most likely gaussians to fit the
training data for each language  each with an associated weight wj that depends on the density
distributions of training data  therefore  for each language           n  where n is the number of
languages modeled  we obtained a probability distribution
p language x   

m
x

wj p x  j   j  

   

j  

where p x  j   j    n  j   j    for the testing phase  we assumed all   ms frames were independent to simplify likelihood computations  which were as follows 
p language clip    p language 

tc x
m
y

wj p xf   j   j  

   

f    j  
 
where tc are the number of frames for each clip  p language     languages
is the prior on each
language  and xf is the    dimensional feature representation of frame f   each clip is classified as
the language that yields the highest likelihood from     
at a high level  we can interpret each of the m gaussians as an accent within the language  this
model then uses the em algorithm to solve the unsupervised problem of identifying accents within
a language  and then solves the supervised problem of matching an audio sample to the correct
language model 

 

neural network

for our neural network implementation  we took a different approach to features  instead of computing features for each   ms frame and then making predictions on the frame level  we modeled each
clip as a multivariate gaussian distribution by computing the mean and variance of each feature
across each clip  changing the dimension of the feature vector to     this has the effect of averaging out any noise in the signal  as well as reducing the number of training examples  considerably
decreasing computation time 
we built a feedforward neural network with   layers of neurons  the input layer  il  the hidden
layer  hl and the output layer ol  il has n      neurons  one for each feature  hl has k     
neurons  ol has p     variables  one for each testing language  each neuron in il and hl outputs
a value to each neuron in hl and ol respectively  every neuron i in hl has bias bhl
and weight
i
hl
wj i
for each input j  we define s to be the sigmoid function s x      e x  
we first trained our model using the backward propagation method  a form of gradient descent
to find the optimal b  s and w  s  after running each training data point through the network we
look at the difference between the actual category and the outputted category by the model   
we update our weights and bias at each layer using a weighted sum of the  from the next layer 
the results yielded a low accuracy  furthermore  accuracy varied with different permutations of the
training matrix 
to improve our accuracy  we instead used scaled conjugate gradient back propagation  scgbp  
this training method is not susceptible to accuracy changes with different permutations of the
training matrix  scgbp is a form of gradient descent that is developed around the idea of conjugate
directions and typically converges more quickly than standard backward propagation  using the
scgbp training method we observed far higher accuracy rates 

 

fifigure    layout of neural network

figure    testing curve for english vs  french      training      testing 

 

results

overall  our algorithms performed well  with the neural network performed the best of the three 
when training the english vs  french model with       samples per language  the neural network
surpassed     accuracy  in fact  even by making a prediction based on just a single   ms frame 
the neural network achieved     accuracy  for both the neural network and the svm  the accuracy
increased along with the size of the training set  see figure     in all of our trials  the accuracy of
the gaussian mixture model with all three m values tended to converge  leading us to believe that
the m gaussians converge to one with a large enough sample size 
we also tested the three algorithms on the german vs  italian task  in which case both the neural
network and the gaussian mixture model had     accuracy  and the svm had     accuracy  both
with     samples per language  the confusion matrices for these tests can be seen in table   

 

figerman
italian

german italian
  
  
 
  
accuracy     

german
italian

german italian
  
 
 
  
accuracy     

german
italian

german italian
  
 
 
  
accuracy     

table    german vs  italian testing      training examples per language  svm  nn  and   gmm respectively

 

limitations and future considerations

many limitations in our implementations were caused by the data set  the three main problems
we observed with our data were that several of the audio clips were recorded by the same speaker 
many contained background noise and many speakers were monotone  not reflecting ordinary speech 
to more accurately test our algorithms  we would need a data set that better represents how the
languages are actually spoken 
despite the strong results obtained from our neural network implementation  there is still plenty
of room for improvement  we did not rigorously determine the optimal number of hidden neurons
to use  we would like to attempt to model the neural network with the number of neurons on the
order of the dimension of the feature vector 
in our mixture of gaussians model we modeled each language with a fixed number m of gaussians  future research could investigate a different value of m for each language 
in our neural network we obtained significantly higher results when inputting the mean and
variance of the frames rather than inputting each individual frame  applying this technique to our
other models could result in a similar increase in accuracy  and would also significantly speed up the
algorithms runtime  to improve further computation time  we could also reimplement our models
in c rather than in matlab  enabling us to run our algorithms on larger data sets more quickly 
although there is room for future work and improvement  we hope this paper provides an
interesting multi pronged approach to the spoken language classification problem 

references
    m  brookes  voicebox  speech processing toolbox for matlab  available  http   www ee ic ac uk hp 
staff dmb voicebox voicebox html  dec            
    m  haggblade  et al  music genre classification  cs     final paper  stanford university  dec       
    madhusudanan  a  designing and implementing a neural network library  available  http   www codeproject 
com articles       designing and implementing a neural network library  dec            
    mathworks  conjugate gradient back propagation algorithm  available  http   matlab izmiran ru help 
toolbox nnet backpr   html  dec            
    matlab neural network toolbox  available  www mathworks com help pdf doc nnet nnet ug pdf  dec            
    mfcc matlab library for mel ceptrum coefficients  available  http   www mathworks com matlabcentral 
fileexchange       mfcc  dec            
    g  montavon  deep learning for spoken language identification  machine learning group  berlin institute of
technology 
    k k  paliwal  on the use of filter bank energies as features for robust speech recognition  isspa     brisbane 
australia       
    voxforge  open source speech recognition engines  available  http   www voxforge org home listen  dec 
          
     m a  zissman  comparison of four approaches to automatic language identification of telephone speech  ieee
transactions on speech and audio processing  vol     no     pp         jan       

 

fi