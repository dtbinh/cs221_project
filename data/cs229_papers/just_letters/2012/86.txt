e commerce product categorization
srinivasu gottipati and mumtaz vauhkonen
introduction 

product classification for e commerce sites is a

backbone for successful marketing and sale of products listed on
several online stores like amazon  ebay  and craigslist etc  since a
large number of business users list their products and expect to
find buyers for their products  it is crucial that the products are
listed in accurate categories  this paper explores the experimental
results that we conducted in using various forms of feature
classification methods in combination with three main classifiers
nave bayes  svm  k nearest neighbors  along with lda an
unsupervised document topic classifier 
high level steps followed for this classification process 
  
  

  

  

  

data collection
pre processing
a  removal of less useful words like  of  the  an  in  and etc 
b  lower case conversion
feature selection and deriving unigram and bigram modals of the
feature set using top occurring terms from each category  infogain  chi square  and latent dirichlet allocation  lda  
apply classification models  nave bayes  multi class svm  k
nearest neighbors  knn  for both the unigram and bigram and
combined unigram and bigram with a split of     with the data
derived from each of the feature selection methods mentioned in
step    these three models were selected with an intention to
compare between generative nb   discriminative svm  and nonparametric models k nn  
analysis of the results

section ii  relevant research discussion 

classification method and compared to svm where svm fared better
but their research indicates potential for chi square to be a robust
classifier  the details are presented in the following section 

section iii  implementation model
data collection  to evaluate and test our approach to test which
classifier with the given feature set would perform best in product
classification  information on        products for    categories
were gathered by crawling amazon site and scraping the pages to
extract the attributes  title and description    the category tree can
be very deep and possibly contain many levels of depth  however 
for the current scope of experiment  category tree was restricted
to the top level and some of the things that could be placed into
subcategories were placed on the top level  for example luggage
bags and gym bags exist in the top level  the attribute list
consists of  title and description 
category selection  the graph in picture illustrates the distribution
of the category classes  this reflects the typical distribution of long
tail products e commerce sites carry on the categories 

with

increasing speed of online marketing a sizable amount of research
has been conducted  several researchers have approached this
problem from various angles and discussed their efficient outcomes 
young gon et al    implemented a modified nave bayes model for
product classification by applying a regular nave bayes instead of
text classifier and by making it treat each word as an attribute and
making it accept weights assigned by the researchers  though the
accuracy is slightly high the main draw back in this approach is how
to pick the right weight as it is based on observing the data and
manually assigning the weights based on the features selected  not
choosing an appropriate weight would alter the results significantly 
lin and shankar     have researched on using effective preprocessing techniques and multiclass features to increase accuracy
in classification  lee et al     discuss the classification process in
terms of what exactly is classification in the context of multiple class
relation models and they present a semantic classification model
scm  wan and peng    used a fuzzy set modeling to identify the
categories  but this model lacked the comparison of classification
accuracies for evaluation  meesad et al     used chi square as a

below are top   categories with highest products 
automotive
watches
shoes
electronics
bikes
table  

    
    
    
    
    

some other dense categories included fitness  musical
instruments  golf  bikes  sports outdoor accessories etc   
data pre processing
for pre processing  apache lucene libraries were used for
tokenization  normalization  stop word removal  and stemming  as
a first step in preprocessing  all the text was converted to lower
case  and applied tokenization based on delimiters   tokenization
can be very complex for multilingual scenarios and the current
focus was restricted to english   stop word removal was achieved

 

fiby manually crafting the list based on the various online resources 
porter stemmer algorithm implementation from lucene was used
to achieve stemming of the data 

section iv of analysis section  we have seen consistently
combination of title and description outperforming just tile alone 
we have excluded analysis based on title in the analysis section  

feature selection

lda  latent dirichlet allocation model was also used but primarily
as a process to pick the top topic words and later using these
topic words as unigrams feeding into the supervised classifiers to
observe if it lead to increased accuracy  

for all the products in the experiment  there were total of       
unique unigrams and around          unique bigrams  in order to
reduce the number of features  feature selection was conducted
by experimenting with various choices for bag of words by picking
top words based frequency of occurrence from each category 
information gain  chi square attribute evaluation and lda top
topic words  unigrams and bigrams were generated for top     
                        feature sets and were tested as
represented below in table   

classification model and algorithms 
to identify the best way to increase accuracy in addition to feature
selection  several classifiers were evaluated against the feature set
and data collected  the classifiers that were used for the
evaluation included multiclass svm  nave bayes  multinomial  
and k nearest neighbors  with   neighbours 

chi square attribute evaluation  the chi square attribute
 
evaluation is based on the implementation of pearsons 
statistic  by applying this model for feature selection  the more
 
unique a feature is the higher the  value   the formula is
expressed as below and weka was used to run the chi square
attribute selection process 

training and test sets
for training the classifier      of the data set was used       
products out of         and for testing the rest of the     of the
data set        products  was used  from each product category
of data     was used as a training data set and     as test data 
the reason the percentages for each category were separately
derived is to avoid the chance of over and under representation of
data from a certain category 

  pearson s cumulative test statistic 

analysis of results 

  is observed frequency 
  is expected  theoretical  frequency indicated by the
hypothesis
  the number of cells in the table 

the results of the experiment with nb  svm and k nn give the
following results 
  
  

the following table illustrates the various combinations feature list
were prepared  the cells that are checked are the ones against
which the experiments were run  
model

frequency
based

info
gain

chisquare

lda

unigrams















bigrams
unigram  
bigram       split 
table    

similarly for  bigram modals  feature selection was conducted by
picking top word pairs are that co occurring in each of the
categories and merging to get the final feature list  once top x
features are identified  then various experiments were carried out
with varying number of features that included     features     
features       features       features and       features  in
addition to varying the number of features  separate experiments
were done once by using only from title and other time using both
title and description  the results of this analysis were detailed in

  

the unigrams as whole group outperform the bigrams for accuracy
in classification 
feature set size at       when the feature set was small at    
size  k nn performed the best with all feature selection models 
 refer to figure   in next page  maintain accuracy of at least   to  
percentage points above the rest  svm trailed right behind k nn
with     percentage points below  however nave bayes accuracy
rates were much lower with a difference of     points  a unique
result set to is that  when the feature set of     derived using chisquare model was used  it significantly increased the accuracy
rates by more than    percentage points for nave bayes and for
svm and k nn at least by   percentage points  of all the classifiers
as shown in figure   on next page  the reason chi square
performed better was  it was able to identify words like pedomet 
trampoline rower sled which have high degree of accuracy in
associating with the respective category 
performance at feature set size of     showed that all models got
close to   or more   point boost using the chi square model
compared to the rest with svm faring the best and knn just
trailing behind refer to figure    on next page   at the feature set
size of       nave bayes started displaying higher accuracy gain
compared to smaller data sets previously reaching     while svm
and knn had steadily improved their accuracy rate till reaching
     feature set size with all models 

 

fi  

  

  
  

minutes  k nn ranged from   min to   hours depending on the
feature set size 
three key points stand out from the experiments 






chi square model showed a significant boost to all models in
accuracy for a small feature set with nave bayes getting the best
boost  though knn performance was the best with a small feature
set  the running time was significantly higher 
for a large number of features  frequency based feature set of
unigrams gave best results for nave bayes followed by svm and
knn  chi square performance was also very similar to these
results 
bi gram models by themselves fared poorly compared to unigram
model 

presented below is the data and graphs that illustrate accuracy achieved by running the feature sets based on various ways
from each feature extraction model  frequency based  infogain  chi square  lda   all the graphs are listed below and next page
to make it easy for comparing values  for knn    neighbors were used 

unigram accuracy comparison    

unigram based on chisquare accuracy
comparison    

   

   
  
  
  
  
  
  
  
  
  
 

  
  
  
  

accuracy

  

optimal feature set size seems to be at      set size where the
performance gap between regular unigram choice based on
frequency and info gain and chi square fared almost comparably
with frequency based unigram model showing the best results for
all classifiers 
lda also showed decent levels of accuracy at      data set but
frequency based and chi square models surpassed lda  refer to
table    below 
nave bayes remained almost flat at the same levels as     
feature set size when the feature set size increased to        and
surpassed all other models at      features 
svm steady improvement with increased feature set size till the
point of      and started a downward curve at        feature set
size  the reason is that it started suffering from over fitting 
knn was the worst performing as the feature set size increased 
running time for  nave bayes for fastest for all feature set sizes
up to        with     minutes  svm approximately max   

accuracy

  

  
  
  
  
  
 

   
words

   
words

    
words

    
words

     
words

naive bayes

    

    

    

    

    

svm

    

    

    

  

    

knn

    

  

    

  

  

figure     unigram frequency model

   
features

   
features

    
features

    
features

     
features

naive bayes

    

    

    

    

    

svm

    

    

    

    

    

knn

    

    

    

    

    

figure     chi square unigram model

lda topic modeling  topics modeled      features        picked top    contributed from each topic 

naive bayes
svm
knn

    
  
    

table    

 

fibigram accuracy comparison    

combination of unigrams and bigrams       split  accuracy comparison    

   

   
  
  
  
  
  
  
  
  
  
 

  
  
  
accuracy

accuracy

  
  
  
  
  
  
 

   
bigrams

   
bigrams

    
bigrams

    
bigrams

     
bigrams

naive bayes

    

    

svm

    

    

    

  

    

naive bayes

    

    

    

svm

knn

    

    

    

    

  

knn

figure    bigram frequency based model

    
features

    
features

     
features

    

    

    

    

    

  

    

    

    

    

    

    

    

    

    

chisquare attribute selector  combination of
unigrams and bigrams        spit  accuracy
comparison    
   
  
  
  
  
  
  
  
  
  
 

accuracy

accuracy

   
features

figure    unigram and bigram split mode

infogain attribute selector  combination of
unigrams and bigrams        spit  accuracy
comparison    
   
  
  
  
  
  
  
  
  
  
 

   
features

   
features

   
features

    
features

    
features

     
features

naive bayes

    

    

    

    

    

svm

    

    

    

    

    

knn

    

    

  

    

    

figure     info gain unigrams and bigrams model

   
features

   
features

    
features

    
features

     
features

naive bayes

    

    

    

    

    

svm

    

    

    

    

    

knn

    

    

    

    

    

figure    chi square model unigram bigram split

 

fibelow is the confusion matrix derived from the best performing chi square feature selection unigram modal with
     features based on svm classifier 

as seen in the above confusion matrix  classifier was getting confused while classifying the items in sports
outdoor accessories with the items in watches category  the reason is that  sports outdoor accessories have
sporty watch items and as watches category is dominant in training  classifiers were trying to maximize towards
watch category  we have found similar patterns with other categories as well  where the product classification
could fall into more than one category  this prompts for further research into multiple class relations 

conclusion  the results of the experiments clearly indicate that with a small feature size set a chi square model
of feature selection gives a significant boost to the classifiers  however at large feature set size  nave bayes seems
to gain the most accuracy with plain frequency based unigram model for feature extraction and lda fared at an
average level  further research needs to be done in refining the above product classification approach that can
include more inputs such as images  tech specification etc  with an added functionality of creating a new category
based on the incremental learning  in addition  a formal approach needs to be explored to suggest multiple
categories where the classifiers have confusion among various categories and further research needs to be
conducted in extending this to deal with nested category hierarchy 

references
  

  
  

  

  

modified nave bayes classifier for e catalog classification  kim  young gon  school of computer science and engineering  center for ebusiness research  seoul national university  seoul          korea  republic of   lee  taehee  chun  jonghoon  lee  sang goo source 
lecture notes in computer science  including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics   v     
lncs  p                data engineering issues in e commerce and services   second international workshop  deecs       proceedings
applying machine learning to product categorization lin i and shankar s  stanford university  cs    
a semantic classification model for e catalogs  kim  dongkyu  corelogix  inc   snu ict          seoul  korea  republic of   lee 
sang goo  chun  jonghoon  lee  juhnyoung source  proceedings   ieee international conference on e commerce technology  cec
      p              proceedings   ieee international conference on e commerce technology  cec     
a technique of e commerce goods classification and evaluation based on fuzzy set  wan  hongxin  mathematics and computer science
college  jiangxi science and technology normal university  nanchang  china   peng  yun source  international conference on internet
technology and applications  itap        proceedings        international conference on internet technology and applications  itap
       proceedings
a chi square test for word importance differentiation in text classification  meesad  p  boonrawd p  nuipian v            
international conference on information and electronics engineering ipcsit vol        

 

fi