stochastic control of electric vehicle charging
kyle  anderson  
cs       machine  learning  final  project  
  
abstract this project attempts several methods to optimize
charging schedules for electric vehicles on a constrained
radial network using machine learning 
in the first
approach  electric vehicles act as independent agents in a qlearning framework  receiving negative rewards based on
congestion charges calculated from their contribution to
overloaded parts of the network  in this model  electric
vehicles may take the action of increase or decrease the
charging rate by a fixed amount at each time interval  but are
subject to charging limits  battery capacity limits  and
charging deadlines  next  i consider a central aggregator
who implements a supervised learning technique to recognize
the discounted future cost of a network state  during the
testing phase  the controller will greedily take actions that
move to a better state  finally  i consider another central
aggregator scheme based on linear quadratic regulation  i
propose an  experimental  alteration to the lqr method to
accommodate exponential action spaces by adding a
polynomial number of quasi time steps to allow the decision
maker to perform many actions within a given time step 

i 

introduction

electric vehicle charging is a unique electric load
because it is deferrable  controllable  and deadline
constrained  in other words  ev owners dont care when
their vehicles are charged so long as they have enough
charge when they need to drive  this type of load
provides a unique opportunity to address three key issues
in grid operation 

degenerate as ev adoption increases  to put the
magnitude of this problem in perspective  high powered
electric vehicle chargers are rated as high as   kw
 eaton   an order of magnitude higher than the average
residential house load between  kw and  kw  jy  
meanwhile  the distribution hardware in many areas of
the united states is already operating above its
nameplate ratings and approaching end of life
 weidmann  
therefore  electric vehicles must be
intelligent to avoid becoming the straw to break the
camels back 

  
ii 

network model

i obtained a set of sample distribution networks from
pacific northwest national labs gridlab d project
 pnnl   as it turns out  distribution networks in the
united states are radial and often have a tree like
structure  in order to focus on the machine learning
computational problem i decided to use my own matlab
script to generate my own networks of similar structure 
allowing me to test my system on networks of various
sizes and branching factors without focusing on building
a robust parser 

reshaping   reshaping the aggregate load curve by
charging during off peak hours or even discharging
during peak hours using a vehicle to grid system 
demand response   reducing the required reserve
capacity by providing ancillary service to the grid during
unexpected increases or decreases in the load
distribution automation providing localized relief to
overloaded power lines or transformers on a constrained
distribution grid in the case of faults or unexpected
overloads
these benefits can only be obtained through the use of
intelligent charging schemes 
in fact  widespread
uncontrolled electric vehicle charging could have
disastrous consequences in many regions of the united
states  residential electricity usage tends to peak in the
evening when people get home from work  turn on the
tv  air conditioner  etc   the same time that many
drivers would  in theory  plug in their electric vehicle  
even deferred schemes such as charging at midnight or
simplified schemes such as randomization could quickly

key
   substation
   transformer
   customer

figure        sample  distribution  network

figure   shows an example of one of my generated
networks  the customers are connected as leafs of the
tree  and the intermediate nodes are the constrained
network elements  transformers  power lines  etc   which
are subject to operational costs when they are
overloaded  we define the operational cost of a network
element as the percent by which it exceeds the average
operational point of that network  scaled by a constant 
note that this cost is calculated on a per node basis using
the per node average load 

  

fia unified model using these datasets  however  for now 
i generate the customer loads using matlab in order to
focus on the ml problem at hand  this model provides
sample customer data including the arrival time 
departure time  amount of charge required  and battery
sizes for each electric vehicle on a given operational day 
these data generation scripts were inspired by the
datasets described above  but are easier to work with
because they are not missing data points  see appendix
a for a detailed description of these models 

   load   load    
cost     max    
 
 load
 
 
in order to determine the operating state of an electrical
network  one would typically solve the load flow
problem  this problem consists of setting the p and q
values on load buses  p and  v  values on generation
buses  and solving the following system of equations
enforcing kirchoffs laws to recover the unknown
values 

iv 

optimal solution

the algorithms discussed in this paper assume we do not
know the future arrival times of evs or the amount of
charge that will be required by an ev before it arrives 
additionally  the algorithms do not assume any future
knowledge the customers base load  the algorithms do
assume that once an electric vehicle is plugged in  its
deadline and charge required are known  if we were
clairvoyant  we could generate an optimal policy that
minimizes the network operational costs while ensuring
hard deadlines for all electric vehicles by solving the
following linear program 

this system of nonlinear equations can be solved using
newtons method    
newton update rule 

min     p        pi t             pi   j t           pi    j k t   
t

t

i

t

i

j

t

i

j

k

s t 

where 

 ramplimitc  rate c t  ratec t   ramplimitc

t  customers

socc t   socc t    ratec t 

t  customers

rate c t    

  

t when customers not home  customers

socc t   socc t    chargerequired c

t when customers drives away  customers

 
pt   loadi  j k l t   loadi  j k l t
t t i j k l
i
j k l
p  i t   loadi  j k l t 
j

  

  
my initial q learning reward function used
matpower     a matlab based solver to determine the
load on each node in the network  and the resulting
operational cost  this method turned out to be a
bottleneck as it limited the speed of my algorithms  i
resorted to an approximation where the load on any node
in the tree network is simply the recursive sum of all the
child loads  this approximation is mostly accurate for
tree networks 
iii 

customer   ev models

in addition to acquiring network models from pnnl  i
found some smart meter data from a pg e pilot
program     that provides a realistic estimate of the nonev base load for my models  additionally  i was able to
acquire traffic data regarding driving patterns in the
united states from national highway traffic safety
administration      the ultimate goal of this project
would be to prove the performance of my algorithms on

k

l

p   i  j t   loadi  j k l t 
k

l

p    i  j k t   loadi  j k l t 
l

 
 loadi  j k l t
t t j k l

 
 loadi  j k l t
t t k l

 
 loadi  j k l t
t t l

t
t  i
t  i  j
t  i  j  k

   socc t  batterysizec

t  customers

 ratemaxc  ratec t  ratemaxc

t  customers

this solution provides a metric against which can
compare the performance of our algorithms  as well as
generate a score for network states to use as training
data for the supervised learning method discussed later 

v 

multiagent q learning

my first attempt was a multi agent q learning model in
which each customer agent maintains its own discrete
state space and keeps track of the rewards it has seen 

firate                
priority                 
l load   under  average  over  critical 
l load   under  average  over  critical 
l load   under  average  over  critical 
l load   under  average  over  critical 
time    pm     pm       pm   pm     pm       pm  pm  pm  other 

 priority   chargerequired deadline  scaled by battery size
   l load  
 l load   l     load    l load   l    load    l load   l     load    
l   load  
r s    rate   
 
 
 
 
 l   load
 l     load
 l    load
 l     load
 
 

the agents do not keep track of the state of other agents 
and only have knowledge of the aggregate load on the
nodes that are on their path to the root  the state
transition probabilities are not deterministic from the
perspective of each agent because they depend on the
actions of other agents in the system as well as on the
future base loads in the system  for this reason  i have
modeled this decision using q learning  where the agent
tries to learn the expected utility of an action value pair
without modeling state transitions 
the epoch for training an ev is a single day with      minute time steps  on each training day  the customer
models generate new data for arrival time  departure
time  and charge required  but they maintain their same
statistics to generate this data from day to day  at the
end of each day  the q values are updated according to 
q st   at      q st   at      t  st   at     rt      max q st    at      q st   at   
at  

since the system dynamics from the perspective of a
single ev are changing over time as other customers
refine their policy  we do not want the agent to stop
exploring possible states 
therefore  we
probabilistically select the next action using the
boltzmann rule 

p a   s   

eq s a  
 eq s a  
a a

it is important to note here that if the optimal action was
not selected  the q value will not back propagate to the
previous state  this problem is more difficult than a
traditional application of q learning because the state
action reward depends on the actions of other evs that
are not visible from the perspective of a single ev 
furthermore  in order to keep the state space small
enough to have a workable model  i was forced to do
course discretization 
in the initial process  all electric vehicles entered the
training phase with q matrix set to zero  in this case  the
operating policies did not seem to converge at all  i

received better results when i enabled the evs on the
network one at a time  this allowed each ev to find a
stable operating policy given the new network  and
future evs to learn their policy on a stable network 
this method may  in fact  more accurately reflect how
such a system would be implemented in practice since it
is unlikely that all ev owners will purchase a new
vehicle at the same time  in the end  this method yielded
mediocre results  see results  
vi 

central aggregator model

in this central aggregator model  i assume there is a
central decision maker who has full network knowledge
at the current time step  but does not have knowledge of
future customer base loads or arrivals of new evs  i
implemented a supervised learning method of training a
neural network to recognize good network states  i
generate random initial starting conditions for the
network  and use the linear program to solve for optimal
solutions  i use the lp solution to calculate the
discounted cost of operating the network over the
upcoming   hour period given the random initial state 
and assuming it follows the optimal policy solved by the
lp    
in order to reduce the dimentionality of a single nn  i
train a neural network for each node and for each time
period  thus  there are a total of      nodes  distinct
neural networks  training samples are   input vector
indicating the summed ev charging schedules for each
node in the network  for example  if the lp decided that
one customer would would charge at  kw for the next  
hours  and another customer would charge  kw for the
next   hours  the training input for the parent node
 transformer  would be 
input                 
output  estimated discounted cost

if all charging operations were deferred for   hour  the
vector would be
input                 
output  estimated discounted cost

note that the input vectors represent the ev load
associated with plugged in customers only  while the
estimated discounted cost would be a function of the
base load and new ev customers as well  thus  the
neural network is implicitly predicting the base load and
arrival of new evs 
intuitively  having a different neural network for each
time period allows us to capture the effect that deferring
a load at a certain time of day may be more costly at a
particular node than at a different time of day 
during the test operation phase  we greedily find the
customer action  increase rate  stay same  decrease rate  
which results in the best increase to the overall system

ficost   i e  the weighted sum of the outputs from the nns
for each node in the network   we continue selecting
actions that decrease the total system cost until there are
no more such actions  at which point we advance to the
next time step 

  
vii  linear quadratic regularization
 experimental 
i have attempted an implementation of linear quadratic
regularization with an experimental modification to the
algorithm from class  but it is not yet working  i would
love some feedback here  
i am trying to apply lqr to this problem using the
central aggregator approach  where the states of each
customer and each node  transformer  are concatenated
to form a large state vector        dimensions   the
actions available at each time step are to increase the
charging rate  decrease the charging rate  or hold the
charging rate for each customer  this would yield   n
potential actions  which makes the algorithm from class
intractable  in order to decompose the action space  i
create intermediate time steps that do not correspond to
actual time steps within the system  precisely  i add a
quasi timesteps between each real time step for each
customer in the system  where the system operator has
the ability to take an action  but the system does not
otherwise change  this method gives the operator the
ability to select any combination of actions in between
actual system time steps  precisely  the at matrices
corresponding to the real time steps will be calculated
from training data and reflect the probabilistic changes to
the system as time advances  such as new evs arriving 
deadlines decreasing  and base loads changing  

st     at st   bat   wt
wt   n    t  
m t  

min  st     at s   bt at  
a

 

i   t  

on the other hand  the at matrices corresponding to the
intermediate steps will be the identity matrix  since the
only changes to the state vector on the quasi time steps
will be a deterministic function of the selected action 
furthermore  in both real time steps and quasi time steps 
the bt matrixes are known since the actions have a fully
specified effect on the state vector  the loads at specific
locations are increased decreased  
with this modification as follows  we can follow the
dynamic programming procedure discussed in class 
specifically  we recursively calculate the value function
for each state using

 
vt   st     max r t  st   at     est    psa  vt  
 st        
at

now  the reward function  r  can be calculated
deterministically since the system operator knows the
full dynamics  and the psa will be estimated from sample
data for the real time steps  and known exactly for the
quasi timesteps 
i chose a random state vector at the end of the epoch as
the base case 

vt   st     sttut st
i select an epoch of   days so we can clip out a    hour
period to find a stationary day  where the randomly
selected base case does not have significant impact 

  
then  the action at each time step corresponds to

at     btt t  bt vt    bt t   at    st
viii  results and conclusion
in this study  the simplest approach using supervised
learning worked best  i quantify these results by
showing the average overuse charge for each node for an
operational day  the penalty contribution of each node
is weighted by the number of customers beneath the
node 
performance  

   
   n

  

     customers 
t   nn

n

   loadn t   n    
  max    
 
n
 
 

when calculating the performance  i ran the model from
a random starting position for    hours  and used the last
   hours as the performance data in an attempt to create
a stationary day  since random initial starting points
could be particularly bad  under this method  the linear
program would  in general  perform perfectly  note that
the lp used to generate our supervised learning training
data starts from random  i e  bad  states  in which case
it may have significant discounted costs 

     
      
     
      
   
q learning  
    percenile  

nn  
mean  

lp  

lqr     tbd   

    percenile  

figure        performance  

fi  
the plot below  left  shows the average overuse in the
network over time for selected operation day using the
supervised learning approach  the plot below  right 
shows the total system load over time  which stays
within     margin throughout the day  scaled axis  

the customers load at each interval  t  will be the sum of
the ev charging rate and the base load 
load     baseload       rate   

the customer will increment the state of charge for his
battery at each time step
soct    soct   ratet

during each day of operation  the customer will generate
a new leaveforwork  arrivehome  and chargerequired
value based on the statistics it generated during
initialization 

  
ix 

leaveforwork  triangular bounds leaveforwork     

appendix a  customer data generation 

arrivehome triangular bounds arrivehome     

a  initialization
i use a matlab script to create a set of realistic
customers  ci j k l   located beneath transformers i j k l in
the network from figure    during initialization  each
customer generates statistics that it will use to generate
its outputs during operation 

the model requires its state of charge to be greater than
chargerequired at time leaveforwork 

bounds loadscaling           traingular     

at time leaveforwork  the stateofcharge will be
reduced by chargerequired  the model will force its
charging rate to increase if it is in danger of missing this
deadline 

batterysize  triangular       

chargerequired triangular bounds chargerequired     

socleaveforwork   chargerequiredleaveforwork

bounds leaveforwork      triangular       
bounds arrivehome      triangular       
bounds chargerequired      tri  battsize battsize 

socleaveforwork    socleaveforwork   chargerequiredleaveforwork

the model also requires the rate to be zero between the
times leaveforwork and arrivehome 

  
b  operation
during operation  the customer model will generate a
baseload for each    minute interval by scaling the
normalized standard residential load shape shown below
    intervals per day  

  

rateleaveforworkarrivehome     

x 

acknowledgements

i would like to thank professor abbas el gamal and
han i su for the contributions in helping formulate this
problem and discussing potential techniques 

baseload     loadscaling       tri ci j k l bounds loadscaling    

references

normalized load

standard residential load shape

  am

 am

 pm

  pm

    http   www smartgrid com wpcontent uploads         eaton dcqc pdf
    http   nj gov emp facts 
    http   www weidmannsolutions cn zhenduan condition based strategies pdf
    http   en wikipedia org wiki power flow study
    r  d  zimmerman  c  e  murillo snchez  and r  j 
thomas   matpower
steady state
operations 
planning and analysis tools for power systems research
and education   power systems  ieee transactions on 
vol      no     pp         feb       
    http   en wikipedia org wiki electric power transmission
    pg e smart meter pilot program 
    http   www nhtsa gov ncsa
    http   en wikipedia org wiki q learning

fi