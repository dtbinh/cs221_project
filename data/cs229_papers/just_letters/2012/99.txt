idec  real time eye state classification via web cam
miguel picallo  heerad farkhoor  thomas von der ohe
december         
abstract
this paper presents a real time eye state classifier via a simple web cam  with the help of   means
color clustering of detected eyes via opencv  features for softmax regression are derived and used to
classify in real time four different eye positions  looking straight  looking left  looking upward  looking
right  given that two eyes are detected in a face  the system classifies the eye states with an accuracy
ranging from         we portray the systems value for both video content providers and consumers 
including the ability to analyze content quality and viewer attention over time 

 

introduction

derived by the percentage of viewers focused on the
video over time  lastly  the video could automationline video education is steadily becoming more im  cally stop once the students gets distracted or tired 
portant  reasons are not only geographical differcompanies surveyed are excited about the potenences between the lecturer and the student  but also tial of such a product     
new teaching concepts such as the so called flipped
classroom  in the u s  alone already     of the   
this is the future  everyone wants to
million students use online education     nonetheless 
know if their content is working 
many concerns still exist towards this new teaching
 khan academy 
technique 
assessing the learning value of a video
 parents and teachers do not know whether or
would be extremely valuable 
not the child really watches the video lectures
 edshelf 
or if it just lets the video run in the background 
the potential for automated curation
 content providers  such as the khan academy 
is huge 
 smartercookie 
have little insights into the quality of certain
parts of their videos  overall ratings might not
a really powerful idea 
consider that certain sequences are not well ex tenmarks education 
plained 
 the student himself gets often tired or distracted while watching a video  which makes
him miss certain parts of the video 

 

eye tracking is an area of computer vision and object detection that has been researched in the past
years  achieving a non sophisticated  but reliable
tool for eye tracking is one of the challenges of this
field  there already exist some commercialized products and guides to build eye tracking tool  some of
them focusing on the pupil  given an accurate enough
image of the eye  and relying on some additional
equipment such as lighting  one idea used for eyetracking is differentiating bright and dark zones of the
eye     k means clustering has also been included in
some algorithms to detect certain parts of the face or
object         and algorithms based on k means have
even been used for eye tracking    

in the following paper  we will introduce a new
application that might help solve the mentioned concerns 

 

previous work

idec   idea

by using real time webcam image processing  we attempt to determine the direction in which a person is
looking relative to the screen  parents and teachers
could supervise whether a child really watches a lecture  content providers could get an attention rate
 or content quality graph  over the time of the video 
 

fifigure    real time face and eye detection via a    x    webcam

these eye tracking systems can then be used for
several applications  some of them have already been
implemented to conduct psychological studies about
attention and interest               most of them rely
not only in the image of the eye  but also in the image
that the eye is seeing  so that regions of interest can
be identified 
we make use of opencv  one of the leading open
source computer vision libraries  its face and eye
detection functions rely on the cv function called
cvhaardetectobjects  which uses machine learning
tools to detect any object  given a training basis for
that object  this particular function uses a method

developed by viola and jones     and improved by
lienhart     the method looks for subregions in the
image which are likely to be the object that needs
to be detected  first  it reduces the dimension of
the features  and thus the computational time  instead of all the intensities of all pixels  differences
among intensities of certain regions are used as features  then it uses a cascade classification  in which a
region is classified as the object only if it goes through
all the classifications  these classifications increase
in complexity and the first ones are used to discard
the regions that are clearly not the object  like the
background of the image 

 

tirety of the sclera that is exposed directly around
the iris  vertical cropping removes portions of the
eyebrows and shadows from the image  while the horizontal cropping corrects for a slight asymmetry in
the eyes  additionally  the relative variation between
eye positions when looking in different directions is
maximized  and the subsequently extracted features
become more well defined  the constraint ensures a
clear distinction between each eye direction  especially between looking upward versus straight 
accounting for facial tilt variation  the optimal
cropping profiles were found to be  two thirds of the
horizontal span  offset by one fourth from the left 
and five twelfths of the vertical span  offset by onethird from the top 

   

implementation
face and eye detection

we developed using the c   opencv library an application which accesses the computers webcam in
real time  frame by frame  the program scans for
faces via cvhaardetectobjects  the upper third of
each face is then scanned using the same function 
but with an eye classifier input  cascading the eye
detection into these two stages rather than scanning
a full frame for eyes reduces computation time and
the probability of a false positive  the face detection
mechanism is more reliable and robust against backgrounds  figure   demonstrates the program tracking
both the face and eyes of a user 

   

   

relevant eye image

collection of training data

before extracting features from the cropped out eyes  with the same webcam used for classification  we colthe image is further trimmed both horizontally and lected for each of the four labels  looking straight 
vertically  while being constrained to show the en  looking left  looking upward  looking right  approx 

fifigure    one example of each labeling taken from the training set

figure    eye images having undergone   means clustering with type   features marked

imately     relevant training images  these images not reliably detect closed eyes 
were taken in various lighting conditions and for different eye and skin colors  we chose to omit a lookfigure   demonstrates a training example for each
ing downward label because human eyes naturally labeling  cropped according to the specifications in
close when looking down  and the haar classifier does the previous section 

   

feature extraction via clustering

blue  respectively  the separation between these two
points gives a clear distinction between each of the
two types of features were extracted from these
four classes 
cropped eye images 
   normalized cartesian coordinates of the centers
of mass of the iris and the sclera

   

softmax regression

with these features extracted from our training data 
   percentage of darker versus lighter pixels in
we calculated the log likelihood optimal parameter
each cell of a  x  grid of the eye
values for softmax regression     which enables clasto extract these features  a given images pixels sification of a eyes feature set into one of four diwere represented in three dimensional  rgb  color rections  maximizing the log likelihood function  the

space  and the k means clustering algorithm with optimal parameters values  can be derived using
k     was used to filter the image into three dis  gradient ascent 
tinct colors  given the nature of the eye  these colors
r t       r t    r l  t   
approximately represent the iris  the skin around the
with
eye  and the sclera in order of darkest to lightest  to
m
remove the skin from consideration  the second clusx
r l  t     
   y  i    r   h t   xi  r  x i 
ter is ignored  each filtered pixel is then placed in its
i  
original location within the image 
k
features of type   are calculated by finding the
x
t
rt
h  x r   e  
ei x
normalized center of mass of every pixel belonging
i  
to clusters   and    after dividing the image into a
h  x     h  x         h  x k  
 x  grid  the percentages of cluster   out of cluster
  and   pixels are calculated in each region  forming
we used          with our training set size of
the second set of features 
figure   demonstrates the images from figure        batch gradient ascent was implemented without
having undergone   means clustering  with the dark incurring a computational penalty  and parameters
and light cluster centers of mass labeled in red and were derived for every  label feature  pair 

 

fifigure    real time classification

   

real time classification

figure   shows this algorithm in action  with a
green dot correctly marking the direction in which
the application accesses the webcam in real time 
the user is looking 
processing each frame by detecting and cropping face
then eyes  extracting features from the eyes  and inputting the features for each eye into the softmax
  error analysis
function with our derived parameters  depending
on the applications success in detecting a face  two we split our error analysis into three stages in order
eyes  and classifying the eyes in the same direction  to identify the major bottleneck in the applications
the following algorithm is followed 
performance 
data  frame from webcam
result  frames classification
if no face detected then return no face 
if no eyes detected then return down 
if two eyes detected then
if same label then return label  
else
return previous label  
end
algorithm    real time classification

   percentage of frames with a face detected
   percentage of frames with two eyes detected 
given a face is detected
   percentage of frames with correct classification 
given two eyes detected
we tested each stage in various lighting conditions  and tested the third stage using a  only
type   features  and b  both type   and   features 
the results are in figure    additionally  we performed leave one out cross validation  loocv  with
    training examples for both cases  adding in type  features into the feature set improves the loocv
error rate from      to    
clearly eye detection is the lowest performing and
least robust stage of the classification algorithm  with
only     of frames with a detected face having two
detected eyes in bad lighting conditions  it is also
apparent that by adding type   features into the feature set  we significantly improve straight and upward
classification 

as mentioned above  because human eyes tend to
shut when looking downward and the haar classifier
does not easily find closed eyes  we chose to interpret
a frame with a detected face but no detected eyes as a
looking down frame  to smooth the output  classifications are only updated when exactly two eyes
with the same classification are detected  otherwise 
the classifier could have interpreted non eye objects
as eyes  and we would have no means of determining
which detected eyes to use 

 

fifigure    worst case detection and classification rates for face  eyes  and directions

 

next steps

    national center for educational statistics       
    r  kaucic and a blake  accurate  real time  unadorned lip tracking  in in iccv  pages        
     

based upon our error analysis results and algorithmic
compromises  we identify three topics of interest for
future work 
because eye detection is more of a bottleneck
than is classification  an important step would be to
generate a custom eye classifier cascade for use in
cvhaardetectobjects  which is robust against lighting conditions  to solve the issue with the looking downward classification  a second  nearly closed
eye classifier cascade can also be developed  finally 
we note that while left right classification works well
both when the head is turned and when the eyes
are angled  the same is not true for upward classification  our classifier only notices upward angled
eyes  adding a binary classifier to the entire detected
face that determines whether the face is angled would
solve this problem 
in general  our goal is to develop a product in
close feedback loops with the different stakeholders
in order to ensure product market fit 

    r  lienhart and j  maydt  an extended set of
haar like features for rapid object detection  in
ieee icip  pages              
    m  couper m  galesic  r  tourangeau and
f  conrad  eye tracking data   new insights on
response order effects and other signs of cognitive shortcuts in survey responding  public opinion quarterly              
    a  ng  supervised learning  cs    lecture notes 
    o  oyekoya and f  stentiford  exploring human
eye behaviour using a model of visual attention 
in international conference on pattern recognition  pages            
    j doyle r  argue  m boardman and g hickey 
building a low cost device to track eye movement       

references

     p  viola and m  jones  rapid object detection
using a boosted cascade of simple features  in
ieee cvpr  pages              

    eduflip team of the stanford startupweekend
         not all members are authors 

    s  amershi c  conati  c  merten and k  muld       q  wang  eye location in face images for driver
ner  using eye tracking data for high level user
fatigue monitoring  in in its  pages        
modeling in adaptive interfaces       
     

 

fi