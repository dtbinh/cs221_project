speaker recognition using deep belief
networks
 cs      fall              
alex fandrianto
afandria stanford edu

ashley jin
ashpjin stanford edu

  introduction
speaker recognition and verification is
essential in confirming identity in numerous realworld applications  speaker recognition is
identifying an individual speaker from a set of
potential speakers while speaker verification is
confirming a speakers identity as the true
speaker or as an imposter who may be trying to
infiltrate the system  in speaker recognition and
verification  one of the major challenges is
choosing good features as inputs to a classifier 
recently  interest in using deep learning
methods to learn features from audio data in an
unsupervised fashion has grown  the high level
representations learned in the higher layers are
found to have comparable and often better
performance than traditional features such as
mel frequency cepstral coefficients  mfcc      
in this study  we will explore the benefit of
using different features to transform input to a
neural network as well as the application of
convolutional neural networks in speaker
recognition and verification  we will study the
results on text independent corpora 

  data
for this project  we used two different
datasets  timit and the west point company
g  american english corpus 

    timit data
we performed initial experiments using the
timit dataset  timit is comprised of recordings
of     speakers      male      female     using
eight dialects of english  each speaker has ten
different utterances over a clean channel  no
noise   each utterance is about   seconds and is

aman neelappa
aman    stanford edu

phonetically diverse  the dataset contains about
     hours of audio in  wav format 

    west point company g 
american english speech data
we also used the west point company g 
corpus  g  contains recordings from    
speakers     male     female   there are    
possible utterances that cover all or most
possible american english syllables and each
speaker recorded     of those utterances  each
utterance is about   seconds  the dataset
contains about    hours of audio in  wav format 
we used only the male audio for our
experiments 

    feature extraction
we used matlab to extract features from the
raw data to input into our neural network  after
reading in the wav files  we used matlab to
generate a spectrogram  the parameters we
gave the spectrogram function were 
 window    ms  copied from     
 overlap    ms  copied from     
 frequencies     
 sampling rate             
 depending on dataset 
the spectrogram is formatted with time on
the y axis and frequency on the x axis  the
matrix of spectrogram values is of size number
of time windows  with overlap  x number of
frequencies  since the dimensionality is very
large and our training data set is small  we
extracted features from the spectrogram by
reducing the information to a single vector in
different ways  we discuss these reductions in
the experiments section 

 

filayers select the max of small non overlapping
windows of the previous activations to send to
the next layer  this reduces the computational
size of the problem and makes the architecture
robust to small translations in the input signal 

  convolutional neural
networks
the motivation for convolutional neural
networks  cnns  comes from the working of the
cats visual cortex  essentially the major ideas
can be summed up as 
a  exploit locality  instead of using a fully
connected network  connect a node to only a
few local nodes from the previous layer  higherlevel nodes will use more global information 
b  feature maps  sets of nodes at certain
layers are forced to have the same weights
connecting to the layer above them  the learned
feature maps allow the cnn to transform the
signal in a piecewise manner 
a cnns hidden layers are typically
interspersed with max pooling layers  these

  experiments
we tested numerous features extraction
reductions on our audio data  we inputted the
feature vectors to a neural network to generate
baseline scores  we then tested the same
features using a convolutional neural network to
determine effectiveness  in addition  we ran
experiments on varied network parameters 

    feature vector experiments

in order to generate more audio data  we
split each wav file into smaller
table    train time comparison with and without noise and normalization  chunks before performing reductions 
normalization decreases train time in both situations 
    timewise reduction  in this

no noise

noise

baseline

       seconds

      seconds

w norm

       seconds

       seconds

timit vs g  recognition error
    
     
    
     
    
     
    
     
    
     
 

reduction we collapse the time axis
of the spectrogram by taking the
average over time of the amplitude
for different frequencies  we expect
variations over time in the amplitude
to be more due to the exact text
being said and not necessarily from
interesting information about the
speaker 
    normalization  for each
dimension of the data  we subtract
the mean and divide by the
standard deviation in order to
achieve zero mean and unitvariance normalized input data 

    pitch  the pitch feature was
estimated via the cepstral method
from time domain auto correlation
trai
coefficients of the audio signal    
n
    noise  because both the
test
timit and g  audio data were
recorded over clean channels  we
wanted to explore the effect of
timit  
timit all
g    
g    
adding noise into the data  to do
this  we added gaussian white
figure    timit vs g  recognition error  the timit data has lower error than the noise to the wav files 
g  data for the tasks  the timit nn had     node hidden layer and used br  the
g  data used a    node and    node hidden layers and lm

 

fi    network parameter
experiments

lm versus br training function

    training function  there
    
     
are two options for training function 
    
by default  levenberg marquardt
     
    
 lm  is used because of its speed 
     
however  for smaller datasets 
    
     
bayesian regularization  br  is
    
train
preferred due to its increased
     
 
test
accuracy 
   
network
architecture
experiments  we experimented with
changing the network architecture to
see how it impacted the learning
process  we tested three sizes   
hidden layer of    nodes    hidden
layers of    then    nodes  and   figure    lm vs br error  while errors are low overall  br outperforms lm with
hidden layers of        and   nodes consistently lower error in the tasks  it appears that the g  data does not have
enough sound clips per speaker to make lm have lower error than br  we
respectively 
prefer to use lm due to its much shorter training time

  results

feature comparison   no added
noise

    timit versus g  corpus
timit recognition was tested on
a simple network with a single  node hidden layer using the br
training function  g  corpus was
trained using a        node hidden
layer network using the lm training
function  the resulting train and test
error are shown in figure   
we found that even though the
network for timit was smaller  the
classification error was lower  this is
because the g  corpus is more
difficult to classify since it contains
no distinct variations in dialect  all
male voices  and because the audio
clip speech is more diverse 

    network parameters

     
    
     
    
     

train

    

test

     
    
     
 
baseline

w pitch

w norm

figure    recognition    on feature experiments with no gaussian noise added 
baseline uses the time averaged amplitudes of each frequency of the data s
spectrogram  the pitch  middle  found by autocorrelation in the time domain
shows a possible improvement in the test data confirmed in figure   
normalization to mean    variance   unexpectedly does not improve results  as
verified in figure   

we compared lm and br on two
different recognition tasks  between
   people and between    people
 the largest set of people that matlabs memory
could handle   we found that br was more
accurate but lm was faster in both tests  this
confirms our previous knowledge about lm and

br  the accuracy comparisons are shown in
figure   

 

fi    feature results

feature comparison     agwn

when testing  we gave the
networks input data and specified a
      train and test split  we found
that our feature experiments generally
    
did not improve on the baseline that
was already very accurate  figure    
     
surprisingly  normalizing the input data
using zero mean  one variance did not
   
improve results  we believe this is
train
because we used the lm training
function that is invariant to scaling 
     
test
pitch was the only feature that
improved the classification  adding
    
gaussian white noise into the data
decreased accuracy and verified these
results  figure     when both the
     
clean and noisy data were normalized 
baseline
w pitch
w norm
training time was consistently lower
than when left unnormalized without a figure    recognition    on feature experiments with      additive gaussian
noticeable difference in performance white noise  the noise increases the error rates  making it clear that adding pitch
reduces the error while normalizing the data does not 
 table    
that once again our speaker tasks and data are
since the accuracy of the neural network
not complex enough to benefit from the use of
fluctuated with the number of layers  we were
cnns 
unable to draw conclusions about the benefit of
larger networks  this inconsistency arises from
the simplicity of the problem  the one layer
  conclusion and future work
network had accuracy of over     so adding
    conclusion
more layers to the network does not necessarily
because the baseline recognition and
increase performance for this problem 
verification system was already very accurate 
individual features did not tend to improve the
    nn vs  cnn
performance  additionally  cnns seem to
we found that cnn consistently performs
perform worse than nn at speech tasks in
worse than the regular nn in both speaker
general  however  this decreased performance
verification and recognition  for verification  we
may be the result of using a bad implementation
took created a universal background model by
of cnn or having data that is so clean that
taking a few audio clips from each person in the
preprocessing has little effect 
corpus and attributing them to new person  ubm 
we found the high errors produced by cnn
    future work
interesting and experimented by running the
future work in this area should focus on
original digit recognition data that came with the
combining and exploring new features to
cnn implementation through the nn  we found
represent the input data  it is possible that where
that the nn performed better on the digit data as
individual features fail to improve performance  a
well  thus  we conclude that the cnn was
more optimal combination of features would help 
poorly implemented 
providing vector instead of matrix input to
the cnn provided similar results  it is probable

 

fiadditionally  future work should try to use
both the female and male audio in order to test
the non dedicated imposter speaker verification
tests  since we are only using male audio  we
assumed a dedicated imposter for speaker
verification  finally  we hope to experiment with

data collected from noisier channels  it is
possible that our clean data and agwn data
was difficult to improve upon using these
features  thus  we hope to run experiments on
lower quality signals and see if our features
improve performance 

figure    nn vs cnn error  the cnn consistently performs worse than the nn  the errors in the verification and
recognition    tasks were extremely high compared to the nn baseline  the original cnn implementation was
intended for digit recognition  but nn still outperforms cnn on that data set  we conclude that we used a poor
cnn implementation 

  references
    h  lee  y  largman  p  pham  and a  y  ng  unsupervised feature learning for audio classification using convolutional deep
belief networks  advances in neural information processing systems  nips                      
    d  a  reynolds  automatic speaker recognition using gaussian mixture speaker models  the lincoln laboratory journal     
              
    t  kinnunen and h  li  an overview of text independent speaker recognition  from features to supervectors  speech
communication                   
    m  skowronski  wavreadtimit m  source code   april           http   www cnel ufl edu  markskow software wavreadtimit m 
november          
    n  seo  spcorrelum py source code   april       http   note sonots com scisoftware pitch html  december          

 

fi