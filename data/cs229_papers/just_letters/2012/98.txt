determining behavior of bid ask prices following
liquidity shocks
jia han chiam  chuanqi shen

   abstract
one significant challenge in the field of algorithmic trading is predicting the shortterm behaviour of stock prices after market events  this project aims to predict the
short term behaviour of the market following a liquidity shock 
   introduction
every stock has an order book of outstanding buy and sell orders  from which a
highest bid price and lowest ask price can be derived  the latter is always higher than
the lower  if someone bids higher than the lowest ask  the trade is immediately satisfied  
and the difference is called the bid ask spread  if a large trade occurs  all of the buy
orders at the highest bid or sell orders at the lowest ask will be satisfied  and the bid
price will fall or the ask price will rise accordingly  causing the spread to temporarily
increase  this phenomenon is known as a liquidity shock  and the market will respond
to it by refilling the order book  the new bid and ask prices may revert to their original
levels  or there may be a permanent change in the price of the stock 
in this project  we use machine learning techniques to construct a model that can
accurately predict a stocks subsequent bid and ask prices after a liquidity shock occurs 
   data collection
this project is based on a kaggle challenge      and uses data provided by the challenge
organizer  the data set consists of over         training examples  approximately    
of the examples correspond to buyer initiated liquidity shocks while the rest are sellerinitiated  a single example is described by   auxiliary values  e g  trade volume  and
    trade or quote events in chronological order  the first    events  called predictors 
are described by transaction time  type of transaction  trade or quote   bid price and
ask price  a liquidity shock occurs between the   th and   th event  the next    events
 called responses  are simply described by a bid and an ask price  figure   illustrates
the data provided 
given the auxiliary data and the    predictors  our task is to predict the values of the
responses  in order words  we want to predict how the bid and ask prices will change in
the immediate aftermath of a liquidity shock  while we are provided with data for   
response events  our project focuses on predicting only the first    responses 
 

fi 

jia han chiam  chuanqi shen

figure    taken from http   www kaggle com c algorithmictradingchallenge data
   problem formalization
the training examples are separated into two sets  corresponding to buyer initiated
and seller initiated shocks  the two sets are analyzed separately  we randomly shuffled
both sets and extracted    of the examples to use as our test set  while the remaining
    were used to train our models  our feature vector x i  consists of     values   
auxiliary values that we felt were relevant  size of trade causing the liquidity shock 
price of trade causing the liquidity shock  trade volume  in pounds  and trade volume
 in number of trades   and the bid and ask prices of the first     predictor  events  we
opted not to include the timestamps or transaction types in our feature vector  our
models output a vector y  i  consisting of    values  namely the bid and ask prices of the
    response  events immediately following the liquidity shock 
given the training set  xtrain   ytrain    we want to learn a hypothesis function h x 
 i 
 i 
that can accurately predict ytest given xtest   we used the error metric stipulated by
kaggle  evaluating a given h x  by calculating the root mean square of   y  h x    for
all pairs  x  y    xtest   ytest   
   methods
our models are benchmarked against a naive algorithm  which simply outputs   
copies of the last bid ask price in the set of predictors  we devised three different algorithms for making predictions on the data  the first employs a simple linear regression
model  and we used the normal equations to derive a  vector that minimises the root
mean square error of h x    t  x  
the next two algorithms follow a three phase framework we call clustering classificationregression  ccr  
     clustering  we divided the training data into clusters according to the general
trend of the post shock prices  for example  a stock may see its price revert to its original
value after a liquidity shock  or the price may continue increasing or decreasing in the
same direction  we accomplished this by running the k means clustering algorithm on
the    dimensional response vectors  we subtracted the last bid price of the predictor
vector from each y and normalized the y vectors before running k means clustering  so

fidetermining behavior of bid ask prices following liquidity shocks

 

figure    left graph is off by translation factor  right graph is stretched

figure    graph showing the mean y vector for each cluster 
that graphs that are off by a translation or a scalar factor  as shown in figure    will
be categorised together  our intention was to categorize stocks according to movement
trends  not the absolute value of prices or prices changes  figure   shows how the
post shock movement trends are broadly categorised 
     classification  after clustering the y vectors  we trained a classification algorithm
to predict which y cluster a data point  x  y  would belong to given the value of x 
the first classification algorithm we employed was gaussian discriminant analysis  the
second was a multi class support vector machine using an rbf kernel  we used the
open source tool libsvm    to perform the latter classification  we trained the gda
classifier on the entire training set  but we were only able to train the svm classifier on
        examples as it was too slow to be applied over a larger data set 
     regression  we divided the training set into groups according to the cluster labels
predicted by the classifier  and trained five separate linear regressions on each of these
subsets of the training set  obtaining                when performing predictions on a
test example  x i    y  i     we first ran the classifier on x i  to predict which cluster c i  the
y  i  vector would fall into  and then applied the corresponding linear regression model
to generate a prediction h x i      ct i  x i   
   results and discussion
from figure    we observe that the largest clusters in the buy and sell training sets
only account for       and       of the data respectively  if the predictor and response

fi 

jia han chiam  chuanqi shen

figure    distribution of training examples among the clusters 

figure    classification accuracy of svm and gda

figure    comparison of training set and testing set error

figure    graph comparing testing set error between algorithms
vectors were completely independent and the test set was drawn from the same distribution as the training set  a reasonable assumption as the test set was randomly selected
from the original kaggle data set   it would not be possible to achieve accuracies higher
than those values  figure   shows that our svm classifier in fact managed to achieve
a classification accuracy of       over the whole training set  while gda performed
significantly worse  its accuracy is still high enough to offer some predictive value  this
means that our classification algorithms were successful in detecting correlations between
the pre shock and post shock bid ask prices 

fidetermining behavior of bid ask prices following liquidity shocks

 

from figure    we note that svm and gda performed better than a simple linear
regression  with svm producing the lowest test error  however  both ccr algorithms
only performed slightly better than the original linear regression  although our classification algorithms are able to uncover patterns in the pre shock and post shock bid ask
prices  our linear regression model was unable to successfully exploit these patterns 
due to the large size of our training set  overfitting was unlikely to be an issue  and
indeed figure   shows that our test set error was in fact marginally lower than our
training set error  our models are therefore suffering from high bias  rather than high
variance 
   future directions and improvements
the high bias means that we may be able to improve on the algorithm if we use higherdimensional feature vectors  or perform a support vector regression with a nonlinear
kernel 
we can also consider changing the error metric used to evaluate the algorithms  and
the cost function the algorithms seek to minimize  as the root mean square error may
not accurately reflect the predictive power of our models  for example  if we were able to
estimate a confidence level for each prediction  e g  using the classifiers confidence in the
label prediction   and identify certain predictions as being more likely to be correct while
others were close to random guesses  we would be able to determine which predictions
we could build profitable strategies around and which we could ignore  such a model
would be superior to one which gave the same root mean square error over the whole
test set  but which did not indicate each predictions confidence level  e g  simple linear
regression  
the root mean square metric also heavily penalises large deviations  and the algorithm
is hence incentivised to produce conservative values  even if it has correctly identified a
trend in the data  for example  if a model determines that a price has a     chance
of increasing from y to y     and a     chance of decreasing to y    outputting a
value of y    would result in an expected mean square error of                  whereas
the naive algorithm which outputs y would produce an mean square error of    even
though the latter has no predictive power whereas the knowledge that a price is    
times more likely to increase than decrease would be considered valuable information  if
the algorithm were trying to minimise the mean square error  it would output y       
for a mean square error of       only very slightly better than the naive algorithm  even
though the model has significantly more predictive power   this value of y       is
arguably less useful than a prediction of y     and certainly less useful than the full
representation of the models internal state  a     chance of the price being y    and a
    chance of being y     which the root mean square metric has no way of evaluating 
references
    algorithmic trading challenge   kaggle  http   www kaggle com c algorithmictradingchallenge 
    chih chung chang and chih jen lin  libsvm  a library for support vector machines  acm
transactions on intelligent systems and technology                     software available at
http   www csie ntu edu tw  cjlin libsvm 

fi