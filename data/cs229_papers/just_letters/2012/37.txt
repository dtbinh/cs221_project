 

when machine learning meets ai and game
theory
anurag agrawal  deepak jaiswal

abstractwe study the problem of development of intelligent
machine learning applications to exploit the problems of adaptation that arise in multi agent systems  for expected long termprofit maximization  we present two results  first  we propose
a learning algorithm for the iterated prisoners dilemma  ipd 
problem  using numerical analysis we show that it performs
strictly better than the tit for tat algorithm and many other
adaptive and non adaptive strategies  second  we study the same
problem from the aspect of zero sum games  we discuss how ai
and machine learning techniques work closely to give our agent
a mind reading capability 
index termsiterated prisoners dilemma  evolution theory 
tit for tat algorithm  intelligent agent  ia   re inforcement
learning 

population of model based adaptive agents and fixed nonadaptive agents playing iterated prisoners dilemma  ipd  
the simulation environment is similar to axelrods well known
ipd simulation study environment 
the rest of the paper is as follows  we first introduce
the iterated prisoners dilemma problem and review previous
learning and evolution based approaches to its study  next  we
discuss our algorithm in detail  in the next section  we then
give a slightly different version of out algorithm when applied
to zero sum games  numerical analysis of the results obtained
is provided in the penultimate section  the paper ends with
a conclusion section with some discussion of possible future
work in this direction 

i  i ntroduction

ii  t he i terated p risoner  s d ilemma p roblem

the interaction of learning and evolution is a topic of
great interest in evolutionary computation                  it plays
an important role in application areas such as multi agent
systems  economics  politics  and biological modelling  each
of these studies have one common thread  they involve the
study of systems of interacting autonomous individuals in
a population  whether the individuals are artificially created
intelligent agents  human beings or other biological organisms 
some of the questions they study are  what will be the
equilibrium set of behaviours  will there be any equilibrium 
how can cooperative behaviours evolve 
approaches to answer these questions have been proposed
in last few decades                   the purpose of this paper is
not to study the evolution of cooperative behaviours  instead 
we take a specific example of a cooperation based game  the
prisoners dilemma  and propose an algorithm which does
strictly better than most of the traditional evolution based
algorithms in terms of maximizing the long term expected
payoff  further  we use these ideas to develop an intellegent
agent for a zero sum game  we find that our algorithm gives
positive expected returns in the long run when run against both
simple and evolutionary strategies 
we report on a simulation study which explores what
happens when learning and evolution interact in an evolutionary game scenario  we explore interactions in a co evolving

the basic version of prisoners dilemma can be presented
as follows  suppose there are two prisoners a and b  arrested
for being involved jointly in a crime  the police does not have
any evidence to file the charges  the police decides to offer
the following deal to each of the prisoners separately  if you
agree that the other prisoner was involved in the crime  and
the other prisoner remains silent  you will be set free and the
other prisoner will receive a ten year sentence  and vice versa 
but if both of you remain silent  both will receive a five year
sentence  the prisoners are not allowed to communicate with
each other  what should they do in order to minimize their
losses  assuming that a is not learned to bs decision making
process  the answer is simple  both should try to maximize
their expected profits  minimize expected losses   in the ipd
problem the same game is played k number of times  k    
is unknown to both the players  the outcome of the previous
game is known to both the players before starting a new game 
this gives an opportunity to learn from experience and make
better decisions in later games 
certain conditions have to hold in defining a prisoners
dilemma game      firstly  the order of the payoffs is important 
the best a player can do is t  temptation to defect   the
worst a player can do is to get the sucker payoff  s  if
the two players cooperate then the reward for that mutual
cooperation  r  should be better than the punishment for
mutual defection  p  therefore  the following must hold  t
  r   p   s secondly  players should not be allowed
to get out of the dilemma by taking it in turns to exploit
each other  that is  taking turns should not be as good an
outcome as mutual cooperation  therefore  the reward for
mutual cooperation should be greater than the average of the
payoff for the temptation and the sucker  r    s   t      to

anurag agrawal  anurag   stanford edu  is with the department of
electrical engineering  stanford university  stanford  ca  deepak jaiswal
 deepakjaiswal    gmail com  is with the department of civil engineering 
stanford university  stanford  ca  this work was conducted as a part of
cs     machine learning course at stanford university  a part of this
work will also be included in a paper to be submitted to ieee journal on
selected areas of communication  special issue on game theory in wireless
communications 

fi 

be definite  we will choose the commonly used values t     
r      p      and s     
the question to be asked here is that  as a perfectly rational
player  playing another perfectly rational player  what should
you do in such a game  the nash equilibrium solution to this
problem is to defect  we discuss this briefly below 
suppose you think the other player will cooperate  if you
cooperate then you will receive a reward of   for mutual
cooperation  if you defect then you will receive a payoff of  
for the temptation to defect payoff  therefore  if you think the
other player will cooperate  you should defect  to give you a
payoff of    but what if you think the other player will defect 
if you cooperate  then you get the sucker payoff of zero  if
you defect then you would both receive the punishment for
mutual defection of   point  therefore  if you think the other
player will defect  you should defect as well 
so  you should defect  no matter what option your opponent chooses  of course  the same logic holds for your
opponent  and  if you both defect you receive a payoff of
  each  whereas  the better outcome would have been mutual
cooperation with a payoff of   each  this is the dilemma  in
other games  there may not be a dominant strategy  and other
notions of solving the game are used  in nash equilibrium
the two players adopt a pair of strategies such that neither
player can get a better payoff by deviating from their strategy 
in other words  each strategy is a best response to the other 
depending on the game  there may be no nash equilibrium 
a unique one  or many equilibria  aside from these strategies 
there is another kind of strategy that can be considered  in
which players are allowed to use randomness  e g  a roll of
a die  to decide their moves  in game theory these are called
mixed strategies or stochastic strategies  whereas those without
randomness are called pure strategies 
in contrast to the rational conclusion of mutual defection  in
real life instances of prisoners dilemma  cooperation is often
observed  why is it so  one suggested explanation is that in
real life  the players would have an expectation that they may
meet the same opponent in the future  and he might remember
a previous defection and take revenge by defecting on us next
time we play 
in the iterated game  player strategies are rules that determine a players next move in any given game situation
 which can include the history of the game to that point   each
players aim is to maximize his total payoff over the series 
if you know how many times you are to play  then one can
argue that the game can be reduced to a one shot prisoners
dilemma      the argument is based on the observation that
you  as a rational player will defect on the last iteration   that
is the sensible thing to do because you are in effect playing
a single iteration  the same logic applies to your opponent 
knowing that your opponent will therefore defect on the last
iteration  it is sensible for you to defect on the second to last
one  as your action will not affect his next play  your opponent
will make the same deduction  this logic can be applied all the
way back to the first iteration  thus  both players inevitably
lock into a sequence of mutual defections 
one way to avoid this situation is to use a regime in which
the players do not know when the game will end  nature

could toss a  possibly biased  coin to decide  different nash
equilibria are possible  where both players play the same
strategy  some well known examples are 
    tit for tat  cooperate on the first move  and play the
opponents previous move after that 
    grim  cooperate on the first move  and keep cooperating
unless the opponent defects  in which case  defect forever 
    pavlov  cooperate on the first move  and on subsequent
moves  switch strategies if you were punished on the previous
move 
in our version of the problem we assume that k is large 
so that the opponents have enough opportunities to learn from
each other  also  the payoff matrix is given by table   
 c c         
 d c         

 c d         
 d d         

where  c  c          means that if both cooperate  both
get a payoff of    and so on 
in       robert axelrod staged two round robin tournaments between computer programs designed by participants
to play ipd  many sophisticated programs were submitted 
in each case  the winner was anil rapaports submission 
a program that simply played tit for tat  in       axelrod
carried out computer simulations using a genetic algorithm
 nowadays it would be called a co evolutionary simulation 
to evolve populations of strategies playing the ipd against
each other  in these simulations  tit for tat like strategies often
arose as the best but it was proved that they were not optimal 
in fact  axelrod used this to illustrate that there is no best
strategy for playing the ipd in such an evolving population 
because success depends on the mix of other strategies present
in the population  axelrods simulations illustrate a different
approach to studying the ipd one in which the players are
not perfectly rational  and solutions evolve rather than being
deduced 
iii  t he a lgorithm
in any study of games involving a multi agent system  the
players can be categorized under two broad sectors   adaptive
and non adaptive agents  consider a population of ipd playing
agents  during their lives  these agents meet and interact with
each other where they may choose to cooperate or defect
and receive payoffs as given by table      higher reproductive
fitness is shown by those who get higher payoffs  the choices
they make are prescribed by genetically determined strategies 
this is the scenario alexrod simulated in his experiment  in
to this population  we introduce our own set of players  by
mutation of a subset of existing set of players   these are the
agents who try to model the strategies of their fellow players
and use this model to maximize their own payoffs  hence 
the population now has a set of intelligent  exploitative and
adaptive agents  this is the scenario simulated by us in our
experiments 
it is evident that their are two kinds of interactions possible 
adaptive v s  adaptive  and adaptive v s  non adaptive  the
objective of each  adaptive  player who interacts with another
player is to first figure out whether the opponent is adaptive or
non adaptive  next  it should make an intelligent move so as

fi 

to maximize its long term profits and minimize the difference
in losses  in this study  we have chosen to restrict the strategies
under consideration to a class of finite memory stochastic
strategies  behavioural strategies  that can be described in
terms of fixed set of probabilities  this is general enough to
represent quite complicated strategies  but it does not include 
for example  some strategies defined by finite state automata 
however  most of the well known strategies for ipd fit into
the framework 
we define the probability function pn such that pn    
fn  e    e          en   gives an agent the probability of cooperation
in the  n     th move given the outcomes e    e          en of
the previous n moves of games played against a particular
opponent  here  f     is the function that depends upon the
strategies in play  in a pure strategy  the value of each
function f     is either   or    otherwise the strategy is said
to be stochastic  a zero order strategy is one in which fn is
a constant  for example  a completely random strategy is a
zero order strategy for which fn           for all values of
n  a first order strategy is one in which fn is a function of
en only  and is independent of e       en     for example  the
tit for tat strategy is a first order strategy in which pn      
if the opponent cooperated in the nth move  zero otherwise 
in other words 
pn       if en    c  c  or  d  c 

   

pn       if en    c  d  or  d  d 

   

where c denotes the outcome cooperate and d denotes
the outcome defect  p  is set to   by default when the
experiment starts 
we now introduce the three basic steps to the generalized
version of algorithm 
    play a bad game  i e  always defect  for the first
n moves against each new opponent  similarly  for the next
n moves against each opponent  play a nice game  isolate
the opponents who play with zero and first order strategies
using the outcomes of these  n moves  note that all the nonadaptive agents will be isolated in this step  that is  stratgies like tit for tat  random  always defect  alwayscooperate etc  can all be decoded in this step  goto step   
    if the opponent is non adaptive  play with the optimum
strategy  derived in the first step  for the next m moves  where
m    n   else  goto step    after m moves  loop back to
step   
    if the opponent is adaptive  use reinforcement learning
and adaptive control techniques to predict its strategy and
maximize your profits  for the next m moves  after m moves 
loop back to step   
the values of m and n should satisfy m     n   we
now define the parameters for our markov decision model
which will be used in step   of our algorithm  for each move 
there are   possible outcome states  c  c    c  d    d  c 
and  d  d  as depicted in table      we denote this set by
s  the set of actions a consists of two elements   c  d  
r   s  a  r is the reward function  the values taken
by the reward function are discussed in the previous section 
psa are the state transition probabilities  given a fixed policy

   s  a  the value function v  gives the total expected
reward in following that strategy 
x
v   s    r s    
ps s   s   v   s   
   
where  is the discount parameter and the the summation is
over the set of all states s  belonging to s  note that the
description of the value function is not yet complete  because
we do not know what our opponents current move is going
to be  we cannot complete determine the state s we want to
visit next  for instance  if we decide to cooperate  our next
state will be of the form  c  x  where x is random variable
taking values in  c  d   hence  we take the expected value of
the reward we expect to receive over the two possible states 
we define the optimal value function according to 
v   s    max e v   s  


   

where expectation is taken over the two possible states  the
optimal policy     s  a is given by
    arg max e v   s  

   



we solve the bellman equations numerically to compare
the performance of our algorithm with other algorithms run
in similar experimental environment  numerical analysis is
presented in the numerical results section 
iv  z ero  s um g ames
we now discuss some similar situations that arise in zerosum games  a zero sum game is a mathematical representation
of a situation in which the sum of profits  may be positive
or negative  of all the participants is equal to zero  in other
words  the profits are exactly balanced by the losses  the
nash equilibrium solution for a two player zero sum problem
can be easily obtained as a solution to a convex optimization
problem  we consider a problem of an iterated zero sum game 
one of the greatest temptations of designers in the gaming
industry has been to create a false impression of learning
and  until recently  machine learning hasnt been used in
many games  in complex zero sum games like poker  decision
making is influenced not only by the scenario of the current
game  but also by a learning agent that is composed of a
few fundamental parts like  a learning element  a curiosity
element  a performance element and a performance analyzer 
the learning element is the one responsible for modifying
agents behavior on each iteration of the underlying algorithm 
here we are assuming that the participants do not have any
information about their opponents playing styles before the
start of first game  the curiosity element is one that alters
the behavior predicted by the learning element to prevent the
agent from developing bad habits or biases  the performance
element is the one that decides the action based on output of
the curiosity element  the performance analyzer is the one
responsible for analyzing the outcome of the decision made
in the previous iteration and feeding it back to the learning element in the current iteration  decision trees  neural networks
and belief propagation networks are all goods methods for
modeling learning and reasoning  the most common pitfall
encountered in designing a learning ai using this approach is

fi 

that the computer program is taught badly if the human player
is not familiar with the game and or plays the game stupidly 
our algorithm  as discussed above  considers a reinforcement
learning approach to model the learning element  for zerosum games  we add the curiosity element of our algorithm to
make sure that we do not get trapped in the above pitfall 
the problem is as follows  two players are dealt one card
each  the cards have either   or   written on them with equal
probability  if you happen to receive a card which says   
it implies you have a strong hand  and if it says   it means
that your hand is weak  same is the case with your opponent 
now both of you are required to declare the strengths of your
hands  but both have an option of bluffing  the objective is
to make the best guess of the opponents hand and also to try
that your opponent is not able to guess your hand correctly 
if your guess is right and your opponents guess is wrong 
you get a score of    whereas your opponent gets     and
vice versa  if both are correct or both are wrong  both get
a score of    thus  in simple words  our objective is just to
maximize our own score  if the game is played exactly once 
the solution is to make a guess of opponents hand by flipping
a coin  and to bluff about your own hand hoping that your
opponent will believe you  we call this problem the bluffcatchers problem 
now  we deal with the iterated version of this problem 
assuming that you will play this game with your opponent a
large number of times  is it possible to make more educated
guesses of your opponents hand to determine wether or
not the opponent is bluffing  the answer is yes  we again
consider a similar experiment environment  the difference is
that now defecting takes the role of bluffing b and that
cooperating takes the role of telling truth  t    the steps
of the modified version of the algorithm are as follows 
    play a bad game  i e  always bluff  for the first n
moves against each new opponent  similarly  for the next
n moves against each opponent  play a nice game  isolate
the opponents who play with zero and first order strategies
using the outcomes of these  n moves  note that all the nonadaptive agents will be isolated in this step  that is  strategies
like tit for tat  random  always bluff  always telltruth etc  can all be decoded in this step  goto step   
    if the opponent is non adaptive  play with the optimum
strategy  derived in the first step  for the next m moves  where
m    n   else  goto step    after m moves  loop back to
step   
    if the opponent is adaptive  use reinforcement learning
and adaptive control techniques to predict its strategy and
maximize your profits  for the next m moves  before making
a move  turn to the curiosity element  with probability   the
curiosity element will inform you to switch to a sub optimal
strategy  if that indeed happens  keep a separate record of the
reward you got after playing this move  feed it back to the
curiosity element  the curiosity element will adjust the value
of  by comparing the reward you got in this move to the
rewards in previous moves  after m moves  loop back to step
  
the reinforcement learning model used is quite similar  and
hence is not discussed in detail again  thestarting value of 

fig       of times algorithm performs close to optimum versus the number
of games played 

fig    

expected score versus number of games played   ibc problem 

was chosen to be       the zero and first order strategies were
easily cracked  hence their plots are not included again  some
interesting observations were obtained when we compared
the performance of our strategy to the pavlov type strategy 
we observed that the nth order pavlov strategy performed
considerably well against our algorithm in the initial phase
of the simulatiion  the overall score it earned was high  and
sometimes even more than the learning algorthm  but the rate
of increase of its score always showed a decreasing trend 
brief comparison between the two strategies are made in the
numerical analysis section 
v  n umerical r esults
bellman equations are very hard to solve analytically even
with small state space as is considered in the current problem 
we employed the method of undetermined equations to solve
a few special cases of the bellman equations  but most of the
other  general  cases were solved numerically  we obtain some
useful insights related to the performance of our algorithm in
our simulation environment as compared to that of tit fortat  pavlov and a few special zero order strategies  fig   
shows the   of times different algorithms performs close
to optimum  here optimum refers to the score that would
have been achieved if you would have predicted each move of
your opponent correctly and hence would have chosen the best
possible series of actions  here close means that the score
obtained was at least     of the optimum score  we see a
clear distinction between the performances of our algorithm
and that of tit for tat when the number of moves is close to
      each algorithm was run against all the other algorithms
and the average performace was plotted as a functon of number
of moves  we observe that the algorithms which do not learn
with time  for ex  the always cooperate algorithm  were
punished badly by our learning algorithm and hence showed

fi 

fig       of moves for which maximum possible payoff is achieved
versus number of games played 

poor performances  tit for tat did well against some adaptive
algorithms  including our own algorithm  but did not exploit
the learning benifit against zero order algorithms 
fig    shows the   of moves for which maximum possible
payoff was achieved  as expected  the ml agorithm showed a
monotonically increasing graph as a function of the number of
moves  it implies that our algorithm gives better predictions
as the number of moves increases and hence increases the
total success    on the other hand  the tit for tat algorithm
gives a mediocre performance which is almost independent of
the number of moves  the performances of the pavlov and
the zero order strategies in this regard are also independent of
the number of moves  in fact  they tend to degrade with time
because the adaptive learning algorithm tends to screw them
up more often 
fig    shows the total average score achieved by our learning
algorithm and the tit for tat  each of these algorithms were
run against the other four algorithms and the score obtained
were recorded as a function of moves  the average of these
four scores were plotted for both the algorithms  an interesting
observation is that when our algorithm is sufficiently learned
 i e  number of moves        the rate at which the score
increases is itself an increasing function of the number of
moves  of course  this is a local phenomenon because once
our algorithm is sufficiently learned it cannot show this trend
anymore  on the other hand  although tit for tat also shows
an increasing trend  the rate of increase of score tends to
decrease after about      moves 
fig    compares the performances of the ml algorithm
with the pavlov type learning algorithm for the iterated bluffcatchers problem  both of these algorithms were run against
the other four algorithms and the score obtained were recorded
as a function of moves  the average of these four scores
were plotted for both the algorithms  one can see that pavlov
performes better than our learning algorithm initially  this
is because of the sub optimal performance given by our
algorithm in the initial  n moves  both the curves show a
decreasing score rate  however a considerable difference in the
scores can be noted when the number of moves       
vi  c onclusion
a machine learning approach to the iterated prisoners
dilemma problem has been studied  when the agents involved
use finite memory stochastic strategies  it has been shown
that the learning algorithm performs strictly better than the

fig     total score achieved by an algorithm versus number of games played 

tit for tat algorithm  the same algorithm  when applied to
the bluff catchers problem  with minor variations including
an introduction of a curiosity element   performs well against
the nth order pavlov type adaptive strategy  it also completely
exploits an opponent running a first or second order stochastic
strategies  for future work  we expect to apply modifications
of our algorithm to many complex zero sum games that we
encounter in daily life  one such example is poker  we are
currently in middle stages of an developing ai agent for poker 
vii  acknowledgements
we thank prof  andrew ng and cs     course staff at
stanford university for their continuous support with the
project work and course material  we also thank gowtham
kumar of electrical engineering department for several brainstorming discussions  although we tried our best to remove
any error  we understand that since our paper has not been
reviewed by peers  it is likely that we overlooked some
logical grammatical diction errors 
r eferences
    g  weiss  distributed artificial intelligence meets
machine learning
    m singh and m  huhns  challenges for machine
learning in cooperative information systems
    sklansky  david       the theory of poker  fourth ed   
las vegas  two plus two
    rabin  steve  ai programming wisdom  charles river
media  inc      
    http   ai depot com gameai learning html
    axelrod  r  and dambrosio  l   annotated bibliography
on the evolution of cooperation
    axelrod  r   the evolution of strategies in the iterated
prisoners dilemma  in genetic algorithms and simulated
annealing  l  davis  ed    pitman      
    philip hingston  graham kendall  learning versus
evolution in iterated prisoners dilemma
    fudenberg  d  and levine  d  the theory of learning in
games  cambridge  ma  mit press       
     fogel  d b  evolving behaviours in the iterated prisoners
dilemma  evolutionary computation  vol       pp       
     
     jehiel  p  and samet  d  learning to play games in
extensive form by valuation  naj economics  peer reviews
of economics publications          

fi