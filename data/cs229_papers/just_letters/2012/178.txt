how a bill becomes a law   predicting votes from legislation text

david goldblatt
tyler oneil

dtg stanford edu
toneil stanford edu

abstract

binary classifier

thanks to the efforts of organizations like govtrack  tauberer         a tremendous amount of
roll call and bill text data has become available
over the past few years  at the same time  most
analyses are based on ideal point models that use
only the votes themselves in modelling voting decisions  and ignore bill text  we analyze a variety
of models for predicting the outcomes of roll call
votes given vote text  we arrive at a neural network classifier which takes counts of words in bill
texts as input and achieves state of the art accuracy in predicting vote outcomes 

svm
random forest
naive bayes
regression nn
logistic nn
always yea

   introduction
academic interest and economic pressures have created an
entire field of quantitative political science that seeks to understand how governments behave  these researchers frequently use the rich dataset of legislative roll calls  which
are a history of how congress members vote on legislation 
in previous work  this data has been mined to find underlying structure like partisan affiliation  evidence of polarization  and even predict future voting outcomes  clinton
et al         gerrish   blei         these approaches typically involve complex models  such as ideal point modeling
that explicitly map legislators to a point along a political
line 
given a large and accessible dataset  and a field that is
historically reliant on complex modelling  we decided to
use simple bag of words models  a topic modelling based
classifier  and a neural network to address the problem of
predicting roll calls from the text of bills  these approaches
have the benefits that they are easier to implement and
understand  and make fewer complex modelling decisions
that are open for questioning 

   baselines
to begin  we trained models for each voter separately  we
took seven members of congress and trained six different
models on two thirds of their voting record  for a total of
forty two models  the training set and testing set were
randomly shuffled and split  our input features were the
top      most common words  with a binary feature of
whether or not they existed  we then tested on the remaining third of the dataset  we chose senators by being
active and recognizable  like the senate and house leaders

accuracy
      
      
      
      
      
      

table    classification test accuracies  classifiers were trained
on seven senators  and we report above the weighted average of
their accuracies  the svm was cross validated and tuned to use
a polynomial decision boundary of degree    both neural nets
used a logistic function for their hidden units  but the regression
used a linear output layer  whereas the logistic used a logistic
output layer 

and former presidential candidates  we used   republicans
and   democrats  all of the classifers except the neural net
were taken from matlab libraries   matlab       
the biggest challenge faced by classifiers is handling the
fact that there is very little data with high dimensional
features  some senators have as few as six hundred bills 
and each bill has      features  this creates a challenge for
algorithms like svms that cannot find a reasonably good
decision boundary in this high dimensional space 

   data cleaning
one problem we identified was the low quality of tokens
we trained on  many of the most common words in the
corpus were low content ones which did not impart significant insight into the meaning of a document  we adopted
a number of strategies to increase the quality of our input
features  these improvements to the source text increased
the accuracy of our final classifiers by a few percent 
as a first cleaning pass  we eliminated as many of the formatting effects as we could  the bill texts contain line and
page numbers  section headings  non ascii characters  and
numerous other characteristics that undermine the effectiveness of our classifiers  we therefore strip out any digits
or non ascii characters from our token stream  and then try
to join words which have been split across lines or pages 
this converts sequences of tokens like this appears at a
page boun          dary into this appears at a page
boundary 
we began using n grams rather than single word tokens
in computing features for a given document  this let us
capture phrases such as homeland security  which have
some semantic meaning that cannot be inferred from either

fihow a bill becomes a law   predicting votes from legislation text

figure    random forest performance

word in isolation  we additionally switched to using the
term frequency inverse document frequency  tf idf  scoring  jurafsky   martin         this increases the score of
an n gram that appears frequently in a document  but decreases it the more documents it appears in  so that words
receive higher scores for being distinctive to a document 
we removed short words  words of length   or less   which
tend to be low content  and which furthermore hurt some
of the significance of our n grams  consider department
of education in a bigram model  department education
captures more meaning than department of   we excluded stop words and lemmatized tokens to make the features independent of tense or declension 

   topic modelling
we still had two problems  first  despite the cleanup discussed above  our features tended to be rather low quality 
second  our classifiers had the ability to overfit to the data
available  the low quality of our features stems from the
lack of semantic information available from simple n gramcounts models  for example  some of the top     n grams
we choose  if picking n grams of length in between   and
   include house  described  project  within  entity  and authorized  these do not give us any information that might be useful in determining whether or not a
given representative will vote for a given bill  is the named
project being expanded or reduced  what changes in the
us code are being described  these facts dramatically
change the nature and meaning of legislation  and thus the
likelihood that representatives will vote for them 
the problem of overfitting is even more severe  for us to
be able to get even a gist of what most pieces of legislation
concern  we need to pick a few thousand representative ngrams  however  in the    session span we consider  most
members of congress will vote on at most a few thousand
pieces of legislation  and many will vote on far fewer      
voters out of the       in our corpus vote on less than one
thousand pieces of legislation    vote on fewer than    
this sort of data makes it very difficult to fit models with

figure    svm performance  the poly   line matches the sigmoid line closely  and is covered up by it

low test error 
to address these deficiencies  we tried to adopt strategies
to convert our large number of weak features into a smaller
number of stronger ones  to achieve this  we assume that
the documents are generated from some fixed number of
topics  and attempt to infer these topics from the documents  to be specific  we ran an online latent dirichlet
allocation algorithm  rehurek   sojka        on the tfidf scores of cleaned   grams  theoretically  we should have
used raw word counts rather than td idf scores  but in practice this approach gave better results   making    passes
over the entire corpus  at which point the topics seemed to
have converged   we then used the resulting output topic
distributions as the inputs to our classifiers  we ran experiments with             and     topics  using both random forests and support vector machines  implemented
in  pedregosa et al         as our classifiers  in the random forest classifiers  we tried maximum tree depths of   
        and     and in the svm classifiers  we tried linear  degree   polynomial  and sigmoid kernels  because of
the computational cost of training svms on the     topic
model  we only trained on     randomly selected representatives  rather than on every member of congress  the
test accuracy is reported via   fold cross validation performance  as a baseline measurement  the classifier that
assumes representatives will always vote yea has an accuracy of       on the corpus of all votes 
we see in figure   that the random forest classifiers tend
to overfit as the forest depth is allowed to grow  this makes
sense   even with relatively few features  the expressive
power of the forest quickly grows to be able to exactly
match any test set data  limiting forest depth therefore
functions as regularization of the model  we suspect that
with more data  this problem could be eliminated  another
promising avenue would be to adopt some sort of feature selection strategy such as mutual information between topics
and the output vote variable 
with svm approaches  we see in figure   an opposite ef 

fihow a bill becomes a law   predicting votes from legislation text

fect  the svm training and test accuracies match almost
exactly  and improve as more features are added  the problem is that despite this  we have relatively poor results 
this points to a need for more or better features  unfortunately  we seem to have levelled out at     topics  our
experiments with higher topic counts did not improve the
test or training error of any of our svm classifiers 

   neural network model
given the failure of topic modelling approaches  models
that more directly use vote outcomes in distilling counts
seemed like a promising approach  our goal is to map
a feature vector for a document to a vector of length n
with the predicted votes for voters i          n   we use
a neural network with layers j          j  each with a response of cj composed of aj neurons  that is  at each
layer j  we have responses cja          aj   to calculate
the responses  the neural net uses the standard non linear
function  cja    wja cj    bj   we use the sigmoid
function   x      e x   we did some experiments with
 x    tanh x  but found no significant difference 

figure    features  we ran the same single layer network with
   hidden units on several different document representations 
counts is a bag of words of raw counts  binary is a bag ofwords with only   for present and   otherwise  baseline is
the result of always guessing yea 

the highest level of the network is composed of a linear
regression layer  that only differs from the previous layers
in that there is no sigmoid function  we also tried an additional logistic layer but with little improvement  as in the
baseline experiments  the output layer uses tied weights 
in that we are predicting all the congress members votes
in the same model  during training  we ignore not voting  abstain and the case where the voter was not in that
house of congress at that time by not back propagating any
error  this has the effect of turning off that output node 
whereas our baseline models required us to train different
models for each voter  this model combines all those models into one with shared parameters for the hidden layers  this has two major advantages  first  the shared
parameters mean that training a model was much quicker 
training       different models is a daunting task  second 
the shared parameters help prevent overfitting by learning
structure common to all voters 

   experiments
we have       bills spread over nearly    years  and split
them into a training set of size        a test set of size
     and a validation set made of the rest  we shuffled the
bills  so they did not appear in any particular order  then
we trained a number of tied weight neural networks on the
data using different features and parameters 
     interaction with features
in addition to the tf idf features we explored in previous
sections we also tried a number of other features  first
of all  we tried just using the raw binary word features   
if the word exists in the document    otherwise  and raw
counts  in addition  we tried more complex features like
latent semantic analysis which is a form of dimensional 

figure    train test error over     iterations of l bfgs  we
ran the same single layer network with    hidden units on the
counts bag of words different document representations 

ity reduction of the tf idf matrix  dumais         since
logistic classifiers are not very effective with sparse data
in practice  we thought dimensionality reduction may help 
furthermore  we tried hashing the counts of all our words
into a feature vector  that is  for every word in the document  we hash it to find its position in a feature vector and
increment that feature  since the number of occurrences
of our words have a long tail  and many of these tail words
are likely good differentiators  like kodiak may indicate
an environmental bill   we hoped hashing would save the
long tail words in a way that would not overfit and keep
our feature dimensions small enough for training 
figure   shows the results of different features  our first
experiment with different features showed us that our document representation did not have a major impact  we
tried different numbers of features between       and      
in our bag of word models  but found little difference and

fihow a bill becomes a law   predicting votes from legislation text

figure    word count features  test error over     iterations
of l bfgs or stochastic gradient descent  single layer network
with     hidden units and the word count features         

eventually settled on        as for representation  using
counts was approximately as effective as more complex features like lsa and bigram counts  using raw counts turned
out to give us the best results on our validation set so we
continued our development on those 
     model structure
next  we tried different neural network models by tuning
the number of hidden layers and the number of hidden
units in each layer  once again  we found that simpler
did better  multi layer neural nets depressed results below
the naive baseline of always voting yea and any a single
layer network with above    units did about equally as
well  we were limited by machine memory  and had trouble
extending beyond     hidden units as a consequence of
our high feature dimension  time to train  and computer
memory 
figure   has the results of the best system we tuned  it uses
   hidden units using counts bag of words and performs
       accuracy on the     held out test set bills  this
is    below the baseline of     error rate of always voting yea  additionally  it does not show signs of enormous
overfitting or underfitting  without tied weights  the training error dropped below    and the testing error spiked
well above the naive baseline  so we concluded that this
behavior was a positive side effect of using tied weights 
we experimented with regularization using both dropout
 hinton et al         and weight decay  adding a quadratic
penalty to weights appearing in the model   as figures  
and   show  we get much better performance on binary
features  which we were overfitting 
     comparison with gerrish   blei
to compare with gerrish and bleis paper  we re created
an experiment where they partitioned six   year congresses
into   folds each for a total of    folds  they then per 

figure    binary word features  test error over     iterations
of l bfgs or stochastic gradient descent  we ran the same single
single layer network with     hidden units on the binary bag ofwords document representation  we used a weight decay term 
 of     

formed k fold cross validation on each congress separately 
with this technique  their baseline accuracy of always voting yea had an accuracy of      and their model had an
accuracy of       gerrish   blei       
when re creating this experiment  we found that the baseline was not      but        from that baseline  using
count features and parameters tuned on our validation set 
we found we had       accuracy  using a neural net to
learn the structure that gerrish   blei explicitly modelled
was at least equally as effective 

   network analysis
having achieved performance competitive with traditional
approaches  we set out to see if the neural network learned
structure like party lines and topics that gerrish   blei explicitly modelled  all the following experiments were done
on a network that used word count features with a single
layer of    hidden units 
our first experiment was visualizing the hidden unit activations for all the documents in a high dimensional space
to see if partisan clusters are obvious  figure   and   show
our results with t sne for visualizing in two dimensions
 van der maaten   hinton         the votes are colored
by sander levin and roy blunts vote outcome on each bill
to see party divides  we chose these two senators because
they were to the far left and far right of gerrish   bleis
ideal point model  so we believed we could see party separation clearly  we were pleased to see the two figures have
a few clear clusters that change from green  voting nay 
to blue  voting yea  between the senators  for good measure  we also tried a few other representatives who had long
voting records and found that they colored the nodes about
as predictably  our model seems to be learning party separation clearly by projecting bills into a space where they
are easily differentiated by partisan biases  there is also a

fihow a bill becomes a law   predicting votes from legislation text

larger cluster farther away that encompasses bills that are
universally agreed upon  also  we can see that not voting bills are very evenly distributed amongst the clusters 
showing that our objective function is ignoring them well 

to more complex models  while revealing structure about
which topics affect voters in which ways 

   notes
this is a joint project with cs    n  david is in both
classes  but tyler is only in    n  both sets of staff have
confirmed this is fine  we used some neural network code
written by andrew maas for a project tyler is working on
separately  we modified the output layers  dropout  and
gradient descent   we also borrowed the icml latex
style 

references
clinton  j   jackman  s   and rivers  d  the statistical
analysis of roll call data  american political science review                      

figure    sander levins votes  we took the neural net response
and colored them by how sander levin voted on them  we used
t sne to visualize the high dimensional features 

dumais  s t  latent semantic analysis  annual review
of information science and technology               
     
gerrish  s  and blei  d m  predicting legislative roll calls
from text  in proc  of icml       
hinton  g e   srivastava  n   krizhevsky  a   sutskever 
i   and salakhutdinov  r r  improving neural networks
by preventing co adaptation of feature detectors  arxiv
preprint arxiv                 
jurafsky  daniel and martin  james h  speech and
language processing  an introduction to natural language processing  computational linguistics  and speech
recognition  prentice hall ptr  upper saddle river 
nj  usa   st edition        isbn            
matlab  version         r    a   the mathworks inc  
natick  massachusetts       

figure    roy blunts votes  we took the neural net response
and colored them by how roy blunt voted on them  we used
t sne to visualize the high dimensional features 

pedregosa  f   varoquaux  g   gramfort  a   michel  v  
thirion  b   grisel  o   blondel  m   prettenhofer  p  
weiss  r   dubourg  v   vanderplas  j   passos  a  
cournapeau  d   brucher  m   perrot  m   and duchesnay  e  scikit learn  machine learning in python   journal of machine learning research                    

we tried to analyze the clusters from the titles of the bills
in each cluster and had some trouble coming up with very
obvious similarities between the bills  however  there were
some patterns  like the large area of all blue seems to take
many inconsequential bills  e g  a bill to designate the
square dance as the national folk dance  

rehurek  radim and sojka  petr  software framework
for topic modelling with large corpora  in proceedings of the lrec      workshop on new challenges for
nlp frameworks  pp        valletta  malta  may      
elra  http   is muni cz publication        en 

   conclusion

tauberer 
joshua 
govtrack 
http   www govtrack us data us         accessed 
           

we introduced a neural network architecture for predicting
the outcomes of votes in the united states congress and analyzed the performance of different feature representations
and regularization techniques  we also explored topic modelling  which turned out to be a less effective approach  our
results show that neural networks have similar performance

van der maaten  l  and hinton  g  visualizing data using
t sne  journal of machine learning research                       

fi