botwar simulation using fuzzy sarsa   learning
by francois chaubard and nikolaus west
fchaubar stanford edu
nwest  stanford edu
stanford university
   approach

abstract
in this paper  we detail the analysis and results of a
reinforcement learning experiment in the case of a bot war
simulation  using a reinforcement algorithm called fuzzy
sarsa   coupled with a few   d pyramid member
functions  we were able to achieve the performance of
regular sarsa   with     million times less states  we
were also able to achieve a convergence rate that was  
times faster 

   introduction
in simulated bot war multiple bots  or agents  compete
against each other in teams  on a gridded playing field 
with the objective of inflicting as much damage as
possible to opponents  in our set up  several bots  in
several teams learn simultaneously  both how the game
works and how to cooperate and compete in it  we
developed a bot war simulator to see how effective a
reinforcement algorithm would be and what could be
explored to better the results  the environment of the
game is set as such 

figure    layout of a bot war

a hurt is logged only when a bot is positioned to the side
or behind another bot  at each step of the game  every bot
must choose between four actions  stand still  turn left 
turn right  and move forward 
every game is run for    steps and the size of the
environment is  x   a team is declared the winner when
they have a combined higher score  which is number of
hurts to the enemy less number of hurts to yourself and
teammates 

during our implementation  we originally used sarsa  
learning  due to memory restrictions and limited computer
ability  we altered this approach to using an algorithm with
state interpolation called fuzzy sarsa   

     sarsa  
for each bot  the game is modeled as a markov decision
process  mdp  where the x and y distances and relative
orientation of each teammate and enemy  as well as the
bots own absolute position and orientation define the
state  s  all actions  a  are modeled as possible actions in
all states  actions that are in fact impossible to take in a
certain state will simply not change the agents position 
the transition probabilities are not explicitly
modeled learned  instead  the bots learn the values of a
state action value function  q matrix   the q matrix is
learned with the on policy algorithm sarsa    using
replacing eligibility traces described in      an  
 policy is used to select an action given the values
of the q matrix corresponding to the current state  the
algorithm  almost exactly as defined by sutton   barto 
      section      is as follows 
    
      
                  
      
               
            
                    
                                                                
                              
                  
              
                                
                       
                       
                
       

fieach time an action a is chosen in state s the eligibility
trace      for that state action pair is set to    every
round all eligibility traces are decayed by the trace decay
parameter   the trace values are then used so that all the
state action values are updated dependent on how long ago
it was the corresponding state action pair was visited
before the current reward was observed   is the learning
rate   is the temporal difference for the current time step 
and  is the discount factor  which is used to discount the
value of future rewards versus rewards in the current
time step 

     fuzzy sarsa  
the main problem with sarsa   applied to bot war is
that the expected value of each state action pair is saved
and the number of states is given by the huge number
                          in games with z grid positions  n bots 
and   orientations  to get around this problem a fuzzy
sarsa   learning algorithm     was used  where the
states experienced by one bot are expressed as all the
other bots responses to a set of fuzzy membership
functions  with m well chosen membership functions  this
results in that a state can instead be expressed as a sparse
vector with                            elements 
instead of using separate membership functions for the xand y directions  we used pyramid shaped membership
functions laid out on the  d playing field relative to the
observing bot  the response of a membership function to
another bots position is given by the height of the
pyramid at that bots relative position to the observing
bot  the height of each pyramid is    and the responses to
each of the membership functions to the relative positions
of all the other bots are normalized to sum to   and
concatenated to a vector  s   where the i th element is
given by          
the pyramids overlay each other in varying degrees to
make use of the fuzzy formulation  we use five outer
pyramids that describe positions not close to the observing
bot  and       for each direction  orientation sensitive
pyramids to describe positions and orientations close to
the observing bot  the layout of these pyramids can be
seen in the figure below 

figure    the layout of the    pyramid member functions  the green
pyramids are angle insensitive  the yellow are angle sensitive
meaning there are four overlaid pyramids for each 

the q matrix is a matrix with  rows and   columns 
   

where the entry at          represents the expected
value of taking action a if         the   
policy in state s therefore becomes 
          
     max  s q
 

   

    max

   

   

   

                

        

   

we also use focused replacing eligibility traces described
in     which changes the update for       given in
    to 
 

 
 

 

          

  
   

           

                
   

    
            
                    
         
                               

   results
we ran our fuzzy smart bots  bots using fuzzy
sarsa   learning  against three types of opponents 
really dumb bots  not moving bots   dumb bots
 bots moving in uniformly random fashion   and smart
bots  bots using sarsa   learning   our results break
down into three findings 
first  fuzzy sarsa  
learning is a very effective
method for this type of learning situation  during the
really dumb trials  the fuzzy smart bots converged to a
direct path to the enemys vulnerable positions and sat

fithere for the remainder of the game collecting points  they
found this optimal path in less than ten games  during the
dumb trials  the fuzzy smart bots also saw similar
success 
second  our analysis shows that fuzzy sarsa   using
   pyramid member functions reduces the state size by
many orders of magnitude  in the  v  case  the number of
states for sarsa   amounts to            as calculated
by equation      which is            times higher than the
fuzzy sarsa   state size of     one might ask what
the performance cost of such a dramatic reduction of state
size is  figure   shows the reduction in performance is
very minimal  the fuzzy sarsa   does not begin to
lose until nearly      runs in the  v  fuzzy smart vs 
smart case  and even then the advantage is no more than a
score of one point  also  figure   shows the learning rate
has increased   fold 
lastly  showing that the fuzzy smart bots can defeat a
dumb bot in an even fight is good but our team was more
interested with how the fuzzy smart bot would fare in an
uneven fight  as shown in figure    one fuzzy smart bot
was able to defeat up to   dumb bots at once  beyond this
point  our bot cannot handle additional enemies 

figure    for the really dumb  v  trials  the fuzzy smart bot
learns the optimal policy in    games compared to sarsa   which
took     games 

we then wanted to ensure that the algorithm would still be
successful when there are more than one bot per team  as
shown in figure   below  the five bots learn to each go
after one specific bot and not get in each others way 

     fuzzy sarsa   vs  dumb   really dumb
we tested our learning algorithm in a number of ways 
the first test was against an enemy that simply stands still 
below is the output of such a trial  the algorithm
converges to the optimal solution in    games 

figure     v  really dumb case  note that all bots find the optimal
policy and do not get in each others way 

now that we saw that the algorithm worked we wanted to
see how it fared in more precarious situations  we tested
the algorithm in a  v  battle against a uniformly random
moving enemy  as show below  all of the fuzzy smart
bots worked together to defeat the opponent  we noticed
that the strategy that began to form was for the bots to
clump together and only move to attack when there is an
easy chance to strike 

fifigure    using classic sarsa     we see convergence occurs at the
  th game  fuzzy sarsa   converges at around   games  notice
the big spike at game      this is the epsilon value taking effect 
figure     v  dumb trial  notice that the fuzzy smart bots are able
to consistently defeat the meandering enemy bots 

we were also interested in seeing how the two learning
algorithms fared in a head to head trial  below we show a
fuzzy sarsa   bot vs  sarsa   bot 

     fuzzy sarsa   vs  sarsa   learning
we originally implemented sarsa   learning but
quickly realized the memory intensiveness of such an
algorithm would not allow us to scale a battle to have
anymore than   bots  in a  v  case  the q matrix fills at
an alarming rate and actually occupies the    megabytes
available in the jvm causing the program to crawl and
then eventually crash  running the fuzzy sarsa  
algorithm takes no where near that amount of memory 
allowing the program to easily scale to higher number bot
trials  we successfully ran a   v   trial which would
never be possible with sarsa   learning  the drawback
to such a dramatic reduction is of course correctness of
action choice and overall ability of the bot  in fact  we
saw the opposite of this  the fuzzy smart bot
performance against the really dumb bot converged in
about   trials on average while the smart bot learned in
about    trials as seen in the figure below 

figure    fuzzy smart vs  smart trial  notice that the fuzzy smart
bot easily wins the first      games  the smart bot does not begin to
win until the     th game 

since the fuzzy smart bot learns much faster  he wins the
early games with ease and every time there is a change in
the policy action of the other bot  the fuzzy smart bot
quickly adapts to it  we noticed also that when the smart
bot finally begins filling its q matrix  its policies are
better than the fuzzy smart bot  this is to be expected
since an approximation of the states will never be as good
as a full state representation 

     the hunger games
our final round of tests conducted we dubbed the hunger

figames  fuzzy sarsa   learning is good but we
wanted to see how good  pinning one fuzzy smart bot
into a battle against many dumb bots gave us some very
interesting results  figure   below shows an example of a
 v  trial 

pyramid member functions  their positions  size  and
number of pyramids  we believe this good improve results
even more  function approximation for multiple agent
learning problems is an active field of research within the
machine learning community  which in itself makes it an
interesting problem  specifically  we are interesting in
extending the game presented here to a zynga style online
war bots manager online computer game 

references

figure    the one fuzzy smart bot vs  seven dumb bot trial shows
the robustness of the algorithm 

we found that the bot could conquer up to    other bots
with ease  however  any additional soldiers added to the
enemy team would give a different result  as shown in the
figure below  the fuzzy smart bot can not defeat    other
dumb bots 

figure     v   overwhelms the fuzzy smart bot  notice the bot does
not lose by much  in fact the bot usually breaks even 

   conclusion
the analysis shows how effective a good state
interpolation technique can be  with the addition of just   
membership functions per soldier  we were able to achieve
a faster learning rate  no real loss of performance  and a
memory reduction that is around   orders of magnitude 
future research must be done on selecting the optimal

    singh  s p   sutton  r s   reinforcement learning with
replacing eligibility traces  machine learning     
         kluwer academic publishers  boston      
    sutton  r s   barto  a g   reinforcement learning  an
introduction  the mit press  cambridge      
    theodoris t   hu  h   the fuzzy saraa   learning
approach applied to a strategic route learning robot
behavior  proceedings of the      ieee rsj international
conference on intelligent robots and systems  beijing 
china       
    kamdem  s   ohki  h   sueda  n   fuzzy sarsa with
focused eligibility traces for robust and accurate control 
ieej transactions on electronics  information and systems 
volume      issue    pp                  

fi