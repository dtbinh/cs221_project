topic modeling using lda with feedback mechanisms
alireza bagheri garakani
department of computer science
university of washington
seattle  wa  usa
abagheri cs washington edu

abstracttopic models provide a way to identify the latent
topics from a collection of documents  although the identified
topics often appear quite representative of the data  just as often 
there are parts of the output that appear erroneous or otherwise
difficult to interpret by humans  this is a limitation of topic
models that can be remedied by user feedback mechanisms  in
this paper  i discuss two feedback actions  removing a word from
a topic and removing a topic from a document  i apply these two
functionalities in the framework of collapsed gibbs sampling of
an lda model using a subset of the wikipedia dataset  a
preliminary evaluation of this method shows that such feedback
mechanisms can be useful and efficiently implemented 

i 

introduction

finding documents among a large collection  whether that
collection is some digital repository of documents or the
internet  is an increasingly difficult task to carry out in a fast
and effective manner  search engines continue to better address
this problem using sophisticated ranking algorithms 
performance metrics for feedback  and even personalized
results using a users social data  however  perhaps there is
better way than repeatedly submitting queries and choosing
among ranked results to discover content  i e   a better way to
refine  broaden  or redirect your search domain  
with the ability to identify the underlying themes of
documents  topic models may provide a better way to represent
and navigate among documents  one popular method for topic
modeling is latent dirichlet analysis  lda       which
defines a generative model for documents where each word is
determined by a latent underlying topic structure  the
implications of such a model allow for a greater interaction
with documents  and similarly words and topics   for example 
imagine a document based on a distribution of topics and
effectively narrowing down or around a topic of interest and
discovering more relevant documents      yet  lda have farreaching applications beyond text and topic modeling  viewed
as a mixed membership model of grouped data  it has been
extended to find patterns in genetic data  images  and many
other areas     
one less explored area of research is how to incorporate a
user triggered feedback mechanism to help the algorithm
converge to a more desirable set of topic labels  this is
essential because  based on a variety of factors  the
unsupervised learning of topics may be poor  i e   poor text
normalization  improper choice of topics in corpus  and others  
as such  it is often desirable from a users perspective to tweak
the results and have the algorithm dynamically adapt  note that
when viewing this feedback as coming from multiple users we

have effectively provided a way for lda to benefit from
crowdsourcing  as many other applications have shown to be
useful 
since we are discussing a mechanism that involves user
interaction  visualization is clearly a very important element 
the proper user interface should not only illustrate the output
of the topic model  but should also incorporate an intuitive
design for performing the feedback actions 
in this paper  i explore two feedback functionalities 
namely  removing word from a topic and removing a topic
from a document  i will show that these functionalities are
intuitive and efficient within the framework of lda inference
via the collapsed gibbs sampling  a preliminary evaluation of
this method using a subset of the wikipedia dataset appears to
show promising results 
ii 

related work

a  topic modeling
among choices for topic modeling techniques  lda is one
of many options  buntine and jakulin explore several
alternatives and how they are related       however  even
within lda model  there are a vast number of variations 
both in terms of model formulation and inference techniques 
among many others  variations of lda include modeling
the change in topics over time      removing the bag ofwords assumption         and even using non textual input
 like learning natural scene categories in a collection of
images       blei provides a more thorough overview of these
and other variations      in this paper  i use the standard lda
model as described in     
moreover  since the task of exact inference is not tractable 
there are also many variations to the method of inference  this
is well summarized by asuncion et al       in this paper  given
its simple implementation and ability to easily integrate
feedback  i use the collapsed gibbs sampling  cgs  method 
which is briefly discussed in the next section         
b  visualization
there has been a large and diverse amount of visualization
techniques for topic model output  especially with extensions
to the basic lda model         
with the standard lda model  it is relatively simple to
display many different types of information beyond document
topic labels  similar documents  or topics or words   most
relevant documents based on a particular topic  most relevant
words from a topic  among many other things             

fichaney et al  implements a topic model browser that displays
this information and others       however  despite the various
types of information and clean interface  i believe that
quantitative information on the ui is important for
understanding the data  further  as it relates to providing
feedback  it will be important to see how distributions change
as a result of different actions 
there has also been a lot of work on different ways to
navigate from one document to the next  gerrishs disciple
browser allows users to refine a topic distribution that is used
to match against other documents       for example  this
allows for refining a query on iran more towards the topic of
archaeology and away from the topic of political science 
c  feedback
there is less work on methods to provide feedback to the
algorithm  some very recent work by hu et al  explores this
idea using gibbs sampling and a tree based topic model that
incorporates a metric of correlation between words       in
other words  adding a positive correlation between a pair of
words would encourage this pair to form a topic  i e  merge  
while adding negative correlation between this pair would
encourage the pair to find different topics  i e  split        the
motivation for using correlation appears valid as it enforces a
soft constraint  instead of explicitly fixating or banning a word
from a topic  however  one downside is that this may require
many more samples in order to again converge  at least     for
techniques described in        using the standard lda model 
i explore a simpler solution by creating a hard constraint on
whether a word is welcomed within a topic or not  i show that
it meets expectations and briefly discuss its limitations 

figure    plate diagram for lda showing how the latent
variables  z      and constant hyperparameters     
determine each word  w   figure from    
the goal of lda is not to generate words w based on
known topic distributions  but rather  to work in the reserve
direction  you are given a collection of documents and must
find the latent topics assignments 
b  posterior approx via gibbs sampling
as discussed earlier  there are many different approaches to
determining these latent variables  i use collapsed gibbs
sampling as described by steyvers and grifths          this is
an iterative sampling technique used to approximate z  which
can then be used to arrive at  and   this algorithm is
summarized at a high level in algorithm   
   randomly assign topics to every word 
   repeat sample  
a 

one important result from hu et al  is that each round of
user feedback via mechanical turk showed improvement in
relative accuracy       here  accuracy was measured using
document labels from their dataset  this shows promise for
incorporating user feedback and interacting with the algorithm 
iii 

i  remove topic assignment for t 
ii  approximate topic assignment for t  given all
other topic assignments   equation   
iii  if not initial burn in period  accumulate sample
s

method

a  lda
lda is a generative model that is assumed for producing a
collection of documents  within this model  we can generate
a document by selecting a distribution over topics  d  and
sampling a topic labeling z for each word  with each words
topic label z determined  we can proceed to selecting the word
w itself  this is done by sampling the distribution over words
for each respective topic  z   in lda  we define  and  as
symmetric dirichlet distributions with hyperparameters  and
  respectively      furthermore  the sampled topic z and word
w z  as described above  are represented as multinomial
distributions  these choices for distributions are convenient as
the dirichlet distribution is a conjugate prior for the
multinomial distribution         a summary of this generative
process is illustrated using plate notation in figure   

for each token t in corpus  

 
 

  

using s  estimate  and    equation   

algorithm   
equations   and   are described in by steyvers and grifths
in     and are not replicated here 
c  incorporating feedback
i explore two different feedback operations      removing a
word from a topic  and     removing a topic from a document 
i will first describe the former  however  i note in advance that
the following explanation can be easily extended to the latter 
intuitively  when a user marks a word as poorly
representing a topic  this means that we should reassign our
word to other topics  however  the question as to which new
topic this assignment should be made needs to be addressed 
since a given word is represented by many topics  we will
select an assignment based on these other topics  sampling a
multinomial distribution based on the current weights of these

fiother topics has the effect of dispersing word assignment
among other likely topic labels 
this can be viewed as running the sampling step defined in
algorithm   with two important modifications  as shown in
algorithm    firstly  we only need to resample tokens that
match the word and topic label of the feedback action  in other
words  this can be seen as selective re sampling of topic
assignments  further  there are substantial performance
benefits here since we only perform on a small subset of a full
sampling procedure 
for the second modification  we must remove topic
assignment for all words in the first step before finding new
assignments for any word  this will help ensure that the
multinomial that we sample in subsequent steps appropriately
represents alternative topics for assigning our word 
   let disperseset be the set of all tokens that equal the chosen
word and topic pair to be removed 
   for each token t in disperseset  
a 

remove topic assignment for t

 
   for each token t in disperseset  
a 

approximate topic assignment for t  given all other
topic assignments   equation   

 

algorithm   
this approach for removing a word from a topic can
trivially be extended to removing a topic from a document  in
this case  we select every token that matches the chosen topic
and document pair to be part of our disperseset  having
defined this set  we sample new topic labels from a
multinomial distribution over other topics in the document 
iv 

dataset

as my corpus  i used a total of      random articles from
wikipedia  the initial format of this data was in the form of
an sql dump with significant amount of unneeded metadata 
like timestamp of article creation  username of last
contributor  etc    even further  the article text had to be
reformatted from wiki markup language to plain text  i used
an open source tool  wikipedia extractor  to extract this plain
text directly from the sql dump      
the next step is text normalization  this step
required some trial and error to work properly  in the end  my
changes to the original text included removing stop words 
non alphanumeric tokens  no alpha tokens  tokens with
extremely high frequency  similar to stop words   tokens with
frequency of only one  made tokens lowercase and a few other
minor changes  these effects can be seen in figure   

anarchism is generally defined as a political
philosophy which holds the state to be
undesirable  unnecessary  or harmful  or 
alternatively  as opposing authority or
hierarchical organization in the conduct of
human relations  proponents of anarchism 
known as  anarchists   advocate stateless
societies based on non hierarchical voluntary
associations 

anarchism
generally
defined
political
philosophy
holds
state
undesirable
unnecessary
harmful
alternatively
opposing
authority
figure    running text normalization on the article text from
the wikipedia  left  prepared the input for lda  right  
i 

evaluation and discussion

as evaluation  i ran lda with the wikipedia normalized
text as input and a small value for number of topics  t       
this small value is not representative of the vast number of
topics in our corpus  however  it benefits from faster sampling
and was preferable for evaluation on my desktop machine 
nonetheless  these values suffice for demonstrating the
proposed feedback mechanisms  for choice of
hyperparameters  i used values as suggested by      where
    t and        i also chose the number of burn in
iterations to be     which appeared to be decent from prior
experimentation 
we would like to observe the effect of removing a word
from a topic  for this experiment  i used the same article as
mentioned above in figure   and i remove the word man
from topic    at the time of action  this is the top topic for
the document  per our implementation  the topic distribution
now missing that word should be updated accordingly  see
figure     similarly  this word should be dispersed among
remaining topics  see figure     both figures illustrate the
effect of removing man from topic   from the point of view
of topic   and topic     there are no sampling iterations that
occur within the snapshots shown below  both figures are
accompanied by percentages to illustrate the effect of this
feedback action 
king
man
led
son
men
war
end
roman
empire
battle
action  remove man from topic  
king
top    words for topic  
        of document 
men
led
war
top    words for topic  
        of document 

     
     
     
     
     
     
     
     
     
     
     
     
     
     

fison
     
end
     
roman
     
empire
     
battle
     
emperor
     
figure    removal of word man causes it to be dispersed
among other top words within topic  a new word  emperor 
emerges among the top    words for topic   
men
     
man
     
red
     
led
     
india
     
china
    
afghan
     
han
     
attack
     
chinese
     
action  remove man from topic  
     
top    words for topic    men
        of document 
man
     
red
     
led
     
china
     
soviet
     
india
     
afghan
     
han
     
chinese
     
figure    removal of word man in topic    encourages
other topics  like topic     to represent it more strongly 
top    words for topic   
       of document 

from figure   we observe the effect of removing man
from topic    as we would expect  it is quickly removed from
the top    words defining topic    observing this effect on
the second dominant topic of this document  topic      we
notice this topic tries to compensate for the occurrences of
man within the document 
it should be restated that no sampling iterations took place
after the action was taken  this is important because we would
expect that with subsequent sampling these topics would
converge to a better result  it should also be noted that  due to
the insufficient number of topics defined and a lack of
iterations prior to the action  topics do not appear to represent
discrete ideas yet 
furthermore  this evaluation is hardly sufficient for
demonstrating the impact of the remove word action on the
entire set of documents  topics  and words  taking a lesson
from hu et al  work  it would be interesting to use mechanical
turk and use similar evaluation metrics on the resulting model
to gauge its value      

music  album  song  band  record  play  bass  release 
form  sing
computer  problem  work  science  research  machine 
engineer  put  engineering  system  
school  university  college  student  education  students 
schools  art  year  program  
character  game  series  story  novel  comic  book  fiction 
stories  characters  
car  engine  signal  design  cycle  cars  motor  engines 
drive  amp  
electron  energy  particle  atom  form  part  matter  field 
mass  universe  
cell  gene  protein  cells  dna  form  proteins  species 
structure  organism  
greek  son  god  name  myth  poet  poem  apollo  roman 
athens  
figure    subset of    topic lda model on wikipedia
dataset  each row represents a topic as represented by the   
words listed  in order of representativeness within that topic  
ii 

conclusion and future work

topic models provide an effective way to discover 
navigate  and understand the content from a large collection of
documents  with the proper choice of parameters and enough
sampling  lda with cgs inference performs surprising well
on the wikipedia dataset  several distinguishable topics can
be identified based on article text  see figure    
however  it is often desirable to tweak with the results of
lda and guide the system to reach better end results  i have
demonstrated this interactivity by supporting two user
feedback actions  removing a word from a topic and removing
a topic from a document  these functions can be implemented
as simple extensions to the cgs method  moreover  since
these actions do not require a full re sampling within the
algorithm to see its effects  operations can be implemented
efficiently and are extendable feedback from multiple users 
future work should address better evaluation of these
mechanisms  explore other forms of providing feedback to the
algorithm  and operate in an environment capable of running
lda with hundreds of topics for the entire wikipedia dataset 
i look forward to exploring these next steps as this work only
scratches the surface of topic models with humans in theloop 
acknowledgment
for guidance on this project  special thanks to professor carlos
guestrin  u  wash   yucheng lo  u  wash   and the entire
teaching staff of cs    led by professor andrew ng
 stanford  
references
   
   
   

blei  d   ng  a   jordan  m  latent dirichlet allocation  j  mach  learn 
res     january                
d  blei  introduction to probabilistic topic models  communications of
the acm       
w  buntine and a  jakulin  discrete component analysis  lecture notes
in computer science               

fi   
   

   
   

   

   

blei  d   lafferty  j  dynamic topic models  in international conference
on machine learning         acm  new york  ny  usa         
grifths  t   steyvers  m   blei  d   tenenbaum  j  integrating topics
and syntax  advances in neural information processing systems     l 
k  saul  y  weiss  and l  bottou  eds  mit press  cambridge  ma 
             
wang  c   thiesson  b   meek  c   blei  d  markov topic models  in
artificial intelligence and statistics        
fei fei  l   perona  p  a bayesian hierarchical model for learning
natural scene categories  in ieee computer vision and pattern
recognition                
asuncion  a   welling  m   smyth  p   teh  y  on smoothing and
inference for topic models  in uncertainty in artificial intelligence
       
steyvers  m   grifths  t  probabilistic topic models  latent semantic
analysis  a road to meaning  t  landauer  d  mcnamara  s  dennis 
and w  kintsch  eds  lawrence erlbaum       

     griffiths  t  and steyvers  m          finding scientific topics 
proceedings of the national academy of science                 
     allison j b  chaney and david m  blei  visualizing topic models  in
intl  aaai conference on social media and weblogs  department of
computer science  princeton university  princeton  nj  usa  march
     
     http   dbrowser jstor org browser cgi
     http   topics cs princeton edu science 
     http   vis stanford edu papers termite
     hu  y   boyd graber  j   satinoff  b  interactive topic modeling 
interactive topic modeling  machine learning journal  under review 
     
     http   medialab di unipi it wiki wikipedia extractor

fi