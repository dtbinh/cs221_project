learning to play  d video games
justin johnson

mike roberts

matt fisher

jcjohns stanford edu

mlrobert stanford edu

mdfisher stanford edu

abstract
our goal in this project is to implement a machine learning system which learns to play simple  d video games 
more specifically  we focus on the problem of building a
system that is capable of learning to play a variety of different games well  rather than trying to build a system that
can play a single game perfectly 
we begin by encoding individual video frames using features that capture the absolute and relative positions between visible objects  this feature transform      generalizes across a wide class of  d games  and     produces very
sparse feature vectors  which we exploit to drastically reduce computation times  to learn an appropriate gameplay
policy  we experiment with model based and model free reinforcement learning methods  we find that the sarsa  
algorithm for model free reinforcement learning successfully learns to play p ong  f rogger  dance  dance r evolution  as well as several other games of comparable complexity 

   introduction
ai systems are capable of playing specific video games 
such as super mario world     and starcraft      with comparable skill to expert human players  however  all such ai
systems rely on a human to somehow perform the challenging and tedious task of specifying the game rules  objectives
and entities 
for example  state of the art ai systems for playing
mario and starcraft can play these games effectively  even
when faced with challenging and complex game states 
however  these systems rely heavily on hand crafted heuristics and search algorithms that are specific to the game they
target  and are not readily generalizable to other games 
in contrast  systems for general game playing  ggp 
     such as cadiaplayer      can play novel games for
which they were not specifically designed  however  ggp
systems rely on a human to provide a complete formal spec mike

and justin are enrolled in cs      but matt is not  matt is a
senior phd student in the stanford graphics group  who will has advised
and collaborated with mike and justin on this project  he wrote the game
model learning algorithm mentioned in section   

figure    our system successfully learns to play the games
shown above  e at t he  f ruit  top left   p ong  top middle  
dance  dance  r evolution  top right   f rogger  bottomleft   s nake  bottom middle   d odge  t he  m issile  bottomright  

ification of the game rules  objectives  and entities in a logical programming language similar to prolog  arriving at
such a formal specification is very tedious even for the simplest games  this limitation significantly constrains the applicability of ggp systems 
very recently  bellemare et al      released the arcade
learning environment for evaluating the performance of ai
agents on a large set of atari      games  in this work 
bellemare et al  evaluate a variety of feature transforms
that generalize across  d games  as well as evaluating the
sarsa   algorithm for online model free reinforcement
learning in this setting  bellemare et al  demonstrate that
the sarsa   achieves reasonable performance on a large
variety of games 
in this project  we aim for comparable performance and
generality to that recently demonstrated by bellemare et
al       indeed  our technical approach is directly inspired
by their work  to our knowledge  the arcade learning
environment is the only system to implement ai agents
that learn to play such a wide variety of non trivial games 
therefore  it is worth emphasizing that the games we consider in this project are at the approximate boundary of what
general ai agents are capable of learning  this is true despite the apparent simplicity of our games  even compared
to classic  d games like super mario world 

fi   games
to evaluate our system we implemented a number of
games of complexity comparable to early arcade games 
each game contains a number of distinct object types  and
game state consists of a fixed configuration of objects  the
state space s of a game is the set of all possible object
configurations  unless otherwise noted  the action space
of each game is a    l  r  u  d     and consists of one
action corresponding to each of the four cardinal directions
and the do nothing action  
each game is played over a series of episodes  where
an episode consists of many frames  to prevent a perfect player from playing indefinitely  we cap episode length
where appropriate  in all situations  early termination of an
episode due to capping earns the player zero reward in the
final frame of the episode 
g rid  w orld   in this game  the player controls a character on a      grid  during each frame of the game  the
player may move in any direction or remain stationary  the
player begins each episode in the lower left corner of the
grid and must reach the upper right corner  when this goal
is achieved  the player receives a positive reward and the
episode ends  in addition  the player receives a negative reward for stepping on the central square  we evaluate the
players performance by counting the number of frames per
episode  fewer frames per episode indicates better performance  since it means that the player navigated to the goal
square more quickly 
e at t he  f ruit 
similar to g ridw orld  the player
controls a character on a fixed sized grid  at the start of each
episode  an apple appears on a randomly chosen square 
the player begins in the lower left corner of the grid and
must move to the apple  after eating the apple  the player
receives a reward and the episode ends  as in g ridworld 
we measure a players performance on this game by counting the number of frames per episode 
d odge  t he  m issile   in this game the player controls
a space ship which can move left or right across the bottom
of the screen  so the action set is a    l  r     missiles
and powerups spawn at the top of the screen and fall toward
the player  the player receives a positive reward for collecting powerups  being hit by a missile incurs a negative
reward and causes the episode to end  we cap the episode
length at      frames  we evaluate the players performance by counting both the number of frames per episode
and the number of powerups collected per episode  larger
numbers for each metric indicate better performance 
f rogger   in this game  the player controls a frog which
can move in any direction or remain stationary  the player
begins each episode at the bottom of the screen and must
guide the frog to the top of the screen  this goal is made

figure    our tile coded feature representation  we encode the absolute positions of game objects  top  as well as relative positions
of game objects  bottom  in spatial bins  relative positions are
computed separately for all pairs of object types  for any game
state s  s  this results in a feature vector  s  of dimension d  
o k    where k is the number of distinct object types in the game 
to be used in the sarsa learning algorithm  the feature transform
must also encode the action ai  a    a            a a     that is
to be taken from the current game state s  to this end  our final
feature vector  s  ai   is simply the vector  s  with all indices
shifted by i a  and with zeros at all other positions 

more challenging by cars that move horizontally across the
screen  the episode ends when the frog either reaches the
top of the screen or is hit by a car  the former earns a reward of r      and the latter receives a reward of r      
we evaluate the players performance by computing her average reward per episode 
p ong  
in this game  two paddles move up and down
across the left and right sides of the screen while volleying a ball back and forth  the player controls the left paddle  whereas the game controls the right paddle  the action
space is a    u  d     failing to bounce the ball yields a
negative reward and ends the episode  we cap the episode
length at    successful bounces  we evaluate the players
performance by counting the number of successful bounces
per episode 
s nake  
in this game  the player controls a snake of
fixed length that moves around a maze  with no player in 

fiput  the snake moves forward at a constant rate  pressing
a direction key changes the direction that the snake travels  the episode ends with a negative reward if the snake
head intersects either a wall or the snake body  we cap the
epsiode length at     frames  we evaluate the players performance by counting the number of frames that it survives
per episode 
dance  dance  r evolution  
in this game  arrows
appear at the bottom of the screen and scroll toward targets at the top of the screen  whenever an arrow overlaps
its corresponding target  the player must press the direction
key corresponding to the direction of the arrow  the trivial
strategy of pressing every arrow at every frame is impossible  since the player can press at most one direction per
frame  each episode lasts for      frames  and a players
performance is measured by the fraction of arrows that it
successfully hits 

   feature design
since we want our learning system to generalize across
games  we must avoid including any game specific state in
our features  for example  explicitly encoding the position
of mario  along with the positions of game entities that we
know can harm mario  into our features would run counter
to our goal of generality  however  we must encode the observable game state with sufficient fidelity to make accurate
predictions  on the other hand  we must carefully design
features of sufficiently low dimensionality that our learning
problems remain computationally tractable 
with these competing concerns in mind  we follow the
approach of bellemare et al      and encode the game state
using tile coded features  this encoding allows us to efficiently encode the absolute and relative positions of objects
within the game  see figure   for details 
although the resulting feature vector is very high dimensional  over         for several games   it is very sparse
 see figure     storing the feature vector sparsely allows
our algorithm to remain computationally efficient despite
the high dimensionality of our feature transform 

   model based reinforcement learning
to learn an appropriate gameplay policy  we began with
the following observation  although the state spaces of
the games we consider are very large  the action spaces of
the games we consider are very small  for example  there
are roughly        different possible states in s nake  but
only   actions  this observation motivated us to learn gameplay policies by performing fitted value iteration 
since fitted value iteration requires access to a game
model  we learned one from recorded examples of gameplay  more specifically  we formulated the program of

algorithm   learn to play a game using the sarsa   algorithm with linear function approximation  see section  
for definitions of the variables 
function learn game   
     w   
s  initial state  a  initial action
       initial exploration rate
repeat
take action a  observe next state s and reward r
s  choose action s   
  r   wt  s   a    wt  s  a 
  
for i  s  a      do
i    
w  w   w    update weight vector
  d     decay exploration rate
until termination

learning a game model as a supervised learning problem 
which we addressed by training a collection of decision
trees  very roughly speaking  our input features encoded
the current observable game state at time t  as well as the
input provided by the player at time t  our target variables
encoded the game state at time t      we then learned a
game model by training a collection of decision trees  see
our midterm progress report for details 
using our learned game model  as well as the feature
transform  described in section    we were equipped to
apply fitted value iteration to learn a gameplay policy  at
its core  fitted value iteration approximates the value function as v  s   t  s   the weight vector  is found
by solving a linear regression problem with design matrix
 s      s              sm   t where s    s            sm   is
a vector of states  unfortunately  we observed severe numeric instability using this approach for games as simple
as g rid  w orld  we speculate that this instability stems
from a severe rank deficiency of the feature matrix  s  
in the case of g rid  w orld  the rank deficiency of
 s  occurs because there are a total of    unique states  assuming a    game area  therefore   s  can have at most
   unique rows no matter the dimension of s  so  s  has
rank      however  using the feature transform described
in section     s  will much greater than    columns  the
linear regression problem is therefore underconstrained and
has an infinite number of solutions  we considered taking the minimum norm solution  but it is not clear that this
approach would sensibly approximate our unknown value
function 
to make matters worse  this numeric instability becomes
more pronounced for more complex games  the dimension of the feature transform  s   and hence the number
of columns of  s   grows quadratically with a large con 

fid odge  t he  m issile cumulative computation time  seconds 

sparse traces  sparse features
sparse traces  dense features
dense traces  sparse features
dense traces  dense features
 

 

 

 

 

 

 

 

 

d odge  t he  m issile average number of features  thousands 

sparse
dense
 

  

  

  

  

   

   

figure    top  comparison of total computation time for learning d odge  t he  m issile using different combinations of sparse
and dense feature and trace vectors  bottom  the average number
of nonzero features for d odge  t he  m issile  we observe that
the average number of nonzero features is very small  which we
exploit to drastically reduce computation times 
g rid  w orld frames to find goal per episode    episode moving average 

      
      
      
      
random movement

   
   
   
   
   
 

 

  

  
  
episode

  

   

figure    the effect of different values for   on the convergence
of our algorithm when playing g rid  w orld  we show convergence rates for various values of  relative to random movement 
lower is better  we observe that our algorithm outperforms random play across a wide range of values for  

stant as the number of unique object types increases  even
if this is smaller than the number of sensible game states 
a numerically stable regression step would require a large
number of training samples  which could quickly become
computationally infeasible 
the numerical stability problems associated with fitted
value iteration prompted us to markedly reconsider our
technical approach  this lead us to the model free reinforcement learning algorithm we describe in the following
section 

   model free reinforcement learning
our system uses the sarsa   algorithm with linear
function approximation  see algorithm    to learn a gamelay policy  sarsa     is an online model free algorithm
for reinforcement learning  the algorithm iteratively com 

   

putes a state action value function q   s  a  r
based on the rewards received in the two most recently observed states  sarsa       is a variant that updates the
state action value function based on the rewards received
over a large window of recently observed states  in our
case  the full state space s of an unknown game may be
very large  so we approximate the state value function as
q s  a    wt  s  a   where    s  a  rn is the feature
transform described in section   and w  rn is a weight
vector 
at each game state s  the algorithm chooses an action
a using an  greedy policy  with probability  the action is
chosen randomly  and with probability     the action is
chosen to satisfy a   arg maxaa q s  a   the parameter
         controls the relative importance of exploration
and exploitation  and as such is known as the exploration
rate  in our implementation we decay  exponentially over
time  this encourages exploration near the beginning of
the learning process  and exploitation near the end of the
learning process 
the algorithm keeps track of recently seen states using
a trace vector   rn   more specifically   records the
recency with which each feature has been observed to be
nonzero  the sparsity of typical feature vectors causes  to
be sparse as well  this sparsity can be exploited for computational efficiency  see figure    
we update the trace vector using a parameter          
which controls the extent to which recently seen features
contribute to state value function updates  varying the
value of  can affect the rate at which the learning algorithm converges  see figure     admittedly  we found that
different values of  were required for each game in order to achieve the best possible performance  for example 
we used        for dance  dance  r evolution and
       for d odge  t he  m issile 
the algorithm also depends on a learning rate          
which has similar meaning to the learning rate in gradient
descent  we found that  required tuning for each game 
for example  f rogger performed best with         
whereas s nake performed best with            

   results
our system successfully learns to play g rid  w orld 
e at t he  f ruit  d odge  t he  m issile  f rogger 
p ong  and dance  dance  r evolution  we evaluate
the performance of our system on these games by comparing with game agents that choose random actions at
every frame to show that substantial learning takes place
 see figures   and     our system learns to play s nake
successfully only when we simplify the game by reducing
the length of the snake body to    see figure    

fie at t he  f ruit frames to find goal per episode     episode moving average 

   

dance  dance  r evolution fraction of hits per episode

   

   
   

   

   

   

   
  
 
  

   

d odge  t he  m issile frames survived per episode    episode moving average 

    

our method

 

   

    

   

    

   

    
 

   

random movement

   
   
episode

    

   

    

   

our method

 

d odge  t he  m issile powerups obtained per episode    episode moving average 

  
  
episode

our method

  

   

    

  

   

  

   

  

   

  

   

 

   

 

our method

 

   

   
   
episode

   

   

   

   

random movement

   
   
episode

   

   

p ong bounces per episode    episode moving average 

  
  
  
  
  

   

random movement

 

f rogger reward per episode     episode moving average 

   

  

 

  

random guesses

our method

 

always move up

   

    
    
episode

random movement

    

    

 
our method

  

 

  

   

random movement

   
episode

   

   

   

figure    performance of our algorithm relative to random play for e at t he  f ruit  top left  lower is better   dance  dance r evolution  top middle   d odge  t he  m issile  top right and bottom left   f rogger  bottom middle   and p ong  bottom right  
for d odge  t he  m issile  we capped the episode length at      frames  for p ong  we capped the episode length at    bounces  for
f rogger   we set r        and r       note that after our algorithm has learned to play d odge  t he  m issile effectively  it is
capable of collecting powerups while simultaneously avoiding missiles  note that since continuously trying to move upwards can be a
viable strategy when playing f rogger  we also compare the performance of our algorithm to an ai player that continuously tries to move
upwards 
s nake frames survived per episode  easy level  short tail 

   

s nake frames survived per episode  easy level  long tail 

   

our method
random movement

s nake frames survived per episode  hard level  short tail 

   

our method
random movement

s nake frames survived per episode  hard level  long tail 

   

our method
random movement

   

   

   

   

   

   

   

   

  

  

  

  

 

 

 

 

    

    
    
episode

    

     

 

    

    
    
episode

    

     

 

    

    
    
episode

    

     

 

our method
random movement

 

    

    
    
episode

    

     

figure    performance of our algorithm on s nake relative to random play  we evaluate our algorithms performance on the following
four different game variations  empty game board with a snake body length of    left   empty game board with a snake body length of   
 left middle   relatively cluttered game board with snake body length of    right middle   and relatively cluttered game board with a snake
body body length of     right   our algorithm was able to learn effectively on the cluttered game board  but not with the longer body  this
is because having a longer body requires longer term decision making  on the other hand  a short body makes it possible to play according
to a relatively greedy strategy  even on a relatively cluttered game board 

references
    m  g  bellemare  y  naddaf  j  veness  and m  bowling  the arcade learning environment  an evaluation platform for general agents 
arxiv e prints  july      
    y  bjornsson and h  finnsson  cadiaplayer  a simulation based
general game player  ieee transactions on computational intelligence and ai in games             
    m  genesereth  n  love  and b  pell  general game playing 
overview of the aaai competition  ai magazine  spring       
    s  karakovskiy and j  togelius  the mario ai benchmark and competitions  ieee transactions on computational intelligence and ai in
games             

    m  mohri  a  rostamizadeh  and a  talwalkar  foundations of machine learning  adaptive computation and machine learning series 
mit press       
    m  wiering and j  schmidhuber  fast online q     machine learning 
                   
    j  young  f  smith  c  atkinson  k  poyner  and t  chothia  scail 
an integrated starcraft ai system  in ieee conference on computational intelligence and games       

fi