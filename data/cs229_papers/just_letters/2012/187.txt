prediction of high cost hospital patients
jonathan m  mortensen  linda szabo  luke yancy jr 
introduction
in the u s   healthcare costs are rising faster than the inflation rate  and more rapidly than other
first world countries  there is a national effort not only to improve medical care  but also to
reduce medical care costs through use of evidence based medicine  electronic health records
 ehrs  present an opportunity from which to find such evidence  in support of these efforts 
many hospitals have recently created large de identified data sets for use in developing methods
for evidence based practice  the stanford translational research integrated database
environment  stride  is one such ehr database  containing    million discharge codes and
de identified clinical notes on over     million patients who received care at the stanford
university medical center beginning in       recent work by moturu and colleagues  uses such
structured patient data to predict high cost patients  the authors suggest these predicted patients
are candidates for additional preventative interventions  thereby reducing cost 
the goal of this project is to predict the future one year cost of a patient using features extracted
from   months of textual clinical notes and discharge codes in the ehr  we develop a system
that leverages the stride data to predict high cost patients  this system will enable hospitals
and other interested parties to automatically identify patients likely to return for costly
procedures in the future and invest in preventative care measures to both reduce costs and
improve health 
methods
our system uses de identified patient information  both textual and structured  as features to
predict cost  the components of the method are     feature engineering to capture the most
relevant features for the task      cost assessment for the prediction interval  and    
classification that identifies high cost patients at a given cost threshold  figure   and figure  
provide an overview of this system 

  

fifeature engineering  patient visits  months     
the feature matrix contains frequencies of concepts and structured codes obtained from the
first   months of clinical notes for each patient with at least    months of data  see figure     we
process each clinical note via the bioportal annotator to create a list of unique words found in
the corpus of free text clinical notes  we include words identified in the context of patient family
history since we believe these provide additional predictive power  we remove negated words
and words with   or less characters  which are often functional words  next  we map terms to
more general concepts from medical ontologies using the unified medical language system
 umls   a comprehensive metathesaurus of medical terminology  for example  all terms
conceptually related to diabetes are normalized to a single concept diabetes  in doing so  we
aggregate potentially low signal terms into a more representative single feature  finally  we
remove noisy  low frequency concepts  occurring fewer than    times in the database   for
structured codes  we extract cpt  current procedural terminology  and icd    international
classification of diseases  codes for each patient  the resulting feature matrix consists of       
features for        patients 
cost assessment  patient visits  months       
to estimate the cost of a patient  we collect all cpt codes associated with a patients prediction
interval        months from initial encounter   we skip months      since this is the timeframe
for possible medical intervention  in other words  a doctor may not be able to avoid a high cost
procedure tomorrow but could initiate preventative care over a   month period to reduce future
costs  we filter duplicate cpt codes entered on the same day for a patient or for the same visit 
we then use these codes as a proxy for patient cost by mapping each patients cpt codes to cost
 in dollars  and summing up the total cost associated with these codes  we utilize cost mappings
from the      medicare physician fee schedules from the state of illinois  since california data
is unavailable  although we cannot make conclusions about exact patient costs  we are confident
that this cost data allows us to make relative comparisons to identify the most costly patients 
as shown in figure    one year costs range from    to         per person  however      of the
patients have costs of less than         understanding this skew in the response variable is
important when selecting and applying machine learning methods for classification 

figure  a  patient cost per year

figure  b  patient log cost  per year

       

      

figure   
 a  patient cost
during prediction
interval 
 b  log of patient
cost during
prediction interval 

      
      

      

  of patients

  of patients

      

      

     

     

     

      
     

 

  

 

 

     

     

     

     

patient cost  dollars 

     

     

     

 

 

 

 

 

patient log cost   dollars 

  

  

ficlassification 
using the textual and coded clinical information as predictors  and the patient cost data as
response  we classify high cost patients  in each classifier  the response is the patients total
cost  or log cost  during their prediction interval  encounters between    months and   
months  see figure     to evaluate these methods  we train a model on a subset of the data and
measure performance by the models ability to maximize the percentage of total cost captured
while minimizing the number of patients associated with high costs in the test set  i e  identify a
small number of patients who are responsible for a large percentage of the cost  
results
to explore the space  we apply a battery of classifiers to predict cost    nave bayes  svm 
logistic regression  regularized linear regression  and knn  considering the large sample size 
we follow the standard practice of     training      tuning  to select optimal parameter
values   and     testing  since we have        features  we implement information gain or
regularization to reduce the feature space to those most informative for a given classifier  table  
reports the top features  in some situations  performance  measured by accuracy of classifier as
well as observation of intuitively meaningful features selected  improves when we use under
sampling to obtain balanced classes  a performance summary of each classifier is shown in
table   
information gain
transplantation
platelet count
measurement
x ray computed
tomography
liver
creatinine
scanning
radionuclide
imaging
albumins
count
phase

regularized linear
regression
difficulty kneeling

performance

  of
features

specificity

patients
classified
high cost

total cost
identified

stimulant abuse
nave bayes

poor

svm

poor

linear reg 

poor

logistic
reg 

moderate

   

      

     

      

concussion  severe
nail problem

linear reg 
 log cost 

moderate

      

      

     

      

non pyogenic meningitis

knn

best

   

      

     

      

large nose
blood iron measurement
anorectal abscess
multiple pulmonary
embolisms
cast brace

table    top    features determined from the two
different feature selection methods used 
  

table    machine learning methods applied to the data set 
statistics not reported for methods that did not perform well 
  

nave bayes we first apply a nave bayes classifier to our dataset since it often performs well
for text based classification  we expect low accuracy for patients near the high cost threshold
as we are binning a continuous variable  however  we find that it also misclassifies patients with
extreme cost values  we observe low specificity over a range of high cost thresholds  with a
dramatic spike at a threshold of          this is likely due to the high density of patients with
cost less than         and a paucity of patients at any given cost above that  note that we use
information gain to filter features before classification  we find that varying the number of
features included from           has little affect on accuracy 

  

fisvm the major issue with svm is the computational complexity of selecting optimal
parameters for our large dataset  ideally  we would perform grid search to select optimal values
for parameters gamma and c  however  this process is too computationally intensive due to the
large number of samples and features in our data  using r package e       tuning did not
complete after    hours   we use information gain to reduce the number of features but the
number of patients remains an issue  we try tuning on a subset of training data by spot checking
a few parameter values  but this understandably does not yield a good model 
logistic regression we apply regularized logistic regression at various cost thresholds to our
dataset  logistic regression develops class probabilities based on the training distribution  so
classifying this dataset is especially difficult given the importance of identifying infrequent but
high cost patients  to correct for this difficulty  we under sample the training set to balance the
number of low cost patients and high cost patients  even with this additional step  regularized
regression still performs poorly based on our performance metrics 
linear regression the skewed data is not appropriate for standard linear regression  instead  we
apply regularized linear regression to the log cost  which is approximately normal  in this model 
we identify as patient as high cost if their cost is greater than     standard deviations from the
mean log cost of the training data  the trained model selected only        features  the top   
features can be found in table     with this model  we achieve        specificity  ci        
       and        sensitivity  ci                 the model classifies       of patients as high
cost patients  these patients costs account for about        of the total cost for a year 
k nearest neighbors we apply knn to develop a classifier using         as the high cost
threshold given the distribution of the cost data  figure    and the peak in specificity observed at
this threshold for nave bayes  we expect this model to perform well due to the large number of
patients in our training set  we use information gain to filter features  and then evaluate
classification performance on the test data with values of k ranging from   to      we also vary
the number of features considered  using the top    to       optimal performance  smallest
number of patients accounting for highest percentage of cost  is achieved with     features and
k     reporting     specificity      ci                 using this classifier        of patients
are classified as high cost  accounting for       of the total cost for patients for   year 
discussion
the final two models  linear regression and knn  suggest that by using our method  a hospital
could identify and intervene on approximately   in every    patients to potentially reduce     
of its yearly cost  this serves as a baseline for addressing the cost prediction issue  however 
there are many opportunities for improvement 
complex data a major element of this project was the process of understanding the data  as we
had a large amount of data  we carefully considered what data to use  for example  when
examining cpt codes marked as billing codes  we manually reviewed a sample of the data and
noted that these codes marked for billing were repeats of previous codes or that these codes only
appeared after       to reduce noise  we removed them  this dataset also contained many
concepts that appeared only within a few patients clinical notes  these concepts would often
appear to be very significant features according to our model  for example  the concept gannet

  

fiappeared as a significant feature with one of our models  gannet  a seabird  is likely is not related
to the patients medical status  to remove such noise  we filtered low frequency concepts from
the feature set  the size and complexity of the data also affected classifier selection  many
classifiers did not work out of the box for such data  each requiring tuning  optimization  and
large computational resources without which initial results are dismal 
ontologies
more than half of the top features were clinical note concepts  not discharge
codes  this suggests that our results depended heavily on the ability to combine terms describing
the same thing  without using relationships defined in medical ontologies  it is unlikely we
would have found any signal  in future work  the ontologies could enable further hierarchical
aggregation of both concepts and codes  for example  diabetes i and diabetes ii would both
contribute to the frequency of a concept called diabetes  the parent of both concepts 
applicability in this work  we addressed the task of identifying high cost patients at a
reasonable threshold  we did not address the question of whether this cost could be reduced  to
fully explore this problem  one would need to collaborate with a hospital and clinicians to
determine the optimal cost threshold and the desired percentage of patients on which to
intervene  balancing the reduction of patient costs with added intervention costs and balancing
the likelihood of successful intervention with its cost to intervene is an important situation to
consider in a true application  for example  to fully address this issue  one might collaborate with
a physician to identify patients that are intervene able and have the potential to largely reduce
their future predicted cost 
conclusion
considering the rapidly growing healthcare costs in the u s   reducing such costs is paramount 
in this work  we developed a method that identifies high cost patients  this method uses textual
and coded data from   months of patient encounters to predict the future one year cost of a
patient  using this method  we can identify       of patients that constitute       of total
patient cost  in a clinical setting  a computer could automatically flag patients for physicians and
care providers as opportunities for them to suggest additional preventive measures 
acknowledgements
we would like to thank dr  nigam shah for providing access to de identified patient data from
stride and for assistance in defining the project goals 
references 
        medicare physician fee schedules  revised zero percent      medicare physician fee schedules        
retrieved from wisconsin physicians service insurance corporation website 
http   www wpsmedicare com part b fees physician fee schedule      fee schedule shtml
   meyer d  et al         misc functions of the department of statistics  e       r project 
   moturu  s  t   johnson  w  g     liu  h          predictive risk modelling for forecasting high cost patients  a
real world application using medicaid data  international journal of biomedical engineering and technology 
            

  

fi