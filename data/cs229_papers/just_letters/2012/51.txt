structure and trends in a selection of academic literature
cs      autumn     
hannah hironaka
hannahh stanford edu

abstract
we investigate the use of unsupervised machine learning
methods for discovery of latent structure in unstructured 
unlabeled text data  we briefly present background material on two such methods  k means clustering and latent
dirichlet allocation for topic modeling  we then describe
a dataset of     text documents that we assembled and results from evaluating the described methods on this dataset 
lastly  we conclude with some general observations 

  

introduction

supervised learning is a primary focus in much of machine
learning  in the context of text document input  supervised
methods have been brought to bear on a wide variety of
tasks including sentiment analysis  spam classification  and
authorship attribution 
however  it is not always the case that categorization of
input documents into predetermined buckets is truly the
task of interest  indeed  the unsupervised setting is often a
more appropriate representation for problems whose input
consists of unstructured text documents  here  the goal is
instead to cluster together similar documents in the hope
that the grouping of documents reveals some latent structure
underlying the data 
moreover  unstructured  unlabeled text data are ubiquitous 
books  magazines  and newspapers are traditional sources 
the explosion of data in recent years has made many more
sources readily available  webpages  blog posts  emails  tweets 
transcripts of audio recordings  etc  while each of these
sources could be shoehorned into a supervised learning problem in one way or another  a direct reckoning of the data
with unsupervised methods can be equally satisfying  and
the resultant insights can be used to summarize the data or
to inform further exploration of it  to the extent that the
volume of data now available to us is overwhelming  such
automated methods for summarizing large text corpora are
invaluable 
k means is a fairly straightforward algorithm for clustering
points in a vector space that can be applied to text documents  latent dirichlet allocation  lda      is a bayesian
hierarchical model for discovering topic models from collections of text documents  with numerous extensions of the
standard lda framework appearing over the past decade 
in the next sections  we briefly discuss k means and lda 

rahul suri
rahul suri stanford edu

   algorithms
    k means clustering
k means clustering groups a set of input vectors into k clusters based on similarity  it starts by initializing k cluster
mean vectors with random values  at each iteration  some
distance measure is used to group each input vector with the
cluster whose mean it is closest to  cluster mean vectors are
then recalculated based on their newly assigned vectors  and
the algorithm loops until convergence 
in our analysis we made several decisions about the construction of the algorithm and its inputs  each input vector represented one document  the vectors are indexed by
words in the vocabulary  with the indexs corresponding
value representing the frequency of that word in the document  we initialized the cluster means with random input
vectors  we chose cosine distance as a similarity measure because the input document vectors were not normalized  this
addresses the scenario where two input vectors have similar directions  i e  similar relative frequencies of words  but
not magnitude  i e  different document lengths   finally 
we adapted the algorithm to handle the case of zero vectors
grouped with a mean after an iteration  in this event  the
algorithm reuses the mean from the previous iteration 

   

latent dirichlet allocation

intuition  lda assumes that the data exhibit underlying topics  each topic is a multinomial  distribution over
the vocabulary  the term topic reflects the fact that the
estimated multinomials tend to place probability mass on
words with thematic coherence  atom  mass  and particle
for a topic such as physics  e g   
given these topics  the words in a single document are assumed to be drawn from a mixture over the topics  this
gives a generative view of the set of all documents  with
each document having its own mixing proportions 
notation  let i for i              k be a set of k topics 
let d for d              d be a set of d documents  let wd n
denote the nth word in the dth document  and let zd n   k be
an indicator that wd n was generated by the kth topic  for
convenience  assume that each document contains n words 
 
technically  all multinomial distributions referred to in
this document are better described as categorical distributions  however  we follow the relevant literatures standard
convention of calling them multinomials 

filastly  let  and  be dirichlet distributed priors for d and
k   respectively 
joint pdf  the joint density of the variables under the
model is given by

the results of these preprocessing steps produced documentterm matrices  which were then used as input to our analyses 

  

results

p   k     d   z  d   n   w  d   n       
     k means
q
qd
qk
n
n   p zd n  d  p wd n    k   zd n    after informal experimentation  we chose k     as the pad   p d   
i   p i   
rameter for the k means algorithm  the clusters it produced
seemed most coherent  and no clusters were empty  we ran
posterior  the posterior of the parameters given the data
k means using each year as a separate dataset  in addition
is
to running it once with the entire corpus as its input 
p   k     d   z  d   n  w  d   n        
p   k    d  z  d   n  w  d   n    
table   displays results from the single dataset analysis  de 
p w  d   n    
spite the small sample of titles from each cluster and the
minimal detail about each ones full content  several patwhere the numerator is the joint pdf and the denominator
terns are evident  for example  cluster   groups papers
is the probability of the evidence 
related to game play  and cluster   groups object manipulation papers  some clusters  such as the third one  remain
the two main approaches for numerically approximating the
incoherent  though 
posterior are gibbs sampling         and variational methods
         
table   shows a few results from clustering within an individual year  several characteristics of the evolution of project
dynamic topic modeling and extensions to lda 
topics are evident in the output  object manipulation and
the lda hierarchical model as presented here has often
robotics were common topics throughout all years  but were
been used as a building block for constructing more sophismore prevalent in earlier years  as was image processing and
ticated approaches by relaxing its modeling assumptions    
detection  a cohesive computational biology cluster appears
            in particular  dynamic topic models     take
in       news recommendation and twitter sentiment analinputs which include discrete valued temporal data  intuysis first appear in      and      
itively  a topic model is learned for each timestep  and the
topics themselves evolve from one timestep to the next as a
both variations of k means clustering suffer from several
random walk  specifically  if we let t i denote the ith topic
limitations  the small size of the corpus attenuated the
at time t  then t i  t  i  n  t  i      i   dynamic topic
closeness of the clusters around their means  this was
models thus allow us to track the evolution of topics over
most evident for             and      which had        
time 
and    project papers  respectively  the smallest numbers  clusters from these years were often incoherent or
   data and methodology
contained many outliers  furthermore  running k means
    data
multiple times produced noticeably different clusters  indithe text corpus for this project was downloaded from the cs
cating that this dataset was particularly likely to cause the
    website containing students final project reports      it
algorithm to converge to local optima 
contains     documents from      to       after removing
stopwords  numbers  and punctuation  the corpus contains
occurrences of       unique tokens  the median number of
    lda
 non unique  word occurrences per document is       with
the results shown below are for a model with    topics  as
an interquartile range of     
with choosing the exact size of the vocabulary  the number
of topics was chosen based on informal experimentation
 and
p
    methodology
on a rule of thumb suggesting roughly n   clusters for a
dataset with n points  
to perform topic modeling analyses on the corpus described
above  we used open source topic modeling software     to
table   shows results produced by lda  each topic is illusstem the tokens and to shrink the vocabulary size from
trated by a list of the ten most probable words from that
      to       by retaining only the highest scoring tokens 
topic  we observe that topic   has mostly grouped terms
as measured by their term frequency inverse document frerelating to natural language processing together  topic   
quency  tf idf  scores       the size of the shrunken vocabuhas grouped together terms about news recommendation
lary was selected via informal experimentation  the selected
projects  and topic    has grouped terms concerning projects
settings are those which appeared to reveal the most interabout image processing  similar conclusions can be stated
esting patterns and which made execution of dynamic topic
for the remaining topics  audio processing  image processmodeling manageable 
ing  game play  robotics  bioinformatics  and stock market
prediction emerge as some of the major underlying themes
for k means clustering  we used the open source natural
of cs     class projects  so  lda has provided a way for
language toolkit  nltk      for preprocessing  we standardized to lowercase  removed punctuation  non alphanumeric automatically discovering latent thematic structure  grouping words from relevant subjects using only the unstructured
characters  and stopwords  and lemmatized the remaining
source documents 
words  the resulting vocabulary size was       

fitable    representative documents for a few clusters  for a   cluster model 

   

dynamic topic modeling

figures   through   depict results from running dynamic
topic modeling on our dataset  each figure corresponds to
a single topic from a    topic dynamic model  lines are
shown for the five tokens whose probability of appearing in
that topic displayed the highest variance across timesteps 
the lines plot each tokens probability of occurring in that
topic against time 
for topic    figure     the    tokens which appear with highest probability  marginalizing across timesteps  are user 
cluster  movi  articl  recommend  feed  rmse  read  stori  and
kmean  this information can be used to inform directed exploration of the original dataset to conclude that this topic
has grouped together terms relating to recommender system projects  from the figure  we see signs of the diminishing popularity of netflix inspired movie recommendation
projects  which were scored using rmse and often included
some form of clustering  typically kmeans   moreover  we see
an increase in tokens related to news article recommendation

    

    

    

cluster
movi
user
feed
stori

    

cluster  
beat cal  machine learning in football play calling
mlb prediction and analysis
a machine learning approach to opponent modeling in
general game playing
cluster  
applying synthetic images to learning grasping
orientation from single monocular images
image processing for bubble detection in microfluidics
value iteration and ddp for an inverted pendulum
cluster  
robochef  automatic recipe generation byte sized
recipes
person following on stair
travel time estimation using floating car data
cluster  
stair subcomponent  learning to manipulate objects
from simulated images
door handle detection for the stanford ai robot
 stair 
learning to pick up a novel object
cluster  
supervised learning   stock trend classifier
finding optimal hardware configurations for c code
statistical analysis and application of ensemble
method on the netflix challenge
cluster  
clustering wordnet senses utilizing modified and novel
similarity metrics
classification of amazon reviews
predicting dow jones movement with twitter
cluster  
crf based point cloud segmentation
clustering autism cases on social functioning
whos in charge here    using clustering algorithms
to infer association of putative regulatory
elements and genes

    

    

    

    

    

    

    

figure    word probabilities in topic   across time 

projects starting in       in which students used data from
pulse to suggest news stories to users in their news feeds 
for topic    figure     the    tokens which appear with
highest probability are stock  price  day  market  trade  tweet 
portfolio  forecast  twitter  and network  the thematic content
of this topic is rather obvious  encouragingly  the figure
clearly depicts the sharp increase in mentions of tweets and
twitter beginning in       as many projects have used twitter data as a springboard for stock prediction  moreover  the
figure also reflects an increase in terms like stock and price 
if we were looking at these graphs to try to understand latent structure in the dataset without any prior knowledge 
figure   would have alerted us to one of the major trending
themes in an entirely automated manner 
lastly  we consider topic    figure     whose top    tokens
are layer  imag  network  video  deep  frame  fig  roi  templat 
and reconstruct  it appears that this topic groups together
terms related to projects on neural networks  the figure
seems to indicate an upward trend in use of neural networks 
which is consonant with the recent increase in popularity of
deep learning with neural networks for image analysis 

  

discussion

one of the unexpected challenges that arose with these analyses was that of simple data manipulation  scraping text
from pdf documents often introduced strange encoding errors that negatively impacted results and took time to hunt
down 
additionally  identical tokens sometimes appeared in distinct contexts  light appeared both in the context of traffic
light control policies and also in image processing  k means
clustering was particularly susceptible to conflating the two
meanings  moreover  the stemming we used to preprocess
words at times caused otherwise distinct tokens to be con 

fi    

network
imag
layer
video
deep

    

    

    

    

    

    

    

    

    

tweet
day
twitter
price
stock

    

    

    

    

    

    

    

    

    

    

    

    

    

    

figure    word probabilities in topic   across time 

figure    word probabilities in topic   across time 

flated  movie and moving were both stemmed to movi even
though the former concerned film recommendations while
the latter concerned video analysis 

dirichlet allocation  j  mach  learn  res             
mar       
cs     final project reports 
http   cs    stanford edu            
t  l  griffiths and m  steyvers  finding scientific
topics  proceedings of the national academy of
sciences      suppl               april      
t  l  griffiths  m  steyvers  d  m  blei  and j  b 
tenenbaum  integrating topics and syntax  in in
advances in neural information processing systems
    pages         mit press       
b  grun and k  hornik  topicmodels  an r package
for fitting topic models  journal of statistical
software                     
m  i  jordan  z  ghahramani  t  s  jaakkola  and
l  k  saul  an introduction to variational methods for
graphical models  machine learning            
              a               
c  manning and h  schutze  foundations of statistical
natural language processing  mit press  cambridge 
ma       
d  mimno  h  wallach  and a  mccallum  gibbs
sampling for logistic normal topic models with
graph based priors  in nips workshop on analyzing
graphs             
y  w  teh  m  i  jordan  m  j  beal  and d  m  blei 
hierarchical dirichlet processes  journal of the
american statistical association                    
     
m  j  wainwright and m  i  jordan  graphical models 
exponential families  and variational inference  now
publishers inc   hanover  ma  usa       
h  m  wallach  topic modeling  beyond bag of words 
in proceedings of the   rd international conference on
machine learning  icml     pages         new
york  ny  usa        acm 

lastly  we note that most algorithmic parameters  k for kmeans  tf idf cutoffs  were chosen via informal experimentation  while more rigorous approaches to selecting these
values  cross validation  e g   could have been used  we decided to focus our efforts elsewhere  as we suspected that the
marginal gains to be achieved from optimizing these values
would be relatively modest  particularly when the ultimate
evaluation of the results was largely qualitative 

  

   

   

   

conclusions

we have presented k means clustering  latent dirichlet allocation  and dynamic topic modeling  which are three approaches to unsupervised learning from unlabeled data  we
applied these methods to a novel collection of text documents and showed how the automatic discovery of latent
structure from free text can be used to highlight interesting
patterns and trends in the data 

  

   

references

    s  bird  nltk  the natural language toolkit  in
proceedings of the coling acl on interactive
presentation sessions  coling acl     pages      
stroudsburg  pa  usa        association for
computational linguistics 
    d  m  blei  introduction to probabilistic topic models 
communications of the acm       
    d  m  blei and j  d  lafferty  dynamic topic models 
in proceedings of the   rd international conference on
machine learning  icml     pages         new
york  ny  usa        acm 
    d  m  blei and j  d  lafferty  topic models  crc
press       
    d  m  blei  a  y  ng  and m  i  jordan  latent

    

    

    

    

    

    

fi      cluster  
stair subcomponent  learning to manipulate objects
from simulated images
door handle detection for the stanford ai robot
 stair 
re learning to walk  adding force feedback control to
the quadruped robot
      cluster  
learning depth in light field images
explicit image filter
learning traffic light control policies
      cluster  
splice site prediction using multiple sequence alignment
cs    project  musical alignment discovery
prostate detection using principal component analysis
      cluster  
automatic graph classification
automatic fatigue detection
automatic beat alignment
      cluster  
discovering visual hierarchy through unsupervised
learning
spoken language identification with hierarchical
temporal memories
      cluster  
learning to splash
stock forecasting using hidden markov processes
recognizing informed option trading
      cluster  
complex sentiment analysis using recursive autoencoders
predicting rating with sentiment analysis
sentiment based model for reputation systems in amazon
      cluster  
pulse news preference prediction
predicting preferences  analyzing reading behavior and
news preferences
reddit recommendation system
pulse project  user interest based news prediction
      cluster  
attentional based multiple object tracking
learning unsupervised features from objects
unsupervised learning of temporally coherent features
for action recognition
table    selected documents and clusters for clustering by years 

topic  
music
instrument
signal
fig
mixtur
genr
facial
autoencod
ica
spectral
topic  
imag
brain
patient
roi
voxel
student
fmri
pixel
neuron
tumor
topic   
cluster
gene
cell
kmean
signal
transcript
protein
rna
damag
genes

topic  
word
document
queri
cluster
review
sentenc
corpus
topic
charact
semant
topic  
user
movi
cluster
student
rmse
friend
social
emot
kmean
recommend
topic   
tag
price
portfolio
trade
stock
market
econom
compani
asset
risk

topic  
game
player
team
win
agent
oppon
games
action
outcom
bet
topic  
network
beat
signal
neuron
onset
snps
fig
music
patent
diseas
topic   
network
layer
node
car
sensor
lane
robot
signal
cascad
polici

topic  
song
audio
music
pitch
speech
mfcc
energi
network
accent
ppca
topic  
robot
gestur
action
sensor
aircraft
finger
reward
mous
flight
movement
topic   
robot
pca
cluster
topic
entiti
terrain
motion
lda
channel
gpr

topic  
day
stock
price
tweet
market
trade
twitter
node
word
network
topic   
user
cluster
articl
feed
stori
read
word
recommend
day
news
topic   
imag
pixel
edg
video
depth
segment
frame
cluster
camera
patch

table    most probable words for each topic  for a
   topic topic model 

fi