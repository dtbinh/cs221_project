event extraction using distant supervision
kevin reschke  kreschke stanford edu
december         

 

introduction

table    sample plane crash infobox 
slot type
slot value
flight number
flight    
operator
armavia
aircraft type
airbus a       
crash site
alder sochi airport  black sea
passengers
   
crew
 
fatalities
   
injuries
 
survivors
 

the purpose of this paper is to explore a distant supervision approach to event extractionthat is  the extraction of template based facts about events from unstructured text  in a distantly supervised system  training
texts are labeled automatically  and noisily  by leveraging an existing database of known facts  this approach
has been applied successfully to the extraction of binary
relations such as a persons employer or a films director
 e g   surdeanu et al          but it has not previously
been applied to event extraction 
concretely  i develop a system which extracts airplane
crash events from a corpus of news documents using
wikipedia infoboxes as a source for distant supervision  
the news corpus is a collection of newswire texts spanning      to the present   i selected    plane crash
infoboxes in that time frame from wikipedia     for
training    for development     for testing  an example
is shown in table    at training time  facts from the   
training infoboxes are used to automatically label training sentences from the news corpus  at test time  the
system takes the flight number of a test infobox as input and produces values for the seven template slots as
output 
the paper is structured as follows  first i detail the
event extraction process and my distant supervision approach  then i describe a series of experiments testing
various models within this framework 

 

has the string flight x   then the document is considered relevant to the flight x plane crash event     mention classification  classify candidate mentions based
on contextual features  surrounding unigrams  syntactic dependencies  etc   the label space is the set of slot
types in the event template  plus nil for mentions which
dont fit any slot     label aggregation  merge labels
from different mentions of the same value to produce
final slot value predictions  for the bulk of this paper
i assume exhaustive aggregationthat is  all non nil
labels are included in the final prediction  but see section     for an improved aggregation scheme 
as an example of the test time procedure  suppose we
are extracting facts about the crash of flight     and we
identify the candidate mississippi in the sentence flight
   crashed in mississippi  a properly trained mention
classifier will give this mention the label hcrashsitei
based on the words crashed and in which precede it 
now suppose that over all of the mentions of mississippi  three mentions were classified as hcrashsitei  two
mentions were classified as nil  and one mention was
 incorrectly  classified as hoperatori  label aggregation
will gives us the final predictions that mississippi is both
the crash site and the operator of flight    

event extraction

at test time  event extraction has three steps     candidate generation  run named entity recognition software  on relevant documents from the corpus to identify
candidate mentions  in this setting  i use the flight number as a proxy for document relevance  if a document


i thank mihai surdeanu  martin jankowiak  david mcclosky 
and christopher manning for guidance on this project 
 
http   en wikipedia org wiki help infobox
 
i use gigaword    tipster    tipster    and tipster    see
www ldc upenn edu  
 
stanford corenlp ner  nlp stanford edu software crfner shtml

 

distant supervision

the mention classification step mentioned above requires a trained classifierthis is where distant supervision comes in  in a fully supervised approach  we would
 

fi 

have humans label a set of mentions and train a classifier
on those gold labels  but in this case supervision comes
indirectly from a set of training infoboxes 
how are infoboxes mapped to text labels  consider
first the relation extraction setting for which distant supervision was first introduced  mintz et al         in
distant supervision for relation extraction  the training
set is a database of binary relations such as hsteve jobs 
applei for the founderof relation  training sentences
are labeled by the following rule  if both entities appear in a single sentence  that sentence is a positive
instance of the relation  otherwise it is nil  this rule
ensures that we apply the label founderof to the sentence steve jobs co founded apple in       but not to
random  unrelated mentions of apple 
unfortunately  for event extraction  this sentence level
rule doesnt work  we might think of template slots as
binary relations between the slot value and the flight
number  but as we see below  slot values often occur in
isolation 

experiments

having introduced the general framework for distantly
supervise event extraction  in this section i present experiments testing various models in this framework  for
all test set scores that i present  the model has been
tuned to maximize f   score on the   infobox dev set 

   

experiment    simple local classifier

first i use multi class logistic regression to train a
model which classifies each mention independently  using the noisy training data described above  features include the mentions part of speech  named entity type  surrounding unigrams  incoming and outgoing syntactic depencies  the location within the document and the mention string itself   for example 
the mississippi example from section   might have the
following binary features  lexincedge prep in crashvbd  unlexincedge prep in vbd  prev word in 
 ndprev word crash  netype location  sentnetype organization  etc 
i compare this local classifier with a majority class
baseline  table   shows the distribution of labels in the
distantly generated training data  the majority baseline
simply picks the majority class for each named entity
type  hsitei for locations  hoperatori for organizations 
and hfatalitiesi for numbers 

 the plane went down in central texas 
    died and    were injured in yesterdays tragic
incident 
instead  i adopt a document level heuristic  given a
slot value and flight number pair from a training infobox 
if the slot value occurs in the same document as the
flight number  mark the mention as a positive example
for that slot type  since were using the presence of
the flight number as a heuristic for document relevance 
this is equivalent to only labeling mentions that occur
in documents relevent to the training event 
named entities that occur in a relevant document but
dont match any slot values are given nil labels  after
the process is complete  nil examples are subsampled 
resulting in a training set with a       split between
nil and non nil examples 
due to the heuristic nature of this noisy labeling
scheme  the resulting training examples are extremely
noisy  in fact  training data noise is a hallmark of distant
supervision  noise is prevalent in the relation extraction
settingfor example  any sentence containing both apple and steve jobs will be marked with the founderof
relation  even the sentence steve jobs was fired from
apple in       likewise there are many false labelings
in the event extraction setting  such as when an airlines
name is mentioned  but the sentence has nothing to do
with that airline being the operator in the target crash
event  in fact  by manually checking    examples from
each slot type  i found that     were wrong  this high
degree of noise is a central challenge to the distant supervision approach  and will be a theme that resurfaces
in the experiments that follow 

table    label frequency in noisy training data 
label
frequency named entity type
 nil 
     
site
     
location
operator
    
organization
fatalities
    
number
aircraft type     
organization
   
number
crew
survivors
   
number
passengers
   
number
injuries
 
number
to compare performance on the final slot prediction
task  i define precision and recall as follows  precision is
the number of correct guesses over the total number of
guesses  recall is the number of slots correctly filled over
the number of findable slots  a slot is findable if its true
value appears somewhere as a candidate mention  in
other words  we dont penalize the extraction model for
missing a slot that either wasnt in the corpus or didnt
occur under our heuristic notion of relevant document 
 

parsing  pos tagging  and ner  stanford core nlp 
nlp stanford edu software corenlp shtml

 

fito see whether this imbalance hurts system performance  i trained a new local classifier using only     documents for flight      this improved dev set precision
slightly       vs        but hurt recall       vs        
however  this effect appears to be a consequence of less
training data  not training set imbalance  i trained another model using all       flight     documents but
only using    training events instead of     the results
were comparable  prec       rec       

table    performance of local classifier vs  majority
baseline 
precision recall f   score
maj  baseline
     
     
     
local classifier
     
     
     
table    accuracy of local classifier by slot type
site
           
operator
           
fatalities
           
aircraft type
           
             
crew
survivors
         
passengers
            
injuries
      na

   

experiment    sentence relevance

with the simple local classifier described in section     
a lot of errors come from sentences that are irrelevant
to the event  for example  northwest airlines was classified as hoperatori in the sentence below  but in fact
neither the sentence nor northwest airlines had any relsome slots  such as hcrashsitei  can have multiple val  evance to the target plane crash event 
ues  this metric only requires identifying one or more
 clay foushee  vice president for flying operations
of them 
for northwest airlines  which also once suffered
the performance of the local and majority classifiers
from a coalition of different pilot cultures  said the
are shown in table    the test set contained    test
process is long and involved and acrimonious 
infoboxes for a total of     findable slots  the local
classifier significantly outperforms the baseline  table  
these errors would be mitigated if we could eliminate
breaks down the accuracy of the local classifier by slot
irrelevant sentences from consideration during mention
type 
classification  to this end  i trained a binary sentence
relevance classifier over unigram and bigram features 
    experiment    training set bias
like the mention models  the relevance classifier was
recall that during distant supervision  training exam  distantly supervisedduring noisy labeling  a training
ples are generated for a training infobox for every doc  sentence was marked relevant if it contained at least one
ument relevent to that infobox  figure   shows the fre  slot value from one event 
two new local models incorporate this sentence relequency of relevant documents for each infobox  we see
that most infoboxes selected a hundred or so documents  vance signal  in localwithhardsent  all mentions from
but one event in particular had several thousand relevant non relevant sentences are classified nil  in localwithdocuments   incidentally  this high frequency event is softsent  sentence relevance is used as a feature in menthe crash of pan am flight    a k a  the lockerbie tion classification 
the test set results for these new models are shown in
bombing   consequently a large portion of the noisy
table    surprisingly  the new models significantly untraining examples are due to this single event 
derperform the simple local model  one explanation is
that the distant supervision for sentence relevance was
just too noisy to train a good classifier  still  it is surprisfigure    relevant documents per training infobox
ing that this hurt performance  if we cant get a signal
from sentence relevance  we would expect localwithsoftsent to ignore the relevance feature  not to perform
worse 
table    classifiers using sentence relevance 
precision recall f   score
local classifier
    
    
    
localwithhardsent
    
    
    
localwithsoftsent
    
    
    
 

fii also applied sentence relevance to the model upgrades described in sections     and      in every case 
sentence relevance hurt performance 

   

figure    error propagation in pipeline classification 

experiment    pipeline model

so far i have presented only local models which classify
this error propagation is particularly worrisome in
mentions independently  but in reality there are dependencies between mention labels  for example  hcrewi our distant supervision setting due to the high amount
and hpassengeri go together  hsitei often follows hsitei  of noise in the training data  to extend the example 
suppose instead that at distant supervision time     was
and hfatalitiesi never follows hfatalitiesi 
given the incorrect gold label hfatalitiesi  now at test
time  we might correctly classify    as hinjuriesi  but
   crew and     passengers were on board 
this will put us in an unseen feature space for subsequent
decisions because usairways saw hfatalitiesi at training
 the plane crash landed in beijing  china 
time  not hinjuriesi 
      died and    were killed in last wednesdays
an ideal solution to this error propagation problem
crash 
should do two things  first  it should allow suboptimal
local decisions that lead to optimal global decisions  for
in this experiment  i compare a pipeline model with the previous example  this means that our choice for
the simple local model  in the pipeline model  mentions    should take into account our future performance on
in a sentence are classified sequentially  at each step  usairways and boeing      second  models of sequence
the label of the previous non nil mention is used as a information should be based on actual classifier output 
feature for the current mention  at training time  this is not gold labels  this way we wont be in an unfamiliar
the previous mentions noisy gold label  at test time  feature space each time our decision differs from the gold
this is the classifiers output on the previous mention 
label 
the pipeline model boosted recall  but took a slight
in essence  we want a joint mention modelone which
hit on precision  table   shows test set results  a qual  optimizes an entire sequence of mentions jointly rather
itative analysis of the pipeline models feature weights than one at a time  to this end  i applied the searn alrevealed that the classifier learned the patterns men  gorithm  daume        to mention classification  searn
tioned above  as well as others  however  this wasnt stands for search based structured prediction  at a
enough to significantly improve performance 
high level  searn is an iterative solution to the following chicken and egg problem  we want a set of decision
costs based on an optimal global policy  and we want a
table    performance of pipeline model
global policy to be learned from these decision costs  a
precision recall f   score
sketch of the algorithm is given in figure    the curlocal model
     
     
     
rent hypothesis is an interpolation of the optimal policies
pipeline model
     
     
     
learned at each iteration  the algorithm is seeded with
an initial policy that simply chooses gold labels  akin to
a pipeline approach   at each iteration  the hypothesis
    experiment    joint model  searn 
moves away the gold policy  and ultimately this inital
there is a common problem with pipeline models which policy is dropped from the final hypothesis 
at each iteration  searn requires a cost sensitive clasmay explain the performance reported above  pipeline
models propagate error  consider the example in figure sifier  for this i follow vlachos and craven        in
   at training time  usairways has the feature prev  using the algorithm in cramer et al          which
label injury  but suppose that at inference time  amounts to a passive aggressive multiclass perceptron 
we mislabel    as hsurvivorsi  now usairways has the searn has a number of hyperparameters  by hill climbfeature prev label survivor  and we are in a fea  ing on my development set  i arrived at the following
ture space that we never saw in training  thus we are settings    searn iterations    perceptron epochs per itliable to make the wrong classification for usairways  eration  interpolation         perceptron aggressiveness
and if we make the wrong decision there  then again we       
the test set results comparing searn to the pipeline
are in an unfamiliar feature space for boeing     which
and
local models are shown in table    searn clearly
may lead to another incorrect decision 
 

fi stockholm hsite       nil       crew      etc i

figure    the searn algorithm for mention classification 

given a distribution over labels   for each mention
m in m   the set of mentions for a particular candidate
value  we can compute noisy or for each label as follows 

n oisyor      p    m       

y

   p r   m 

mm

if the noisy or of a label is above some threshold  we
use the labelotherwise we return nil  i found     to
be an optimal threshold by tuning on the development
table    performance searn  pipeline and local models 
set  table   shows test set results comparing noisy or
precision recall f   score
and exhaustive aggregation on the simple local claslocal model
     
     
     
sifier  we see that noisy or improves precision while
pipeline model
     
     
     
decreasing recall   this is to be expected as noisy or
searn model
     
     
     
is strictly more conservativei e  nil preferingthan
exhaustive aggregation   in terms of f   score  noisydominates  a likely explanation is that searn was able or aggregation is the better method 
to model the inter mention dependencies described in
section     while avoiding the error propagation endemic table    two label aggregation schemes applied to loto the pipeline model 
cal model 
precision recall f   score
exhaust 
agg 
    
    
    
    experiment    noisy or aggregation
    
    
    
noisy or  agg 
as described in section    the final step in event extraction  after mention classification  is label aggregation 
so far i have assumed exhaustive aggregationas long   conclusion
as at least one mention of a value gets a particular slot
label  we use that value in our final slot filling decision  i have presented a distant supervision approach to event
intuitively  this approach is suboptimal  especially in a extraction using plane crash events as a test bed and denoisy data environment where we are more likely to mis  scribed how various models perform in the framework 
classify the occassional mention  in fact  a proper ag  in future work  i will explore better notions of document
gregation scheme can act as fortification against noise relevance  to replace the naive contains flight number
heuristic   and i will look into better methods for apinduced misclassifications 
with this in mind  i adopted a max aggregation plying sentence relevance to the system  additionally 
scheme  when multiple non nil labels occur for men  i will apply this framework to the muc   shared event
tions of a particular value  choose the label that occurs extraction task to compare distantly supervised event
most often  interestingly  this scheme had little effect extraction with the fully supervised state of the art 
       precision  on system performance  it turns out
references  koby crammer  ofer dekel  joseph keshet  shai
the scenario with multiple non nil labels is relatively
shalevshwartz  and yoram singer          online passiveaggresrare  instead  it was most common to see mention la  sive algorithms  journal of machine learning research           
bels split between a single non nil label and nil  in  hal daume         practical structured learning techniques
this case  exhaustive and max aggregation always re  for natural language processing  phd thesis  usc    mihai
turn the non nil label  but we would prefer a scheme surdeanu  sonal gupta  john bauer  david mcclosky  angel x 
chang  valentin i  spitkovsky  christopher d  manning        
that can select nil under the right cirumstances 
stanfords distantly supervised slot filling system  proceedings
to achieve this i use noisy or aggregation  the key of the tac kbp workshop   mike mintz  steven bills  rion
idea is that classifiers gives us distributions over labels  snow  daniel jurafsky        distant supervision for relation exnot just hard assignments  a simplified example is given traction without labeled data  acl afnlp              andreas vlachos  mark craven         search based structured prebelow 
diction applied to biomedical event extraction  in proceedings
of the fifteenth conference on computational natural language
learning  association for computational linguistics       

 stockholm hnil      site       crew       etc i
 

fi