magic  the gathering
deck performance prediction
roger hau  evan plotkin  hung tran

introduction 
magic  the gathering  mtg  or magic  is the oldest
and most popular trading card game played today 
due in part to the complex interplay of thousands of
cards  in two player games  each player constructs a
 main  deck  referred to as simply  deck  hereafter 
which consists of    cards  allowing players to
pursue an enormous number of strategies and card
combinations  when experienced magic players
select cards for their decks  they work to select both
powerful cards as well as cards that complement
each other  in other words  a magic decks strength
is based not only on cards that are powerful
individually  but also are synergistic with the other
cards in the deck 
our project seeks to accurately predict the strength
of a deck in tournament play by assigning it a score
between   and       in order to assign scores to a
deck  we have examined several deck scoring
algorithms that consider both the individual cards in
a deck as well as the synergy within that deck 

methods 
overview
the ultimate goal is to train an algorithm that  given
a deck of magic cards  will be able to assign this
deck a score that reflects its true tournament
performance  to generate the deck scoring  our
algorithm considers several features  namely the
individual cards that compose the deck as well as
various measures of synergy  which we must
generate from the training data  thus  there are three
primary steps in generating the deck scoring 
   our algorithms must generate a set of feature
parameters representing synergy from the
training data 
   our algorithms must compute a deck scoring
using the generated feature parameters and the
presence of individual cards 
   the error  or difference between the predicted
and true value  will be calculated and recorded 

data sources
to obtain a large number of high quality decks  we
collected tournament results from wizards com  a
site that hosts internationally ranked  official magic
tournaments  the tournament results included the
decks and their respective rating  in total  we have
collected        decks representing     cards 

features
we hypothesize that the individual cards of a deck 
along with the synergies within that deck would be
strong predictors of a decks tournament
performance  we resolved to evaluate two particular
areas of synergy  the presence of small combinations
of synergistic cards  and the overall synergy of a   card deck  however  there is no simple way for our
algorithm to identify complementary card
combinations in the context of magic  similarly 
there is no simple way for our algorithm to analyze a
deck of cards and determine the archetype  thus  we
must develop computable heuristics to represent
card combinations and overall deck synergy  a more
in depth discussion of these heuristics and feature
generation is below 
deck archetypes  k means clustering 
algorithm
an important characteristic of a magic deck is its
archetype  weve mentioned archetypes before  but
let us more rigorously define them  in magic  when
two decks have very similar compositions  they
belong to the same archetype  since archetypes have
relatively specific composition requirements 
archetypes also tend to define the strategies available
to a particular deck  strong archetypes emerge from
human analysis and tournament results  and wellperforming archetypes often make up the majority of
decks in a given tournament  however  although
archetypes do have composition requirements  there
is still variability among decks of the same
archetype 
we hypothesized that archetype is be a strong
indicator of tournament performance  as a result 
our group sought to capture archetypes as a feature
and determine an algorithm to classify decks based
on their archetype  since we sought to determine
groupings of decks  our initial choice was to use the
k means clustering algorithm to classify our decks 

fiin our implementation of k means clustering  our
clusters          k rn were initialized to be the
composition of randomly selected decks from all of
the decks in our training data  since most
tournament decks belong to an archetype and the
 deck compositions are good approximations of a
general archetype composition  it is reasonable to
initialize the clusters close to their eventual true
values  our feature vector x represented a deck  and
corresponded to a vector of frequencies  where each
index corresponded to a particular card 
results
to evaluate the quality of our clusters  we found the
average distance of a point in the cluster to the
cluster center  the smaller the average distance  the
better the quality of the cluster  after running our
algorithm with varying numbers of initial clusters 
we found that the average distance converged with
   clusters  we also performed a manual inspection
of our clusters  which increased our confidence in
this algorithm  a personal examination of the top
cards in a given cluster revealed that that cluster
represented a known popular archetype 
  
mean distance from cluster center

  
 
 

figure    scatterplot of data projected on pca components

the below graphic is a visualization of two axis
created by our pca analysis  the circled groups of
cards actually represent deck archetypes  the green
grouping is a type of delver deck  the red
grouping is a type of poison deck  and the blue
grouping is a type of zombies deck  these
groupings are far from the origin because the cards
within them are typically used only in that certain
archetype  the orange grouping is interesting 
because many of the cards in that grouping are used
in numerous archetypes  as such  it makes sense
that this grouping is more clumped together and
closer to the origin 
when comparing our pca algorithm to our
clustering algorithm  we found that they computed
very similar groupings  decks that were clustered
together with our clustering algorithm also had
similar compressed pca feature vectors 

 
 
 
                                                  

card combinations  association rule learning 

analysis 

algorithm

the clustering algorithm worked exceptionally well 
and correctly classified the decks into their
appropriate archetypes  although k means clustering
worked well  we wanted to check and see if an
alternative algorithm would produce the same
results  specifically pca 

since it would be extremely difficult to evaluate the
true synergy of card combinations within the context
of magic  we instead used association rule learning 
which identified combinations where constituent
cards primarily appeared in conjunction with the
other cards in the combination  the number of
possible combinations  though  is enormous  and so
we set an upper limit on the grouping size to be   
  where n
is the size of the grouping  and n is the number of
unique cards in the training set 

our pca algorithm identified deck archetypes as
hidden variables that influenced the presence of
cards in a deck  a visualization of the clusters
verified that the pca output was indeed correct 

fithe confidence of an associative rule is a measure of
how accurate we expect that rule to be  by summing
the confidence of relationships between different
cards within a set  our rating achieves a measure of
how related the cards in that set are 
we then selected the varying numbers of the toprated combinations as features  i e  the top        
          and     combinations  our results are
below 
as with the deck clustering  we manually examined
the top rated combinations and found that they were
indeed logical combinations of cards 
analysis 
when we examined the combinations  we found that
most combinations were simply subsets of the most
popular archetypes  and suspected that using
combinations as a feature may not add more
information to our scoring algorithm 
scoring algorithm  regression 
in order to score the decks using our generated
features  we needed an algorithm that would be able
to compute a continuous score  we decided that
regression algorithms fulfilled this requirement 
since regression is a supervised learning algorithm 
we needed to a score for each deck before we
could perform the regression  we based this score
on the decks placement in its tournament  the first
place deck would receive a score of       the last
place deck would receive a score of    and the decks
in between would be assigned scores in between
these values  for example  in a   person tournament
the scores would be  in order of last to first         
               
weighted linear regression
we decided to use weighted linear regression
because we believed that the scoring of one deck
would be more dependent on the scoring of similar

decks  and less dependent on the scoring of less
similar decks 
for the input to our linear regression  we considered
each unique card to be a feature  the top rated
combinations to be features  and the deck archetype
to be a feature 
after implementing weighted linear regression  we
found that it was not a good approach for scoring
decks  because many of the decks were so different
from each other that they shared few if any cards 
the majority of values in the weights matrix were
nearly zero  the loss of precision meant that our
weights matrix was approximated as a singular
matrix  which has no inverse  as a result  matlab
was unable to compute a closed form solution for the
weighted linear regression 
linear regression
after failing to perform weighted linear regression 
we decided to try linear regression without using
weights  we performed linear regression using
various combinations of the features we discussed
above  below are our results 

square root of mean squared error

in order to filter out superfluous combinations  we
assigned each combination a rating  which
represents the synergy of a combination  using the
following functions 

linear regression
performance
   
   
   
   
   
   
   
   
  
 

linear
regression
performance

error calculation
in order to have a data set for testing purposes  we
decided to remove    tournaments  representing
around      decks from our training set  we then
used these tournaments as our testing set  we
evaluated each deck in our testing set using our
linear regression model  we then calculated the
mean squared error by finding the average of

fisquared difference between our prediction and the
actual score of the deck 

conclusions and suggested future work
our goal was to find an accurate scoring algorithm 
while our algorithm performs around     better
than random  there is still a significant error in our
algorithm s predicted score  a large issue is high
amount of variance within the actual data  which
reflects the unpredictability of the card game 
typically  stronger decks only have a marginally
higher chance beating a weaker deck  and so a deck s
strength does not necessarily guarantee a deck s
success in a tournament  for example  it is not
uncommon to see a tournament where identical
decks achieve significantly different placements 
although more advanced and more complicated
algorithms could possibly improve the scoring
accuracy  we have realized that there are
fundamental issues in our approach to the deck
scoring problem  specifically  the data we collected
does not capture all of the features that would help
score a deck  and our generated features captured the
same quality  in the rest of the conclusion we will
discuss these two issues and make suggestions for
future work 
data
while we had a very large quantity of data points 
there are significant factors that contribute to a
decks placement in a tournament that our data does
not reflect  such as the skill of the player playing that
deck or the properties of the cards  another
important factor is the tournament bracket  which
determines which decks will play against each other 
knowing the bracket would allows us to identify
which other archetypes an archetype is strong
against or weak against  then  provided with a new 
incomplete tournament bracket  a new algorithm
might be able to predict a completed tournament
bracket instead of calculating absolute deck
scorings 
feature generation
the two features we generated were the deck
clusters and top rated card combinations  after
generating these features  though  we had noticed
that the clusters and card combinations appeared to
capture the same deck characteristics  after
computing the scoring for these characteristics  note
that the scoring error using only the clusters  and the

scoring using only    combinations are     and    
respectively  confirming our suspicions  even
though we had generated two seemingly different
features  because the two features represented the
same data their combination did not improve the
scoring accuracy 
to further improve the scoring accuracy  our team
believes that it would be necessary to generate
different features  however  our team could not
come up with additional meaningful features from
this particular data set  which leads our team to
conclude that a more informative data set would be
required 

references
tan  pang ning  michael  steinbach  kumar  vipin
         chapter    association analysis  basic
concepts and algorithms   introduction to data
mining  addison wesley  isbn               
 what s happening    magic online   digital games
  magic  the gathering   what s happening   
magic online   digital games   magic  the
gathering  n p   n d  web     dec       

fi