detecting bad wikipedia edits
brett kuprel
december         
introduction
wikipedia is often criticized for having inaccurate information  it would be nice if potentially bad edits were   
automatically flagged  and    brought to the attention of users more acquainted with the area  consider the
following  simplified  scenario 
amy edits mostly physics articles  bob edits mostly history articles  amy edits a history article  the edit
is flagged and brought to bobs attention 
i present a method for detecting and managing these anomalous edits 
data
a weeks worth of wikipedia edits were collected from august          to september         in the following
form 
 user id   article id   time of edit 
i save the last     of data  about a days worth  to test on  after removing articles edited less than   times 
users who made less than   edits  and multiple edits by a user to an article  the data represents    
edits to
articles made by
users 
matrix representation
i reorganize the data into a sparse matrix
 

 

using the following code in matlab
load  data mat   
data sortrows data    
data data          
   ia    unique data  rows   
data data sort ia     
data train data   round    end     
data test data round    end    end    
m max data train        n max data train       
vet and data test       m data test       n  
data test data test vet    
a sparse data train      data train        m n  
a test sparse data test      data test        m n  
editmin   
while sum sum a    editmin     sum sum a    editmin 
x sum a     editmin 
y sum a     editmin 
a a x y  
a test a test x y  
end
 m n  size a  

fithe resulting matrix representation of the data looks like this 

adjacency matrix 



   

edits


articles

the data is much more sparse than it appears  only       of s elements are nonzero 
model
the adjacency matrix

can be modeled by a low rank matrix using singular value decomposition  svd  











 u s v  svds a k  

where is the rank of the approximation  looking at a histogram of the top     singular values of a  it appears
  is a reasonable choice for  

top   singular values

fidata parameter analysis
it is important to not overfit the data 
number of users 
number of articles 
number of features 
number of edits 
ratio of edits to parameters

 

 



there is about   data point for each parameter  could be better  but it is reasonable 
features
the rows of and can be thought of as feature vectors for articles and users  an element of is then
represented by the dot product between the editing users feature vector and the articles feature vector 
 

    
    

 
 

 
 

 





i normalize the feature vectors so that the inner products are between    and   
   

   

let me plot distributions of the features for the wikipedia data 

distribution of normalized features
users

articles
feature  

feature  

feature  

feature  

feature  

they look similar  this is good  i also want to visualize the locus of points in feature space  i cant make a
dimensional plot  but i can plot
planar projections  to best see the structure in the data  i will plot
along the principal axes using pca   i dont subtract the mean or divide by the standard deviations because the
feature vectors would no longer be unit length 

fiu normr u sqrt s   
v normr v sqrt s   
 q    eigs  u v    u v  k  
u u q 
v v q 

principal cross sections of normalized features

 

users
articles

it is clear from the planar projections that the locus of articles in feature space is very similar to the locus of
users  this implies that physicists edit physics articles  history people edit history articles  etc  
anomaly detection
the normalized feature vectors lie on a k dimensional sphere  similiarity is then easily expressed as an inner
product between user and article feature vectors  a dot product of    represents unrelated feature vectors 
whereas a dot product of   represents related feature vectors  the negative dot prodcuts correspond to
anomalous edits  e g  an arts major editing a biology article  

anomalies
  
feedback
these anomalous edits should be reported to users with feature vectors more aligned with those articles  the
more experienced user would then mark the edit as good or bad  this rating would affect the less experienced
users quality score  i e  it could have just been a grammar edit  no harm done 

 

to see this  imagine that there were only two articles on wikipedia  then there would be   articles in feature space  if users tend to edit
only one article or the other  you would see   clusters of users in feature space  if they edited each article randomly  there would be one
random disparate set of users in features space  i e  the article point locus   pts  would not look like the user point locus  since there are
many interrelated articles on wikipedia  the locus of points does not form distinct clusters  but it is similar for users and articles 

ficonclusion
i have presented a method to detect anomalous wikipedia edits without knowledge of any content  this is
useful because there is an overwhelming amount of edits made to articles everyday and inaccurate edits often
go unnoticed  a computationally efficient detection algorithm  i e  a dot product  would allow edits to be sorted
in real time in order of suspicion and shown specifically to field competent users as determined by their edit
history and feedback scores  such a distributed method would improve the quality of wikipedia as a whole and
make it a more reliable source of information 
references



konect wikipedia data  http   konect uni koblenz de networks edit enwiki 
montanaris ee    b lecture notes  http   www stanford edu class ee   b handouts html 

fi