breast cancer prognosis
catherine lu and jj liu
 cglu  jsquared  stanford edu
december         

 

 

introduction

objective

our goal was to predict survival for each individual  we
had two approaches  predicting a discrete survival status
based on time since diagnosis and other features  and predicting a continuous survival time based on all features 

an accurate breast cancer prognosis  or breast cancer survivability prediction  is important as it often guides the
treatment course of action  ability to claim additional financial support from the government  actions of the patient and family  and more      predicting breast cancer
survivability is commonly done using clinical features 
tnm staging  the globally accepted standard used to describe cancer  was devised more than    years ago and
only looks at three features  size of the tumor  number of
regional lymph nodes with cancer  and the spread of cancer to other parts of the body  with the advent of affordable genomic sequencing and acceleration of findings in
molecular biology in the past decade  molecular features
may be practical to improve breast cancer prognosis 
molecular diagnostics for cancer therapy decisionmaking have shown initial promising clinical results 
this has lead to a flood of published reports of signatures predictive of breast cancer phenotypes  and several
molecular diagnostic tests for cancer therapy decisionmaking have gained regulatory approval in recent years
        however  there is no consensus for the most accurate computational methods and models to predict breast
cancer survivability  in addition  it is unclear that incorporating molecular data as a complement or replacement
for traditional clinical diagnostic tools adds any value     
therefore  it is necessary to objectively assess whether
genomic data currently provides value beyond traditional
clinical diagnosis tools 
to aid in efforts to solve this problem  we predicted
breast cancer survivability with machine learning techniques as part of the dream breast cancer prognosis
challenge  the ultimate goal of the challenge is to objectively compare many computational algorithms through
providing a common training dataset in an effort to find
the best features for breast cancer prognosis  the dataset
provided contains standard clinical measurements in addition to genomic information  thus allowing genomic information to be compared with standard clinical features 

 

data

breast cancer sample data is made available through the
dream challenge from the metabric data of      
breast tumor samples used in a previous study      where
data origin and preprocessing is explained in detail  we
further process the data by discarding samples with missing values  and are thus left with     samples 

   

survival

there are two indicators of survival  time from breast
cancer diagnosis to last follow up and status of the patient  alive or dead  at last follow up time  survival data
is right censored  since patients may be alive at the end
of the study or lost to follow up 

   

gene expression

gene expression is generated using molecular profiling
platforms  described in full detail in another study     
the genes used as training features are narrowed to a list
of   suggested by the dream challenge and previous
literature  we used two estrogen pathway genes  er and
pr   two human epidermal growth factor   receptor amplicon genes  her  snp  and gii   and five immune
response genes  cxcl    stat   gbp   gzma  and
cd    

   

clinical annotations

in addition  we have the following clinical annotations 
the classic features used for breast cancer prognosis 
 

fi 

development and results

feature
age
treatment

 
metric
years
none  ht  rt 
ct  ht rt  ht ct 
rt ct  ct ht rt

lymph nodes positive 

          

size 

          

grade

       

estrogen receptor immunohistochemistry  er ihc 

   

description
age of patient at diagnosis
ht  hormone therapy
rt  radiation therapy
ct  chemotherapy
number of lymph nodes found with cancer
   no nodes
       nodes
       nodes
   over   nodes
        mm
         mm
   over    mm
   direct extension to chest wall or skin
   nottingham score    
   nottingham score    
   nottingham score    
the score is a semi quantitative measure of
three histopathological characteristics seen
under a microscope by a pathologist 
presence of er from ihc protocol

 used in standard tnm classification of breast cancer

 
   

measuring performance
predicting survival status

ing and predicting on the same  entire data set 

 

development and results

we initially build machine learning models that predict
the patients status  dead or alive  based on all other fea      predicting survival status
tures  we measure performance using   fold cross validation accuracy in addition to a data set accuracy for we used patient status as the target variable and all other
features as the input features  we used the r caret packtraining and predicting on the same  entire data set 
age  which provides a library for a number of machine
learning models  to write and run different algorithms 

   

predicting survival time

next  we predict survival time of the patient  however 
we do not have survival time for all patients  the data is
highly skewed and right censored  patients may drop out
of the study at any point or still be alive by the end of the
study 
with a data set of only      it is extremely important
to still use all of the training data  two patients survival
times can be ranked not only if both have uncensored
survival times but also if the uncensored time or one is
smaller than the censored survival time of the other  one
of the most commonly used performance measures for
survival models is the concordance index  ci       ci
is the fraction of all pairs of subjects whose predicted
survival times are ordered correctly across all patients  a
ci of   indicates perfection prediction accuracy  while a
ci of     is as good as a random predictor 
hence  we measure performance using   fold cross
validation    fold cv  for ci in addition to ci for train 

     

results

first  we used the k nearest neighbor algorithm to classify our data based on the closest feature training samples  we use a k value of    to see if there were any underlying relationships among features for patients based
on status  however  our   fold cv accuracy was low
        
we then tried   supervised learning models  none of
them performed better than       for   fold cv  though
running and predicting on the entire data set gave values
ranging from       to        the models were overfitting the data and were not representing the relationships
between the features accurately 
in particular  the gradient boosting model  gbm   an
ensemble learning method which uses multiple weak prediction models to form a single model in a stage wise
fashion  resulted in the most overfitted model 

fi 

development and results

algorithm
k nearest neighbor
multinomial
linear discriminant analysis
generalized linear models
linear support vector machines
generalized boosted model
cox proportional hazard regression 
random survival forest 

 
  fold cv
     
     
     
     
     
     
     
     

data set
     
     
     
     
     
     
     
     

 concordance index

out of the standard machine learning approaches  linear svm performed slightly better than the rest  possibly
because it did not overfit the data as much as other models 
it is interesting to note that linear discriminant analysis  lda  performed approximately the same as generalized linear models  glm   even though lda is a
more simple model than glm  lda finds a linear combination of our clinical features which characterizes the
patient survival status  we also used glm  a generalization of ordinary linear regression models that allow for
response variables that do not follow a normal distribution  because our response variables do not necessarily
follow a normal distribution  but instead could follow a
distribution more similar to a log odds model due to our
prediction of status as a bernoulli variable 

the cox proportional hazard     approach estimates
weight w by leaving the baseline hazard function unspecific and maximizing the likelihood 
tx

l w   



ti uncensored

ew

i
tx

tj ti ew

j

   

where ti is survival time of patient i 
after this estimation  we trained using weighted linear
regression  in order to avoid overfitting  we use akaike
information criterion  aic  on the features passed to the
cox model  the aic is a measure of the relative goodness of fit of a statistical model  often described as a
tradeoff between bias and variance or between model accuracy and complexity  we first find the corresponding
aic values  and selected the model that minimizes information loss 
we obtained a   fold cv ci of        comparable to
    predicting survival time
the ci of       for training and predicting over the entire
we then predicted survival time using all features as input data set 
and the ci as the measurement of model performance 
the outputted survival models compute the time it takes       random survival forest
for death to occur according to the features 
the random survival forest  rsf  algorithm     is an
ensemble tree method for the analysis of right censored
      cox proportional hazard regression with survival data  more specifically  the algorithm performs
akaike information criterion
the following 
proportional hazard  ph  models are the standard for
studying the effects of features on survival time distributions  a hazard function  t  measures the instantaneous
rate of death at time t 
the ph model assumes there is a multiplicative effect
of the features on the hazard function 
t x 

 t x       t e w

   

where  t x  is the hazard function with features x 
   t  is the baseline hazard function when x      w is
t
the vector of unknown parameters  and ew x is the relative hazard function 

   draw b bootstrap samples from the original data 
where each bootstrap sample excludes on average
    of the data  called out of bag data  oob data  
   grow a survival tree for each bootstrap sample  at
each node  randomly select p variables  then  split
the node with the candidate variable which maximizes survival difference between daughter nodes 
   grow the tree to full size 
   calculate a hazard function  hf  for each tree  and
average to obtain the ensemble hf 

fi 

rsf analysis

 

based on the size of our data  we ran a rsf algorithm
with the number of trees to grow to       we use the
logrank splitting rule  which splits tree nodes by maximization of the log rank test statistic 
we obtained a   fold ci of        which is also comparable to the ci of       for training and predicting over
the entire data set 

the best   fold cv ci was achieved by taking all
features except for ehr ihc status and er expression 
ehr ihc status appears to lower the ci and er expression does not add any value 

   

ensemble analysis

the following figure shows the ensemble survival function for each patient  the thick red line is overall en  rsf analysis
semble survival  and the thick green line is nelson aalen
we chose the rsf model  the best performing model  to estimator  the nelson aelen  often used to give an idea
of the survival rate shape  is given by the equation 
gain insights into relationships among features 

   

h t   

feature selection

   

ti t

we determined which features contributed most to the
learning using backward search feature selection 
features ommitted  cumulative 
none ommitted
er ihc status
er expression
grade
her  snp  state
gbp  expression
cd   expression
treatment
cxcl   expression
gzma expression
pr expression
size
gii
lymph nodes posititve
age

di

 ni

  fold cv ci
     
     
     
     
     
     
     
     
     
     
     
     
     
     
n a  all omitted 

where di is the number of deaths at ti and ni is the total
number of patients alive at ti  

note that the overall ensemble survival begins to deviate from the nelson aelen estimator at later times 
the second figure below shows the same relationship 
where it is shown that rsf tends to predict higher survival probabilities when survival proportions in the data
set are low 

fireferences

 

discussion

breast cancer prognosis presents an important challenge
with many real life implications  in this paper  we
have described our use of various machine learning approaches to the complex problem of predicting breast
cancer survivability rate  with the data provided through
the dream breast cancer prognosis challenge 
our results indicate that it is difficult to create accurate
standard machine learning models for predicting patient
survival status  survival data has many unique properties  the standard machine learning models did not have
any notion of a hazard function for determining patient
survival status  instead  it found unreal relationships that
solely existed in the unique data set  which was seen
from the large difference in accuracy between   fold cv
and accuracy from training and predicting on the data set
 which were also quite low  
on the other hand  the two models that predicted hazard functions seemed to do quite well  though it is difficult to compare due to the different model performance
measurements  it appears that both the cox and rsf
models capture the relationship among features and survival outcome  as seen in almost identical values between
the   fold cv ci and ci from training and predicting on
the data set 
from feature analysis  we learned that at least for the
rsf model  age at diagnosis was the best feature predictor  in addition  eliminating two features  estrogen receptor copy number and estrogen receptor gene expression 
in the model lead to a slightly higher   fold cross validation score than with all features 
from rsf ensemble analysis  we saw that rsf
seemed to perform better at predicting either patients
with less time since diagnosis or when there is higher
probability of survival  or both  therefore  rsf combined with another algorithm that performs well in these
conditions may produce even better results 
this work has limitations and could be improved in
three major ways  first  we should examine all genes
available in the data set and  using feature selection  find
the most predictive genes  second  we should modify our
regular machine learning models to predict the cox hazard function to give each model the right censored data
relationship that exists  it is not necessarily that rsf is
the best predictor of survival out of the algorithms we
have used  third  we should run our algorithms on more
data  to do so  we should modify our algorithms to impute or skip missing features without discarding the entire training example and use publicly available data sets 

 

references
    r  henderson  m  jones  j  stare  accuracy of
point predictions in survival analysis  statistics in
medicine       
    l  j  vant veer  h  dai  m  j  van de vijver  y 
d  he  a  a  m  hart  m  mao  h  l  peterse  k 
van der kooy  m  j  marton  a  t  witteveen  g  j 
schreiber  r  m  kerkhoven  c  roberts  p  s  linsley  r  bernards  and s  h  friend  gene expression
profiling predicts clinical outcome of breast cancer 
nature  vol       no        pp          jan       
    s  paik  s  shak  g  tang  c  kim  j  baker  m 
cronin  f  l  baehner  m  g  walker  d  watson  t 
park  w  hiller  e  r  fisher  d  l  wickerham  j 
bryant  and n  wolmark  a multigene assay to predict recurrence of tamoxifen treated  node negative
breast cancer  n  engl  j  med   vol       no      pp 
          dec       
    c  curtis  s  p  shah  s  f  chin  g  turashvili  o 
m  rueda  m  j  dunning  d  speed  a  g  lynch 
s  samarajiwa  y  yuan  s  grf  g  ha  g  haffari 
a  bashashati  r  russell  s  mckinney  m  group 
a  langerd  a  green  e  provenzano  g  wishart  s 
pinder  p  watson  f  markowetz  l  murphy  i  ellis 
a  purushotham  a  l  brresen dale  j  d  brenton 
s  tavar  c  caldas  and s  aparicio  the genomic
and transcriptomic architecture of       breast tumours reveals novel subgroups  nature       
    v  c  raykar  h  steck  and b  krishnapuram  on
ranking in survival analysis  bounds on the concordance index  nips       
    j  fox  cox proportional hazards regression for
survival data  appendix to an r and s plus
companion to applied regression       
    h  ishwaran  u  b  kogalur  e  h  blackstone  and
m  s  lauer  random survival forests  the annals
of applied statistics       

fi