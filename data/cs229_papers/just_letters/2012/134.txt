voxel selection algorithms for fmri
henryk blasinski
december         

 

introduction

functional magnetic resonance imaging  fmri  is a technique to measure and image the bloodoxygen level dependent  bold  signal in the human brain  the bold signal is strongly correlated with the brain activity  consequently fmri makes it possible to image activity patterns in
a brain of a living organism and thus observe and record its responses to a variety of stimuli  for
this reason fmri techniques have become very popular in psychology and cognitive sciences 
a typical fmri experiment consists in stimulating subjects in some way  vision  hearing  touch
etc   and recording the corresponding activity pattern for a large number of small brain volumes
called voxels  activities within voxels are used as features for machine learning algorithms to relate
brain areas with certain types of stimuli  unfortunately the fmri data has many pitfalls  in most
cases it is characterized with a very large number of features formed by brain voxel activations  up
to several thousands  and a low number of training examples  typically of the order of hundreds 
data pre processing is of particular interest for two reasons  first  selecting a smaller set of features
can improve stimulus prediction accuracy and reduce computational complexity  second  given the
location of voxels most relevant to the prediction task  conclusions about brain circuits can be
made 
current research employs a number of voxel selection techniques  they can be as simple as
manual segmentation of regions of interest from the entire brain     or more elaborate involving
statistical significance tests  recently more advanced algorithms such as ridge regression  lasso 
sparse logistic regression or graph laplacian based methods have started to be more frequently
used      many of these methods promote sparse solutions  i e  such where many of the voxels are
assigned a zero score and therefore considered uninformative  this notion agrees with neurological
understanding of brain organization into different regions responsible for different tasks  for example speech recognition will cause increased activity of certain areas of the brain  and will have
little influence on other 
so far none of the above methods explicitly encodes correlations between features using a
covariance matrix  one of the problems with using empirical covariance is the fact that just a few
data points are available making this matrix non invertible  therefore it cannot be directly used
in optimization algorithms  this project investigates an approximate inverse covariance matrix
estimation technique and its applicability to imposing a gaussian prior distribution on feature
scoring weights 

 

fi 
   

algorithms
univariate tests

univariate tests evaluate the predictive power of each voxel independently of others  the most
popular algorithm used is the t test  which measures the probability p of a population being drawn
from a distribution with some mean   for each voxel q  this mean is assumed to be the average
activity during resting state when no stimulus is shown  it is then compared to the activity
distribution of each of the p stimuli and the corresponding pq l is computed  for an individual
stimulus  the voxel score is then determined as    pi k and therefore the entire score for p stimuli
becomes
k
x
sq ttest  
    pq l    
l  

another univariate test consists in evaluating individual voxels based on their performance as single
features used for classification  the output label is predicted using just one voxel at a time  and
the prediction accuracy denotes the feature score 
the two remaining scoring methods consist in computing the mutual information  mi  or co   
variance  cov  between the voxel q time course xq and the class label indicator variable   y       l  
since the mutual information definition for continuous variables is not convenient to use  the voxel
time course can be quantized to discrete values 
sq mi  

p
x

mi   y       l   x   
q  

l  

sq cov  

p fi
fi
x
fi
   
    fi
cov   y
 
l  
x
 
fi
q fi
l  

   

multivariate tests

as opposed to univariate tests  where each feature is considered independently  multivariate tests
aim at using multiple features at a time  theoretically such an approach allows to discover feature
correlations  consequently a set of features  each of which performs poorly on its own  may result
in a substantially improved classification accuracy or better representation of a particular stimulus 
since in most fmri experiments stimulus classification is performed  it is desired to find a set of
weights  such that y  i    f  t x i    provides the best estimate of the true stimulus label y  i    very
high or low values of  typically indicate that a particular feature has a significant contribution to
the classification decision  in general  consider a p class classification problem with n different
features  a feature q has a score
p fi
fi
x
fi  l  fi
sq  
fiq fi

q              n 

l  

in the most general case  weight parameters are obtained by maximizing their likelihood function
together with some penalty p  l     in this approach the task is to maximize the likelihood function
of the stimulus class label y  given the activity pattern time courses x 
 m
 
n
o
x
 l 
 l 
 l 
 i 
 i   l 
 l 
   arg max l     p      arg max
log p y   l x       p   
   
 l 

 l 

 

i  

fiwhere  is some tradeoff parameter  with p  l         the expression simplifies to a maximum
likelihood estimation  however  this method does not naturally promote sparsity in the solution
and due to few training examples it overfits the data and generalizes quite poorly  the penalty
term p alleviates these issues  popular penalty functions involve the l  and l  norms  lasso and
ridge regression  seen from the probabilistic map perspective adding a penalty is equivalent to
imposing a constraint on the parameter distribution  for example in ridge regression the prior is a
gaussian distribution p  l     n     i  
in a recent paper kamitani et al      investigated the sparse logistic regression model for feature
selection  their method imposes constraints on hypothesis weight distribution  it is assumed that
 l   l 
these weights are gaussian with zero mean and some variance     p q  q    n     q     the
individual weight variance parameter is not deterministic either  but rather distributed according to
a gamma distribution  p q    q    if during the likelihood maximization process the i parameter
becomes very large  the corresponding voxel is deemed irrelevant and therefore pruned from the
set  while this method is quite efficient at selecting a sparse voxel set  the solution relying on the
newton method is computationally complex  it requires multiple inversions of a n  p square
matrix  where n is the number of features and p the number of stimuli  a more computationally
efficient  albeit approximate  solution has been proposed by     

   
     

proposed algorithms
correlated logistic regression
 l 

other methods typically assume independence between weights q   in correlated logistic regression
a gaussian prior distribution on weights  l  is assumed  i e  p  l   l   n     l    it however has a
non diagnonal covariance matrix  the penalty function is therefore given by
 l 
p  l       l t  
l   

since the number of data points is smaller than the number of features  the empirical covariance
matrix e is not full rank  and therefore it cannot be inverted  in a recent paper friedman et al 
    proposed a method for a sparse  inverse covariance matrix estimation  the matrix   is a
solution to the maximization problem
 


    arg max log det    tr e       
l    
 

where e is the empirical covariance matrix and  is a sparsity promoting hyperparameter  with
the inverse covariance matrix estimate in place  the penalty function p can be easily incorporated
into the likelihood function     and the maximizing argument can be found using for example
gradient descent 
     

factor analysis scoring

the final scoring method is based on the factor analysis  by performing factor analysis with k
dimensions it is assumed that the n dimensional data can be approximated by using k dimensions
only 
x i    z  i    
where   n     d  and d is a diagonal covariance matrix  the lower the variance of  associated
with a particular feature  the closer this feature is to being a member of the k dimensional subspace 
assuming that a given stimulus is represented by points on a k dimensional hyperplane  then the
 

fidq q is inversely proportional to the average distance of the feature q from this hyperplane  the
feature score can be therefore given by
fi
p fi
x
fi   fi
fi
fi
sq  
fi dq q fi  
l  

 

results

the experiments were conducted on fmri data  where subjects were shown words at six various
eccentricities  three stimuli were displayed to the left and three to the right of the fixation point 
for each subject three experimental runs were performed  each run consisted of     time points 
in order to limit the computational complexity  the data for two distinct regions of interest  roi 
of only one subject were evaluated  the regions of interest were the early visual areas  left and
right v   each having     and     features respectively  for each run the time course data for each
voxel were normalized to n        
feature selection algorithms were evaluated using   fold cross validation  one third of the data
was used for feature ranking the remaining two thirds were used as training and test sets in a   fold
cross validation  given a particular voxel ranking  the training and test data sets consisted of n
best scoring features only  due to differences in roi sizes  n is in fact a fraction of the total number
of voxels constituting an roi  the classification was performed with an svm classifier with linear
kernel      and the c parameter selected in a grid search  the remaining hyperparameters  and 
were also found using extensive search  however the experimental results did not vary significantly
with their choice 
experimental results are presented in figure   where stimulus prediction accuracies for    
different subsets of best scoring rois feature sets of the left and right v  are given  in each plot a
single solid line corresponds to one feature selection method  this line is drawn on top of a colored
region  whose boundaries are error bars  the random guess performance of       is represented
by the red dotted line  in all cases feature selection methods allow to achieve an above chance
performance with a very small subset of about     of features  for both rois  all eight methods
exhibit very similar performance  only the feature output label covariance based scoring is slightly
inferior  all curves also demonstrate the law of diminishing returns  increasing the feature count
from   to     has a much bigger impact on prediction than a similar change from    to     
selecting more than half of features has virtually no effect on the prediction performance  by
comparison using a greedy forward filtering approach  resulted in selecting     of the left v  voxels
and yielded a     prediction accuracy       and     respectively for the right v    these results
are comparable with the ones obtained via different feature scoring methods 

 

conclusions

this project proposed two new fmri data voxel selection methods  covariance logistic regression 
and factor analysis  in the covariance logistic regression a gaussian prior on the feature weight
distribution is imposed  since the empirical covariance matrix is not invertible  its approximate
sparse inverse is used  the second method  based on the factor analysis  used the random noise
variance from this model as indicative of feature relevance  the two methods achieved comparable
performance to other commonly used selection mechanisms 

 

fi a  left v 

 b  right v 

figure    fmri data feature selection algorithm evaluation 

references
    s  song  z  zhan  z  long  j  zhang  and l  yao  comparative study of svm methods combined
with voxel selection for object category classification on fmri data  plos one  vol     no     p 
e       jan       
    l  grosenick  b  klingenberg  b  knutson  and j  e  taylor  a family of interpretable
multivariate models for regression and classification of whole brain fmri data  most  vol 
       no       pp              online   available  http   arxiv org abs          
    o  yamashita  m  a  sato  t  yoshioka  f  tong  and y  kamitani  sparse estimation automatically selects voxels relevant for the decoding of fmri activity patterns  neuroimage  vol     
no     pp          oct       
    b  krishnapuram  l  carin  m  a  t  figueiredo  and a  j  hartemink  sparse multinomial
logistic regression  fast algorithms and generalization bounds  ieee transactions on pattern
analysis and machine intelligence  vol      no     pp         jun       
    j  friedman  t  hastie  and r  tibshirani  sparse inverse covariance estimation with the
graphical lasso  biostatistics  oxford  england   vol     no     pp         jul       
    c  c  chang and c  j  lin  libsvm  a library for support vector machines  acm transactions on intelligent systems and technology  vol     pp                   software available at
http   www csie ntu edu tw  cjlin libsvm 

 

fi