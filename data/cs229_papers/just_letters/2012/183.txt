learning stochastic inverses
jacob taylor  ashwin siripurapu  andreas stuhlmller
december         

 

abstract

expression  lambda  x     x         it is first necessary
to evaluate  lambda  x     x      then    then    x   
in the environment  x      therefore  the sub traces for
execution trace for  lambda  x     x        will consist
of the execution traces for these   expressions 

for this project  we worked with andreas stuhlmller to
develop and implement a new inference algorithm for the
church programming language  church is a programming
language that extends scheme with random primitives  for
more details  see the original church paper    
andreas developed the basic algorithm and we did most
of the implementation work  also working with andreas to
extend the algorithm 

 

 

sampling traces

our inference engine samples an execution trace  given an
expression  its environment  and its return value  intuitively  it imagines a sequence of evaluations that would
have resulted in the provided return value  for example 
if the expression is     gaussian   gaussian    and the
return value is       then in the sampled execution trace 
perhaps the first  gaussian  expression returned      and
the second returned      
ideally  the sampling distribution q trace result  would
exactly match the posterior p  trace result   however  due
to our approximations  they will not be equal  therefore 
we attach an importance weight to each sample  the imp  trace  result 
  for an ideal
portance weight is equal to
q trace result 
q   p   this weight is always equal to p  result   therefore 
if the variance of the importance weights approaches    this
is a sign that q is approximately correct 
q can either be used as an independent metropolis hastings mcmc proposal distribution  or as an importance
sampling distribution 

execution traces

in church  the evaluation of an expression produces an execution trace  intuitively  the execution trace is the full tree
of evaluations that was necessary to evaluate the expression 
an execution trace consists of the following 
 the expression that was evaluated
 the environment  mapping of variables to values  it
was evaluated in
 the return value
 a list of sub traces
the sub traces item requires explanation  a typical
scheme church interpreter will have a recursive eval function  for example  to evaluate an application like         
it is first necessary to evaluate the expression    then the
expression    then the expression    all of these evaluations
have their own execution traces  which are included in the
execution trace for the expression         
in addition to the function and the argument  sometimes
additional expressions are evaluated during the evaluation
of an application expression  for example  to evaluate the

 

learning the inverse

to create q  we train separate learners for each application
expression  a parenthesized church expression in which a
function is applied to arguments   this learner will take as

 

fiy  x  first we get the k nearest  x  y  pairs to our sample
x  next  we find the distribution produced by selecting a
random y value and wiggling any real numbers contained
in it by a gaussian distribution 
throughout inference  we keep running counts of q and
p  in log space   partway through inference  q will be the
probability of the random decisions that were made through
sampling from the learned distribution  p will be the probability that applications of erps  whose arguments have
been decided produce the results they do 
both x and y may be structured scheme objects  not
only numbers  we decompose each object into its structure
 everything about the object except real numbers contained
in it  and its vector of real numbers 

input the return value of the expression and the values of
any free variables used in the expression  as output  it produces a joint distribution over the non constant items in
the application  possibly including both the function and
the function arguments  for example  consider the expression     gamma      poisson   gaussian     let
us label the expression as     gamma  a     poisson  b
 gaussian  c  d  e  we would train   learners 
   a  d e
   b  c d
to provide training data for these learners  we evaluate
the top level expression and collect execution traces many
times  we will get many a  b  c  d  e tuples  learner  
will learn using  a  d  e  tuples  and learner   will learn
using  b  c  d  tuples 
after training  we can sample an execution trace  pseudocode that implements this sampling algorithm is shown
in algorithm   
suppose we knew that e      then we use learner   to
sample from the joint distribution a  d e  then we feed
the sampled value of d into learner    producing a sample from the joint distribution b  c d  it is easy to verify
that  if the learners learn the correct distributions  then the
distribution over execution traces sampled using this mechanism is equal to the true distribution p  a  b  c  d e  

 

applications

we applied the system to a few problems to determine its
strengths and weaknesses 
as a simple example  we applied the system to inferring object properties from luminance  we defined the reflectance to be a gaussian variable and the illumination to
be a sum of a gamma and gaussian  the luminance is defined to be a product of reflectance and illumination  plus
some noise  inference proceeds in a standard way  use knn
to infer reflectance and illumination from the luminance 
then infer the   variables making up reflectance  after
    training runs  the standard deviation of log importance
weights of sampled execution traces is        which is reasonable  we seem to be running up against the limitations
of the gaussian kernel in representing the true posterior 
another application that we tried was a simple hidden
markov model  taken directly from the wikipedia page on
the topic      our church code is reproduced below 
 define  sequence from state n 
 if    n   
 list  observe state  
 cons  observe state 
 sequence from  transition state 
   n       

figure    a sample execution trace  it can also be interpreted as a bayesian network

  an elementary random primitive  or erp  is a basic random function that can be used to construct other random functions  examples
in church are gaussian  gamma  and flip  the requirements for an
erp are that it is possible to sample a value given the parameters 
and also compute the probability of a value given parameters

for the actual learners  we use a knearest neighbors
algorithm  if we want to learn the conditional distribution
 

fialgorithm   algorithm to sample an execution trace
p  
  the running probability of the trace according to the program 
q 
  the running probability of the trace according to the sampler 
learners    
  a mapping from expression to its learner
run the program forwards multiple times to update learners 
function sampleexecutiontrace expr  env  result 
if expr is deterministic then
trace  evalandgett race expr  env 
verify that trace result   result
return trace
else expr is an application 
distr  learners expr  result  env getf reev ariables expr   
  in reality  only the non deterministic items need to be sampled 
items  sample distr 
q  q  getp robability items  distr 
subtraces    
for i          length items     do
subtrace  sampleexecutiont race expr subexpressions i   env  items i  
append subtrace to subtraces
end for
f unction  items   
arguments  items         length items     
if f unction is a lambda function then
inner  createlambdaenvironment f unction  arguments 
subtrace  sampleexecutiont race f unction body  inner  result 
append subtrace to subtraces
else if f unction is an erp then
p  p  erp p robability f unction  arguments  result 
else f unction is a deterministic primitive 
verify that f unction arguments    result
end if
return m akeexecutiont race expr  env  result  subtraces 
end if
end function

 

fi define  get sequence n 
 sequence from  initial state  n  
the get sequence function samples an initial state  rainy
or sunny  from a given distribution  not shown   and then
generates a sequence of observations from that initial state
by iteratively generating an observation and transitioning
to a new state using a transition probability distribution
 also not shown  n times 
our learner now uses the church inference algorithm
to infer a likely sequence of underlying states for some observed output  in contrast to more traditional methods of
hmm inference  such as the viterbi algorithm  our program
freely uses the structure of the program which encodes the
hmm to generate multiple traces of the model  which are
then used as training data  moreover  whereas the viterbi figure    example of training data for the hmm applialgorithm gives only a single most likely state sequence  our cation  points labeled sunny are colored orange  while
inference algorithm generates an entire probability distribu  points labeled rainy are colored blue  the red line detion over state sequences  conditioned on the observation picts the observation sequence we want to explain 
sequence that we want to explain 
the training points we compute are the output sequences
of observations generated by the various runs of the pro  the inference looks very similar to that of cyk parsing 
gram  and for each point  the training label is the underlying except that  rather than consider every possible parse  it
state sequence that was generated in the corresponding run  guesses the correct parse based on machine learning  the
now the inference algorithm can solve the machine learning system sometimes returns valid parses  but sometimes reproblem of predicting a state sequence for an observation turns invalid parses  this is to be expected  sometimes a
sequence given some training data using the techniques de  production will be selected that cannot produce the given
tailed above  some generated training data is shown below  phrase  or the phrase will be split such that one or both
thirdly  we applied the system to perform parsing for parts cannot be formed by their respective non terminals 
a probabilistic context free grammar  pcfg   here is an usually  for short sentences  the parse will be the unique
excerpt of the model  showing the different productions for correct parse  for example  the system correctly parses
noun phrases 
buffalo buffalo buffalo as  s  np  n buffalo    vp  v
buffalo   np  n buffalo      long sentences are unlikely to be parsed correctly  they often assign words to
parts of speech they cannot come from  causing p to be   

 define  np 
 if  flip     
 list  noun  
 if  flip     
 cons  adjective   np  
 append  np   cons that  vp      

 

problems and solutions

one problem with the algorithm  as presented  is that the
inferred arguments of the function are not necessarily consistent with the return value of the function  for example 
in the expression     gaussian   gaussian    if the return value is    the algorithm could guess that the first
argument is     and the second is      obviously  a trace in

the relevant learning problems are to determine which
production a phrase came from   and to determine how appended phrases are split into sub phrases  the first problem
arises from the  if  flip           calls  the second
arises from the append calls  the cons and list functions
do not require inference because they are fully invertible 
 

fiwhich             evaluates to   is invalid 
initially  we dealt with this problem by modifying the
execution trace so that the return value of the expression is
    instead  once the return values of calls to elementary
random primitives are decided  the consequences of these
decisions are propagated forwards so that the overall trace
is consistent 
unfortunately  this approach makes it difficult to update the counts for p and q  once we decide what the
arguments to a function are  and multiply q accordingly  
these arguments might themselves change through forward
propagation  for example  in the expression  gaussian   
 beta   gamma     once the sum has been decided the
sum might change if it turns out that the sum of  beta  and
 gamma  does not match this decided sum  this means
that it is possible to get the same execution trace through
  different initial decisions of what the sum is  this makes
our estimate of q by repeated multiplication of probabilities
of random decisions invalid 
andreas is currently working on a way to handle the importance weight in this case  in the meantime  it would be
good to ensure that the proposed arguments always match
the return value  so far  this has not been difficult  it requires the notion of a pivot value  a pivot value is a value
that  along with the return value  can be used to recover
the arguments  for example  for the append function  the
pivot value is the length of the first argument  or the splitting point  if we know that  append x y            and
 length x       then we can recover the arguments  x  
      and y        we can have the learner predict the
pivot value  splitting point  rather than directly predicting the values of x and y  a pivot value is also possible
for addition  subtraction  multiplication  and division  in
all cases  the pivot is the first argument  and the second
argument can be recovered as a function of the first argument and the result  different logic applies when one or
more of the arguments is a deterministic expression  but it
is straightforward 
a second problem is that  sometimes  the queried result
is not a possible return value of the expression  for example  perhaps it was earlier assumed that the expression
 list  get word   returned  a b   in this case it is
not possible to assign any sensible values to the  get word 
argument  because  get word  returns a single word and
the queried result consists of two words  as a partial resolution  we reevaluated the expression and changed the

result to equal the result of that evaluation  this does not
resolve the importance weight correctly  so it is always best
to avoid this situation entirely 
a final problem  which appears very difficult to resolve 
is that the performance of the inference depends strongly on
how the program is written  as an example  consider a few
ways to encode an if expression  assuming we have a strict
version of the if combinator  which we call ifstrict  one
translation of  if  cond   then   else   is  ifstrict
 cond   lambda     then    lambda     else    for
this expression  inference proceeds from first guessing which
function   lambda     then   or  lambda     else    was
called based on the return value  then guessing what the
value of  cond  is based on which function was called  this
is definitely not a good inference strategy  because it requires inferring functions  which are possibly closures   on
the other hand  if the expression is translated as   lambda
 c    ifstrict c   lambda     then    lambda     else 
      cond    then inference proceeds by predicting the
value of the condition based on the return value  this inference strategy is superior because it only requires predicting
a single boolean value 
the dependence of the inference on how the program
is written leads to problems with practical applications 
while the system can sample an execution trace  implementing churchs query operation is difficult because it is
hard to specify exactly how to extract the queried value
from the execution trace  if the expression for the observation can be written as  observe  hypothesis    then it
is easy to extract the hypothesis from the execution trace 
however  the program will ordinarily not be written this
way  because this might prevent efficient inference  specifying the program in a way so that inference is efficient and
it is also easy to extract the hypothesis from the execution
trace is difficult 

references
    n  d  goodman  v  k  mansinghka  d  m  roy 
k  bonawitz  and j  b  tenenbaum church  a language for generative models  http   www stanford 
edu  ngoodman papers churchuai   rev  pdf
    wikipedia 
hidden
markov
model 
http 
  en wikipedia org wiki hidden markov model

 

fi