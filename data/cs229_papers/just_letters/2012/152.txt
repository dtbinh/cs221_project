human activity recognition in videos
vignesh ramanathan ankur sarin rishabh goel
stanford university
 rishgoel  vigneshr  asarin   stanford edu

   introduction

   related work

our project deals with the problem of classifying
real world videos by human activity  such videos usually have a large variation in background and camera
motion  this makes the performance of models using low level appearance and motion features unsatisfactory  particularly in the case of video classes sharing similar objects and background  e g  snatch and
clean jerk weightlifting actions  
objects present in a video  and the event label are
mutually related  for instance  the presence of a barbell in a video would help classify it as a weightlifting event  similarly  we would expect to see a barbell in a weightlifting video   thus  recognizing
the object presence and motion in a video should aid
the event classification task  however  this requires an
accurate detection of object tracks  which is a highly
challenging task in itself  works like       use humans in the loop to obtain good quality object tracks 
however this requires significant human effort  we
address this issue by extracting candidate tracks from
a video  and modeling the choice of correct tracks
as latent variables in a latent svm  lsvm       
this formulation enables us to peform action recognition and weakly supervised object tracking in a joint
framework  this leads to a more robust as well as discriminative choice of object tracks for event classification  candidate object tracks are extracted using deformable part based models  dpm      and a tracking
algorithm from       we capture the object appearance
and motion in a video through features extracted from
the object tracks to use in our lsvm model  finally 
we test the performance of our model against different baselines  and show imporvement over the stateof the art method on the olympic sports dataset introduced in     

while a lot of work has been carried out in the field
of action recognition  most of the past works             
focus on using local shape and motion features to
model the video semantics  a few of the methods
have also tried to explicitly model object motion in
the video by extracting coherrent motion tracks and
representing them with the aid of local motion and appearance features               has also explicitly extracted human tracks from videos  however unlike our
approach  they represent the human track with local
features and consider a bag of features collected from
all tracks in a video      uses high level human object trajectories similar to our method  however they
require an explicit annotation of objects in training
videos  in this context  it is to be noted that our method
is weakly supervised  we model the spatio temporal
object paths in a video without requiring human or object annotations in the training videos 

   the model formulation
in this section  we present our method for modeling
object behavior to recognize actions in videos  the
spatio temporal position of between object tubes provides a good description of actions at a high level 
extraction of correct object tubes from a video is a
challenging task in its own right  especially when the
annotations on training videos are not available  our
method allows video recognition while facilitating this
extraction  given a video sequence  we first extract a
set of candidate tubes for each object  we then adopt
a latent svm framework to model the spatio temporal
object motion where the choice of object tubes acts
as latent variables  we elaborate our model formulation in sec      and then describe how to extract candidate tubes in sec     and perform model learning in
 

fistep   
object detection

step    tube extraction

step   
feature
extraction




step   

 
 

 

  

train latent
hog hof svm by
trajectory mining hard
negatives
tube
score

figure    an overview of the iterative training procedure is
shown 

sec     

figure    the trajectory features extracted from an object
tube are illustrated  when we break the tube into    segments 

     modeling object motion
given a video sequence  we have a set of object
tubes o    o         om   that correspond to m different objects  a tube is the result of tracking an object across the video  it is represented by a sequence
of boxes across frames of the video  tube i in frame
t is represented by  xi  t   ai  t    where xi  t  is the
normalized location and size of the corresponding box
and ai  t  is the appearance information of the box 
however  object tracking is difficult on real world
videos  extracting and obtaining reliable tubes itself
is challenging  therefore  instead of assuming that the
object tubes are given  we first extract a set of candidate tubes t for each object  our method then selects
the best tubes o from the candidate set t   further  we
also consider the overall video context through lowlevel space time interest point  stip      features extracted from the video  the recognition score for a
video v is
 m
 
t
x
x
sw  v     max
 i t   oi   t       b  
o t

t  

tube score mean of the object detection scores using     for the boxes along the tube 

     extraction of object tubes
in this section  we discuss the process for extracting candidate tubes from a video for each object  as
shown by fig     we start by running a standard object detector from     on each frame of the video 
the video is then split into   equal segments  from
each segment we pick the frame that has the maximum score  score is defined as the sum of the top  
object detector scores for the frame  where each box
is non overlapping  using these   frames  with   object detections per frame  we initialize a      model to
extract    candidate tubes  note that the method described above can be adjusted to capture finer variation
in tracks 

     model learning
in the training stage  the goal is to jointly learn a set
of optimal feature weights w as well as select the best
tubes o from the candidates t   we are given a set of
training video sequences  v            vn   where each vj
is assigned a class label yj   the learning problem can
be formulated as an optimization problem similar to latent svm      where the latent variables are the choice
of object tubes o   the discriminative cost function is
minimized with respect to classifier weights w as in

i  

   
where b denotes the vector of stip features  t is
the total number of video frames  w        denote
the feature weights corresponding to different components    oi   t  is composed of   sets of features that
capture the change in appearance and motion of the
object over time as given below  we use a   kernel 
histogram of gradient hog  and flow  hof 
captures the change in appearance and local motion
of the object over time 
trajectory captures object motion over time by
binning the flow vector across   directions  the trajectory feature extraction process is illustrated in fig    

n

x
 
min kwk     c 
   sw  vj    yj    
w  

   

j  

where     is a loss function and sw  vj   is specified in
eq   
 

fiwith   baseline methods as well as a control method
explained below 

the minimization problem is semi convex as described in      when the choice of tubes is fixed for
positive examples  the optimization in eq    becomes
convex  when the weight vector w is fixed  eq   can
be solved to identify the best tubes for positive examples  hence  the complete opimimzation problem is
solved iteratively  where at each iteration w and  o  
for positive example are optimized alternatively  assuming the other variables to be fixed  however  since
the search space for object tubes is large  optimization of eq    with fixed tubes for positives is still very
expensive through stochastic sub gradient  ssg  descent  to speed up the calculations  we use hardmining for negatives as explained in      we use the
cvx optimization package     to implement the ssg
algorithm   o   is initialized with tubes corresponding
to the best detection scores 
since the search space for o grows exponentially
with the number of objects  a naive search would be
very time consuming  in order to reduce the computation  we use the method of additive kernels      to
approximate the   kernel in eq   with linear weights 
this reduces the search complexity to be linear in the
number of objects considered 

   bag of words  bow  in this method  low level
hog and hof features extracted from a video are
vector quantized and used in a svm model   
kernel is used 
   niebles et al      this method tries to find the
temporal alignment of different low level action
segments in a video to classify it into an event
class 
   tang et al       a duration hmm model is used
to classify the videos by finding the semantic temporal segments in an event 
   no latent this corresponds to a control setting 
where we present results without a lsvm model 
we use features from the highest scoring tubes in
each video to train and test a svm model 
note that the results for         are taken from      
we observe that results obtained by using the highest scoring tubes performs worse than a simple bow
model  this confirms our intuition that the highest scoring tubes are not necessarily the correct ones 
by treating the choice of object tubes as a latent
variable  we are able to identify the most informative tube in a video  this enables us to perform
a more accurate classification as demonstrated by
the results  further  we outperform the state of theart method on this dataset  our method is seen to
do better by a significant margin for events where
the human motion carries significant information like
high jump  long jump  diving springboard and
javelin throw  however  the performance drops
for pole vault and tennis serve  where the performance of the initial dpm detector is bad and leads to
poor candidate tubes  this can be accounted to significant deformation of the human in these events 
we further show qualitative results in fig     where
the highest scoring tube in a video as well as the tube
chosen by our method are shown for a few test videos
from different event classes  interestingly  we observe
that even in the presence of multiple people in a video 
our method chooses the person performing the relevant action in the event  the ability to pick the most

   experiments
we present results on    events from the olympic
sports dataset for complex event classification  the
dataset contains events involving object motion as well
as plain human actions without objects  the experiments considered as part of the project only extract
human objects from each video 

     olympic sports dataset
the olympic sports dataset contains     sports
videos collected from youtube  we use the same
training and testing splits as used in      for easy
comparison with the previous works  each video segment depicts a single event and is temporally localized 
since the videos are well localized  the results are presented for classification of these videos into different
event classes 

     results
the classification results are presented in tab    
we use average precision as a measure to evalutate the
model for each event class  our results are compared
 

fitable    average precision  ap  values for classification on the olympic dataset  the best performance for each class is
highlighted in bold 

baseline niebles tang et
our method
 bow  et al      al       no latent full model
high jump
                   
      
      
long jump
                           
      
triple jump
                   
      
      
pole vault
                   
      
      
gymnastics vault       
     
     
      
      
shot put
                   
     
      
snatch
                           
      
clean jerk
                           
      
javelin throw
                           
      
hammer throw
                           
      
discuss throw
                           
      
diving platform
                   
      
      
diving springboard                            
      
basketball layup
                           
      
bowling
                           
      
tennis serve
                           
      
mean ap
                           
      
sports class

discriminative human tube in an event class can be accounted to the max margin training of our model 
   

   conclusion
in the current work  we developed a method to perform human activity recognition in videos by modeling the object motion through a lsvm framework 
our method performs weakly supervised object tracking in addition to video event classification by treating
the choice of object tracks from a candidate pool as
the latent variable  we developed an efficient way to
restrict the choice of candidate object tracks by using
a dpm based initialization  results from experiments
demonstre the increase in performance obtained by the
lsvm model over the state of the art event classification scheme  as a next step we wish to quantitatively evaluate the object tracking performed by our
algorithm  we would also further experiment with a
wider range of objects and datasets 

   

   
   

   

   

   

references
    
    i  cvx research  cvx  matlab software for disciplined convex programming  version     beta 
    p  felzenszwalb  r  girshick  d  mcallester  and
d  ramanan  object detection with discriminatively

    

 

trained part based models  transactions on pami 
       september      
n  ikizler cinbis and s  sclaroff  object  scene and
actions  combining multiple features for human action recognition  in eccv       
a  klaser  m  marszaek  and c  schmid  a spatiotemporal descriptor based on  d gradients  in bmvc 
pages               
i  laptev and t  lindeberg  space time interest points 
in iccv       
q  le  w  zou  s  yeung  and a  ng  learning hierarchical spatio temporal features for action recognition
with independent subspace analysis  in cvpr       
j  c  niebles  c  w  chen  and l  fei fei  modeling
temporal structure of simple motion segments for activity classification  in eccv       
a  prest  v  ferrari  and c  schmid  explicit modeling
of human object interactions in realistic videos  technical report       
c  schuldt  i  laptev  and b  caputo  recognizing
human actions a local svm approach  in icpr       
k  tang  l  fei fei  and d  koller  learning latent
temporal structure for complex event detection  in
cvpr       
a  vedaldi and a  zisserman  efficient additive kernels via explicit feature maps  in cvpr       

fifigure    the tubes extracted  human tubes  by our algorithm for different video classes are shown from olympic dataset 
the green boxes indicate the tubes selected after identification of latent variables  the red boxes indicate the tubes selected
based on best detection score  it can be seen that the green tube captures the correct object relevant to the event class even in
the presence of multiple object instances   best viewed in color 
     c  vondrick and d  ramanan  video annotation and
tracking with active learning  in nips       
     h  wang  a  klaser  c  schmid  and c  l  liu  action
recognition by dense trajectories  in cvpr       
     c  n  yu and t  joachims  learning structural svms
with latent variables  in icml       

 

fi