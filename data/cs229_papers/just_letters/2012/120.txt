million song dataset challenge 
fengxuan niu  ming yin  cathy tianjiao zhang 

  introduction 
million song dataset  msd  is a freely available collection of data for one million of
contemporary songs  http   labrosa ee columbia edu millionsong    the million song dataset
challenge  msdc  is a large scale  music recommendation challenge posted in kaggle  where
the task is to predict which songs a user will listen to and make a recommendation list of    
songs to each user  given the users listening history  we    implement both user based and itembased collaborative filtering algorithm to learn the historical data and make recommendations 
and    improve and mix these two models to get a better result  

  data set 
the dataset for msdc is the taste profile subset of msd
 http   labrosa ee columbia edu millionsong tasteprofile   which consists of more than    million
triplets  user  song  play count  gathered from  m users  
the dataset is then split in two     the train set contains a little over a million users  full history
released  available on the msd website      the validation and test sets combined contain    k
users  half of their history released  our recommender system will need to predict unreleased
history  
the data looks like this 
user id
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e
b     d   b ccb    f     f d e  d  dca e

song id
soakimp  a c      
soapdey  a  c   a 
sobbmdr  a c     b
sobfnsp  af  a e  
sobfovm  a  a d   
sobnzdc  a d fc   
sobsuje  a d f cf 
sobvfzr  a d f ae 
sobxalg  a c  c   
sobxhdl  a  c   c 
sobyhaj  a    bf d

count
 
 
 
 
 
 
 
 
 
 
 

the users by songs matrix is very sparse as the fraction of observed non zero entries in the
matrix is only       the full msd dataset contains richer information about songs  such as lyrics
and sound tracks  

  method
in general  there are two types of collaborative filtering  memory based algorithm and modelbased algorithm  memory based algorithms recommend items based on similarity between users
or items  here its songs   user based model make prediction for a certain user based on its
similar users  the intuition is that if user a has similar taste to another user b then a will be
likely to enjoy the songs that b listened  alternatively  item based model make recommendation
by finding similar songs to the songs in certain users listening history  here we implement

fimemory based algorithm to this msdc  
    user based model 
common similarity measures between two users are pearson correlation coefficient and cosinebased similarity  since the data is very sparse  we prefer to use cosine base similarity for this
problem  
           

        
     

     

 

where s is the set of songs  x and y are the listening history whether the user listened song i
before or not  of user x and user y  the score of song    for user    is calculated as  
                                  where           indicates whether user i listened to song k  
memory based algorithms are simple and has little training phrase  but predicting a user history
is very costly  
    item based model
similarly to the user based model  the item based model has expression as follows   
           

        
     

     

 

where u is the set of users  x and y are whether the song x and song y is listened by user i
before or not  respectively  the score of song    for user    is calculated as  
         

                     
   

where           indicates whether user    listened to song       
    local scoring rule
we found that sometimes a popular song can get a higher score for a jazz music fan than a nice
jazz song  which is perfect choice for the user  this kind of situation results from that the final
recommendation for the pop song may be computed by aggregating a large number of low
similarity scores  many people who this pop song dont have the same taste with jazz fan   but
the final recommendation for the jazz song may be calculated by only several large similarity
scores  people who share similar listening preference with the jazz fan   
to avoid the above situation  it is very important to determine how much each individual score
component influences the overall score  we want to drop the very small similarity scores  while
relatively emphasizing the high ones  
therefore  we use a f w  to measure the importance of the individual similarity score  
         
    generalized cosine similarity
instead of using regular cosine similarity  we consider using a general form  
              

        
 
 
   
     
     

fiwhen we calculate recommendation for      we are actually using all the similarity scores of   
with respect to all other users  therefore        term is the same for all the similarity scores  to
emphasize the influence by      we try to decrease the value of parameter     
    mixed model
      mix popular song to the recommendation
for each user  we find that in the recommendation list of     songs the least x  x         
scores are so small that these recommendation may not be meaningful  the user have a very low
probability to listen these low score songs  so it could be better to just recommend the popular
songs to replace the x  songs with the least scores 
      linear combination of user based and item based models
using user based and item based models  we get two different scores of song    for user     
      
and       
  and then get two different recommendation lists of all songs for each user 
     
     
which is ordered by the value of scores  
combining the results of two models  we can probably get a better prediction  our first try is to
combine the two models linearly with weight   and       respectively  as follows  
     
                      
                         
here  due to different scales of
     
     
     
    
    
        and           we first normalize the scores and then combine them linearly to get the new
scores  the     songs with the largest new scores will be chosen to form the new
recommendation list  
      aggregation of user based and item based model
except combining the scores  we try to merge the two different recommendation lists of    
songs from two models  
we first pick up the songs that exist in both user based and item based recommendation lists 
then use x  recommendations from item based list  and    x   from user based list for the rest
of the final recommendation list 
      stochastic combination of user based and item based model
this model is a variation of the previous one  after picking up the songs that exist in both
user based and item based recommendation lists  we randomly choose the rest of final
recommendations from the item based list with probability p and from the user based list wit
probability   p 

  evaluation
in the kaggle competition  the truncated map  mean average precision  is used as the evaluation
metric  the map metric emphasizes the top recommendations  and is commonly used
throughout the information retrieval literature   
     is   if user u listened to item i  and   otherwise  let y denote a ranking over items  where
y j    i means that item i is ranked at position j  for any         the precision at      is the
proportion of correct recommendations within the top k of the predicted ranking 

fi 
         
 

 

    

 

   

for each user  we now take the average precision at each recall point 
         

 
  

 

             

 

   

where    is the smaller between   and the number of user us positively associated songs 
finally  averaging over all m users  we have the mean average precision 
     

 
 

          
 

  result  
due to the large scale of the data and the high time complexity of the algorithm  we have
adopted multiple approaches to promote the efficiency of the experiments with limited
computation resources  first  we used mapreduce to distribute our works onto multiple
machines  each mapper worker keeps a copy of training data in memory with multiple threads
taking testing data and producing recommendations  the reducers compute the ap and then
aggregate them to generate the map evaluation  
benchmark 
recommendation by popularity 

map 
        

 
    user based and item based model 
model 

best map 

parameter for the best result 

user based 

               

q          

item based 

               

q           

    mix popular song to the recommendation 
cutting last p  and substitute with the most popular songs 
p  

map 

  

p  

            

     

            

    

            

     

            

    

           

     

            

    

           

     

            

    

            

     

            

    

            

map 
                  
    
            
                  
    
            
                  
    
            
                  
    
            
                  
  
            
 
 

    linear combination of user based and item based models
     
                      
                         
 
     
     
     

fialpha 

map 

alpha 

map 

  

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

           

     

          

  

    

            

 

            
 

    aggregation of user based and item based model 
x  
  
     
    
     
    
     
    
     
    
     
    

map 
            
            
           
            
            
            
            
           
            
           
            

x  
     
    
     
    
     
    
     
    
     
  
 

map 
          
            
            
            
            
            
            
            
            
            
 

 

    stochastic combination of user based and item based model 
probability 

map 

probability 

map 

  

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

            

     

            

    

            

    

            

     

            

     

            

  

            

    

            

 

 

    conclusion
item based model generally generates better results than user based model  mixing popular
songs into recommendation is not helpful  one possible explanation is that the model itself
already takes account into the popularity  aggregation does not perform well possibly because
the existence in recommendation lists from both models takes too much weight without
considering the scores  stochastic method takes certain portion from each list choosing more
from the preferred item based model also taking top scored ones from user based model  the
linear combination generates the best result               of all  in the kaggle leaderboard  our
map result is in the third place among     teams 

fireferences 
    http   www kaggle com c msdchallenge 
    http   labrosa ee columbia edu millionsong  
    http   en wikipedia org wiki collaborative filtering 
    t  bertin mahieux  d p w  ellis  b  whitman  and p  lamere  the million song dataset  in
ismir       proceedings of the   th international society for music information retrieval
conference  october              miami  florida  pages         university of miami         
    f  aiolli  a preliminary study on a recommender system for the million songs dataset
challenge preference learning  problems and applications in ai  pl      ecai    workshop 
montpellier 

fi