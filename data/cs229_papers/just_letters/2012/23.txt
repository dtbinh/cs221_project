applying reinforcement learning to competitive tetris
max bodoia  mbodoia stanford edu 
     sand hill rd
palo alto  ca      

arjun puranik  apuranik stanford edu 
     sand hill rd
palo alto  ca      

introduction
for our project  we attempt to apply reinforcement learning
to the game of tetris  the game is played on a board with
   rows and    columns  and each turn a player drops one of
seven pieces  the seven tetrominoes  onto the field  points are
gained by dropping pieces in such a way that all ten squares in
some line  row  are filled  at which point the line is cleared 
multiple lines may be cleared with a single piece drop and
will result in more points being earned  a player loses if the
height of the pieces on his board exceeds the height of the
board itself 
the fully observable nature of the tetris board and the simple probabilistic transitions from state to state  i e  adding a
randomly selected piece to the end of the piece queue each
turn  naturally suggest the use of reinforcement learning for
tetris  specifically  tetris can be modeled as a markov decision process  however  the state space of tetris is extremely
large   the number of ways to fill in a        board is       
and the tetris requirement that no row be completely filled
only reduces this to              as a result  the complete
mdp for tetris is entirely intractable  in our paper  we will
outline the different approaches we took to dealing with this
intractability  we begin by describing our initial  unsuccessful attempts  and commenting on the reasons why they may
have failed  next  we describe the approach that eventually suceeded   fitted value iteration   and give a detailed
analysis of its results  finally  we conclude by considering
the strengths and weaknesses of our implementation of fitted
value iteration  and highlight other potential approaches that
we did not try 

initial attempts
in this section we will outline our early approaches to the
problem  in each case we describe the motivations for the approach  the extent to which it was successful  and the reasons
why it was ultimately unsuccessful 

block drop
as an initial pass at the problem of tetris  we chose to implement a simple game which we will call block drop  block
drop is comparable to tetris in that players drop pieces onto
a rectangular grid  and gain points for filling up an entire row
 clearing a line   the main difference is that in block drop 
the only type of piece available to players is a single unit
square  although the relationships between states in block

drop are much more simple than in tetris  and thus the optimal policy is much easier to learn   the size of the state
space for block drop and tetris are similar  in this sense 
the tractability of block drop can be used as a rough lower
bound on the tractability of tetris 
we wrote a program to formulate an m  n game of block
drop as an mdp  note that a valid block drop gameboard
never has any holes  where a certain grid square is not filled
but a higher up square in the same column is filled   this allows us to represent each state by a set of n values corresponding to the height of the pieces in each of the n columns  these
column heights can take on values from   to m    independently of each other  columns heights are bounded by m   
since the game ends if a column reaches height m   we also
added a single terminal state to represent losing the game 
so the total state space of block drop is mn      one possible action exists for each column  since the player chooses
which column to drop a piece in  so there are a total of n actions  transition probabilities are completely deterministic dropping a piece in a particular column either increments that
columns value by    or decrements the value of every column
by    in the case when a line is cleared   our reward function
assigns a reward of   for clearing a line and a cost of m for
losing the game  all basic moves had   cost 
in order to solve the block drop mdp  we used the zmdp
package written by trey smith  smith   simmons        
this package is designed primarily for finding  approximate 
solutions to partially observable mdps  and as a result it requires that the input problem be described as a pomdp rather
than an mdp  unfortunately  we were unable to find any other
software that works for pure mdps  so we decided to convert our problem into a pomdp by adding a single observation that is always made  we assumed that this modification
would not affect the tractability of the problem significantly 
we found that the solution to block drop matched the intuitively obvious optimal policy  and provided a good sanity
check for our use of mdps to model the problem  the resulting policy drops a single block into each column  from left to
right  until a line is cleared  at which point it repeats  however  the block drop problem also illustrated the tractability
issues that we faced       block drop took less than a second to solve  but      block drop took    seconds and     
block drop took over a minute  all sizes above      were
unable to complete a single solver round  figure   shows the
time in seconds required for a solution to block drop as a

fifunction of the board size  in terms of total number of grid
squares   these results made it clear that the actual game
of tetris would be intractable without significant approximations  our next task was to decide how to make these approximations 

figure    solution times for block drop

no holes tetris
in tetris  unlike in block drop  we cannot represent the states
of the board by a single height for each column  it is possible that holes   where a column has an empty block below
the highest filled block   are formed in the board when pieces
are dropped  in fact  the management of these holes is one of
the more difficult aspects of the game  however  we decided
to approximate board states in this manner by emulating the
state representations for block drop and tracking only the
height of the highest block in each column  we refer to the
version of tetris that results from this approximation as noholes tetris  the number of possible boards for no holes
tetris is significantly smaller than for tetris  mn compared
to around  mn   and was the main motivation for making this
approximation  we attempted to counterbalance the inaccuracy of this simplification by also modifying the reward function of no holes tetris  so that the player loses points upon
taking actions which produce holes  this causes the optimal policy for no holes tetris to avoid actions that lead it
to states that differ from the corresponding true tetris state 
furthermore  we observed that competitive tetris players almost never leave holes in their stacks  as a result we hoped
that the optimal policy for our approximate no holes tetris
would remain close to the optimal policy of true tetris 
we then wrote a program to formulate an m  n game of
no holes tetris as an mdp  as noted above  each board of
no holes tetris is represented by a vector of n values between   and m     representing the heights of the blocks in
each column  in addition  each state of no holes tetris also
includes the next piece available to the player  for p possible pieces  this gives a total state space of p  mn      including the terminal state   the set of actions available to
the player at each state is the set of ways  including rotations 
that the given piece can be dropped onto the board  transition

probabilities are deterministic to the extent that they affect the
board and uniformly random to the extent that they affect the
piece  so each state action pair leads uniformly to one of p
possible next states  the reward function assigns a cost of
m to losing the game  and the reward for a given state action
pair is the number of lines cleared minus the number of holes
created 
we found  unsurprisingly  that no holes tetris is significantly more difficult to solve than block drop       noholes tetris took around    seconds to solve  while on larger
boards our solver was unable to find good policies  the best
policies found had regret values greater than     on a     
board and greater than   on a      board  even when the
solver is allowed to run for long periods  we tried varying
the number of pieces to reduce complexity and found that the
difficulty of the problem scaled extremely quickly with the
number of pieces  no holes tetris with only one piece could
be solved about as quickly as block drop  but attempts to
solve      no holes tetris with two pieces were unable to
reduce regret below    figure   shows the solution times of
no holes tetris as a function of board size when different
total numbers of pieces are used 

figure    solution times for no holes tetris
these results were disappointing  to say the least  we
knew from our analysis of block drop that solving no holes
tetris would be intractable for board sizes above       however  our hope was that we could find a good enough policy for the full   piece no holes tetris on a large enough
board  this policy would serve as a starting point for local
piece placement that we could use to build policies for larger
boards  we planned to do this by dividing larger boards into
smaller sub boards and choosing actions based on some function of the sub board policies  unfortunately  no holes tetris
proved to be too computationally difficult to find suitable subboard policies  on the one hand  the      board was small
enough that the outputted policy for no holes tetris was determined primarily by height constraints  and did not generalize well to larger boards  on the other hand  the best     
policy performed poorly even on the      board  this left us
with no good starting point from which to construct a policy

fifor larger boards  and no obvious way of applying our results
effectively to the general problem of playing tetris 

factored mdps
we briefly considered representing tetris  or no holes
tetris  as a factored mdp  some of the existing literature on
mdp and pomdp solution algorithms explores ways of optimizing the solution process by representing the state space
in factored form  guestrin  koller  parr    venkataraman 
        poupart         that is  we would represent each
state in terms of some finite number of state variables  and
describe the transition probabilities and rewards as functions
of these variables instead of defining a particular value for
every state  tetris  like most games  is highly factorable and
therefore well suited to this kind of approach  possible factorizations of an m  n board include using a binary variable for
each of the mn grid squares  which encodes the full version
of tetris   and representing each of the n columns as a variable that takes m possible values  which encodes no holes
tetris  
ultimately  however  we decided not to pursue this approach  the benefits of representing mdps in factored form
come primarily from the ability to represent particular transition probabilities and rewards as functions of strict subsets
of the full set of state variables  in other words  in order for
a factored mdp to be more efficiently solvable than its unfactored counterpart  the state variables of the factored form
must be independent to some extent  at first  no holes tetris
seems to exhibit a great deal of independence  each time a
piece is dropped  at most four state variables  i e  columns 
will change in value  and these changes will depend only on
the values of the other three state variables  however  the fact
that state transitions must also take into account line clearances eliminates this independence  line clearances only occur when a particular row is filled in at every column  and as
a result  the transition probabilities for each state variable depend on the value of all n state variables  for this reason  we
judged that the use of a factored representation would not provide significant reductions in tractability and decided against
this approach 

fitted value iteration
the next approach that we tried  and the one that ultimately
proved most successful  was fitted value iteration  the basic principle behind fitted value iteration is to choose a small
set of features and represent each state in terms of this feature set  in this respect  it is reminiscent of the factoring approach  importantly  however  the state space of a factored
mdp is necessarily the same size as the original state space 
while the set of possible combinations of feature values may
be much smaller  furthermore  fitted value iteration does not
consider this full set of possible feature values  but instead restricts itself to the feature representations of a sampled set of
states  this means that the tractability of fitted value iteration
depends on the number of samples rather than the size of the
state space 

in general  two basic properties must hold for a particular
mdp in order for fitted value iteration to be effective  first 
it must be possible to approximate the true value of a particular state using only a small set of information about that
state  this ensures that the featurization of the mdp is reasonably representative of the original  second  the number
of samples from the original state space needed to represent
the relationship between state values and state features must
be relatively small  this ensures that an accurate function
from features to values can be learned using the sampled set
of states  intuitively  tetris seems to satisfy both properties 
so the application of fitted value iteration is a natural choice 
our algorithm is roughly identical to the fitted value iteration algorithm presented at the end of the reinforcement
learning handout  with a few small caveats  it begins by sampling a set of states ss from the state space s and initializing
a parameter vector  to zero  then  it alternates between two
steps  in the first step it calculates  for each state  the maximum over all actions of the expected utility of being in that
state 
ys   maxaa r s    es  psa  v  s     for each s  ss  
in the second step  it fits the values of the parameters to this
set of maximum expected utilities 
    arg min    sss  t  s   ys     
here  a is the set of all actions  r s  is the reward received
from being in state s  psa is the distribution over possible transition states resulting from taking action a in state s  and  s 
is the vector of features values for state s 
the main difference between our algorithm and the one
presented in the handout is that instead of sampling from the
state space as a whole  we take samples by randomly placing
pieces around the board  this seems reasonable  since a truly
random tetris board is unlikely to look anything like the kinds
of boards found in tetris gameplay  and the set of states that
can be generated using this sampling process is identical to
the set of states it is possible for our agent to encounter during
test time  the other difference is that because we already
know the transition probabilites in tetris  we can compute
r s    es  psa  v  s     directly instead of having to sample it 
once the algorithm converges  the parameter settings can
be used by an agent to play tetris  the agent chooses moves
in a manner similar to the first step of the algorithm  when in
state s  it chooses action a  according to 
a    arg maxaa es  psa  v  s     

feature selection
the fitted value iteration algorithm given above assumes the
existence of a function  from states to feature values  a full
implementation therefore requires the choice of a feature set
and the definition of   note that  can be any conceivable
function    s  rk for some k  n  where k is the number
of features  this means that the feature space of possible
functions  is both enormous and difficult to define  although
techniques exist for automating the process of feature selection  it is far more common to define features manually using

fidomain knowledge  hall        
we considered a variety of possible features based on our
personal experience playing tetris  the first kind of features
we considered were summary features that are functions of
the whole board  the most straightforward summary features
we examined were max height  the maximum height of all
the columns  and num holes  the total number of holes on
the board  where a hole is an unfilled grid square for which
there exists a filled grid square higher up in the same column   other more complicated features included  avg diff 
the average of the absolute values of differences between adjacent columns  max diff  the maximum of these absolute
values  and num covers  the total number of covers on the
board  where a cover is a filled grid square for which there
exists an unfiled grid square lower down in the same column  
for each proposed feature  we conducted preliminary tests of
the feature by running our algorithm using only this feature
and a constant feature with value   for every state  we then
measured the average lifespan  i e  number of moves made
before losing  for an agent that plays using the learned parameters and compared it to the average lifespan of an agent
that places pieces randomly  only the features max height 
num holes  and num covers led to better than random performance 
in addition to the summary features  we also considered
sets of non summary features that were functions of particular columns  these feature sets corresponded to different
summary features  examples include i col height  the height
of the ith column  and i col diff  the absolute value of the
difference between the ith and  i    th columns  however 
when we tested each of these potential feature sets individually  we found that an agent playing with the learned parameters performed no better than random play  this is most likely
because the relationships between these feature sets and the
true value function are non linear  since our algorithm finds
parameter settings using linear regression  it does not have
the potential to learn non linear relationships well and typically ends up converging to arbitrary parameter values  in
theory  the parameter update step could use a wide variety of
machine learning algorithms  and using a more complex regression algorithm could allow it to learn more intricate relationships between the features and the state values  without
such modifications  however  our algorithm cannot perform
well using the non summary feature sets considered and so
we ultimately chose not to consider them further 

results
after this preliminary testing  we were left with three features
  max height  num holes  and num covers   that seemed
promising  we then conducted a series of more rigorous tests
to determine which combinations of features led to the best
performance  for each possible combination of these three
features  we ran our algorithm to convergence   times and
tested an agent for each of the resulting parameter vectors 
the algorithm used      sampled states and a discount factor
of     on a full           piece board  and iterated until the

maximum difference between corresponding parameter values from one iteration to the next dropped below       in testing  each agent played     games and the average number of
moves per game was computed  table   shows the average of
the parameter vectors and average lifespans over all   agents
for each possible feature set  the first value in each parameter vector is the value of the constant feature  followed by the
values of the parameters corresponding to the other features
in the order listed 
table    average agent lifespans and parameter vectors
feature set
constant
max height
num holes
num covers
mh  nh
mh  nc
nh  nc
mh  nh  nc

avg lifespan
    
    
    
    
     
    
    
     

parameter vector
       
            
            
            
                 
                  
                   
                         

max height seems to be the most informative feature since
it gives the best single feature performance and the two feature pairs that use it perform better than the third  num holes
likely comes in second given the stark difference between
performance with max height  num holes and with maxheight  num covers  num covers has the worst individual
performance and it makes a difference only when combined
with max height  so it seems to be the least informative of
the three  these rankings seem to make intuitive sense since
looking at the maximum column height and total number of
holes on a tetris players board is one of the most obvious
ways to judge whether they are doing well 
on the other hand  however  the differences in performance
given by these three features may not be due to inherent differences in their informativeness about the true value function  but rather a product of the random sampling process we
used  in general  random play clears lines rarely but loses often  and thus the set of sampled states contains many more
examples of terminal states than of line clearances  since
max height is especially indicative of whether the player is
near a terminal state  while the other two features relate more
to how easily lines can be cleared  it is possible that the correct value of max height is simply the easiest to learn from
random play  to test this hypothesis  we conducted another
set of trials for the three feature set using a modified version
of the algorithm with multiple training rounds  in the first
round  we learn parameter settings for the three features using a set of states sampled from random play  in subsequent
rounds  the set of states is sampled from play using the parameter settings from the previous round  with a probability
of     to choose a random action  however  this algorithm
showed no significant change in parameters from round to
round  even though subsequent rounds reinitialized the pa 

firameters to   after collecting the samples   this suggests that
the parameter settings for the three feature set given in table
  are likely the optimal values achievable by our algorithm 
the last method we tried for improving the effectiveness
of our agent was to allow it to preview the upcoming two
pieces and add a   step look ahead feature  most versions of
tetris allow players to preview several upcoming pieces  so
it seemed fair to allow our agent the same privilege  during
training  our agent learns a parameter vector for the threefeature set as it did before  in gameplay  however  it maximizes the expected value of the next state over all possible
combinations of actions with the current piece and the two
previewed pieces  in other words  if psa   s  a   s  a  is the transition distribution over next states after taking actions a    a   
and a  in s    s    and s  respectively  the agent chooses an action a according to 
a   arg maxa  a r s      r s      es  ps  a   s  a   s  a   v  s     
we found that this   step lookahead agent was a highly
effective tetris player   over the course of several thousand
moves  it never lost once and rarely entered a state with a
maximum column height above half of the total board height 
to ensure that this performance was influenced by our parameter learning  as opposed to merely the lookahead   we tested
lookahead agents that had no learned parameters and simply
chose actions based on the maximum reward attained during lookahead  in this case     and   step lookahead agents
performed no better than random play and agents with larger
lookaheads took dozens of seconds to choose moves  these
results demonstrate how agents that utilize both low order
lookahead and parameter learning can significantly outperform agents which use only one or the other 

conclusions
we began this project with the hopes of creating an agent capable of playing competitive tetris against a human player 
very quickly on  however  we realized that we had underestimated the difficulty of basic  single player tetris  the beast
of exponential size reared its ugly head  and it became clear
that we could not simply encode the game as an mdp and
expect it to be tractably solvable  even reducing the board
size by a factor of eight and ignoring holes was not enough to
allow for the computation of an optimal policy  and the tricky
nature of line clearances prevented us from taking advantage
of independence between factored state variables 
this forced us to change tactics and use a less straightforward approach to find an approximate solution to the mdp 
the fitted value iteration algorithm was a natural choice  since
tetris boards are well characterized by a small set of features
that can be learned with relatively few sampled states  after some experimentation  we found three features   maxheight  num holes  and num covers   that seemed to be
roughly linearly correlated with the true value of tetris
states  an agent that learned parameter values corresponding to these features was able to survive for an average of
around     moves  this was not a trivial feat  a random

agent only survives for an average of    moves  and without
the use of previewed pieces   a staple of almost all versions
of tetris   humans without plenty of experience would likely
not perform much better  furthermore  the addition of two
previewed pieces and a   step lookahead allowed our agent
to stay alive indefinitely 
this final instantiation of our agent is particularly interesting because of the way it combines fitted value iteration and
simple search and ends up performing better than with either
technique individually  in this case  the paradigm of uniting
machine learning and conventional algorithms proved to be
extremely effective  future versions of our agent could likely
be made even more effective if we incorporated other standalone techniques as well  the linear regression step could
easily be replaced by a more advanced regression technique
that would allow our agent to learn non linear relationships
between the features and state values  and automated techniques for feature selection might be able to find relationships
that are not readily apparent to human players  nevertheless  our final agent is a reasonably effective tetris player as
it stands  and could provide a solid foundation if we wished
to extend our agent to the competitive tetris arena 

references
guestrin  c   koller  d   parr  r     venkataraman  s         
efficient solution algorithms for factored mdps  j  artif 
intell  res   jair              
hall  m          correlation based feature selection for
machine learning  unpublished doctoral dissertation  the
university of waikato 
poupart  p          exploiting structure to efficiently
solve large scale partially observable markov decision processes  unpublished doctoral dissertation  citeseer 
smith  t     simmons  r          point based pomdp algorithms  improved analysis and implementation  arxiv
preprint arxiv           

fi