who  got  style   
learning  to  recognize  literary  styles  
  
cs       final  project  
fall  quarter        
  
konstantinos  balafas  
simon  calvet  
  

introduction  
  
most   well   known   authors   have   a   distinct   writing   style   that   renders   their   work  
unique    for   the   erudite   reader    that   style   is   quite   obvious   and   it   is   not   hard   to   tell  
between   jane   austen   and   charles   dickens   or   leo   tolstoy    to   name   a   few    that  
distinction   however   requires  skill  and  knowledge  that  can  take  a  lifetime  to  attain   
for   that   reason    the   application   of   machine   learning   algorithms   in   author  
identification  is  a  problem  that  has  garnered  a  lot  of  research  interest   our  objective  
for   this   project   is   to   apply   some   of   the   algorithms   that   we   learned   in   class   on   the  
problem   of   author   identification    more   specifically    we   will   pose   the   following   two  
problems   
 select  a  book  and  choose  between  the  actual  author  and  another   
 select   a   book   and   identify   the   author   from   a   larger   pool    containing   the  
actual  author   

dataset  
  
our   dataset   consists   of         books   from        different   authors    the   books   were  
downloaded   in    txt   format   from   the   project   gutenberg   website  
 www gutenberg org     while   the   books   and   the   authors    as   well   as   their   numbers   
were   selected   arbitrarily    we   tried   to   keep   most   authors   between       and        books   
once  the  books  were  selected  and  downloaded   they  were  compiled  into  forms  that  
could  be  used  in  our  algorithms   each  book  was  compiled  firstly  into  a  vector  of  all  
the   words   appearing   in   the   book   and   their   occurrences    secondly    books   were  
compiled  into  vectors  of  the  frequency  of  each  word  and  sentence  size   

word  occurrence  approach  
  
a  fairly  obvious  measure  of  an  authors  style  is  the  words  that  they  use   as  such   our  
first   idea   was   to   train   an   algorithm   on   the   occurrences   of   words   in   a   book    as  
expected   this  led  to  input  vectors  of  very  large  size   since  every  word  that  appeared  
at  least  once  in  any  of  the  books  in  our  dataset  had  to  be  accounted  for   naturally   

fithis   meant   that   the   algorithms   that   we   would   have   to   run   would   be   very  
computationally   expensive    in   order   to   remedy   that    we   tried   using   a   stemming  
algorithm  that  would  reduce  the  size  of  our  input  vectors   this  makes  sense  from  a  
linguistic   point   of   view   as   well    our   first   approach    with   a   full   dictionary  
differentiates  between  the  occurrences  of  different  forms  of  the  same  word   in  other  
words    the   full   dictionary   will   consider   do   and   does   as   completely   different  
words    assuming   that   each   one   influences   the   style   of   the   author   to   a   different  
extent    something   that   is   obviously   not   true    a   stemming   algorithm   would   lump  
these   words   together    providing   more   insight   on   the   style   of   an   author    after   a   brief  
online  
search   
the  
porter  
stemming  
algorithm  
 http   tartarus org martin porterstemmer    was  used   furthermore   clearly  being  
in  a  supervised  learning  setting   we  tried  nave  bayes  and  logistic  regression  as  a  
first  approach   

nave  bayes  
  
a  nave  bayes  was  implemented  and  tested  for  two  different  problems     
 the  first  will  be  called  binary   classification   selecting  two  authors  and  a  book  
that  was  written  by  one  of  the  two   we  want  to  predict  the  actual  writer   it  was  
tested   for   a   few   author   couple    using   leave   one   out   crossed   classification  
 loocv    and  each  time  had      error     
 the  second  will  be  called  one   versus   all   and  is  a  way  to  apply  nave  bayes  to  
multivariate  prediction   when  a  prediction  has  to  be  done   the  algorithm  is  first  
trained  on  all  authors   labeling     if  the  author  considered  wrote  the  book  and     if  
not   when  testing  on  a  given  book   the  author  selected  is  the  one  that  gives  the  
highest   probability   of   being   written   by   the   considered   author    this   approach  
surprisingly  gave  good  results   using  loocv   the  error  obtained  was        
  

logistic  regression  
  
as   one   of   the   simpler   supervised   learning   algorithms    we   tried   using   logistic  
regression   for   our   problem    at   first    we   used   logistic   regression   for   binary  
classification   using  loocv  and  all  possible  permutations  of  author  pairs   the  final  
test  error  was        furthermore   the  error  was  not  significantly  affected  when  the  
reduced  dictionary  that  was  obtained  from  the  stemming  algorithm  was  used   using  
the   reduced   dictionary   did    however    greatly   reduce   the   time   that   the   algorithm  
needed  to  run   
in  trying  to  select  the  author  of  a  book  from  the  entire  set  of  authors   the  following  
procedure  was  followed   
a   logistic   regression   algorithm   was   trained   for   each   author   on   the   entire   dataset  
with   the   exception   of   one   book    the   label       was   the   case   when   the   book   did   not  
belong   to   the   author   and   the   label       was   the   case   when   the   book   belonged   to   the  
author   the  quantity  h x   was  calculated  for  each  author  and  the  predicted  author  
was  selected  as  the  one  with  the  largest  h x    firstly   the  input  vector  contained  the  
occurrences   of   words    due   to   the   fact   that   there   were   some   words    such   as   a   or  

fithe    had   a   much   larger   number   of   occurrences   than   others   and   that   the   zero label  
training  examples  were  a  lot  more   x  was  a  very  large  number  and  the  exponential  
of   it   tended   to   infinity   and   the   algorithm   failed    then    the   occurrences   were  
normalized   by   the   number   of   words   in   the   book    essentially   representing  
frequencies    even  in  that  case   this  algorithm  had        test  error   
  

word sentence  length  approach  
  

word length frequencies in emma  austen 

sentence length frequencies in emma  austen 

    

    
compiled frequencies
lognormal interpolation
    

    

    

   

    

frequency

   

    

    

   

    

    

    

 

 

 

  

 

  

 

  

  

word length  in caracters 

figure     

  
  
  
our   language   is   made   of   words   that   are  
characters  combinations   under  certain  
grammatical   rules    words   are   also  
combined   to   make   sentences    when   we  
write    we   have   a   personal   way   to  
combine  words  to  make  sentences  that  
will  be  long  or  short  depending  on  our  
style    for   instance    prousts   style   is  
recognizable   because   of   his   usage   of  
very   long   sentences    to   some   extend   
words   length   usage   might   also   be   an  
indication   of   some   writers   literary  
style     
instead   of   building   a   vocabulary   for  
our   dataset   and   then   count   for   each  

  
sentence length

  

   

   

figure     
lognormal distribution on phrase length
 
austen
dickens
dostoievsky
doyle
dumas
goethe
london

   

   

standard deviation

frequencies

compiled frequencies
lognormal interpolated frequencies

   

   

 

   

   

   
   

 

   

   

   
mean

figure     

   

 

   

fibook  the  words  occurrences   frequencies  for  each  word  and  sentence  lengths  were  
computed  for  each  book   and  each  book  was  then  represented  by  two  vectors     
 one  containing  the  word  length  frequencies   with  the  ith  component  being  the  
frequency  of  the  words  of  i  characters  
 the   other   containing   the   sentences   length   frequencies    with   the   ith  
component  being  the  frequency  of  the  sentences  made  of  i  words  
this  approach  has  a  double  advantage   first  it  reduces  considerably  the  size  of  the  
data    word   lengths   frequencies   are   null   for   sizes   higher   than        characters    and  
sentence  lengths  frequencies  are  usually  null  for  length  of  more  than       words   but  
it   also   makes   it   possible   to   actually   visualize   the   data    figure       and   figure       show  
the  frequency  distributions  for  respectively  word  lengths  and  sentences  lengths  for  
austens  novel  emma   
but   we   can   go   further   and   notice   that   both   distributions   look   like   they   are   log 
normally   distributed    and   since   our   vectors   are   discrete    it   is   very   easy   to   get   the  
mean   and   the   variance    the   only   two   parameters   that   fully   describe   a   given  
distribution    if   we   note   x i    the   frequency   of   the   word   or   sentence   of   length   i    we  
have   
 

log          

log 

 

 

     

  
this  allows  us  now  to  represent  a  book  in     dimensions     
  
 book       word   word   sentence   sentence   

test error

figure       shows   a   reduced   form   of   the   data   in       dimensions   for   a   reduced   set   of  
authors    only       of   them     using   the   mean   and   variance   for   phrase   lengths    this  
shows  that  using  this  criteria   some  author  can  be  distinguished   austen  and  london  
for   instance    whereas   some   couple   of   authors   cannot   be   separated   using   linear  
regression  or  some  clustering  algorithm   london  and  doyle  for  instance   
nave bayes   and   logistic   regression   were   tried   on   both   frequency   vectors   and  
interpolated   parameters   vectors    word   
 
word    sentence    sentence     for   each   existing  
author   couple    loocv   was   used   and   the      
error  incremented  when  the  algorithm  was      
making  an  incorrect  prediction     
   
for  nave  bayes   surprisingly   the  reduced  
   
  component  vector  test  gave  a  lower  
   
error        compared  to       for  the  
   
frequency  vectors   also   the  error  for  each  
   
couple  was  calculated  and  was  highly  
fluctuating   from       to         depending  
   
on  the  couple  considered   that  confirms  the      
visual  intuition  we  got  looking  at  figure        
 
 
  
  
  
  
  
  
  
  
for  logistic  regression   the  one  versus  all  
author pair id
approach  gives  surprisingly  better  results  
figure     
than  the  word  occurrence  approach   the  binary  classification  

  

   

fiperformance  strongly  depends  on  the  pair  of  author  considered  though   as  it  is  
shown  on  figure      
  

conclusion  and  suggestions  for  further  work  
  
the  errors  for  each  algorithm  and  set  of  data  used  are  displayed  below   
  
  
data  used  

logistic  regression  

word  occurrence  
word  occurrence  with  
stemming  
word sentence  
distribution  
 book       
   
   

     
     

logistic  
regression  
multivariate  
      
      

     

     

     

     

word



sentence

   

word

sentence

   

nave 
bayes  

nave bayes  
multivariate  

    
    

     
     

         
     ave   
         
     ave   

      
     

  

  
nave  bayes  for  word  occurrences  works  very  well  to  recognize  one  author  in  a  pool  
of   two    it   works   well   also   when   trying   to   recognize   an   author   in   a   larger   pool   
running   the   algorithm   takes   a   lot   of   time   though   since   matrices   of   size   of   about  
         components   are   manipulated    also    the   other   attempt   to   make   shorter  
vectors   showed   that   phrase   lengths   and   word   lengths   frequencies   were   of   some  
significance  to  characterize  one  authors  style   however   the  error  found  is  very  high  
on  average   some  authors  may  be  distinguished  with  these  parameters  but  it  cannot  
be   applied   universally   to   all   authors    in   particular    the          error   for   the   one  
versus   all   nave   bayes   can   be   explained   by   the   way   the   algorithm   works    since   it  
choses   between   the        authors    also    surprisingly    one versus all   logistic   regression  
performs   better   using   the   frequencies   or   interpolation   parameters   than   when   using  
word  occurrences   
  
since   input   data   was   entire   books   here    it   appeared   early   in   our   work   that   the  
information   that   we   chose   to   extract   from   them   was   of   high   importance   for   the  
performance  of  our  algorithms   we  chose  to  work  with  relatively  simple  algorithm   
logistic   regression   and   nave   bayes    to   focus   on   the   data   extracted   and   its   relevance  
to  the  style  recognition  problem   finding  the  relevant  data  is  key  here   and  should  
we  have  more  time   we  would  spend  it  on  this  key  issue   
  
  

fi