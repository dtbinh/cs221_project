text detection and recognition in natural images
steven bell
stanford university
sebell stanford edu

abstract
natural scenes pose a major challenge to traditional
optical character recognition methods because they often
contain noise  occlusions  distortions  or relatively small
amounts of highly styled text  in this work  we build a
probabilistic system which unifies the tasks of text detection
and recognition with a language model  we use an efficient
multi scale character detector to locate characters within
an image without performing segmentation  this is followed by a graph based search which groups the detections
into words and evaluates their relative probabilities  avoiding binary decisions except where computationally necessary 

with traditional ocr methods  by forcing binary decisions
at several points  this approach forfeits information which
could be used to produce a more accurate result 
this project uses a probabilistic framework where image regions are assigned probabilities of containing different characters  and the result is read with the help of language characteristics  specifically the co appearance of letters 
to limit the scope of the project  we considered only
horizontal strings of upright lowercase letters  we worked
primarily on synthetic images with automatically generated
ground truth  which reduced the time spent creating training
data 

   prior work
   introduction
the modern world is filled with text  which humans use
effortlessly to identify objects  navigate  and make decisions  although the problem of reading text with a computer has been largely solved within limited domains  such
as identifying license plate numbers  digitizing books and
documents  or reading handwritten mailing addresses  the
more general question of how to detect and read text in natural  unstructured images remains an open challenge 
the applications for such a system are numerous  for
example  buildings photographed in googles street view
could be automatically tagged with their address if a computer could detect and read the building numbers  likewise 
robots could be made to more intelligently act in a world annotated for humans  and translation systems for blind persons or foreign travelers would be improved 
several factors make reading text in natural scenes particularly difficult  in unstructured scenes  such as a photograph of a storefront  clutter dominates the image  text
regions are of varying size and orientation  scattered  and
possibly occluded  letters may appear in multiple fonts and
some letters may be ambiguous based on their shape alone 
many approaches to this problem treat the recognition
problem as two separate tasks  first locating text and drawing a bounding box  and then binarizing and reading it

optical character recognition for printed documents and
handwritten digits has a long history  many methods have
been proposed and used successfully  but most of these assume that the input image has been cleanly binarized and
that characters can be segmented easily  which is rarely the
case in unstructured images  because of the complexity of
the input images  text recognition in natural scenes is more
closely related to object detection than it is to traditional
ocr 
a large body of past work has focused purely on the
challenge of locating text within scenes  spurred primarily
by the icdar text detection challenges of      and      
these works can be roughly categorized into connectedcomponent based methods  which segment the image and
attempt to pick out which blobs are characters  and patchbased methods  which use convolutional filters  image patch
correlations  wavelets  or other features to label the probability that an image patch contains text        
another domain has focused on the task of reading scene
text given a bounding box  centered around the icdar robust reading challenge  weinman et al  use a bayesian
model to unify text segmentation and reading  and show
that reading accuracy can be improved by incorporating additional context such as language characteristics and the visual similarity of letters     
more recently  several groups have created end to end

fidetection and recognition systems  these use a variety of
features for detecting and classifying characters  including
gabor filters  and connected component dimensions and
densities      a support vector machine or linear discriminant analysis is used to perform classification based on
the features  this work is an extension of these  where
higher level information is used to aid text detection  not
just recognition 

written
v  v   

n
x

p di  w v  yi  

i  

where v is the pixel height  xi and yi are the detection position  p d  is the probability of a detection d  i e  the result of
logistic regression   and w is a window function weighted
toward the center of the detection  we currently use a hamming window  but a triangular or gaussian window would
also be appropriate 
an example of a detection and the corresponding line is
shown in figure   

   method
     character detection

 

the core of our algorithm is a multi scale histogram of
oriented gradients  hog  detector  the hog descriptor is
a patch based algorithm which provides a dense estimate of
shape over an entire image  using histograms of quantized
gradient orientations for small image patches  it was originally introduced for pedestrian detection      but has since
been applied to a wide variety of recognition tasks  including character classification     
because it works on a images patches rather than on
pre segmented components  the descriptor is robust against
characters which are accidentally split apart or joined together  which are difficult for a connected component recognizer to handle  by using gradients rather than raw pixel
values and by normalizing the resulting histogram  hog
is invariant to illumination and brightness changes  hog
inherently encodes position information  but also allows a
degree of variance by virtue of its coarse spatial binning 
however  for hog to work correctly  the sizes of the
letters must roughly match the dimensions of the training
examples  so the descriptor must be run at multiple scales 
additionally  characters have an extreme range of aspect ratios  which means that the detector must also run across a
range of widths 
to detect characters  we use logistic regression on blocks
of the hog descriptor  logistic regression was chosen
since it is efficient to evaluate and provides a direct estimate
of probability  we train and save a unique detector for each
character  then the hog descriptor is computed on the input
image and each detector is run in a sliding window fashion
across it  this produces a   d matrix of detection probabilities for each character  points with probabilities meeting a
threshold   generally far more than the number of characters actually present in the image   are carried forward to
the next stage 

  

  

  

  

   

   

   

   

   

   

figure    detection of lines in an image  the left image shows the
detections and the resulting line  the right shows the voting result 

     recognition and reading
given a set of lines  we can determine the probability
that each detected character belongs to that line based on
its vertical position  using a straightforward application of
bayes rule we can write
p lij  yi    

p yi  lj  p lj  
p yi  

where lij represents the probability that detection i belongs
to line j  and p yi   is the probability of detection i appearing
at vertical position yi  
assuming that p yi    and p lj   are constants  i e  that
the positions of the characters and lines are uniformly distributed in the test images  the equation simplifies to
p lij  yi    p yi  lj  
the distribution on the right can easily be calculated using detected lines and ground truth bounding boxes for each
character 
to read words  we scan horizontally along each line and
build a directed graph of word possibilities  an empty root
node begins the graph  and letters are successively added as
the algorithm scans left to right 
overlapping detections of the same letter are merged together  while overlapping detections of differing letters create multiple paths in the graph  each node is connected to
all of the possible letters immediately preceding it  that is 
the nodes in the graph closest to the leaves which do not
overlap  an example is shown in figure   

     line detection
given a set of detections  the next step is to find the most
likely lines of characters  we do this by taking weighted
votes across all character detections  the vote v can be
 

fispurn
spurn
s p u

r

wider than t or f in many fonts  additionally  the characters i and l  are extremely distorted  stretched to fill
the whole width  and thus the training images bear little resemblance to the actual characters which must be detected 
instead  we computed the median width for each character
using the training set bounding boxes  and then normalized
each character to a single height and the median aspect ratio 
this preserves the relative shape  but means that a separate
training set must be generated for each character 
we experimented with a variety of training set sizes and
relative ratios of positive and negative examples  further
details of the training data are described in section     
we used the liblinear package to perform logistic regression in m atlab  training on        images takes approximately an hour on a   ghz intel core   duo laptop with
    gb of ram   
to run the detector  we take the dot product between the
detector and a flattened block of the image hog features in
a sliding window across the image  by keeping the detector
and the image hog features as   dimensional matrices  this
becomes a series of cross correlations between the corresponding planes of the descriptors  which can be computed
efficiently as a   dimensional convolution in matlab 

n
m

figure    completed word graph for a simulated example of the
word spurn  a false detection of the letter m causes the graph
to have multiple paths 

with the graph completed  we can can assign edge
weights based on the probability of the letter combination
appearing in english text  and node weights using the probability of detection and the probability that the character
belongs to the line  by taking negative log likelihoods of
the probabilities  finding the most likely word becomes a
minimum cost graph traversal  which can be solved with
djikstras algorithm or a  
using a complete dictionary of english words would allow more aggressive and accurate guessing of words  however  the corresponding disadvantage is that a large proportion of words in natural images are non dictionary words 
such as proper names 

     reading
character position statistics were calculated by taking
the mean and variance of ground truth character centers 
the vertical position standard deviation is on the order of
    pixels for a text height of    pixels  somewhat smaller
than we expected  character coappearance statistics were
calculated using    popular documents from project gutenberg  comprising approximately    million characters 

   implementation
we implemented our own hog descriptor in m atlab 
which efficiently calculates the descriptor at multiple scales
by computing the gradients once  building a multidimensional equivalent of an integral image  and then computing
the descriptor at each scale  while our code is theoretically
more efficient than recomputing the hog descriptor from
scratch at each scale  we found that in practice our m atlab
implementation runs significantly slower than the c implementation of hog included in the open source vlfeat library 

   testing
     synthetic dataset generation
because obtaining a large dataset with ground truth is
difficult and time consuming  we relied on a synthetically
generated dataset  which labels ground truth for several
stages of the pipeline  this approach was used successfully
by neumann et al  in     to train character classifiers for natural images using only computer generated data  despite
using synthetic data  building a good training set turned out
to be the most difficult and time consuming challenge of the
project 
the input to our data generation program is a dictionary
of words and a list of fonts installed on the computer  for
each requested training sample  the program selects a word
and font at random  and generates a black and white image
containing the word  this image can easily be segmented to

     detectors
the detectors are trained by computing the hog descriptor for all input images and using logistic regression
to obtain a linear classifier  input images are scaled so that
the resulting descriptor is    n      where n varies with
the aspect ratio between   and     this gives feature vectors
with between one and three thousand elements 
we initially normalized all of the characters to a constant
width and height based on their character bounding boxes 
however  this vastly increases the range of scales which the
detectors must run at  since m and w are several times

  i attempted to run my code on the corn cluster  but the network disk
latency from afs made it far slower than running locally  since the training
includes reading tens of thousands of tiny images 

 

fiprovide character bounding boxes and other ground truth  a
second color image is generated by selecting random colors
for the background and text  noise is added  and the final
result is saved  compression artifacts add a small amount
of additional noise 
for character detector training  it is important to include
a large number of examples with many small variations 
since the hog cells form a coarse grid and are run at a
discrete set of scales  it is important for the detector to find
matches which may have small scale and position differences 
to achieve this  we took each of      input examples
and created    new images for training  approximately
half of the images contain small position offsets and scaling  while the other half contain large offsets or scales  the
former are kept as positive examples  while the latter are
marked as negative examples  along with images of all the
other letters  this helps prevent the detector from misfiring on parts of letters which might otherwise be considered
matches 

matrices for the test set are shown in figure   
 
   
   
   

precision

   
   
   
   
original dataset
augmented dataset

   
   
 

 

   

   

   

   

   
recall

   

   

   

   

 

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z

truth

truth

figure    precision recall curves for the original and augmented
datasets  most of the images contained multiple detections of at
least one character  these were combined into one for calculating
precision and recall 

a b c d e f g h i j k l mn o p q r s t u v w x y z
detected

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
a b c d e f g h i j k l mn o p q r s t u v w x y z
detected

figure    confusion matrices without  left  and with  right  augmented training set  tested on a separate testing set of     images 

figure    top  sample input image  middle  positive training examples which exhibit small variations  bottom  negative training
examples which exhibit large variations 

while this classifier performs fairly well on the test set
which contains single characters  it fares poorly on words
and sentences  often misfiring on combinations of letters
and spaces between letters  additionally  it fails badly on
the letters i and l  and has some difficulty with f and
t  because the training set for these letters contains many
negative examples which appear identical to positive examples  figure   shows several negative examples for the letter
i  which could easily be considered positive examples 

we found that it was important to provide a small
padding space around each training example  if the outside
edge of a letter is cut off  the gradient along that edge no
longer contributes to the descriptor and the result is much
weaker  this is similar to the finding in     that space
around the pedestrian is important  and that increasing the
size of the pedestrian but removing the border space causes
a lower accuracy 
after training a set of detectors with this dataset  we created an augmented dataset to reduce false positives  the
detectors were run on the original      input images  and
training examples were pulled from all false positives 

figure    negative training examples for the letter i which are
very similar to positive examples  because of these  the i detector
does a very poor job 

     evaluation
to provide a quantitative metric on the accuracy of our
detector  we ran it on a separate test set of     images  the
precision recall curves are shown in figure    and confusion

in order to mitigate these problems  we created a new
 

fitraining set based on images with complete words  as before  positive examples are taken from the character bounding box with small perturbations  we used a larger border space to allow better differentiation between characters
such as a  d  and q  which can look identical if the
ascender or descender is ignored  negative examples are
drawn from random patches of the image  we used an input set of      images with an average of    letters each 
which produced approximately        training images per
letter  split     positive     negative 
subjectively  the resulting detector performs better on
the problematic letters above  however  it performed poorly
on the single character test set  with precision less than    
across all recall values  most of the false positives were on
flat patches of noise  possible solutions for this might be to
include a higher percentage of flat examples in the training
set  or to replace hogs local normalization with a wide or
global normalization 
the detection algorithm is not accurate enough to provide reliable input to the graph construction and evaluation
algorithm  so we did not get to the point of testing it  running the graph code on ground truth bounding boxes is not
particularly enlightening  since it consists of a single true
path 
example results for some images in the icdar dataset
are shown in figure    on real photographs  our detector
frequently reports hundreds of false positives  particularly
textured regions of the image such as brick  using textured
backgrounds or image patches as training examples would
probably produce better results here than flat colors 

figure    sample detection results from the icdar dataset  the
detector finds most of the characters  but includes many false positives  in the bottom image  only the strongest results are shown 

on document analysis and recognition  icdar   sep 
      pp         
    j  weinman and e  learned miller  improving recognition of novel input with similarity  in ieee computer society conference on computer vision and pattern recognition  vol     jun        pp         

   future work

    l  neumann and j  matas  real time scene text localization and recognition  in computer vision and pattern recognition  cvpr        ieee conference on 
june       pp           

some letters  particularly the lowercase letter a  can be
written in multiple ways  it would therefore be more appropriate to split the character into two classes with separate
detectors but the same label 
due to the computational load  we did not have time
to experiment with as many parameter permutations as we
originally hoped  further work should test larger descriptors     or    blocks high  and a range of padding widths 

    n  dalal and b  triggs  histograms of oriented gradients for human detection  in computer vision and pattern recognition        cvpr        vol     jun       
pp          vol    
    l  neumann and j  matas  a method for text localization and recognition in real world images  in computer vision accv       ser  lecture notes in computer science  r  kimmel  r  klette  and a  sugimoto 
eds  springer berlin   heidelberg        vol       
pp         

references
    k  wang and s  belongie  word spotting in the wild 
in computer vision  eccv       ser  lecture notes
in computer science  k  daniilidis  p  maragos  and
n  paragios  eds  springer berlin   heidelberg       
vol        pp         

a  project sharing

    a  coates  b  carpenter  c  case  s  satheesh 
b  suresh  t  wang  d  wu  and a  ng  text detection
and character recognition in scene images with unsupervised feature learning  in international conference

this project was done in combination with cs    a
 computer vision   i was the only student working on the
project in either class 

 

fi