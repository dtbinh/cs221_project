lasso on categorical data
yunjin choi  rina park  michael seo
december         

 

introduction

in social science studies  the variables of interest are often categorical  such as race  gender  and
nationality  however  it is often difficult to fit a linear model on such data  especially when some or
all of the explanatory variables are categorical  when the response variable is the only categorical
variable  it is common to use the logit model to overcome the defects of ordinary least squares 
however  when the covariates are also categorical  corresponding variables are coded using dummy
variables into our design matrix  in this approach  the data matrix becomes sparse  the column
dimension increases  and columns might be highly correlated  this might result in a singular data
matrix causing coefficients of linear square estimation lse  impossible  in order to avoid this
pitfall  researchers use group lasso gl   though gl has beneficial properties when dealing with
categorical data  it is not devised specifically to analyze factor variables and there still remains
room for improvement  in this project  we propose modified group lasso mgl  for improvements
in categorical explanatory data  it performs better than lasso or gl in various settings  particularly
for large column dimension and big group sizes  also mgl is robust to parameter selection and has
less risk of being critically damaged by biased trial samples 

 
   

background
lasso

lasso is a penalized linear regression  in linear regression  the underlying assumption is e y  x  
x    xt   where  y  rn and x  rn p    while lse minimizes    ky  xk   with respect to  
lasso minimizes the penalty function    ky  xk     kk    since the additional l  penalty term is
non smooth  lasso has variable selection property that can deal with the multicollinearity in data
matrix  this property suggests that only the selected variables via the procedure will be included
in the model  in this sense  lasso is a proper method for factor data analysis  as it takes care of
difficulties described above  however  the variable selection property of lasso yields a new problem 
partial selection of dummy variables  it is not reasonable to select only a portion of dummy variables
derived from one categorical variable  many researchers use group lasso to bypass this problem 

   

group lasso gl 

group lasso performs similarly to lasso except that it selects a group of variables rather than a
single variable at each step of selection  the groups were pre assigned on covariates  therefore  in

 

ficategorical case  we group the stack of dummy variables originated from a factor variable and each
group represents a corresponding factor variable  this group selection property of gl has been
facilitated by minimzing 
l

l

x
x
 
pl k  l  k 
ky 
x  l    l  k     
 
l  

   

l  

here l              l  denotes the index of a group  pl   the size of the l th group  x  l    corresponding submatrix and   l    the corresponding coefficient vector  the additional l  terms not
squared  in     takes the role of l  term in lasso 

 

modified group lasso for categorical data matrix mgl 

gl has been developed in order to select an assigned group of variables at each selection stage 
regardless of the data type  however  gl is not specialized for the categorical mix data case  note

that the value of     becomes bigger as pl increases  as a consequence  gl tends to exclude
groups with big dimensions  as for categorical data  all the variables in a same group basically
represents one explanatory variable  thus  favoring small groups can cause severe bias  for example 
nationality variable which has more than     levels us  canada  mexico  etc    and gender variable
with two levels male and female  should have equal chance of getting in the model when other
conditions are the same  our modified group lasso gets rid of this effect by minimizing 
l

l

l  

l  

x
x
 
k  l  k   
ky 
x  l    l  k     
 

   

unlike      the additional l  terms in     are equally weighted  this approach ignores the size
of groups in theselection procedure  and agrees with ravikumars
sparse additive model fitting

pl
pl
 
 
 
idea ravi   min   ky  l   fl  x k     l   kfl  x k 
one great advantage in this method is that we can still use the optimization algorithms in gl  gl
and mgl have functionally the same form of target function to minimize and adaptive lars can
be adopted in both cases     in this project  we implemented modified lars    

 
   

application
simulation

we have applied mgl under various settings  in each case  n   the total number of observations is
     each row of the data sub matrix follows multinomial distribution with equal probability when
the corresponding group size is bigger than    otherwise  follows standard normal distribution  thus 
a multinomial distributed row of sub matrix x  l  represents dummy variables of the categorical
variables  p
using this categorical data matrix x  the response vector y was generated as y  
 l 
x      l
l   x  l      here the coefficient vector  and the noise vector  is adjusted to
have signal to noise ratio    also     percent of  elements are set to be   to observe the model
selection property 
estimation of models were conducted by comparing two types of errors  estimation error   ky  yk  
 

fiand coefficients error   k  k 
in the figures below  black  red and green lines represent mgl  gl and lasso respectively  the
estimation error and coefficient error were provided in first row and second row respectively  in
every graph  x  axis is the constraint coefficient   each column in the figures implies two error
estimations from the same simulation run 
for the model fitting  following the convention  we choose the lambda which minimizes the
estimation error  thus to compare the performance of each models  it is reasonable to compare the
minimum values over lambda  in this simulation  with respect to the estimation error  it seems that
mgl surpasses other methods when the number of covariates is large and the size of groups is big 
this coincides with our minimization criteria since when the size of groups are small  the distinction
between mgl  gl  and lasso would vanish  except for several exceptions mgl performs better
than gl ans lasso 
in terms of coefficient error  it seems to be unstable and no method has dominance over others  this
can be explained by the multicollinearity in the data matrix  as mentioned above  using dummy
variables can introduce severe multicollinearity as the number of covariates and the size of p grow 
when there exists multicollinearity in the data matrix it is possible that there exists a linear model
other than the true model which explains the data as good as the true one  this can explain the
poor outcome in coefficient error while the estimation performance is nice 
one remarkable thing about mgl is that it is robust to the choice of   we observe from the figures
that mgl has the smallest curvature in any case  as a consequence  even though we choose the
wrong lambda in application  the fitted y would not be catastrophically deviated from the true y 
group size     or  

    
    
 

 

 

 

 

 

 

lambda

lambda

group size     or  

group size     or  

 

 

 

 

   
   

coefficient error

   

   

   

   

   

   

   

   

   

   

 

coefficient error

    

estimation error

    
    
    

estimation error

    

    

group size     or  

 

 

 

 

 

 

lambda

 

 
lambda

figure    number of covariates   from left to right  categorical variables with level     and       
and  

 

figroup size     or  

group size     or  

group size    or  

 

 

    

estimation error

    

   
   

    

    

 

 

 

 

 

 

 

   

   

   

   

   

   

   

 

 

 

 

lambda

lambda

lambda

group size     or  

group size     or  

group size     or  

group size    or  
    

 

    
    
    
    

 
  

 

    

    

 

  

    

coeffecient error

  
  

coefficient error

  

coefficient error

  

    
    
    

coefficient error

  

  

    

  

  

  

lambda

    

 

    

   
   

estimation error

   

    
    
    

    

    

    

estimation error

    
    

estimation error

    

    

   

    

group size     or  

 

 

 

 

 

 

 

lambda

 

 

 

   

   

   

lambda

   

   

   

   

 

 

 

lambda

 

 

lambda

figure    number of covariates   from left to right  categorical variables with level      and      
  and         and         and  

group size     or  

group size    or  

    

estimation error

    

    
    

estimation error

   
   

estimation error
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

lambda

lambda

lambda

groupsize    or  

group size    or  

group size     or  

group size     or  

 

  
  

  

  
   

   

   
lambda

   

   

   

coefficient error

  

coefficient error

  

  
  

  

coefficient error

  

  

   

  
  
  
  
  

coefficient error

 

lambda

  

 

    

    

   

   

    

   
   
   
   

estimation error

group size     or  

   

group size    or  

 

 

 

 

 

 

lambda

 

 
lambda

 

 

 

 

 

 

 

lambda

figure    number of covariates   from left to right  categorical variables with level      and      
  and         and         and  

 

fi   

real data

mgl  gl and lasso are applied on the real mixed data in education  the data was collected from
          national education longitudinal study  nels   data consists of     observations with
   mixed types of covariates  such as gender and race for the factor variables  and units of math
courses and average science score as quantitative variables  the response variable is the average
math score 

   
   
   

estimation error

   

   

cv
modified lasso
group lasso
lasso







 

 

 

 

  

lambda

figure       fold cross validation on educational study data for estimation error
out of     data points  we chose     random points as a trial set and tested on the remaining 
figure   is a    fold cross validation estimation error result  broken lines represent one standard
deviation of the error for each method  the tuning parameter s were chosen via one standarddeviation rule following the convention of lasso regression  the convention is to choose the biggest
lambda within the range of one standard deviation from the minimum value in order to avoid
overfitting and bias in the test data 
mgl achieves the smallest error on the test set  even though its cv estimation error is the worst 
this can be a good example of robustness in mgl  on the test set  the estimation error ratio showed
a significant improvement for mgl to lasso  mgl to gl  and gl to lasso  which was             
and       respectively  we can improve the performance on cv with more factor variables 

 

conclusion and discussion

among previously introduced linear models  mgl performs relatively well  it has two main advantages  small estimation error and robustness to parameter selection in categorical data  with these
advantages  we can apply to a wide academic realm dealing with categorical data  such as social
science and education  future research goal is to    analyze the case in which mgl has dominance
both theoretically and empirically and    develop an analogous approach in logistic regression 

references
    tibshirani  r  regression shrinkage and selection via the lasso  journal of the royal statistical
society b                 

 

fi    park  m  and hastie  t  regularization path algorithms for detecting gene interactions  available
at http   www stat stanford edu  hastie pub htm       
    yuan  m  and lin  y  model selection and estimation in regression with grouped variables 
journal of royal statistical society  series b                    
    chesneau  ch  and hebiri  m  some theoretical results on the grouped variables lasso  mathematical methods of statistics                   
    freidman  j  h   hastie  t  and tibshirani  r  regularized paths for generalized linear models
via coordinate descent  journal of statistical software              
    wang  h  and leng  c  a note on adaptive group lasso  computational statistics and data
analysis       
    zou  h  and hastie  t  regularization and variable selection via the elastic net  journal of
royal statistical society  series b                      
    ravikumar  p   lafferty  j   liu  h   and wasserman  l  sparse additive models  journals of the
royal statistical society  series b                         issn          
    kim  s  and xing  e  tree guided group lasso for multi task regression with structured
sparsity  proceedings of the   th international conference on machine learning       
     simon  n  and tibshirani  r  standardization and the group lasso penalty  statistica sinica 
                 

 

fi