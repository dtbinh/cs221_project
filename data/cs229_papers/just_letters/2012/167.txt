intelligent technology for innovation in urban disasters
joe fan  adam lazrus  anand ravulapalli  michael ross
abstract
by gathering and training on data relating to hurricanes we illustrate techniques to predict which areas will be most
impacted  how successful a recovery plan will be  how best to allocate resources  and the capability of social media
data sources to improve the accuracy and responsiveness of damage projection models 
i  introduction
natural disasters in urban areas pose both a unique challenge and a unique opportunity  the high population
density  potential access issues due to narrow and blocked roadways  and other issues make responding to  and
planning for the response to  natural disasters in urban areas in many ways much more challenging than in rural
areas where far fewer individuals and families are affected and routes of access are less constrained than in the
urban landscape  conversely  the same population density allows an economy of scale to leverage the efforts of
well planned first response resources to effect greater relief for a greater number of individuals  and the density of
utilities and people make the data collected during recovery much more expansive and comprehensive  as there
are many different varieties of disasters with different dynamics  our initial goal was to focus on modelling the
damage and recovery from hurricanes in urban areas 
following the landfall of hurricane sandy in late october       we quickly decided to refocus our efforts around
collecting data for this event  the goal of our project was to model disaster recovery preparedness through
machine learning and identify potential subject domains of data for further study  we generated systems to predict
the economic  social and technological problems caused by disasters that may be used to assess how effective
recovery efforts are and to make predictions for how best to prepare for and respond to a disaster recovery
scenario 
we envision that with further development our models will assist in assessing how effective a particular disaster
response and recovery plan is likely to be and in developing and executing response and recovery plans by
deciding on efficient division of resources and manpower 
ii  data collection
data pertinent to the recent hurricane sandy were collected from various federal and non profit organization
websites primarily for the date ranges from october    through november         all data were provided by zip
code or aggregated by zip code during data preparation  the resulting longitudinal data source by zip code
contained primary subject domains referencing demographic information  disaster damage  utilities interruption  and
twitter volume and behavior features 
data were collected from the following sources 
 google crisis response  information such as storm paths  shelter locations  emergency numbers  and
donation opportunities  http   google org crisismap      sandy nyc
 department of homeland security   federal emergency management agency  dhs fema  
categorical flooding severity by latitude and longitude  with classifications of  destroyed  major  minor 
affected and no damage  http   goo gl q cze
 safe guard properties  power outages due to hurricane  http   goo gl  exwl
 united states census bureau  demographic and economic information 
 demographic data by gender and age as of       http   goo gl kutt 
 demographic data pertaining to housing as of       http   goo gl jxk  
 economic data for the demographics  http   goo gl s lnc
 united states geological service  usgs   flood levels from data collection stations or measure by
fema  http   water usgs gov floods events      sandy sandymapper html
though much of the data were sourced by zip code  data provided by latitude and longitude were aggregated to
zip codes based on us zip code centroid reference data from the census bureau and euclidian minimal distance
 

fiassignment  while this method is not as accurate as geo polygon assignment  we determined that the accuracy
was sufficient given the scope and amount of time allocated for the project 
in addition to the above reference data sources  twitter data were obtained through a combination of methods 
given the twitter search api s limited geosearch capabilities  an html parser was written in python to search
twitter via html and create a cohort of       twitter users who could be confidently localized to zip codes for
which demographic and storm damage data were readily available  due to the twitter apis limited search history
of approximately   weeks  a python interface to the topsy otter api  toffaletti        was written to collect all
tweets for this cohort for the time period between october        and november          to ensure coverage of
both the storm event and its aftermath and to provide a baseline period of approximately one month  from the
resultant data set of approximately         twitter status updates  we used the python natural language toolkit
 bird et al         referred to as nltk  to tokenize the status updates and remove filler words to look for the
emergence of trending words or phrases around the time of the storm landfall which may indicate the
communication of storm related damage  using these observations  we derived features representing raw update
volume  the volume of occurrences of key words  and indicators of behavior changes by individual twitter users  for
example  users with over    updates a day who have no updates as the storm makes landfall may indicate a power
outage  
the resulting longitudinal data set was comprised of     training examples  zip codes  with     features
distributed across the following subject domains 
   demographic  population  building types and density  number of automobiles
   disaster damage  number of homes units damages and their value in usd
   flood severity  wave flooding height
   utilities interruption  counts of people who lost electricity
   social media  keyword occurrence and indicators of changes in user volume from twitter
the overall system design is represented in figure   

figure    overall system design

 

fiiii  methods
linear model
our primary objective was to measure the effectiveness of using social media to enhance machine learning models
for predicting a hurricanes impact  to this effect  we first joined the demographic and storm data with the twitter
data  using zip code as the key  the obtained data were used as input to our models 
we first attempted to learn a linear regression model using regularization of parameter size  the data
comprises many features  but has relatively few training examples  and the model ran the risk of overfitting  to
combat this  we ran a feature selection algorithm to pick out at least    features to train on  the algorithm continued
to pick new features beyond the first    as long as the model did not become too overfitted  at each iteration  a
new feature was chosen and training and testing error were computed using leave one out cross validation  to
determine when the model was being overfitted  we also compared the change in testing error to the change in
training error in each iteration  when the training error changed drastically in comparison to the testing error  the
feature selection was ended 
the regularized linear model with feature selection was implemented as a python module integrating the scikitlearn  pedregosa et al        package  it was first used to train on the traditional data set  without the twitter usage
data  we then augmented the data with the dataset collected from twitter  and ran feature selection again to retrain
the model 
clustering
as mentioned earlier  the availability and reliability of data varied  femas classification of building damage was
often subjective and inaccurate  the height of the storm surge could vary  and the twitter data could require
additional signals  another approach was unsupervised learning  specifically k means because the assumption and
quality of data are more amenable  the storm surge  power outage  and femas multiple classifications of building
damage also provided multiple response variables  so we hoped that clustering would help us identify common
characteristics across damage  demography  and twitter  clustering would work better than principal component
analysis because we want to grasp the commonality across variables instead of reducing the dimensionality 
we used k means and top down hierarchical k means in which the distance measure was euclidean distance 
we experimented with initializing various numbers of clusters and levels  and we found that the optimal number of
clusters was    and going beyond the  st level did not provide sufficient separation  one group member works at a
company that provides in database analytic solutions and consulting and was a key resource in implementing
clustering  in database analytics could handle massive datasets with ease  and the computational performance
does not deteriorate as the data size increases  we converted the data into the appropriate format and used the
companys in database stored procedure to perform clustering  the stored procedure is implemented on netezza
and includes proprietary queries and user defined functions to perform the expectation maximization of the
euclidian distance  the stored procedure and user defined functions perform the computations in parallel across
multiple computers in the server rack  which increases performance dramatically  after computing the centroids  we
improved the comparability across variables by converting them to the percent rank of that variable 
iv  results
linear model
the regularized linear model was run through feature selection  stopping after    iterations to prevent overfitting 
the final testing and training errors are displayed in table   

table    linear model training and test error on traditional and augmented data

 

fithe training error and testing error were plotted against each iteration for both the traditional data and the dataset
augmented with twitter data  the plots are displayed in figure   

figure    error plot for traditional and augmented data models with ridge regression

clustering
the three centroids that emerged could be roughly categorized with respect to the amount of damage that
hurricane sandy caused  and we could identify their association of other variables 
 damage  cluster   suffered significant inundation  damage  and power outage  cluster   focused more on
the buildings that were inundated and majorly damaged  cluster   was spared of heavy damage except for
minor flooding  storm surge was highest for cluster    and cluster   had high amount of not inundated
severely damaged building  which suggest that additional non water response variables could be available 
 commuting to work  there is a separation between communities that drive and do not drive to work  it is
unfortunate that communities that drive to work suffered more infrastructure damage  whereas commuters
who use public transportation  walked  or worked from home were spared  also the mean travel time to
work would increase even more after hurricane sandy for communities that already have relatively long
commutes 
 industry of employment  about     of the industries are listed here  people working in construction 
transportation  warehousing  and utilities were more exposed to storm damage  on the other hand 
professionals in information  finance  and management were lightly affected 
 income  the poor were more exposed to storm damage than the rich  the centroids of cluster   increases
to     for the poor whereas the centroids of cluster   increases to      for the rich 
 units in structure  buildings with higher number of units avoided significant damage  they have high
centroids in cluster   and cluster   
 vehicles available  severe damage and flooding affected households with any number of vehicles 
communities that did not use vehicles had light damage  implying that urban environments fared better
than suburbs 
 twitter  there were few twitter users in high damage areas  across the high and moderate damage areas 
twitter usage decreased by     after hurricane sandy made landfall on the night of october     the
majority of tweets about the storm were located in low damage areas whereas very little tweet usage
occurred in high damage areas probably because of power outage and life saving priorities  the keywords
storm and hurricane were the most used and could be indicators of tweets related to the event 
 

fitable    clustering results
v  discussion
the initial results obtained from linear regression demonstrate that social media can be effective in augmenting the model
learned from the other data sources  after incorporating the twitter dataset into loocv  we were able to lower the testing error
while not overfitting by too much  as indicated by the training error in each model  however  the plots also show that linear
regression is not an overwhelmingly effective tool for modeling the data  nevertheless  the linear model proved to be a good
starting point for showing the effectiveness of social media to enhance our models 
the clustering identified similar variables across damage  demographics  and social media  the demographic variables are
updated less frequently so the association between damage and demographics could serve as rules of thumbs for first
responders and for allocating budget to the appropriate municipalities  the twitter data shows reasonable results for associating
the tweeting frequency and words associated with hurricane sandy  however  we should use other methods such as nave
bayes if we want to properly parse the tweets for richer contextual information  the clustering results provided support that this
effort could be worthwhile 
vi  conclusion and future work
we feel further study is warranted into the near real time identification of damage areas using social media  though it is a noisy
data source  it can serve as an aggregator of the thousands of eyes and voices within an urban area  serving as flood and
damage sensors where none are installed  much more sophisticated techniques can be applied to derive robust features from
the twitter firehose  and more appropriate models than the linear model used here can be selected  but our experiment
illustrated that social media features can be useful additions in modeling storm damage  given that theyre available in near
real time without additional data collection efforts  social media can serve as a valuable addition in planning and responding to
urban disasters 
references
bird  steven  edward loper and ewan klein  natural language processing with python  oreilly media       
pedregosa et al  scikit learn  machine learning in python  jmlr     pp                  
toffaletti  jason   topsys otter api   google code  http   otter topsy com  accessed nov          

 

fi