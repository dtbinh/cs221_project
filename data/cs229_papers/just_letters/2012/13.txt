employer health insurance premium prediction
elliott lui

 

introduction

the us spends       of its gdp on health care  more than any other country  and the cost of
health insurance is rising faster than wages or inflation  per year  employers spend     
billion on health premiums for their employees  one important question these employers
must always consider is if the coverage they are getting is worth what they are paying  the
health insurance providers have their proprietary actuarial methods and complex models to
determine these premiums  but they are hidden from the public 
the purpose of this project is to explore the use of machine learning algorithms to predict the
prices of annual health insurance premiums given the specifications of the contract and the
companys demographics  that is  given a health insurance contract and information about a
companys employees  can we accurately predict how much it will cost per year  using
svm  multinomial naive bayes  and a decision tree classifier  we can try to predict the
premium costs of a contract  as well as determine the optimal set of features using feature
selection 

 
   

data
dataset

the data set used for this project encompasses over       anonymous companies
representing over   million lives  it comes from the kaiser family foundation  which
conducts an annual survey to collect this data to create the annual health benefits report  a
vector of      variables represents each company  or data point  to put into relevant context 
some of these features include industry  size of firm  percentage of workforce age    or
lower  copayment amount for prescription drugs  coinsurance rate for hospital visit  and
percent of workforce earning         or less  which conceptually seem quite relevant  on the
other hand  there are features such as does hdp use copay  coinsurance  or both for
generic drugs and firm offers this ppo plan last year that seem irrelevant in insurance
premium pricing 

   

training data

because there are three different types of health insurance plans  and each type of plan has
slightly different features  we come up with three sets of training data to predict three sets of
prices  we are trying to predict the annual premium cost for a company for the categories of
ppo  hmo  and hdp  some companies provide more than one type of plan  but because
there are slightly different features  we treated the company as separate data points in each
plan  for example  if company a offered both ppo and hmo plans  we put the relevant
feature vectors of a in both the ppo training data and hmo training data 

   

feature selection

fito quantitatively determine which features are most relevant  we ran forward search first
using the svm classifier with a gaussian to maximize    fold cross validation  and then with
the other classification algorithms  interestingly but perhaps not too surprising  the top
features made sense conceptually  consisting of variables representing coinsurance amounts
for surgeries  hospital visits  and drugs 

figure    variables added to feature set through forward search

improvements in accuracy stopped after just   iterations for the svm as seen in figure    the
forward search algorithm resulted in a set of just   features  down from the      in the raw
format as seen in figure    furthermore  after initially running with just svm  running with
multinomial nave bayes  and decision tree classification yielded the same top   features
with variances of at most three features 
feature set

raw data

figure    raw data to relevant features

 
   

predicting insurance premiums
bucketing target values

since the premium prices are continuous  and we wanted to leverage the power of svm and
other classification algorithms  it was necessary to bucket these values  in order to make

fimeaningful predictions  it would make sense to limit the maximum error to what is
acceptable in practical situations  year to year changes in insurance premiums max out at
about      so we bucketed our data such that each of n buckets corresponds to a maximum
price differential of      for example  a bucket with a minimum value of         would
have a maximum value of         

   



svm

we trained a multi class svm to classify which of n buckets a given data point belonged in 
with one svm trained for each type of plan  when training the svms  we experimented
using gaussian and linear kernels  the results shown in figure    in the gaussian kernel 
 
 
k x z    e  x z        whereas in the linear kernel k x z    x t z  
the parameter  for the gaussian kernel was tuned by using a grid based search to maximize
   fold cross validation  we made sure to not overestimate this parameter to avoid it
performing like a linear kernel  underestimating it on the other hand would make it too
sensitive to noise in the training data  
we created the multi class svm by training a svm for the upper and lower limit for each
bucket  each svm predicts whether the price of a contract should be greater than or less than
a bucket value  the final classification is then made by a majority vote by the svms  in
addition to using different kernels  we also experimented with different bucket sizes of     
     and     to see its effect on accuracy 

   

multinomial nave bayes

multinomial nave bayes models the distribution of feature values as a multinomial  the
nave bayes assumption is that each feature is generated independently of every other 
using the same methodology described above  we trained multiple binary multinomial nave
bayes classifiers  we wanted to keep consistent with our previous method of multi class
classification  and it performed much better than multiclass nave bayes on its own 

    decision tree
though it wasnt explicitly taught in class  decision tree learning seemed like a good model
for this data  one hypothesis for the low accuracy of the models could be the combination of
continuous and categorical variables  of the   features in our feature set    are categorical 
even copay amounts were discretized into ranges  decision tree classifiers are able to handle
categorical variables especially well  so it made sense to train this classifier 

 

results

the results of our three models
are summarized in figure    each
model was run on three datasets
corresponding to ppo  hmo 
and hdp plans  within each

ficategory we see the accuracy of predicting two values  the premium cost for family and the
premium cost for an individual  the ppo training data contained     rows  the hmo     
and hdp     

algorithm
svm
multinomial naiive bayes
decision tree

overall accuracy
     
    
   
figure    overall performance of models

    svm results
with the accepted     bucket size standard for this project  the svm predicted premium
prices with the highest accuracy among the three models at a rate of                 we see
increasing bucket sizes improves classification accuracy  intuitively  it makes sense that it is
easier to classify data into larger ranges 

bucket size

accuracy
linear kernel

   
   
   

    
    
    

gaussian kernel
    
    
    

figure    svm accuracy

the two different kernels on the other hand didnt display any significant differences in
performance  though we made sure to tune our gaussian parameter  to not overestimate
and behave linearly  it still didnt produce a substantial difference 

   

multinomial nave bayes results

at an average accuracy of       multinomial nave bayes exhibited the worst performance
among our models  for all three categories  ppo  hmo  hdp   nave bayes was the only
algorithm to pick the companys industry as a feature in forward search  conceptually it
would make sense that a company in a labor intensive industry like construction would
command higher premium prices than a company with young  highly skilled workers like a
tech company  at first glance at the raw data  the poor performance could be attributed to a
violation of the nave bayes assumption  however  the features it selected were all
independent from a conceptual standpoint 

   

decision tree results

the decision tree classifier performed almost as well as the svm overall with an accuracy
rate of      figure   shows a sub tree of the decision tree that was ultimately constructed
from our training data  the whole tree would be too large to display  

fifigure    sub tree of the decision tree classifier

the tree contained     nodes representing possible decisions  the number of samples with
that decision  and its error  because these decisions are binary    operations  the model
seemed to work on the categorical nature of our features 

 

discussion

the results from our models didnt predict insurance premium costs very well  while the
accuracy rates werent abysmally low  there was much left to be desired  the low accuracy
across all models suggests that the feature vectors dont encompass all pertinent information 
in fact  the data doesnt account for the strength of the health plan network  for example  two
identical data points can have differing physician networks  one could include a top notch
institution like stanford hospital while the other includes only small clinics with fewer
doctors  the former would obviously command a higher premium price  but such information
was not included in the data set  presumably because it is extremely difficult to quantify  the
forward search component of the analysis was quite successful in choosing the most
important features  that is  without supervision  it was able to select the features such as
copays for drugs and hospital visits that make sense in determining the price of health
insurance premiums  perhaps with key additional information like network strength  we
could have predicted premium prices at a much higher accuracy 

references
    n chapados  y bengio  p vincent  j gohsn  c dugas  i takeuchi  l meng  estimating car
insurance premia  a case study in high dimensional data inference  advances in neural
information processing systems       
    d biggs  b ville  e suen  a method of choosing multi way partitions for classification and
decision trees  journal of applied statistics             
    g cass  an exploratory technique for investigating large quantities of categorical data 
applied statistics               

fi