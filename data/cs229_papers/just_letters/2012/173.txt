clustering of cities by craigslist
posts
charles johnson and michael kim  stanford university   cs    
   december     
e aimed to evaluate the similarity between us cities by clustering them
based on postings from the classifieds
website craigslist  craigslist is a data source
that could provide particular insight into
the character of a city because the content
is community driven and contains unfiltered
natural language  our clustering was performed agnostic to geographic location so as
to determine a more nuanced indicator of
similarity  here  we report our methods for
creating feature vectors from the raw posts
and our results from subsequently clustering
the cities  we experimented with features
that considered the metadata  categorical distribution  as well as the natural language
of posts  after generating feature vectors
for each city  we applied two clustering algorithms  k means and mean shift  and then
compared their outputs  clustering with kmeans produced fairly consistent  promising
clusters  whereas clustering with mean shift
did not produce any meaningful clusters  our
results from clustering could prove useful as
an input to supervised learning problems that
involve cities  or the clustering may be interesting as a qualitative metric in and of itself 
feature vector 

w

data collection
to obtain data from craigslist  we elected to use
the  taps api   taps com   an api for querying
up to date and historical posts from all of craigslist
in a json format  the json response includes
source data from the posts such as post titles  post
bodies  and post categories  as well as additional
annotations such as location codes at varying
granularity 
for our exploratory work  we collected a dataset of

the        most recent posts at the time of collection 
we initially created feature vectors and clustered
based on city codes  e g  at the level of palo alto  
however  as we expanded our dataset  we found that
there were far too many city codes to work with and
most of the cities were small  unrecognizable ones 
instead  we chose a list of    major metro areas
based on americas best cites  business week
       with these    metro areas  we collected the
     most recent posts for each city  which served as
the dataset for the work that follows 

initial clustering
as a first pass  we created feature vectors from the
batches of      posts by calculating three meta
data  the average post length  the average title
length  and the average number of images used
per post  instead of using these raw features  we
normalized them in order to weight the features
equally against one another  otherwise  a feature
which is naturally larger  like post body length
compared to post title length  would artificially
create larger distances between vectors  as a first
pass  we simply normalized all values of a each
feature dimension by dividing by its maximum
observed value  moving the values of all features
onto a        scale 
we used these three dimensional feature vectors as input for the k means clustering algorithm 
prior to actually running k means  we plotted the
average within cluster distance versus the number
of clusters in order to determine a reasonable value
for k  based on figure    we chose the smallest
reasonable value of k that minimizes the average
within cluster distance  the curve is promising
because it does have a natural knee where we can
choose an optimal value for k   
 

stanford nlp website  cluster cardinality in k means

page   of  

ficategorical information of the posts  conceptually 
the natural language and categorical data seem to
be the likely sources of any intangible characteristics
of the cities that might be similar and thus influence
clustering 

categorical features
on craigslist  posts are categorized under seven major buckets  for sale  vehicles  housing  discussion  community  jobs  and services  for
each city  we calculated the percentage of its posts
that were in each category  furthermore  within each
figure    average euclidean distance from nearest cen  category for each city  we also calculated the percent
of posts that were unpriced  i e  free or not related
troids 
to any transaction  and the average price of those
posts that were priced  this created a total of   
figure   shows an example run of k means clustering new feature dimensions 
on these initial feature vectors  in order to qualitatively analyze the clustering of the feature data 
we performed principal component analysis on the
feature vectors so that we could plot them in two di  lda features
mensions  qualitatively  the initial clustering seems
reasonable but not fantastic  there are distinct clusin addition to the categorical features described
ters which appear to be reasonably grouped  but
above  we also computed features of the natural lanthere certainly is not clear separation between the
guage used in the posts using an lda topic model 
centroids 
we used the stanford topic modeling toolbox to
run lda on the post titles and bodies from each
of the craigslist categories  creating a set of topics
discussed within each category  we experimented
with various techniques to preprocess the post titles
and bodies before running lda  such as keeping or
removing stop words and setting a minimum word
length  the topics that seemed most promising were
produced from posts that ignored case and punctuation and had a minimum word length of    we also
filtered out stop words and the    most common
words from each category and required that a word
occur in at least    posts 
upon inspection of the topics generated by lda 
we observed that the model using these rules seemed
to find vectors of words that seemed fairly inuitive 
figure        means clustering plotted with pca
once we had generated topics  we created feature
vectors for each city by concatenating the average
per document topic distribution for each category 
feature expansion
because many cities had very few posts in the discussion and community categories  we did not include
though the initial clustering results were favorable  these in the expanded feature vector 
looking exclusively at meta data ignores the potentially informative and valuable natural langauge and

page   of  

fisystem
laptop
computer
power
audio
video
drive
speakers
windows
memory
digital
screen
sound
remote
player
wireless
display
processor
monitor
stereo

iphone
phone
samsung
galaxy
sprint
verizon
screen
mobile
phones
repair
trade
unlocked
tmobile
service
white
android
broken
store
factory
green

season
level
stadium
parking
lower
upper
group
yards
various
state
thursday
field
football
sunday
party
cheap
monday
together
weekend
playoff

dryer
washer
kitchen
silver
jewelry
brand
bedroom
sectional
diamond
delivery
leather
microfiber
electric
stainless
rings
warranty
appliances
white
steel
works

mattress
furniture
queen
brand
mattresses
spring
patio
frame
delivery
outdoor
pillowtop
plastic
springs
pillow
quality
warranty
plush
support
foundation
bedding

more arbitrary  nonetheless  after running the clustering with various values     clusters seemed to work
fairly well  because we were working with such a
high dimensional feature space  roughly r       pca
was somewhat ineffective in visualizing the clusters 
instead  we used the tsne algorithm for visualizing
the clusters in a plane   

figure    examples of topics from for sale posts

clustering results
in this section  we lay out the results of clustering
the cities according to their expanded feature vectors  we used the same normalization technique as
described above  we also experimented with other
figure    k means on expanded feature vector
normalization methods throughout  but the results
did not change significantly 
an example run of k means is shown in figure   
the clusters seem to be reasonably distinct and well
grouped  however  even with the tsne algorithm 
k means
the high dimensionality of the feature vectors makes
the visual evaluation of the clustering quite diffionce again  we plotted the average within cluster
cult 
distance versus the number of clusters 

mean shift
because the within cluster distance curve for k means
was not particularly compelling  we decided to experiment with the mean shift clustering agorithm    mean
shift does not presuppose the number of clusters as
an input to the algorithm but rather takes a bandwidth parameter which affects the convergence rate
and number of clusters  before implementing any
sophisticated choice of bandwidth  we experimented
with a range of values  we in turn found that there
was no bandwidth parameter that produced any reasonable clustering  if the bandwidth parameter was
too large  then there would be only one cluster  and
if too small  there would be only singletons  furtherfigure    average euclidean distance from nearest cen  more  in all of the intermediate values there was one
dominant cluster and the remaining clusters would
troids 
be all singeltons or memberless centroids  figure  
as can be seen in figure    this distance curve does   using a matlab implementation written by dr  laurens j p 
not have a natural knee as the initial distance curve
van der maaten of delft university of technology
had  unfortunately  this makes a choice of k slightly   using matlab file exchange submission by bart finkston

page   of  

fishows an example plot of such  for this application 
the mean shift algorithm proved a worse method
than k means 

jcs

hon

tls

lin

ctn

nym

san

chi

ido

sfo

ctn

dal

okl

cle

det

den

nas

pit por
kan

atl

phx

anc

ral

boi

aut
lek

was

miw

cuo
stl
abu
coo

phi

sea

hon iow

slt

ral

maw
tpa

neo

cin

orl

hou
ron
bir

jaf
min

oma
sat

des
lit

lax
bos

figure    abstract representation of pseudo clusters

figure    mean shift on expanded feature vector

correlation through iteration of
k means
while the results from our trials of k means seemed
promising  the clusterings were unsatisfying for two
main reasons  firstly  the feature vectors that were
used as input to k means were in such high dimension
that plotting the clusters in two or even three dimensions in a meaningful way proved nearly impossible 
secondly  the individual runs of k means frequently
produced slightly different clusters  possibly due to
the random initialization of the algorithm and the
lack of obvious separation of clusters 

pseduo clusters are self contained  some contain overlaps with other pseudo clusters  these overlaping
cities represent cities that were strongly correlated
to both groups  even though they were not as highly
correlated  we can still interpret this to mean that
the two pseudo clusters which share some cities  are
more related than any two pseudo clusters chosen at
random 

discussion

using the small set of three meta data features 
k means clustering performed reasonably well  however  we were more interested in the rich categorical
and natural language features of the posts  we
expanded the feature vector to include categorical
data as well as average lda topic weights within
each category for each city  the clustering on this
to circumvent both these issues  we aimed to pro  expanded feature space was reasonably promising
duce clusters with a greater confidence by grouping but certainly presented some drawbacks  herein we
cities that were clustered across multiple runs  in  discuss the results in more depth 
stead of relying on a single run of k means  ran the
algorithm     times and looked at the frequency though this new feature space is certainly
with which cities were clustered together  we gen  more conceptually compelling for the purpose of
erated pseudo clusters by grouping together cities evaluating the similarity of cities  it is generally
that were clustered together more than     of the known that the easiest clustering algorithms 
do not perform well for hightime  we found that these psuedo clusters provide a including k means 
 
more stable and reliable measure of whether cities dimensional data   the reason  briefly  is that with
are similar or not by executing this process multiple random initialization  it is possible to get stuck in
times and comparing the results  figure   presents an a local optimum prior to converging to a global
example of a pseudo clustering  while not graphed optimum  this is one plausible explanation for
in a real space  this type of representation allows the less than ideal within cluster distance curve
us to understand more about which cities are re  for the expanded feature vector  ideally  the
lated to one another and also about how they are   ding et al   adaptive dimension reduction for clustering
related  for instance  we note that while many of the
high dimensional data

page   of  

fidistance curve would sharply descend  forming
a bend in the curve near lower values of k  but
in our case the curve was close to being linear 
which suggests a non optimal convergence for
most values of k  more sophisticated algorithms
for high dimensional clustering have been developed  which we discuss in our section on future work 
the mean shift algorithm performed noticably worse than k means  one possible hypothesis
is that mean shift  using a gaussian kernel  is
attempting to converge on distinct normal like
cluster distributions  and so with only    data points
it is unlikely to have separate clusters that appear to
have any normally distributed qualities about them
simply because the separate clusters would be too
sparse  admittedly  a more thorough understanding
of the mean shift algorithm is necessary to evaluate
any hypotheses of its poor performance 
lastly  we can ask ourselves what it means
for two cities to be clustered together given our
model of the feature vector  on first pass  the
clusterering does not map to any obvious features of
a city such as its geography  population size  wealth
and economic activity  while it seems tempting
to interpret this as a red flagthat something
must be wrong with the clustersin fact this might
suggest that our clustering discovers some intangible
similarity that cant be found with standard metrics 
testing the true quality of the clusters as a new
metric of similarity would require further tests which
are discussed below 

or disliked visiting a similar city in the past  where
the measure of similarity in this case could be based
on the clusters we produced  with a supervised
problem like this  we could quantitatively evaluate
whether using the clustering as an input improved
or degraded the accuracy of predictions 
furthermore  with such a supervised framework we could also perform a more sophisticated
analysis of our feature space using feature wise
cross validation  in other words  we could remove
a subset of the features  such as vehicle topics 
and observe the effect on the performance in the
supervised predictions  this could be a fantastic
tool towards better understanding the feature
space before embarking on any attempts to employ
complicated high dimensional clustering methods 
nonetheless  there is a decent amount of literature on
k means based high dimensional clustering methods 
after setting up the framework to numerically
evaluate the clustering in a supervised setting and
exploring the feature space with cross validation 
if the results continue to be promising it would
certainly be worthwhile to attempt to implement
k means based algorithms specifically tailored for
high dimensional feature vectors 
even further down the road  one exciting application of the methods we use here would be to
track the clusters of cities throughout time  because
all craigslist posts have a timestamp  extending
the methods described here  we could track the
relationships of cities throughout time  effectively
viewing not only how cities are related but also how
they trend and interact on a personal level 

future work
the results of k means clustering on the expanded
feature vector are a promising start  but a number of steps need to be taken to improve the
clustering method further as well as to numerically evaluate the results as we iterate on the method 
before moving forward and applying more
complicated clustering algorithms  an exciting and
valuable metric would be to apply the clustering
results as an input to an already explored supervised
learning problem  an example problem could be
predicting whether someone will enjoy visiting a
particular city to which they have never been  via
couch surfing or a hotel stay   for this problem  it
could be very relevant whether they have enjoyed

page   of  

fi