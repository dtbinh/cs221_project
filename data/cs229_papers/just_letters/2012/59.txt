classifying applications based on api consumption 
cs     autumn     
arne roomann kurrik
december         
is it possible to classify applications based off of
records of their calls to an api 

simple analysis  such analysis may benefit from
machine learning to expose inherent categories of
application and highlight candidates which should
be reviewed by policy teams 

providers of public apis take on the burden of
supporting the ecosystem of applications which
depend upon the api to function  insight into
the types of applications issuing queries into the
system is valuable to the maintainers of the platform 

as an employee of twitter  i was able to run
distributed queries against the api logs using
a hadoop cluster  the scale of the raw log
data required a    minute job to pull records for
a single hour  the final training dataset contained         applications which made requests
to api twitter com during a period of october
 th        the final test dataset contained       
applications from a period of october   th      a week later 

this paper is concerned with two use cases addressed by classifying applications into categories 
the first addresses identifying applications which
are considered to be abusive users of the platform 
this can be modeled through a binary abusive
and not abusive classification 

a log line contains a request to a single path  e g      statuses update  on the
server  some normalization is performed  e g 
   statuses destroy  id   but the set still
contained       unique paths 

the second is categorization of applications into
distinct groups for the purposes of identifying application verticals  such verticals of similar applications can be used to tune rate limiting  plan
new features  and gain insight into the ways the
platform is being used which may not be totally
intuitive from an initial inspection 

to assign a score to the entries in the test and
train dataset  i used up to date lists of suspended
applications  there were     applications which
had made calls in october which had been suspended since 

examining this problem required collecting and
identifying characteristics of application usage
logs  experimenting with ways to clean the
dataset  and training classifiers to determine the
potential success rates of applying machine learning toward application classification 

the process which collected the log lines output a matrix where the columns corresponded to
individual endpoint paths  and the rows represented applications  the mi j th element of the
matrix represented the number of times application i made calls to endpoint j 

   data collection
twitters api supports an ecosystem of millions
of applications  this volume complicates even
 

fi   properties of the dataset

mobile clients also fell into the smaller clusters 
mobile clients typically display the same data and
differentiate on presentation  so this seems intuitve 

a suspended application is the result of a manual review  so the list of suspended applications
is a strong dataset for training a classifier to look
for abusive applications  however  not being suspended is not an indicator that an application is
non abusive  just that it hasnt been reviewed and
flagged  so while there are no false positives  there
are potentially many false negatives 

cluster   also showed some patterns  as the second largest cluster  it contained sequences of applications with almost contiguous ids and very
similar naming patterns 
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    
xxxxx    

reviewing an application requires a non trivial
amount of manual work  it was not very feasible to build a large  accurate dataset by hand  i
realized that a successful classifier would identify
as many suspended applications as possible while
keeping the false positive rate low enough as to be
realistic to review by hand 
the ratio of suspended to non suspended applications is very unbalanced  it is possible to achieve a
        success rate simply by writing a classifier
which says that no applications are abusive 
   clustering applications
my intuition was that an application of k means
clustering would separate applications into logical
groupings which could be individually analyzed 
however  running the k means algorithm to convergence did not result in even groupings of applications   one cluster always gained the vast majority of applications  as shown in this grouping 
k
 
 
 
 
 
 
 

size
  
 
  
   
 
  
  

k
 
 
 
  
  
  
  

size
 
  
 
 
 
 
  

k
  
  
  
  
  
  

xxxxxxxxxxapp
xxxxxxxxxxapp
xxxxxxxxxxs app
xxxxxxxxxxs tweetapp
xxxxxxxxxxs tweetapplication
xxxxxxxxxxs testapp
xxxxxxxxxxssp app tweet awsome
xxxxxxxxxxs app
xxxxxxxxxxs app test
xxxxxxxxxxs app
xxxxxxxxxxsapplication
xxxxxxxxxxs app
xxxxxxxxxx application
xxxxxxxxxxs app
xxxxxxxxxxs app
xxxxxxxxxxs application
xxxxxxxxxxs app
xxxxxxxxxxs app
xxxxxxxxxxs app
xxxxxxxxxxsapp
xxxxxxxxxxs app

even some relatively distant apps  by id  were
obviously similarly registered 

size
 
 
 
      
  
  

xxx      
xxx      
xxx      
xxx      
xxx      
xxx      

foo
foo
foo
foo
foo
foo

app
app
app
app
app
app

  
  
  
  
  
  

p
q
b
a
t  
n  

table    cluster size for    k means groups
this would seem to indicate that these apps were
registered and operated by some sort of automated system  and warrant investigation  that
the clusterings were created by analyzing usage without regard to id or application name is

there were some patterns in the groupings  for
example  all of twitters official clients were clustered into various smaller groups  many popular
 

fitwo dimensional space  which could be used for a
visualization 

promising  the sheer size of cluster    means a
different categorization approach would be needed
for the majority of applications  however 

 

x   

  

   logistic regression

 

most of the work for this analysis was spent trying to classify abusive applications  to get an intuition about where to spend my time  i wrote a
quick and dirty logistic regression classifier using
the following stochastic gradient ascent rule 

 

 

 

 

  i 
j    j    y  i   h  x i    xj
 

 
where h  x      e
t x   the sigmoid function 
graphing the performance of this classifier using increasingly large subsets of the training set
showed a slow convergence between the error rates
for the test and training sets 

 

    

   

   

   

 

 

figure     d plot of applications
by normalizing the data to zero out the mean
and variance  i was able to find the corresponding
eigenvectors and generate image   
by plotting suspended applications in red  it
seems that types of application fall into specific
linear combinations of the two principal components  applications which have been marked as
abusive tend to vary along the second principal
component  y axis in the above graph  with little variation along the first principal component
 x axis  

test error
train error

   

 

 

    

error

   

 

x   

train vs  test error  stochastic gradient ascent



 

   

samples

figure    sga performance
this was an indicator of high variance  general
strategies to address this are to increase the data
set size and to reduce the number of features used
in the model  i pulled more data for my datasets 
and planned strategies for reducing the dimensionality of the data 

   endpoint collection

a look at the features in the dataset made it
obvious that many were redundant  during the
time that the data was pulled there were two
available versions of the api  and calls were distributed across each  a simple way to reduce
features seemed to be to collect the endpoints
which accomplished the same task  e g  posting a
tweet  with different semantics  e g  update vs 
update with media  and combine the numbers of
calls into a single number 

   principal component analysis
when collecting the dataset  i really had no idea
whether the suspended applications would be located in proximity to each other  or if they would
be spread out throughout the dataset  i wanted
a way to determine whether a classifier was even
likely to find a separation between the two categories of application  principal component analysis seemed like a good way to cast the data into
 

fiendpoint
mapping
   statuses destroy 
statuses destroy
   statuses destroy  id 
statuses destroy
     statuses destroy  id  statuses destroy
     statuses update 
statuses write
   statuses update 
statuses write
  
  
 
 

the results  listed in table   indicate the    endpoints which contributed the most to classifier
accuracy  intuitively     friendships create 
is used for follower spam and appears to be the
best single endpoint feature for classifying abusive
apps  the    followers ids endpoint would
also be useful for identifying possible targets for
spammy follow requests 

table    logically grouping endpoints
   scoring
as mentioned earlier  it would be possible to write
a classifier with         accuracy simply by asserting that every app is not abusive  my experiments with logistic regression were yielding    
accuracy  so i investigated different scoring mechanisms which may give better insight into how
well a given classifier was doing 

this was best accomplished via a manual process 
reviewing      endpoints by hand seems like a
lot of work  but many contained ids which had
not been collected properly by the logs processor 
many contained mistakes or typos made by the
application  and some were otherwise restricted or
internal endpoints made by official clients  this
process reduced the number of features to    

precision and recall are useful for the unbalanced
data set case 
tp
tp   fp
tp
recall  
tp   fn

   feature selection and forward search

precision  

another approach for reducing the number of features was to identify which endpoints contributed
most toward reducing the error of a classifier  to
implement forward search  i generated one dataset
per feature  at this point having culled out typos  unparsed ids  and restricted endpoints  and
saw which endpoint produced the best logistic regression score  keeping that endpoint  i created
datasets with   features  and ran the classifier
again 
i
endpoint
     friendships create 
     followers ids 
     statuses friends timeline 
     statuses home timeline 
     users show 
     account totals 
     statuses friends 
     statuses update 
     account rate limit status 
      statuses update with media 

i desired a single value metric  though  so looked
into f  score 
f      

precision  recall
precision   recall

when investigating f    i also came across
matthews correlation coefficient  c   which was
supposed to be useful for ranking the performance
of binary classifiers even in cases where classes
were of very different sizes 

error
        
        
        
        
        
        
        
        
        
        

tp  tn  fp  fn
c  p
 tp   fp  tp   fn  tn   fp  tn   fn 
in practice  i found matthews correlation coefficient to be most in line with my intuitions
about well performing classifiers  in that classifiers which appeared to be doing a good job had
higher coefficients than ones which appeared to be
doing poorly 

table    forward search for api features

 

fimodel
endpoint collection
feature selection
 d pca
all endpoints

   support vector machines

using libsvm  classifiers were trained against each
type of feature reduction 

fp
   
   
   
  

c
              
               
               
                  

table    comparison of models for svm

i paid special attention to the number of false
positives each classifier returned  by adjusting weights given to each category  the svmgenerated model could be tweaked to return more
or fewer false positives as needed  it was important to develop a model which identified as many
abusive applications as possible  while keeping the
false positive rate below a quantity which would
be appropriate for manual review  i estimated
that a review might take    minutes for an experienced reviewer with appropriate tools     
reviews would cost    reviewer hours  so over two
weeks of work for a single person  i rejected models which returned significantly more false positives than this 

threshold
   
   
   
  
 

tp
tn
        
        
        

fp
    
    
   

fn
  
  
  

  
  

   
   

  
  

   
   

     
     

table    endpoint collection counts

i was also sometimes able to tune the amount of
returned results by changing the threshold of the
svm score past which a positive score would be
awarded  surprisingly the only model which really differed much here was the endpoint collection method 

threshold
   
   
   
  
 

p
     
     
     

r
        
        
        

f 
mcc
              
              
              

   
   

     
     

        
        

        
        

     
     

    

table    endpoint collection scores

    
    
endpoint collection
feature selection
pca
all endpoints

    

mcc score

   

    conclusion
machine learning appears to have merit for classifying applications  particularly in the context of
surfacing candidates for eventual human review 

    
    
    

while not perfect  the current svm results represent a pool of applications realistically sized to be
able to review by hand 

    
 
    
   

   

   

   

   
   
   
classification threshold

   

   

 

figure    threshold vs mcc score for svm
ultimately the endpoint collection method produced the best results out of all of the models 
the result was a svm classifier which identified
    of the suspended applications in the test set
of        applications  with     false positives 
 

there are a few steps which could be used to improve classifier accuracy while reducing the number of false positives returned  more complicated kernels may be able to better categorize
the data  and a system which took the output of
human reviews  in particular applications which
were marked as candidates and not suspended  in
order to retrain its model would have a cleaner
dataset to work with 

fi