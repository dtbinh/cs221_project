predicting bank default  december       stanford university

predicting bank default from the
quarterly report
caiyao mai and sunyoung baek
cymai stanford com sbaek stanford edu
abstract
possibility of a bank default in the quarter after the lastest financial reporting can be estimated from
the banks balance sheet in relation to the general economic situation of the time  although creating
the exact formula is not at all clear  it is possible to construct an algorithm that produces little error
in categorical default prediction given bank data and economic indices  this paper seeks to suggest a
machine learning based approach to predicting bank default based on the publicly available information 

i 

introduction

uring the financial crisis of       the
world has seen that solvency of financial institutions must be tightly regulated by the federal government  it is to ensure
that no taxpayers money is to be squandered
to cover up the banks poor financial management  like in the lehmans case  banks financial problems are so well hidden that their
default is difficult to predict without insider
knowledge  sudden cascades of default shock
shareholders  call for a series of bailouts with
taxpayers money  and cause far reaching impacts on both large and small businesses  recent financial crisis has highlighted the importance of estimating each banks susceptibility to
economic recession and forcing the banks that
are likely to default to address their balance
in advance  as a first step  we aim at creating a machine learning algorithm to predict
whether a bank would default in the following
quarter based on the latest financial reports
and economic indices  this algorithm will be
very useful if we can lengthen the period of
prediction from a quarter ahead to a year  or
even a few years ahead  for the moment  this
paper focuses on the quarter long prediction
only 
in section    we begin by processing data
as preliminary steps  in section    we outline
our cross validation scheme and the models
that we will test  in section    we do feature

d

selection using different methods  in section
   we present the test errors for each model 
choose the optimal hypothesis  and obtain the
hypothesis error by running it on the entire
dataset  in section    we conclude the paper
with a note on the further study 

ii 

preliminary steps

we have      data points  each of which contains quarterly financial report data from a
period within             and economic indices of the corresponding quarter  each data
point is matched with y which is   if the bank
defaulted in the quarter following the latest
reporting  from which the information in x
variable had come from  and   if not  some
of the data points are from the the same bank
but in a different reporting period  economic
indices that we use in the prediction are published every quarter  rows in our final design
matrix x indicate different data points  and
columns indicate different variables  in the entire dataset  we have    defaults in total which
means our default rate is        

ii  

choose a set of variables to begin with

we are originally given    variables but we
take out the ones that are obviously redundant 
for instance  we have both eq and eqpct  which
 

fipredicting bank default  december       stanford university

mean total equity and total equity divided by
total assets  respectively  in this case  we choose
to include eqpct only because we prefer to represent each data point in relative terms instead
of absolute terms  in the end  we have the following   bank variables and   econmic indices
for each reporting period  columns in x 
 deppct  ps  not dep   deposits percentage
 eqpct  equity percentage
 rbc rwaj  tier   risk based capital ratio
 rbcrwaj  total risk based capital ratio
 p assetpct  percentage of assets    to   
days pass due
 nclnlspct  percentage of noncurrent
loans and leases
 scpct  investment securities percentage
 voliabpct  volatile liabilities percentage
 roa  return on assets
 libor m  london interbank   month
borrowing rate
 libor m  london interbank   month
borrowing rate
 libor m  london interbank   month
borrowing rate
 csw  case shiller home price index
 cpi  consumer price index published by
u s  bureau of labor statistics
 unemploymt  monthly national unemployment rate published by u s  bureau
of labor statistics

iii 
iii  

method and strategy

algorithm outline

using the logistic regression model  we run
cross validation for each feature set and obtain
the optimal variable set  we assume that the
selected features are inherently significant so
that the choice of the optimal set does not
depend on whether we use svm or logistic
regression  though the resulting error might
vary   we have the following algorithm 
 

for each feature set  
k fold cross validate  
divide x to train set and test set
obtain optimal hypothesis in the model
based on the train set
compute error on test set
 
 
after getting the best feature set  we are testing
which model to use 
for each model  
run cross validation
return cross validated error
 
test optimal model with the optimal variables
in the entire data points to obtain optimal
hypothesis and the hypothesis error 

iii  

models

in the beginning  we considered the three models  simple logistic regression  support vector
machine with five different kernels  and naive
bayes  however  when we tested each model
on the entire dataset to get a rough idea  the
error we obtained from the naive bayes model
was over      so we decided not to include
naive bayes in the further research process  we
test the logistic regression and the support
vector machine in in the following sections 
for the support vector machine  we consider
linear  quadratic  polynomial  gaussian radial
basis function  and multilayer perceptron kernels  in this case  we return the optimal kernel
along with cross validated error to be run on
the entire data points 

iv 

feature selection

for feature selection  we have two approaches 
the first approach is     to use principal component analysis  pca  to reduce feature dimensions and then     run feature selection algorithm  in this case  we add one more method
of feature selection by choosing the first n prin 

fipredicting bank default  december       stanford university

ciple components  the second approach is to
run feature selection directly on the original
dataset  for both approches  we use three methods to evaluate the features   a  mi scoring  b 
forward search and  c  backward search  in every case  model errors in the feature selection
are obtained by using    fold cross validation
with the linear regression model 

iv  

principal component analysis

this method applies only to the pca feature
selection approach  after converting    feature
vectors to    principle components  we run logistic regression cross validation on first to nth
principle components  thus  n                   
we choose the n for which model error is the
lowest  we obtained    principle components
as a result 

iv  

mi scoring

we use mutual information to see the correlation between each feature and y  since our features are continuous variables  we assume that
our features are jointly gaussian distributed 
firstly  we rearrange our data to be three
sets  the original data  the data with label y    
and the data with label y      secondly  we
use matlab library function to fit each feature
to the gaussian distribution for each sets and
got the corresponding mean and covariance
for each gaussian distribution  this means we
got p  x    p  x  y      and p  x  y       and we
change the formula for the mutual information
to be 
mi   xi   y   

r



y      p  xi  y  p y log

p   xi   y   p   y  
p   xi   p   y  

we do mi scoring for principle components set
and original dataset separately  we choose the
top k features which minimize the error rate 
table    mi scoring

data
pca
original

number of features

error

 
 

      
      

the result shows that the algorithm chose
the first   principal components except for the
 th  for original dataset  the model chooses
the features with the following index             
             

iv  

forward search and backward
search

from the forward and backward search  we
obtain the following result 
table    forward seach

data

number of features

error

 
 

      
      

pca
original

for pca  the result shows that the first   
principal components except for the  th and
 th are chosen to be included  for original
dataset  the  nd   rd   th   th   th and   th
features are picked up 
table    backward seach

data

number of features

error

 
  

      
      

pca
original

for pca  the first   components except for
the  th are chosen  for original data  the model
chooses features with the following index       
                              

iv  

decision

from the above analysis  we can see for each
feature selection approach  we got a better result from the pca  the error rates for three
methods are very close because the three feature sets are very similar  so we run the crossvalidation for each feature sets and compare
them with the precision and recall rate  we got
the following table 
 

fipredicting bank default  december       stanford university

table    comparing pca features

feature set

precision

recall

kernel

      
      
      

      
      
      

linear
quadratic
polynomial
gaussian
perception

x mi
x forward
xbackward

for predicting bank default  we want to
obtain the best precision because we want to
minimize the false alarms as much as possible 
since much of a banks business is based on
its clients trust on its security and stability 
false alarm might be a self fulfulling prophesy
which jeopardizes financial world even more 
for this reason  we pick the pca features obtained by backward search as our feature set 

v 

table    support vector machine cross validation

model selection

we run logistic regression and support vector
machine models and obtain the following error
rate  precision and recall rate 

error

precision

recall

      
      
      
      
      

      
      
      
      
      

 
      
      
      
      

based on the result  we decided that the
logistic regression creates a model balanced
and stable error  precision  and recall rate  according to the simple logistic regression result 
we have both low generalization and crossvalidation error  and precision and recall rates
are stable around high    percent  polynomial
kernel in the svm  on the other hand  have a
very good generalization result but less promising cross validation result  we suspect that
optimal number of features might be different
for svm model  since its selection was based
on logstic regression model  and we might be
doing overfitting in polynomial kernels case 

table    simple logistic regeression error

generization error
      

cv error
      

generization
cross validation

result and analysis

we got the following learning curve for our
variable set and the linear regression model 

table    slr precision and recall

test set

vi 

precision

recall

      
      

      
      

table    support vector machine generization

kernel
linear
quadratic
polynomial
gaussian
perception

 

error

precision

recall

      
      
      
      
      

      
      
      
      
      

 
 
 
 
      

general
and the error rate  precision and recall rate
for both generization and cross validation of
the whole data set are in table   and table   

fipredicting bank default  december       stanford university

vi  

learning rate

the followings are the learning plots for logistic regression and svm  we plotted crossvalidation error result for a randomly chosen
sample dataset with size n  n is indenpendent
variable in the below plots  since our default
rate is low  our cross validation is subject to
random effects that creates variance in error
rate  however  in general we can see that error
rate decreases  note that th error rate doesnt
improve much as n goes close to      and that
svm learning curve is not much better than
logistic regressions  for the given dataset 
we might need a different kernel function to
have effective svm result  furthermore  since
the feature selection process used logistic regression model  it might be the case that the
optimal set is different for svm especially if we
use higher dimensional kernel  if time allows 
we should repeart the feature selection process
for svms with different kernel and see how
the result goes  also  we might want to change
the data format to have more effective learning
curve 

vi  

after selecting the optimal features and optimal model  we analyzed the data points for
which our model made a false prediction  we
observed the following two types of mistakes 
    false positive for the quarter prior to the
banks default
    false negative for the first quarter of
bank default
the problem is due to the fact that we have
multiple data points from one bank  a series
of reports from one bank are closely related 
bank balance shows ominous signs when its
default comes close even though the response
variable y does not catch that  their balance
in the defaulting quarter and a quarter prior to
the default are very similar  and our algorithm
often makes mistake classifying them 

vi  

linear regression

false prediction

default rate

our dataset has a very low default rate  gathering default data is inherently difficult because
one bank defaults in one quarter only  since
we take take multiple data points from nondefaulting periods but end up with just one
default quarter  we cannot gather comparable
number of dataset for each  also  majority of
banks do not default at all  because we have
a very low default rate  our cross validation
result varies widely sometimes  having no default at all in the k fold and uniformly predicting non default to get a zero error rate  by
changing the data structure  we might be able
to form a dataset with higher default ratio 

references
supporter vector machine

 ng  andrew  lecture notes of cs           
 

fi