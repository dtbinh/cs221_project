the k discs algorithm and its kernelization

michael lindsey
a new clustering algorithm called k discs is introduced  this algorithm  though similar to
k means  addresses the deciencies of k means as well as its variant k subspaces  the k discs algorithm is
applied to the recovery of manifolds from noisy samplings  a kernelization of k discs is exhibited  and the

abstract 

advantages of this modication are demonstrated on synthetic data  for this project  the kernelized and
linear algorithms were implemented from scratch in matlab 

  

introduction and motivation

a clustering algorithm is usually run on a data set under the assumption that there is some structure of
various connected components underlying the data set  or to test whether this is a good assumption   suppose
that a set of n dimensional data has been sampled  with noise  from a smooth submanifold  potentially with
boundary  of rn   one wishes to recover  in some sense  the underlying structure of the data  given only the
data itself  arguably the most fundamental aspect of this structure is the characterization of the connected
components of the manifold  manifested as clusters in the data set  unfortunately  standard clustering
algorithms such as k means and expectation maximization often fail to properly characterize the shapes of
these components  kernel k means succeeds in many circumstances in clustering data that is not separable
by ordinary linear k means  however  due to its zero dimensional characterization of clusters  it fails to
identify clusters drawn from elongated subsets of submanifolds 
k  subspaces is a variant of k  means which seeks to minimize the total distance of the data from a set of k
d dimensional subspaces  the kernelization of k  subspaces achieves good results in identifying clusters drawn
from nonlinear d dimensional submanifolds  in particular  submanifolds without boundary   however  the
innite extent of this algorithm s clusters can result in awkward clustering  for example  k subspaces will
have diculty in identifying bounded clusters drawn from the same subspace  or the same submanifold  
  

the k discs algorithm

in short  the k subspaces algorithm attempts to minimize the total distance of the data from a set of
k d dimensional discs  since the extent of data is nite  we do not pay a great price for viewing clusters
as bounded subsets of subspaces  in fact  this algorithm can be seen as a renement of both k means and
k  subspaces in that it 
 can identify each point with a cluster centroid  the center of its assigned disc  or with its projection

onto the subspace containing the disc

 endows clusters with some  non zero dimensional  notion of shape and scalar measure
 allows for multiple clusters within or near the same subspace 

  

algorithmic details

the k discs algorithm consists of the following steps  all points are in rn   and we are given a data set
 x           x m    
    for a given k  number of clusters  and d  dimension of cluster discs   initialize k cluster centroids
j and a random set  ujl   of d orthonormal basis vectors for each cluster j   select initial values for
the disc radii rj  
    repeat until convergence   
   

date   december          

 

fithe k discs algorithm and its kernelization

 

 i 
 i 
 a  for all i  j   let x i 
 j   compute the distance aij of xj to the subspace spanned by
j   x
 ujl    equal to the distance of x i  to the ane space generated
by the
e j  th cluster disc  
pd d  i 
 i 
 i 
we see that aij     x i 

p
 x
   
 
where
p
 x
 
 
x
 
u
j
j
jl  
j
j
j
j
l  
 i 
 b  next  for all i  j   compute the distance bij of pj  x i 
j   to the origin  bij     pj  xj      we see
 i 
that if bij  rj   then the distance d i  j  between x and the j  th disc is aij  
 i 
rj
if bij   rj   then d i  j      x i 
j  bij pj  xj     
 c  for all i  set c i     arg min d i  j  
j
 d  for all j   set

j   

pm
 i 
  j x i 
i     c
p
 
m
 i 
  j 
i     c

 e  for all j   set  ujl   to be the set of d principal components of  x i   j   c i    j    this step
involves pca  i e   nding the d principal eigenvectors of each cluster s data covariance matrix  
 f  for all j   set rj    max
  pj  x i   j      noting that the meaning of pj has changed due to the
 i 
 

update to  ujl   

i c

 j

  

convergence

we can guarantee that k discs converges to a local optimum  repeated random initializations are recommended
to approximate the global optimum  in specic  we dene our objective function j c i    j   ujl   rj    
pm
   i 
 is the j  th disc  which can be viewed as a function on j   ujl   rj   using the
i   dist  x   ci    where
p j
 i   
notation from above  j   m
i   d i  c     the value of this function is non increasing over all iterations 
to see this  we can view the k discs algorithm as coordinate descent on the arguments of j    note that
the arguments of j are  more precisely   c i      j     ujl     rj    though we will omit the curly braces for
visual clarity   suppose that at some iteration our parameters have values c i    j   ujl   rj   we will write
their respective updated values as c  i     j   u jl   rj    note that in step  c  we set c  i     arg min d i  j   thus
d i  c  i      d i  c i     for all i  and j c  i    j   ujl   rj    j c i    j   ujl   rj   

j

it is well known that pca  taking an expanded view of pca that includes the computation of means

   chooses a set of ane subspaces  or rather  values for j and ujl   which minimize h c i    j   ujl    
pj m
   i 
 
 
i   dist  x   sci    where sj is the ane subspace containing the j  th disc j    sj and j will denote
the updated ane subspace and disc   so h c  i     j   u jl    h c  i    j   ujl    and of course h c  i    j   ujl   
j c  i    j   ujl   rj   because j  sj for all j   finally  observe that by the update in step  f     pc i   x i  
c i       rc   i  for all i  hence dist   x i    sc  i     dist   x i     ci    and
j c  i     j   u jl   rj      h c  i    j   ujl    j c i    j   ujl   rj   

as was to be shown  the objective function is non increasing and bounded below by zero  hence convergent 
note that we can slightly modify step  f  of the algorithm and still guarantee convergence of j   for
example  we might set rj   constant for the rst several iterations of the algorithm  then update rj as
above  in fact  we can set rj     initially  which is equivalent to running k means for several iterations 
this is a good heuristic for initializing the cluster centroids before running k discs proper and allows us to
skip the pca step until it is particularly useful  for high dimensional data  pca involves nding a  partial 
eigendecomposition of an n  n matrix  which is very expensive  note also that k subspaces can be recovered
from k discs by permanently setting rj    
allowed to run as initially described  k discs almost always converged in many fewer than    iterations
for all data sets tested  however  the convergence behavior depends greatly on the shape of the data as well
as initialization procedures  a useful initialization heuristic  to be used before any other processing  which
can enhance convergence rates and values is to select special landmark points as clusters  in particular  the
maxmin landmark selection procedure simply adds a random point to the landmark set l  then repeatedly
adds the point with maximal distance from l to the landmark set l  this procedure gives initial cluster
centers that are roughly evenly spaced  this initialization  however  may sometimes bias the clustering

fithe k discs algorithm and its kernelization

 

algorithm toward a particular set of local minima that are not particularly desirable  and in such cases 
repeated random initialization is preferable 
  

illustrative synthetic linear examples

the following visualizations on the plane demonstrate the improved capacity of k discs to recover the set
of line segments from which a random data set is drawn 

above left      points sampled from indicated lines with gaussian noise          i   above right  result of k discs on random
sampling  below left  result of k means  below right  result of k subspaces  all clustering initialized with landmark selection 

  

application to submanifold reconstruction

given a noisy sampling from a d dimensional submanifold  k discs  using d dimensional discs  can be
used to generate a piecewise linear approximation of the manifold  the highest value of k that can be used
eectively relates inversely with the noisiness of the data  for noisy data  a value of k that is too large will
yield cluster centroids that deviate greatly from the submanifold  for  n     dimensional submanifolds  if
we repeat the k means algorithm many times and associate each data point with the mean of its associated
cluster centroids as well as the mean of its associated normal vectors  we can construct images as below 
this capacity could be of use in computer vision applications in which noisy point cloud representations
of surfaces are obtained  the reconstruction points could be used for surface reconstruction as well as
topological calculations  of betti numbers  for example  which could be used for hole detection  

fithe k discs algorithm and its kernelization

 

left  shown in red are     points sampled from the unit circle with gaussian noise         i   in blue are reconstructed points
obtained from averaging    repetitions of k discs with k      and in black are the reconstructed normal vectors passing through these
points  right  same as left  except        i   k      landmark initialization was used for both trials  similar results were obtained
for surfaces in three dimensions  such as the sphere and torus  though these cannot be visualized here  note that for smaller values
of k  the radius of the reconstructed circle is signicantly smaller than    this occurs because the cluster centroids are closer to the
origin  analagous results will occur for any surface of signicant curvature  the topology of the surface  however  will be preserved 

  

kernelization

we observe that k discs has a natural kernelization  let  be a mapping from rn to some high dimensional
feature space  and suppose that we are given a function k   rn  rn  r such that k x  y    h x    y i 
we do not work directly with vectors in the high dimensional space  rather  we can only process them
implicitly by using inner products  in particular  the algorithm requires the following inner products to be
stored in place of the vectors  x i     j   ujl  


d
e
 
 
 x i      x i       k x i    x i     for all i  i               m  these values are stored in a kernel matrix

immediately


ff and never updated 
  x i     j for all i  j   these values are updated every time the assignments c i  are modied  they
can be computed

pm
 pm
 
d
e 
  c i    j k x i    x i    
  c i    j  x i   
 i   
 i   
i  
i  
pm
pm
 
 x    j    x   
 i    j 
 i    j 
i     c
i     c
 hj   j i for all j   updated when the assignments are modied 
pm
hj   j i  

i i    

 

 

  c i    j   c i     j k x i    x i    

pm
 i    j   
i     c



ff
  x i     j   ujl for all i  j  l  this update is more sophistocated and makes use of the kernel pca
algorithm  let xjl be the matrix whose rows are  x i   c i  for all i such that c i    j   relabel the
rows  x i   c i  as x p  for p              p   regular pca considers the eigendecomposition of xjt xj  
but we cannot directly compute this matrix  it can be shown that xj xjt has the same eigenvalues
 with all additional eigenvalues equal to zero  and that the  orthonormal  eigenvectors vjl of xj xjt


ff
 
relate to the corresponding eigenvectors ujl of xj by ujl   jl   x t vjl   so  x i     j   ujl  
ff


 
c i   l hy  vjl i  where the p th component of y is yp    x i     j   x p    which can in turn be

written in terms of inner products computed above 

the computations in the description of the linear algorithm above can all be carried out using these inner
products  but the details are unenlightening and will not be reproduced here  the steps of the kernel k discs
algorithm  including the calculation of rj   follow from those of the linear algorithm 
we note that the initialization of kernel k discs is somewhat modied  first  cluster centroids are randomly
selected
from


ff the original data  then mapped into the high dimensional feature space by   we store
 x i     j and hj   j i as above  then clustering assignments are made as in kernel k  means  i e   using
distances in the high dimensional
space from ffthe cluster centroids   then kernel pca is performed on each


cluster as above to compute  x i     j   ujl for all i  l  the disc radii are then updated in the usual way 

  

illustrative synthetic nonlinear examples

the following visualizations demonstrate the advantage of k discs in a particular situation 

fithe k discs algorithm and its kernelization

 

all images show     points sampled from three quadratic curves with gaussian noise         i   above left  result of kernel k discs
clustering on random sampling  polynomial kernel of degree    above right  result of kernel k means  same kernel  below left  result
of kernel k subspaces  same kernel  below right  typical result of linear k discs  it is of interest that even though the data is separable
with respect to the linear k discs algorithm  a perfect classication is almost never achieved  random clustering was used for these
trials   landmark selection guaranteed poor results in this case  

  

conclusions and further work

the k discs algorithm and its kernelization have shown promise in their intended goal of clustering data
sampled from bounded submanifolds  of course  one disadvantage  shared with k means  of the algorithm
is that k must be selected as a parameter  furthermore  d must also be selected as a parameter  and all
discs are constrained to have the same dimension  a more sophistocated version of the algorithm might
automatically select dimensions for its cluster descriptions and allow for discs of dierent dimensions 
finally  it remains to be seen how k discs will perform on real world data  an application of the algorithm
to three dimensional point cloud data could demonstrate the feasibility of using the algorithm for surface
reconstruction  it would be interesting to see the results of topological algorithms such as javaplex    on
such reconstructions  in addition  linear and kernel k discs could be applied to arbitrary clustering problems
and compared in performance with k means  k subspaces  and any of a multitude of clustering algorithms 
clustering problems are diverse  and the applicability of k discs in any particular scenario depends mainly
on whether the desired clustering has the structure of an ane subspace  either in the original feature space
itself or in a high dimensional feature space that respects a kernel  for example  the success of li and
fukui    in performing facial recognition with linear and kernel k subspaces suggests that k discs could have
similar  if not better  results for the same type of data 
assuming that a data set does have this structure  k discs may sometimes fail converge to or even approximate a global optimum  it would be worthwhile to try to develop some adaptive initialization techniques
that improve initialization based on the converged values of the objective function 
references

    a  nielsen  a kernel version of spatial factor analysis          
    a  tausz  m  vejdemo johansson  and h  adams  javaplex  a research software package for persistent  co homology          
software available at http   code google com javaplex 
    d  wang  c  ding  and t  li  k subspace clustering          
    g  carlsson  topology and data        
    h  adams and a  tausz  javaplex tutorial  
    x  li and k  fukui nonlinear k subspaces based appearances clustering of objects under varying illumination conditions          

fi