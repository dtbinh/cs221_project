time series analysis  the effect of adding an unsupervised
layer to nn time series prediction
david seetapun
december         
introduction and synthetic data
let  yt   be an observed time series where the interval between observations is fixed  we consider models
which take a window of length d of the time series so that ys d is the response to hys   ys             ys d  i  we
will train the models to predict the next observation in the time series  in order to predict an observation
some number of periods f in the future we will predict the next observation and iterate the predictor  the
method of stochastic sampling may be also used     but we will not consider it here 
p
q
x
x
an arm a p  q  process is given by yt  
i yti  
i ti   t   following     we start the investigation with synthetic data as we will then have solutions for the distributions of the the yt and we can
then compare e yt   to the model predictions  in what follows yt is a sample of         arma      with
t
x
            and              also we add     to each element so that zt  
yi will have a non constant
mean 
arma       gaussian process models and ffnn
we use a set of       observations  split by random selection into     training set      validation
set for the nn  the test set consists of the first     observations  we fit a gaussian process model with
exponential kernel  by optimizing the hyperparameters   a ffnn with hidden layer size   and window size
  and an arma      model  the optimal predictions were obtained from the fitted arma      model 
the results are shown in figure   below 
the ffnn produces the optimal predictions  the gp is unable to model the non zero mean  as expected for most kernels   since gp with standard kernels cannot model non stationary processes  we will
not consider them further 
arima        and ffnn
t
x
yi which is non stationary  we train the ffnn directly without
we consider the times series zt  
differencing to the stationary yt   the results are in figure   for different size windows 
there is much more variability in the final model arising from with starting with random weights  from
a qualitative standpoint  we will interpret this as the increased difficulty of modelling the joint distributions
of the integrated time series 

autoencoders
we know from the above that there is a representation of the time series zt   namely zt  zt  that is
easily modelled by a small ffnn  so we can now ask if we can arrive at such representations with unsupervised learning and in particular if the method of training the layers of a multiple layer ae recursively
results a good final fit for our synthetic data set 

 

fiwe train two ae with one and two hidden layers each and then use the encoded data for the supervised
training of a ffnn with hidden layer size    no further fine tuning is done  nn indicates where we train
the overall network corresponding to one ae stack as a single network  the results are below in figure   
we see that the method of training each layer separately  does not produce a better result than training
the network as a single entity  further  if fine tuning the ae networks starting from the parameters obtained
by training each layer separately are in the same basin of attraction as the parameters obtained by training
the network as a whole  we will obtain results as shown on the plot 
we also plot the decoded representation of the time series along with the original times series in figure
   we see that the effect of the ae is largely to smooth the time series  this will not remove the predictive
difficulties associated with the nonstationarity  this concludes our experiments with synthetic data which
we used to evaluate the difficulty of training nn with and without ae on data from a well understood time
series  we will use our understanding of the tools and techniques employed above to investigate observed
data 

global energy forecasting competition   wind forecasting on kaggle 
initial experiments and observations
the data set and predictive challenge is described at www kaggle com c gef     wind forecasting  the
problem is to predict the power output of seven wind farms in given    hour windows from    hour wind
forecasts  training data is provided in which    hour wind forecasts are given every    hours  the test data
only has forecasts that would be available when the test prediction period starts  the wind forecasts are
given as x and y  planar  components of the wind  and the polar  r    representation is also included in
the data set  or in the notation of the datasets provided u v ws wd  
as a first experiment  we assume that the observed wind is normally distributed around the prediction
with the variance increasing with the lead time  justifying the use of the latest prediction as the best
estimator of the wind speed at any time a power prediction was required  the feature vector is then the
x  y  r   of the latest forecast  we also add windows of previous power observations to each of the seven
predictors  the results are given as window sp 
window sp
 
 
  
  
  

rmse
       
       
       
       
       

window tp
 
 
  

rmse
       
       
       

window tf
 
  

rmse
       
       

int 
      
       
      
     
     

rmse
       
       
       
       
       

from this it is clear that using the windowing technique with power observations to capture autoregressive
properties of the time series of power generation does not increase the accuracy of the predictor 
now we predict the power output of each of the seven wind farms simultaneously using the seven most
recent predictions in an attempt to capture any correlations that may be present  this predictive method
yielded rmse of         or        on the leaderboard  the top score was          indicating that we
are benefitting from the covariance structure of the seven farms  windows consisting of previous power
observations and forecast observations are added  the results are window tp and window tf 
to refine our predictive model we have to address the issue related to the lead in the wind forecast for
a required forecast date  because of the way the forecast data is supplied  any date in the training set will
have a forecast that is at most    hours old but dates in the test set can have most recent forecasts that
are    hours old  as a next experiment we will construct a test set where the lead in the forecast time is a
feature of the training examples  this only results in a small improvement to         
we will use another approach to refine our model  until now we have supposed that the observed wind
is normally distributed around the latest prediction  the way that the wind forecasts are presented means

 

fithere are times t where the prediction for t has a long lead but t is very close to a date s for which there
is a prediction with lead of one hour  giving a very good indication of the wind at time s   using a simple
linear interpolation to modify the forecasts does not improve the model  which obtains          thus we
introduce a decay term so that the influence of the s forecast is more local  thus if ft is a recent forecast
for time t and ft j is an old forecast for t  j we use kej ft       kej  ft j for the forecast at
t  j  with such a term  the results are in the table above under int we also add the time of day
and day of year to capture seasonal effects  with        we obtain an improvement to          in what
follows we will include these seasonal features 
random forests and boosted decision trees
rf 
    
   
   
   

rmse
       
       
       
       

gbm  cv
    
    
     

rmse
       
       
       

having settled on feature vectors which give good performance  we will experiment with the underlying
regression technique  using a two layer nn has no effect on performance nor does predicting each farm
individually from the whole feature vector  using the same  parameterized interpolation above with a
random forest of     trees we obtain the results in the table  with gradient boosting we show the results
in the table with   fold cross validation 
autoencoders
in an effort to further improve performance  we will address the issue of the lead in the forecast  we
will use an autoencoder on the time series of the most recent forecasts  we will do this for each component
separately and then recalculate the polar representation  the motivation here is to obtain a representation
of the forecasts in the test sets consistent with the forecasts in the training set and also interpolate the
data as above  this results in         which is comparable with linear interpolation with no decay factor 
in what follows we use the  interpolation scheme above as we require the smoothing to be local for good
performance 
window  tp
 
 
 

rmse
       
       
       

window  ae 
 
  
  

rmse
       
       
       

window  ae 
 
  
  

rmse
       
       
       

if we use as a feature vector the planar forecast of each wind farm  we can again use the windowing technique to provide a temporal context  however the length of the feature vector will be    times the length of
the window  here we omit the polar representation   for small window lengths the results are in the table
above  for larger window lengths  we use an autoencoder on the features for dimensionality reduction and
non supervised feature selection  the autoencoder is trained with l bfgs and we optimise over the whole
training set and not smaller batches as the number of examples is small  for one and two hidden layers 
the results are under window  ae  window  ae  respectively 
conclusions
the best result is achieved with a random forest and  interpolation giving rmse          this result
places in the top      from the experiments for both the wind data and the synthetic data above  we failed
to achieve any performance gains with multilayer nn with the use of windowing to provide temporal context 

 

fiit may be that the method is more suited to time series where there are more directly observable predictive
variables rather than time series where there are few predictive variables such as he ones considered here 

references
    volker tresp  reimar hofmann  graphical models  foundation of neural computing  eds  m jordan
and t  sejnowski mit press  massachusetts       
    jerome t  conner  r  douglas martin  l  e  atlas  recurrent neural networks and robust time series
prediction  ieee transactions on neural networks  vol    no    march      

arma     
optimal prediction
ffnn w   h  
gp

   
   
 
   
   

   

   

   

   

   

   

   

   

figure    ffnn and gp
  

 

arima     
optimal
w   h  
w    h  
w    h  

 

 
   

   

   

   

   

   

figure    ffnn and arima

 

   

   

   

fi 
arima     
optimal
  stack
  stacks
nn

 
 
 
 
   

   

   

   

   

   

   

   

   

figure    ae predictions
   

arima     
ae decoded

 
   
 
 

  

  

  

  

  

  

  

figure    result of ae

 

  

  

   

   

   

fi