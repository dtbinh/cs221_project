qwop learning
gustav brodman
ryan voldstad
   introduction
     qwop
qwop is a relatively well known game made in adobe
flash  available for free on the internet  it was created by the
british hobbyist game maker bennett foddy  in qwop  the
user plays as a sprinter whose goal is to run a     meter
dash  the game is notoriously difficult and beating the game
has been used online as an example of something that is
impossible the main difficulties in qwop come from the
physics engine and the control scheme available to the
player 

     why qwop
on the surface  qwop learning seems like a silly project 
though difficult  it s not a famous problem or industry
staple  but it is still important  though it s just a game 
solving qwop can help us learn about solutions to many
other problems in machine learning 





simulating mechanical motion helps in creating and
automating bipedal robots
having such a multidimensional action space
 positions of joints  limbs  and their velocities can
all be different and depend the result in different
ways depending on the others 
in general  it can help how us how various learning
algorithms can help solve complex problems 

so even though the game is silly and the project seems silly 
by working on the problem  we can gain a greater knowledge
and understanding of complex machine learning problems 
especially in the fields of motion and higher dimensional
input 
     general strategies

left  qwop  right  our qwop simulation

       control scheme
qwop gets its name from the controls available to the user
trying to run the     meter dash  instead of usual control
mechanisms  the user is given direct control over four
muscle groups  q and w move the runner s left and right
thighs forward  and o and p move the runner s right calves
backward  with the right motions and inputs  this can be
used to simulate actual running  however  this is not how
we  as humans are used to running  we don t generally think
about how specific muscles have to move in order to
maintain balance and move forward  it just happens
naturally  this means that the user s collective knowledge on
balance and movement is essentially ignored  figuring out
how to mechanically move each limb is part of the difficulty 

we want to use reinforcement learning to make an ai
capable of beating qwop  just like how we ve seen
reinforcement learning applied to complex physical tasks
such as driving a car  flying a helicopter  or even just flipping
a pancake  simulating a running motion as an mdp is
difficult  so we ll have to have ways of dealing with infinite
state spaces  in particular  we d like to use 



discretization of the state space with regular value
iteration
fitted value iteration using a set of reward features 

though these only scratch the surface of techniques
available  we figured we d try these to see how well we could
apply them to a complex problem 
   relevance   other research

the task of learning to play qwop may be seen as an
optimal game playing problem or as a parallel to a robotic
       physics engine
motion planning problem  we would like to emphasize
qwop uses a physics system known as  ragdoll physics  
connections to problem in the latter category  under this
this means that normal physical interactions are greatly
lens  the game itself can be viewed as a model for the
simplified  in particular  this means that any muscle that isn t potential real world task of getting a robotic biped to learn a
being directly stimulated is dormant  meaning it just falls in walking   or running   motion  rather than data gathered
the direction that it happens to already be traveling  this is
from sensors about the positions  speeds  or acceleration of a
useful because it makes the system simpler  able to be
robots limbs  we can instead use less noisy data inferred
simulated in an internet browser game run by flash  but it
directly from running qwop 
makes the physical world less intuitive  if the runner gets
slightly out of balance  generally  he falls without the user
in many ways  learning to play qwop is a dramatic
being able to help at all  this  combined with the control
simplification of the task of learning bipedal walking in
system  means that the game is very unintuitive and
robotics  this task has been the subject of considerable
unforgiving 
research  as in morimoto et al     and tedrake et al      

fithis is often seen as a harder task  met with less success 
than learning other types of robotic motion  such as walking
using more legs or flight  this has to do with the inherent
impracticality of bipedal motion  stability is difficult and leg
motions must be finely coordinated with each other and with
information about foot placement  traction  slopes  etc 
getting physical robots to learn a sprinting motion has
proven especially difficult  as tedrake et  al describes 
 a chieving stable dynamic walking on a bipedal robot is a
difficult control problem because bipeds can only control the
trajectory of their center of mass through the unilateral 
intermittent  uncertain force contacts with the ground    
learning to walk or run in a simulation such as qwop
simplifies many aspects of the problem  in fact  learning first
against a model such as this would often be the first step in a
real bipedal motion robotics problem  first  since were not
using a physical robot  it is possible to run a vastly greater
number of tests and generate more training data  each
simulation can be run at speeds much faster than real time
and be run unsupervised  additionally  much of the
uncertainty and complications of real world motion planning
are eliminated when using a simple model  to name a few 
the effects of the actors choices are better understood and
more reliable  the model is deterministic and the range of
state spaces is dramatically reduced  the range of the actors
possible decisions is also just   controls  as opposed to a
more realistic scenario  varying the power output on each of
the motors in a physical robot  lastly  were modeling a
much more controlled environment than in real life  a
perfectly flat surface with consistent traction  with no
barriers or obstacles 
   building a model of qwop

     motivation
our original idea was to have a reinforcement learning
system integrate with the actual flash game on the website 
we came to the decision  however  to program our own
version of qwop  and learn on this program rather than use
the actual game  this decision was motivated by several
difficulties that would arise in trying to work with the flash
program  these hinge around retrieving data from the flash
program   its easy enough to feed keystrokes into the
browser to automate the game playing process  but retrieving
feedback would be prohibitively hard  for reinforcement
learning  we needed to develop models for the desirability
and transition probabilities of states in order to build a
reward function based on a particular state  this requires
information for both modeling the current state and
information the relative success of each trial for use in the
reward function  for the former category  this potentially
includes the positions and velocities of limbs  location of the
runner  and his vertical speed  horizontal speed  and angular
momentum  for the reward model  we would want to be able
to build functions based on total distance traveled  time spent
before falling  and the stability of the runner  we concluded

that it would be pretty much impossible to read all of this
data from the window of a flash game without the source
code  and we couldnt find the source code online   as a
result  we implemented our own game 
     the program
our game uses a stick figure model of the qwop model  in
order to facilitate reinforcement learning  our game takes the
form of a series of states  each state contains the following 






the state of the body parts
 for each limb  modeled as lines for arms 
legs  and torso  and a circle for a head   its
coordinates and angle with the y axis  for
the lines 
 the limbs current angular velocities
 angles between adjacent limbs  knee
angles  angle between legs at the crotch 
etc 
time spent  number of states to reach this state 
center of mass
distance traveled

each body part contains two points determining its start and
and location  as well as a mass  point mass   and current
angle to the vertical  we store the angles of the body parts
 thighs  calves  and a single point mass for torso head  so
that we can keep track of limitations  for instance  the knee
joint should have a range of motion of at most     degrees
to     degrees  by comparing thigh and calf orientations  we
can make sure that body parts stay in locations that are at
least close to anatomically possible 
if the current state is a fail state  that is  the body has fallen
over  we stop the iteration  we define this as any part of the
runners body except the feet and knees touching the ground 
otherwise  we branch off  at any point in time  the user can
be pressing some combination of   buttons  so there are   
possible transitions for each state  if either thigh key is
pressed  we add angular velocity to the corresponding thigh
and some fraction of this angular velocity to the other thigh 
in the opposite direction  calf keys behave in the same
fashion 
after applying these rules for each keystroke registered  we
apply the following transition model to generate the next
state 
 move the body based on horizontal and vertical velocities 
 move the limbs based on angular velocities  restricting
based on location of the ground and maximum joint angles 
if they try to propel themselves against the ground  move
them horizontally forward  if a joint angle comes up against
a limit  angular velocities are adjusted to move the angle
away from this limit 

fi calculate the horizontal center of mass  if there is at least
one foot on the ground  then depending on where the center
of mass is located  increase or decrease rotational velocity of
thighs and the torso in the appropriate direction  tilting
towards the direction of imbalance   if one foot is on the
ground we use the distance of this foot to center of mass in
order to calculate these adjustments  if both are on the
ground  then we use the the average of the two feet xcoordinates 

number of test runs versus length of maximum run 

 based on the updated angular velocities  translate some
fraction of the angular velocities into vertical and horizontal
velocities for the entire body 
 if no feet are on the ground  add to vertical velocity for
gravity 
 apply a decay constant  multiplier  on every velocity to
model friction   air resistance
   discretized value iteration

number of training examples versus distance of longest run 

the graph data is unsurprising  the more trials we run  the
better the maximum result  but it doesn t look like the ai is
learning much  the longer test runs are short enough to  in
this case  be attributed to luck and random chance  in this
model  we had to have the model choose random actions
many times at the beginning of the simulation  and we
believe that random choices alone are not enough to lead the
model to achieve good results  we figured we would need a
smarter way of discretizing the state space 

our first strategy was to do something similar to what we ve
done before in the homework  in the inverted pendulum
problem  that is  given the position and velocity of the
object at a given point in time  translate that position and
velocity into one state in a set of finite states  then we could      first modulation in discretization reward
use value iteration to determine positive and negative reward
functions for each state  to implement this  we ported much the next step in our process was to figure out how to achieve
a smarter discretization and reward process  we realized that
of the code that we wrote for ps  into our project in java 
distance alone isn t what we want to reward  but rather
distance combined with continued distance and stability  in
value iteration is fairly simple  we implemented  from the
other words  we wanted to introduce a penalty for failure  to
notes
do this  we just have the reward function be positive for
distance traveled  and negative for being in a fallen state 
 repeat until convergence 
state discretization was a bit more complex  distance alone
isn t enough to determine the state of the runner  so we
needed some sort of measure of stability  in order to do this 
we though of several things that might impact the runner s
stability and ability to maintain an upright position 

where v s  is the value for each state  r s  is the reward
function  a is the set of actions  and s  is the state achieved
by taking a particular action from state s  in our case  the
action is selected from one of    actions  each corresponding
 how many feet are on the ground
to some combination of buttons selected  values  rewards 
 left and right knee angles
and probabilities observed are all similar to how they were
 angle between the right and left legs
controlled in ps  s question    so we ll ignore them here 
 thigh rotational velocities  left and right 
the real trick is in how we control the state discretization
and how we control the reward function r s   we tried
we took these features and created a state discretization
several options for both  and here we can show what
function based on the four  we tried to group like with like in
happened 
order to create a viable state mapping that worked  on the
next page is the graph of results  though it will require some
     simple state reward
explanation 
the simplest way of controlling the state and reward of the
system is to have both be controlled solely on how far the
bot has traveled  this means that a higher distance will lead
to a higher state and a higher reward  this was done as kind
of a control group  as we didn t think that we would achieve
good results at all  here is the graph of our results  showing

fias we can see  after a certain number of learning trials  the
runner simply stopped running and fell flat  in the graph
presented  the distance achieved before that point is low
because the time penalty was high  as we decreased the time
penalty  for the most part  the runner was able to go farther
before falling  we then hypothesize that by introducing a
time penalty  we introduce a time cut off  after which point
the reward diminishes too much for the simulation to
consider it valuable 
     overall error analysis 

number of training examples versus distance of longest run  note  we cut
off runs at        steps 

at first glance  the results seem too good to be true 
relatively quickly  the runner learned how to travel very far
without falling  when we go past the raw numbers  though 
we determine that the simulation was not behaving exactly
as we would like  the runner was achieving runs of       
distance  but was doing so in a very odd way  the runner  or
qwopper  would fall on to his knees at the beginning of the
run  then shuffle forward on his knees constantly  legs spread
apart  this created a very stable position  one capable of
achieving long distances  however  it seemed almost like
cheating  the purpose of qwop learning is to create an ai
that is capable of learning how to run  not an ai that is
capable of achieving a stable position and inching its way
forward  taking many minutes to get anywhere  we
hypothesize that this is because the penalty for death and
reward for stability meant that the robot would stay stable at
all costs  never taking any risks to develop a better running
motion 
     reward based on time
to combat the conservative nature of the previous run  we
though we d try a different approach  to reward based on
distance and stability but also divide by the amount of time
necessary to reach that state  this means that the reward for
a state is lessened the longer it takes to reach that state  we
kept the state discretization function the same  here again  is
the graph of results  which will also need explaining 

number of training examples versus distance of longest run

value iteration is a well known algorithm  the only
influence we had over it was in 
 number of states
 state discretization function
 reward function
as a result  we ll study those  due to memory constraints  we
were limited in the number of states we could have to around
    or      this isn t a very huge number  especially when
we consider that we had six variables contributing to that
function  of which five could take on any number of values 
this means that the contribution of each variable was
diminished  because we just could not have enough states to
handle them all  this is why we achieved odd results for the
latter two tests  in addition  as mentioned before  since
qwop is based on ragdoll physics  small differences can
become huge when the simulation is continued  a slight
imbalance in one direction or another can lead to
catastrophic failure a few time steps down the road  that
slight imbalance  though  may not be captured by our
discretization function  as it is fairly coarse 
the reward function also was inexact  rewards based on
distance alone meant that the ai would try to cheat the
system  instead of learning how to run  rewards of inverse
time meant that the system would fall without becoming
stable  as a result  we were never able to get a reward
function we were completely satisfied with 
   using fitted value iteration
     method
to address the shortcomings of our performance under a
discretized state space  we decided to implement fitted value
iteration  keeping the state space continuous  this gave us
the freedom to use as many dimensions for our state space as
we saw necessary  we decided to use a deterministic model
for state transitions  using our  known  state transitions
implemented in writing qwop  we justified taking
advantage of this extra knowledge by viewing our program
under the lens of a model  rather than as the actual qwop
program  writing our own version is akin to writing a
simulation for a real world problem  in this simulation we
take advantage of our knowledge of the physics used to

figenerate the simulation 

follows  run m randomly initialized qwop models for
      steps or until death   whichever comes first   updating
to get a sample of states  we simply ran the program a
theta at each time step  we used m         so each iteration
number of times with randomized starting angles for thighs  represents      state examples  performance converged
calves  and torso  confined so as to make the problem
quite quickly as a result of the large number of examples in
solvable in most cases  i e that right actions should be able each iteration 
to prevent the model from falling and subsequently make
forward progress   we then looped this  the loop of part   in performance topped out at around        of our arbitrary
the algorithm outline in the lecture notes  by taking the next distance units   compared to about       for the discretized
set of states to be the successor states observed from the
model  so there was considerable improvement over the
actions chosen for the previous set 
previous model  watching trials that used the policy after   
iterations  we witnessed a gait closer to that of actual
the algorithm is this  taken from andrew ng s
walking motion  however  as before the policy was very
notes      run until theta converges  
careful to avoid any imbalance  it was often more of a
shuffle than a walk  and certainly not a sprint   only very
rarely did both feet come off the ground  as would be
observed in an actual sprinting motion  it is worth noting
however  that the death rate   the percentage of trials in
which the model did not make it up to       iterations  was
considerably higher under fitted value iteration than in the
discretized model  thus this model was more inclined to a
riskier strategy  which paid off in higher performance 

we used the apache commons math java library to solve the references 
ordinary least squares equation given in the last step     
while using fitting value iteration helped by allowing to use
a much more expressive feature mapping  it also presented
the difficulty of requiring a feature mapping that could be
directly translated a useful value function  specifically  since
we chose to use a linear combination of features  this
constrained us to v   t theta    phi s   intuitively  this meant
that we needed features that could be linearly combined to
evaluate a states likelihood of generating higher reward 
through some experimentation  we settled on a feature
mapping using the difference between thigh angles  the
angles of each knee  the overall tilt of the runner  and the
runners horizontal speed  for our value function  results
varied similarly to those observed in the discretized model
and so we used the same value function 
     results

    j  morimoto  c  atkeson  and g  zeglin  a simple
reinforcement learning algorithm for biped walking  in
proceedings of the      ieee  pages                 
http   www cs cmu edu  cga walking morimoto icra   pdf
    r  tedrake  t  zhang  and h  seung  learning to walk
in    minutes  in proceedings of the fourteenth yale
workshop on adaptive and learning systems       
http   groups csail mit edu roboticscenter public papers tedrake   pdf
    apache commons math java library for ordinary least
squares regression solving  found at
http   commons apache org math 
    andrew ng s cs    lecture notes  found at
http   cs    materials html

plotted above are the mean distances travelled by runners
trained on fitted value iteration after       time steps 
against the iteration of fvi  here an iteration is defined as

fi