family history detection from clinical text
srinivasan iyer
introduction
electronic health record  ehr  systems are becoming highly prevalent today and have the potential  to
add to the success of spontaneous reporting systems for post marketing surveillance of drugs  ehr systems
broadly contain two forms of information viz  coded  structured  information and unstructured text 
researchers have shown  that the textual data have much more utility for cohort building than coded
information  which  being mainly used for billing and insurance purposes  is biased and does not truly
reflect the state of the patient 
the text in ehrs have already been successfully used in several areas of drug safety such as learning
adverse drug reactions   learning drug drug interactions and determining off label use  the first step  in all
these applications is the tagging of the unstructured text with concepts pertaining to drugs  diseases  devices
and procedures  the more accurate this tagging process  the better the results of subsequent analysis  most
efforts until now  make use of natural language processing  nlp  methods to test if a particular sentence is
about the family of the patient or about the patient himself  several such methods  were developed as part
of the i b  challenge in clinical nlp        however  such methods are usually slow and are not suitable
for processing billions of text documents 
the use of machine learning to process clinical text has been somewhat limited  owing to the lack of a
good quantity of labeled data and this applies to the problem of family history detection as well  however 
in our dataset  we observe that several clinical texts explicitly define a family history section and a history
of present illness section and these section headers could be used as labels for supervised machine learning
algorithms  in this work  we to take a semi supervised approach to learning to identify sentences related to
a patients family by using section headers in clinical text as labels for training  once a model is learned 
it can then be applied to large datasets of clinical text without explicitly defined sections 
dataset
we have access to a corpus of over    million unstructured clinical notes from the stanford hospital 
corresponding to approximately   million patients  these notes have been de identified for research
purposes  some of the notes have explicitly defined sections such as family history  history of present
illness etc  and we use these notes as labeled training data  additionally  we use terms from    biomedical
ontologies from the unified medical language system  umls  as features in our classifiers 
methods
our ultimate goal is that given a document of unstructured clinical text  we wish to identify terms that
supply information about the family of the patient  in this project  we choose to tokenize the document into
sentences  separated by a full stop or newlines  and treat each sentence independent of the others  this is a
simplifying assumption  since sentences could refer to ideas introduced in preceding sentences  thus
affecting their meaning  also  it could be the case that some terms in a sentence refer to the patients family
and some terms refer to the patient  however  we make an assumption that all of the terms in a sentence
refer to the family or refer to the patient  thus  for this project  given a sentence  we wish to classify the
entire sentence as referring to the patients family  fh   or not  ph  
preparation of labeled dataset
to prepare our set of positive training examples  fh   i e  sentences that actually refer to the patients
family  we first look for a family history section within our documents  we define a family history section
as a paragraph beginning with family history  and we include all the text till the end of the paragraph as
belonging to the section  end of paragraphs are located by the presence of a double newline or a double

ficarriage return character  following this  the section header is stripped off and the section text is tokenized
into sentences  each sentence forms a positive training example  we follow a similar approach to locate
sections labeled as history of present illness  and these sentences form the set of negative training
examples  ph   see table    
from our dataset  we obtained
table   a sample of the training data used  text contains missing words owing
          examples for the family
to the de identification process for protected health information  phi  
history
class
and
          
training example
class
examples for the personal history
grandmother diabetes insipidis
family history
class  of this  we use       
his aunt and uncle
allergy
family history
randomly chosen examples from each
uveitis right eye greater than left arthritis
personal history
class for training and a different set of
observed
upper extremity rhinitis
personal history
       randomly chosen examples
  
from each class for testing  we do
this mainly for computational tractability 
rule based methods
by visual inspection  it appears that many of the sentences in the family history class contain some family
member term  see table     we test a simple rule based method  which classifies a sentence as fh if it
contains any term from table    and classify it as ph otherwise  this method gives an accuracy of       
and a specificity of       at a sensitivity of        f measure        
bernoulli and multinomial nave bayes
for a baseline estimate of performance using machine learning methods  we use
multivariate bernoulli  bnb  and multinomial nave bayes  mnb   using a
simple bag of words model  we therefore tokenize the sentences into words and
treat each unique word as a feature  we construct a       x       matrix in which
every row is a sentence and every column is a feature  mnb seems to perform
slightly better than bnb in terms of accuracy  see table     note that the training
error is very close to the test error  convincing us about the absence of over fitting 
lib linear svm
using the same matrix of word frequencies as mnb  we use the lib linear svm 
classifier and it delivers a greater accuracy than mnb  see table     training error
seems to be lesser than test error  and this classifier may be subject to over fitting 
data preprocessing
we use three methods  a  elimination of punctuations  b  removal of stop words 
c  stemming   incrementally in order to reduce the number of features and choose
more useful features  in general  accuracy improved on removal of punctuations
and stop words  resulting in       features   however  the stemming procedure
reduced accuracy  these results are summarized in figure   

table   subset of family
names used for rule based
methods  full table
contains     names 

term
father
dad
mother
mum
mummy
mom
parent
parents
child
children
son
daughter
brother
sister
grandfather
granddad
grandpa
grandmother
grandma
granny

  

fitable   baseline values for three classifiers 
bnb
mnb
training accuracy
      
      

figure   classifier accuracy for various text preprocessing methods
svm
      

   test results    true positives
  false positives
  false negatives
  true negatives
  correctly classified
 total        

     
    
   
     
     

     
    
    
     
     

     
    
    
     
     

sensitivity
specificity
accuracy
f measure

      
      
      
       

      
      
      
      

      
      
      
      

  

  
we now look at misclassified examples to get insights into additional feature engineering that can be used
to increase the accuracy  table   lists certain examples that were misclassified by mnb 
table   some examples that were misclassified by multinomial nave bayes  the actual examples cannot be disclosed  phi  
however  these examples are modifications of the originals  keeping the essential anomaly intact 
test example
patient with hypoglycemic conditions in the company of this mother
family reports that patient is having hallucinations
her daughter was not at home and she developed x
she followed the advice of her mother and took y
his son was diagnosed with arthritis 
patient is living with sister who has z
father
strange behavior at night
mother has a history of z  with x and y

misclassified as
family history

personal history

feature selection
to reduce the amount of over fitting  we attempt to keep only the most useful features with respect to
information gain  table   shows the top    most useful features  it agrees with our
intuition  for example we would expect that sentences containing mother are mostly
table   top features by
information gain
fh and sentences containing diagnosed are mostly ph  we find that retaining only
the top     or      features reduces accuracy  however  removing the last     term
     features increases our accuracy  figure   shows the variation of accuracy with
mother
the number of features 
diagnosed
n gram features using biomedical ontologies
instead of using all bigrams and trigrams as features  we take an approach that uses
existing knowledge of biomedical text  the umls group of biomedical ontologies
contain sets of phrases typically found in biomedical text  we recognize these
phrases in the sentences and use them as features  this results in       features  see
table     we also add the length of the sentence as the number of words as another
feature  figure   shows the variation in accuracy of mnb and svm with feature
selection  using these more elaborate features  svm performs the best with       
accuracy 

breast
cancer
age
negative
malignancy
significant
history
family
father
died
emphysema
otherwise
grandmother
asthma
chronic
headaches
insect
sensitivities

  

fi  

figure     accuracy  vs   features  for  various  classifiers  

discussion conclusion
the best performance was achieved by svm with features from biomedical ontologies 
and feature selection by information gain  this achieved an accuracy of         since
the training set was build artificially  there were many cases where personal history
items were actually labeled as family history and vice versa  this puts an upper bound
on the maximum accuracy that can be obtained for algorithms that predict family
history perfectly  it would be interesting to build a manually curated training set and
test whether the same methods perform well  also  several gaps were introduced in the
sentences owing to the phi de identification process  these gaps were ignored in this
project  but nevertheless  they could possess significant predictive power 
overall  simple machine learning methods seem to do much better than rule based
methods  more improvement can perhaps be obtained by using features from preceding
and following sentences and by using language modeling methods like hidden markov
models and conditional random fields 
acknowledgements
we thank the cs    teaching staff for their help  resourcefulness and guidance for this
project  we thank prof  nigam shahs group at the stanford dept  of bioinformatics
for access to the stride dataset 
references
    
    
    
    
    
    

  

figure   accuracy vs  features for n gram features 

table   features derived
from biomedical
ontologies  total        
term
is a
history of
physical examination
evidence of
blood pressure
status post
review of
does not
review of systems
medical history
no evidence of
past medical history
follow up
last name
family history
history of present
illness
due to
social history
consistent with
vital signs
secondary to
prior to
final report
children s hospital
normal limits
ct scan
per day
was a
reason for
by mouth

schuemie  mj   coloma  pm   straatman  h   et  al   using  electronic  health  care  
records  for  drug  safety  signal  detection   a  comparative  evaluation  of  
statistical  methods   medical  care          
classen  dc   resar  r   griffin  f   et  al    global  trigger  tool   shows  that  
  
adverse  events  in  hospitals  may  be  ten  times  greater  than  previously  
measured   health  aff   millwood                         
lependu  p   iyer  sv   fairon  c   shah  nh   annotation  analysis  for  testing  drug  safety  signals  
using  unstructured  clinical  notes   journal  of  biomedical  semantics           suppl    s    
harkema  h   dowling  jn   thornblade  t   chapman  ww   context   an  algorithm  for  
determining  negation   experiencer   and  temporal  status  from  clinical  reports   j  biomed  
inform                        
roberts  k   harabagiu  sm   a  flexible  framework  for  deriving  assertions  from  electronic  
medical  records   journal  of  the  american  medical  informatics  association     jamia   
                     
jensen  pb   jensen  lj   brunak  s   mining  electronic  health  records   towards  better  research  
applications  and  clinical  care   nat  rev  genet                        

fi    
    
  

fan  re   chang  kw   hsieh  cj   wang  xr   lin  cj   liblinear   a  library  for  large  linear  
classification   the  journal  of  machine  learning  research                      
porter  mf   an  algorithm  for  suffix  stripping   program   electronic  library  and  information  
systems                        

fi