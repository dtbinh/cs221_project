 foggy  crystal ball through the lens of facebook
cs     final report
justin li  yangluo jim wang
december         

 

introduction

given the abundance of information on facebook profiles  particularly the favorites section where users like
various forms of entertainment  such as movies and television shows  there is ample opportunity to employ a
machine learning algorithm in order to learn the preferences of users and to make informed recommendations
that may be of interest to these users  in particular  we applied a machine learning algorithm to generate
television show recommendations  for this work  we partnered with screaming velocity      a start up
company who provided scripts and resources for mining facebook profiles  through which        facebook
profiles were obtained  screaming velocity also provided a dataset on the most popular american television
shows from which we obtained a set of descriptive genres 
first we built a bag of words model to describe both the input feature space generated from facebook
profiles  such as movie interests and group affiliations  as well as a feature space of relevant genres associated
with the list of television shows  then we implemented a support vector machine  svm  algorithm to
predict which genres a user is likely to prefer given his her facebook profile 
this work has useful practical applications  directly so in the entertainment industry  retailers may be
able to use similar approaches to cater to personalized customer preferences and interests  this type of
work can further enable more comprehensive data driven predictions and recommendations by coupling with
growing prevalence in the mobile media and social media spaces  similar work can also be generalized to
other situations where automated recommendations may be useful 

 
   

data processing
data extraction

screaming velocity has developed and provided a set of python scripts that  with user consent  scan through
an individuals facebook profile and the profiles of his her facebook friends  by advertising on the cs   
piazza page  they collected more than        individual profiles  we downloaded this set of profile data 
anonymized to protect privacy  in a large  xml formatted file  based on the  xml file structure  we wrote
a matlab script to extract the file into a struct  with one entry corresponding to each individual  more
specificially  each entry contains    fields from the individuals public profile  listed in order  gender  locale 
atheletes  teams  about  tv  movies  music  books  activities  interests  and sports  for example  a user
might have these fields 
gender  male
locale  us
atheletes  andrew luck
teams  stanford cardinals  indianapolis colts
about  n a
tv  battlestar galactica  eureka  the beginning
movies  monty python and the holy grail  iron man    the avengers
 

fili and wang

cs     project report

 

music  n a
books  the lord of the rings  alexander the great  the illiad
activities  n a
interests  n a
sports  n a
then  for every field i  we built a cell array di where each element dij of di was itself a cell array
of the entries for user j regarding field i  for example  the first field  gender  may be represented as
d     male    female    female    male       indicating the first user is male  the second
is female  and so on  likewise  the  th field  tv  may be represented as d     battlestar galactica 
eureka  the beginning    the office  star trek       
now  we defined a feature to be an element in the inner cell array  so female would be a feature in
gender  and the office a feature in tv  we then defined the dictionary to be the set of unique features in
a field  given n total users and mi unique features in field i  then for each field i we build a sparse matrix
xi of size mi  n 


x   x    x       x i     x n 
where each x j         mi is the feature space for user j  for example  gender has two features  male and
female  in its dictionary  so the example above may be represented by


       
x   
       
where the first row represents male and the second female  x       indicates the first user is a male  and
x       indicates the second user is not a male  and so on 
in the tv example  each user could have multiple tv preferences  so dij could have multiple elements 
furthermore  some of these elements could be repeated by other users  thus we specify each feature to be
a unique element  we parsed each dij to extract features  delimited by a comma  continuing with the d 
example above  we would construct a vector x    for the first user as follows 
x                             t  
where the first five features correspond to the first five elements of the tv dictionary  which are battlestar
galactica  eureka  the beginning  the office  star trek    indicates the user likes this tv show  and  
means the user did not specify he she liked it  now  we repeat this process for all    fields to get a large
sparse matrix x


x 
 x  


x       
    
x  
notice x is particularly row sparse  since features are not commonly shared  to decrease the sparsity  we
ignored features shared by only two or fewer users  this means that if at most two users liked a particular
sports team  for example  then we will not use this sports team in our training data  it is worth noting that
since we implemented exact string matching to find common features  we run the risk of losing data that is
actually shared by multiple users if they spelled it differently  e g  houston texans versus texans   given
that we have        users  losing this amount of data is probably fine 

   

inputs and labels

although we use x as the input to make predictions about tv preferences  we first need to make some
modifications  since x  is the data for tv preferences  we leave x  out of x and instead set the test matrix
y   x    if there are m total non tv features and n users  then x is binary matrix of size m  n  if there
are r tv tokens  then y is a binary matrix of size r  n 
initially we used individual tv shows as labels  later  we found that this y matrix was much too
sparse and decided to classify by genres instead  see section     for details   screaming velocity provided

fili and wang

cs     project report

 

a document which grouped     tv shows into    genres  note a tv show could fall into several different
genres   for the new genres based y matrix  yij     means user j liked a tv show which is part of genre
i  and   otherwise  notice yij could be greater than   as each user may have liked several tv shows in this
genre  for our algorithm  we ignored this information as we only considered a binary classification  but in
the future  it is possible to extend our work to multiclass classification  furthermore  a tv show can be more
strongly associated with one genre over another  such association would normally require many unbiased
people to watch each tv show and rate them  since we did not have such information or resource available 
we ignored this fact and acknowledge that our algorithm could be improved given such knowledge 

 

model selection

with properly formatted data  we can begin applying machine learning techniques in order to build a useful
predictive model  here we describe the development steps we went through in building a model and selecting
parameters 

   

svm modeling

we planned on using a set of support vector machines  one svm for each television show in the given list 
in particular  with x representing the input feature space and y modified to be a vector of  s and   s
for each television show  the svm fit some model for each television show  with this sort of naive first
implementation  we find that the svm claims to have excellent results  with prediction errors ranging from
      to     however  prediction error presents an incomplete picture of the trained model  since the
outputs  i e  labels  are generally sparse  say    out of        individuals having a  like for a television
show  predicting a    as the output regardless of the input gives a very low prediction error that is not
meaningful  indeed  quantifying performance using false positives and false negatives showed that these
naive models in fact perform quite poorly  i e       false positive  
in order to handle the issue of sparsity  we switched from modeling specific television shows to predicting
television show genres  this approach greatly reduced sparsity and produced more realistic predictions 
however  there were still a number of genres which were particularly sparse  as such  our svm only learned
models for genres with enough entries over some threshold  in our final model  we set the threshold to
    people and thereby trained on    genres  initially  we chose a model based on the linear svm package
liblinear     as used in our homework   

   

model results and parameter optimization

predicting by genres gave more realistic results  however  the training and testing error  as shown in the
learning curve in figure   a   did not converge  there was a     difference between them   which indicated
overfitting  it turned out that our data could not be sufficiently modeled by a linear svm like liblinear 
so we switched to using libsvm developed by the same authors      this gave significantly improved results 
with an average training and testing error around      as shown in figure   b   in both figures   a  and
  b   we also included a green dashed line illustrating the baseline prediction accuracy  which was computed
by assuming a uniform distribution based purely on the fraction of individuals who liked a genre  in order
to further break down the distribution of the errors  figure   illustrates the learning curves for each genre 
from this  we see that while most genres had prediciton error between        a few of the genres had
considerably higher error rates          these errors were the average of   fold cross validations 
there are two parameters in libsvm that we optimized to get better performance  in particular  libsvm
used the radial basis function f  u  v    exp  u  v     as the kernal type and implemented a cost function
for regularization  which is controlled by a c parameter  we ran a grid search algorithm to find the best 
and c parameters for the svm for each genre using both brute force double for loops in matlab and the
grid py script supplied by libsvm  it took a significant amount of time to find the optimal parameters for
even a single genre  on the order of six hours  for grid py and even longer for our own brute force method 
consequently  we focused only on optimizing genres with error rates of over      which are the top six lines
in figure    using grid py  with these new optimal cost and gamma parameters  we retrained our svm
models and found that the error rates for all six genres were reduced by        

fili and wang

cs     project report

 a 

 

 b 

figure     a  learning curve plot using the liblinear svm package  with a clear gap between training and
testing error   b  learning curve plot using the libsvm svm package  eliminating the overfitting introduced
by the liblinear package  all errors are estimated by   fold cross validation 

figure    learning curve plot using the libsvm svm package  for each genre  a solid and dashed line of the
same color comprise one training and testing pair for a genre 
the end result is that in terms of prediction error for television show genre  our model is nominally
accurate  however  in table    we see that for the six genres with the highest error rates and for which
we optimized with grid search  precision is high  but recall is low  we further note that recall is more
important than precision for our application in terms of providing a meaningful metric  as false negatives
are more detrimental than false positives for evaluating predictions  overall then  with the exception of
drama  the model failed to make many useful predictions  in part  this is because the grid search on the
libsvm implementation using grid py optimized c and  to improve accuracy rather than precision or
recall  furthermore  due to the sparse nature of the genre data  many of the other genres most of the
predictions are still predicting false  hence accuracy being high but not recall  it is somewhat unclear as to
why the drama genre shows significant improvement  the six most populated genres all had roughly the same
population  but only drama had a reasonable recall percentage  having increased from      to       after
grid optimization  after considering the shows excel file that we used to determine genres from television
shows  it may be that there is some underlying linkage between the other genres that interferes with the
assumption of independence  many of the television shows that fall into any of the other six genres often
contain two or three more of the six genres as well  alternately  it may be that the grid search in these other
genres were based on an incorrect value function  so  for future work  we suggest writing a grid optimizer
which optimizes for recall for each of the genres  noting that this will be computationally expensive given
the size of our feature space 

fili and wang

cs     project report

genres
comedy
debauchery
drama
goofy
satire
sitcom

before grid
precision
     
    
    
     
    
  

search
recall
    
    
    
    
    
  

after grid
precision
     
    
     
     
    
    

 
search
recall
    
    
     
    
    
    

table    precision and recall for the six genres with the highest error rates 

 

conclusions and suggested future work

in conclusion  we implemented a machine learning algorithm to predict individuals tv show genre preferences given information about their facebook public profiles  we first pre processed        facebook public
profile data  extracting out relevant features  constructing feature and label matrices  and re processing to
make the matrices denser  then we built a model by training a radial basis function type kernal svm on
the data and achieved an average test and training error of      however  low recall and high precision
indicated that the data was still too sparse  and further optimization and more data are needed  as a result 
in general it was not successful in making genre predictions 
this work provides insight into some challenges and solutions encountered in predicting information given
facebook data  we hope this work may be of use to screaming velocity and other parties who are interested
in predicting user preferences  broadly speaking  this work can also be extended to general situations desiring
automated recommendations  for future work  we think the following are worthy items to address 
   use pca to condense data and make it less sparse  right now  we simply throw away rows that are
too sparse  a better way to do this may be to condense rows 
   run grid search to optimize c and  for recall 
   consider weighting different features  for example  books and interests may be better predictors than
location  so they would get a higher weighting 
   a neural network can be used to introduce tags for dependencies  maybe using ufldl     

 

acknowledgements

this work could not have been done without the generous help from screaming velocity and our head ta 
andrew maas  graham darcey and wayne yurtin from screaming velocity helped us write the script to
collect facebook data and distributed the data to us  andrew was always available to discuss our project
and provided guidance when we were stuck  we would also like to thank the        people who contributed
their data  last but not least  we would like to thank andrew ng and all the tas for an awesome class and
for their time reading our final report  hope you enjoyed it 

references
    screaming velocity  http   www screamingvelocity com   last accessed dec          
    chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm
transactions on intelligent systems and technology                     software available at
http   www csie ntu edu tw  cjlin libsvm 
    unsupervised feature learning and deep learning  uflpl  
http   ufldl stanford edu wiki index php ufldl tutorial 

fi