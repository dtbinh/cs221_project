predicting congressional bill outcomes

pamela chua
stanford university
ylpchua stanford edu

zach cain
stanford university
zcain stanford edu

kristian gampong
stanford university
kgampong stanford edu

abstract
this election season  as those before it  has demonstrated the importance of buzz
words in congressional  as well as presidential  campaigns  we have been bombarded with vague references to jobs  family  crime  and so on  as candidates
attempt to gather as many votes as possible  in this project  we attempt to explore
this question  how much of an impact do these words have once those candidates
take office  more specifically  can we predict the voting behavior of a representative on a given bill using only the frequency of words in the bills text 

 

introduction

text classification is a well known field of information retrieval and has been applied to many different areas as well  although this broad area of research has been extensively explored  we did not find
any substantial published work on this particular application of congressional vote predictions  our
goal is to build a binary classifier for each representative that predicts what he or she will vote based
solely on the frequencies of words contained in a bill  we implemented three different algorithms 
multinomial naive bayes  k nearest neighbors  and support vector machines  and evaluated their
strengths and weaknesses  on top of that  we also used an ensemble of the above three algorithms
to predict the voting patterns of a representative 

 

prior work

previous work in a similar field was done by yano  smith and wilkerson     who explored textual features to predict bill survival in congressional committees  our project will focus on the
individual congressman and his voting record  while yano  smith and wilkerson looked strictly at
congressional committees  furthermore  our planned methodology differs significantly from their
work 

 
   

data collection and conversion
data collection of voting records

we chose to work with the    th congress             and chose to further focus only on the
house of representatives  since it does not have anomalies like filibusters  which occur in the senate  and has     voting members  as opposed to the senates      providing a wider encompassing
view of congressional voting habits  although all congressional histories are publicly available
 

fionline  there was a substantial amount of preprocessing required to transform this data into a usable format  we retrieved the full roll call history of the    th house of representatives from
http   polarizedamerica com house    htm  but these roll calls included votes on all administrative
actions in the house between      and       for example  there were votes to add amendments to
existing bills  to redirect failed bills to an appropriate committee  to table bills  to suspend the rules
and agree  and so on  since we were only looking at cases when a bill was being voted into a law 
the overwhelming majority of these votes were not useful to us  we had to parse through the roll
calls to find only those roll calls that involved voting a bill into law  and then further filtered these
roll calls to focus only on the h r  billsthose that originated in the house  as opposed to bills that
originated in the senate  were passed there  and were then being voted on in the house   we chose
to ignore the bills that originated in the senate because we wanted to avoid the possible extra noise
that accompanied these bills  such as different writing style word choice or additional party pressure
to vote in a certain direction  given that such bills had already passed the senate 
once we determined which roll calls corresponded to votes on bills  we used the publicly available
roll call vote data at http   www govtrack us data us     rolls  to retrieve the vote of each representative for each relevant roll call  using the data at http   www govtrack us data us     people xml to
match the votes to the name  state  and party of each individual representative  using this data  we
constructed a matrix l of labels  with each l i  j  corresponding to the jth representatives vote on
the ith bill    for yes    for no  

   

preprocessing of text

for our feature matrix  we had to collect the full text of all the house resolution bills passed during
the    th congress  we obtained the corpuses from the u s  government printing office website  http   www gpo gov fdsys    some basic preprocessing of the text included removing punctuation  setting capitalized words to lowercase  and removing stop words commonly seen in legal documents  we acquired this list of legal stop words from the california legislature website
 http   www leginfo ca gov help stopwords html  
to standardize the tokens  we used porters stemmer to stem tokens in an attempt to eliminate redundancies in our tokens  for instance  the words representative  represent  and representation are
all mapped to the stemmed token repres  we then split the space delimited processed text into
tokens  sorted them and converted them into an index of unique tokens  we then generated a sparse
representation of the document word matrix t with each t  i  j  corresponding to the count of the
jth token in the ith bill 

 
   

method
multinomial naive bayes

although it makes a strong assumption of the conditional independence of words given the labels 
the naive bayes algorithm has shown to produce surprisingly good results and is frequently implemented in text classification  we decided to implement the multinomial naive bayes model  which
uses the frequency of occurrence as a probability  as opposed to the multivariate alternative  improvements made to the model were made through stemming and preprocessing of the text in the
bills  other adjustments attempted including tweaking the value for laplace smoothing  in laplace
smoothing  the constant   is usually added to account for words that have not been seen before  we
tried to vary this value to see if the accuracy improved but it did not improve accuracy much if at all 
possibly due to the spareness of the word matrix 
what produced really interesting results for this model was training the representatives separately
based on whether they were republicans or democrats  the model for the republicans achieved a
high mean accuracy          while democrats performed poorly          even though the number
of republicans is about the same as the number of democrats in the training set  hence pointing to
the possibility of an improved model based on party lines 
 

fi   

k nearest neighbors

we implemented two slightly different versions of knn  one using jaccard similarity  and the other
using cosine similarity  the process behind the two  however  was the same  for each bill in the
test set  we searched the training set to find the k most similar bills to the test bill based on the
given similarity metric  then  for each representative  we looked at the votes of the top k bills  if
the representative voted yes on a majority of the k  we predicted yes for that representative  and
predicted no otherwise  we then checked the actual vote on that test bill  and updated the error
accordingly  through experimentation  we found the optimal value of k to be   

figure    knn accuracy with varying k values
   

support vector machines

we also used support vector machines  svms  to solve our text classification problem  from
class  we expect the svm classifier to have relatively higher error on very small training sets like
ours  but perform asymptotically better than naive bayes  this is because generative learning algorithms  such as naive bayes  have smaller sample complexity than discriminative algorithms  such
as svms  but generative algorithms may also have higher asymptotic error 
we used the library sliblinear  http   www csie ntu edu tw  cjlin liblinear   with default parameters
to run our svms for each representative but we first converted our matrices to the requisite liblinear
format by converting classes to         
   

evaluation of models

in order to evaluate how well our classifiers are performing  we decided to use    fold cross validation  randomly dividing our data into    sets  and for each iteration  using one of the sets as our
test set and the other nine as our training set  we chose to do    fold cross validation instead of
hold out cross validation because of our relatively smaller data set  leave one out cross validation
 loocv  was not considered because training would be computationally expensive 
   

improvements made to models

though our initial results were very promising  we thought of several areas where we could make
improvements to the accuracies of our models  one of our main ways of doing so was improving
the relevance of the tokens in our token matrix by removing tokens that were erroneous and did not
help with our predictions 
to eliminate unhelpful tokens  we calculated the inverse document frequency  idf  scores for all
tokens in order to determine which tokens are more important  we define the idf score of a token t
 

fito be 
idft   log n dft   
where n is the number of bills and dft is the number of bills that contain the token t 
thus  the idf of a rare term will be high while the idf of a frequent term will be very close to zero 
we ranked the tokens based on the idf scores and eliminated those that can be found in all bills
 idf scores equal to zero  
another way we boosted our accuracy was by improving the label matrix of congressional votes 
one major issue we faced was that congressional votes are not mandatory  representatives can  and
often do  miss votes for a variety of reasons  illness  campaigning  family emergencies  and other
circumstances that may or may not be under that congressmans control 
at first  we experimented with a three way classifier  that would predict whether a congressman
would vote yes  vote no  or be absent from the vote on a given bill  however  we found that the
votes missed by congressman were more or less random  just as we suspected  therefore  we began
treating not present votes as no votes for all of our training and testing data  unfortunately  since
these absences could be totally unrelated to the text of the bill under consideration  it was extremely
hard to predict the behavior of representatives who were absent for many votes  therefore  the
improvement we made was to remove habitually absent representatives from consideration  creating
cutoffs for the representatives we would consider based on how many votes they cast during their
time in office  out of     total votes that we tracked  there were very few representatives who
missed more than    of those votes  but most representatives missed at least a few        votes 
we experimented with different cutoffs between    and    missed votes  and as expected  removing
representatives who missed many votes noticeably improved our classification accuracies across
all models  our best results occurred with the strictest cutoff of    missed votes  meaning we only
considered representatives who voted on at least    out of     bills  these results matched up quite
well with our expectations 
   

ensemble method

another way that we sought to improve our accuracy is through an ensemble method  we looked
at the predictions of knn  cosine similarity   svm  and naive bayes  and then for a given bill and
representative  we predicted yes if   out of the   models predicted yes  and no otherwise  the
ensemble approach ultimately performed better than svm and naive bayes  but slightly worse than
knn 

 

results

as can be seen from figures   and    the peak accuracy is achieved by the knn model using a
cosine similarity metric with k      a minimum of    out of     votes cast for each representative 
and the stemmed token matrix adjusted for token idf scores 

 

conclusion

we faced several challenges in this project  one was a relatively small set of training examples 
since the house changes its members every   years  each set of representatives has a very short
period in office  for example  the    th house of representatives voted on just over     bills  and
those from other recent years had around the same number of votes  another challenge  as discussed
previously  was the large number of votes missed by some representatives for reasons completely
unrelated to the text of the bills they missed  furthermore  our input features consisted solely of
word frequencies  without any measure of whether a bill was for or against the words it contained 
despite these challenges  we were quite pleased with the results of this project  our goal was to
determine whether or not there was any useful link between the word frequencies of bills and voting
patterns in congress  since our classifiers use only this data  and were able to achieve relatively
high accuracies  we have demonstrated that there is a definite and useful connection between word
frequencies and voting behavior  therefore  in possible future work  it seems likely that word frequencies could be used as a single feature in a larger classifier along with other features such as
 

fifigure    accuracy for knn and svm using stemmed and idf adjusted tokens

figure    main plot of accuracies for different models implemented
party of the voting congressman  party of the bills sponsors and co sponsors  and so on  an alternate approach for future work would be to apply more sophisticated language parsing to bill text to
identify not only the frequency of political buzz words  but also the bills sentiments towards them 
and use these as features to make predictions 
acknowledgments
we would like to thank professor andrew ng for his invaluable teaching and guidance that made
this project a possibility 
references
    yano  t   smith  n a    wilkerson  j d         textual predictors of bill survival in congressional committees 

 

fi