using twitter to predict voting behavior
mike chrzanowski
mc     stanford edu

daniel levick
dlevick stanford edu
december         

background 
an increasing amount of research has emerged in the past few years using social media to
either predict the outcomes of elections or  in the case of the united states  to classify users as
democrat or republican  while predictions based on social media are not representative of the
voting population  they have been shown to compete with surveys in accuracy and can be done
in real time to provide instantaneous feedback to political events  tweetminster        
twitter provides an excellent platform for solving political classification problems  the realtime availability of large amounts of data as well as built in content sorting mechanisms  e g  
hashtags  retweets  followers  are major advantages over using other forms of social media  if
these advantages can be leveraged to predict how twitter users will vote  this information could
be used to predict an election outcome or predict the likely impact of real time events on voting
patterns 
problem statement 
our goal is to train an algorithm that could have predicted an arbitrary twitter user s vote in
the      us presidential election  using methods that could be applied to future elections 
recent studies have shown the ability to accurately classify users as democrat or republican
 e g  pennacchiotti        boutet         with less consistent accuracy  other studies have
attempted to predict the outcome of major elections based on aggregate data such as the volume
of candidate mentions  tumasjan         no studies we are aware of have attempted to directly
predict an individual user s vote in an upcoming election outside of the framework of political
parties  by eliminating this framework  we hope to better predict the voting patterns of users 
especially those that do not identify strongly with either party 
data collection 
we collected two groups of tweet data  the first group is composed of tweets from the
candidates and people that have publicly endorsed them  these are the sources that most likely
originate much of the retweeted content and hashtags that might be used by supporters  the
second group exploits the revealing of voting preferences by average twitter users on election
day  for example  some users explicitly stated whom they voted for after leaving the voting
booth  while others retweeted tweets of the form retweet this if you voted for candidate x  this
data set is closer to representative of the average twitter user and provides an observable
ground truth by which to train and test our algorithm  this is still  admittedly  not a
representative sample of all twitter users 
the use of reveal tweets has three advantages over data collection techniques we have
encountered in prior research  first  it is fast and minimally subjective  we did not have to
manually classify users based on their past tweets  second  it does not rely on user selfclassification through websites like wefollow or twellow and therefore may capture users that
either do not use these services or do not have strong allegiance to a particular party  third  it is a
direct measure of the variable we are trying to classify  voting behavior  this is especially
relevant in the u s  where only     of the population identifies with a political party  jones 
      and typically less than     of the population actually votes in presidential elections 

fiour data corpus contains over     million tweets created before election day      came
from obama voting authors and the other half came from romney voting authors  the tweets
were generated from       accounts  of which     voted for obama and     for romney 
reference model methods and results 
our primary algorithm uses all available data to train and cross validate an svm  we first
harvest tweets via a python implementation of the twitter api and push them into a mysql
database  each tweet  paired with its authors classification  is a training example  for
preprocessing  we use nltks lancaster algorithm for stemming and we substitute all numbers
with numbr  this data is then stored  when beginning a model run  this data is pulled and
randomly divided into training and testing sets according to a cross validation ratio  we then
transform the training and test sets into scipy sparse matrices by constructing frequency vectors
of tokens that appear at least twice in the training set corpus using scikit learns countvectorizer
class  we then use the training set matrix to create inverse document frequency weights that are
used to do tf idf transformation on both matrices using the librarys tfidftransformer class 
we train an instance of the librarys linearsvc svm on the training matrix and test it on the
test matrix to classify each tweet as     user voted for romney  or     user voted for obama   at
the end  users are classified by the sign of the sum of their tweets predicted classifications 
with an       split of training versus test samples  the user classification model achieves
    testing accuracy and     training accuracy  this is higher than accuracies achieved using
tweet text to predict political alignment in recent literature  conover         our models
runtime is just over ten minutes when the preprocessing and data gathering steps are ignored 
algorithm exploration 
alarmed by our high scores  one of our priorities was determining whether any assumptions
we made in designing the reference model yielded inflated accuracy ratings  we point out that
our disregard of which account a certain tweet is authored by when partitioning our data corpus
allows for tweets from the same author to enter into both the training and test sets  to test the
effects of this  we ran the reference model but placed all of each accounts tweets in either the
training or testing set  testing accuracy was     with a     cross validation ratio  we found
that we did not have enough data  in terms of the total number of accounts available  to produce
consistent results with other cross validation ratios and were therefore unable to explore this
property further  that is  accuracy rates had a very high variance depending on whether the
tweets of certain accounts were placed in the training or testing sets  and the effect was
exacerbated by uneven cross validation ratios  however  the experiment shows that while the
assumption has a noticeable effect on performance  accuracy rates remain very high 
we also began exploring the effects of certain characteristics of the training and test data
upon our accuracy rates  we ran the reference model while precisely controlling the exact
number of tweets  their age  the number of users from which the tweets originated  and the
maximum size of the token corpus used to create our sparse matrices 
we were primarily interested in seeing how well we could predict the classifications of new
tweets based on older tweets  thus  most of our experiments followed the format of cleaving the
tweet data into two sets based on whether they were created before or after a certain point in
time  tweets that were authored before that temporal pivot constituted the training set  and those
created afterwards composed the testing set 

fias it turns out  for any pivot point we used between          and            increasing the
number of tweets within the training set while holding constant the number of tokens  i e  
restricting countvectorizer objects to only produce matrices with a certain number of columns 
and originating authors is almost never useful  the prime drivers in increasing accuracy come
from increasing the number of accounts or increasing the number of tokens  in addition  we
found that if all the other described variables are held constant  tweets generated later in time
have more predictive power of future tweets than tweets authored earlier  though we did not
specifically test for statistical significance  and thus are not confident in the magnitudes of our
observed accuracy rate increases  our outlined results held despite repeated runs of the model
with the same parameters and using millions of randomly selected tweets 
to analyze whether increasing the number of originating accounts or tokens is more helpful 
we decided to create a token corpus independent of our data by mapping every unstemmed token
to a token within a predefined dictionary  i e   the scowl word list  as determined by
pyenchants spell checker functionality  which traverses a trie to uncover tokens with the longest
common substrings  we then stemmed and subbed this text just as we would the normal  raw
tweet text and ran the reference model on it using temporal pivots while controlling for the same
variables as described above  using this new corpus  we again found that increasing the number
of tweets in the training set while holding the number of originating users and tokens constant
resulted in no increase in accuracy  and we again found that accuracy rates increase based on
how close to the election tweets were created  we attributed this to the fact that the use of
dictionary words still changes with time based on how new raw tokens are being mapped to the
dictionary token list  unfortunately  we saw the same increases in accuracy rates when we varied
the number of accounts or the number of tokens as we did when dealing with the non spellchecked corpus  we thus come away with the qualitative conclusion that working with as diverse
and fresh a data corpus as possible is the prime driver of prediction accuracy of future tweets 
other algorithm exploration  failed experiments  
one avenue for exploration that we were interested in pursuing was better accuracy rates
using only hashtag data  when we used the reference model on hashtag data alone  which
consisted of    m tweets composed of         unique hashtags  we achieved a tweet level
classification testing accuracy of     and training accuracy of      we then tried to perform
unsupervised clustering of the hashtag data using k means  our pipeline remained unchanged
from that used in our reference model except that instead of classification  we performed
clustering based on a variable number of centroids and  after the algorithm converged  used
majority voting of the assigned tweets within each cluster to determine a label for that whole
cluster  we then calculated the prediction accuracy on the testing set by assigning each testing
tweet to the closest group and determined whether that clusters label matched that of the test
samples label  as it turns out  even after grouping the data into        clusters  and being able
to go no further because of memory limitations   we could only achieve tweet level testing
accuracy rates of      this was not a competitive tweet level classification accuracy given the
algorithms extremely long runtime  and since account level classification accuracy seems very
closely tied to the success of tweet level classification  we didnt spend more time on this 
another attempt to improve on the accuracy rates relied on adding features that signified
whether a certain tweets author was following the twitter accounts of the presidential or vice
presidential candidates  as the hashtag corpus is far smaller than the stemmed token corpus used
in the reference model  adding   new features would show a more noticeable accuracy change

fiand so would allow us to determine whether to spend the engineering effort to also add them to
the reference model  as it turned out  after running the experiment many times  tweet level
accuracy rates were completely unchanged  account level testing accuracy went up by     to
      and account level training accuracy fell by      to        we felt that while these results
are impressive given that we only added   additional columns to each training and test vector  it
would not be possible to add these features to every vector in the reference model and maintain
the same runtime of the reference model  i e   there are a lot of expensive intermediate steps
required to create sparse matrices to accommodate these new columns  without first drastically
shrinking the size of the data we were working with  as we were more concerned with keeping
the model runtime low to be able to run many experiments quickly  we decided to only leave this
feature in place when processing hashtag data 
attempting to plot a subset of the tweet data as well as the svms decision boundary by
applying pca resulted in an enormous loss of information  the end result was garbage data 
temporal performance experiment 
one potential application of our algorithm is vote prediction in advance of an election  to
model this situation  we trained and tested the algorithm only on tweets created before a target
date  in the real world  data labels would be obtained via surveys of voting intention 
interestingly  we note that the breakdown of tweets and accounts by candidate supported and cutoff dates used didnt change very much at all from the corpus level breakdowns  the accountlevel testing accuracy results are presented in figure   

figure    temporal performance experiment results

the algorithm achieves greater than     testing accuracy when using only data from before
january          this suggests that our model could provide accurate voting predictions up to
two years before an election 
demographic analysis application 
classification of over      twitter accounts allowed us to perform interesting comparisons
of the two voter groups and which may be a starting point for future research in the area  this
represents another potential application of our algorithm  post election demographic analysis 
for instance  we found that obama voters use curse words between      x more often than
romney voters  tweets from romney voters were     more likely to mention religious terms 

firomney voters also used hashtags far more often and for more political speech than obama
voters  figure   shows a list of the top ten most frequent hashtags for each voting class 
figure    top ten hashtags for each voting class

conclusions 
through the construction and optimization of our algorithms  we accomplished our goal of
accurately predicting twitter user voting behavior  we feel that analysis of our algorithms
provides useful recommendations for future social media machine learning design efforts 
classifying twitter users by voting patterns also resulted in several interesting demographic
findings  and a study of performance on older tweets showed some promise for applications in
pre election vote prediction  future areas of research include closer inspection of the
demographic information of users and their tweets as well as establishing quantifiable 
statistically significant measurements of how useful adding new accounts or tokens would be in
raising accuracy rates 
acknowledgements 
wed like to thank the cs     teaching staff for helping us formulate a research topic to
pursue and for suggesting interesting questions to spend time exploring 
references 
atkinson  k         january     kevin s word list page  retrieved december           from kevin s word list page website 
http   wordlist sourceforge net 
bird  steven  edward loper and ewan klein         natural language processing with python  o reilly media inc 
boutet  a     yoneki  e          member classification and party characteristics in twitter during uk election  collocated with opodis
     toulouse  france editors  llia blin and yann busnel     
conover  m  d   gonalves  b   ratkiewicz  j   flammini  a     menczer  f         october   predicting the political alignment of twitter users 
in privacy  security  risk and trust  passat        ieee third international conference on and      ieee third international conference on social
computing  socialcom   pp            ieee 
jones  j          record high     of americans identify as independents in      retreived november           from
http   www gallup com poll        record high americans identify independents aspx 
pennacchiotti  m     popescu  a  m          democrats  republicans and starbucks afficionados  user classification in twitter  in proceedings of
the   th acm sigkdd international conference on knowledge discovery and data mining  pp           
tumasjan  a   sprenger  t   sandner  p   welpe  i  predicting elections with twitter  what     characters reveal about political sentiment  in
proc  of the fourth international aaai conference on weblogs and social media  icwsm        
tweetminster         is word of mouth correlated to general election results  the results are in  retrieved
november           from http   www scribd com doc          tweetminster predicts findings 

fi