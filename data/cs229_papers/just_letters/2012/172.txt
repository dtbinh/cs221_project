evolution of movie topics over time
cong meng  mian zhang  wenqiong guo
december         

 

introduction

topic modeling has emerged as a powerful tool for understanding and managing large electronic archives 
       it provides the methods to discover the hidden themes that pervade the collection  annotate the
documents according to those themes  and then use annotations to organize  summarize and search the
texts  besides discovering topics from corpus  topic modeling algorithm has also been used to model the
evolution of topics over time  as well as the connections hierarchies of topics by treating documents as time
series data  in this project  we are particularly interested in applying the topic modeling method to explore
the dynamics in the movie topics evolution 
the contents of a journal article  specifically the words or terminologies used  can indicate the topics
this article focuses on  similarly  the synopsis storyline of a movie can serve as a good indication of what
topics it relates to  by treating the storyline of each movie as a document  we can analyze the words used
in the storyline to get the information of topics  the most frequent movie topics in one time period will
give us some idea about the trend of movies at that time  putting the topics of movies in a time series will
hopefully reveal some interesting dynamics in theater  by using the data from imdb  especially the story
lines  we are hoping to show how the movie topics evolve over time 

 

model build up

we build up our initial model based on the lda in probabilistic modeling  the data are assumed to
be observed from a generative probabilistic process that includes hidden variables  the hidden thematic
structure in movie   we will infer the hidden structure using posterior inference  the topics that describe
a movie collection   and try to situate new data into the estimated model  fit a new movie into the topic
structure  

   

latent dirichlet allocation

the intuition of latent dirichlet allocation  lda  is that documents exhibit multiple topics  for
example  a movie can exhibit a combination of dierent topics as computer hacker  car chase and so on 
the generative model for lda assumes that each topic is a distribution over words  each document is a
mixture of corpus wide topics  each word is draw from one of those topics in reality  we only observe the
documents  while the other structures are hidden variables  our goal is to infer the hidden variables  i e  
compute the conditional probability distribution p  topics  proportions  assignments movies   let k be a
specific number of topics  v the size of the vocabulary  
 a positive k vector  and  a scalar  we let dirv  
 
denote a v dimensional dirichlet with vector parameter 
 and dirk    denote a k dimensional symmetric
dirichlet with scalar parameter 
    for each topic 
 a  draw a distribution over words k  dirv   
    for each document 
 a  draw a vector of topic proportions k  dir 
 
 b  for each word 
 zd n             k
 i  draw a topic assignment zd n  m ult   
 ii  draw a word wd n  m ult zd n    wd n             v
 

fithe graphical model for lda is shown in figure   

figure    this figure is taken from      a graphical model representation of the latent dirichlet allocation
 lda   nodes denote random variables  edges denote dependence between random variables  shaded nodes
denote observed random variables  unshaded nodes denote hidden random variables  the rectangular boxes
are plate notation  which denote the replication
lda provides a joint distribution over the observed and hidden variables  the hidden topic decomposition of a particular corpus arises from the corresponding posterior distribution of the hidden variables given
the d observed documents 
   d  
p   d   z  d   n     k    d   n       

  k




p   d   z  d     k    d    

p   d   z  d     k    d    


  k

   


z

this posterior can be thought of the reversal of the generative process described above  the quantities
needed for exploring a corpus are the posterior expectations of the hidden variables 

   

algorithms

the central computational problem for topic modeling with lda is approximating the posterior distributionin above  we choose to use mean field variational inference to do this  which is implemented in the
package provided by professor david blei  this allow us to quickly get our preliminary result  the basic idea
behind variational inference is to approximate an intractable posterior distribution over hidden variables 
such as     with a simpler distribution containing free variational parameters  these parameters are then
fit so that the approximation is close to the true posterior  variational inference is chosen because it can
be faster than sampling based approaches such as gibbs
sampling  we
variational distribution
k
dintroduce a 
over the latent variables q     z   where q     z    k   q k  k   d   q d  d   n
n   q zd n  d n    this
indicates that the instance of each variable has its own distribution  and each component is in the same
family as the model conditional 
p  z      h  exp g  z   t   a g  z     
q      h  exp t   a   

   
   

the object to be optimized is the evidence lower bound  elbo  with respect to q 
l     n     eq  logp   z      eq  logq   z  

   

this is equivalent to finding the q   z  that is closest in kl divergence to p   z    and the update rule
can be summarized as 
initialize  randomly
repear until the elbo converges
    for each data point  update the local variational parameters  ti   et   l    i    for i          n
    update the global variational parameters  t   et  g  z  n     n   

   

dynamic topic models

in the lda model movie synopsizes are exchangeable no matter which year the movie was from  however 
this assumption is not appropriate  because the movies span tens of years  during which time the language
as well as the style of a topic are very likely to have changed    and certain movies from earlier time are
 

filikely to have impact on later movies  furthermore  we want to track how these changes happen over time 
dynamic topic models  dtm  captures the evolution of topics in a sequentially organized movies  in the
dtm  we divide the data by time slice  e g   by year  we model the movies of each slice with a k component
topic model  where the topics associated with slice t evolve from the topics associated with slice t    the
time series topics is modeled by a logistic normal distribution of k    k         k t   specifically  we
assume t k  t  k  n  t  k   i      and p  t k    exp t k    we can approximate the posterior over the
topic decomposition with variational methods explained in previous section  at the topic level  each topic is
now a sequence of distributions over words  thus  for each topic and year  we can calculate the probability
of the words and visualize the topic as a whole with its top words over time  this gives a global sense of
how the important words of a topic have changed through the span of the collection  for individual words of
interest  we can examine their score over time within each topic  we can also examine the overall popularity
of each topic from year to year 

 

data and preprocessing

we have modified the python script provided by professor chris potts to scrape the data from imdb
website  in order to get useful informations  we only collect the data of movies that have synopsis  this
criteria  we assume  will ensure that the movies we analyze have good size of audience and represent the
mainstream of the film industry  the information of a movie recorded is  the html index  name  screen
time  number of reviews and synopsis  this dataset was preprocessed in r to generate data in the standard
format  all the story lines were gone through to generate the dictionary  the non informative words and
the numbers indicating years are filtered out in this process  we construct a sparse matrix for this collection
of movie story lines based on the dictionary  the movies in the matrix were reorder by time for the purpose
of dynamic modeling  then the data is ready to be trained and tested 

 
   

result
topic identification

to test our data pre processing correctness  and to pre identify topics in our data set  we first modeled
the entire movie synopsis data set with a     topics lda model  figure     if time permits  we would also
like to compare lda model fitting with dtm model  to see the potential advantages and disadvantages of
treating movies data as time series   out of the     topics  around    topics were found more probable than
others  thus in dynamic topic modeling  dtm   we picked    topics to fit our movie data  by treating
movie data as a time series text data  dtm could catch the trend of movie topics evolution  five out of the
twenty topics along with top key words within the topics are listed below  figure    

figure    topic proportion histogram from lda model  movie synopsis data  n      was used for test 
were modeled through a     topics lda model  the text data were stemmed to word roots  and high and
low frequency words   numbers  punctuations and stop words were filtered out of the vocabulary 

 

fifigure    dynamic topic modeling  dtm     topics model results  time series of movie synopsis data
 n       were run through dtm  here shows   topics out of the     along with top    words within the
topic  topic names were assigned based on top key words 

   

trending of topics

if we pick out certain topics and monitor their proportion change through time  we could extract information in how certain movie topics evolve  figure    and potentially observe how they were aected by
certain social events at particular time in history  in figure    the proportions of action and sci fi movies
were drawn through time  as shown in the figure  action movies were more popular in earlier days  and
became a bit less popular in   st century  and sci fi movies have been popular ever since  interestingly  we
saw a large spike in the year of      for both action and sci fi movies 
changing of topic proportion over years
    
action
sci fi
    

proportion

   

    

    

    

    

 

    

    

    

    

    

    

year

figure      action and sci fi movies proportion evolve through time  in general action movies had a higher
proportion in early   s  and sci fi movies have been popular ever since 

   

the evolution of each topic

top key words were picked out to look at how multinomial distribution evolves through time  figure  
shows top key words from topic action and sci fi  we found in action movies  gang has become less popular
through the years  while superman and monster are becoming more popular  and gun has been a top key
 

fiword in action movies since the early days  in sci fi movie  wolverine and mutant have become more popular
since the   s  possibly due to the appearance of x man movie series 

figure    top key words evolution in action and sci fi movies 

 

conclusion and future improvement

by applying dtm on movies synopsis data set  we were able to extract information on main movie
topics  as well as how they evolve over time  by looking at top key words probability form each topic  we
could monitor the popularity of them and observe them changing over time  and their correlation with other
key words within the same topic 
there exists much more information from dtm modeling for us to explore given enough time  for
example  the two topics  action and sci fi  had a peak at the year of       one potential explanation is to
look back into the history  a lot of excellent movies such as forrest gump  pulp fiction  the shawshank
redemption  quiz show  the stand  four wedding and a funeral  all from imdb top     movies  came to
the market in       in one aspect verifies the popularity of several topics in       we could also instead of
dividing movie synopsis data in a unit of years  divide movies monthly or quarterly to see if there is seasonal
eects on movie topics evolution 
dtm is a versatile and general model  but there exist a few simplified assumptions  for example  all the
topics and vocabulary within the model are assumed to remain existing over the years  but almost certainly
some of the topics or vocabulary may fade away from movies that were made more recently  on the contrary 
some new topics may emerge as well  thus  if we could incorporate topics that are more flexible instead of
being fixed and never disappear  we could potentially fit the data better 

references
    david m  blei  probabilistic topic models  communication of the acm     
    david m  blei  john d  laerty dynamic topic models icml     

 

fi