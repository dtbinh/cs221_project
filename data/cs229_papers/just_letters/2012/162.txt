jazz melody generation and recognition
joseph victor
december         

introduction
in this project  we attempt to use machine learning methods to study jazz solos  the reason
we study jazz in particular is for the simplicity of the song structure and the complexity and
diversity of the solos themselves  a typical jazz song has an introduction  followed by each
musician taking a turn improvising a melody over a fixed set of chord changes  the chord
changes constrain what note choices a soloist can make  but also guide the melodic evolution of
the solos  abstractly  a jazz musician is a machine which reads in a set of chord changes and
non deterministically outputs a melody   
there are then two obvious problems one can attempt to solve computationally  the first is
generation  can a computer be a jazz musician  the second is classification  given a solo  can a
computer guess the chord changes which inspired them  one should believe the first question
is possible  although admittedly hardworking musicians should always be able to beat it  one
should believe the second question is possible because if one listens to a soloist without the
accompaniment one can still hear the changes  especially with the jazz greats  so long as the
soloist isnt playing anything too wild  the hope is that a computer can statistically hear the
changes at least as well as a toe tapper 
the basic philosophy is this  fine a bunch of reasonably structured jazz melodies  that
is      having the format described above   chop them up so we have a bunch of chordprogression melody snippet pairings of the same length and analyze them to solve the two
problems above  the attempted solution to the first problem was a markov chain model  while
the attempted solutions to the second were  in order of success  a tree metric space model  a
naive bayes model and a hidden markov model 

data accumulation
there are many educational websites that offer free pdf files containing transcriptions of jazz
solos from great recordings by famous artists  it is easy to write scripts that grab all the jazz
pdfs from said websites  once you have these pdf files  there is a program called pdftomusic
pro which reads in pdfs that have nice enough sheet music on them and outputs a parse able
format called musicxml  as far as i can tell  the pdftomusic pro software is good at getting
all the notes and their values  but less good at getting the rhythms correct  it was very difficult
to fix all the output of this data  and almost half of the songs i began with needed to be
thrown out  including all songs which had non standard formats or  for simplicity  were not in
    time   there were many systematic issues with the rhythms that could be found and
 

this is drastic simplification of course  in reality the improvised melody is being influenced in real time by
the choices of the other musicians  who themselves are responding to the soloist  i only model the soloist here  i
also do not model soul 

 

fifixed automatically  and when songs did not pass the sanity checks after these fixes they were
eventually thrown out  this part of the project was highly frustrating  but eventually i got a
dataset i believed in 
the harmonic data  that is  what chords were being improvised over in each measure  was
more or less impossible for the pdftomusic pro software to reliably extract from the chord
information  the chord information came from ralph patts vanilla real book  which is a
relatively comprehensive ascii index of chord progressions from famous jazz songs  by doing
about a minutes worth of semi manual work per song  ugg   i was able to match the chordprogression data to the solo transcriptions  the issues were things like solos not starting on the
first measure or songs being transcribed in a different key   luckily  many of the songs were
multiple pages  so once the chord data was matched at the beginning of the song  i would many
measures worth of data 
in the end  my dataset consisted of pairs of lists of chord changes and lists of notes from the
solos  my dataset came from    songs and contained      measures of music 

some definitions
a notevalue is one of the    note values  for instance c or g or bb  a note is an octave  which
is an integer   a duration  also an integer  and a notevalue  a modality is a type of chord 
modalities include    major   denoted m    minor      or m    m b   dim  augmented  altered
and a few more  i do not consider what musicains all higher extensions  so many modalities
end up being the same  for instance  i do not differentiate a major chord and a major   chord 
or a dominant   th chord and a  th chord    a chord is a duration  an integer which records the
number of beats this chord is played for   a notevalue and a modality  a bbm  chord played
for a measure is then the triple  bb m      a interval is the distance between two notevalues 
and is between   and    inclusive  a d is   above a c and    below  because of the wraparound 
the direction matters   we can subtract notevalues to get an interval or add an interval to a
notevalue  note or chord to get a new notevalue  note or chord  respectively   a datapoint
is a list of chords and a list of notes which has been be played over them  a melody or solo
is just a list of notes  and we will use these phrases interchangeably  the level of detail of a
datapoint is the length of musical time of that datapoint  which ranges from   measure phrases
to single beats 

data normalization
say that a musician plays some lick over  say  a c  chord  if i transposed each note up a bit and
played it over a d  chord  it should be just as good  the logic is that if the computer learns
to play over some chord progression  a shift of that progression shouldnt throw it off     thus 
for any data point  that is  list of chords and list of notes   we can fix some notevalue n and
only consider only the intervals between the chords in our list and n  and the notes in our list
and n  we say that the datapoint in question is normalized with respect to n  in a certain
sense  we can think of this as multiplying the size of our data set by     since transposed chord
progressions are isomorphic  the prior of seeing  say  a c  is now the same as seeing a d  
although the prior of seeing a c  or a cm  can still be different  as can the prior of certain
 

i am trying to be light on the music theory  it is not that important  just think of it as an attribute from
some list 
 
there is a likely anachronistic story about the great musician charlie parker coming to a jam session having
learned a certain song in bb and the leader saying to play the song in b  just to throw the young musician off 

 

ficombinations of chords or certain chords held for certain lengths of time  and all this depends
on how finely we choose to chop up our dataset 

markov chains for solo generation
one can use markov chains to generate convincing sounding jazz solos pretty easily  since 
when soloing  the chord progression is known  one cay say the state is the current chord and
the last k beats  each step you advance the state by picking from the dataset a one beat note
selection consistent with that state and look up the fixed next chord  of course  it is somewhat
easy for this method to get stuck  especially for large k  so when it does i have it insert a few
beats of rest  this is a good thing   and then start again  the results sound convincing and the
structure of the underlying chord progression is easily detectable  unfortunately  i have no way
of quantifying how well it worked 
the values of k for which i felt the midi playback sounded best were between   and   for
smaller values of k the note choices were very in my opinion random sounding  while for larger
values of k it got lost to often 
while this was by far the simplest part of my project and least scientific  it is certainly the
most entertaining and fun to show off  and possibly the biggest success 

metric structure on melodies
musical data has a natural tree structure  a phrase is divided into measures  a measure into
beats  a beat into eighth notes  or perhaps triplets  etc  storing a solo  phrase  measure  halfmeasure  or whatever level of detail is desired in this format is rather easy  the level of details
i considered were the phrase  the half phrase  the measure  the half measure and the beat  each
of which is twice as long as the next  the edit distance for two trees is defined as the length of
the shortest path of deletions of nodes  insertions of nodes and changes of labels of nodes to get
from one tree to another  more generally  one can assign a cost function to insertions  deletions
and changes of labels  which could depend on the tree being operated on  i implemented shasha
and zhangs edit distance algorithm in hopes that it would reasonably tell me how different two
solos were  the real hope is that solos coming from the same progression  or snippets of solos
coming from similar progressions  would have lower edit distances 
the first thing to try is of course say that all edits are equal  for various levels of detail 
i created the matrix of edit distances between each datapoint in my dataset representing that
level of detail  normalized with respect to the first chord in that datapoint  this approach failed
pretty drastically at all levels of detail  the hope would be that the k nearest neighbors would
contain solos coming from the same chord progression  but this did not happen  on average 
the value of smallest k for which the kth nearest neighbor of a datapoint came from the same
chord progression was the very close to random  except for the beat level of detail  where the
datapoints were just too close together 
the next thing to try is modifying the cost functions  intuitively  the number of notes
played  which is somehow measured more by the insertions and deletions  is less important that
the note choices themselves  and inserting or deleting an eighth note is less important than a
quarter note or half note   in fact  it seems it should be half as important   unfortunately  this
did not change the output of my tests  i wanted to change the cost function of note changing
so that it fit my dataset better  but at the time when the tree edit distance is being computed
you know only the notes  and  effectively  every note is as good as every other  only relative
note choices matter  so this method was abandoned 

 

finaive bayes
since trying to say how different things are based on the tree like structure didnt work out  we
want to ask the following question  based on a bag of notes  what chord or chords inspired
this  of course  there are some details to consider  like a quarter note counts twice as much as
an eighth note  etc 
in this version of naive bayes there are many more buckets than  say  in spam classification 
because the things we are trying to classify between are chord progressions of a given length 
there are also many less tokens in each document in each document  since the tokens are
notevalues and the documents are melodies of a certain fixed length  luckily  everything ended
up being quite tractable  with interesting results 
for training  pick a level of detail and chop up and normalize the dataset  and for each
mini chord progression make a histogram  using laplace smoothing  of all the notes seen  using
assigning points to the histogram based on the note duration  this is good for figuring out
what each  should be  but has a bit of an issue which i will discuss later  all of the apparent
priors should be divided by       since each datapoint can be transposed into any of the   
keys with equal probability  but there is no need to explicitly process each datapoint    times 
interestingly enough  i found that there are only    harmonically unique measures in my
dataset     harmonically unique half phrases and     harmonically unique half phrases  there
were only   and   harmonically unique beats and half measures  these relatively small numbers
boded well for my the run time of my testing function 
for testing  one takes a snippet of a solo and sums up the total duration of each notevalue
in the measure  then of each of the normalized chord progression observed and each of the   
intervals  rank the combinations by probability  the reason i have it rank them rather than just
say what the highest probability one is that there are many many options  and often the second
or third or even fourth as the most likely was the correct answer  this is not surprising  as jazz
musicians do things like chord substitutions in their solos  which means they intentionally
imply the wrong chords for effect  but the histogram for the wrong chords will be similar
enough to the histogram for the right one  
one interesting thing about testing was that rather than discreet  one off tokens  like with
documents   i have notes which play for a certain amount of time  in naive bayes  if you see
the word buy a bunch of times you assume that the word was randomly selected that many
times  however  the duration of a note is a very big number  for reasons unknown to me 
a quarter note has duration        seeing a c played for a quarter note does not mean     
independent decisions were made  again  for forming the histogram this does not matter  but
for computing the relative probabilities one needs to compute the prior times that particular
 raised to the power of the number of occurrences  i had a nice logfloat data type which
acted like a regular number but was internally represented as a log of a number  so i never had
to think about changing multiplication to addition and exponentiation to multiplication   the
effect here is that the prior would be less important  because the naive bayes algorithm would
think it had over six thousand times more data than it actually did  a document with just
two words  both of which are buy  is more likely to be spam than a document with just one word
which is spam  regardless of the priors   my solution was to divide this exponent by       the
duration of an eighth note  this means that an eighth note counts as one independent choice  a
quarter note counts as two choices  and so on  it also means  somehow  that a triplet counts as
two thirds a choice and a sixteenth note counts as half a choice  my results were good enough
that this logical strangeness wasnt too bad a problem 
i did leave one out cross validation  implemented by subtracting entries from the histogram  rather than retraining the entire time   not testing datapoints with no notes  the

 

firesulting number is the average ranking of the correct answer divided by the number of possibilities  so a score of   means the top ranked progression was always correct            at the
level of detail of measures means it always ranked the correct answer last   my rankings were 
phrase        half phrase       measure        half measure       beat        beat i think was
particularly confusing for it because it had at most   or   notes to make its guess  many  but
not all  of the horribly misclassified datapoints were ones with far fewer notes than average 
which could happen  say on the measure level  if a soloist is resting on that measure except on
the very last beat  where they play a pickup to the next measure 

hidden markov models
the last thing tried for chord progression classification was a hidden markov models  i am very
glad i stumbled upon them while wikipedia grazing late in my project  because they model the
relevant question extremely well  a hidden markov model is like a markov chain except that
you do not know the current state  the idea is that there is a set of states and probabilities of
transitioning between the states  at each step  an observable is emitted based on a probability
distribution depending only on the state  and then the state randomly goes to a next state
based on the transition function  the input of a markov model problem is the things observed
 the solo  and the algorithm has to play detective and guess which sequences of states created
that solo  the progression  
in my hmm  the states were the chord progression for the last k beats  the observables
to be emitted were the one beats worth of notes  the transition probabilities were just came
from what was observed in the chord progressions in our dataset  taking whole songs at a time 
rather than chopping the solo up into pieces  although we still do the normalization and    time
covering   like in naive bayes  we assumed that the note output over a one beat period were
independent and depended only on the state  that is  the harmony for the last k beats  
this is slightly different than the genie and urn description of hmms  because in that
description you have a complete list of observables and the probability of each observation
given each state  in my case  i could just  given an observation  compute the probability i
observed that given a state  this meant being unable to use hmm libraries   for testing  my
first try is i use viterbis algorithm to compute the most likely sequence of states  which was
straightforward to implement from wikipedias pseudo code  this was remarkably effective in
leave one out cross validation  often times the viterbi output was spot on  or could be off at a
phrase or so  other times it would be completely off 
i computed the average faction of chords correctly classified by the viterbi output for
various values of k in leave one out cross validation  again  being smart not to retrain the whole
thing every time   my results were  it took a long time to run  so i only did a few values of k  
for k            k            k            k           k             i think that k too large was
confusing because it was able to have less data on each possible set of last k measures 

data stolen from  thank you  
   ralph patts vanilla real book http   www ralphpatt com vbook html
   prof bert ligons transcriptions http   www music sc edu ea jazz transcriptions html
   jazztrumpetsolos com
   sokillingit com

 

fi