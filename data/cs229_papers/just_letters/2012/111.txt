understanding the effectiveness of bank direct marketing
tarun gupta  tong xia and diana lee

   introduction
there are two main approaches for companies to promote their products   services  through mass
campaigns  which target the general public population  and directed campaign  which targets only a
specific group of people  formal study shows that the efficiency of mass campaign is pretty low  usually
less than    of the whole population will have positive response to the mass campaign  in contrast  direct
campaign focuses only on a small set of people who are believed to be interested in the product service
being marketed and thus would me much more efficient  here in our project we focus only on the direct
marketing data  our goal is to predict if a customer will subscribe the service provided by the bank 
thereby improving the effect of direct marketing 

   data pre processing
we are using data from direct marketing campaigns  phone calls  of a portuguese banking institution 
there are       records  which of which is composed of    features and   binary response  the
following table gives a brief introduction of all features 
name
age
job
marital
education
default
balance
housing
loan
contact
day
month
duration
campaign
pdays

attribute
numeric
categorical
categorical
categorical
binary
numeric
binary
binary
categorical
numeric
categorical
numeric
numeric
numeric

previous
poutcome

numeric
categorical

description
age of the customer
type of job  e g  management  entrepreneur  housemaid  student
marital status
education level  e g  unknown  secondary  primary  tertiary
has credit in default 
average yearly balance in euros
has housing loan 
has personal loan 
contact communication type  e g  telephone  cellular  unknown
last contact day of the month
last contact month of year
last contact duration in seconds
number of contacts performed during this campaign and for this client
number of days that passed by after the client was last contacted from
a previous campaign     means client was not previously contacted
number of contacts performed before this campaign and for this client
outcome of previous marketing campaign  e g  success  failure  other 

the outcome is a binary variable indicating if the client has subscribed a term deposit  the percentage of
positive outcomes in the data is       this gives us a benchmark test error of      when all outcomes are
negative  so  we should focus on recall and precision 

fiwe consider contact  month and day to be non interesting attributes and ignore them in this analysis 
     categorical attributes
some algorithms like svm and k nearest neighbours only accept numerical attributes  thus  the attributes
have to be converted appropriately to numeric values for these algorithms  binary attributes have been
mapped to  no      yes     values  ordered categorical attributes have been converted to discrete values 
for example  education level has been mapped as  primary      secondary      tertiary      and
poutcome is mapped as  failure       success       other unknown       for unordered attributes  the
categories have been duplicated to create additional features 
     feature scaling
it is important to scale the continuous numeric features to improve data quality  for example  balance
attribute contains values like        this can cause multiple  in svm  this value will blow up for highdimensional polynomial kernels causing numerical errors in kernel inner products  in regression  the
feature matrix may become ill conditioned or a single feature may dominate other features with small
numeric ranges 

   model selection
     mutual information
the following list gives attributes in the order of mutual information of a single feature with the outcome 
 balance   poutcome   previous   bluecollar   single   management   technician   loan 
 admin   divorced   housing   services   married   retired   selfemployed   entrepreneur 
 housemaid   campaign   student   default   education   pdays 
 duration   age 
     stepwise search algorithm
we have carried out stepwise model selection in r using aic which maximizes the likelihood function
for the estimated model  both forward and backward search yield the same model 
y   duration   poutcome   housing   job   loan   campaign   education   marital   balance
     ensemble classifier
we plan to carry out an ensemble technique like random forests  we can combine the best classifiers to
obtain our predictions  in this report  we will combine the classifiers  logistic regression  adaboost 
naivebayes and random forests  to generate an ensemble to improve the performance across atleast one
of the metrics  test error  recall and precision  for each classifier  the final prediction is made by taking
a majority vote among the four classifiers 

fi     svm parameter tuning
we need to study the variation as we move the decision boundary using a precision recall curve  this is
accomplished by varying the parameters  c    in the svm model  the table below shows the results
obtained for different parameters using     cross validation  we can observe the precision recall tradeoff in the following results  we also note that the test error increases significantly when we try to improve
the recall by using a high value of parameter c  the training error goes down monotonically with
increasing values of c and  

     
    
   
   
   

c
 
 
 
  
   

training error
     
     
      
      
      

test error
     
      
      
      
      

recall
      
      
   
     
     

precision
      
      
     
     
      

cross validation to find best parameter values for svm  to find the best tuning parameter c of the svm
learning algorithm  we give a list of candidates for c and   for each of these candidates  we estimate the
   fold cross validation error  the optimal c is identified as the one that minimizes the cv error  we then
fit the svm model with the optimal c on the whole training set and use it for future prediction  we use
rs tune function to perform cross validation on a  c    grid using     of the data set  the optimal
parameter values are obtains as  c                 the results seem to change slightly when the tuning is
performed again  this can occur because each time the data is partitioned randomly 

   results
we randomly choose     of the data as the training set  and the remaining as the test set 
algorithm
logistic
svm
naivebayes
decision trees
adaboost
random forests  ntrees      
random forests  ntrees       
random forests  ntrees        
ensemble  lr nb ab rf 

training error
      
      
      
      
      
      
      
      
       

test error
      
      
      
      
      
      
      
      
      

recall
      
      
      
      
      
      
      
      
      

precision
      
     
      
      
      
      
      
      
      

fifor all the methods except naivebayes  the precision is higher than      this is pretty good  note that
the percentage of positive outcomes is      so  compared to mass  random  campaign  our learning
procedure is much more efficient 
however  on the other hand  the recall is lower than     for all the methods  this means that we are
missing more than half of the potential customers  clearly this result is unacceptable  in the next step  we
would focus on how to increase the recall 
evaluation of performance metrics 
we present the recall precision curve for this application using different classifiers  the curve is obtained
by changing the decision boundary threshold in the case of logistic regression  nave bayes and decision
trees  in the case of svm  we obtain the curve by assigning different weights to the positive and negative
class 

we also observe that for a high recall  low precision setting  the test error goes up significantly due to
increase in the number of false positives  in svm  the low precision        and high recall        values
correspond to a test error of      but  we are more interested in recall for this application and accept this
increase in the number of false predictions to capture most of the potential customers 

fi   conclusions
we have implemented several machine learning algorithms for this application  although these
algorithms fail to be significantly accurate for this application  we are able to obtain high recall classifiers 
svm outperforms other classifiers in terms of the performance on the recall precision curve  logistic
regression seems to work well and narrowly loses compared to svm 
the ensemble classifier improves the performance in at least one of the metrics compared to using an
individual classifier 

fi