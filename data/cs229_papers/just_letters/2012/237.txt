predicting acceptance of github pull requests
nikhil khadke  ming han teh  minghan shen
 nkhadke  minghan  sminghan  cs stanford edu
fall       cs    
 

   
problem statement
we define the pull request prediction problem as follows 
given a pull request created at time t  decide if the pull
request will be eventually accepted or rejected within the
interval from time t to a future time t  if the pull request
remains open after time t  we assume the pull request has
been rejected 

introduction

social coding tools such as github     have transformed
the way software gets developed collaboratively and
openly on the world wide web  github has various
avenues and features for collaborative development  but
perhaps the most fundamental feature that allows nonlisted collaborators to contribute to a repository is a pull
request  a pull request     is a formal request from a
potential contributor to a project owner to have his her
code improvements incorporated into the codebase 
because pull requests are made from external  unlisted
contributors  there is an element of uncertainty in whether
a pull request will be accepted  while there are certain
guidelines and factors that may approximately indicate a
successful pull request  the overall result is not clear and
deterministic  we aim to use machine learning techniques
to glean insights into what contributes to a successful pull
request  we believe that if we can model pull request
prediction accurately  we may be able to concretely
understand the mechanisms that motivate successful pull
requests  and use this to improve the collaborative
software engineering process 
 

 

data collection and processing

   
dataset description
our primary data source is the github archive     data that
is available for querying via googles bigquery  because
all data is stored in a columnar large table  any explicit
relational information is lost  and we had to spend time
understanding what the columns corresponded to  we
used google bigquery to extract relevant data by filtering
data on related event types 
   
data processing
once a resulting data has been generated using google
bigquery  we export the data over to google cloud
storage in comma separated values  csv  format  we
then download the csv file  do some pre processing  and
finally import the data into sqlite  tables for easy querying 

preliminaries

   
data crawling and feature extraction
in order to create our training and testing data  we
extracted      pull requests events that were made in the
window from            to             after data
sanitization and filtering  this resulted in      usable data
points  for some features such as contrib prob and
repo prob  as reflected in table     we ran queries on
github archive using google bigquery to get information
such as the number of successful pull requests and total
number of pull requests made for each contributor and
repository 

   
terminology
borrowing github terminology  we define the three states
of a pull request at any point in time as 
i  open  pull request issue is awaiting decision on
whether it will be accepted or rejected
ii  rejected  pull request was rejected
iii  accepted  pull request was merged into the code
branch 
typically the contributor and owner discuss the pull request
via comments till a decision is reached  additional revisions
may also be required on the committed code  in practice  it
is possible for a pull request to get accepted even though it
had been rejected initially  additionally  some pull requests
are not explicitly rejected  but remain open for an indefinite
amount of time  pull requests sometimes remain open
because the project owner lacked the time to act on the
pull request or simply because the owner did not want to
discourage the contributor by explicitly rejecting the pull
request 

in search of predictive features  we realized our data set
did not have an explicit representation for some of the
features we were interested in  for example  the number of
collaborators for a given repository was not directly
provided  and we had to use the github api to extract this
information  because data fetching via the github api is
slow and we are only limited to only      api calls an
hour  we wrote a web cache to cache all pages that we
fetched 
 

fifeature

description of feature

justification

changed lines

number of line changes in
patch 

the longer the change  the more scrutiny it will face and consequently affect
its pull request acceptance 

changed files

number of file changes in
patch 

if a few files are changed  the change may be simple and this could affect
its pull request acceptance 

commits

number of commits in patch 

the number of commits in a patch may indicate the complexity of a patch 
and this could affect its pull request acceptance 

open issues count number of open issues on the
head repository 

a repository with many open issues may be more willing to accept pull
requests to fix these issues 

watchers count

number of watches on the head we suspect that if a repository is highly watched  it may be popular and
repository 
more active in accepting pull requests 

forks count

number of forks on the head
repository 

the number of forks is a loose indicator of the developer interest in the
repository  which may affect pull request acceptance 

pushed delta

time difference in seconds
between last push activity on
the head repository 

this quantifies loosely the activity of the repository  which may affect the
likelihood of pull request acceptance 

repo collaborators
 count

no  of collaborators in project 

we suspect that large repositories may be more willing to accept pull
requests 

contrib prob

the pull request success rate
for the contributor 

we assume that a contributor high pull request acceptance rate has a
higher chance of getting an accept 

repo prob

the pull request acceptance
rate for the repository 

we assume that a repository with a high pull request acceptance rate has a
higher chance of granting an accept 

ngram title

the probability of the pull
request title text n gram vector 
given that the pull request is
accepted 

we try to identify word n grams in the title text which are more persuasive
and lead to a better chance of acceptance 

text sim

the text similarity between lines
added and deleted  we used
the text similarity function
available in pythons
difflib sequencematcher     

we realized that features such as changes lines and changed files do not
accurately measure the engineering complexity of changes made  for
instance  a pull request that alpha renames variables names would have a
higher chance of being accepted compared to one that implements a new
library  as such  text similarity aims to measure the complexity of the
change involved 

table    features with their corresponding description and justification
 

logistic regression model and performed a combination of
forward and backward search on the feature set to
determine the predictive set of features for our dataset  we
experimented with using multiple models to find a model
that best predicts our data  for any given model  we used
stratified k fold cross validation  k    with     test and
    train on a dataset of      pull requests  we
calculated the f  scores and accuracy of each classifier to
compare and contrast their performance  for every model 
we tried to improve the models predictivity by using
techniques such as feature selection and parameter tuning
via grid search maximizing the classifiers objective
function 

methodology

as described above  we wrote a lightweight python
framework  on top of scikit learn      a python machine
learning library  that allowed us to fetch relevant github
data to be used as our input features  this pipe and filter
framework is summarized below 

figure    pipe and filter framework used to train and test
various models 

 

results

we begin with describing our feature choice and the
process of using different machine learning techniques to
arrive at reasonable models to solve the pull request
prediction problem  when comparing accuracies  we note

for our evaluation  we used a value of    days for t   t 
because it represented an identifiable time period of  
month  additionally  we scaled all features by using
standardization  such that each feature is scaled to zeromean and unit variance  to select our features  we used a
 

fithat baseline probability of pull request being accepted is
       

feature
changed files
log    changed lines   
commits
open issues count
log    watchers count   
log    forks count   
pushed delta
repo collaborators count
contrib prob
repo prob

   
feature choice
we summarize the description and intuition on our feature
choice in table    the head repository refers to the
repository being asked to accept a pull request 
   
logistic regression
we use logistic regression as an initial method to build a
model for the pull request prediction problem because of its
ease of implementation and interpretation 

mean
std dev
accuracy
      
      
      
      

mean
f 
      
      

std dev
      
      
      
      
      
      
      
      
      
      

table    weights for different features in l  regularized
logistic regression 

      logistic regression and regularization
using the features described above  we achieved the
following results 
regularization
metric
l 
l 

coefficient
      
       
       
      
       
       
       
      
      
      

from table    we notice that the most predictive features
are repo prob  commits  pushed delta and contrib prob 
because contrib prob and repo prob are based on the
recent history of pull request acceptance  it makes sense
for these values to be very predictive of pull request
acceptance  we notice that repo prob is more than twice
as predictive as contrib prob and pushed delta was more
predictive than contrib prob and most other contributor
features  this highlights that perhaps repository based
features may be more useful in predicting the acceptance
of a given pull request  we also see that commits are the
second most predictive feature  and it intuitively follows
that the more commits there are in a pull request  the more
complex it is  reducing its chance of acceptance  what was
surprising to note was the very low importance of
open issues count  since we would expect that the more
open issues a repository had  the higher opportunity of
using and accepting potential pull request contributions 
this may indicate that most of these repositories had
issues that were only for the jurisdiction of formal listed
contributors 

std dev
      
      

table    results of logistic regression on initial feature
set
we observed that l  regularization performed slightly
better than l  regularization 
   
feature refinement
as seen in table    both l  and l  regularization were not
able to reasonably improve our accuracy by implicitly
penalizing features  this encouraged us to refine our
features explicitly  when inspecting the coefficients for
each feature in standard regression  we noted that
ngram title had an absolute coefficient of        while the
other features had absolute coefficients in the range of
                  indicating that ngram title severely overfit
the training data  reducing the overall accuracy  as a
result  we decided to eliminate this feature  to further
improve our feature set  we ran a backward search and
isolated text sim as an undesirable feature and eliminated
it  lastly  we inspected our training data  and realized that
there were significant outliers  and to reduce their effect on
the predictivity of our model  we used log transforms on
features that took up large values  we applied a log base  
transform on the following features  changed lines 
watchers count  forks count 

grid search over parameters
to further improve this accuracy  we performed a grid
search over the main parameters for logistic regression
 l    c and tolerance  the new parameters c      and
tol     yielded a meager improvement in accuracy from
      to       

logistic regression on the new feature set yielded the
following results 
regularization
metric
l 
l 

mean
accuracy
      
      

std dev mean
f 
             
             

std dev
      
      

table    results of logistic regression after feature
refinement 

figure    logarithmic grid of parameters c  penalty
margin  and tol  tolerance  for l  logistic regression
 

fifigure    l  logistic regression
learning curve

figure    random forest classifier figure    tuned svm  rbf kernel 
learning curve
learning curve
parameter tuning
we realized that for the radial basis function  rbf  kernel
svm  setting correct hyperparameters are essential to
achieving good results  we used a grid search over the
main parameters to the rbf kernel svm to arrive at
     and c        this improved our accuracy
marginally from        to         but with a slightly higher
variance  we summarize our results and logarithmic plot in
figure   and table   

looking at the learning curve for l  logistic regression  fig 
    we observe that there is high bias in the model 
because error is high and there is only a small gap
between accuracies for data set size greater than     
points  the accuracy graph is still increasing  which implies
that a larger training set may improve performance 
   
decision trees
in an attempt to improve the accuracy of our model  we
used decision trees and its ensemble variant random
forests  using default parameters we obtained the
following results 
method
decision trees
random forest

mean
accuracy
      
      

std
dev
      
      

mean std dev
f 
             
             

table    results of decision tree methods 
we observe that the accuracy for decision tree is lower
than random forest  probably due to the decision tree
overfitting the training data  in practice  random forest
classifiers are more robust to outliers  and less likely to
overfit the data compared to the simple decision tree     
however  the results for the random forest classifier still
look suspicious  looking at the learning curve for random
forest in figure    we see that training accuracy is
significantly lower than testing accuracy  confirming our
suspicion that the random forest classifier is a highvariance estimator which over fits the data 

figure    logarithmic grid of parameters c  penalty of
error term  and gamma  kernel coefficient for rbf  for
svm
svm type
rbf
parameter
tuning  rbf 

mean
std dev
accuracy
      
      
      
      

mean
f 
      
      

std
dev
      
      

table    results of parameter tuning the rbf kernel
svm on the refined feature set 

   
support vector machine
as it was not possible to reduce the high variance in the
decision tree models after parameters tuning  we
attempted to address this issue by using a support vector
machine which is generally effective with large features 
we used the refined features reflected in table   

the learning curve in figure   for the tuned svm shows a
quantitative view of our results  informing us that accuracy
reaches a plateau as training size increases  as compared
to the learning curve for our other models  our tuned svm
model shows a better balance between bias and variance 
 

fi
 

discussion

comparing results across table          we see that while
the random forest classifier has the highest accuracy and
f  score  its learning curve suffers from overfitting with
high variance  on the other hand  the logistic regression
classifier suffers from high bias  with a good accuracy rate
and tolerable deviation  it appears that svm  giving       
accuracy against the baseline of          is the most
satisfying model we have tried 

 

in the future  we would like to spend more time crawling a
richer dataset from github and use this to build a richer
feature set  we also plan to reevaluate our prediction
based on each new event for a particular pull request 
additionally  given more time we want to enrich our model
with features based on sentiment analysis on pull request
comment logs 
 

references

   
github  social coding   online  available 
http   github com 
   
using pull requests  online  available 
https   help github com articles using pull requests
   
github archive  online  available 
http   www githubarchive org 
   
scikit learn  online  available  http   scikitlearn org 
   
difflib  helpers for computing deltas  online 
available  http   docs python org   library difflib html
   
l  breiman   random forests   j  machine learning
     pp        oct     

figure    plot of t t window in terms of days against f 
score and accuracy for various models  l  logistic
regression  left  and svm  right 
by looking at the feature coefficients assigned to each
feature  table     we gleaned some insight into pull
request acceptance 



conclusion and further work

building a highly predictive model for pull request
acceptance is a difficult problem as pull requests represent
informal patch based contribution and subsequently are
highly variable  using the methods outlined above  we
have presented reasonably predictive models such as the
parameter tuned svm and random forests that are able
to predict a pull requests acceptance with an accuracy of
       and        respectively  much better than the
       baseline 

curious as to how our problem definition of using fixed time
window restricted our experiments  we decided to evaluate
our models for values of t t for step intervals of   days  we
excluded the random forest classifier from this analysis as
it had too high variance to be useful for this portion of the
analysis  from figure    we see a general trend of
accuracy and f  scores increasing as the time window
increases  the metrics peak at around a time window of   
days     months   before decreasing slightly  this shows
that the fate of the majority of pull requests get decided
within   months  this window might seem long  but is
supported in practice as decisions on pull requests are
affected by project activity and management  and the
persuasiveness of the contributor to get his code accepted
in back and forth comments 



the track record of the contributor plays a role in pull
request acceptance  since it is an indicator of the
quality of their work 

changes to code should be succinct  having too many
lines changed reduces the chance of acceptance 
if possible  the contributor should not put off the pull
request until the branch has multiple commits different
from the master code  because pull requests are made
by people not directly involved in the planning of the
project  a high complexity change often implies that the
contributors intention differs from the authors original
intent of the code  multiple commits imply a complex
change and reduces the chance of acceptance 
 

fi