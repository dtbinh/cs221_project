deep learning for time series modeling
cs     final project report
enzo busseti  ian osband  scott wong
december   th      

 

energy load forecasting

table    results for different learning models
learning method
rmse   rmse
kernelized regression
     
    
frequency nn
     
    
deep feedforward nn
     
    
deep recurrent nn
   
    

demand forecasting is crucial to electricity providers because
their ability to produce energy exceeds their ability to store it 
excess demand can cause brown outs  while excess supply
ends in waste  in an industry worth over    trillion in the u s 
alone      almost    of gdp      even marginal improvements
can have a huge impact  any plan toward energy efficiency
should include enhanced utilization of existing production 
energy loads provide an interesting topic for machine
learning techniques due to the availability of large datasets
that exhibit fundamental nonlinear patterns  using data from
the kaggle competition global energy forecasting competition        load forecasting     we sought to use deep
learning architectures to predict energy loads across different
network grid areas  using only time and temperature data 
data included hourly demand for four and a half years from
   different geographic regions  and similar hourly temperature readings from    zones  for most of our analysis we
focused on short term load forecasting because this will aide
online operational scheduling 

 

figure    load prediction with recurrent neural network

deep neural networks

 

we successfully implemented deep learning architectures for
forecasting power loads and found that this produced superior
results to both linear and kernelized regression for our given
data  we found that  due to the huge dataset we were able to
implement complex nonlinear models without encountering
much problem of over fitting  deep networks allowed us to
add significant complexity to our model without specifying
what forms the variation should take  many papers extol the
benefits of greedy layer wise unsupervised training for deep
network initialization               i e   stacked autoencoders
and restricted boltzmann machines   which was the starting
point for our work  see appendix c   this is in contrast to the
prohibitive scaling properties of either polynomial regression
or even nonparametric gaussian processes  which scale o m   
with data     
our most successful iteration was a recurrent neural network with rmse of     kwh h and       correlation to test
data see table     figure   shows a sample comparison of
actual load demand in blue  from the test data set  and the
recurrent networks prediction in red  tesla energy forecasting boasts mean absolute percentage error of           
     suggesting our neural network implementations are beginning to approach the best of the private sector 

review of existing techniques

there has been extensive research performed in the area of
energy load forecasting      and in particular  the efficacy of
neural networks in the field      additionally  numerous consultancies offer demand forecasting as a service to producers 
while other producers have taken such analysis in house  difficulties in implementation and lack of transparency of results
have been cited as their main weaknesses  our hope was that
through implementing a deep architecture as per the ufldl
tutorials we could successfully unearth complicated underlying features without specific oversight from the modelers
perspective 

 
   

step     the linear model
linear models as the degenerate nn

the underlying premise of neural networks is that multiple layers of generalized linear models can combine to produce non linear outputs  linear regression can therefore be
thought of as a special case for a degenerate neural network
with no hidden layers  neural networks are complicated  with
significant programming challenges and non convex optimiza 

fitions  so we first wanted to establish that they really do provide some worthwhile reward over simpler models 
after considering some necessary state space transformations to our data  appendix a  we implemented least squares
regressions over a simple linear model  unsurprisingly  this
suffered from severe high bias as our model was far too simple  repeating this exercise for quadratic and cubic features
we were still able to train over the whole dataset  but the
problems of high bias remained still 

   

once again  computational expense proved to be our limiting factor and we were only able to complete the kernelized
regression with     clusters per geographic zone  this produced disappointing results as the means over simplified our
dataset and our kernels were unable to scale to the necessary
data sizes 

 

step     neural nets

neural networks are a more principled way to add complexity
on large datasets  we implemented feedforward networks to
explore model parameters and then augmented these with
recurrent structure to improve forecasting accuracy 
we chose our model parameters through k folds validation
on our data  including  sample set size  feature engineering 
anomaly filtering  training algorithms  and network topology 
due to the computationally intensive nature of this process 
only feedforward networks were considered at this stage  unless otherwise stated  the networks were trained with     of
the data      was used for validation stopping  and     for
testing  chosen at random   this was repeated    times per
experiment to average out sample error 

scaling issues with large data

this process of feature creation was wasteful as full polynomial expansion gave many features without statistical significance but still necessitated lengthy calculations  using subset
selection methods  such as forward backward stepwise or the
lasso  and only expanding relevant variables helped weed
these out  however  this was only computationally possible
on smaller subsets of the data  which might cause us to lose
valuable training information 
to maintain most of the information of the dataset while
facilitating more complex calculations  we implemented kmeans clustering  running the algorithm until convergence
was prohibitively slow  however we found good results with
some expedited heuristics  using different numbers of clusters and performing a naive nearest centroid classification
we saw improved results as we increased model complexity
 figure     we found that clustering was more effective using
feature standardization  mean zero unit variance  

   

learning curves

to assess levels of bias and variance in a mid sized feedforward network  two hidden layers of    and    neurons  respectively   we trained the network with varying proportions
of training data and examined the learning curves  figure  
summarizes the results  once again demonstrating a high bias
problem even with this complex neural network  this led us
to focus our work on developing new features and incorporate
increased model complexity 

figure    cross validation of number of clusters using nearest
neighbor

figure    learning curves show high bias

   

kernel methods add model complexity

   

in a final bid to add more model complexity  we implemented
a kernelized local regression based upon squared exponential
distance to centroids  we found that our results were improved by additionally considering the number of data points
forming part of each cluster 

feature engineering

while neural networks have the benefit of being able to fit
highly non linear models  modeler insight of how to represent
the data can be equally important  to determine the impact of feature engineering  we trained feedforward networks
with additional hand engineered features  see appendix a  
 

fithe features were introduced cumulatively  and    models transforms in figure   and appendix a  our conclusion was
were trained for each feature set  figure   shows that human that  except for the most basic transforms  it is better to allow
added features can make a significant difference in the overall the neural network to find the nonlinear relationships itself 
performance of the model  in particular  circular seasons and
hours and the day of the week feature decreased rms error
     and       respectively  that being said  models that
can realize more complexity than the mid sized network  i e  
deeper networks and recurrent networks  have had even better
performance  and demonstrate the power of deep learning 
figure    feature engineering

figure    the right axis shows rmse of the network in transformed space with various sizes of the time window of the
transform  the left axis shows the number of samples in the
dataset  training stops when the backpropagation algorithm
reaches a minimum or we complete      iterations  performance is evaluated against     test data 

   

   

fourier transform network

anomaly filtering

literature     suggests that filtering time series data for
anomalies may reduce error for neural networks  we use a
mixture of gaussians to estimate the least likely data  and
label it as anomalous  we fit ten through     multi variate
gaussians to the data in    dimensional feature space  from
feature engineering   and identified the lowest         and
    likelihood data with anomaly features  neural networks were then trained in this new feature space  however
there was no discernible improvement in the generalization
error of these models 
diving deeper into the data  figure   shows the anomalous
data points identified by mixture of gaussians in red  embedded in   d with pca  and how it maps to the time series  mixture of gaussians identified peaks in demand in the
summer and winter  which were not particularly anomalous 
gaussian methods also failed to identify known outlier data 
such as the huge power outages experienced on january   
      marked in green in figure     although we might hope
these green points are flagged as outliers  they were actually
probable data points in the gaussian model  as you can see by
mapping to the first two principal components  it is possible
that alternative methods could provide modeling gains  but
we decided this was not worth pursuing 

to exploit the strong periodicity of the data  see figure   
we tried to predict the power loads in frequency space  using
modified discrete cosine transforms  appendix b   to perform a discrete fourier transform we first need to fix a time
window size  l  to split the time series  a natural choice is   
hours  because the daily pattern of power load variation and
temperature are very consistent  we also tried smaller time
windows  down to   hours 
the transformed dataset is made of samples  x i    y  i   
where x i  is the transform of the i th time window of the
temperature time series for all    zones  plus the date  and
y  i  is the transform of the i th time window of the power
loads time series  this reduces the number of samples by a
factor l  each x i   r  l   and each y  i   rl  
we used a network topology with two hidden layers  the
first had the same number of nodes of the input x i  and the
second the same of the output y  i    to speed up computation
and reduce risk of overfitting we applied pca to x i  and
worked on the first     of the total variance  significantly
reducing dimensionality 
the resulting rmse of the network for different sizes of the
time window of the transform are shown in figure    as we
increase the size of the time window we     reduce the number
of samples in the dataset and     increase the dimensionality
of each sample by  l  the complexity of the model thus
increases while simultaneously the dataset becomes smaller 
we expected better results from the fourier transform network model than the non transformed model  since it should
help the network find repeating patterns  however this was
not the case  this was in contrast to the more elementary

   

training algorithm

neural networks  and in particular deep neural networks  are
notoriously difficult to train due to their non convex objective
functions  we examined five training algorithms to determine
the best and fastest solution  levenberg marquardt  lm 
a combined quasi newton and gradient ascent algorithm  was
 

fifigure    anomaly filtering

   

network topology

performance of the networks varies highly depending on the
number of hidden layers and neurons per layer  figure   illustrates network performance as a function of complexity
 number of weights   feedforward network achieve near optimal performance around       weights    layers of        
and    neurons  respectively   with rms error of        additional complexity through more neurons did not improve
performance  but deeper networks occasional produced better results  however  performance was erratic due to finding
local non global optimum  
figure    topology selection

found to converge fastest and almost always produce the best
error  bfgs  quasi newton   scaled conjugate gradient 
and conjugate gradient with polak ribire updates were all
slower and had worse error than lm  lm with bayesian regularization produced the lowest error with   layer networks
 though  not with     but convergence time was prohibitively
long to be useful  table   summarizes rms error for the
different algorithms on different sized networks 
table    training algorithms
rmse
training algorithm
levenberg marquardt
bfgs quasi newton
bayesian regulation
scaled conjugate gradient
conjugate gradient

   

  layer

  layer

     
     
     
     
     

     
     
     
     
     

recurrent neural networks performed drastically better for
comparable complexity  all recurrent networks shown in figure   are   layers  we experimented with deeper networks 
however training time increased rapidly  i e     hours per
model   best   layer results were achieved with    and   
neurons per layer  respectively  one input tap delay  and one
through    hours of feedback tap delay        weights  

recurrent neural networks

using a purely feedforward network we ignore some of the
temporal structure time series data presents  in particular 
this structure can be exploited through the use of recurrent
neural networks where we explicitly construct a sequential
representation of our data 
recurrent neural networks may have multiple types of feedback loops  the two that were employed here were input
delays and feedback delays  each type of delay effectively increases the number of input nodes by providing the network
with delayed information along with current information  in
the case of input delays  multiple consecutive time steps of
the input features are presented to the network simultaneously  for feedback delays  the output of the model is provided to input nodes  along with previous data  this can
either be done with open loops  where the known output
is provided as an input  or with closed loops  which connects the network output to the input directly  here  we have
trained and forecasted one step ahead with with open loops 
to predict farther ahead  closed loops need to be used  briefly
experimenting with closed loops  error increased     due to
clipping of peak and trough demand 

 

conclusion

deep neural networks are able to accurately gauge electricity
load demand across our grid  it is clear from table   that
recurrent neural networks outperformed all of our competing
methods by a significant margin  we believe that  in general 
deep learning architectures are well suited to the problems of
power load forecasting since we have large availability of relevant data with complicated nonlinear underlying processes 
we were disappointed with our output using competing
nonlinear kernelized methods  and perhaps using pre built
svm packages would have produced superior results  due to
their poor scaling properties we did not pursue our initial work
in gaussian process modeling  however  potential for sparse
computational approximations might provide interesting future avenues of research            we believe that within deep
learning  cutting edge techniques such as restricted boltzmann
machines might allow us to train more complex models efficiently 
 

fiappendix

day  and so on  the original signal is recovered by summing
the overlapping parts of each inverse transform  time domain
aliasing cancellation   it is well known that minimizing error
a transformation of variables
in the frequency space is equivalent to the original problem 
within our dataset  it was important to frame our feature we were motivated by mp  compression  which uses similar
variables in ways that were conducive to sensible models  and techniques  to use this to express our data in a more efficient
exploiting natural symmetries  for example  each training manner 
example was taken from one of    distinct geographical zones 
since we had no idea  a priori  of how these zones relate we
stacked autoencoder
found better results modeling zone z  r   than naively z  c
               
building on the literature      we trained feed forward neural
on the other hand  much of the observed structure to the networks with stacked autoencoders  greedy layer wise traindata was found to be periodic in nature  daily  weekly  sea  ing         has been shown to improve network performance
sonal   with that in mind we took care  particularly for in scenarios where large amounts of unlabeled data is availour linear models  to reflect this a priori symmetry in our able  but minimal labeled data  while all of the data used
data  for variables with inherent symmetry present  hours     in this paper are labeled  it was our hope that greedy layermonths    etc  we transformed to polar coordinates via a wise training would help avoid local non global minima in the
 sin cos  pair of variables to seek solutions of given period  optimization problem  however  results with this technique
icity 
were discouraging  with no significant performance gain but
added computational complexity 
figure    transformation of variables

references
    http   www eia gov totalenergy data annual 
pdf sec     pdf
    http   www bea gov itable error nipa cfm

    http   www kaggle com c 
global energy forecasting competition      load foreca
    gaussian processes for machine learning   rasmussen
and williams
    y  bengio  learning deep architectures for ai  foundations and trends in machine learning  vol      no    
     

 x
figure    x   y    y       sin   x
    cos      we show x as
a red dot mapped to y in blue  we can then fit the desired
relationship as linear in r  for the new circular y features

b

frequency space transform

    bengio  y   lamblin  p   popovici  d     larochelle  h 
        greedy layer wise training of deep networks  in
scholkopf  b   platt  j     hoffman  t   eds    advances
in neural information processing systems     nips    
pp           mit press 
    http   www teslaforecast com 
    j  connor  r d  martin  and l e  atlas  recurring neural networks and robust time series prediction  ieee
transactions on neural networks  vol     no     march
     

more generally  we might want to exploit the periodicity in the
data  figure   when we are not sure about the exact period 
    http   repository lib ncsu edu ir bitstream 
for example  we know there are pattens around the work               etd pdf
ing day which might be more easily seen in the transformed
fourier space  we implemented the modified discrete cosine      hinton  g  e   osindero  s     teh  y  w          a fast
learning algorithm for deep belief nets  neural computatransform  mdct       which performs a discrete transform
tion                   
over fixed size windows returning real coefficients and wellbehaved edges 
     http   arxiv org pdf          v  pdf 
the time windows overlap by half of their length  for example in case of    hours time windows the first sample starts      malvar  henrique s  signal processing with lapped transat   and ends at    of the first day  the second sample from   
forms  artech house  inc        
to    of the second day  the third from   to    of the second
 

fi