cs    project report  recommending movies and tv shows
based on facebook prole data
benjamin paterson  paterben stanford edu 
weifeng zhang  wfzhang stanford edu 
tim mwangi  tmwangi stanford edu 
december         

abstract

compared with the table to generate labels for each
user prole  these labels are represented in y 
         as each user can like or not like each genre 

the aim of this project is to learn how to make predictions on what genre of movie a user is likely to be
interested in based on their raw facebook data  we
collaborated with other groups to collect a dataset
of   k unique users which was then parsed  stemmed
and tokenized  we trained two predictors each using a dierent model  svm and naive bayes  in order to correct for the skew in the training sets  we
generated an independent unskewed dataset that was
a subset of the original data  and compared performance of the classiers using several metrics  accuracy  mcc  auc   we then attempted several improvements  rst to the naive bayes model itself by
using tf idf weighting and document vector length
normalization  second to the features themselves by
ltering tokens using mutual information  the combination of tf weighting and feature selection yields
signicantly better performance in terms of mcc and
auc 

raw data processing
the elds of raw data  interests  about  etc   for
each user are concatenated into a single string and
then stemmed using the python nltk library  punctuation  stopwords  non alphanumeric characters are
eliminated to produce a single list of about      
numbered tokens which acts as the vocabulary  each
word in each prole is converted to the index of
the corresponding token and the resulting matrix is
stored in a le  the labels for each user are computed by comparing the table mapping movie titles
to genres with the movie  likes  of each user through
simple string matching  and are then stored in a separate le  users with empty proles or with    liked 
categories are eliminated  shrinking the dataset from
  k to about    k users 
as a result  the features are a vector of about      
tokens  x  r      and xi  r where xi is the frequency of that token in the user prole  each line in
the feature le corresponds to a user prole and lists
the indexes of the prole s tokens in the vocabulary 
each line in the labels le corresponds to the categories assigned to the user based on the movies and
tv show information present in the user s prole 
the most popular categories in order are  comedy
      likes   goofy       likes   satire       likes  
drama       likes  and debauchery      likes  
we decided to take a multi label classication approach to this problem  that is we treat placing a user
into a category as a binary problem  and treat all categories independently  for each category c  y        
and y     if the user does like any movie in that category and y     if the user likes at least one movie
in that category 

introduction
the facebook graph api was used to scrape friend
facebook prole data  this results in a dataset of
facebook user proles in xml format  listing for each
user the gender  locale   about   liked athletes  teams 
books  tv shows  movies  music  activities  interests
and sports sections of their facebook prole  the
 liked  tv show and movies sections act as the labels
for our training and test data and the rest of the
sections are used as the attributes 
with several groups working on this project around
  k users worth of facebook prole data was collected  a table associates     movies  title and description drawn from imdb and wikipedia  with
their associated movie genres     unique genres  
user facebook movie and tv show  likes  are then
 

fimethods attempted

 auc   error rate  precision and recall  the results
are detailed in table   
we trained a naive bayes classier and an svm 
on average  unskewed naive bayes makes more
positive predictions as the bias in the training set 
which is negative for most categories  is no longer
present  as expected  recall is much increased at the
cost of the error rate  perhaps more importantly 
basic naive bayes classier
mcc and auc are both higher in the unskewed verour rst approach was to train a naive bayes classi  sion 
er using the multinomial event model with laplace
smoothing  since we are doing multi label classicanaive bayes improvements
tion  we trained one binomial classier per category 
ie  classier i tries to predict whether a queried user     suggests using a  transformed  version of naive
prole will have liked category i  in order to test bayes using the three following operations 
the classiers we split the user proles randomly into
   transform the raw word frequency dij of word i
training set       and testing set        some rein document j by dij    log     dij    tf transsults for these classiers are shown in g       preciformation 
sion  recall and error are all globally increasing functions of the number of users interested in the classi   normalize word frequenciespby their inverse docer s category  number of labels y      as long as
 
ument frequency idfi   p jij where ij     if
the average number of users interested is less than
j
     after this point  error goes down and precision
word i appears in document j  idf transformaand recall continue to increase  this suggests that
tion 
the classier is strongly aected by the prior distri   normalize word frequency vectors to length   for
butions of the labels  we come back to this later  in
d
each document  dij    p ij    l normalizaorder to get a performance metric that is not biased
k  dk j 
towards one type of label  high precision and recall
tion 
are not good indicators of performance when the data
the paper also suggests improvements to alleviate the
has a lot of positive examples   we use the mcc 
t p t n f p f n
eects of skewed training data  in particular using the
m cc   
 t p  f p   t p  f n   t n  f p   t n  f n  
complement class  but they have no eect in the case
mcc values are between    and    with   indicatof binomial classication 
ing perfect classication and    complete misclassiwe implemented all possible combinations of the
cation  naive bayes achieves a low average mcc of
above three transformations  on both basic and
       see fig     
unskewed naive bayes  on the basic version  we
the mcc values seem to have the same monotonicfound that length normalization when paired with the
ity as the error rate when plotted against the priors 
tf and idf transformations often gives uniformly
with a maximum around p y              the valnegative predictions  but each transformation works
ues are overall very low  in order to know whether
acceptably on its own  none of the transformations
the problem came from the feature space  we tried
yielded a signicant improvement on the unskewed
using a smaller vocabulary of      tokens using the
dataset naive bayes  however  applying the tf and
movie description data  this new feature space did
idf transformations on the basic dataset yielded signot improve performance 
nicantly better results in terms of mcc  auc and
recall at the cost of accuracy 

naive bayes

unskewed naive bayes

feature selection

since the prior distribution of the labels seemed to
have a strong eect on the classier  we trained a
second naive bayes classier using unskewed training data  this data was selected randomly and independently for each category so that the training
set had an exact       distribution of positive and
negative examples  we compared the two versions of
naive bayes using several metrics  averaged across all
   categories  mcc  the area under the roc curve

description
we attempted feature selection using an adaptation
of the mutual information technique discussed in
class  given the    categories and about       tokens  features   for each category we computed the
mutual information of that category per token using
the formula supplied in the class lecture notes 
 

fim i xi   y   

x

x

p xi   y  log

xi       y     

p xi   y 
p xi  p y 

the probabilities p xi   y   p xi   and p y  were computed from the empirical distributions and we used
laplace smoothing to avoid dividing by   where applicable  we got an    by       matrix and then we
computed the columnwise average to obtain the average mutual information and used it to select features
that have above a certain mutual information score
since the features with a score equal to or higher than
this score are the most highly correlated with the categories  we used a threshold mi score of     to reduce the number of tokens from       to just      
the features removed are those tokens that are very
infrequent throughout the training set  for example tokens that appear only once in a single training example will have very low correlation with the
categories  on the other hand  tokens that are very
frequent like number have a very high correlation
with the categories and hence have high mutual information  thus it makes sense to discard those tokens
that do not have a high correlation with the categories 

fig     mcc comparison for basic naive bayes
with no lter feature selection applied and with lter
feature selection applied 
with feature selection  the performance of basic
naive bayes in terms of average mcc jumped from
      to        and in terms of average auc from
      to        with only a small loss in terms of accuracy  however  the best results were obtained using
tf weighting  with the tf idf combination close
behind  these results are shown in table    feature
selection did not seem to have a positive eect on the
unskewed version of naive bayes 
in terms of auc  the unskewed  non mi ltered
classiers remain the best performers  however  we
believe mcc is a better metric than auc for the
same reasons we preferred mcc over precision and
recall 

application to naive bayes

applying naive bayes with this new document space
was simply a matter of applying the mi lter after the tokenization step and running the resulting
we trained an svm in parallel with naive bayes 
dataset through the naive bayes pipeline  again 
two datasets were generated  one skewed and one
unskewed  and all the combinations of tf weighting  scaling
idf weighting and length normalization were tested
we used scaled data to avoid tokens with greater nuon the two datasets 
meric ranges dominating those with smaller numeric
below is a plot that shows the mcc dierence on ranges  since kernel values depend on the inner proda per category basis between the basic versions of ucts of feature vectors  it is also helpful to simplify
naive bayes  no transformation  skewed data  on the calculation by scaling  we scaled data to       for
raw dataset and on the mi ltered dataset  the plot both test and training data 
shows that in most of the movie categories  there was
an improvement in the mcc after performing feature kernel selection
selection  furthermore  since there are less features
involved in training and testing naive bayes  running commonly used kernels are the linear kernel 
k xi   xj     xti xj and the radial basis function  rbf 
the algorithm takes much less time and space 

svm

 

fimetric
mcc
auc
error rate
precision
recall

basic
     
     
     
     
     

tf idf
     
     
     
     
     

l
     
     
     
     
     

unskewed
     
     
     
     
     

mi
     
     
     
     
     

mi   tf idf
     
     
     
     
     

mi   tf
     
     
     
     
     

svm linear
     
     
     
     

svm rbf
     
     
     
     

table    comparison of dierent versions of naive bayes  rst   columns  and svm  last   columns  
against it  see fig      the best score happens at
  
test set size
ratio       which is similar to training
set size       
         this indicates that svm s performance is
better when training data is less skewed  similar to
naive bayes 

k xi   xj     exp   xi  xj              we used both
to nd the boundary for each category 

cross validation for parameter
to nd the penalty parameter c  and gamma for
the model  we used the grid search method with  fold cross validation  grid search is time consuming 
however  it is good for parallel computing  since each
pair c and gamma are independent 

comparison between naive bayes and
svm
for both naive bayes and svm  mcc is large in categories with many likes  while error is small in strongly
skewed categories  rbfsvm s mcc is comparable to the best performance by nave bayes  though
slightly lower 
the svm approach seems to yield the best accuracy           some of the naive bayes approaches
 length normalization only  for example  yielded similar accuracy but at the cost of increased mcc  naive
bayes with mi ltering and tf transformation still
performed best in terms of mcc while maintaining
an acceptable error rate and auc 

conclusion

fig     grid search for parameter in svm with
  fold cross validation 
the result above shows that for category    c    
and              is preferred for the rbf model 
for the linear kernel  we used the l  regularized l loss linear support vector classication model  

the nal performance of our classiers is much better than the basic naive bayes and linear kernel svm
that we rst implemented  however  an mcc score
of       is still very low compared to the best theoretical performance of    the sparsity of the tokens
in the user proles and the uniformly negative preresults
dictions of our classiers on some categories suggest
rbf has better performance than linear kernel ex  that more data per user prole may yield signicant
cept in recall rate from table    rbf nonlinearly improvements  for example by incorporating status
maps the features to an innite dimensional space  updates in the user prole data  however  we beso it has more exibility than the linear kernel hy  lieve that the biggest culprit is the way the data is
perplane  furthermore  linear kernel is a special case labeled  there is no guarantee that a user that does
of rbf      we calculated the ratio of likes in test enjoy a particular category of movie has actually liked
set and likes in training set and plotted the mcc a movie from that category on facebook 
 

fifig     error rate as a function of the average
number of likes per user for a category for both naive
bayes and svm 

fig     a plot of the mcc of basic naive bayes
and naive bayes with mi   tf for each category 
the latter has consistently higher mcc 

acknowledgements
we would like to thank the screamingvelocity
founders  wayne yurtin and graham darcey who
came up with this project and provided the initial
data scraping scripts  as well as andrew maas and
the rest of the cs    course sta for their guidance
while we were working on the project  we are also
grateful to every one of the students in cs    who
provided their own facebook data 

references
    fan rong en  chang kai wei  hsieh cho jui 
wang xiang rui  lin chih jen  liblinear  a
library for large linear classication  journal of
machine learning research                    
submitted       published      
    jason d  m  rennie  lawrence shih  jaime teevan  david r  karger  tackling the poor assumptions of naive bayes text classiers  in proceedings of the twentieth international conference on
machine learning        pages        
fig     a plot of mcc against the number of likes     keerthi  s  s   c  j  lin  asymptotic behaviors
of support vector machines with gaussian kernel 
in the test set divided by the number of likes in the
neural comput                pages          
training set  both svm and naive bayes do best
when the training and test set have similar label distributions  this was one of the motivations for creating an unskewed dataset 
 

fi