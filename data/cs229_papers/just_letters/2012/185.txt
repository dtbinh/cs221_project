text mining to detect insults in online discussion
ryan foley

ben kasper

robert macnguyen

rfoley stanford edu

bkasper stanford edu

rmacngu stanford edu

december         
abstract
the anonymity of online communication makes it particularly prone to hostility  given text from
online discussions      we train a variety of supervised learning algorithms to classify insults directed at
other forum members  after meticulous text processing to generate features  we fit naive bayes and
regularized logistic regression to establish a baseline  then we attempt to maximize accuracy with more
complex models  random forests  svm  and boosted regression trees  in order to avoid overfitting  we
emphasize thorough model tuning and evaluation via    fold cross validation  while maintaining a separate
test set throughout  we evaluate models primarily via roc curves and auc  paying special attention to
performance when strictly constraining false positive rate 

 

preprocessing

features  this is a canonical text classification model 
and we find it has impressive performance for its simbefore applying learning algorithms  we must gener  plicity  achieving a test set auc of        however 
ate meaningful features from the text  we begin with we find that models not relying on strong distribu      strings  where       are labeled as insults  to tional assumptions will be able to improve upon this
clean the data we tokenized  converted to lowercase  result 
stemmed  using a porter stemmer   and removed one
letter words  numbers  and symbols  primarily  we
count the number of occurrences of each tokenized     elastic net regularized logistic
regression
value  then we include bigrams and trigrams to capture local context  we retain only those features that we apply elastic net penalized logistic regression 
occur at least twice  leaving us with        total  this adds to logistic regression the penalty p     
 
with so many more features than observations  we                         where  is the vector of
 
 
  
 
recognize the need to regularize and employ meth  coefficients 
note that      corresponds to a pure
ods that are resistant to the inclusion of irrelevant ridge penalty  and      to a pure lasso penalty 
features  finally  we split the data into a training thus  for          the elastic net can be seen as
and test set at random  with     of the observations a compromise between ridge and lasso  it has been
belonging to the former 
shown to outperform the lasso  especially with corre 

 

lated variables   while maintaining its desirable sparsity properties      in particular  the elastic net excels
when the number of variables exceeds the number of
observations 
for a given   the parameter  parameterizes a
path of solutions  where smaller  corresponds to
a harsher penalty  we compute these solutions using pathwise coordinate descent as described in     
and perform a grid search maximizing area under the
roc curve  auc  to pick optimal values of  and  
for each choice of   we pick the optimal  by
the one standard error rule  that is  the largest 
that is within one standard deviation of opt   after

methodology

in this section we describe the model fitting and tuning process  our general strategy is to tune each
model on the training set with respect to some performance criterion  usually some measure of generalization error   while keeping the test set hidden 

   

naive bayes

we train a naive bayes multinomial event model 
which assumes conditional independence among the
 

fi     
     
     

training set auc

     

a split is chosen  the out of bag  oob  error of a
random forest is an unbiased estimate of the generalization error and is guaranteed to converge as more
and more base learners are constructed      thus  we
need only train the random forest until oob error
converges  the only parameter to be tuned  then  is
m  we perform a grid search minimizing oob error  and find m       the final random forest has
       test set auc  a noticeable improvement over
the simpler models 

   

   

   

   

   

   

   

alpha

we now build a support vector machine  svm  classifier  we use a radial basis kernel and show that 
on our data  it outperforms svms with several other
kernels 
first  we use a radial basis kernel and perform   fold cross validation on the training set  performing a
log   scale grid search across parameter values shown
in figure    we find that the minimum misclassication error on the test set is           with the soft
margin cost parameter of   and the radial basis kernel
parameter                   

figure    logistic regression  grid search to optimize
auc over elastic net penalty  

    

    

                                          

svm with radial basis kernel

    

  fold cv estimate of auc

support vector machine

 

 

 

 

 

 

log lambda 

 

error

cost

    

figure    logistic regression  grid search to optimize
   fold cv estimate of auc over   when         

    
    
 

picking the best  for each   the  with the best
training set auc is chosen  the grid search yeilds
                        and a final model with
just     features  with a test set auc of        
this is the worst performing of our models  we susgamma
pect that even with regularization  logistic regression
suffers from the high dimensional feature space 
figure    svm tuning  error as a function of the pa 

   

   

  

  

   

  

  

 

  

  

   

  

  

  

  

   

  

 

  

  

   

  

  

  

   

 

  

  

   

  

 

   

  

   

 

rameter gamma and soft margin cost for the radial basis
svm 

random forests

next  for comparison we replace the radial basis
kernel with a linear kernel  performing    fold cross
validation again and varying the value of the cost
parameter  we have the resulting misclassication error
rates shown in figure   

we now pursue more complex models in an attempt
to maximize performance  the first is a random forest  in which decision trees are bagged to reduce the
models variance  each tree is constructed by picking
m features at random at each node amongst which
 

fitraining set  without replacement  on which to fit the
next base learner  this combined with a small shrinkage coefficient reduces the variance of the ensemble
in much the same way as bagging  we find empirically that setting the shrinkage coefficient as small as
possible results in the best performance  though this
comes at the cost of more iterations  our best run
uses a shrinkage coefficient of       
we use regression trees as base learners  despite the
fact that this is a classification problem  this means
that predictions are unbounded real numbers  but are
thresholded to generate predictions  our tuning process  again by grid search minimizing    fold cross
validation error  focuses on the complexity of these
base learners  there are two parameters at our dis  
 
 
 
 
 
 
 
posal  the maximum interaction depth each tree is allog   cost 
lowed to have  p   and the minimum number of observations required in a node to allow continued splitting
figure    svm tuning  error as a function of soft mar n   we execute a two dimensional grid search over
gin cost parameter for linear kernel svm 
p                  and n                                      
for each run tuning the number of iterations by   kernel
auc
fold cross validation  we find the best parameter pair
polynomial       
to be p     and n     
linear
      
sigmoid
      
radial
      
svm with linear kernel





    





    







    

misclassification error rate











    







table    svm results with various kernel choices

in figure    we see that the linear kernel has a
minimum misclassication error of           with the
cost parameter set to     
with this tuning procedure repeated  table  
shows the auc of the test set predictions from the
svm with several different kernels 
from the table we see that the radial basis kernel
significantly outperforms the others  the
 radial basis
function  k u  v    exp ku  vk    for the svm
creates a decision boundary from weighted gaussians
at the support vectors  furthermore  it allows for
more flexibility in the decision boundary as it can be
nonlinear and more spherical at particular training figure    boosting  tuning model complexity  training
points 
set error  black  and    fold cv error  green  
following tuning  this model has a test set auc
of         beating our previous models by a substanwe implement boosted regression trees using the ad  tial margin  we attribute the success of this model
aboost exponential loss function  however  in con  in part to the well known robustness of tree based
trast to traditional adaboost  the optimization is methods to irrelevant features  furthermore  we find
carried out through friedmans stochastic gradient it enlightening that an interaction depth of   proboosting machine      most notably  this means that duced the best model  our experience is that includat each iteration we randomly sample half of the ing high order interactions tends to overfit  but here

   

boosted regression trees

 

fimodel
log  reg 
naive bayes
random forest
svm
boosting
ensemble

the prediction accuracy benefits under    fold cross
validation and generalizes to the test set  we suspect
that this is because our features are word counts  so
the high order interactions may be capturing the underlying complexity of language 

   

ensemble

table    comparison of auc between models

as one final attempt to maximize accuracy  we construct an ensemble of the previous models  to do
this  we fit ridge regression using as features the
training set predictions of each model  we choose
a ridge penalty because we expect the predictions to
exhibit multicollinearity  on the test set  our ensemble achieves an auc of        

 

however  practical applications often require constraining false positive rate  in the context of detecting insults on the web  a high false positive rate
would result in a high rate of unwarranted censorship 
potentially angering forum members and discouraging users from posting  thus  we are more concerned
with each models performance whe false positve rate
is low 

evaluation

we evaluate each model on the randomly chosen test
set consisting of the unseen     of the original data 
when evaluating the trained models  we wish to observe the tradeoff between true positive and false positive rates when classifying on the test set  to do so 
we plot their receiver operating characteristic  roc 
curves  to summarize performance  we calculate the
area under the roc curve  auc   a common measure of classifier performance 
figure   shows the roc curve corresponding to
each model  note in particular that boosting and
the ensemble appear to outperform the other models 
this result is corroborated by the summary of auc
values in table    where we see that boosting and the
ensemble are the top two performers by this metric 

   
   
   

tpr

   
   

   

   

   

   

   

nb
log reg
rf
svm rbf
boosting
ensemble

tpr

   

   

nb
log reg
rf
svm rbf
boosting
ensemble

   
      

auc
      
      
      
      
      
      

   

   

fpr

   

   

   
    
   

    

    

fpr

    

    

    

figure    zoomed in view of roc curves for each model 

figure    roc curves for each model 

figure   displays the behavior of the roc curves
at low false positive rates  we see a particularly en 

firegardless  we have achieved promising results 
our detection rate  even when enforcing a strict limit
on false positive rate  is encouraging  our results
gives hope for automated moderation of online discussion 

couraging curve for boosting  which correctly identifies almost     of the insults while making very few
mistakes  table   provides the true positive rates
for each model at false positive rates of     and      
although the ensemble rises to the top for much of
the plot in figure    we observe that boosting has the
highest true positive rate at each cutoff  we conclude
that boosting and the ensemble are the top performers for our objective of detecting directed insults in
online discussions  even when restricted to low false
positive rate 
model
log  reg 
naive bayes
random forest
svm
boosting
ensemble

tpr  
fpr      
      
      
      
      
      
      

references

tpr  
fpr       
      
      
      
      
      
      

table    true positive rates for each model at constrained values of false positive rate 

 

   

breiman  leo         random forests  machine
learning            

   

forman  george         an extensive empirical
study of feature selection metrics for text classification  the journal of machine learning research             

   

friedman j h   hastie  t  and tibshirani  r
        regularization paths for generalized
linear models via coordinate descent  journal
of statistical software             

   

friedman  j h          stochastic gradient
boosting  computational statistics and data
analysis                

   

nadig  raghuvar  j  ramanand  and pushpak
bhattacharyya         automatic evaluation of
wordnet synonyms and hyper nyms  proceedings of icon        th international conference on natural language processing 

   

tong  simon  and koller  daphne         support vector machine active learning with applications to text classification  journal of machine learning research         

   

wang  sida  and manning  christopher d 
        baselines and bigrams  simple  good
sentiment and topic classification  acl   
proceedings of the   th annual meeting of
the association for computational linguistics 
short papers          

   

zou  h  and hastie  t          regularization
and variable selection via the elastic net  j 
royal  stat  soc  b                 

   

http   www kaggle com c detecting insults insocial commentary

further work

we tried several other methods of extracting features
from the data  but had poor or insignificant results 
we tried removing stopwords  but leaving pronouns 
since these are likely to be present in directed insults   but doing so dramatically decreased the performance of our models  this may have removed
some meaningful context of the comment  additionally  we found that including word dependencies   either as a substitute for bigrams or as individual features of two words and their dependency   made a
very slight but negligible performance gain  we tried
synonym set counts and squared counts  so as to remove any linear dependence with word counts  but
did not see a performance gain  lastly  we varied
the degree of n grams and found   to have the best
performance 
we think the most fruitful area for future work is
in gathering new types of features  for example  we
could get user based features such as location  post
history  and past comments  this might allow us
to construct a profile of each user  to classify users
themselves as toxic  additionally  it would be useful to follow a conversation in its entirety to look at
characteristics like thread length  number of replies
to a comment  and nested comments 
 

fi