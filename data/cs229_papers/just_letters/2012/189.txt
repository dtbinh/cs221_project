using chemical data to predict wine ratings
eric sebastian soto
          

introduction
somewhat uncommon for a computer science student  the author of this paper has
the dream of someday becoming a world class chef  but being an excited artificial
intelligence student fan  he dreams of running a kitchen with the help of robotic machines 
thinking it would be cool to have a robotic sous chef someday  this paper hopes to begin
the process of creating one 
with its latest movement of molecular gastronomy  our society continues to find more
and more complex ways to concoct its edible creations  a trend that has resulted in food that
is sometimes hard to recognize as being meant for human consumption  with this newly
found complexity in food  there begs the question of what makes something so tasty  or
what makes a person prefer one food over another  while  we can conjecture about just how
exactly we taste food  as russel and norvig suggest  it may be better to find a model for
taste that relies less on our own understanding of taste  and more on empirical data and any
abstractions the data may help us concoct 

a short word on taste
our understanding of taste continues its constant evolution  as it remains poorly
understood by modern science  one of the original models for taste was that our sense of
taste came from our taste buds passing on information about which of the five common
flavors  sweet  bitter  sour  salty and the recently christened umami  were present in our
food  now modern science has come to find that complicated interactions between protein
receptors in taste cells maybe be responsible for our perception of taste  scientists at uc
san diego found that the same ph protein receptors in the spinal cord were also found in
our taste cells  helping us measure the acidity content of food to give us the perception of
sourness and acid content      therefore  it seems a sound idea  that in any attempt to
model human taste we seek to include such features in our feature space 

wine
since creating a model for taste seems a gargantuan and vaguely defined taste  we
can first start with a way of creating a rating system  such a rating system achieves what we
are after with a model for taste  what tastes the best  since machine learning tasks achieve
the best results with a set of great data  wines rating system is a fantastic place to start 
viticulture comes with a very storied  and tried tradition of ratings its wine  hence  we can
use wine to explore how to predict if something taste good 
noting that wines vary by region and variety it would be wisest to choose a single
type of wine and region  hence we choose red wine from portugals vinho verde region 
since it was readily available on the uci machine learning data repository  due to previous
work done on this subject      we began the process by choosing a feature set that consists
of fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total
sulfur dioxide  density  ph  sulphites  and alcohol content  variables related to the sourness
of a wine such as fixed acidity  volatile acidity  citric acid  and ph should give us a good
indication of how the acidity of these wines affects the taste  while it remains unknown how
important or what effects sulfites have on a wine  we include suphate content  amount of
free sulfur dioxide  and the total sulfur dioxide to give a measure of the sulphur content 
which is a contentious part of the wine making and consumption process  it remains
contentious because recent studies indicate that sulfites can lead to a negative experience

fito the wine drinking process  causing some people to feel sickness     

svm and lda
as a first attempt to classify our multi class data  i used the one against all variant
of the support vector machine  svms have a very good history of being excellent binary
classifiers by concentrating on maximizing the geometric margin  rather than the error in
relation to all training examples  using the liblinear library      we used the  s   flag when
training the svm model  according to liblinear documentation  the svm would use the ova
implementation utilizing the logistic kernel  for each data point it makes a separate
prediction for all the wine rating and an accompanying probability that represents how
confident that prediction is  once a prediction for each class has been made  the most likely
prediction determined by the recorded probabilities gets chosen as the solution to the
classification problem  the other implementation of a multiclass svm from the liblinear
library  written by cramer and singer  gets called by using the s     this implementation    
uses a generalized notion of margins to create a compact quadratic optimization problem 
the dual gets decomposed into multiple small optimization problems and uses this
representation to create a multiclass svm 
to model our data using linear disriminant analysis  we used a library written by
will drinnelll  lda works very similarly to our multiclass svm  since when classifying an
example lda calculates a value representing its attempt to match the example to a certain
wine rating  and then elects the wine rating with the highest value to be its guess  these
values come from how far into the space designated for a certain wine rating a data point is
determined to be  in order to create a model representation of the data  the lda divides the
data so as to maximize the distance between classes     
examining the data
one major difficulty in utilizing the svm algorithm came from the shape of distribution
for wine ratings  shown in the figure below 
distributions of wine ratings
   
   

instances

   

   
   

   

   

 

 

 

 

 
rating

 

 

 

the distributions appear to follow a normal distribution  which due to there being very few
different ratings assigned leaves there being very few examples that receive high ratings or
low ratings  hence  we could feasibly achieve reasonably good accuracy by only
concentrating on doing a good job classifying only the ratings that are in the middle 
resulting in poor performance  we do not worry about too much about this when using lda 
since it takes into account a prior  which it takes to be distribution seen in the training
examples  on the other hand  running an unweighted version of the svm on the data results
in terrible precision and recall results for the lower and higher wine ratings  hence  it was
decided to use a weighted implementation  where the weights would get manually adjusted 
and as expected  by weighting the results we achieved much greater precision and recall
results  especially with the ova svm  using crammer and singers svm implementation 
however  we found that even using the same weights on different training sets would lead to

fiwidely varying skewed output distributions when the testing data was run though the svm 
these leads us to say the cs svm is inappropriate to use with a skewed distribution 
possibly because it may converge to local minima due to it trying to represent the dual with
several smaller optimizations 

perceptron
the previously described algorithms were used out of the box  on the other hand  a
multiclass perceptron was written to help us classify the data  and support the previous
results  the multiclass perceptron we created runs by having a different vector for each type
of wine  in training  whenever we incorrectly classify an example we move the correct vector
closer to the training example and the incorrect one further away         ultimately  this
multiclass perceptron returned some results that while worse than the svm and lda
returned  were not so significantly worse we decided that any of the implementations were
being used improperly 
results
the following figures present our precision  recall and f scores 
perceptron
wine ratings
precision
recall
f score
 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

one vs all support vector machine
wine ratings

precision

recall

f score

 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

 

      

      

      

linear discriminant analysis
wine ratings

precision

recall

f score

 

      

      

      

 

      

      

      

 

      

      

      

fi 

      

      

      

 

      

      

      

 

      

      

      

the multiclass perceptrons strength seems to be in predicting wines with low or high
ratings  rather than those in the middle  on the other hand  both the ova and lda seem to
have an easier time classifying highly weighted wines  lda recalls almost half of the wines with
a maximum score  furthermore  while lda seemed to be better at both precision and recall
across the board  it is interesting to note that the average error of the svm when run for fifty
trials was         while when run the same number of trials the lda resulted in an error of
        as mentioned before the multiclass perceptron performed only slightly worse giving an
average error        when run for fifty trials 

feature selection
running a feature selection algorithm on lda to find which features of a wine contributed the
most to a wines rating gave the following order of importance  citric acid  free sulfur dioxide 
alcohol  ph  volatile acidity  density  fixed acidity  sulphates  residual sugar  chlorides  and total
sulfur dioxide  one of questions in the world of wine revolves around the presence of sulfur in
wines  as a      winter issue of practical winery and vineyard journal points out  smaller
concentrations of sulfur than those found in wine can be identified by human taste and smell     
so it would make sense that our feature selection revealed that the amount of free sulfur dioxide
would be one of the more important features when a machine tries to learn a model for wine
rating  other more influential features include citric acid content  alcohol levels  and ph level 
finding citric acid to carry more influence than ph is a surprising result  since citric acid  along
with other acids is responsible controlling the ph level and the sourness of a wine  hence  one
would expect that the ph would sort of encapsulate the information provided by the citric acid 
therefore  there must be some information that the citric acid provides about taste beyond
acidity 
principal component analysis
looking into how the feature space affects the final rating of the wine a little bit more  and
the fact that something like citric acid may carry more information about taste than just means
that trying to simply group features together would be a difficult task  hence  when running pca
and trying to couple all the sulfur related variables into one vector and all acid related vectors
into another vector  and then adding the sugar and alcohol content as two separate features 
the accuracy fell quite dramatically to an overall error mean of     when run for    trials using
the logistic regression ova svm 
reducing everything to three dimensions  by combining sugar and alcohol content  we
found the resulting graph that attempts to give us some idea of how the data is mixed together
and why the methods encountered such high error 

fi    

    

    
    

    
    

    

    
    

    
 
   

 
   
   

   
   

   

   
   

   
 

   
 

    

   
   

   

   
   

    
 

   
 

the figure on the left shows all of our      plus examples and the rightmost figure
shows    randomly selected examples  the data seems to be very intertwined based off of our
features  it is possible that the features we choose do not provide much differentiation 
conclusion
looking at the svm and lda implementations and their achievement of     error  we find
that it seems a difficult task to create a model of taste  part of this error  must be coming from
the fact that these wines are subjectively rated by people and are not scientifically grounded 
furthermore  both the svm and lda returned very similar results  suggesting our machine
learning seem to be reaching some sort of convergence point for accuracy  a suggestion for
improvement would be a different set of feature vectors  since the most important feature was
citric acid  it would be wise to include the amount of tartaric acid and malic acid  since they play
much bigger roles in the taste of a wine than citric acid 
citations
   p  cortez  a  cerdeira  f  almeida  t  matos and j  reis  modeling wine preferences by
data mining from physicochemical properties 
in decision support systems  elsevier                      
   a look at suhlphur in wine  winemag co za  september         
   r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for
large linear classification  journal of machine learning research                    
software available at http   www csie ntu edu tw  cjlin liblinear
   on the algorithmic implementation of multiclass kernel based vector machines  koby
crammer and yoram singer  journal of machine learning research      
   linear discriminant analysis  lda   will dwinnell             web             
   pieter abbeel  lecture     perceptrons  uc berkeley web            
    biologists discover how we detect sour taste   science daily  august          
retrieved    september     
   henderson  pat  science behind this anti microbial  anti oxidant  wine additive 
practical winery and vineyard journal  januray february       magazine
   kernel perceptron in python  mathieus log  machine learning  data mining  natural
language processing  mathieu blondel             web          

fi