cs    final report

chang su  wenjia xu and liangliang zhang

 

deep understanding of financial knowledge
through unsupervised learning
chang su  wenjia xu and liangliang zhang

abstractin this project  a universal information extraction
method was implemented and applied to financial area  which
supports aggregation and self analysis of complex information
from massive correlated sources  in order to extract
domain independent relations between entities  open information
extraction algorithm is used  firstly  we actively label dataset
using unsupervised learning algorithm by leveraging semantic
analysis techniques based on features such as grammatical
structure  semantic roles  etc  the labeled training set will then be
used to train extractor model  which will be further utilized to
extract relations from any text efficiently and effectively  finally 
the extracted relations will be stored in database for future query 
we compared and analyzed results of different models  measured
the importance of features using feature selection and also built a
simple ui tool to illustrate our methods 

index terms  relation based search  supervised
unsupervised learning

and

i  introduction

s

engine is one of the greatest inventions in the
information era  however  traditional search engine has
limited capability of providing answers to complex requests 
traditional search engines are keyword based  thus only return
pages that directly match the words in users query without
aggregating essential information from multiple resources  the
limitations restrict the quality and scalability of search results 
which impedes further organization and usage of the searched
results  nowadays  large web corpus themselves has large set of
data with numerous relations  but were still in shortage of tools
to fully leverage these valuable resources to improve searching
experience 
earch

aiming at smarter searching with deeper understanding of
relations between entities words relations  in users query  it is
important to build up relations between search keywords 
enrich search results by providing users with not only directly
related articles  but also indirect sources 
there are several reports on unsupervised learning algorithms
that gather script like information without referring to

chang su  wenjia xu and liangliang zhang are with the department of
electrical engineering  stanford university  ca  usa 
 authors  e mail address  changsu stanford edu  wenjiaxu stanford edu 
lianglia stanford edu  

previously labeled key words or predefined language
structures  in this project  a universal information extraction
method was implemented and was applied to financial area 
which supports aggregation and self analysis of complex
information from massive correlated sources  moreover  the
proposed algorithm will be able to find the chain of articles
between seemingly unrelated keywords 

ii  method and algorithm
in order to extract domain independent relations between
entities words  open information extraction algorithm will be
used  initially  we will label dataset using unsupervised
learning algorithm by leveraging semantic analysis techniques
based on features such as grammatical structure  semantic
roles  etc  the labeled training set will then be used to train
extractor model  which will be further utilized to extract
relations from any text efficiently and effectively  finally  the
extracted relations will be stored in form of inverted index table
for future query 
we apply this method to financial area as an example  since
financial topics are generally very hot and highly correlated 
which makes the data resource easier to retrieve and the general
result more interesting to users  while it can obviously be
applied to other areas easily since it is a domain independent
model 
the tasks include
   collect seed data from nytimes api 
   generate tree structure using stanford nlp parser and
label data as positive negative and collect seed data 
   generate features  phrase chunk  part of speech tag  etc 
   train extractor based on different models  including
naive bayes  svm  logistic regression  decision tree and
boosting 
   use extractor on seed dataset collected and predict 
   export samples and predicted labels to database 
   build ui client 
below  we will provide the detailed description of the tasks 
a  collect seed data 
we have built a php script that supports pulling data from
new york times through article search api  there are

fics    final report

chang su  wenjia xu and liangliang zhang

numerous fields and facets can be extracted or searched by  and
we tailor the usage to our application by extracting title  url and
head paragraph  due to the fact that the api does not provide
full article body search  we crawl their web pages based on url
to get first page of the full article  the output of the script are
files in json format 
b  build application framework 

fig     implementation of the framework 

 

output  this module will output labeled relations into matlab
as samples  where we will implement all kinds of learning
algorithms and evaluation 
input  this module will also input the predicted labels of new
samples generated by our learned model 
c  relation independent feature extraction 
given a candidate relation consists of two noun phrases  e  
e   and the whole sentence 
 words between e  and e 
 number of words between e  and e 
 number of stop words between e  and e 
 number of punctuations between e  and e 
 entity type of e  and e 
 number of phrases between e  and e 
 pos to left of e 
 pos to right of e 
there are also other features that may or may not useful for
the model  we will test out using feature selection in training 
d  apply partial rules heuristics in self labeling 
the algorithm we used for separation and labeling positive
and negative  to determine if the provided sentence could be
classified in one of the certain relationships  is based on the one
proposed in dr  michele bankos thesis open information
extraction for the web 

we have build a java application framework that partially
implemented the whole unsupervised learning method as
shown in figure   
      processor
process is the core part of the framework in terms of
functionality  it imports and parses files  transforms articles
into corpus of relations  namely samples that will be the input
of our machine learning model and also saves samples into
database for future query 
       relations generator
relations generator consumes articles passed by the
processor and generate relations stemming from each article 
and finally return the corpus back to processor  it will trigger
two core models of the methods  self labelling module and
feature extraction module 

fig    a   illustration of tree parser 

       self labeling module
this module utilized set of heuristic rules on each candidate
relation and label it as positive or negative sample  please refer
to point   apply partial rules heuristics in self labeling for
details 
       feature extraction module
in order to realize relation independent model  we extracted
several features that do not depend on syntactic or semantic
analysis at extraction time  please refer to pint   extract partial
features for details 
       interface with matlab

fig     b   the algorithm to label the sentence  which is implemented using
language parser     the algorithm is a modified version of the one in ref     

fics    final report

chang su  wenjia xu and liangliang zhang

 

which case naive bayes model is not a good fit 
the input of the algorithm is a parsed tree of a sentence with
the first node labeled s  it first finds and stores the first np
 noun phrase  e  and another np e  that appears after e   the
algorithm then judges if
   there is a verb in the given sentence 
   if e  is parent of e  
   if e  is the subject and head of the sentence 
   if the relation crossed the boundary of the sentence 
   if e  and e  are within the correct semantic role  etc 
then a true or false label is returned which indicated that the
sentence belongs to one of the relationships 

iii  result and analysis 
with label and features at hand  we applied several
classifiers for the binary classification  as shown in fig   

considering the model performs a little bit better than
random guesser  we then applies adaboosting to construct a
strong classifier based on these weak classifiers  in fig     we
can see that adaboosting improved the performance of the
model considerably  though dropped a little bit in metric of
precision  boosted naive bayes classifier increase    percent in
terms of recall and f  score  and     in accuracy  we also
applied svm considering that its widely used  adaptive to
features due to the fact that different kernels can be applied to
map features to higher and more separable dimension space  in
svm  some of the kernels like linear and quadratic cant
converge in expected time  then we tried other kernels
including gaussian radial basis function  rbf  kernel and
multilayer perceptron kernel  mlp   from the figure we can
conclude that rbf kernel performs better than mlp and the
dataset is non linear separable 
additionally  considering the fact that our feature space
contains both scale and also category features  we then apply
decision tree which can be fit in this case  in decision tree
model  we apply three different kinds of split criterion  gini s
diversity index  twoing rule  and maximum deviance
reduction cross entropy  and choose the minimum one as the
final result shown in fig    as well  finally  we apply another
common classifier logistic regression and get result similar to
decision tree and svm  however  logistic regression runs faster
and is computationally efficient 
overall  comparing all models  we can see that apart from
naive bayes which performs like a random classifier  all other
models boosted naive bayes  decision tree  svm rbf  
logistic regression can achieve precision around      recall
around      accuracy and f  score around      svm mlp 
performs relatively poor compared with the four above  but can
still achieve accuracy around      in further observation 
among all models except boosted naive bayes  all of them have
higher precision than recall  which means that the classifier is
relatively more likely to ignore a real relation than include a
false relation  the property is expected in our case in the sense
that tons of relations can be generated among large web corpus
of articles and were more in need of accurate and well filtered
relations rather than spans of non relevant relations 

fig     the comparison of the pecision  recall  accuracy and f  score by using
different models 

taking the building and fixing approach  we first applied
naive bayes classifier because it is a quick  though dirty  way
to get a sense of variable distribution and also feature
correlations  in fig     it is observed that the precision we got
about naive bayes is around     while the accuracy and
especially the recall is very poor  the classifier almost
classifies all ground truth data as false and performs similar to a
random classifier in terms of accuracy  the results indicate that
the features we extracted are not independent to each other  in

features eliminated

all feature
eliminate num stop words
eliminate e  entity
eliminate num cap words
eliminate e  entity
eliminate num nps btw
eliminate num words
eliminate num puncs
eliminate right e  pos

accuracy

       
       
       
       
       
      
       
       
       

table     comparison of the accuracy of all features 

fics    final report

chang su  wenjia xu and liangliang zhang

fig    the accuracy of feature selection 

to investigate the influence of different features  we applied
backwards feature selection which eliminates one feature at a
time to illustrate its influence  the feature eliminated each time
is the one which made the least influence to accuracy  due to
the performance we choose logistic regression as the model 
we plot the accuracy with remaining features along with the
procedure in fig     the x axis indicates the feature we
eliminated each round and the y axis indicates the accuracy 
from the figure we can see among all features  feature num
cap words  e  entity and right e  pos count the most 
iv  application 

 

basically  the tool is targeted to support search based on
relations  three parts compose the query e   e   usually
nouns   r  usually verb   though you can use all combinations
of the three criterions  we expect to issue queries in form of e 
  e   e    r or r   e  because it unveils the essence of query
based on relations  fig    and fig    illustrate some examples
use cases  it should be also noted that since the tool is used just
as a simple illustration of our methods  the current relation
database is generated by only     articles collected from new
york times related to finance  in addition  many relations are
eliminated by our trained model  therefore  the data pool is
relatively small and you should not be surprised with the fact
that you may get no result   out    times or so  for detail usage 
please refer to our documentation in the git repository    

v  conclusion and future work
in this project  we leverage the open information extraction
algorithm to actively label data  then train domain independent
models that can filter and extract useful relations from articles
automatically and efficiently  the next big step to go is that
since we have generate many relations  we can build a huge
knowledge graph that provide us with the capability of graph
search  which kind of stimulates and facilitates reasoning  in
this case  given two entities  we can not only return the result if
they appear in one document simultaneously  its also possible
for us to traverse the graph and find path from entity  to entity   
namely  chains of relations  articles  they may indicate deeper
relation between entities 
acknowledgment
the authors would like to thank the useful discussion and
suggestion from mr  botao hu 
references

fig     the demonstration of the use case   

   
   
   
   

fig    the demonstration of the use case   

combing all part of the system  data collection  unsupervised
labeling  supervised learning   we finally built a simple ui
tool    to better illustrate and visualize our goal 

the stanford parser  http   nlp stanford edu software lex parser shtml 
m  bancho  open information extraction for the web  ph d  thesis 
     
http   www stanford edu  changsu cgi bin cs    cs    ui tool 
https   github com changsu cs    git

fi