cs     final project  autumn     

 

classification of induction signals for the exo    
double beta decay experiment
jason chaves  physics  stanford university
kevin shin  computer science  stanford university

i  i ntroduction

t

he exo     experiment searches for double beta decay
in xenon      in both the   neutrino and   neutrino
modes  the exo     detector has a set of wires that collect
all charge released within the detector volume  aside from
the double beta decay events that occur within the detector 
there are other types of background events that can release
charge  such as compton scattering  some of these types of
events are single site  whereas others are multi site  being able
to classify single site from multi site events is important for
accurately fitting backgrounds which obscure the double beta
decay signal  multi site events are simply classified as events
where multiple charge clusters are collected by the wires  with
certain timing constraints   an issue is  however  that there
are some instances where a wire collects charge from a true
single site event  but the closely packed adjacent wires may
also show an induction signal  which can be mistaken as an
additional charge collection in event reconstruction  and can
lead to mistakenly classifying the event as multi site  removing these induction signals during the event reconstruction
would improve the single site multi site fraction by     
and the total reconstruction efficiency by       two critical
improvements for increasing our sensitivity to detecting  neutrino double beta decay for the experiment  the ultimate
goal is to find a model that can be trained in one sitting and
then can be incorporated into event reconstruction to classify
all signals in the experimental data 
ii  data s ets
running event reconstruction on low background  lb 
physics data from the experiment provides us with    realvalued attributes for wire waveforms  cluster raw energy 
uwire raw energy  pulse width at half max  unshaped pulse
integral  max pulse height  channel number  chi   fit to
template dep signal  restricted chi   fit to template dep
signal  chi   fit to template ind signal  restricted chi   fit
to template ind signal  fit pulse amplitude  time between max
and min pulse  incoming pulse width  total pulse width  
a set of         lb real events has been collected  each
sample with the    attributes calculated  moreover  to provide
labelled data  there is also a monte carlo  mc  simulation
package designed to model events in the detector and the
electronics which process the wire signals  from running the
mc simulations  we have obtained         mc events  each
with the    attributes calculated  but also with a labeling of
whether a signal is induction or collection  aka deposition  

an issue that we kept in mind with the mc data is that the
simulation isnt particularly accurate with producing induction
signals  because it only uses one template induction signal 
whereas in the lb data there can be varying induction signals 
and also noise signals which we would also like to discriminate as induction  or better phrased not collection if we
can  luckily  the mc data does accurately model deposition
signals  and so theres reasonably good agreement between
mc deposition and lb deposition signals 
for the sake of testing different classification models  we
have taken advantage of the fact that induction and collection
signals are usually distinguishable to a human  and so we have
been able to manually parse through the lb data and handlabel several hundred signals that we are confident we can
identify  in order to ensure our confidence in our labeling 
we also performed a cross validation of our human model
on our labeled mc data and attempted to classify some of the
labeled mc data and received an accuracy score of        on
the training set  we would like to mention that the above two
monte carlo and low background datasets were collected by
dave moore  a post doc on the experiment  we then converted
and formatted the data such that it was no longer dependent
on root or any of the exo libraries located on slac
computing in order to locally train and test our models 
iii  p rior attempts at c lassification
work on removing induction signals during event reconstruction started with comparing labeled mc events for each
of the    metrics  figures  such as the one below  were made
for each metric  showing the distribution of deposition and
induction signals in that metric and where the cut line would
be  based on simple cuts on metrics  a discriminator was
proposed consisting of   cuts  based on   attributes  which a
signal must fail all of them to be called induction  the standard
for the cut line of the data was chosen to be where     of
the deposition signals would pass through the discriminator 
these figures showed that single metrics could provide very
good removal of induction data  but this was all done on mc
data and not on lb data  the distribution of events from real
lb runs from the detector for these single metrics  however 
was not as nicely bimodal  and so theres the difficulty of
proposing a model that we are fairly certain passes     of lb
deposition while also being able to remove some  if not most 
of the induction signals  we suspect that this model is too
simplistic in terms of utilizing all of the available information
to attain the best possible induction removal and thus plan to
build upon it using machine learning techniques 

fics     final project  autumn     

 

deposition signals  which was unacceptable  so it was omitted
from the final logistic regression model 
b  svm

fig    

iv  p lan of attack
our approach to classify deposition and induction signals
involves taking full advantage of the    calculated attributes
and working with a multitude of classifier algorithms  we
also strive to come up with some methods for validating and
comparing performance of different classification models  so
we can gain as much knowledge as possible about the lb
induction signals without having lb labelings  we quickly
saw that supervised learning on the mc data gave the best
balanced results on both the cross validation and the handlabeled testing data and we tried to use a variety of classifiers 
each addressing certain expected patterns in the data  while
we make a naive assumption that the single induction template
from the simulation will suffice to generalize for real world
testing data  we note that the results of testing the classifier on
real hand labeled data show great gains over the prior attempts
at classification  by the end of this work  we evaluate the
performance of our classifiers  as well as compare them to the
previously proposed discriminator described above 
v  s upervised l earning
a  logistic regression
the first method we explored was regularized logistic
regression  sklearn lm logisticregression     we have a few
model parameters we are free to adjust  such as regularization
type  l  or l   and regularization strength  we also chose
to increase the strictness of convergence testing from the
default value  which we found to improve model predictions 
for a given model configuration  we set up a   fold crossvalidation on the mc data to make sure that the model is at
least consistent on mc data  for each of the cross validations 
a confusion matrix is reported to check deposition passage
and induction blockage  then the model is trained on the
whole mc set and tested on the hand labeled lb data  with
a confusion matrix reported  it was found that the default l regularization with a strength factor of     actually gave the
best performance on the mc data and the hand labeled lb
data  when tested on the hand labeled lb set  the model passes
       of deposition signals while correctly classifying    
of induction signals  this deposition accuracy is comparable
to the mc k fold performances of         while the induction
accuracy is expectedly worse than the       on the mc
data  since the mc induction signals are all from the same
template waveform  additionally  it was found that feature
scaling  centering and scaling  slightly improved accuracy
on induction signals  but significantly reduced accuracy on

linear kernel  motivated by the early promising results of
logistic regression  we proceeded to apply the svm classifier
 sklearn svm linearsvc   and sklearn svm svc    to consider
the well regarded out of the bag classifier  tuning the svm 
we note that we are free to adjust the kernel function  the cost
parameter  the loss function type  hinge loss or squared hinge
loss   as well as the class weights to continuously lower our error rates  in testing the model  we again utilized a   fold crossvalidation on the original training data and a testing session
on a hand labelled data set while training on the labelled mc
simulation data  to account for the many parameters to tune 
we also implemented a custom grid search algorithm over the
parameters and repeated running the svm hundreds of times
using varying parameters optimizing on the testing data  we
began running the svm on our regular unscaled features  as
with logistic regression  we set a minimum deposition rate
threshold to be     and the results showed that using the
linear kernel provided the most balanced test results as other
kernels such as the radial basis kernel tended to over fit on our
training set and result in high deposition pass through rates
but very low induction block rates  moreover  we found that
a combination of a cost parameter around     a squared hinge
loss in addition to a equally balanced class weight yielded
in the best results on the hand labelled test set of an overall
accuracy of          with a deposition accuracy of         
and an induction block of           we investigated further
into the theory order to account for the similar passing rates in
the linear svm as well as logistic regression to see if we could
improve on the results  we note that the regularized empirical
loss minimization appear very similar in both the case of the
svm and logistic regression  as shown by jaakkola     for
the svm we find the weights with respect to minimizing
  m 

m
x

    y  i   w     x i   t  w        w      

i  

and for logistic regression
  m 

m
x

log g w     x i   t  w        w      

i  

from the minimization functions  it is clear that the main
difference between the two models lie in the use of the kernel
trick in the svm and the varying error functions between the
two model  because we are using the linear kernel for the
svm  the main source of variation between the two models
then lies in the definition of the error function which could
potentially explain the similar error rates in both of the models 
probing further  if the deposition and induction events of
the exo     experiment were best modeled by the logistic
regression error function over the svm error function  it would
explain the slight improvement variations we see in the logistic
regression model over the svm  as with logistic regression 
scaling the data only served to lower deposition pass rates and
therefore was not considered for the linear kernel svm 

fics     final project  autumn     

gaussian kernel  we then attempted various different kernels for the svm  it was found that the gaussian kernel
gave the best results and as we had before  we explored
further into varying the results by scaling and centering the
dataset  the svm was retuned on top of these features
to see if we could gain any increases in performance  we
found that this instance  scaling the dataset actually increased
our performance and our initial results without accounting
for varying class weights resulted in a promising         
deposition pass through rates and a        induction block
rates  with the desire of increasing deposition to meet our
    threshold  we then varied our class weights to give a
higher weight towards deposition       and found that very
interestingly  unlike logistic regression  after retuning the svm
with feature scaling and varying the kernel functions  we
resulted in better performance than the unscaled unweighted
model with a final deposition rate of      pass through and
an induction block rate of          on the testing data  on
the training data  we received deposition and induction rates of
         and          respectively  moreover  we noticed
that in our tuning there were results with deposition rates
near     while induction block rates had increased near     
by running the gaussian svm on the scaled feature set  we
received very promising results  but we also noted that we
could not increase induction beyond     while keeping our
threshold even by altering the class weights  as a result  we
gained a new optimistic perspective to selecting for future
different classifiers that would inherently bias themselves
towards weighting deposition more significantly with the belief
that we would meet our deposition pass through rates from
the classifier and be able to tune the classifier beyond that to
increase the induction block rates  keeping this in mind  we
researched and began experimenting with two such classifiers 
the one class svm and the random forest classifier 
c  one class svm
one class svm as a method for outlier detection was
appealing since it captured the spirit of ultimately wanting
a classifier robust enough to distinguish between deposition
and any not deposition signals  being able to tightly confine
the region of existence for deposition signals with a oneclass svm is an unlikely accomplishment  but perhaps the
decision boundary of the one class svm could better exploit
the geometry of our data  the oneclasssvm class was used to
train the model with a gaussian kernel  which was easily found
to be the best kernel for our task  the classifier was optimized
over choices of convergence tolerance  kernel degree  and a
parameter  which defines an upper bound on the number of
training errors and a lower bound on the fraction of support
vectors  the final model was trained on the entire mc set 
and this time  utilized feature scaling to get better results 
while this classifier consistently achieved over     deposition
passage on mc data  it only achieved        deposition
passage on the hand labeled set  which excludes it from being
considered for final adoption  however  we can learn about
the geometry of our data in parameter space by comparing
the performance of this model with the performance of linear

 

classifiers  the one class svm achieved lesser deposition
passage on the mc data and hand labeled than the above two
classifiers  which tells us that the deposition signals do not
lie with a well restricted region in parameter space  and that
a hyperplane defining a halfspace does better than a finite
volume in space to capture the amount of deposition that we
require of ourselves  it is also of interest to compare induction
blockage among these models  the numbers can be found
above for the previous models  but for one class svm  we
had     blockage on the mc data and     blockage on the
hand labelled data  this much worse performance on the mc
induction signals is likely due to the fact that it is the only
model that didnt include any induction signals in its training 
d  decision tree and random forest
decision trees  sklearn tree decisiontreeclassifier  were a
promising classifier model because of two characteristics 
first  we hypothesized that given the widespread use of
decision tree boundaries among physics research in general 
it could very well be that a rule based cutting system could
perform an accurate distinction between induction and deposition  in fact  the current filtering system as implemented
in the exo     experiment involves using three cuts on the
attribute values which create rectangular decision boundaries
in an attempt to filter out the induction  decision trees come
closest of our classification models to performing such logical
boundaries and thus had from the onset had potential to
perform better than the current system as it would be a
more rigorous cutting process rather than three speculative
cuts  second  decision trees fit our requirement of biasing
themselves towards deposition classes as they have a tendency
to inherently biasing towards the class with more training
examples  we knew that we had more deposition examples
than induction examples and as we saw in the svm  if we
could maintain a high deposition pass through rate  we could
scale the features to increase our induction block  we decided
to take this attribute of the decision tree and convert it to
our advantage  understanding these two characteristics  we
began our process of applying the model by working from
the decision tree classifier to gain a preliminary insight into
the performance of the model then began varying the decision
boundary geometrics by including a majority ruling random
forests of decision trees  first  we discuss the result of the
single decision tree 
single decision tree  in order to assure ourselves that we
werent actually impairing the results of the decision tree by
running it on the deposition heavy data set  we ran a sanity
check by using a filtered balanced example set and comparing
the results with the original unscaled feature set  we saw
that the equal distribution tuned decision tree resulted in a
deposition pass through rate of          and an induction
block rate of           the decision tree trained from the
entire data saw a slight increase in induction of          and
a decrease in deposition of           from these preliminary
results  we saw that a single tree itself would not give the
performance as desired  however  having a rule based cutting
system appealed to us as a logical way of understanding the

fics     final project  autumn     

data further  as a result of running the single tree  we were
at least able to see a visible rule based system  subtree shown
in figure    to logically and easily intuit how the system was
classifying between the two classes  inspired by that there was
potential in such a cutting system and after reading more about
the gains in ensemble methods  we decided to apply random
forest as an ensemble method of the decision tree 

 

briefly explore some unsupervised learning methods  for our
purposes  unsupervised learning has the advantage of directly
dealing with the lb data and not the mc data  we expect that
unsupervised learning could help us understand that geometry
of the lb data that we care about  and we decided that
clustering would be the method of choice 
a  k means clustering

fig    

subtree of the decision tree

random forest ensemble method  the random forest
model gave great gains in performance from the other classifiers  interestingly  however  we saw that scaling the feature set
did not have any different results on the final tuned forest  this
may be that the loss function used to compute the boundaries
is unaffected by the affine transformation of the data  however 
regardless  we saw a great increase in performance as the final
results on the hand labeled testing set resulted in         
deposition pass rates and an          induction block rate 
on the training data  both deposition and induction rates were
      even though the results of the random forest took a
slight hit in deposition as opposed to the svm  we note that
the random forest gives the most balanced results among all
of our classifiers  there is a slight room for error as the hand
labelled testing set is not      representative of all of the
events that occur in the real world  but given the high rates
on the training data as well as the high rates on the hand
labelled test set we note that the random forest performed the
most admirably of the classifiers  the high performance of
the classifier could be attributed to the boolean logic of the
system  since the data is based on    real world attributes  it
could very well be that a rule based classifier best models the
difference in the inherent characteristics between deposition
and induction 
vi  u nsupervised l earning
our work with supervised learning based classifiers above
has indeed been fruitful  but we felt that we wanted to

k means  sklearn clustering kmeans    clustering was run
to find two clusters  hopefully aligning with the deposition induction distinction in the data  training a k     clustering model on the lb data set  and then predicting classes for
the lb set  it first found that all but one point is classified in a
single group  so we adopted a process of training the model 
predicting classes  inspecting the confusion matrices on the
mc and hand labeled sets  and then creating a new set for the
model to train against composed of just the data points in the
super dominant class  and repeat until there is a reasonable
split of signals between the two classes  after   iterations of
this process  we arrived at a clustering model that achieved
       deposition passage on the mc set and        deposition passage on the hand labeled set  oddly enough  its
induction blockage on the mc set was significantly worse than
on the hand labeled set       and       respectively  this is
odd because the supervised learning models always had a very
easy time blocking mc induction signals when k fold crossvalidation was used  and it was presumed to be because the
mc induction signals are all so similar and probably easily
confined in parameter space  so when those classifiers moved
onto the lb set where inductions signals are more varied  they
were expected to block induction with worse performance 
this interesting pattern in the clustering models induction
blockage implies that there is a significant variation in the
lb induction signals  and so the clustering ended up finding
centroids that actually preferred blocking types of induction
signals more often seen in the lb data than the mc data 
which other models could not do because of how they were
trained 
vii  validation and c omparison
the first sanity check we actually did was look at the coefficient vector learnt by logistic regression to see which attributes
it put the most weight on  taking the coefficient vector from
our classifier and multiplying each entry by the mean value of
that attribute in the unscaled testing data  we found that the
most importance was placed on the chi   fit to the template
induction signal  and intermediate importance was given to
uwire pulse energy and pulse width  all of which we expected
from physical understanding to be strongly characteristic for
distinguishing our two classes of signals  we also saw that
no weight was placed on channel number  as we would also
expect  because induction happens on all wire channels  other
models may have slightly different weights  but this shows that
what our classifiers are learning is consistent with physical
expectations 
as we have been already mentioning  for each model we
get an estimate of the generalization error by testing on the

fics     final project  autumn     

hand labeled set  this serves as a check for satisfying the
necessary criterion on deposition passage  also recall that the
mc deposition signals are good models of depositions seen
in lb data  so they too give good verification of classifier
performance on deposition signals  another sanity check that
we do is run the classifier on the lb data set and collect
the waveforms of the signals classified as induction  which
we then randomly trace through and view to make sure that
they all look like signals that we indeed wish to exclude  the
acceptable candidate models are defined as having      
deposition passage on both the mc and hand labelled data
sets  while also passing the sanity check  and those models
are logistic regression  linear svm  gaussian svm  and
random forests 
now for directly comparing the acceptable models  along
with the previously proposed cuts model  heres a table giving
the deposition passages and induction blockages for the  
classifiers developed by this work and the previously proposed
discriminator 

fig     deposition passages and induction blockages for   new classifiers and
one previously proposed classifier  tested on mc data set and hand labeled
 hl  data set 

as can be seen from the table  we now have several
competitive and appealing alternative classifiers for separating
deposition and induction signals  particularly  our random
forest classifier is the most appealing  with perfect separation
on the training data  and very consistent generalization error 
the gaussian svm is also impressively close to the performance of the random forest  all classifiers proposed by this
work show significant improvement in induction blockage over
the previously proposed discriminator 
on top of our sanity check on the induction classified
signals in the lb data  we were interested in examining the
similarity or overlap of the sets of signals that each classifier
called induction  this sort of similarity measure describes the
extent of agreement on which signals are induction in the lb
data between each pair of classifiers  and can also be thought
of as a measure of similarity between the classifier geometries 
 si sj  
in the following table  each entry is defined as  s
  where
i 
i  j represent row  column indices and the indices range from
  to   for the   classifiers in the above table  and si is the
set of signal ids that are called induction by classifier i  so
that each entry is the fraction of signals that classifiers i and
j both call induction out of the total number of signals that
classifier i calls induction 

fig    

similarity matrix for the classifiers

 

the hope of this table was to perhaps see that one classifiers
set of induction signals was a subset of another classifiers set
of induction signals  implying that we can more absolutely
claim that the second classifier is better than the first since
the second classifier agrees with and expands on the first
classifiers definition of induction and both have at least    
deposition passage  we do not end up seeing that  and that
is likely because the geometry of the data is so complex in
the    dimensional parameter space that the subspaces that
the classifiers call induction are quite varied  regardless of
not seeing full superset subset encapsulation  we do see reasonable agreement percentages among the classifiers and we
acknowledge that we wont get consistently perfect induction
subspace definitions 
viii  c onclusion
we have successfully generated several competitive and
appealing models for classifying induction signals in the exo    detector  our criterion passing models show significant
improvement in induction blockage over the previously proposed discriminator  at a factor of about    we have taken
several measures to validate the generalization error of our
models  and have taken steps to help us understand the
geometry of the data and the differences in results between
our models  our random forest classifier will succeed the
previously proposed discriminator and will be included in the
reconstruction module that will remove induction signals from
our low background data  the exo     experiment hopes
to complete another analysis and release another paper in the
coming year  and we hope to see the improvements in reconstruction efficiency and single site multi site discrimination
in this next analysis due to this addition 
acknowledgment
while both authors contributed to all of the above classifiers
and discussion  jason chaves was the primary contributor to
the data formatting  the exo connections  logistic regression 
one class svm  k means  and validation and kevin shin was
the primary contributor of creating various data subsets  linear
svm  gaussian svm  decision trees  random forests  and
the gridsearch parameter optimization implementation  the
authors would like to thank dave moore  a postdoc working
on the exo     experiment  who graciously helped us collect
the monte carlo and low background data with calculated
features  we would also like to thank andrew maas for helpful
suggestions along the way  and the cs     staff for this course 
r eferences
    jaakkola  tommi  machine learning  lecture    ai mit edu  mit 
    sci kit learn  classifiers  scikit learn  web     dec       http   scikitlearn org stable  
    exo collaboration
    exo reconstruction workshop
    exo week conference

fi