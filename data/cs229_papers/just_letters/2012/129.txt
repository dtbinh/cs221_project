predicting retail website anomalies using twitter
data
derek farren
dfarren walmartlabs com
december   th      

abstract  anomalies in website performance are very
common  most of the time they are short and only affect
a small portion of the users  however  in e commerce an
anomaly is very expensive  just one minute with an
underperforming site means a big loss for a big ecommerce retailer  this project presents a way to detect
those anomalies in real time and to predict them with up
to one hour in advance 

anomaly detection
to detect anomalies on real time  the approach i used
involves looking at the customers last    minutes web
browsing data  model it  and then comparing that model
with the next minute customers web browsing data  if in
that next minute the browsing behavior changes drastically
from the model  that instance is called an anomaly 
there are several ways to model these last    minutes
web browsing behavior  the ones i tried in this research are
based on 

index terms  machine learning  anomaly prediction  ecommerce  web performance 
introduction



e commerce web site operations are heavily transactional
and prone to small  short time failures  most of the time
these anomalies are small  and as such  they are not caught
by the retailer web operations  however  the customers do
perceive these anomalies  in this project i propose a model
to predict website anomalies 
since the web browsing data has no classification for
anomalies  i divide this model in two sub models  figure    



modeling the distribution of the data and setting the
boundary that divides a normal behavior from an
outlier as a point of low probability  see figure   
finding the support vectors that divide the normal
website behavior from the anomalies  i e  one class
svm  schlkopf et al       b 

 an anomaly detection model that catches not expected
patterns in the data on real time  the output of this
model is a label that states whether a specific instance is
an anomaly or not 
 an anomaly prediction model that predicts the labels
given by the anomaly detection model 
the data i used in this research was all customers web
browsing data for two months from one of the main us ecommerce retailer  aggregated by minute 

anomaly
detection

labeled anomalies

anomaly
prediction

figure  
the time series represent page views of a specific page  the   
minutes before the anomaly was modeled with a one dimensional
gaussian distribution  the anomaly is too far away from the
mean  note that this example is based on a one dimensional
gaussian for easy examplification purposes only  i used   
dimensions in the final experiment 

figure  
model workflow

 

filabeling anomalies from gaussians

 i tried using a mixture of gaussians but the em
algorithm took way too long to run  some times over one
minute  which is impractical for real time application  
also  most of the time the data comes from the same
gaussian distribution 
 the one class svm always finds outliers  the number
of outliers it finds is defined by the constant   since
most of the time there are no anomalies in a    minutes
timeframe  this algorithm is impractical 

the general multivariate gaussian model is 
       

 
 
 
         

exp 

 
           
 

since this is a real time system  computational speed is
important  thats why the first approach i tested was using a
multivariate gaussian distribution assuming independent
dimensions  since the covariance matrix of this distribution
is a diagonal matrix  all computations are very fast  for
independent dimensions this multivariate gaussian can be
expressed as 

anomaly prediction
as a first option  i tested predicting anomalies with the
same data they were labeled  last    minutes of web
browsing data   however after i tried several options  the
accuracy of this model was very low  as a way to increase
this accuracy  new features were used  two sets of features
were especially important in the model  twitter data  day of
the week and time of the day 
since the number of features used was not big     
features  i selected the features by doing a wrapper model
features selection  kohavi et al         table i shows the
cross validation error  for the most important features
combination tested 

 

             

       
   

  

where
is the variance of the data in the ith dimension  ith
diagonal element in covariance matrix   
the second approach was using a multivariate gaussian
distribution without assumptions about the dependency of
the dimensions 
the third approach was using a mixture of gaussians 
this model would better suit cases where the web browsing
data has clusters of data with different distributions  the
parameters of this model are usually found by using the em
algorithm 

table i
accuracy meassures
set of features used
cross validation error

labeling anomalies from a one class svm
the fourth approach was using a one class support vector
machine  schlkopf et al          this model labels data out
of unlabeled data by separating the data from the origin  the
mean in this case  and creating the two classes  normal
and outliers 

same features used in anomaly detection

   

twiter features

   

time features

   

twitter and time features

   

for the twitter data  turns out that some customers share on
twitter their frustration when an online retailer webpage is
underperforming  some times they experience that
underperformance before the retailer data shows it  the
reason for this is that its still too expensive for the retailer
to measure every single thing happening on the website  not
all webpages are measured  usually there are millions of
them  and not every single webpage object load is
measured  also  even if these things were measured  the
anomaly usually starts affecting only a few customers
before it affects all customers  if these few early affected
customers tweet about their experience  a prediction can be
made from those tweets 
the concrete twitter features used to make this
prediction were aggregations by minute of a sentiment
analysis of all tweets that had the name of the retail
company on it  or variations of it  then  from this sentiment
analysis by minute  five moving calculations were made for

 
 
min
       
    
     w              

 
            
     
here   is the percentage of outliers to find and  is kernel
feature map  boser et al         vapnik        schlkopf et
al         
comparing anomaly detection approaches for
real time applications

having in mind that speed is crucial in this algorithm  all
anomaly selection approaches showed in this research were
non practical but the multivariate gaussian with
independent dimensions  the reason for that is 
 since the data used is small  very often the  matrix is
singular and as such  not invertible 
 when  was non singular  it took too long to invert 

 

nave bayes  svm with gaussian kernel and logistic regression  all
regularized with cross validated parameters 
 
this cross validation error follows the same procedure as described in the
results  i e  oversampled anomalies 

 

fithe last    minutes  max  min  mean  standard deviation and
median   this final dataset has five sentiment measures that
represent the customer feelings about the retail company for
the last moving    minutes 
i did the sentiment analysis by using the afinn list
 finn rup nielsen         this is a list of the       most
used words in the english language  with a corresponding
sentiment score that goes from    to     for each tweet  i
matched the words to the ones in the afinn list and added
their score to get a final score  a final score of   means the
tweet was neutral in terms of the authors feeling about the
retail company  if the score is positive  the authors feeling
about the retail company is positive  if the score is
negative well you get the idea 
regarding the day of the week and hour of the day
feature  the model performed the best when these features
were dummy codded as categorical features instead of using
numerical features 
the classes to predict were two 



was set by visually looking at the results ordered from low
to high probability and setting the division where the data
changes from looking anomaly like to looking normal
like  that division happened when the probability of the
data point is under    x      
the result of this sub model was extremely accurate 
an anomaly can be seen in figure    the new  labeled
dataset was  as expected  heavily biased in favor to
normal examples  the exact number cant be disclosed
here  
for the anomaly prediction part  i set aside     of the
data for final testing and accuracy setting purposes  from
the other     of the data  i took all anomalies and a similar
seized random sample of normal data  this gave me a
dataset with equally weighted classes that i used for training
and cross validating purposes  split as             this
improved the accuracy of the model measured by an f 
score over the testing dataset 
the parameters of the l  regularized svm with
gaussian kernels were found by cross validation  c     
      

   there is at least one anomaly in the next hour
   there is no anomaly in the next hour 

results

the model i used was a svm with l  regularization and
gaussian kernels 

the model has a cross validation error of    and a f  score
over the testing  biased  dataset of      

experiment
i used specific features from the e commerce retailer web
browsing data for the dates between oct  th      and dec
 th       for the anomaly detection part  the features were
page views for each of the seven types of pages  browse 
item  cart  order  search  verify order and verify shipping  
time to load page for these same seven types of pages     
errors and     errors  in total they were    features 

figure  
each iteration increases the data size by     of the total
dataset  the learning curve shows that this is a high variance
model  in this model having more data would be very usefull to
improve the performance  however  since the performance is
already good      accuracy       f  score  using more data was
left as a future improvement 

the figure   shows that our model has high variance  this
could be fixed by adding more training examples in the
dataset  however  given the fact that 
figure  
an anomaly found by the model

 more twitter data is not easily available
 the cross validation error is already low
 the testing f  score of this model is high

after the model assigned the probabilities  the cutting
probability that divides normal examples from outliers

 

fii decided to leave it as it is and plan on adding more data as
a future improvement 
an important finding of this applied research is that the
set of features that have the most important predictive value
are the time related features   day of week and time of day  
it seems like most of the anomalies in the webpage happen
around   pm and  am 
conclusions
anomalies on an e commerce website can be accurately
detected in real time  most of the time modeling the web
features to be measured  page views  time to load  etc   as
multivariate gaussian with independent dimensions is good
enough to have a very accurate real time model  predicting
anomalies using the same web measures used to label them
is not recommended  the best approach to predict web
pages anomalies showed to be selecting new features from
other sources  in my experiment  twitter data  day of week
and time of day had a strong predictive value for anomalies 
references
   

b  schlkopf  j c  platt  j  shawe taylor  a j  smola  and r c 
williamson  estimating the support of a high dimensional distribution  neural computation  vol      no     pp                 

   

b  e  boser  i  m  guyon  and v  n  vapnik  a training algorithm for
optimal margin classifiers  in d  haussler  editor  proceedings of the
 th annual acm workshop on computational learning theory 
pages         pittsburgh  pa        acm press 

   

v  vapnik  the nature of statistical learning theory  springer
verlag  new york       

   

b  schlkopf  c  j  c  burges  and a  j  smola  advances in kernel
methods  support vector learning  mit press  cambridge  ma 
     

   

finn rup nielsen   a new anew  evaluation of a word list for
sentiment analysis in microblogs   proceedings of the eswc    
workshop on  making sense of microposts   big things come in small
packages     in ceur workshop proceedings               may 

   

r  kohavi  g h  john   wrappers for feature subset selection  
artificial inteligence             

author information
derek farren  data scientist  walmartlabs 

 

table i has the details 

 

fi