supervised multi class classification of tweets
zahan malkani
zahanm stanford edu

evelyn gillie
egillie stanford edu

   december      
methods such as support vector machines  neural nets 
k nearest neighbor and naive bayes approaches  their
results do not offer much by way of quantitative outcomes
of using these methods to classify short text snippets as
we have  lee makes some fascinating observations about
how humans perform  long form  document classification
in      providing a bayesian model that fits how people
seems to reason about this problem  his approach seems
specific to the limited domain topic classification problem though  that will be relevant for the second dataset
in our paper 
in      agarwal et  al  discuss polarity features as inputs for supervised learning methods  specifically svms 
in order to limit the feature space so that it can meaningfully be divided up by an svm  these features  though
we hadnt realized at the time  are similar in nature to the
conditional frequency features that we trained our svms
on eventually to capture the conditional probability of
features corresponding to classes 
oconner et  al  provide very useful twitter specific
tokenizers and emoticon analyzing algorithms in      and
the generalization of binary classification trees to the
multi class problem used by lee and oh in     is the same
as what we use for our multitree classification algorithm 

abstract
we present a study of a variety of supervised
learning methods used for the problem of twitter tweet classification  this includes support
vector machines  svm   neural networks  nn  
naive bayes  nb   and random forests  we evalutate these methods using their performance on
two sources of labeled data  an attitudes dataset
that classifies tweets with an attitude that the
tweeter might have had when composing it  and
a topics dataset that classifies tweets into one of
a  limited domain  topic set  we find that svms
out perform the others in accuracy and when used
with feature selection have a reasonable runtime
too 

  introduction
the explosive growth of data being produced in the form
of short text snippets has lead to an equally voracious
growth in the methods used to analyze and decode these
snippets  machine learning plays an important role in enabling such analyses  since the vastness of these datasets
vexes most hand labeling attempts 
one of the more popular mechanisms producing this
data is twitter  the medium is notorious for confounding traditional text analysis methods  since the wealth of
context that older natural language processing techniques
depend on is simply missing in these tweets that are artificially confined to being     characters in length  accordingly  selecting informative features to extract from
the little context that we are given was a priority 
we explore a variety of supervised learning techniques
to use in two classification tasks      labelling tweets
with an attitude from the speakers perspective  this is
in contrast to most classification systems  that categorize tweets from the audiences perspective  and usually
just has two classes  positive and negative   and   
labelling tweets based upon topics  where we have a predefined set of topics that we are looking for 
we pursue modern approaches to classifying tweets 
and explore their applications to real world phenomena 

  data
    data collection
in august  we started collected twitter data using twitters streaming api on an elastic cloud compute instance  this collected around       tweets per second
during peak hours  and       during non peak hours 
      obama   romney dataset
we selected a sample of     tweets from october   th for
romney and      tweets from november  th for obama 
we then had these tweets labeled with emotional attitudes on amazon mechanical turk using amazon certified categorizers  getting two categorizations for each
tweet 
the emotional attitudes were chosen with consulation
from a social data analytics startup  interested in a similar problem  accordingly  we ended up with the set

  related work
text classification is a well traversed area of machine
learning  thanks to its potential for wide reaching impact      and     present good overviews of the methods
used so far and their relative strengths and weaknesses 
in      yang and liu go over the theory behind classifier

 happy  mocking  funny  angry  sad 
hopeful  none   other 
workers agreed on the label on only       of the samples  we manually reviewed a sample of the labels and

 

fiemotion
mocking
funny
sad
angry
happy
hope
none

percent of dataset
    
    
    
    
    
    
    

 emoticons   emoji
 punctuation        
 capitalization  lmaoooo 
 dialogue  retweets and   
 negation
 sentence level sentiment

table    distribution of classes in emotions dataset

 cursing
found that for some tweets this is understandable  as
what one person may construe as mockery another can
readily define as anger  yet for a few tweets we would
not have agreed with either  after consulting with the
course staff we ultimately decided that the data was good
enough to use as an experiment and left it at that  for
the rest of the computations  if there are two labels and
a classifier predicts one of them  we consider it correct 

we were able to see some small improvement to svm
accuracy for certain classes through addition of these features  as can be seen in table    these tests were performed on a small  high accuracy dataset  
bag of words
emotion accuracy
angry
     
mocking
 
happy
     
sad
     
hopeful
     
funny
 
none
     

      topic labeling dataset
our second dataset uses a sample stream from august
 th  in which the olympics are beginning  the mars rover
had just landed  the apple samsung lawsuit was in the
press  and obama was expectedly in the news  hence 
tweets are labeled by their topic 

  twitter specific
emotion precision
angry
     
mocking
     
happy
     
sad
 
hopeful
     
funny
     
none
     

table    adding twitter speech features to svm input
 obama  olympics  mars  apple  none 
it was disappointing to see that for most of the emotions our engineered features hurt performance  so wed
like to offer a special shout out to the angry tweets for
their user of upper case and exclamation points 

overall  we collected around   million tweets  of which
we labeled         using mechanical turk  of those
         around       are labeled as being about any of
the above topics 
the topic distribution are shown in  
topic
olympics
apple
mars
obama

      class distribution features
while exploring the performance of mocking and angry 
we decided to analyze the normalized distribution of the
sum of the weights by each token over an entire feature
set  believing that this would enable us to see where some
labels were nearly the winner only to just barely lose out
to another label  more precisely 
let

percent of dataset
    
    
    
    

table    distribution of classes in topical dataset

 f   f     fm be the vocabulary of features 
 s   s     sn be the sample input such that si is defined by its features si   f  f     fk

    data processing
we take each tweet through a pipeline of steps  first  we
lowercase the text to improve recall of unigram matching  then we tokenize it with the twitter specific tokenizer from      which preseves hashtags and emoticons 
then we perform porter stemming on the words  again to
improve recall of unigram matching  then we construct
singleton and bigram features for the tweet  the other
features are discussed separately 

 l   l     ln be the corresponding known labels be a
multi class vector 
 w
p be a m  n matrix such that wi j  
lk j       namely w counts the cosk i fi sk
occurences of features and labels 
p
 for sample sk   vk as vk   fi sk wi j

      twitter speech features

upon analyzing the data we made the important realization that these vectors were strongly correlated with
the labels  much more so than any element in isolation 
we quickly tried these as feature vectors in our svm

we wanted to more carefully consider the type of language used in the these text snippets  we considered a
multitude of features that we chose ourselves  including

 

fithreshold
n a
 
    
   
   
   
   
    

method
none
frequency
mi
mi
mi
 
 
 

framework and produced excellent results  we analyzed
the literature for similar feature selection  and it seems
agarwal  et  al  introduce a similar polarity prior term
in      our feature is an extension of this to the multi class
case 
ultimately  we realized that these are precisely the conditional feature class counts that naive bayes exploits to
good effect  but it provided a useful insight into the workings of svms 

  algorithms
after stemming words and adding all words and bigrams
to the feature set  our feature space is in the thousands 
with methods such as svm or random forests  we need
to trim down our feature set to get reasonable running
times  we tried three feature selection methods 
our first feature selector was a basic frequency pruner 
features that did not occur frequently at all were thrown
out  often  these were misspellings  
our second feature selector was a mutual information
feature selector  given a feature set f and class set c 
the mutual information score is 

f c

p  f  c  log

argmaxp c   c 

p  f  c 
p  f  p  c 

 

n
y

p f   fi  c   c 

   

i

   

which translates to 
n  
n n  
n ln  n   n     

accuracy
    
    
    
    
    
    
    
    

then makes a prediction for an unlabeled point by calculating the posterior probability for each class and predicting the maximizing assignment 

c

x

runtime
 m  s
  s
  s
  s
  s
  s
  s
  s

table    results of an svm run using feature selection 
 using     examples of the topics dataset 

    feature selection

i f   c   

features
    
    
    
    
    
    
    
   

the independence assupmtions made in nb  as can be
seen in the second term of       has the important sideeffect of allowing a naive bayes classifier handle a large
number of features with very little sacrifices in accuracy
due to the curse of dimensionality that often effects other
learning algorithms  this is very important for text classification  where often the feature space size dwarfs the
other relevant dataset metrics 

n  
n n  
n  
n n  
n ln  n   n        n ln  n   n       
n n  
n  
n ln  n   n     
total number of tweets  n   is the

where n is the
number of tweets of the given class containing the given
word  and n   is the number of tweets of any other class
containing the given word 
our third feature selector was   feature selection 

we see this particular trait serve naive bayes well 
since it does rather well when using the simplest feature
set  bernoulli prescence vectors  leading to what is often
called the multivariate bernoulli naive bayes classifier 
the results are summarized in table   

 

 n     n     n     n       n   n    n   n    
 n     n       n           n     n       n     n    

   

mutual information and   feature selection is usually
formulated in terms of binarry classification problems 
we extend it to our multi class problem by throwing out
features whose maximum score with any class is below
some threshold  that is  is a feature does not score highly
with any class  we do not include it in our feature set 
examples of high scoring features for each feature selection method are shown in  

name
romney dataset
obama dataset
topics labeling

size
   
    
      

   fold accuracy
   
   
   

table    accuracy of naive bayes for each dataset

    naive bayes
the first learning algorithm that we tried was naive
bayes  despite naive bayes assumption of feature independence  an assumption which doesnt hold for natural language  it appears to perform resonably well on
our datasets  the algorithm works by learning the conditional probability for each particular feature  conditioned
on the label of the training example being processed  it

after running naive bayes  we looked at the conditional probabilities of each feature  and pulled out the
most informative ones for each class as shown in table
   

 

fitopical dataset
 
mutual info
presid
googl
billionair
ipod
money
 teamusa
 marscurios
us
mar
planet
 msl
iphon
money
 s
iphon
 london    
rover
basketball
olymp
planet

obama
 
mutual info
ridicul  sad 
vote  hopeful 
lost  afraid 
lie  angry 
nate  happy 
elect  angry 
disturb  sad 
campaign  angry 
     happy 
presid  angry 
lie  angry 
left  angry 
newspap  afraid 
tie  sad 
cnn  sad 
cnn  sad 
die  angry 
fight  hopeful 
random  afraid 
final  hopeful 

table    high scoring features using feature selection
money
land on
rais
 marscurios
app
what you
safe
join
land
latest
busi
youtub

obama   olympi              
mars   olympi              
obama   olympi             
mars   apple             
apple   olympi             
mars   olympi             
mars   olympi             
obama   olympi             
mars   olympi             
mars   apple             
obama   olympi             
apple   olympi             

    support vector machines
svms are widely known as an excellent learning model 
we used an implementation from the milk    library  and
customized that as detailed below 
      multi class classification
in order to work with with multi class classifications  we
looked at two svm models  one vs one and one vs rest 
dataset
attitudes
attitudes
topical
topical

table    nb most informative features

mode
one vs one
one vs rest
one vs one
one vs rest

accuracy
    
    
    
    

table    svm multiclass methods
one final note is that naive bayes is also a very fast
classifier  it handles a large feature space without a
huge time cost  our implementation of nb classified all
datasets in under   seconds 

the one vs rest method appears to have slightly higher
acuracy over the attitudes dataset  so we proceed with
that 
we trained over the same features as in previous models  singletons  bigrams and twitter speech features   and
to preprocess the data  we used feature selection to remove linearly dependant features  and also ran stepwise
discriminant analysis  sda   sda functions by selecting the feature that is most correlated with preditions 
removing its variance from the other features and then
repeating the step to add more features 
kernel  we experimented with two kernels  radial basis function and dot product  for the radial basis function
kernel  we performed a grid search over values of c from
            and  from         

    neural networks
the next machine learning algorithm we applied was an
artifician neural network  we used a tahn layer  softmax output multiclass neural network  using three hidden layers  we were able to achieve     accuracy over
the five class topic dataset  anns outperformed naive
bayes  but with a huge time cost  running    fold crossvalidation in parallel over     examples took several minutes 
layers
 
 
 
 
 
 
 

feat sel
 
none
 
mi
mi
mi
 

threshold
   
n a
   
   
   
   
   

accuracy
    
    
    
    
    
    
    

dataset
attitudes
attitudes
topical
topical

mode
rbf
dot
rbf
dot

accuracy
    
    
    
    

table     a svm kernel comparison
again  the results are close  but the rbf kernel with
grid search appears to outperform the dot product kernel 

table    neural net accuracies over the topical dataset

 

fiusing the feature selection techniques outlined earlier
and these optimal svm settings  we were able to achieve
a very high accuracy over the topical dataset  using  
feature selection  rbf kernel  and a one vs rest svm
classifier  we achieved        fold cross validation accuracy over all      examples 

    random forest
we experimented with three random forest classifiers 
using one against one  one against rest  ecoc       and
multi tree learners  the multi tree learner performed in
the     range on the topical dataset  using   feature selection  and ecoc  one against one and one against rest
yielded accuracies in the     range  we used stringent
feature selection          given the slowness of random
forests 
learner
ecoc
multi tree
one against rest
one against one
one against one
one against one
one against rest

training examples
   
   
   
   
    
    
   

accuracy
    
    
    
    
    
    
    

table     neural net accuracies
despite its high accuracy  we decided not to try ecoc
with larger datasets due to its prohibitive slowness 

figure    svm accuracy and dataset size  topic
dataset 

  case studies
    fans reacting to an underdog comeback

      class distribution features for attitudes
dataset

now that we have a working model  we would like to evaluate it on real world data  we collected approximately
       tweets containg the keywords saints or seahawks between      pm and      pm on the day of a
playoff game between the two teams  
in this case  the      saints were slated to dominate
the game against the     seahawks  surprisingly  the
seahawks won the game  causing a ripple of emotions to
spread through the twitosphere  we captured that effect
by analyzing the change in attitude distributions over
twenty minute intervals 

as discussed in the section on class distribution features  we had a breakthrough on the attitudes dataset
front  when we decided to use the class conditional probabilities of each feature to construct our svm input space 
we ended up settling on using just one input per class 
that was the sum of the condition probabilities of each
feature for that particular class  as seen in the training
data 
since these features were presumably highly seperable
with the radial basis function kernel  the svm accuracy was greatly increased  and the runtime was brought
down considerably  running    fold cross validation on
the datasets took no longer than   minutes  the results
for running a one vs rest svm using these inputs over
the various attitudes datasets are presented in table    
size
   
    
    

both teams
saints only
seahawks only

    
percentage

name
romney dataset
obama dataset
romney   obama dataset

tweets expressing mockery
   

   fold accuracy
   
   
   

   
    
   

table     accuracy of svms with class distribution features

    

  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

we would expect this technique to do better with increasing dataset size  but we suspect that internal variations in the datasets themselves  the obama dataset
had more uninformative tweets like loool    overpowered this overall effect 

we can see where the saints took an early lead  at the
end of the first quarter they were up       over the next

 

fitweets expressing happiness

  conclusions
overall  svm using a grid search over a radial basis
function kernel outperformed naive bayes  neural nets 
and random forests  random forest methods were able
to come close to svm accuracy  but were considerably
slower  further  our feature selection methods were able
to cut down dramatically on time without significant accuracy penalties 
even on the attitudes dataset  when used with our class
distribution features  the svm classifiers performed well 
and seemed accurate enough to explore previously unseen
tweets about a real world event 

both teams
saints only
seahawks only

   

percentage

   

   

   

  acknowledgements
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

we would like to thank austin gibbons for his consultation on the project  we would like to thank richard
socher for his help in guiding the project  we would like
to thank the infolab for lending us their computational
and data resources 

two hours  however  the seahawks outscored the saints
       then at the end of the game the saints mounted
a brilliant       rebound  but unfortuantely it was too
little too late  and the seahawks won the game 
intuitively  these events align with the rise and fall
of fans emotions  we can even observe subtle differences  for example the difference at the      mark between mocking and angry  where saints fans are angry
at the seahawks but not mocking them  while saints fans
are angry at the saints and the seahawks fans are mocking them 
we can see an interesting phenomenon with hopeful 
at the beginning of the game both sides are hopeful  initially the saints are expected to win and the saints to
lose  so when the saints take an early lead the seahawks
lose their hope for an upset  when the tide is turned we
can see a resurgence in hope  the truly interesting part
is that henceforth both teams observe patterns related
to when they scored  yet the overall amount of hope decreases steadily after the first half of the game  as fans
are hopeful for the teams at the beginning of the game
and then later pre occupied with other emotions 

references
    yiming yang and xin liu  a re examination
of
text
categorization
methods
http 
  www inf ufes br  claudine courses ct   
artigos yang sigir   pdf
    michael
d 
lee 
fast
text
classification
using
sequential
sampling
processes
http   lvk cs msu su  bruzz articles 
classification fast   text   classification 
  using   sequential pdf
    agarwal et al  sentiment analysis of twitter data 
     
    brendan oconnor  michel krieger  and david ahn 
tweetmotif  exploratory search and topic summarization for twitter  icwsm      
    milk  http   luispedro org software milk
    nltk http   nltk org 
    jin seon lee and il seok oh  binary classification trees for multi class classification problems http   www primaresearch org icdar     
papers          lee j pdf

tweets expressing hope
    
both teams
saints only
seahawks only

    http   www cs cmu edu afs cs project jair 
pub volume  dietterich  a pdf

    
   
    

  

  
  
  

  

  

  
  

  
  

  

  

  

  

  

  

 

   
  
 

percentage

   

 

fi