recommendation system for hcd connect
kesinee ninsuwan  mike phulsuksombati  umnouy ponsukcharoen

abstract hcd connect is a social platform by ideo org
that aims to connect social entrepreneurs around the world 
the users can post the stories about their past experiences in
service work as well as their prospective project ideas on the
website  when other users see the project ideas of their interest 
they can interact on the platform and get connected with other
users  with each story or project  a user is allowed to click
like  follow a story  post comments  or share it via other social
networks  moreover  the website provides a space for the users
to ask and answer questions which are not limited to specific
stories or projects 
this project aims to build a recommendation system on
the stories to the users  there are four main states for this
project including processing data  implementing unsupervised
text classification algorithms  implementing supervised text
classification algorithms  and building a score system for the
stories  the unsupervised classifications that we uses includes
hierarchical agglomerative clustering  hac   k means clustering  and topic modeling  the results indicate that they do not
discover significant topics underlying the stories  the supervised
classifications used in the project includes k nearest neighbors
 knn   support vector machine  svm   and multi label ranksvm  knn performs best among all algorithms  however  the
precision are still low  this suggests that more data needs to
be collected  therefore  we have to wait for more stories to be
posted on the website in order to improve the performance of
the recommendation system 

i  introduction
since hcd was launched in       the system is still in the
state of initial development  our project aims to use machine
learning to build a recommendation system to feed users
with the projects and stories that potentially interest them 
also  we expect to build a system that connects a user to
another user who share the same interest which can create
huge impact to social sector around the world 
from the initial data sets provided by the ideo org  hcd
now has        users and     stories  each user provides
personal information including sex  age  location  interest 
and organization  for each story  the data includes title 
description  author  topic  location  and design methods used 
due to the lack of information on browsing history and
user story interaction  we cannot build the personalized recommendation system as we wish before  therefore  this
project will focus on the text classification of the stories  on
each page of story  we aim to build a recommendation section
on the relevant stories of the same class based on machine
learning algorithm  this will improve the users experience
on the website 
this project consists of four main states  the first state is
processing data  in the second state  we use unsupervised text
classification algorithms in order to discover topics or clusters underlying the stories  in the third state  we build auto 

labeling system from supervised learning algorithms  when
a user posts a story on the website  he is asked to choose
appropriate labels for the story  there are nine labels to
choose from including agriculture  community development 
education  energy  environment  financial services  gender
equity  health and water   a story can have multiple labels 
given the stories with labels  we can use supervised text
classification algorithms to automatically recommend the
labels for each story  the last state does not involve machine
learning  however  it creates a complete recommendation
system that can be used on the website  in this state  we
give a score to each story based on page view  post date 
number of comments and likes  on each page of story  we
can then recommend three stories with highest scores which
are in the same class as the story on that page 
ii  processing data  
vector space model and metric
in this project  the training set is a collection of m
documents with n unique tokens  each story gives one
training example  here  a token is one single word  not an ngram  we use a vector space model to represent the training
set as an m  n term document matrix  each document is
a vector of m dimensions  the number in row i  column
j of the matrix represents the number of times word i
appears in document j  note that this representation does
not keep the information of the order in which each word
appears in the document  the size of our training set is
m        and the number of words throughout documents
after preprocessing is n         in order to construct a termdocument matrix  we first have to preprocess the document 
preprocessing takes an input as a plain text document from
the name  title  and description of a story and provides an
output as a set of tokens  preprocessing are done in steps
by filtering  tokenization  stemming  stopword removal  and
pruning  we use tmg matlab toolbox    to preprocess the
documents  once we obtain the term document matrix  a
weight is assigned to each token indicating the importance
of that token  this is done by tf idf weighing scheme  the
document vectors are now composed of weights reflecting
the frequency of terms in the document multiplied by inverse
of their frequency in the entire collection 
in order to measure the distance or similarity between
two documents  we use the euclidean norm  since svm is
developed under euclidean norm  this gives the consistency
in testing different algorithms that we use in this project 
then the document vectors are normalized to unit vectors 

fiiii  unsupervised text clustering
after we process the text document to get a matrix of
training set  we apply unsupervised text clustering algorithms
to the training set in order to discover the structure of the
data and possibly new topics in the cluster 
a  hierarchical agglomerative clustering  hac 
the first algorithm that we implement is hierarchical
agglomerative clustering  hac   this algorithm is based
on the idea of hierarchical clustering in which clusters build
by the algorithm creates hierarchy or a rooted tree  there are
two types of hierarchical clustering  the first type is bottomup  where at the beginning each document is its own
cluster  at each step  clusters merge based on some similarity
measure  the second type is bottom down  where initially
all documents are in one cluster  then at each step  a cluster
is divided into two clusters  hac uses the idea of bottomup algorithm  the clusters will be merged and converged
to the root of hierarchy  the merges are determined by
similarity between two clusters called linkage  there are
three common linkages  single  complete  and group average
link  here we use group average link only 
the algorithm can be summarized as follows 
   calculate linkage matrix whose entry  i  j  is the
linkage between the ith and jth clusters 
   merge two clusters with highest linkage 
   update the linkage matrix to reflect pairwise linkage
between the new cluster and the original clusters 
   repeat step   and   until there is only one cluster
remains 
when implementing the algorithm  we can decide where
to cut the hierarchical tree into clusters  there is a quality
measure for hac known as insufficiency measure cutoff 
the results are the followings 
cutoff
   
   
    
     
    

number of clusters
  
  
  
 
 
table i

t he result from hac

here one can see that the number of clusters is either
too large or too small  the results indicate that there is no
significant topics or clusters discovered 
b  k means clustering
another unsupervised algorithm is k mean clustering  it
is categorized as a non hierarchical clustering  the data is
assumed to have k clusters as priori without hierarchy  in our
implementation  we measure the distance between clusters
using the euclidean norm  the algorithm can be summarized
as follows 
   select k points as initial centroids 

   assign all points to the closest centroids using euclidean norm 
   recompute the centroid of each cluster to be the
centroid of those points assign to it 
   repeat step   and   until centroids no longer move 
first we attempt to find appropriate number of clusters
k  given k  we measure how well separated the resulting
clustering are using silhouette numbers  we then find the
average of silhouette values of k clusters  small silhouette
value means the clusters are not well separated  the plot
below shows the average silhouette numbers for each k 

fig    

the graph of average silhouette value vs  k

the average silhouette value is small         for all k 
and for k       the algorithm fails since an empty cluster
is created  therefore  there is no new significant topics or
clusters discovered 
c  topic modeling
while hac and k means algorithms are known as hardclustering  topic modeling is known as soft clustering because it clusters the training set by probabilistic model of
topics  it specifies a simple probabilistic procedure by which
documents can be generated  for each document  we assume
that words are chosen in a two stage process 
   randomly choose a distribution over topics 
   for each word in the document
 randomly choose a topic from the distribution
over topics in step    
 randomly choose a word from the corresponding
distribution over the vocabulary 
the algorithm is designed to find the hidden structure  i e 
the per document topic distributions  the per document and
per word topic assignments  we use the observed documents
to infer the hidden topic structure  this is known as latent
dirichlet allocation  lda   after constructing the joint
distribution of the hidden and observed variables  bayess
rule gives the conditional distribution of the topic structure
given the observed documents  however  in general  the
marginal probability of the observations  which is the sum
of the joint distribution over every possible instantiation
of the hidden topic structure  is exponentially large  topic
modeling algorithms need to form an approximation for

fisuch probability  those algorithms fall into two categories
  sampling based algorithms and variational algorithms  in
this project  we only focus on a sampling based algorithm
in which gibbs sampling is used  it is a statistical technique
meant to quickly construct a sample distribution  here we
implement gibbs sampling based lda algorithm using   
topics  which is comparable to the number of labels we had 
the resulting topics are shown below 
topic
 
 
 
 
 
 
 
 
 
  

significant words
ideo learn organ org women earli tedx
commun creat model live work understand impact
project experi hear interview anim power base
team org busi fellow chang idea cookstov
improv health small process support opportun educ
prototyp field help research challeng india kenya
farm innov water urban program aquapon light
farmer food school local connect incom low
design center social rural human garden system
develop agricultur product sustain hcd servic technolog
table ii

t he significant words for each cluster from topic modeling
algorithm

notice that these clusters do not form according to the
labels  for example  words related to agriculture appear
in topic           yet words in each topic do not share
outstanding category  hence  no new significant topics or
clusters are discovered 
iv  supervised text clustering  
auto labeling system
in this section  we use the information on labels from the
writers of the stories to implement the supervised learning
algorithms  for each story  we will use clustering algorithm
to identify the labels that are fit the story the most based
on our models  this system will help the writer identify
the correct labels to the story  throughout this section  we
use q to denote the number of all possible labels  and m
to denote the number of data  for this project  q     and
m        because a story can have multiple labels  this
system relies on multi label classification and corresponding
evaluation methods  the multi label classification methods
can be divided into two categories  problem transformation
methods and algorithm adaption methods 
the problem transformation methods transform one multilabel classification problem into multiple single label classification problems  or into one classification problem with
multiple labels  there are two canonical ways to do this 
the first way is to do q binary classifications  where q is the
number of all possible labels  for each label i  we perform
a single label classification on the data to identify if each
story should have label i or not  the other way is to do one
classification with  q classes  each of  q classes represents
a subset of the set of q labels  in this project  q     gives
         classes  while there are     training examples  the
training size is very small compared to the number of classes 
this method will not work well with our problem  therefore 
we will only implement the first method of transforming the

problem into multiple single label classification problems 
we will use the k nearest neighbor  knn   and support vector
machine  svm  for this project  for the algorithm adaption
methods  we will use method based on knn and svm 
a  evaluations
for the evaluations  we use three methods including persistent precision  persistent recall  and hamming loss  suppose
there are m test documents and q labels  the persistent
precision and recall break the data into m  q instancelabel pairs  and evaluate by regular precision and recall 
notice that these two evaluations do not utilize the idea
of multi label data  another evaluation is hamming loss  it
measures the cardinality of symmetric difference between
the set of predicted labels and the set of actual test labels
averaged over the number of test documents and number
of all possible labels  hamming loss equal to zero means
the algorithm makes a perfect prediction  and the algorithm
performs well when hamming loss closes to    while the
algorithm performs poorly when hamming loss closes to   
in this project  we use simple cross validation by splitting
    data instances into     instances for training  and    
instances for testing 
b  problem transformation methods
k nearest neighbors  knn   in knn method  for each
label and each example in testing set  we find its k nearest
neighbors in the training set using euclidean norm  then that
example is identify whether it should be tagged with that
label or not based on the statistical model we construct from
the training set  the results from implementing q single label
knn methods are below 
value of k
precision
recall
hamming loss

 
     
     
     

 
     
     
     

  
     
     
     

  
     
     
     

  
     
     
     

table iii
t he evaluation of knn method

support vector machine  svm   we implement
svm using   different kernels including linear kernel
k x i    x  j       x i   t x  j    gaussian kernel k x i    x  j     
exp kx i  x j k     and n degree polynomial kernel
n
k x i    x  j       x i   t x  j    here we use      for both
gaussian kernel and n degree polynomial kernel  the cost
parameter c is chosen to be one by default  the results are
shown below 
kernel
precision
recall
hamming loss

gaussian

linear

     
     
     

     
     
     

polynomial of degree
 
 
 
     
     
     
     
     
     
     
     
     

table iv
t he evaluation of svm

fivalue of k
precision
recall
hamming loss

 
     
     
     

 
     
     
     

  
     
     
     

  
     
     
     

  
     
     
     

table v
t he evaluation of muti   label knn

for this application  it is more important that the users
do not miss a chance to read the stories they are interested
in than they are recommended the stories in other topics 
therefore  recall is more important than precision  in knn
algorithm  recall is highest when k       in svm algorithm 
it is somewhat surprising that different kernels give the same
performance through all evaluation measure  one possible
reason is the examples x i  s do not contribute significant
changes  thus  we conclude that among svm algorithm 
linear is sufficient to implement  overall  knn algorithm
outperforms svm algorithm 
c  algorithm adaptation methods
algorithm adaptation methods consider all q labels together  most of these methods are variations of single label
classification algorithms  in this project  we use methods
based on knn and svm 
multi label knn for this method  we used the algorithm
based on the paper by zhang and zhao      proposed by zhang
and zhao in       multi label knn is a variation of knn
algorithm applied to multi label classification  in regular
knn algorithm  an unlabeled vector is classified by assigning
the label which has the greatest probability among the k
nearest training examples query point based on the statistical
model constructed from the training data  in multi label
knn  we need an alternative method to utilize information
from neighbors  after identifying all neighbors  the algorithm
chooses labels on the query point to maximize a posteriori
 map  principle  when training the data  for each label t  we
construct the probability that a given example has label t and
the probability that it does not have label t given that it has
l neighbors with label t  where t              q and l              k 
based on these probabilities  we can assign the labels to each
test example  and evaluate the performance of the algorithm 
the results are shown below 
similar to regular knn  multi label knn performs best
when k        notice that the value of all evaluation measure
are same as regular knn method  this is because in the
paper by zhang and zhao      each label is treated separately
in building the statistical model and in assigning the labels to
the test examples  therefore  this is equivalent to the regular
knn we present in section iv b 

rank svm proposed by eisseeff and weston in      
rank svm is a variation of svm algorithm applied to
multi label classification      in regular svm algorithm  an
unlabeled vector is classified by its location with respect
to a separating hyperplane  the separating hyperplane is
chosen to maximize the margin and minimize penalty from
separating hyperplane violation  in multi label kernel learner 
we need an alternative method to draw hyperplanes and
assign penalty function for multi label data  in regular svm 
we find the hyperplanes by solving the optimization problem
 
m
 
    w    c i   i

   

y i     w  x i    b      i  

   

i     i              m

   

min
  w b

subject to

ideally  we would prefer all data with positive label to
be above the hyperplane   w  x i    b       represents the
violation to such ideal  in rank svm  we now have multiple
hyperplanes  wk   bk   where k              q  the optimization
problem associated to rank svm is the following 
m
 
  q
  wk      c 
 ikl

 y
  
  w j  b j   j       q   k  
i   i yi    k l y y

min

i

   

i

subject to   wk  wl   xi   bl   bk     ikl    k  l   yi  yi     
ikl    
where yi is the set of labels for document i  and yi is the
complement of yi   here  we may view quantity   wk   x    bk
as a kth score  ideally  we would prefer all data with label k
to have kth score higher than any scores associated to nonlabels l  that is why this algorithm called rank svm  
represents the violation to such ideal 
to implement rank svm algorithm  we need to specify
parameters associated to each kernel as in regular svm in
section iv b  here we use      for gaussian kernel and for
n degree polynomial kernel  the cost parameter c is chosen
to be one by default  the results are shown below 
kernel
precision
recall
hamming loss

gaussian

linear

     
     
     

     
     
     

polynomial of degree
 
 
 
     
     
     
     
     
     
     
     
     

table vi
t he evaluation of rank  svm

here one can see that rank svm performs better than
regular svm throughout all evaluation measures  it shows

   

fieffectiveness of algorithm adaptation methods  however 
rank svm gives relatively poor performance compared to
even regular knn  overall  regardless of algorithms  the autolabeling system performs poorly  recall rate is below     
there are two explanations for this  first  the quality of labels
given in our data set may be poor  consider the histogram
below 

effects  less preferable to users   simple functions for f s and
gs are linear functions with positive and negative parameters 
respectively  these parameters should be weighted depending on importance of each factor  for example  if it does not
matter how long the stories has been published  we may set a
factor of g  to be zero  then  we can recommend three stories
with highest scores on the page of the current story  this
score system may extend to personalized recommendation
system if we know a story user interaction  for example 
we may add g   k   j  k   where k refers to the stories that
the user has visited  to complete machine learning based
recommendation system even with the early state  we need
to improve auto labeling system which requires more data 
vi  conclusion

fig    

histogram of number of labels for     documents

there are    documents with at least   labels  these labels
may not reflect important topics that we can capture by
learning from the content of the document  it is possible
that the writers know by themselves that the story related to
many labels  however  the content of the documents do not
reflect the relationship with the labels  if we remove those
documents out  we might improve the performance of the
auto labeling system  another way to improve the labeling
system is to have a person other than the writer assigning
the labels to the stories  another reason explaining why the
algorithms perform poorly is the dataset here may be too
small  this might be the reason why knn  a local based
algorithm  outperforms svm  a global based algorithm  we
need to wait for more stories and labels from the users in
order to fully implement the auto labeling system 
v  score of recommendation system
in this part  we assign a score for each story  once the
user visits a story j  we will collect all stories with at least
one similar label to the current story and compute the score
as follows 
score  story j  current story i    f   number of page views
j    f   number of comments j    f    number of likes j 
 g   current date   post date j    g   distance i  j   
where f refers to an increasing function since parameters
inside f s contribute to positive effects  more preferable to
users   while parameters inside gs contribute to negative

the unsupervised classification algorithms do not give
significant topics underlying the stories  from the supervised
classification algorithms  the multi label svm performs better than the regular svm  overall  the k nearest neighbors
algorithm  knn  outperforms all other algorithms  however 
the precision of the algorithm is still very low  this is
because the data set is too small  therefore  looking at the
local structure at each data point  knn  gives the better
results  moreover  this indicates that we should wait for more
stories to be posted by the writers in order to improve the
performance of the recommendation system  however  in
term of memory storage  svm is the most efficient algorithm
because the computer only have to store the relevant parameters instead of the whole data set  therefore  we predict that
once there is large enough data set  rank svm will perform
best in term of both precision and efficiency 
r eferences
    a  nicholas o   and f  edward a   recent developments in document
clustering  october      
    b  david m   introduction to probabilistic topic models  princeton
university 
    e  andre  and w  jason  a kernel method for multi labelled classification 
    g  tom  and s  mark  matlab topic modeling toolbox      release
date  april      
    g  tom  and s  mark  probabilistic topic models  university of
california  irvine 
    hierarchical clustering  mathworks  the mathworks  inc 
    k mean clustering  mathworks  the mathworks  inc 
    tmg 
text
to
matrix
generator 
 http   scgroup   ceid upatras gr      tmg   
    z  min ling  z  shi hua  ml knn  a lazy learning approach to
multi label learning  pattern recognition                         

fi