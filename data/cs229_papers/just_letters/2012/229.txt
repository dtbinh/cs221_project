the perfect ihum essay
predicting ihum essay grades

andrew moreland
charlie guo

  introduction

its weighting of certain features allowed it to be
tricked by doing things like writing longer essays  the problem was that peg did not analyze the semantics of the essays  instead  it only
analyzed the structure 

introduction to the humanities  otherwise
known as ihum  has been a required course for
stanford freshmen for several years  ihum is a
name for a now discontinued collection of classes
that covered topics ranging from archeology and
world religions to russian literary history  intended as a way to introduce new stanford students to a set of core concepts in the humanities
in order to assure a rounded education  ihum
has been havily criticized for its opaque and  to
students  seemingly arbitrary grading 

more recently  however  other systems such as
intelligent essay assesor  iea   use more sophisticated techniques to predict scores  one
of these techniques is latent semantic analysis
 lsa   a statistical model of word usage that
permits comparisons of the semantic similarity
between pieces of textual information  along
with iea  other programs like criterion and
e rater used other natural language processing  nlp  techniques to generate essay scores 
which proved more effective than the initial lsa
approach 

our project attempts to use various features of
writing and diction in order to predict the final grades of essays written for ihum classes 
we draw our training and testing examples from
a corpus that we have gathered from past students essays and apply machine learning techniques in order to classify these essays into one
of two categories  a or b 

our project attempts to replicate the statistical approach of peg but also leverages a technique associated with classifying spam emails 
we implement two strategies  first we compute
numeric data about the structure of the essays
and use an svm to classify them  and then we
use a multinomial naive bayes classifier over the
word frequency counts of the essays 

  background
automated essay scoring has been around for
some time now  with approaches and use cases
becoming more varied with time  some of the
original systems were developed in the     s 
at the request of the college board     one
of these systems  project essay grader  peg  
used simple features of the written essays in order to determine a relative score  while these
features were labeled ahead of time  peg did
not use machine learning to analyze the input
features   while peg was somewhat effective 

  data collection
generally machine learning projects do better if
they have a larger body of training data to draw
from  we figured that since nearly every student
in recent years who has passed through stanford
has taken an ihum course  it would be easy
to collect a large corpus  to facilitate this we
created a site  http   ihumessayproject com  

 

fi  methodology

which allowed students to easily upload their
previous ihum essays along with the grade that
they received  in reality  it turns out that people are not so willing to invest the few minutes
it takes to find their old essays files and upload
them  as such  we received a total of     usable
essays  which is a fair number and more than we
had hoped  but is fairly small compared to the
corpuses used for other efforts   

    algorithm    svms
initially we attempted to use svms to classify the essays based on the computed statistics above  we used libsvm  in order to train
svms on     of our collected essays  we computed training accuracy on the data with linear  polynomial  radial and sigmoid based kernels with regularization  we attempted a grid
search of the various parameters of these models but were never able to achieve better than
    training accuracy  for a binary classification problem where the training data is split
roughly        between the two classes this is
clearly abysmal performance  so we abandoned
this strategy and moved on hoping to find more
success elsewhere 

  pre processing
in order to generate features for our svm  we
pre processed the essays in order to extract the
following data 
   essay length
   paragraph length

    algorithm    multinomial naive
bayes

   number of quotations

we decided to start with a binary classification
problem  we assigned essays to two classes  essays that were below an a became b essays 
and essays that were above a b  became a
essays 

   length of quotations
   average word length
   word frequency  using a porter stemmer 

drawing inspiration from the example of spam
classification  we decided to use multinomial
naive bayes trained over our essays in order
to classify them  we split the data set into
    training and     testing  drawing inspiration from machine learning competitions like
the netflix challenge and used   fold cross
validation on the rest of the data in order to
maximize our training accuracy  upon maximizing our training error  we computed our test
error against the holdout set and recorded that 

   part of speech frequency  using the natural language toolkit for python   
   misspelled words  words very close to
words in the dictionary according to edit
distance 
   flesch kincaid grade level 
    flesch reading ease scale score 
    fog score 

initially we tried the most naive implementation
of naive bayes  we simply trained our classifier on the entire vocabulary found in all of
the essays and then attempted to classify using
the trained model  we found that we were able

in addition to extracting these features  we
cleaned the data by removing things like stop
words  and the headers that people included in
their essays 
 

http   nltk org 
http   www readabilityformulas com flesch grade level readability formula php
 
https   github com sebbacon pyflesch
 
http   www readabilityformulas com gunning fog readability formula php
 
http   www lextek com manuals onix stopwords  html
 

 

fimind  we reran our computation and found that
our training accuracy improved to roughly    
and that our vocabulary was cut from roughly
       words down to only        

to achieve roughly     accuracy with this approach  which is barely more than what would
be achieved if our classifier simply guessed the
most prevelant class each time  still  since it was
more accurate than guessing that we were encouraged to explore more advanced techniques 

this was still fairly poor performance but the
general approach seemed promising  so we decided to attempt more advanced feature selection  research indicated that a strong metric
for feature selection in text classification problems was mutual information    using the rainbow classifiers  built in algorithm for mutual
information computation  we initially selected
the      most informative words and saw our
training accuracy jump to     

we researched improvements to basic naive
bayes and found that feature selection was often
the most important factor in the success of the
algorithm  research indicated that one of the
simplest feature selection algorithms was often
helpful  simply removing words that only appeared in a single document helped improve the
overall accuracy of the classifier  with this in

figure    as we increased the number of documents in which a word in our vocabulary was required
to appear  training accuracy sharply decreased  our vocabulary size played a smaller
and less consistent role  the error in the near right is roughly      and the overall error
moves towards     on the left side 
at this point we decided to perform a more systematic optimization of parameters  we ran
a coarse grid search over the parameter space 
searching for the optimal number of documents
in which to require each word to appear  and the
optimal number of informative words to select as
features  our grid search ran over the integers
in the intervals         and           respectively 

we ran a coarser grid search initially  and narrowed in on this area by an informal evaluation
of the results 
eventually  we narrowed in on a choice of the
    most informative words that appeared in at
least   documents as the optimal parameters 
with which we achieved a consistent     training accuracy   note  we previously observed

 

fiset and found that we achieved a test accuracy
of      we ran several more tests  varying the
training test size  and generating the new sets
randomly each time  and found that the exact
test accuracy tending to vary but was consistently over     for reasonable training sizes 
there also appears to be a trend towards convergence between test and training error somewhere between     and     as the training size
increases 

that     was a tipping point after which training accuracy decreased  so we did not evaluate
anything past it in our fine grid search  

  results
using the parameters that we determined in our
grid search  we ran tests over our     hold out

figure    as the size of the training set increases  we see that training error tends to remain fairly
constant and there appears to be a trend towards convergence 

  conclusions

sociated with the essays  if we had known this
information  then we could perhaps have applied
more techniques to the data  above all though 
we express the same sentiment that many people
who have attempted to apply machine learning
have expressed  we wish we had more data 

overall  we are fairly satisfied with the results
of this project  we believe that we have demonstrated that it is possible to use naive bayes
to predict a distinction between essays that is
fairly fine  the difference between an a and a
b can be fairly slight 

  references

we feel that with more training data  we could
have improved the accuracy of our classifier 
we feel that our results are good though given
that the essays are from different eight different classes with different prompts and subjects
and even more than eight graders  if we were
to do the project again  we would attempt to
capture more information about the context of
the essays in our training set  it would have
been nice to know the word limits of the essay
prompts and the actual classes and graders as 

notes
 

dikli  s          an overview of automated scoring
of essays  journal of technology  learning  and assessment       
 

http   urd let rug nl nerbonne papers santos et al     grading pdf
 

a

 

chih chung chang and chih jen lin  libsvm 
library for support vector machines 
acm

ficlass pdf

transactions on intelligent systems and technology                    
software available at
http   www csie ntu edu tw  cjlin libsvm

 
mccallum  andrew kachites  bow  a toolkit for
statistical language modeling  text retrieval  classification and clustering  http   www cs cmu edu  mccallum bow       

 

aggarwal  charu c   a survey of text classification algorithms  http   www charuaggarwal net text 

 

fi