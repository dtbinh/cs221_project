extracting vocal sources from master audio recordings
derek mendez  tarun pondicherry  chris young
abstract
our goal is to separate vocals from background music in single source songs  we examined prior work                     
in the area and decided to experiment with  d adaptive probabilistic latent component analysis  plca         because it
is a good trade off between implementation difficulty and resulting accuracy  the training data for the  d adaptive plca
algorithm is a set of song segments containing only background music  the adaptive algorithm is more practical than
plca because typically  a song will have background music only segments  but not vocal only segments  this prevents
training directly on the vocal segments  thus  the adaptive version learns about vocal segments dynamically by adding
spectral basis vectors to represent voice as its performing the separation         we also introduce an svm at the input
stage to create the training samples for the plca algorithm to improve automation of the process  perceptually  we are
able to separate background music and vocals in a very noticeable way  and make use of an established method to
quantify our results 
introduction
semi blind source separation given a single observation containing multiple sources is a popular and difficult problem  in
particular  there has been much work in the area of audio source separation  of in this works case  vocals and
background music                  however  there are few actual products on the market  motivating examples range from
editing previously mixed songs  creating karaoke tracks  and removing undesirable noise 
figure   gives an overview of our proposed algorithm  first  a support vector machine  svm  is used to label sections of
songs that contain vocals and sections with just background music  this improves upon prior work by allowing for the
separation of a large class of songs after training the svm to classify vocal containing segments in a particular band or
genre  the labeled song is then passed to the pcla algorithm which after taking the short time fourier transform
 stft   also known as the spectrogram  uses expectation maximization  em  to learn the spectral signature of the
background  this spectral signature is then used with the same pcla algorithm to estimate the spectral signature of the
vocals  along with a time signature for the whole mixture   we then can extract both background and vocals by projecting
the spectrogram on the learned basis  the following sections will describe the svm and the pcla algorithm in detail as
well as summarize our results 

figure    overview of dataflow in our implementation

svm
creating training data for plca is tedious since it involves manually classifying song segments in every song thats being
separated  to improve the scalability of this process  we experimented with training an svm to automate this step  it is
possible to simply train plca directly on the training data used for the svm  however  plca doesnt perform well in this
case because the training data isnt local enough to accurately represent the background in the section being separated 
by using an svm on the input phase  we guarantee that the training data is localized to the section thats being
separated 
feature representation
we experimented with two potential input feature vectors  based on prior work in the field of audio source separation  we
concluded that spectrogram based approaches are most promising and focused our attention on generating an input
vector from spectrograms of song segments which have vocals and song segments which dont have vocals  in our first
approach  we reshaped the spectrogram matrix into a vector and used that directly as input to the svm  we were
surprised that such a simple approach yielded     accuracy with certain parameters 
we also experimented with re scaling the power spectrum according to the mel frequency scale  the mel scale is derived
to be a natural scale with regard to human auditory and speech processes  it is therefore a logarithmic representation of
the original power spectrum  we chose to use the mel basis because many speech from noise separation algorithms also

fimake use of it  we can also achieve significant reduction in training example dimensionality  while a typical audio power
spectrum contains frequency values between    hz and        hz  the corresponding mel representation contains    
numbers  figure     with this we were able to achieve nearly     accuracy  figure    

figure    a comparison between the standard time frequency scale and the corresponding mel frequency scale  we were able to
reduce the dimensionality of our problem significantly by using the mel components 

in addition to using the mel spectrum as feature variables  we also computed spectral properties such as the flatness 
brightness  and root mean squared amplitude  in doing so we were able to gather a wealth of information about a power
spectrum and work with it in a relatively low dimensional space  this significantly improves the svm training efficiency 
to get the training data  we acquire a library of songs  randomly select a song  and randomly select a corresponding
  second segment  we then manually determine whether the segment has vocals or not  these segments  with labels 
are the inputs to the svm  this process is mostly automated and very rapid  hundreds of training examples can be
gathered in minutes 
accuracy evaluation
for estimating prediction accuracy  we used    fold cross validation  our preliminary training set library is the first   
songs from the beatles album rubber soul  we observed the effects of using different kernels in the svm as well as the
effects of down sampling the audio segments  we also experimented with changing the number of fft points and using
various windows  but found no significant differences in the results by varying these parameters 
decimation
we observed the effects of down sampling the audio segments that are chosen as training examples  and fed into the
training algorithm  the audio is sampled at      khz  by decimating the original audio we were able to achieve a higher
svm prediction accuracy until the decimation factors get large to the point where important information is lost  figure    

figure    left  
effect
of
decimation
factor on accuracy with different kernels using      fft points  a hanning window of      points   overlap  training on closer to
you and using    fold cross validation to determine prediction accuracy 
figure    right   effect of decimation factor on accuracy for a quadratic kernel using a vector of the mel components  amplitude 
flatness and brightness on a spectrogram with      fft points  a hanning window of      points   overlap  training on a collection of
beatles songs and using    fold cross validation to determine prediction accuracy 

kernels
we experimented with linear  quadratic and gaussian kernels  with varying width   we found that the width of the
gaussian kernel doesnt play a large role in determining the accuracy of the svm algorithm and performs poorly overall 
we achieve the best accuracy with a quadratic kernel  figure    

fi d adaptive plca
we implemented  d plca in matlab based on         the algorithm is an em algorithm that operates on the magnitudes
of the spectrograms  s t f  where t is time and f is frequency  the spectrogram is normalized and treated as a distribution
such that the sum of all  t f  pairs  quanta  is one  the equations in figure   are iterated on until convergence  typically    
iterations   refer to     and     for a detailed explanation of the theory  we later obtained and experimented with a more
performance optimized  but functionally identical implementation of  d plca from the authors of     
expectation 
p    fzt
r f t     s f t    p f t 

maximization 
f    frtt
t    tttr
zi   

fij

figure    equations used for  d plca on the spectrogram

f is composed of nz  number of latent variables  columns that represent the probabilities p f z   that is some distribution
over all frequencies for a given latent variable  similarly t is composed of rows of temporal distributions p t z  and z is a
diagonal matrix of the probabilities p z  of the latent variables  in such a way  we are able to represent the whole
spectrogram as a distribution p   fzt  composed of frequency and time signatures 
separation performs two passes of plca  in the first pass  plca is used to find a spectral basis  fmusic  for the
background  in the second pass  plca is used to find both a spectral  fmixed  and temporal basis  t  for the mixed signal 
however  during this phase  the spectral basis learned for background is kept constant and used as a component of fmixed 
thus  any spectral vectors found during this phase are mainly important in expressing the vocal component  and we have
fmixed    fmusic   fvocals   to extract voice  we project s onto the basis spanned by f and t  then set the magnitudes of the
basis vectors of fmusic to zero to remove the background component  the scaled magnitude spectrogram corresponding to
the background is given by pmusic   fmusiczt and the scaled vocals by pvocals   fvocalszt  the signal is then reconstructed by
combining these magnitudes with the phase data from the original spectrogram and taking inverse ffts 
toy example
a toy example of separating two sine waves shows the algorithm in action  the background source is considered to be a
sine wave at     hz  the vocal source is a sine wave at     hz  the mixed source consists of a linear combination of the
background source and the vocal source  shown in figure    as parameters  we choose to consider    latent variables
such that five p f z s are initially learned for the background and five p f z s are learned for the vocals given the mixture
and holding the background p f z s constant  the learned p f z s are shown in figure   

figure    spectrogram of mixed source

figure    learned f f z  during first plca pass

as can be seen  the p f z s contain the distributions of the frequencies estimated to be the highest likelihood basis for the
background and vocals      hz and     hz   figure   then shows the result of separation  because we have initially
trained perfectly on the     hz sine wave  we see a near perfect extraction  and in the second plca is able to suppress
the     hz wave by more than    db 

fifigure    result of separation after using projecting the spectrogram on the background p f z  basis and vocal p f z  basis

separating closer to you
the audio files of the results of the separation of several popular songs can be found at cs    weebly com  figure  
shows the original spectrogram of a nine second clip from brandi carliles closer to you and the resulting spectrograms
of the extracted background and vocals 

figure    resulting spectrograms of closer to you after separation

the vocals can be seen as the harmonics with a lot of vibrato  the background music is focused more in the lower
frequencies of of the spectrum  one can visually see the success of the separation 
accuracy evaluation
how well our proposed algorithm performs is inherently a subjective measure as humans will have different perceptual
tastes  however  in order to evaluate the performance of our algorithm as well as optimize its various parameters  it is
desirable to have some metric that given the actual separate vocal and background tracks for a song will quantify how
well the extracted track matches the actual track  this is a very involved  and open  research problem in itself  for
instance  we may suppress the vocals of a song very well  however in the process also suppress a portion of the
background music we are trying to keep  while the result may sound fairly pleasing  if one was to take a standard cross
correlation with the actual background music track  we might find that a maximum value for the correlation would be
achieved for a different set of parameters that still audibly left a significant portion of vocals  thus  for this project we
made use of the peass toolkit that is meant for evaluating blind audio source separation algorithms      the toolkit
computes a target related perceptual score  an artifact related perceptual score  an interference related perceptual score 
and an overall combination of these scores  each score is between   and      with     being the best perceptual match 
we found that the overall score generally matched our particular tastes  yet there was still slight variability in our
preferences 
dimensionality of z
there are many parameters that can be varied in the proposed separation algorithm  these include the number of fft
points for the construction of the spectrogram  the type and size of the time window used in the spectrogram  and the
overlap of successive time windows  based upon trial and error  as well as suggestions in prior work      we set the
number of fft points as       and used a hanning window of the same size with an overlap of      samples  we also
experimented with decimation of the original      khz sampling rate  but ultimately decided upon no decimation 
another parameter that had a great effect on the quality of separation was the number of latent variables  nz  chosen for
the model  in addition to nz  the proportion of variables allocated to each source  background nzb  and vocals nzv  where
nz   nzb   nzv  also greatly affected the quality of separation  the following figures show an experiment performed on
the clip from brandi carliles closer to you  nzb and nzv were swept from    to     and the overall perceptual score
using the peass toolkit was evaluated for both the background extraction and the vocal extraction  between          

fifigure    error according to peass for various values of nzb and nzv

it should be noted that upon listening to the results of these experiments  we do not necessarily agree that the tracks that
received the highest perceptual score were the best  however  the general trend was preserved  in agreeance with the
authors of      we found that nz       was a good general number of the latent variables  further  for optimal background
extraction we found that if we proportioned the variables such that nzb      and nzv        we were near the best we
could do for background extraction  however  this was not the case for optimal vocal extraction  while figure   suggests
that we would achieve good results with fewer background latent variables and more vocal latent variables  we found that
this ultimately depended on the song selected 
according to      the logic behind choosing the number of latent variables for each case is this  if we choose too many
variables for either the background or the vocals  then we are prone to capturing components of the undesired track in our
spectral description  further  if we choose too few latent variables  then we are not able to fully represent the extracted
source and we might notice it sounds like there are missing high or low frequency components  unfortunately  we could
not easily exploit any trends in our experiments  as the resulting function of nzb and nzv is not convex  thus  we swept
the entire space to determine the best separation for each song 

conclusion
we found that plca is useful for separation in low fidelity environments and analyzed the error under various different
parameters  we also improved upon prior work by introducing an svm phase to automate more of the process making it
more practical for real world use  some samples of our results are on cs    weebly com  we found that this performs
poorly on songs where much of the frequency distribution of the background is close to the vocal range  for example
female vocals and guitar   in the future  one could explore using various other transforms  dct  wavelet  continuous q  to
improve accuracy of the svm as well as separation  using a measure of correlation between the testing and training
could also be used to improve the localization of the distributions learned by plca and improve separation 

references
    b  raj  p  smaragdis  m  shashanka  and r  singh  separating a foreground singer from background music 
international symposium on frontiers of research on speech and music  mysore  india         
    fuentes  benoit  roland badeau  and gal richard   blind harmonic adaptive decomposition applied to supervised
source separation          
    l  benaroya  f  bimbot  and r  gribonval  audio source separation with a single sensor  ieee transactions on audio 
speech  and language processing                    
    peass toolkit  http   bass db gforge inria fr peass peass software html
    smaragdis  paris  and bhiksha raj   shift invariant probabilistic latent component analysis   journal of machine
learning research        
    virtanen  tuomas   monaural sound source separation by nonnegative matrix factorization with temporal continuity
and sparseness criteria   audio  speech  and language processing  ieee transactions on                        
    yipeng li  deliang wang     separation of singing voice from music accompaniment for monaural recordings  
audio  speech  and language processing  ieee transactions on   vol     no    pp            may       doi 
        tasl            

fi