automatic detection of dark matter via weak
gravitational lensing

thomas colvin
tcolvin stanford edu

 

francisco capristan
fcaprist stanford edu

introduction

dark matter is an elusive form of matter that makes up approximately     of the known universe  since it does not absorb or reflect light  it can never be detected via direct observation with
a telescope  instead  its existence must be inferred from the effect of its gravitational field on the
background stars   galaxies  if a source of dark matter lies between an observer on earth and a
field of background stars  the dark matters gravitational field will distort the light from the background stars to change the shape and distribution of the stars that the observer sees from their true
distribution  this effect is called gravitational lensing and it can be split into two regimes  weak
and strong  the effects of strong lensing are somewhat obvious because the distortions produced
are very pronounced  i e  affected galaxies will appear to be bent into long arcs  but this regime is a
relatively rare occurrence  weak lensing  however  produces distortions that are difficult to separate
from the background noise of the universe  i e  the size   ellipticity distributions of galaxies  and
measurement error  i e  thermal distortions in telescope mirrors  effects of atmosphere  etc    this
is the predominant regime for observations of the universe and automatic detection of weak lensing
effects is a relatively new field of study 

 

training data and signal
visualization of galaxies and ellipticities for sky   

    

    

    

    

ydim  pixels 

ydim  pixels 

signal map of sky   

    

    

 

    

    

 

    

    
    
xdim  pixels 

 

    

 

    

    
    
xdim  pixels 

    

figure    the figure at left shows a signal map for the sky  notice that the area of maximum signal
corresponds closely to the location of the dark matter source when there is only a single source
present  the figure at right shows the galaxies in a single sky with their ellipticities and angles 
measured by the line length and angle from the x axis respectively 
the training data was taken from the data mining competition website kaggle com   the data consisted of     skies  each containing anywhere from     to     different galaxies specified by a
position  x y  in the sky and an ellipticity vector  e  e   which specifies its orientation  each sky
 

ficontains at least one and up to three sources of dark matter  which distort the images of the galaxies
through weak gravitational lensing  specifically  weak lensing distorts the image of a lensed galaxy
by amplifying the galaxys perceived size and by rotating such that it lies more tangent to the dark
matter source  we define the tangential ellipticity of a galaxy at a position  x y  tangential to a point
 x y  in equation   where  is the angle from the point to the galaxy defined by equation   
etangential    e  cos     e  sin   

   arctan  

   

y  y 
 
x  x 

   

this motivates the idea of a tangential ellipticity signal  i e  a measure of the tangential ellipticity
of galaxies in the sky with respect to a point  which is simply the sum of the tangential ellipticity of
galaxies around the point  x y   high signal strength tends to be a good indicator for the location
of the dominant dark matter source  the signal map could sum over all galaxies present in the sky
or only sum over a smaller number of galaxies that lie near the point of interest  an example of a
signal map is seen in figures   and   

 

methods attempted

   

logistic regression

for this technique we attempted to cast the problem as a classification problem by discretizing the
sky and asking does this location contain the peak of the dark matter  the motivation for this
approach was the difficulty in creating feature vectors that were meaningful across each sky using
the locations and ellipticities of each galaxy   for instance  how can we ensure that the nth element
of the feature vector will be meaningful across the different skies when     there is no obvious
ordering by which to rank the galaxies within a sky and     the number of galaxies per sky varies
from         so some galaxies will have to be omitted  discretizing the sky solves this problem
because nth element of the feature vector will always correspond to a specific region of the sky 
using a discretized sky we tried a few different logistic regression schemes 
     

signal response surfaces
gaussian mixture fit for signal
gaussian mixture fit for partitioned signal

 

x   
   

 

x   

 
   

   

   
   

 

   
 

   

    

 

    

    
    

    

    
    

    
    

    

    

    
    

    

    

    
    
ydim  pixels 

    

    

    

    

    
    

    

    

    
   

   

ydim  pixels 

xdim  pixels 

    

   

    
   
xdim  pixels 

figure    left  signal gaussian response surface for entire sky  right  normalized signal gaussian response surface for partitioned sky 
the proposed approach involves the use of response surfaces fit to regular signal maps and modified
signal maps that amplify any possible halo effect  these response surfaces are then used to create a
feature vector  since the skies could have     halos  multiple local maxima are possible  derivative
information can provide valuable insight in finding the coordinates of any local maxima  and thus
could be useful in capturing some halo effects  signal maps are not necessary smooth functions  but
 

fisignal contour and halo locations for sky    
    

    

ydim  pixels 

    

    

    

    

    
actual halo locations
estimated halo locations

   
   

    

    

    
    
xdim  pixels 

    

    

    

figure    left  shows the probability of finding a halo at a given coordinate  right  comparison of
estimated and actual halo locations
they have a tendency to have a behavior similar to that of a sum of gaussian distributions  therefore 
in order to get smooth derivative values  a response surface of the total signal  sum over all galaxies
in the sky  is created by using a mixture of gaussians 
to amplify the effects of non dominant sources of dark matter we divided the sky into    equal
partitions and locally calculated a signal map for each partition  the signal in each partition was
normalized such that the maximum signal value is the same in each of the different partition  this has
the effect of amplifying other local minima  see figure   on right   and thus increasing the chances
of recognizing the effects of weak dark matter sources  the number of divisions was selected to
ensure that a significant number of galaxies are in each partition  derivative information is desired
in this step as well  so a gaussian response surface is generated for the entire sky by using the locally
normalized signal values at each partition  in theory  by using locally normalized signal maps we
should be able to capture and amplify local tangential ellipticity effects due to the presence of dark
matter  in practice  however  by amplifying the signal we are also amplifying the noise  thus it is
expected that logistic regression will return some false positives 
to create the feature vector for logistic regression the sky is gridded into smaller cells    x    and
the properties of the signal in each cell become elements of the vector  the feature vector thus
contains values for the full signal map along with its derivatives and the locally amplified signal
map with its derivatives as well  all quantities are calculated at the center of the sky cells  the
response variable equaled   if the cells center is located within a   cell radius of a halo and zero
otherwise  thus a single source of dark matter will produce a  x  cell region in the sky that counts
as a positive classification 
we trained the algorithm with     skies  over   million cells   as expected  the signal from most of
the less dominant halos was amplified  but we underestimated the effects from noise amplification 
figure   shows a sky prediction where the noise was not dominant  after looking at several cases 
we determined that the noise amplification considerably affects the logistic regression by generating
a high number of false positives  overall  few skies had acceptable halo location predictions  but in
most cases the results suggests that this particular method is not appropriate to find halo locations 
other variations for this method were implemented and the results were unsatisfactory 
     

sliding window

we gridded the sky into a   x   mesh of     boxes and for every box asked is this the peak of the
dark matter  construction of the feature vector was confounded by the discretization  things get
sparse since our data is already at discrete positions  and the size of our training data and feature
vector lengths are time constrained by newtons method 
within the gridded sky  we construct a feature vector by looking at  x  groups of boxes and generate
many such feature vectors per sky by sweeping the  x  window across the sky  the signal in each
sub box was used as an element in the feature vector and the associated response variable equaled  
if the dark matter peak is contained within the center box  zero otherwise 
 

fi d histogram of galaxy locations for sky    

logistic sliding window predictions for sky    

 

x   
 
 

   

 

 
   

 

 
 
 

 
  

  

  

  

  
  
  

  
  

  

  

  
ybin

  

  

  
  

  

 

ybin

xbin

  
 
xbin

figure    left  signal map of discretized sky where floating red dots indicate the locations of dark
matter that we are trying to predict  right   d histogram of probabilities for dark matter locations
calculated from logistic sliding window 
figure   shows the predicted probabilities for dark matter locations in a sky that was not trained on 
notice that there is a lot of noise which makes it difficult to make a confident prediction  also notice
that we only seem to be locating the dominant source  the minor source does not even appear to be
visible in the signal map on the left  in general  training on skies that only have a single source of
dark matter seems to give better results for multiple halo testing  as opposed to training on multiplehalo skies  this is likely due to a more straightforward relationship between signal and dark matter
location in single halo skies 
   

partial least squares regression  pls 

pls regression is used to find the fundamental relations between two matrices by investigating the
covariance structures in these two spaces  specifically  a pls model looks for the multidimensional
direction in the feature space that explains the maximum variance direction in the response space 
pls regression is generally useful when you have more features than observations  which seems to
be our case since we have only     training skies  this is the only method that we have seen which
allows a regression on multiple dependent variables  i e  y  rn  
pls using only values of full signal map
   

percent variance explained in y

  
  
  
  
  
  
  
  

 

  

  

  

  
  
  
  
number of pls components

  

  

   

figure    typical plot showing number of pls
components required to capture the variance in the
response space 

three types of feature vectors were considered 
type    we construct a feature vector using
only the full signal evaluated at the discretized
sky grid points  as was previously seen in the
logistic regression approach  type    we construct a feature vector using the signal in each
grid cell as calculated from the nearest n  
          galaxies  matlab notation  as well as
the signal calculated from all the galaxies in the
sky  type    we use the full signal from all
galaxies as well as the average ellipticity  average orientation angle  and number of galaxies
in each grid cell  for all types of feature vector  we used a  x  vector response variable that
contained the grid locations for the dark matter
sources  sorted by signal strength at their location  this ensures that the first two elements
of the response vector always correspond to the
strongest dark matter source and so on 

unfortunately  this method did not yield good
results  training on     of the skies and testing on the remaining     no feature vector scheme
worked well with all of them performing similarly poorly  type   does a decent job at locating dark
matter in skies with a single source  but for the case of multiple sources it does poorly  the rationale
 

fibehind using local calculations of the signal in type   was that the perturbing influence of dark
matter falls off with distance from the source so the galaxies closest to the source should feel the
largest lensing effect  while this is conceptually true  in reality the noise associated with calculating
the signal due to a small number of galaxies can easily overwhelm the signal due to a non dominant
source of dark matter  the hope was that by using the values calculated for many values of n  the
learning algorithm could use these multiple snapshots to discern some patterns from the noise 
type   suffers from the problem that as your
grid gets finer  you find that most of your galaxy
features are zero  for instance  if you discretize
a sky containing     galaxies into a   x   grid 
you will find that at least     of your grid cells
contain zero galaxies  doing this for     skies
 a very small number compared to the feature
vector length  produces a training set where the
feature vectors themselves are very high dimensional and linearly independent  doing a pls
regression in this case only gives a relatively
small number of components       needed to
capture all of the variance in the output  when
looking at the results for the data we trained on 
you can see that we are spot on for every dark
matter source  but if you test on untrained data 
the guesses are wild  we have severely overfit
the data 
   

conclusions

figure    example of a sky where pls regression
predicts locations that are obviously misplaced 
red dots are true locations  blue dots are predicted
locations 

in addition to the methods outlined above  we
also explored the use of neural networks and
spent a few days attempting to model the effects of distributed sources of dark matter and their
weak lensing effects  these two methods failed to produce results and due to space limitations for
this final writeup  are not discussed in any detail  all of the methods that we attempted failed to
adequately predict the locations of dark matter in skies that had more than one source  however our
solutions performed better than randomly guessing the dark matter locations  the specific failures
of each method are outlined in their respective sections above 
by creating many random skies containing     galaxies each  with no dark matter present  and
looking at their signal maps we noticed that the background noise was generally less than a factor of
two less than the peak signal found from our training skies  thus  with background noise so large 
using signal strength will never be enough to properly locate minor sources of dark matter 
through the course of this project  we became convinced that while some machine learning techniques may be helpful in solving this problem  the most appropriate way to solve this problem is to
develop a physical model that connects a distribution of dark matter to its distorting effect on the
background galaxies and then fit that model to the available data  with this model in hand and a
testing sky  the inverse problem can be approximately solved  i e  the distribution of the dark matter
density should be optimized such that by undoing the effect of weak lensing on the testing skies  the
background galaxies are seen to be distributed uniformly  this problem then becomes a matter of
developing an appropriate model for weak gravitational lensing 

references
    herve abdi partial least square regression  pls regression
    bartelmann  matthias  and peter schneider  weak gravitational lensing  physics reports      
                
    all data and competition background provided by http   www kaggle com c darkworlds 

 

fi