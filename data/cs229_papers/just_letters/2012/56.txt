non homogeneous swarms vs  mdps
a comparison of path finding under uncertainty
michael comstock
december        

 

introduction

the left side of the plane and a goal location near
the right  between the two locations is small barrier which the algorithms must learn to navigate
around 
each learning system will navigate this space
with no apriori knowledge of the location of obstacles or rewards in this world 

this paper presents a comparison of two different
machine learning systems tasked with creating a
path to and from a goal point in a simulated
world 
the first system which is tested in the simulated world is a very simple markov decision
process  mdp  which learns by sampling the
world  estimating transition probabilities and
recording rewards at each state  after estimating these transition probabilities and recording
the rewards  learning is completed by performing value iteration 
the second system which will be implemented
and tested is a modified non homogeneous
swarm algorithm which learns again by a process similar to pheromone trail creation in ant
colony optimization algorithms 

 

 

the mdp algorithm

the first learning algorithm implemented to
solve this problem was a simple mdp  this
choice algorithm was selected because it provides
a very natural framework for dealing with the
unknowns of this problem  additionally this algorithm is widely used in reinforcement learning
problems in many fields and thus serves as a reasonable basis of comparison 
the mdp handles the fact that obstacles are
not known apriori by having transition probabilities between states  this means that if there
is an obstacle between state s and state s  the
mdp may learn this by estimating p  s  a  s       
where a is the action go to s    additionally the
mdp can be easily extended to incorporate not
rigid barriers by setting p  s  a  s      p         
rewards are in this model are also learned and
set to be   for any state which is not a goal

world description

learning systems in this paper navigate through
a two dimensional grid world  which should be
viewed as a continuous space in r    in this case i
examine the space                     this choice
was made as it goes well with rendering onscreen 
the world will have a starting location near
 

fi   

state and   if is a goal state  the learning algorithm is not told if s is a goal state until it
occupies state s  thus learning is done in an
online fashion 
a more detailed description of the mdp algorithm is given in the appendix 

 

     

analysis
ram usage

the mdp needs to store the transition probabilities between each state for each action  thus
it needs     states    actions    state  bytes
of data are needed assuming each probability is
stored as a    bit float  for a        discretiztion of the world  this takes about   gigabyte of
space 
by contrast the swarm algorithm does not
store any world states  it stores only the state
of each member of the swarm  this takes   
megabytes of space  since each agent stores only
a limited number of values  see appendix for details on what each swarm agent stores  

the swarm algorithm

the second learning algorithm implemented to
solve this problem is a swarm algorithm with
non homogeneous agents  this algorithms creation was inspired by nature   particularly ants
  ability to solve this exact problem on a regular
basis 
this algorithm learns by having agents wander around randomly until they hit a goal state 
agents then communicate to each other how long
it took them to reach the goal state  agents that
have visited a goal state recently will stop along
their path to the goal in order to act as markers
leading other agents to the goal  by repeatedly
traveling to and from a goal state the swarm is
able to rapidly build a  near shortest  path from
the start state to the goal state  a  much  more
detailed description of this algorithm is given in
the appendix 

     

elapsed time

the elapsed time metric indicates how much
time seconds elapsed before the algorithm converged on a solution  note that the metric is
seconds rather than clock cycles as neither the
mdp nor the swarm was written in a manner
which optimized for reducing cpu cycles  this
metric is only meant to provide a very rough
comparison in the amount of time required for
each algorithm to solve the problem 
by this metric the swarm once again greatly
out performed the mdp  this is due to a combination of multithreading the number of compu  results
tations required for each algorithm  the mdp
a brief overview of results is given in the table performs value iteration after several samplings
of the transition probabilities  value iteration
below
mdp
swarm
requires many iterations  up to thousands  over
each state for the values of each state to conram usage    gb
    mb
verge  given there are thousands of states per
elapsed
    s
  s
iteration  this computation can take several sectime
onds  by contrast the swarm algorithm explores
solution
jagged 
smooth
states and assigns accurate values to each state
quality
grid like
 

fi 

simultaneously  see appendix for more detail  
     

concluding remarks

this document presents two very different algorithms for solving a path finding problem with
no a priori knowledge of the environment in
which each algorithm operates  while much further optimization is needed for both algorithms 
the above results indicate the non homogenous
swarm algorithm is a potential improvement
over mdps for navigating through continuous
spaces 
please see the appendix for a detailed description of the novel way in which the swarm algorithm builds solution path 

solution quality

the solution quality metric is a purely subjective metric which compares what each solution
looks like  this is useful for visualizing how aesthetically pleasing each algorithm would be for
controlling a game ai or what the expected behavior of a robot controlled by each algorithm
would look like 

figure    swarm algorithm demonstrates a
smooth path after converging on a solution 

figure    the mdp demonstrates a jagged path
after converging on a solution 

 

fia

mdp definition

x
v

the mdp learning system implemented in this
paper had the stanford form      each action was
produced from the following equation  given a
state s the action  a  taken at s is

t
d
g

a   argmaxa

x

p  s  a  s   v  s   

s 

m

where v  s    is the value of being in s   

b

the value v  s  is computed by value iteration

v  s    r s    max
a

x

the position of the agent in the
problem space 
the velocity of the agent in the
problem space 
the time since the agent has passed
through its goal location 
the distance over which the agent
may communicate 
the goal  either base or resource 
which the agent is trying to reach 
a true false bit indicating if the
agent is in the path marker state    
or not     
a true false bit indicating if the
agent is in the border marker state
    or not     

examining the m and b fields  one sees
the agents may exists in up to four states 
                                interally however we
define the state        as unreachable and thus
the agent may exist in only   states 

p  s  a  s   v  s   

s 

the values for p  s  a  s    are computed by sam  b   agent states
pling the world while executing the current policy which chooses actions randomly in its first an agent which is in state        is in the default state  in this state the agent moves through
iteration 
the problem space seeking its goal  either the resource or the base   henceforth i will refer to
this state as the active state 
an agent which is in the state        serves as
a marker for other agents  this marker directs
b swarm agent definition
the agents in state        towards the goal which
the agent in state        has previously visited  i
each agent in this system should be treated as will refer to this state as the marker state 
an agent which is in state        serves as a
a   tuple with the following fields 
border marker for other agents  these markers
direct agents in state        away from obstacles
as well as limit the distance over which agents
 x  v  t  d  g  m  b 
can communicate  i will refer to this state as
 

fiwhen an agent is in the marker state it does
the bumper state 
agents in active state move through the prob  not update its value for t 
lem space  the agents in bumper and marker
agents in the marker state transmit three
states form the path structure in the problem
pieces of information to other agents within their
space 
neighbor distance d  the first piece of information is the value of t which they have stored  the
second piece of information is their location x in
b     the bumper state
the problem space  the third piece of informathe bumper state is the simplest state of the tion which the marker transmits is which goal
three states  agents transition into the bumper   g   the agent was seeking before it became a
state whenever the collide with an obstacle in marker 
the problem space  agents transition out of the
one crucial task the marker agents carry out is
bumper state after a set period of time if no other
setting
the proper neighbor distance d  this disagents are communicating with them 
once in the bumper state agents limit their tance is set in the following manner  the marker
communication distance to a small radius around agent sets its communication distance d to be
the minimum distance from the marker location
them 
to a bumper  setting a distance larger than
agents in this state communicate two pieces
this would cause the marker to pull agents in
of data to other agents  the first piece of data
the active state into barriers  setting a distance
the bumper agent communicates is that it is a
smaller than this would be inefficient  see figures
bumper agent  it has hit an obstacle   the secbelow for examples of how setting the neighbor
ond piece of information the bumper agent comdistances for markers affects the systems behavmunicates is its location 
ior 
b    

the marker state

agents in the marker state form the main path
structure of the system  agents transition into
the marker state whenever they are have the
smallest value for t within half the communication distance of the nearest marker  if there are
no other markers nearby  within communication
distance  then an agent will transition into the
marker state 
agents transitions out of the marker state after a set period of time  once an agent has figure    markers with properly set distances d
transitioned out of the marker state  it may not guide active agents around a barrier
transition back into the marker state until it has
passed through a goal point 
 

fitance 
let c    c    c  be weights and r  and r  be random values in the range        in this implementation c          c          c          this
implementation is a slight modification of the
implementation presented by eberhart  shi and
kennedy    
d   nearestm arker d 
t   t    
 x  xb   if xb is defined
figure    markers with distances d set too large v  
c   v   c   r    xm  x    c   r    xa  x 
guide active agents into the barrier
x   x v
b    

the active state

c

agents in the active state move about the problem space alternating seeking the base and resource locations  the motion of the agents in
this state is entirely governed by communication
with other agents 
agents in the active state set their neighbor
distance d to be equal to that of the marker nearest to them 
during each iteration  the agents in the active
state update their states fields as follows 
let xm be the location of the best marker
within communication distance of the agent 
here the best agent is defined as the marker
within the markers communication distance
who most recently visited the goal which the
agent is trying to reach  marker gm    g   in
other words the best marker is the marker with
the lowest value for t 
let xa be the location of the best active agent
within communication distance of the agent 
the best agent is defined as the agent with
ga    g and minimum t 
let xb be the location of the nearest bumper
agent within the bumpers communication dis 

swarm behavior

the swarm as a whole undergoes two distinct
phases in this problem  in the first phase agents
attempt to find the unknown goal location for
the first time  in the second phase  agents build
a path to and from this goal 

c  

finding the goal

given each agent has cannot observe the environment and  by definition  no agent has found
the goal in this phase  agents move about the
problem space randomly during this phase 
agents start this phase by scattering randomly
from the base location  they proceed to bump
into obstacles  transition into the bumper state
and direct other agents away from these barriers  agents also transition into marker agents
whenever they are the first agent to enter a new
location  or whenever they reach a location in the
shortest amount of time t since they last visited
the base location  thus the agents build a map
of the environment as they search for the goal 
this map  though is better though of as bread
 

ficrumbs as it is overlayed onto the environment
rather than kept in some internal state 
after some time  whose probability depends
on the number of agents  distance to the goal
and number and distribution of obstacles  an
agent will randomly find the goal location and
the swarm will transition into the second phase 

the goal is  at that time  the only agent to hit
the resource goal  it will immediately turn into
a marker agent  once in this state the agent
transmits its location to all other agents within
its neighbor distance d  this will cause all agents
within this distance to no longer travel randomly 
but rather head toward the marker agent  and
thus head toward the goal 
as more and more agents reach the goal  they
will leave the goal and head back towards the
base location by following the marker agents
which came from the base  note that individual agents do not have a memory of where they
have gone so they cannot simply reverse back
to the base 
the process of transitioning from the first to
the second state can be seen in figure   
examining figure   we see several notable features  first we note that near the goal  red  
swarm agents are beginning to demonstrate some
organization  the red agents are now moving together since each one is drawn to the
same marker agents on the bottom right  note 
marker agents are not shown in this figure 
additionally we see the agents located to the
left of the barrier are still moving about randomly  this is expected behavior as no agent
will be able to communicate the location of the
goal beyond the barrier if there are any bumper
agents located along the barrier 
over time the agents follow the markers back
to the base  as they do so they too transition
into marker agents and guide agents from the
base around the barrier to the resource  thus
the path is first build from the base to the resource and then from the resource to the base 
a more detailed look at this process  especially
the role of the marker agents is presented in the
next section 

figure    agents scatter initially upon leaving
the base for the first time 

figure    agents search randomly for the goal 

c  

building a path

once an agent reaches the goal state the swarm
transitions into a new phase where it builds a
path to the goal  because the first agent to hit
 

figiven marker agent 
given this construct  we see a policy is generated by the active agents dynamics  it should be
noted that this policy is generated greedily and
is not necessarily optimal 
an illustration of the marker dynamics is given
in the following figures  in these figures each
state is shown as a hollow circle  note that states
may over lap  when an agent is in a location
figure    agents have recently discovered the with overlapping states  then that agent is said
to be in whichever state s which has the greater
goal for the first time 
value 

d

marker agents

note that because agents transition out of the
marker state periodically  there is redundancy
the preceding section examined the behavior of built into the discretization  this helps prevent
the swarm from with focus on the active agents  information loss when agents transition out of
what is of far greater importance to this system the marker state 
however are the marker agents  thus this section
will take a more detailed look at how marker
agents behave 
a useful way to think about the marker agents
is to imagine the the above problem as a markov
decision process  mdp   mdps are very useful in discrete spaces but become intractable in
continuous spaces  one way of applying mdps
to continuous spaces is discretization  choosing
the proper way to discretize a space however can
often prove difficult 
the role of the marker agents in this system
is to automatically and simultaneously discretize figure    agents initially scatter  during the
a space and assign a value to this new discrete scattering process may agents hit obstacles and
space 
transition into the bumper state  this abunin this context each discrete state s is a lo  dance of bumpers limits the marker agents to
cation x which is given by the agents location a relatively small radius  as no goal has been
and a radius r which is given by the neighbor found states to not demonstrate any structure
distance d  thus an agent a is in state s if and appear randomly selected 
  xa  xs     ds   the value of this state is simply t where t is the value t associated with the
 

fifigure    the goal has now been reached  we figure     more agents converge onto the path 
see the presence of two different kinds of states  previously created states are now removed as
the blue states provide insight into the problem they go unused and become unnecessary 
of finding the base  the red states provide insight into the problem of finding the resources 
note that this is a picture of a proposed discretization immediately after the goal has been
found  not enough time has elapsed to allow for
optimizing the discretization  also note there
are some areas still which are not covered by
the discretization  these areas should viewed
as states of unknown value 

figure     the swarm creates a minimal discretization of the problem space  in this stage
all unnecessary segmentations are removed  one
should note that states are as large as possible without going beyond the barrier  additionally it should be noted that each state become
smaller as it gets closer to the barrier  this allows the active agents to more precisely navigate
near to the obstacles edge  the discretization
aligns with what one intuitely thinks of as the
states of this system  there is a state where you
are to the left of the obstacle  a state where you
figure     a path now exists both to and from are to the right of the obstacle and a state where
the base  discretization remains unoptimized 
you are going around the obstacle 
 

fireferences
    stuart russell   peter norvig  artificial intelligence   a modern approach  prentice
hall   rd edition       
    russell eberhart  yuhui shi   james
kennedy  swarm intelligence  morgan kaufman   st edition       

  

fi