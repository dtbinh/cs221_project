cs     action recognition in tennis
aman sikka
stanford university
stanford  ca       

rajbir kataria
stanford university
stanford  ca       

asikka stanford edu

rkataria stanford edu

   motivation
as active tennis players we are always trying to
improve our game by observing the
professionals  by observing their forehand 
backhand and serve techniques  one can easily
bring changes and perfect their own form 
unfortunately videos are not annotated with
info stating the type of action the player is
performing  our goal for this project is to
recognize the action a player is performing  so
searching for such an action is plausible 
   introduction
this report discusses the findings of our project 
action recognition in tennis  it discusses
our dataset  key point detector and
descriptor  the experiments we performed to
determine the sampling mechanism and our
overall results 
   dataset
our dataset for this project consisted of video
clips from   match videos  we collected
    videos from the first match  and    
videos each from the second and third matches
and limited the videos to right handed male
tennis players  due to great variation of
camera angles  we only looked at clips shot
from a birds eye view as shown in figure   

of interest to us  i e  backhands  forehands  and
serves  a video annotation tool  vatic  video
annotation tool from irvine  california  was
used to create bounding boxes around the
tennis players that were performing the action
of interest  we manually drew bounding boxes
around the tennis players for each of the clips 
figure   below shows the bounding box we
drew on each frame 

figure    bounding box around player of interest

extracting the region of interest enclosed
within the bounding box removed most of the
background and ensured that we werent
capturing any other actions  i e  actions of the
umpire 
figure   below shows our final processed data
 in the form of video clips  that was fed into
our algorithm to perform action recognition 

figure    extracted region of interest

figure    backhand  forehand and serve

since majority of the tennis match is shot from
the birds eye view angle  it provided us with
sufficient data for our learning algorithm  we
manually noted the start and end of action
frames and extracted the clips from the match 
this resulted in isolating the actions that were

due to limited amount of time  our pipeline
initially used one dataset  we tried to
generalize it with more datasets near the end of
the project  but realized our third dataset had
erroneous data  as shown in figure    quite a
few clips contained part of the score  due to
this we required a larger dataset so to avoid the
score from becoming an important key point 

fiall our analysis excluded the third dataset 

orientations   m     and n      giving a total of
    dimensions  these numbers were used for
our testing as well  the figure below gives a
quick overview of the descriptor computation 

figure    example of noisy dataset

   algorithm
the first step in our algorithm involved
detecting and describing important key points
in a clip  we used dense sampling as keypoint detector and hog d as a descriptor 
   
dense sampling
dense sampling extracts video blocks at
regular positions and scales in space and time 
there are   dimensions to sample from   x  y  t 
     where  and  are the spatial and
temporal scale  respectively  in this project  the
minimum size of a  d patch is    pixels and
  frames  spatial and temporal sampling is
done with     overlap  these numbers
correspond to the one resulting best result on
ucf   dataset 

figure    hog d overview

once the points are described  a dictionary of
visual code words is created using the k means
clustering algorithm 
   
k means
k means is a method of cluster analysis that
aims to partition n observations
into k clusters in which each observation
belongs to the cluster with the nearest mean 
the k means algorithm is described in the
figure   below 

once the key points are selected  they must be
described  the standard describer  or rather
descriptor  for videos is hog d 
   
hog d descriptor
for a given spatio temporal block  hog d
splits it into m x m x n sub regions  m for
spatial and n for temporal splits   within each
region  this method counts how many  d
gradients are roughly aligned with each of the
directions of a polyhedral structure  this
computation is done efficiently using basic
geometric operations  in     the authors claim
that  for bow  the best performance in the
validation set of the ucf   dataset was
obtained with an icosahedron  i e     

figure    k means clustering algorithm

through research       we determined the
optimal number of clusters to be between
           we experimented with a range of
values between            but they didnt
seem to have much if any effect on our results 
if the number of clusters picked is too small 
then the visual words in our dictionary may not
be representative of all the patches  however
if the number of clusters is too large  then we
could be over fitting  since the number of

ficlusters didnt seem to have any effect  we
picked the number to be      in decrease
computation time 
once our dictionary was created  we employed
the bag of words model to generate a
histogram representation of our videos 
   
bag of words
the bag of words model  in the field of
computer vision  can be applied to image
classification  by treating image features as
words  bow finds its roots in document
classification  where bow is a sparse vector of
occurrence counts of words  that is  a sparse
histogram over the vocabulary  in computer
vision  a bag of visual words is a sparse vector
of occurrence counts of a vocabulary of local
image features     
the code words obtained after k means a
mentioned in section     provided us with the
vocabulary of local video features  figure  
summarizes our entire process including bow
approach when applied to images and is similar
to video data 

   
classification
after generating the histograms for each action
clip we were ready to classify the data as
forehand  backhand or serve  for
classification we focused on support vector
machines  svms   based on our research
         svms are the most commonly used
classifier used in recognition based tasks  our
data could fall into one of three classes  either a
forehand  backhand  or serve  so we tried a  vs all approach  this was somewhat effective 
but didnt give us information on the
misclassifications and required three different
models  the models required one of the
actions as positive data  and the remaining
actions as negative data  once we discovered
a multi class classifier  we abandoned the   vsall approach in favor of the multi class 
we explore two kernels  linear and rbf in our
svm model
the radial basis function  rbf  is usually
represented by a gaussian function  as a result
it maps the data to an infinite dimensional
space  the function depends on the euclidean
distance between x and z where either x or z is
the support vector  and the other is the test
point  it is controlled by a the  parameter  a
larger value of  indicates that the support
vector has strong influence over a larger area 
and hence gives a smoother decision boundary 
the figure below gives an equation of the
radial basis function kernel      

figure    equation of the rbf kernel

figure    illustration of bag of words model     

   results
the linear kernel was used as a baseline and
worked quite well  but the rbf kernel
outperformed the linear kernel for both of our

fidatasets  figure   and    show how accuracy is
affected with the change of cost and gamma
parameters for linear and rbf kernel
respectively 

figure    accuracy comparison between linear and rbf
kernel for dataset  

figure     accuracy comparison between linear and rbf
kernel for dataset  

for both of our datasets  the best result given
by the rbf kernel is at least     better than
the linear kernel  the confusion matrix shown
in table   outlines the test classification errors
for each class for dataset   

actual

predicted
forehand backhand serve
forehand
      
  
      
backhand
  
    
  
serve
  
  
    
table    confusion matrix using rbf kernel  dataset   

we performed a         split between
training and testing examples  and each action
used the same number of examples for training 
   observations and conclusions
the process of collecting the dataset
 identifying the frames containing the action 
annotating the bounding box and generating a
dictionary  was very a time consuming task
due to which we were limited to the amount of
data we could collect  we collected     videos
for the first dataset  and     videos each for the
second and third datasets  some of our
interesting observations are noted below 

   actions learnt on player a from one set of
match videos do not generalize well for the
same player in a different match  this is
probably due to slight variations in camera
angle  the accuracy dropped from over    
to      this implies that multiple clips
from different matches are required to
better generalize the actions 
   the accuracy was     when trained on
men matches and tested on woman  i would
suspect this to be the case due to the fact
that woman play at a slower pace 
   one of the training sets had large
illumination variation  since the match was
shot on a cloudy day  there were strong
shadows casted throughout the game  for
such situation it makes sense to do some
kind of background removal 
   since both the hog d descriptor and the
bow representations are based on
histograms  one of the most appropriate
kernels is the rbf  due to a high accuracy
of our results  the rbf kernel did not make
any difference  on testing on the minimal
data available from the other two matches 
rbf did seem to perform around      
better than linear kernel  more data would
be required to confirm the results 
   references
    video annotation tool from irvine california  vatic 
http   mit edu vondrick vatic 
    bag of words
http   en wikipedia org wiki bag of words model in co
mputer vision
    a klaser  m  marszaek  and c  schmid  a spatiotemporal descriptor based on  d gradients  in british
machine vision conference  pages          sep      
    l  gorelick  m  blank  e  shechtman  m  irani  and r 
basri  actions as space time shapes  ieee transactions
on pattern analysis and machine intelligence 
                 december     
    i  laptev and t  lindeberg  space time interest points  in
iccv       

fi    d  g  lowe  distinctive image features from scaleinvariant keypoints  int journal of computer vision 
january     
    n  dalal and b  triggs  histograms of oriented gradients
for human detection  in proc ieee conf on computer
vision and pattern recognition  san diego ca  june            
    h  uemura  s  ishikawa  and k  mikolajczyk  feature
tracking and motion compensation for action recognition 
in british machine vision conference       
    p  scovanner  s  ali  and m  shah  a   dimensional sift
descriptor and its application to action recognition  in
proc of the acm multimedia  augsburg  germany 
september           
     h  wang  m  m  ullah  a  klaser  i  laptev  and c 
schmid  evaluation of local spatio temporal features for
action recognition  in proc   th british machine vision
conf  london sept            
     a  a  efros  a  c  berg  g  mori  and j  malik 
recognizing action at a distance  in proc  th int conf on

computer vision  nice  france  oct        pages    
          
     m  everingham  l  van gool  c  k  i  williams  j  winn 
and a  zisserman  the pascal visual object classes
challenge  voc  results 
http   www pascalnetwork org challenges voc voc     
workshop        
     c  harris and m j  stephens  a combined corner and edge
detector  in alvey vision conference       
     g  willems  t  tuytelaars  and l  van gool  an efcient
dense and scale invariant spatio temporal interest point
detector  in eccv      
     k means clustering  http   en wikipedia org wiki kmeans clustering
     using a radial basis function as a kernel  http   svrwww eng cam ac uk  kkc   thesis main node   html
     image and vision computing 
http   www sciencedirect com science article pii s      
          

fi