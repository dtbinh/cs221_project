a novel system for hand gesture recognition
matthew s  vitelli
mvitelli stanford edu

dominic r  becker
drbecker stanford edu

abstract  the purpose of this project is to create a real time

dynamic hand gesture recognition system from front to back 
users interact with the system by wearing a special glove 
motions from the user are interpreted by our application
running on standard computer hardware with a commodity
webcam  these motions are analyzed using computer vision
and machine learning  in particular hidden markov models 
in order to determine which gesture is being made  over time 
the user may train the system to adapt to and learn new
gestures 

development will allow our system to perform gesture
recognition on any input device that supports the standard data
format 
in this project we create a modular system in which a
custom made input device recognizes the location of
fingertips  outputs the data into a standard text file and a
separate system reads the data in real time and performs
gesture recognition  our system is highly modular so that
gesture recognition can be performed using any input device
that recognizes fingertips and output the data in a known
format 

i  prior works
hand gesture recognition has received a great deal of
attention in recent years  due to its many potential
applications to mobile technology  gaming systems  and realtime imaging technologies  it has become an area of increased
interest 
hand gesture recognition has been explored by many
researchers using a variety of methods  visions of minority
report like computer interaction are becoming somewhat
feasible  mistry et al  present a wearable projector and camera
setup that recognizes hand gestures acting on the projected
images      google glass promises similarly futuristic gestureaugmented reality interaction 
other explorations include using the microsoft kinect 
which has a built in stereoscopic sensor  ren et al  recognize
static hand gestures using a modified earth mover s distance
metric      biswas and basu recognize upper body gestures
using kinect depth data and svms     
as early as       yang and xu used hidden markov
models  hmms  to recognize gestures drawn with a mouse on
a computer      in       starner and pentland built an hmmdriven system to recognize american sign language     
keskin et al  created a  d gesture recognition system that also
uses hmms 

iii  method  vision
one major system in our project is the custom built input
device which draws together technology from different fields
such as computer vision and basic circuitry 
a  the glove
users interact with our system using a custom made glove 
the glove is fitted with   different led bulbs  each with a
unique color  since each brightly colored led corresponds to
a unique finger  the process of recognizing fingertips is
simplified down to extracting brightly colored blobs from an
input image 
b  computer vision
the users fingertips must be correctly identified in order to
accurately track their gestures  to accomplish this  the image
captured by the webcam must be properly processed to
identify the position of the users fingertips  as well as
categorize each finger  the vision process can be broken down
into several stages as follows 
  

threshold pass  the image is thresholded to extract
the brightest pixels  the benefits of this process are
that most of the background is eliminated and most
the brightest pixels are likely candidates for the leds
of the glove 

  

convolve pass  the image is then convoluted using
a special kernel that favors brightly colored pixels
over white light  since most of the leds appear
oversaturated in the camera image  this pass is useful
for approximating the true colors of the individual
leds 

  

downsample pass  the image is then downsampled
to a low resolution for later use during centroid
estimation 

ii  purpose
many proprietary computer vision systems that can detect
the location of a hand exist in the market today  these
technologies  such as microsofts kinect or leap motions
the leap  can be used as an input device for a gesture
recognition system  however  these devices can be quite
costly  our goal is to make a gesture recognition system that
can take data from any device and perform gesture
recognition  currently  there is no standard data format for
gesture recognition devices  however  we hope that proprietary
computer vision systems will eventually adopt one  this

thinsit  laza  upatising
lazau stanford edu

fi  

dilation pass  the image is dilated to increase the
size of each region and provide better centroid
estimates 

  

centroid estimation  the centroids of each blob in
the image must be computed to accurately measure
the position of each fingertip  to perform this task 
we used a recursive flood fill algorithm  essentially 
the algorithm scans through each pixel in the image
and finds all pixels connected to the current pixel 
because the algorithm needs to be performed at every
frame  we use a downsampled image to reduce the
number of computations necessary  using this
approach  we can easily compute the centroids and
get accurate position measurements 

to increase the performance of our vision system  we
parallelized steps     to run entirely on the gpu using
programmable shaders  the systems capture pipeline utilizes
directx to communicate with the gpu and perform data
processing 

the input frame

proved to be cumbersome  as we wanted our gestures to be
invariant to time 
in an attempt to overcome this  we normalize each finger s
velocity vector in order to compute the raw direction 
however  informally  this does not seem to improve
recognition of gestures that are made more quickly  the
reason for this seems to be the sample rate of the data  if the
gesture is made too quickly  only a few frames are captured by
the cameraand these may not include important frames in
the middle of the gesture  which make the gesture less
recognizable 
b  quantizing feature data
before feeding the features into the hidden markov model 
each frame s feature datathe normalized x and y velocities
for each fingeris quantized using a codebook generated by a
clustering algorithm  this is primarily done to group similar
features across frames together  thus reducing the size of the
dataset   as well as to discretize the feature space for later use
in the hidden markov models 

threshold pass

clustering

dilation

convolve pass

downsample

iv  method  learning algorithm
based on the literature  it seemed that hidden markov
models would appropriately model the four fingered hand
gestures that we hoped to recognize  given the input data  x y
coordinates per finger over time  it made sense for our feature
extraction to follow a similar pipeline to that in yang and xu
     as such  the feature data is quantized using a clustering
algorithm before it is fed into the hmm 
a  feature selection
we experimented with a variety of different feature models
and representations of the feature space  our first approach
incorporated velocity data from each fingertip  however this

in particular  we implemented the lbg algorithm  due to
linde  buzo  and gray  to perform the clustering  yang and
xu employ this clustering algorithm to        accuracy with
    samples of training data for mouse gesture recognition
    
using the codebook  each input feature per frame is
classified into a given cluster  and the observation sequence is
transformed to a sequence of the clusters corresponding to the
nearest centroid in the generated codebook to each frame s
feature vector  again  in order to recognize a gesture  the
frame features are quantized using this lbg generated
codebook 

fic  hidden markov models
hidden markov models are used to predict which gesture
the user is currently performing  one model is generated for
each gesture  the hmms are trained by taking a collection of
the codebook discretized sequences  used as the actions of the
hidden markov model  corresponding to each raw training
sample  the hmms are trained using the baum welch reestimation algorithm either until convergence or to a
maximum of     iterations  for the sake of timely model
generation   this training is done offline as it cannot be
completed in an acceptable amount of time for an end user to
interact with directly  i e  on the order of hours  
once the models are built  on the other hand  recognition is
performed in real time  during recognition  the users current
input gesture is first quantized using the process described
above  next  the viterbi algorithm computes the likelihood of
the quantized observation sequence given each model 
selecting the model that maximizes the likelihood  our
application is able to guess which gesture the user is
performing 

a  number of clusters
figure   shows the average accuracy over eight gestures of a
four state hidden markov model trained over a varying
number of clusters  it is apparent from the image that
increasing the number of states can actually detract from the
hidden markov models performance  figure   shows the
normalized and unnormalized     clusters generated by our
algorithm on only four simple gestures  horizontal and vertical
gestures  see appendix   the figure shows that having too
many clusters will cause the algorithm to begin differentiating
between motions that are extremely similar  which is
undesirable  figure   is    clusters generated by all eight
gestures  we can see that lowering the number of clusters will
allow the algorithm to recognize principle motion directions
without causing similar gestures to be classified as different
clusters 

v  results and analysis
we tested our system under a number of different
parameters  including various numbers of clusters and markov
transitions  we also performed diagnostic tests with
normalized and unnormalized feature data  due to the fact that
computing hidden markov models is a time consuming
process  we were only able to capture a limited number of
varying transition states and cluster sizes  ultimately  we
settled on    unique clusters with   markov transition states 
we tested our results using hold out cross validation 
training on     of the data  the data consists of eight
gestures  each with around     training samples  for the final
presentation  we retrained the hidden markov models with all
of the available training data  and did not notice any
significant drop in accuracy 
figure  

figure  
figure  

fifigure  
c  improved feature selection
certain gestures are harder to recognize than others  with
only finger velocities as features  gestures like circles are
difficult to recognize  in many of the gestures that were
successfully recognized  the finger positions relative to one
another were constant  for other gestures though  say a snap
of the fingers  additional features like relative position may be
more valuable  another feature manipulation to explore is
normalization  better normalization may lead to improved
recognition regardless of temporal length of the gesture 
vii  conclusion

b  number of hidden markov model states
we can see from the figure   that the optimal number of
states in the hidden markov model is    we thought that
increasing the number of states in the hidden markov model
would allow the model to capture more states that represent
the users gesture  however  empirical data shows otherwise 
we postulate that this may be due to the limited number of
training samples that we obtained  a closer analysis of the
emission matrices for hidden markov models with more than
  states shows that many of the emission probabilities were
too low 
vi  future work
a  live recognition
having to click a start stop button to recognize an
individual gesture is inconvenient  in particular  using gesture
recognition as an input method would be infeasible if the user
needed to indicate the beginning and end of each gesture 
instead  it would be ideal for the system to automatically
determine when a gesture has been made  one way to do this
would be to identify gestures by applying some threshold to
the likelihoods generated by the viterbi algorithm  while the
basic idea would be to run the viterbi computations at some
per frame interval  issues may arise such as what data to
include  last    frames  last   seconds  etc   
b  more flexible input data
our current training and recognition system accounts for
exactly four fingers  if a finger is hidden during data capture
 or another is added   the data captured becomes very erratic 
it would be ideal to simply remove such data before feeding it
into the model  however  with such different data sets  there
would have to be more data  perhaps encapsulated in different
markov models  with without those corresponding features  a
system that handled fewer or more fingers could be much
more flexible in terms of practical usability 

we successfully prototyped a front to end gesture
recognition system using hidden markov models and a
custom built input device  the system is highly accurate for
the majority of the gestures in our database  while we
successfully prototyped a flexible system for hand gestures 
this project just scratches the surface of what is possible 
given more time  we would like to increase the complexity of
our gestures  as well as the number of gestures used in our
system  additionally  we would like to parallelize more of our
codebase to accelerate the process of training the clusters and
hidden markov models 
viii  acknowledgement
we gratefully acknowledge professor andrew ng for
valuable feedback on our project and the excellent lecture
notes on hidden markov models 
ix  references
   

pavlovic v   dynamic bayesian networks for information fusion with
applications to humancomputer interfaces  dept  of ece  university
of illinois at urbana champaign  ph d  dissertation        

   

stenger  b   model based hand tracking using a hieradynamic time
warping

   

blob recognitionrchical bayesian filter        

   

ren  zhou  junsong yuan  and zhengyou zhang   robust hand gesture
recognition based on finger earth mover s distance with a commodity
depth camera   proceedings of the   th acm international conference
on multimedia  acm       

   

biswas  k  k   and saurav kumar basu   gesture recognition using
microsoft kinect   automation  robotics and applications  icara  
      th international conference on  ieee       

   

yang  jie  and yangsheng xu  hidden markov model for gesture
recognition  no  cmu ri tr        carnegie mellon univ
pittsburgh pa robotics inst       

   

starner  thad  and alex pentland   real time american sign language
recognition from video using hidden markov models   computer vision 
      proceedings   international symposium on  ieee       

   

keskin  c   a  erkan  and l  akarun   real time hand tracking and  d
gesture recognition for interactive interfaces using hmm  
icann iconipp                    

fi   

mistry  pranav  pattie maes  and liyan chang   wuw wear ur world  a
wearable gestural interface   proceedings of the   th international
conference extended abstracts on human factors in computing systems 
acm       

x  appendix
here are the eight recognized gestures 

swipe up

thumbs up

swipe down

thumbs down

pinch in

swipe right

pinch out

swipe left

fi