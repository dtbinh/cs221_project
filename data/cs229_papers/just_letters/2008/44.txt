predicting candidate responses in presidential
debates
seth myers

stephen kemmerling

david chin lung fong

institute for computational
and mathematical engineering
stanford university

institute for computational
and mathematical engineering
stanford university

institute for computational
and mathematical engineering
stanford university

abstract in this paper we tried to predict what a candidate in a political debate  namely george w  bush  is going
to say  based on what the moderator and the other candidate
just said  in order to achieve that  we first used pca to reduce
the dimensionality of the problem  and then applied several
regression models in order to do the actual predictions  while
the pca turned up some interesting patterns  the best test
error  in a complex enough model  we achieved was about
    

i  i ntroduction
human thought is an incredibly complex thing  but
simultaneously humans are creatures of habit  more
specifically  most people seem to have a fixed pattern
concerning how they argue on a given topic  the authors
included   while they might use different words  the
points they make remain largely the same 
there is a group of people that is often seen as being
particularly notorious  as far as always making the same
point s  goes  politicians  this is made very evident
during political elections when the two candidates argue
over the same issues in a series of debates throughout
the election without either one changing their viewpoint 
we believe that learning what a politician is going to say
in a given context will be easier than the general case 
we have chosen to model the presidential debates
because of the availability of the data  and because the
topics in such a debate are usually constrained to a loose
theme  which might make predictions easier  president
bush will be our candidate of focus  he is the most recent
candidate to participate in two elections  and thus six
debates   so this will provide the most amount of data 
the first five debates will serve as training data  and the
last debate will be the test data 
our goal is to predict candidate responses based on
the questions asked by the moderator and the arguments
made by their opponent  in doing so  we hope to develop
a model of a candidates thought process during such a
debate 
in oder to account for the fact mentioned above  that
they might make the same point using different words 
and also as a form of feature selection we will first project
the individual respones down into a lower dimensional
concept space  using principal component analysis  we

will then try several regression models  namely neural
networks  linear and polynomial regression and support vector regression  in order to make the predictions 
ii  a pproach
due to the natural high dimensionality of written text
in combination with the fact that some of the machine
learning algorithms we will be using are computationally expensive  it is necessary to first map the text into
a lower dimensional space  we will assume this can be
done linearly  we begin by removing stop words from
the text  and then replace all words with their word stem 
next  we let x be the word vector for a speech made
by a candidate or the moderator during the debate  per
usual  the ith element of x is the number of times word
i is used in the speech   thus  x   n where n is the
number of different words used in the all of the debates 
let    kn where k is the desired dimension of the
reduced text space  the goal is to find  such that x 
defined as
x   x 
is a meaningful projection of x onto this k dimensional
space  we have run initial experimentation on values
of k as high as     and as low as     from this point
forward  this lower dimensional space will be referred to
as the concept space  and the basis with which we define
the concept space  i e  the rows of   will be referred
to as eigenconcepts  it is important to note that both the
input variables  i e  the speeches made by the debate
moderator and bushs opponents  and output variables 
i e  the speeches made by bush in response  will be
projected onto the concept space  we implement two
techniques for determining   latent semantic analysis
and principle component analysis 
once the debates have been mapped into the concept
space  we train various machine learning algorithms to
predict the main content of the speech given by the
candidate of focus in response to the speech delivered
by the moderator and the opponent  the techniques used
are 
 linear regression
 polynomial regression

fi


neural networks
support vector regression

a  latent semantic analysis
with reference to alvarez lacalle      we break down
every speech made by the candidates and the moderators into eigenconcepts using a variation of latent
semantic analysis  the algorithm goes as follows  first 
we construct a co occurrence matrix of all words used in
five of the six debates bush has participated in  the last
debate will be reserved as our test data   this matrix
will be a tally of how often any two words occur in
the same speech  next  we normalize the matrix by
the expected co occurrence of the words in a random
text  we apply singular value decomposition to the cooccurrence matrix  and the resulting eigenvectors are
the eigenconcepts  each eigenconcept corresponds to a
singular value as a result of the decomposition  and
we can interpret each singular value as a measure of
importance of the eigenconcept  thus  we define  such
that its rows are the eigenconcepts corresponding to the
k largest singular values 

d  polynomial regression
we repeat the same procedures as in linear regression  with the exception that we replace our input matrix
x by a extended matrix with higher order terms x  i e 
for a regression of n th degree
x    x     x          x  n   
 
where each x  i  is the matrix  x  i   jk   xjk
  also note
that y is not changed 

e  neural networks
all neural networks we use are feed forward networks with a symmetric sigmoid activation function 
we will try several different hidden layer sizes  as
well as different numbers of hidden layers  all neural networks are implemented using the fast artificial neural network library  which can be found at
http   leenissen dk fann    in an attempt to improve
results we perform several transforms on the data 



b  principle component analysis
the second method we implement for deriving the
eigenconcepts is principle component analysis applied
to the word vectors of all the speeches  ignoring speeches
of less than    words  our eigenconcepts become the top
k eigenvectors that pca yields 





c  linear regression
to investigate the structure of the data  we run the
linear regression model on our data  suppose we have
the input data  speech by moderator and opponent  as
x  and the output data  speech by bush  the candidate
of interest  as y   where x  y are in the same format as
in our lecture notes  with the difference that the output
being in the same dimension as the input  we define x
and y as the projections of x and y onto the concept
space  i e 
x   x

using only the top n eigenconcepts as output features 
using only the top m eigenconcepts as input features 
normalizing the concept vectors 
rescaling the individual eigenconcepts to        
a very simple approach to try to account for context  politicians in debates will sometimes use their
allocated time to answer or elaborate on previous
questions  so we attempt to take this into account 
specifically  viewing a debate as a sequence of concept vectors  instead of trying to learn a function f  
s t 
f  x  k       x  k 
trying to learn a function g  s t 
 
k
y
g
 ki  x i    x k     
i  



for      
nearly all meaningful combinations of the above 

f  support vector regression

and
y   y 
we have our hypothesis matrix m satifying the equation 
xm   y
by minimizing the empirical error  or maximizing the
likelihood with the error term following normal distribution  we have
m    x t x   x t y

if x i  is the ith speech made by the moderator opponent and y  i  is bushs corresponding response 
then we let x i  and y  i  be the corresponding projections
onto the concept space  we then use support vector
regression to model each component of y individually 
the optimization problem
m

x
 
i
min   wj       cj
 
i  
 i 

s t   wjt x i    bj  yj    

i   i       m

fiiii  m ain r esults
a  latent semantic analysis
our implementation of this analysis has yielded word
groupings within the eigenconcepts that are nonsensical
 it is generally expected that latent semantic analysis
yields eigenconcepts that demonstrate coherent ideas
through the words associated with them   additionally 
regression models built on the lsa eigenconcepts yield
extremely high error  and we attributed this to a poorly
defined concept space  this failure can be attributed to
the fact that this method of lsa was originally designed
to reduce the dimensionality of long prose such as
novels  the debate transcripts simply did not contain
speeches long enough for lsa to extract meaningful
word correlations 

c  linear regression
the linear regression model was trained using
different number of features  i e  different values of k 
each time  we start from k      and increment it up
to k        we obtained a test set error of     for   
features  this shows that a not too small portion of the
speech response mechanism was governed by a linear
component  the test set error increases as the number
of features that we try to predict increases  and it goes
beyond      if we try to predict more than    features 
error on linear regression against number of features using pca data
 

   
   
   
 
   
   

b  principal component analysis

   

the pca was successful  for a given eigenconcept 
taking its inner product with a word vector of a single word generates a measure of association for the
word to the eigenconcept  this is because the inner
product is equivalent to observing the component of
the eigenconcept corresponding to the word  often  the
words most positively associated with an eigenconcept
are related in meaning and demonstrate a coherent idea 
and these ideas are usually political issues such as stem
cell research or gun control  this is also true for negatively associated words  but they describe a completely
independent idea from the positively associated words 
this implies that a single eigenconcept can effectively
express two different political issues  see table i for more
details 

   

eigenconcept
 

 

  

most positively
associated words
school  governor  children 
public  teacher  oneonon 
district  privat  tuition 
money  learn  classroom 
educ  feder  account
law  gun  enforc 
societi  background 
school  trigger  lock 
learn  lawabid  carri 
age  licens  read
cure  embryon  bodi 
ethic  scientist  michael 
embryo  refug  artic 
chri  oil  visit 
oversea  god  embarrass

most negatively
associated words
iraq  war  saddam 
hussein  troop  terror 
weapon  world  threat 
mass  afghanistan  destruct 
laden  bin  qaida
technolog  energi  oil 
shortterm  sourc  develop 
cleaner  incent  wildlif 
environment  arctic  refug 
oversea  deep  ga 
greati  program  neighbor 
land  common  eat 
expand  health  uniqu 
care  middl  situat 
difficult  practic  easi

 
  

  

  

  

  
  
number of features

  

  

  

   

d  polynomial regression
the polynomial regression model was trained with   
feature  while changing the degree of polynomial being
used  we started from the linear version  up to the    th
degree polynomial  we see that from the test set result
that the linear regression is working better than all the
higher degree polynomials  therefore we conjectured
that the essence of the speech response might be better
represented by the cross term among different features 
with which the essence should be captured by neural
networks 
error on polynomial regression against degree of polynomial using pca data
   
training error
test set error
   

   
relative error

table i
s ample e igenconcepts

training error
test set error

   

relative error

is performed for all j           k  a gaussian kernel
of width j will be used  and leave one out crossvalidation will be performed to find the optimal values
of j and cj   we will be using the libsvm library     

   

   

   

   

   

 

 

 

 

 
 
 
degree of polynomial

 

 

  

fie  neural networks
the general behavior of the error vs  the number of
eigenconcepts to predict was about the same as the linear
regression  the best results in general  i e  independent of
any other transforms on the data or network type  were
achieved with predicting only the top    eigenconcepts 
but using the top     eigenconcepts as input data 
no significant improvement could be achieved with
further lowering the number of eigenconcepts to predict 
the best absolute error was achieved with rescaling
each eigenconcept to         approximately       mean
square error for each concept  as most concepts were
close to zero in our test data  this did not produce the
best relative error 
the best relative error  in the normal linear algebra
sense  was achieved with normalized data  on a network
with one hidden layer of    nodes  predicting only the
top    eigenconcepts  but using the top     eigenconcepts as input features  the error was nevertheless still
about     
multiple hidden layers in general overfitted the data
very quickly and therefore did not improve the test
set error at all  as far as the size of the hidden layer
is concerned  best results were generally obtained with
hidden layer sizes of about   to   times the number of
eigenconcepts to predicts 
additionally  implementing our method that accounts
for candidates referring to previous questions in their
responses increased the error in the predictions significantly  this is most likely due to it introducing too much
noise for our comparatively little data set 
of note is that the best test set error achieved with the
neural networks was higher than that of the linear regression  at least with a low number of output concepts 
however  because of the fact that the training error for
neural networks goes down very quickly  whereas for
the linear model it does not go to zero at all  we think
that the neural network model has more promise in a
situation where more data is available 
f  support vector regression
the support vector regression yielded a mean square
error much larger than the neural network method  on
the order of     so we did not pursue this method
further  one possible explanation for svrs failure is
that in predicting the presence of each eigenconcept independently it ignores the relational properties between
eigenconcepts 
iv  c onclusion
from our implementation of a range of different
machine learning techniques  we see that a significant
portion of the speech response human process has been
captured computationally  the generalized error  however  remains quite high  even after experimenting with
a varying number of features  the high variance of our

regression models  in combination with the size of our
training dataset  only     training examples   suggests
that our results can be improved by using more data 
a possible source of more training data are political
interview transcripts  the reponses should be more standardized and a much larger amount of data would be
available  all in all  the project succeeded in probing the
tip of the iceberg of the human thinking process  as far
as in a structured dialogue 
r eferences
    e  alvarez lacalle  b  dorow  j  eckmann  and e  moses  hierarchical structures induce long range dynamical correlations in written
texts  proceedings of the national academy of sciences of the united
states of america                     
    c  c  chang and c  j  lin  libsvm  a library for support vector
machines        software available at http   www csie ntu 
edu tw cjlin libsvm 

fi