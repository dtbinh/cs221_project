cs     final report  location based adaptive routing protocol lbar  using
reinforcement learning
by  eunjoon cho and kevin wong

abstract

sent  each node forwards the packet in a greedy fashion to
the location of the final destination  usually the requirement
of a location service for the sensor nodes is an expensive
assumption for wireless sensor networks  however  in our
vehicular network scenario with the advent of gps devices
we can assume that this is possible 
another major drawback of using a position based routing method is that there is no link quality measure of the
current path that is being used  since each node has location information of its neighbors only  it is likely that our
greedy packet forwarding will lead us to a local minimum 
the contribution of our work is that through reinforcement
learning we can find routes that may not necessarily optimal
in a local greedy sense  but are more likely to eventually deliver the packet to our final destination 
we can imagine a scenario where we would like to send
a packet from here to somewhere in san francisco  when
sent via a greedy manner  it is likely that packets will be
forwarded to cars on the el camino real  since this each
hop along this path will make it closer to our final destination  however  we know that cars on the     or    
are more densely and evenly distributed  therefore  even
though sending the packet through cars on university avenue might temporarily get us further from our final destination  we would like the network to learn this path since it
will lead us to a stronger connected path and therefore result
in a higher throughput 

in this paper we present an algorithm for a location based
adaptive routing protocol that uses both geographic routing and reinforcement learning to maximize throughput in
our mobile vehicle network  we use reinforcement learning
to determine the correct direction to forward a packet and
then use geographic routing to forward a packet toward the
network sink  we use an extension of the q routing algorithm  originally proposed by boyan  et al      except that
we apply it on a location based routing protocol that benefits over a link based routing protocol in scenarios where
the network is highly mobile 

   introduction
standard wireless routing protocols do not work well in a
mobile setting because they were not designed to adapt to
a quickly changing environment  the environment we have
set is a vehicular sensor network where sensor nodes within
each vehicle will communicate with other nodes across the
network in a multi hop fashion 
routing protocols in wireless sensor networks can be
generally categorized into link based and location based
routing methods  link based routing uses the link information between nodes  and can be further divided into proactive and reactive routing  in proactive routing methods  each
node keeps information of the network topology and finds
the shortest route whenever it needs to transfer a packet 
nodes using reactive routing methods flood the network
when needed to send a packet  the sink node determines
the optimal route based on its specific algorithm  and then
initiates a path between the source and the sink  this path is
used until a disconnection is discovered  where the source
initiates a flood to search for another path  when the network is highly mobile  like in a vehicular network  topology
based methods described above are usually unstable since
the link path is not sustained over a substantial period of
time  the source repeatedly floods the network to initiate
new paths  which increases the overall overhead in the network 
position based routing methods assumes that each node
has geographical location information of itself  its neighbors and the destination  when a message is needed to be

   related work
q routing
q routing was one of the earliest works that dealt with combining machine learning with network routing  routing algorithms typically route along the shortest path from sink
to source in an attempt to minimize hop count and latency 
but if the network contained a bottleneck  a link could become oversubscribed and packet losses would occur due to
queue drops and packet collisions at the bottleneck  qrouting attempted to deal with this issue by selecting its
next hop among its neighbors that had a minimum delivery
times to the network sink  this delivery time included any
queuing and transmission time a packet encountered during
any transmission  q routing defines an equation qx  d  y 
 

fiwhich defines the estimated delivery time of a node x if
it routes through another node y  after forwarding to y 
the minimum delivery time to the destination from y is returned to x  this minimum delivery time is defined below
in equation    and is used to update node x  s delivery time
in equation    the values of q and s are the queuing and
transmissions times of the packet respectively 
t 

min

zneighbor of y



qy  d  z 

new estimate

we have associated with each of the terms in the q learning
algorithm 

state action definition
in order to simplify potentially complicated road networks
for our simulation  we decided to partition our simulation
space into regions defined by a rectilinear grid  each grid
block was considered a state and vehicles within that region
were associated with that state  it was assumed that the grid
boundaries and spacing was global knowledge and all vehicles would be able to unambiguously determine their proper
state  figure   has an example of such a grid partitioning of
a simulation space 
using this simplified state space  we define our state actions as forward a packet to a node in a neighboring state 
we considered an   connected neighborhood for each state 
so nodes had potentially   possible routing actions at each
state  since each action is associated with sending the
packet to a next state  we express action as simply the next
state that the action would lead us to  as expressed in the
q routing algorithm  we decided to forward the packet to a
node in the next state that was closest to the destination in
terms of euclidean distance 

   


old estimate

z      
 z     
qx  d  y      q   s   t  qx  d  y  

   

as a result of using delivery time as a metric to decide routing paths instead of a simple shortest route  the
authors demonstrated that q routing outperformed shortest path routing in a irregular network during periods of
high load  unfortunately  the gains of q routing are only
available after the learning algorithms policies and delivery times have converted according to the above equations 
   

antnet
reward function

learning methods have been adapted to wireless sensor
networks  wsn  with a more practical view using concepts
of ants and stigmergy in antnet      in this model  forward
ants are periodically sent throughout the network in a random manner to the destination node  while an ant moves
through the network it keeps track of the node and time it
passes through  the destination node selects the path that is
optimal in time and then initiates a backward ant back to the
source using the same path saved in the forward ant  as the
backward ant moves back it updates the goodness of the
nodes in its path  i e   increases the probability of that node
to be chosen when sending to the same destination  packets that are later forwarded across the network rely on this
probability to select the next best hop to its destination 
it has been shown that various versions of the antnet
algorithm shows good performance in learning the dynamic
topology of a wsn  however  given that this is a link based
routing method  there is reason to believe that convergence
will be difficult in our highly mobile vehicular scenario 

 
xx yy p rr  x  y 

sle  x  y     p

   

our reward function  which is an estimated measure of
link quality and potential throughput from a specific state
to another  was defined as a function of the packet reception rate  prr   which is defined between nodes  prr is
a measure used in routing that measures the probability that
any given packet transmission will be successfully received 
it can be estimated through active beaconing or previous
packet forwarding trials  generally prr can change significantly over time due to the inherent variability associated
with ad hoc wireless communication 
we define our state link estimator  sle  reward function  shown in eq    as an extension to etx  except it measures the link quality between two states by taking the inverse of the prr of all routable links between two states 
in eq     x and y refers to a node in state x and y respectively  etx is a bidirectional path metric used in wsn that
measures the expected number of transmissions required to
successfully send a packet between two nodes  it is typically calculated through beaconing or previous packet forwarding trials  our metric extends etx to account for the
multiple nodes in each state and is proportional to both the
link quality as measured through prr and the number of
nodes in both states  which is rough substitute for the possible throughput  since our calculations only consider uni 

   adaptive location based routing
q st   at    

   
i
t  st   at   rt      max q  st     a   q  st   at  
h

a

for our routing algorithm we generally follow the qlearning algorithm given in eq     we will describe what
 

fidirectional link estimates  our etx measurements across
states will be somewhat skewed  however  for the sake of
simplicity  we are ignoring the effect asymmetric links for
our simulation     

learning q values
q x  d   

   

 sle x  ymin      min q y  d   q x  d  
y n
we used q learning to make incremental updates to our
q values for each of our states at regular intervals as shown
in eq     where n is the set of all neighboring states of x 
d is the state containing the destination  and y represents
a neighboring state  we chose the learning rate  and the
discount factor  so that our q values would be stable for
our simulation 

routing algorithm
n extstate  x    argminy n q  y  d 

   
figure    map of converged decision flow 

to make a routing decision at each state  shown in eq    
we find a state in our   connected neighborhood that has
a minimum q value and route a packet toward the node
in that state that has a minimum euclidean distance to the
packets destination  we used gpsr as the underlying routing mechanism when routing between states 

learn  we have divided up our map into  x  grids  and each
block within our grid represents our states 
we have a simple communication setting where the
source sends packets to the destination every second  and
the simulation lasts for     to      simulation seconds  the
underlying routing protocol that we use is gpsr  and our
lbar learning algorithm acts simply decides which node
would be our next hop over the greedy decision made by
gpsr  the destination and source nodes our fixed at static
locations whereas all other nodes our dynamic within their
path  we use beacon intervals of   second for nodes to
maintain information of their neighbors and to update the
q values 

   simulation and evaluations
evaluation on mobile networks is a difficult problem in that
most of the current used simulators for wireless sensor networks are inefficient to scale and mobility  jist swans
is a simulator that has focused on solving these problems
and is designed to overcome such problems in simulating
mobile communication environments  an extension of this
jist swans simulator also provides strong visualization
for understanding the communication process in the mobile
network 
using this simulation tool we have implemented a setting with the actual roads of a small section in chicago  as
shown in fig   we have then placed     routable nodes or
cars  on top of these roads with varying speeds similar to
the actual movement of a car  actual communication is limited to one source node at the bottom of the map  and one
destination node located at the top left corner of the map 
we have set two possible paths for routing  the first route
which is sending packets directly north is greedy  but has
a weak connection  the other route which is a detour and
might be less intuitive to consider has an actually stronger
connection  and it is this route that we want the network to

   results
route decision convergence
we check whether the network converges to a q value that
gives the correct decision for routing at each state  the convergence of the decisions occur faster than the actual convergence of the q values  this is because of the fact that
decisions are made based on the minimum q value among
its neighbor nodes  a certain state can have a smaller q
value compared to the other states and can be continued to
be this way even though there are minimal changes in the
actual q values 
 

fifigure    convergence graph of q values over different
learning rates

figure    comparison of prr at destination to gpsr
tent we can set the learning rate low  so the q values are
stable and the nodes are consistently led to make the correct
decisions 

in fig    we can see the final converged decision flow of
our simulation setting  the direction of the red arrows corresponds to the next state the network learns to forward the
packet  the length of the arrow corresponds to the inverse
of the q values defined at the next hop state  meaning that
the larger the arrow the closer  in connection  our next state
is to our destination  we can see  for example  that nodes
in state    learn it is better to forward its packet to state
   even though it is actually moving further away from the
final destination 

comparison with a greedy algorithm
we show our final evaluation by comparing the packet reception ratio at our destination with a greedy algorithm  as
expected our results in fig    show that a greedy algorithm
can easily face situations where it will not be successful 
however  using our adaptive learning algorithm  the network learns over time to find the optimal path to the destination  we can see in our graph that lbar initially starts of
with a lower prr since it lacks information of which paths
are stronger than other  however  over an amount of time
that depends on the learning factor  the prr increases and
maintains this link quality until a significant change in the
network occurs 

q value convergence
we next check whether the actual q values of our network
show some convergence  we are not guaranteed any strong
convergence in our q table since the reward value sle  that
we are using is dynamic and can change over time  we have
tested that with a fixed sle that our q values do indeed converge to a certain value  although our reward value sle 
alters over time  we assume that the general distribution
over the roads stay somewhat consistent over a period of
time  with this assumption we can say that although the
links between two nodes may be constantly changing  the
links between two states  i e   the sle value would be somewhat consistent 
fig    show the convergence of our q values over different learning parameters  we can see that with a small
learning factor our network is slow in learning but maintains to be stable over a long period of time  on the other
hand  when a large learning rate is used we can see that it
reaches to a value equivalent to the converged value in a
short amount of time  but is unstable  we can adjust our
learning factor according to our road environment  if we
believe that our road traffic changes rapidly over time during the day  we seek to set a high learning rate so that the
network quickly adapts to the new density measure in the
roads  if we have a road where the traffic is fairly consis 

   future work
the lbar algorithm shows promise in its ability to utilized high throughput paths long road networks in routing
compared to greedy geographic routing algorithms  further
refinement of our algorithm could further increase its performance and improve its potential reliability in more complicated traffic patterns 

state definition improvements
our grid based state space partitioning was efficient in
terms of storage complexity and determining neighboring
states  but it does not conform well to the underlying road
network  using fixed grid sizes would cause us to either
waste state on areas with no roads  or group complicated
road sections into the same state  both of which were undesirable  using a finer grid would help model complicated regions  but would be inefficient for areas with sparse straight
 

firoads  as we would have to maintain q value storage for
states devoid of any roads  additionally  the grid boundaries
would be somewhat arbitrary with respect to the roads  as a
state boundary could lie exactly along a road  which could
make routing ambiguous or could form routing loops 
one possible alternative is to treat each intersection as
a state  and associated mobile users with the closest intersection  modeling our simulation space in this way would
cause the number of states to be proportional the complexity
of our road network  we could then define our state actions
as forwarding packets to one of the neighboring states  or
in our case intersection  this would add extra complexity
to our existing algorithm  as we would have to have variable sized data structures to keep track of neighboring intersections and more complicated preprocessing of our road
network would be necessary to extract all possible intersections  additional preprocessing would be also be necessary
to ignore short road segments  such as driveways and service roads that would appear in maps  but are not typically
used in vehicle through traffic 

   conclusion

reward function improvements

references

our current reward function is a simplified version of etx
that has been extended so that it can be defined across states
instead of nodes  we originally chose this metric so that
we could have a metric that measured both link quality and
throughput  so we could identify high traffic roads  to extend our reward function  we could use bi directional link
quality estimates to better characterize individual links  we
also do not distinguish endpoints of each link only the total
number of links that cross between two states  so if there
were a scenario where state a had    nodes  and state b had
   nodes  and if each node in a had a single link to a unique
node in b  these ten links might have the same sle compared to a scenario where state b only had a single node 
with which all nodes in state a had shared a link  if the
beaconing was infrequently and non overlapping  each of
these ten links could have the same prr  but the throughput between these states would be very different  since the
first scenario could potentially handle ten times the traffic
as the second  these two scenarios are not well represented
by their respective sle values  more research into alternative throughput sensitive reward functions between states is
clearly warranted 

    j  a  boyan and m  l  littman  packet routing in dynamically
changing networks  a reinforcement learning approach  in
advances in neural information processing systems    pages
        morgan kaufmann       

our lbar algorithm shows improvement over gpsr in
terms of packet reception rate in our simulated test case 
lbar delivered       of packets with an        
while gpsr only delivered       of packets  the long
term packet delivery ratio would be somewhat higher for
lbar  as we counted packet transmissions during the initial route setup and convergence period for our packet reception statistics  this period had frequent packet drops as
nodes did not have a reliable nest state to use in packet forwarding  our simulation environment was setup to specifically show a situation where lbar should show improvement  but in general  lbar will favor routing along roads
with the highest estimate throughput and link quality  on
the other hand  the gpsr algorithm ignores roads and will
route toward any node that will get a packet closer to the
destination  as in our simulated test case  ignoring roads
and traffic patterns completely can lead to suboptimal routes
and packet drops do to the inability of nodes to find a next
hop 

    g  d  caro and m  dorigo  antnet  a mobile agents approach
to adaptive routing  technical report       
    d  s  j  d  couto  d  aguayo  j  bicket  and r  morris  a
high throughput path metric for multi hop wireless routing 
in mobicom     proceedings of the  th annual international
conference on mobile computing and networking  pages    
     new york  ny  usa        acm 

larger scale testing
we only tested with a modestly sized road map due to time
and processing power constraints  a future larger scale simulation could help us identify any degenerate routing cases
with lbar 
 

fi