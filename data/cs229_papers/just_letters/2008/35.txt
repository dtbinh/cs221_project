cs    final project  dec     

 

protein secondary structure prediction
based on neural network models
and support vector machines
jaewon yang
departments of electrical engineering  stanford university
jaewony stanford edu
these problems      research in computational structure
prediction concerns itself mainly with predicting secondary
structure from known experimentally determined primary
structure  this is due to the relative ease of determining primary
structure and the complexity involved in tertiary structure 
the secondary structure prediction approaches in today can
be categorized into three groups  neighbor based  model based 
and metapredictor based      the neighbor based approaches
predict the secondary structure by identifying a set of similar
sequence fragments with known secondary structure  the
model based approaches employ sophisticated machine
learning techniques to learn a predictive model trained on
sequences of known structure  whereas the metapredictor
 based approaches predict based on a combination of the results
of various neighbor and or model based techniques 
historically  the most successful model based approaches 
such as psipred     were based on neural network  nn 
learning techniques      however  in recent years  secondary
structure prediction algorithms based on support vector
machines have been developed and have been showing good
performance      in this paper  these two successful methods
will be compared 

abstract the prediction of protein secondary structure is an
important step in the prediction of protein tertiary structure 
protein tertiary structure prediction is of great interest to
biologists because proteins are able to perform their functions by
coiling their amino acid sequences into specific three dimensional
shapes  tertiary structure   therefore  this subject is of high
importance in medicine  e g  drug design  and biotechnology 
instead of costly and time consuming experimental approaches 
effective methods have been developed continuously  the
secondary structure prediction approaches in use today can be
categorized into three groups  neighbor based  model based  and
metapredictor based approaches  the model based approaches
employ sophisticated machine learning techniques such as neural
networks  hidden markov models  and support vector machines to
learn a predictive model trained on sequences of known structure 
with the help of growing databases and the evolutionary
information available from multiple sequence alignments 
resources for secondary structure prediction became abundant 
however  this paper focused on single  sequence prediction in
order to compare algorithmic efficiency and to save
computational time  in this paper  the neural network and the
support vector machine based algorithms will be compared 
keywords  protein structure prediction  secondary structure  neural
network back propagation  support vector machines

i  introduction

ii  method

protein structure prediction is one of the most important
goals pursued by bioinformatics and theoretical chemistry  this
subject is of great interest to biologists because proteins are
able to perform their functions by coiling their amino acid
sequences  primary structure  into specific three dimensional
shapes  tertiary structure   this process is called protein
folding  in other words  the linear ordering of amino acids
forms secondary structure  arranging secondary structures
yields tertiary structure  therefore  protein structure prediction
is of high importance in medicine  e g  drug design  and
biotechnology  e g  the design of novel enzymes  
a number of factors exists that make protein structure
prediction a very difficult task  two main problems are that the
number of possible protein structures is extremely large  and
that the physical basis of protein structural stability is not fully
understood  in this sense  the techniques such as spectroscopy
and far ultraviolet  far uv          nm  circular dichroism for
structure prediction are time consuming and expensive 
however  due to the increase in computer power and especially
new algorithms  much progress is being made to overcome

a  database
a 

definition

no unique method of assigning residues to a particular
secondary structure exists  although the most widely accepted
protocol is based on the dssp algorithm  dssp uses the
following structural classes  h   helix   g      helix   i
  helix   e   strand   b  isolated  bridge   t  turn   s  bend  
and   other   in this paper  the reduction scheme that converts
this eight state assignment to three states by assigning h the
helix state  h   e to the strand state  e   and the rest  i t s and
   to a coil state  c   this is the simplest format used in
structure databases 
b 

training and testing sets   

cross validation appears to remove the problem of a limited
data set for training and test  however  artificially high
 

fics    final project  dec     

 

accuracies can be obtained if the set of proteins used in
cross validation show sequence similarity to each other 
accordingly  cross validation sets must be pruned stringently
to remove internal sequence similarities  but if it is not possible 
then a completely independent test set must be used  therefore 
in this paper  the hold out cross validation technique  where test
proteins are removed from the training set  was used  the
training data set is the cb    and the rs    set is used for
testing  the    pairs which showed up homologies in the
cb    set were removed from the rs    set  and protein
chains of     residues were also removed  in conclusion     
proteins        residues  from cb    set for training and    
protein chains        residues  from rs    set for testing were
used 

are determined by an input window of amino acid residues
through an input coding scheme  starting from the hidden layer
 h  and moving toward the output layer  o   the state of each
hidden unit i in the network is determined by 
o

v 
h

 w i

   
   

the back propagation learning algorithm can be used in
networks with hidden layers to find a set of weights that
performs correct mapping between sequences and structures 
starting with an initial set of randomly assigned numbers  the
weights are altered by gradient descent to minimize the error
between the desired and the actual output vectors 

secondary structure fractions
total number of residues for training        
total number of residues for testing        
train set   
test set   
     
     
h  helix 
     
     
e  sheet 
     
     
 others 
table    proteins in training and testing set

c 

performance measures

there are many ways to access the performance of the
method for predicting secondary structures  the most
commonly used measure is a simple success rate  q   which is
the percentage of correctly predicted residues on three types of
secondary structures 
q



h e c

figure    the  st neural network design

b 

the neural network was trained with different sequence and
structural information using a sliding window scheme which
was also used in the svmmulticlass and svmhmm  in the sliding
window method  a window becomes one training pattern for the
predicting structure of the residue at the center of the window 
in many input encoding methods  orthogonal encoding was
used  each residue has a unique binary vector  such as
                                       in orthogonal
encoding  each binary vector is    dimensional that means   
amino acids  in this method  the weights of all residues in a
window are assigned to   
for a given input and set of weights  the output of the
network will be a set of numbers between   and    the
secondary structure chosen was the output unit that had the
highest activity level  this was equivalent to choosing the
output unit that had the least mean square error with the target
output 
the performance of the network on the training and testing
sets depends on many variables  including  the number of
training examples  the number of hidden units  and the size of
window  the ability of a network to extract higher order
features from the training set depends on the layer of hidden
units and the types of encoding scheme 

      

b  neural network
the neural network is trained using the supervised learning
method  here the training process is finding the appreciate
value for the weight of each layer in the network to maximize
accuracy of prediction  in supervised learning  a training data
set is encoded into feature vectors combined with correct class
labels  such as helix  sheet  or coil  the psipred method by
jones        is a successful approach for predicting secondary
structure      in psipred  a two stage neural network was
used based on the position specific scoring matrices generated
by psi blast  this paper is based on the algorithm of
psipred  but instead of applying pssm  position specific
scoring matrices  into input  single sequence prediction
method is used in order to focus on the algorithm and to avoid
expensive computational time 
a 

network design

neural network and properties

a feedforward network is composed of two or more layers of
processing units  the first is the input layer  the last is the
output layer  and all the other layers between are termed hidden
layers  the state of each unit has a real value in the range
between   and    the all input units  i  that form an input vector

c  svmhmm and svmmulticlass

 

fics    final project  dec     

 

among the many machine learning approaches  support
vector machine  svm  methods are the most recent to be used
for structure prediction  the paper     designed classifiers for
the three cluster problems based on the binary classifiers
generated by svms  however  in this paper  the generalized
multi class svms were used       unlike the case of multiclass
classification where output space with interchangeable 
arbitrarily numbered labels  structured output spaces are
considered in generalized multiclass svms  i used svmlight
which is widely used software implementations of svm 
svmhmm and svmmulticlass are applications using svmlight for
generalized multiclass classification 
svm  min  
s  t  i  y

c

    s  t  i  
w
 
t
  w  y
 y  y


where  y

 x  y

 x  y

in generalized multi class svms  we need to select a kernel
function and the regularization parameter c  the primal
formulation of the generalized soft margin svms maximize
margin and minimize training error simultaneously by solving
the previous optimization problem         
b 

the key challenge in solving the quadratic problems for the
generalized multi class svm learning is the large number of
margin constraints  if the length of each amino acid is l and
there are n training data  we have exponential number of
constraints  n l because each amino acid can be one of three
classes 
however  only a much smaller subset of constraints needs to
be explicitly examined by using cutting plane  the
cutting plane using dynamic programming  viterbi algorithm 
aims at finding a small set of active constraints that ensures a
sufficiently accurate solution  this method can reduce the
number of constraints to a polynomial sized subset of
constraints that the corresponding solution fulfills all
constraints with a precision of   in other words  the remaining
exponentially many constraints are guaranteed to be violated by
no more than   without the need for explicitly adding them to
the optimization problem 

   
   
   

svmmulticlass is an implementation of the multi class svm
x   y with
described in      for a training set x   y
labels y in i           in the secondary structure prediction  for
linear kernels  this is very fast and runtime scales linearly with
the number of training examples  non linear kernels are not
 really  supported  the loss function  y   y is the number of
misclassified tags 
svmmulticlass 
c

min w
s  t  for all y x  w

x w

    y   y

   
    

x w

    y   y



dynamic programming for cutting plane

iii  results
a  neural network

x w

   

a 

svmhmm is an implementation of structural svms for
sequence tagging      given an observed input sequence
x
x  x    x
of feature vectors  the model predicts a tag
according to the following
sequence y
y   y    y
optimization problem  for the given training examples
x   y   x   y     x   y   the feature vector is x
x    x
with their correct tag sequence  y
y    y  
for the secondary structure prediction  only one entry has   and
others are zero by orthogonal encoding 
svmhmm 
min w
s  t  for all y 


    




    

    

c



x w

   
y



x w
x w

 y
y


y



    

a 

parameter optimization



w
 y

 y



x w

training and testing with increasing the number of
iteration 

y

w

 y  y

    
figure    learning curve for real proteins
with    hidden units and    window size 

w
 y

w

 y  y

further training improved the performance of the networks
with hidden units on the training set  but performance on the
testing set did not improve but tended to decrease  this result is
an indication that memorization of the detail of the training set
is interfering with the ability of the network to generalize  the

    

 

fics    final project  dec     

 

peak performance for a network with    window size and   
hidden units was q           after    iterations 
b 

input network comprising just    input units  for this network 
a smaller hidden layer of    units were used instead of    units
of the psipred  q  performance was similar but showed a
little improvement 

dependence on the number of hidden units
hidden units
 
  
  
  
   
   

q    
     
     
     
     
     
     

b  svmhmm and svmmulticlass
non linear kernels are not supported in the svmhmm and
svmmulticlass  without the optimization of kernel parameters  i
focused on selecting the optimal parameter c which plays a
critical role  common practice is to choose the parameter that
maximizes the accuracy by using hold out cross validation
method 

table    dependence of testing success on hidden units

table   shows that the peak performance on the testing set
depends on the number of hidden units  also  it is shown that
having more hidden units is not always good because it can
cause high variance problem as mentioned in the previous
section 
c 

c
 
 
 
 
  
  
  
  
  
  

dependence on the size of window
window size
 
 
 
 
 
  
  
  
  
  
  

q    
     
     
     
     
     
     
     
     
     
     
     

the optimal window size of the encoding scheme for
svmmulticlass and svmhmm was obtained by testing the accuracy
for the various window sizes  and it was shown that    for
svmmulticlass and    for svmhmm are optimal window size  the
interpretation of the result is similar with that of the neural
networks  also this shows that the local sequence environment
of a residue substantially determines its secondary structure  it
was considered the fact that residues far apart in sequence but
close in three dimensions can have the tertiary interactions 
c
 
 
 
 
  
  
  
  
  
  
  
  

 nd network
q    

  sv w    
 
 
 
 
 
 
 
 
 
 

table    dependence of the number of support vector and

table   shows the dependence of testing accuracy rate on the
size of the input window with    hidden units  the result shown
in table   indicates that when the size of the window was small
the performance on the testing set was reduced  probably
because information outside the window is not available for the
prediction  when the size of the window was increased  the
performance reached a maximum at around    window size 
for larger window sizes  the performance deteriorated 
probably because of the effects of extra weights that could not
contain any information about the secondary structure of the
center  thus  irrelevant weights can interfere with the
performance of the network 

 st network
     

q      w    
     
     
     
     
     
     
     
     
     
     

testing success on c regularized parameter   svmmulticlass 

table    dependence of testing success on window size

d 

q      w   
     
     
     
     
     
     
     
     
     
     

 nd network
     

q      w   
     
     
     
     
     
     
     
     
     
     
     
     

q      w    
     
     
     
     
     
     
     
     
     
     
     
     

  sv  w    
  
  
  
  
  
  
  
  
  
  
  
  

table    dependence of the number of support vector and
testing success on c regularized parameter   svmhmm 

table    testing accuracy of the  st and  nd network

interestingly  the performance of svmhmm is better than that
of svmmulticlass  also  we can see that svmhmm has much more
support vectors than svmmulticlass does  based on the algorithm
of hmm hidden markov model   svmhmm considers the

in psipred  a second network is used to filter successive
outputs from the main network  as only three possible inputs
are necessary for each amino acid position  the network has an

 

fics    final project  dec     

 

transition probabilities between hidden state which means
secondary structure in this problem  but this information is not
involved in svmmulticlass  intuitively  the secondary structure of
an amino acid are correlated with that of the neighborhood
sequences  not far from it  this concept is very similar to the
reason why we take window to input feature vector 

figure    hidden markov model hmm 
for protein secondary structure prediction

c  compare
training with svms has crucial advantages including much
faster convergence than neural networks  nns   the svms
tend not to overfit and the ability to find the global optimum by
solving the dual problem of quadratic convex function
minimization  however  the neural networks approach suffers
from the local minima  determination of appropriate structure
of neural networks and too many parameters 

figure    bar graphs showing the distribution of q  scores

as we can see  the performance of the neural network
outperforms the svms  in the neural network  i used non linear
network by using sigmoid function which attributed to increase
the accuracy because this problem is not linear  however  the
svms are still linear because kernels cannot be applied  as a
result of it  the performance of svms was worse than that of
nns 
methods
non linear nns with    hidden units
svmhmm
svmmulticlass
linear nns with no hidden units

q    
     
     
     
     

table    the best performance of linear nns and svms

in a different point of view  the overall accuracy of svms is
less than the nonlinear neural network  but the variance of
accuracy is smaller than that of the neural network  this means
that the average accuracy of svms is more reliable 
however  the prediction levels of all three methods i used in
this paper are not sufficient to compare with the result using
multiple sequence alignments  i will discuss about the ways to
improve the accuracy of both approaches in the next section 

 

fics    final project  dec     

 
   

s  hua and z  sun  a novel method of protein secondary structure
prediction with high segment overlap measure  support vector machine
approach  jmb       
    h  kim and h  park  protein secondary structure prediction  based on an
improved support vector machines approach  protein eng       
    k  crammer and y  singer  on the algorithmic implementation of
multi class svms  jmlr       
    y  altun  i  tsochantaridis  t  hofmann  hidden markov support
vector machines  international conference on machine learning
 icml        
     i  tsochantaridis  t  hofmann  t  joachims  and y  altun  support
vector learning for interdependent and structured output spaces 
icml       
     i  tsochantaridis  t  hofmann  t  joachims  and y  altun  large margin
methods for structured and independent output variables  jmlr      
     l  rabiner  a tutorial on hidden markov models and selected
applications in speech recognition  ieee       

iv  discussion and future work
the highest accuracy is        of the neural network  but it
is not high enough  i think that there are two weak points for
this low accuracy  multiple sequence alignments and kernel
methods  for example  one recent study adopted frequency
profiles with evolutionary information as an encoding scheme
for svm      and another approach is to use incorporated
psi blast pssm profiles as an input vector         both of
them showed the improvement of accuracy  and based on the
result of these studies  the success of svm methods depends on
the proper choice of encoding profiles and kernel function 
a  encoding profile

appendix

in frequency matrix encoding  the frequency of occurrence of
   amino acid residues at each position in the multiple
sequence alignment is calculated for each residue  and in
pssm coding the individual profiles were used to reflect
detailed conservation of amino acids in a family of homologous
proteins  the previous study reported that by using reliable
local alignment  the prediction accuracy could be improved 
therefore  if multiple sequence alignment methods are applied 
the accuracy could be higher 

a  proteins in training and testing set
amino acid fractions
total number of residues        
amino acid
train set
test set
a alanine 
      
      
r arginine 
      
      
n asparagine 
      
      
d aspartic acid 
      
      
c cysteine 
      
      
e glutamic acid 
      
      
q glutamine 
      
      
g glycine 
      
      
h histidine 
      
      
i isoleucine 
      
      
l leucine 
      
      
k lysine 
      
      
m methionine 
      
      
f phenylalanine 
      
      
p proline 
      
      
s serine 
      
      
t threonine 
      
      
w tryptophan 
      
      
y tyrosine 
      
      
v valine 
      
      
others
      
      

b  kernel methods
the choice of kernel function is critical to the success of svm 
by applying the kernel method  a nonlinear classifier can be
built  most studies adopted the radial basis function  rbf 
kernel      therefore  applying kernels methods could increase
the performance  however  we have still a remaining problem
to handle huge training data sets  which prevents from applying
kernels to svms 
v  acknowledgement
specially thanks to chung tom  do  ph d candidate in the
department of the computer science  in stanford  without his
advice and guideline  i was not able to make a progress on the
final project  i really appreciate his efforts to help me out 

b  the result of neural network for each test data

references
   

   
   
   
   

protein
 acx all
 azu all
 bbpa all
 bds all
 bmv  all
 bmv  all
 cbh all
 cc  all
 cdta all
 crn all
 csei all
 eca all

hae jin hu  robert w  harrison  phang c  tai  and yi pan  current
methods for protein secondary structure prediction based on support
vector machines  ch    knowledge discovery in bioinformatics 
techniques  methods  and applications       
hae jin hu  robert w  harrison  phang c  tai  and yi pan  protein
structure prediction using string kernels  ch    knowledge discovery
in bioinformatics  techniques  methods  and applications       
james a  cuff and geoffrey j  barton  evaluation and improvement of
multiple sequence methods for protein secondary structure prediction 
proteins       
david t  jones  protein secondary structure prediction based on
position specific scoring matrices  jmb       
ning qian and terrence j  sejnowski  predicting the secondary
structure of globular proteins using neural network models  jmb 
     

 

 residues
   
   
   
  
   
   
  
  
  
  
  
   

qalpha
 
      
      
 
      
      
 
      
 
      
      
      

qbeta
      
   
      
      
      
      
    
 
      
   
      
 

qcoil
    
      
      
      
      
      
      
      
      
      
    
      

q    
      
     
      
      
      
      
      
      
      
      
      
      

fics    final project  dec     
 etu all
 fc c all
 fdlh all
 fdx all
 fkf all
 fnd all
 fxia all
 gd o all
 gdj all
 gp a all
 hip all
 il a all
 l   all
 lap all
 lmb  all
 mrt all
 ovoa all
 paz all
 ppt all
 pyp all
 r    all
 rbp all
 rhd all
 s   all
 sh  all
 tnfa all
 ubq all
 wsya all
 wsyb all
   ba all
 aat all
 ak a all
 alp all
 cab all
 ccya all
 cyp all
 fox all
 fxb all
 gbp all
 gcr all
 glsa all
 gn  all
 hmza all
 i b all
 ltna all
 ltnb all
 mev  all
 or l all
 paba all
 phh all
 rspa all
 sns all
 sodb all
 stv all
 tgpi all
 tmvp all
 tsca all
 utga all

   
  
   
  
   
   
  
   
   
   
  
  
   
   
  
  
  
   
  
   
   
   
   
   
  
   
  
   
   
   
   
   
   
   
   
   
   
  
   
   
   
  
   
   
   
  
  
  
   
   
   
   
   
   
  
   
   
  

      
      
 
 
 
      
      
      
      
      
   
      
      
      
      
 
   
      
 
      
      
 
      
      
 
 
   
      
      
      
      
      
     
 
      
      
    
      
      
 
      
 
      
 
 
    
 
     
     
      
   
      
 
      
     
      
      
      

 
      
 
      
 
      
      
      
      
 
      
      
      
   
      
 
 
      
      
 
      
      
      
      
      
 
      
     
      
      
 
   
     
      
      
 
     
      
   
      
      
      
   
 
      
      
      
 
 
      
      
      
      
      
      
      
      
      
 

      
      
      
      
      
      
      
      
      
      
      
    
      
    
      
      
      
      
      
      
      
      
      
   
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    
      
      
      
      
    
      
      
      
      
      
     
      
      
      
      
      
      
   
      
      
      
      
      
      
      
      
      
      
      
      
      

 wrpr all
 ait all
 b c all
 blm all
 cd  all
 cla all
 cln all
 gapa all
 hmga all
 hmgb all
 icb all
 pgm all
 rnt all
 tima all
 bp  all
 cpai all
 gr  all
 pfk all
 rhv  all
 rhv  all
 rhv  all
 rxn all
 sdha all
 sgbi all
 ts a all
 xiaa all
 cytr all
 er e all
 ldh all
 lyz all
 acn all
 cpa all
 cpp all
 cts all
 dfr all
 hir all
 tmne all
 cata all
 icd all
 rsa all
 adh all
 apia all
 apib all
 pap all
 wgaa all

 

   
  
  
   
   
   
   
   
   
   
  
   
   
   
   
  
   
   
   
   
  
  
   
  
   
   
   
   
   
   
   
   
   
   
   
  
   
   
   
   
   
   
  
   
   

      
 
      
      
 
      
      
      
   
      
      
      
      
      
      
 
      
      
      
      
 
 
      
 
   
      
      
      
      
      
      
      
      
      
      
 
      
      
      
      
      
      
 
      
      

 
      
      
      
      
      
    
      
      
      
 
   
      
      
    
 
      
      
      
      
 
     
 
      
      
      
 
      
      
    
      
    
      
 
      
 
      
      
      
      
      
      
      
      
     

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    
      
      
      
   
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
   
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

fi