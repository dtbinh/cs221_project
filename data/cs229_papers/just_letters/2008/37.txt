bagging for one class learning
david kamm
december         

 

introduction

consider the following outlier detection problem  suppose you are given an unlabeled data set and
make the assumptions that one particular class is well represented but you have no prior knowledge
on how many outliers it contains  this scenario can arise in a variety of real world applications such
as detecting intrusions in a network or spotting malignant tumors in medical images  although
constructing labels for the data is rarely impossible  in many cases  it may be cost prohibitive or
inefficient 
most outlier detection methods such as mixture of gaussians and parzen window estimators fit
a distribution to the data and classify points as outliers if the density function evaluated at that
point is below a certain threshold  these approaches work well if the data has a particular  known
distribution  since this is not often the case  a popular approach towards outlier detection is the
one class svm  oc svm  in which unlabeled data is treated as positive examples for a particular
class 
oc svm addresses the following problem  given a data set drawn from an underlying probability distribution p   how do you estimate a simple subset s such that the probability a test point
drawn from p lies outside of s is some a priori specified value between   and        the oc svm
accomplishes this by mapping the data to a corresponding feature space and finding the optimal
separating hyperplane between the data and the origin      a more geometrically intuitive approach
is support vector domain description  svdd  which estimates the minimum hypersphere enclosing the data points in feature space  the two approaches are equivalent when the gaussian kernel
is applied  so only the oc svm formulation of the classifier is considered in this paper     

 

motivation

the oc svm suffers on certain data sets since it separates outlier points off from the origin in
addition to true representatives from the class      to improve the oc svm classifier for outlier
detection  this paper applies ensemble methods  the justification behind this is that  theoretically 
they can boost the performance of any classifier  because the oc svm captures too many outliers
points in training  it can be interpreted as having an overfitting problem      bagging in particular
was an attractive choice for its simplicity to implement and since it is generally thought of as a
variance reduction technique 

 

fi 

algorithm

the unlabeled input training set is denoted as x    x i     i           m  we make the assumption that
one particular class is well represented aside from outlier points which we have no prior knowledge
on how many there are  with this assumption in mind  we can apply our one class learning approach
without worrying about the complications caused by multiple classes being represented in the data 
in the oc svm  the training data is separated from the origin in the feature space by solving the
following quadratic programming problem
m

 
  x
min wt w  
i  
w    
m
i  

 i 

s t  w  x      i   i           m
i     i           m
the corresponding dual problem can be altered to efficiently work in feature space by replacing the
dot products between training vectors with an appropriate kernel k  the parameter  is important since it is an upper bound on the percentage of outliers and a lower bound on the percentage
of support vectors      when there is prior knowledge about the number of outliers  the parameter  can be tuned to give the optimal rejection rate  which is taken advantage of in the experiments 
bagging works by resampling the data with replacement and training a classifier on the new
data set for a pre specified number of trials  a prediction on a test point by taking the majority vote of the classifiers under a certain voting scheme  the bagging algorithm is given as
follows 
   bagging oc svm 
   input  the number of classifiers n  the data set x
pm
   output  a linear combination of oc svm classifiers
i   ci
   for i     to n do
  
draw m examples  x i    y  i    uniformally from x with replacement to form training set di
  
train a oc svm classifier ci on di
  
add ci to the linear combination
   end for
aside from the free parameters regulating the oc svm  there are two factors that influence the
performance of the bagged classifiers  the voting scheme and the number of classifiers  the more
pressing decision to make is for the voting scheme  unweighted majority vote is accepted as
the standard voting scheme for bagged ensembles of classifiers and was thus implemented in the
algorithm  although there are weighted voting schemes that have been shown to perform better
than unweighted majority vote on various data sets in supervised learning  it is difficult to construct
such weightings on unlabeled data  the effect of varying the number of classifiers was addressed in
the experiments 

 

experiments

the algorithm was applied to two   d synthetic data sets where the decision boundary could be
visualized easily  two real world data sets were used to test the algorithms overall performance
 

fiand effectiveness at detecting outliers 

   

data sets

square noise  the true positives of the square data set are     points uniformly drawn from  
strips of length     and width     which can be identified in fig     the outliers were    points
drawn randomly from   x  y  x          y          and compose     of the data 
sine noise  the true positives of the sine data set are     points with x         and y  
     sin  x   the outlier points were     points drawn uniformly from the rectangle   x  y  x 
        y              and compose     of the data set 
usps handwritten digits  the training set is comprised of      images and the test set is
comprised of      images  each image is   x       pixels and is represented by a     dimensional
feature vector  the algorithms were trained on the     instances of the digit   in the training set
and tested on all of the digits in performance evaluation experiments 
breast cancer  the breast cancer dataset was obtained from the uci machine learning repository  the data is composed of benign and malignant samples with    features  a pre scaled data
set is available from the libsvm website  which was used in these experiments  for evaluating
classifier performance  the algorithms were trained on the first     benign samples and tested on
the remaining samples  which were composed of     benign samples and     malignant samples 

   

methods

in each experiment  the radial basis formulation of the gaussian kernel k x  z    exp   x  z     
was used since the data set is always separable from the origin when it is mapped into feature
space by this kernel      to make a comprehensive observation of the effect of varying the number
of classifier  the bagging oc svm algorithm was ran with varying the number of classifiers n with
values                 and      libsvm version      for matlab was used for the oc svm
implementation      on the real world data sets  performance of the classifier was measured by the
roc curves on the test data  which is a standard criterion for outlier detection 
in addition to the above experiments  the following outlier detection experiment was performed
on both of the real world data sets  the usps digit   training set was augmented with    instances of digits     drawn uniformly and with replacement  so the training set has      percent
outliers  the testing set is composed of the same    outlier digits  on the breast cancer data
set     malignant samples were taken and added to the training set corresponding to    percent
outliers  the test set is composed of the    malignant samples for the respective experiments  in
these experiments  the assumption that the proportion of outliers is unknown is again dropped to
make selection of the  parameter straightforward  for each of the outlier detection experiments
and the   d synthetic experiments  the  parameter was set to be the fraction of outliers that were
pre specified in the construction of the training set  however  without enforcing the number of
outliers the training data contains  selecting the optimal  becomes difficult 

 

results

qualitatively  the bagged oc svms resulted in a better fitting decision boundary around the target
class as opposed to the oc svm in both the sine noise and square noise experiments  although
 

fithe decision boundary is influenced by the kernel parameter  for both classifiers  the bagged ocsvm always excludes more outlier points  however  on the square noise data set decision boundary
determined by the bagged oc svm seems to exclude a few positive points on the square as well 
which was discouraging 

fig    the decision boundary of oc svm is shown in blue while the decision boundary of the
bagged oc svms is shown in dark red  in both cases  the bagged oc svms determine a more
refined boundary around the target class  in the square noise example  the kernel parameter
          in the sine noise example  the kernel parameter          in both cases  the best
number of bagged classifiers is n      
unfortunately  the results on the real world data sets were more discouraging  there is not a
noticeable difference between the curves in the usps data set  in the breast cancer data set  the
bagged oc svm slightly degrades the classifier performance 

fig    excerpts of the roc curves for the regular and bagged oc svms trained on the usps
and breast cancer data sets  in both examples  the oc svm curve is shown in blue and the bagged
oc svm curve is shown in green  the kernel parameter        in both experiments as well  the
best number of bagged classifiers is given by n      for the usps data and n      for the breast
 

ficancer data 
the outlier detection experiments did not produce any fruitful results  either  on both data
sets  the oc svm and bagged oc svm detected the same outliers in the training set when the
kernel parameter  was set to      in all of the experiments performed the optimal number of
bagged classifiers was found to be either    or     bagging over    classifiers always resulted in
performance degradation 

 

conclusions

although bagging oc svms improved the decision boundary on the   d synthetic examples  there
was no noticeable improvement on the real world data sets which are much more representative
of those that would be encountered in practice  additionally  there is an n fold increase in computational complexity when applying the bagged oc svm algorithm as opposed to oc svm  if
computational requirements are an issue  any possible improvement garnered by the bagged classifier would probably be outweighed by the computational cost  otherwise  a computationally
intensive procedure such as kernel pca as reported in      would be a better investment 

 

acknowledgements

i would like to thank the tas tom do and honglak lee for their advice  i would also like to thank
the instructor andrew ng for teaching cs     

 

references
   b  scholkopf  j c  platt  j  shawe taylor  a j  smola  r  c  williamson  estimating the
support of a high dimensional distribution  neural comptutation    
   d  tax  r  duin  support vector domain description  pattern recognition letters          
          
   h  hoffman  kernel pca for novelty detection  pattern recognition
   http   www csie ntu edu tw  cjlin libsvm 

 

fi