learning and predicting flow duration and rate
kok kiong yap  kwong aik goh and myunghwan kim
a final report for cs     fall        
december         

 

introduction

openflow is an exciting stanford technology  which enabled flow based networking with centralized control 
the architecture is intended to open up opportunities for networking  here  we explore an interesting
possibility which is to provide network monitoring and dynamic traffic engineering in network  however 
these applications require the central controller to be aware of the rate and duration of each flow  while this
information can be readily polled  it is often difficult to do so for each and every flow  given the number of
flows in a network 
in an openflow network  each flow can be identified with the source  destination  entry switch  transport
type and etcetera  this often communicates a fair amount of information on the nature of the flow  for
example  tcp traffic on port    is likely to be secure shell connections  exploiting this  this project attempts
to investigate the feasibility of learning and predicting flow rates based on the above mentioned metrics in a
network  this allows for adaptation of the prediction in accordance of the different networks  for example  an
enterprise network is likely to have different traffic combined to googles data center or stanfords network 
this project investigates the accuracy of various learning algorithms in prediction of flow rate and duration
in a network  to compare different algorithms  exact data are logged from a network and used in quick
assessments of the algorithms suitability to the problem  network data are recorded 

 

problem setup

supervised learning algorithms is applied to problem above  this is because the outputs  i e   flow rate and
duration of the training sets  are known values  which are in the space of positive real numbers 

   

data acquisition

to evaluate accuracy and efficiency of the various learning algorithms  we log over        flows in an openflow network  for a single user  the inputs associated with each flow are
   ingress port and switch data path identity
   vlan identity and type
   ethernet source and destination address
   ip source and destination address
   transport source and destination address
with these  we seek to predict the flow rate and duration of each flow  in this project  we use hold out
cross validation to assess the accuracy of each learning algorithm  here  we use        flows as the training
set while using the rest of the        flows to estimate the generalization error 
 

fioutput types
durations sec 
rates bps 

   
   
   

class  
     
     

class  
       
       

class  
      k
      k

class  
 k    k
 k    k

class  
  k     k
  k     k

class  
   k   m
   k   m

   
   
   

table    output classes

   

ouput classification

we discretize the continuous output into a few classes rather than exact values  this allows us to use more
advanced algorithms such as gda and naive bayes  we intend to discretize flow rates and durations into
the following classes respectively  note that we choose classes to discretize along a logarithm scale due to
the large range of values and the heavy tailed distribution of flow rate and duration 

 

learning algorithm

we implement and test the following learning algorithms to obtain our goals  in this section  we describe what
is the mathematical problem of each algorithm and how it is applied  we predict each output independently 
thus only for the definition  we state the problem using one output feature  y  rm where m           is the
number of training samples  then  we obtain the cross validation error by examining mcv           holdout samples 

   

linear regression lr 
m

min

  x  j 
 y
 t x j    
  j  

where x j   r n    is each training sample whose components indicate input features and an intercept
term n        every input feature is definitely a discrete class label  neither continuous nor meaningful in
a vector space  however  these features can be managed as coded values  therefore we can regard models
differently according to coding scheme  we do not gurantee that this coding scheme is the best one  but at
least we know this provides a lower bound of the prediction  then  we classify outputs based on the values 
t x i  for i              mcv  

   

generalized linear model glm 

for most of network flows  we can imagine that their durations are short and rates are low  it could be a great
guess that given network features  durations and flow rates are exponentially distributed  y x    exp   
under this assumption  we can try to fit the generalized linear model  since exp   belongs to exponential
family where      b y       t  y    y  a     log    we solve the following problem 
m

min

  x  j 
 y
 h  x j     
  j  

where h  x     t  x and input features are defined as same as section      similar concept about the input
features can be applied here  then  we classify outputs based on the values  h  x i    for i              mcv  

 

fi   

gaussian discriminant analysis gda 

another thought is that there exists a centroid for each class and farther points from the centroid is less likely
to belong to the given class  one of possible assumption is that network flows in the same class are normally
distributed  after estimating of   common covariance   k  a centroid for each class   k  probability for
each class  according to maximum likelyhood  we select a class that maximizes p  y   k x   this class is
equivalent to
 
 x  k  t    x  k    k
k
 
where input features  x  are exactly same as section      therefore the concept is still same 
arg max exp 

   

simple naive bayes snb 

we can make a rough assumption that every input feature is conditionally independent given output  y 
thus  by estimating each probability p  xi  y  for i              n  we need to solve
arg max p y 
k

n
y

p  xi  y 

i  

unlike above cases  naive bayes works indepently of the coding scheme  since this algorithm does not use
input features actual value  rather  it counts every event and estimate the probability of each feature given
outputs 

   

complex naive bayes cnb 

acutually  we cannot guarantee conditional independence described in section      it is easy to show a
counterexample  for instance  when xi and xj are respectively a source ip address and a source transport
port  p xj  y  xi   is different since different server provides different services  or  it is possible that xi does
not offer any service because xi is a client  therefore  at least  coupling ip address and transport port for
both source and destination seems necessary  the class number to choose is
arg max p y  p x    x   y  p x    x   y 
k

n
y

p  xi  y 

i  

where x    x    x    and x  respectively indicate source ip address  source transport port  destination ip
address  and destnation transport port 

   

support vector machine svm 

the basic idea of this algorithm is similar to section      however its detail process is totally different 
we do one versus all svm which partitions the problem to   independent problems and picks up the class
corresponding to the largest function value f  x    wt  x  b  since this algorithm is rooted on the geometric
distance  it is more important to define the distance between two flows than in the other algorithms  however 
the difference of class labels mentioned in section     has no meaning  for example  we cannot say that
a                is closer to a              than a                even though dist a    a      dist a    a    
a kernel method gives a great possibility to transform the input features  then we solve the following problem
for each binary classification 
pm
pm
 
 i   j 
max
i j k x i    x j   
i   i   
i j   y y
s t 
 p i  c
m
 i 
  
i   i y
p
 i 
 j 
n
where k x i    x j      exp     p     xp    xp     the computational complexity is too large for the entire
training set  we apply the randomized algorithm in  lr    
 

fi 

result

using matlab  we make a prediction and evaluate the error rate for each algorithm  the table shows their
error rates  on the other hand  figures display the error distributions  the error rate of the best fit is below
   for duration and approximately     for flow rate in terms of cv error 

figure    lr left   glm right    duration upper   flow rate lower 

figure    snb left   cnb right    duration upper   flow rate lower 

algorithm
lr
glm
snb
cnb
gda
svm

training error duration 
      
      
      
      
      
      

cv error duration 
      
      
      
      
      
      

 

training error flow rate 
      
      
      
      
      
      

cv error flow rate 
      
      
      
      
      
      

fifigure    gda left   svm right    duration upper   flow rate lower 

 

conclusion and future works

as shown in the previous section  the accuracy of duration prediction is very good  while that of rateprediction is not  unfortunately  in our data set  duration does not vary much compared to flow rate  when
the duration becomes to change a lot  we cuold not be sure that its error rate still remains low 
to evaluate how our models work well  we need to know the lower bound of error rate  bayes rate 
however  to estimate the bayes rate  statistically many samples with the same network features are required 
due to the issue  with leaving it as the future work  we see the results in the practical view 
for the flow rate  over     error is not good enough for the prediction to be useful  assuming that it
might be possible to improve the accuracy  we consider reasons why they do not work well  one possible
reason is that there exist nonlinear factors in the features  because we usually apply linear algorithms  it
may not reflect any high nonlinear factors  another reason which we can come up with is that the number
of features is not enough  for instance  if the flow rate of tcp stream depend on tcp state rather than ip
address or transport port  then our feature selection is wrong 

references
 lr    yumao lu and vwani roychowdhury  parallel randomized support vector machine  in pakdd 
     

 

fi