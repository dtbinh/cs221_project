mapping of hierarchical activation in the visual cortex
suman chakravartula  denise jones  guillaume leseur
cs    final project report  autumn      
introduction
there is much that is unknown regarding the
communication between different areas in the
brain  this is a research area of growing
interest with the realization of applying
technological advances to this field  in this
paper we will describe a method to apply
machine learning to map communication
between regions of the visual cortex  vc  
neural activity in the vc is hierarchical  and
activation due to stimuli at one area leads to
corresponding activity in specified regions of
another area      this can be illustrated by
viewing the activation in voxels of each region
over a given period of time using fmri data 
it has been shown that the signals for vision
pass through the first area  v   before entering
other areas such as v   and there is a mirrored
image of the vc on each side of the brain 
which we will refer to as rv   rv  and lv  
lv   we will treat these as separate regions of
interest  rois  in our mapping   in addition 
we ignore any possible cross wiring between
hemispheres in these regions  given the
hierarchical processing of information from
the stimulus to v  and then to v   our intent is
to build a model to confidently predict
mappings between these regions  in the
process  we also build a model to predict v 
from the stimulus 
data and methods
the fmri data for our project was provided
by professor brian wandells lab at stanford
university  it is preprocessed to remove noise
from the scanner and compensate for minor
head movements of the subject  our training
sets and test sets consist of      and     data
points respectively  for example  in the model
of stimulus to v  prediction  our training set

consists of the stimulus over      time points
and corresponding activation of a single voxel
in v   given the stimulus at a specified time
point  the response variables are the voxel
activation numbers at that time point 
we analyze the data by implementing a special
type of linear regression called lasso
regression  in addition  we implement
regression using svm and compare it with
results from lasso  we also compare our
stimulus to v  model with the pre existing
model of professor wandells lab  we then
build a model for v  to v  prediction  since
the data is very high dimensional  we
implemented principle component analysis
 pca  to reduce computational costs  by
using pca  we were also able to interpret the
parameters coherently and visualize the
mappings more clearly 
lasso regression
lasso  or  least absolute shrinkage and
selection operator   is a technique which
improves upon ordinary least squares  ols 
regression by minimizing
n

 y
i  

 i  

  t x   i                

where  is the tuning parameter  it is
implemented in a way as to shrink some   i   s
and set others to    for example  if    is the
normalization in the case of ols and we set
         then half the coefficients shrink to
zero      we choose to implement lasso for
the ease of interpreting the parameter  to
construct the mapping  we use    fold cross
validation in order to determine the best 
and then retrieve  from that model  this is
done using lars package of r     

fionce we have  for the prediction of a given
lv  voxel from the stimulus  we reconstruct
the stimulus by setting the value of each pixel i
in the stimulus equal to  i   we then threshold
the pixel values and set all pixels below the
median value to the minimum value  by doing
so  we obtain a receptive field mapping for a
specific lv  voxel as shown in fig    a   the
corresponding receptive field given by the preexisting model is shown in fig    c   by using
lasso regression  we are able to
conveniently interpret  to map the receptive
field of any given voxel  we use the same
procedure for the v  to v  model  fig    c 
shows the strongly correlating voxel group
 bright yellow  for an arbitrarily selected lv 

voxel  by this result  we can hypothesize that
this group of lv  voxels drive the selected
lv  voxel 
fig    a  and   a  show the actual and
predicted signals together for the test data 
which shows that the model predicts the voxel
activation signal within a small margin of
error  generalization errors listed in table  
also show that the model is reasonably
accurate  therefore  in addition to providing
interpretable results  the model provides a
good prediction 

fig    receptive field mapping of a lv  voxel according to parameters of  a  lasso regression
before pca   b  lasso regression after pca  c  pre existing model of professor wandells lab 

fig     prediction curve plotted together with the actual signal for a lv  voxel using the first    
time points of the test data set  according to  a  lasso regression  b  svm regression 

fimodel
stimulus    lv 
stimulus    rv 
lv     lv 
     
     
na
lasso
      
     
     
lasso after pca
     
     
na
svm
     
     
   
svm after pca
table    generalization errors obtained by averaging    distinct voxel regressions per model 
svm regression
as another method to predict activation from
the stimulus to v  or from v  to v  we also
applied a svm regression algorithm  we
decided to do so in order to have another
prediction to compare with the lasso
regression results  we chose svm for its
capacity to handle high dimensional data and
its renowned efficiency  we used the
svm light software that can be used for both
classification and regression 

pca
the facts that our data are highly dimensional 
thousands of dimensions per voxel  and that
these dimensions are highly correlated  nearby
voxels have a very similar activity and the
stimuli are simple geometric shapes  strongly
encourage the use of pca before applying the
regression algorithms  pca proved to be
useful in two ways  namely  dimensionality
reduction and coherence of mapping 

ficontrary to the stimulus  it is not possible to
downsample v  data without losing much
information  the computational cost of
predicting the activation in a whole area from
the whole stimulus is so high  that reducing the
input dimension is a necessity  upon
performing pca and plotting the decrease in
component variance  we find that it drops very
close to zero after about     pcs  with this
    reduction in dimensionality  the
computational cost of each regression lowers
drastically  previous figures show that we still
achieve very efficient prediction within a
reasonable period of time 
fig    a  and table   show that there is some
loss in predictability after pca for stimulus to
v  prediction  particularly for the lasso
regression  however  the receptive field
mapping is substantially more coherent and
accurate compared with existing models as
shown in fig    b   this is because these
nearby voxels have a similar behavior and
therefore they have a close decomposition in
the new basis  the principal component basis  
this improves the coherence of the mapping
and its visualization 
results
we have results for the two types of
predictions and mapping we worked with 
stimulus to v  and v  to v   for these two
types of predictions we followed a very similar
method  for the stimulus to v  mappings  we
had an existing model with which we could
compare  we used this to reinforce the
confidence in the results of our method  as
there is no existing model for the v  to v 
mappings 
estimation of error 
in order to measure the correctness of our
predictions  we chose to use the average of the
sum of squared errors  this method allows us
to compare the predictions of each method in
order to know how they perform and to what

extent their results are reliable  the numbers
given in table   have been obtained with the
predictions on the test set  two hundreds time
points per voxel  and have also been averaged
on several different voxels  these numbers
have to be analyzed together with prediction
curves and the mapping 
stimulus to v  
results for the prediction of an lv  voxels
activation from the visual stimulus can be
found in the two first columns of table    it
shows that these predictions are quite accurate 
the svm predictions tend to outperform
lasso predictions even if they are close for
the full stimulus  fig    a  is a good error
analysis example of why the error of lasso
with pca is significantly higher  while the
mapping is still correct  it appears that it tends
to amplify the actual variations of the signal 
however the way the signal varies is still
accurate  svm predictions lose less accuracy
when non crucial information is removed 
these results give us the possibility to detect
the receptive field of each voxel  fig      this
part is particularly important as we can then
compare our results with the receptive fields
given by professor wandell s lab  the strong
correspondence between our results and the
labs results reinforces the accuracy of the
prediction and indicates that the method we
followed is efficiently solving the problem 
v  to v  
results for v  to v  predictions can be found
in the third column of the table  these results
are for our prediction with the     principal
components of v   but we tried several
numbers and the results seem very robust to
this parameter 
again  svm predictions are slightly better
than lasso predictions  but they are very
close  these errors are low and show a very
accurate prediction of v  activation with v 
information  this can also be seen in fig  

fiprediction curves on which one can see that
predictions are very rarely far from the actual
signal 
it also appears that we are able to predict far
more precisely v  activation from v  than v 
activation from the signal  this result was
absolutely not obvious a priori as the data we
have on the stimulus signal are perfect  but
fmri data of v  activation are far from this
resolution  one possible explanation for the
lower results for the stimulus to v  predictions
could be due to what is determined to be the
true stimulus  we assume that the stimulus is
limited only to the image that the subject sees 
but there may be other factors that contribute
to what the patient actually sees 
these good results  as well as the confidence
we can have in the method as a result of
verifying the stimulus to v  predictions with a
given model  enable us to give an accurate
activation mapping for v  voxels  this
mapping which can be seen in fig   c achieves
our main goal 
conclusion
we were able to build successful models for
both stimulus to v  prediction and v  to v 
prediction  by implementing machine learning
algorithms  we were able to develop accurate
models  these results will be useful to
neuroscientists in understanding the relations
between these two regions of the visual cortex 
these methods could be generalized to
determine mappings in other regions of the
visual cortex as well as elsewhere in the
brain 
acknowledgments
we acknowledge jonathan winawer and brian
wandell from the psychology department at
stanford university for providing the data 
comparative model  computing power and
guiding us through this project 

references
    brian a  wandell  foundations of
vision  pp          
    tibshirani  r          regression
shrinkage and selection via the lasso  j  r 
statis  soc  b   vol      no     pp          
    http   cran r project org 

fi