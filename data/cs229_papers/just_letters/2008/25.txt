identifying epilepsy using mris of the hippocampus
andrew duchi
        
background 
temporal lobe epilepsy is a form of epilepsy rooted in problems in the temporal region
of the brain  specifically the amygdala and hippocampus  among these cases  approximately
    manifest mesial temporal sclerosis  a loss of neuron cells in the hippocampus  evidence
of this can often be observed by the shrinking of different regions of the hippocampus and
changing of intensity of regions as viewed in mris 
the goal of this project will be to create an algorithm that can identify pathological and
non pathological hippocampi by analyzing coronal mris 
the model 
in this study we will use a bag of words model  which previous studies have shown to
be a simple  yet effective  approach to scene and object classification        the basics of this
method are a decomposition of an image into a certain set of visual features  which we will call
keywords  these keywords can be used to create a codebook or dictionary  just as in text
analysis  with such a codebook  images can be classified by analyzing which keywords  and
how many of each  are included in a given image 
this study will apply this approach to see if it can make finer differentiations than
overall object or scene identification  specifically  we will attempt to use the bag of words
approach to differentiate a given object type  hippocampus  into two classes  healthy and
unhealthy  
the data 
in this study we had    coronal mris     from patients with temporal lobe epilepsy and
   normal control mris  first  all    slices given by each mri were reviewed and the slice
in which the body of the two hippocampi had the greatest contrast to the rest of the brain tissue
was selected  next  all    of these slices were cropped into two    x    pixel images  a
square around the right hippocampus and a square around the left hippocampus  initial results
with this data were poor and it was noted that the hippocampi generally occupied only a small
portion of the    x    image  thus  all images were recropped to a size of   x   pixels  a
smaller size that still included the entirety of each hippocampus  to reduce the amount of noise
contributed by information taken from non hippocampal regions of the brain 
features and keywords 
the first set of feature used were generated using the scale invariant feature transform
algorithm  sift   the implementation used was created by andrea vedaldi of oxford
university  these features were used because they have been effective in previous studies for
object identification     additionally  these local features are relatively resilient against changes
in illumination  minor viewpoint change  and random noise    
the sift algorithm was run on each of the right and left hippocampus images 
generating a total of      features for the left hippocampi and      features for the right
hippocampi  these features were then translated into keywords by running the k means

fialgorithm on the data for clusterings of                     and     keywords for each of the
right and left  each detected feature was then assigned the number of the closest keyword and
the number of times each keyword occurred for a given image was recorded to give the features
describing the image 
unfortunately  preliminary analysis suggested that the sift features did not results in
good performance for classification  thus  another set of features were selected  specifically 
local patches of the images  as were used in a study by l  fei fei and p  perona     the sizes
used were   by   pixel    by   pixel  and    by    pixel patches  these patches were gathered
in two ways 
   evenly spaced patches of each size were gathered from all images
      randomly located patches of each size were selected from each image
these two methods were used in an attempt to ensure that the entirety of each image was
considered  evenly spaced   while not unintentionally creating patterns among the patches or
leaving out a significant pattern that might fall across a border of patches in the grid  random  
these patches gave       x  pixel patches        x  pixel
patches  and        x   pixel patches for the right hippocampi and for
the left hippocampi               and      total   to eliminate noise
cause by differences in brightness  as some mris were generally lighter
or darker than others  the patches were greyscale normalized to values
from   to      following this  sets of keywords were extracted from
each patch set using the k means algorithm  grouping the features into
clusterings of               to     keywords  an example of one such
codebook is shown at the right  it is the codebook of    keywords for
  x   patches 
the algorithm 
a standard nave bayes algorithm using laplace smoothing was trained on the data and
used for classification  given the many different potential codebook sizes  a derivation of the kfold cross validation was used to select the best codebook for training  specifically  the positive
and negative training examples were each split into three approximately equal sized groups 
the algorithm was then trained on the   possible permutations in which a group from each of
the positive and negative examples was withheld for testing  testing was done in this manner to
maintain the ratio of positive to negative examples  the codebook size with the best average
estimated generalization error was then chosen and used for training on the entire training set 
after the nave bayes algorithm had been trained on this training set  it was tested on
    of the data  which was withheld from the training set  as with the k folds cross validation 
the ratio of positive to negative examples was maintained between the training set and test set 
classifying an unknown image 
to classify an unknown image  one of two approaches is taken depending on the features
we are using to classify 
 sift  sift features are extracted from the image and each feature is matched to the
keyword in the codebook that best approximates that feature
 patches  we iterate pixel by pixel over every possible patch of the appropriate size in the

fiimage  each patch is then matched to the best approximating keyword from the
codebook and thus counts for the keywords in a given image are realized 
the trained nave bayes classifier is then used to classify the unknown image 
results 
sift feature results 
average error      random iterations 
 average training set error       
 average generalization error        
to the right is a graph showing the change in
training and test set error for the nave bayes
algorithm trained on the codebook of    test
features  for each side   all codebooks had
similar error graphs for the sift features 
discussion of sift 
the training set error was relatively
low for the sift features  but the
generalization error was unacceptably high  in fact  if we were simply to classify the training
set according to which of the classes was more frequent  we would have achieved only    
expected generalization error  thus  the sift features appear to actually be worse than random 
combining this with the fact that the generalization error did not decrease with increases
of training set size after    training examples  while the training set error continued to increase 
seems to indicate that the sift features were not giving a signal indicating whether or not the
hippocampus had mts 
patches results 
average error      random iterations 
type of descriptor
predicted generalization error

training set error

 x  patches

     

     

 x  patches

     

     

  x   patches

     

     

all patches

     

     

fidiscussion of patches results 
the average estimated generalization error for any of the patch features was better than
the sift features  additionally  it was better than the       error boundary which would have
been attained simply by always guessing that the hippocampus was from a patient with tle 
unfortunately  the estimated general error was rather high  at best        as was the
error on the training set for each set of patches  first  let us address the high generalization
error 
graphs of the expected generalization  as in the results section  seem to indicate that we
had a case of relatively high variance  this could be helped by addition of more training
examples  as the generalization error does not seem to have converged  unfortunately  attaining
more training examples was not possible in this study   another possibility would be to use a
smaller set of features  this will be addressed later 
we have an idea that increasing the number of training examples would improve the
classifier  however  given how high the training set error was  we might consider this a case of
high bias were we to have more training examples  this would imply two solutions  a larger
feature set and or using different features 
the concept of a different sized feature set was addressed by investigating how often
each size of codebook was used for classification with each set of patches  below are the
histograms for the  x   left  and  x  patches  right  

the histogram for the  x  patches seems to indicate that a larger training set may be needed 
however  preliminary testing with     cluster and     cluster training sets actually gave slightly
worse expected generalization errors        and        with only moderately better training set
error        and        over    random trials  the  x  patches on the other hand seem to be
well represented by the given cluster sizes  and the   x   patches had even fewer examples than
the  x  case and also had a median in the middle of our choices for clusterings 
thus  it seems that in order to get better classification of hippocampi would require a
different set of features  this makes sense as one major feature completely ignored by this
algorithm is spatial relationships within images  which may be important as the signs of tlemts are primarily manifested in the ca  and ca  areas of a hippocampus  and these two areas
do have a consistent anatomical spatial relationship that might help to identify a hippocampus as
pathological or not 

fifinally  an roc curve is given  left  for the
algorithm trained on the combination of all
patches  it was generated by running the
algorithm on     randomly generated      
splits of the data and testing with different
classification thresholds  the area under the
roc curve was        which was better than
the area under the roc curve for any of the
individual patch sizes  which had areas      
for  x         for  x  and       for   x   

conclusion 
the sift features were uninformative for deciding whether a hippocampus was
suffering from tle mts  the patches on the other hand did enable us to make a better than
random guess about the state of the hippocampus  additionally  the best decision was reached
by using the information from a combination of all patch types 
this indicates that a bag of words approach is viable for this type of classification 
however  new features considering other aspects of the image  perhaps emphasizing spatial
relationships  would be needed to attain acceptable levels of predicted generalization error 
additionally  the training set used in this study was not large enough for the algorithm to fully
converge 
references 
    g  csurka  c  dance  l  fan  j  willamowski  and c  bray  visual categorization with
bags of keypoints  in eccv workshop on statistical learning in computer vision       
    l  fei fei and p  perona  a bayesian hierarchical model for learning natural scene
categories  in ieee conference on computer vision and pattern recognition  volume   
pages              
    k  mikolajczyk and c  schmid  a performance evaluation of local descriptors  in ieee
transactions on pattern analysis and machine learning  volume     pages           
     
    j  zhang  m  marszalek  s  lazebnik  and c  schmid  local features and kernels for
classification of texture and object categories  a comprehensive study  international
journal of computer vision  volume     pages         
acknowledgements
i would like to thank geremy heitz of the stanford computer science department for
his advising and direction throughout this project  i would also like to thank ian cheong and
susanne mueller of the university of california san francisco for providing the data set 

fi