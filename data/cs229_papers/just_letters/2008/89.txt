video montage   video abstraction
i ting fang

 

introduction

the amount of video has rapidly increased in recent years
years  and therefore how to extract useful
video content and retrieve video from huge database is a crucial issue   video sharing websites
now use metadata to describe videos  while there might be so many scenes contained in single
video  the information that tags reveal could be limited  furthermore  there might be chances
that only a small portion in a video that users are interested in  while some useful and relevant
information across videos  considering those situation
situations  its then important to describe the
th
content
nt directly from videos and be able to collect those users interested shots 
shots before the
shots collection we need to first know users preference  and thus an overview of video
contents becomes necessary which is th
the main topic of this project  video abstraction 
abstraction see
figure    a   in fact  above description is just one of applications of video abstraction  the
technique can be applied
plied in facilitating browsing video database back and forth between
joined interesting videoss and original videos
videos  and to save the bandwidth as well 
well
the relevant researches     in video abstraction are mostly done for single video  how to
shorten the video while containing as much information as possible and how to extract the
highlight of a video  in this project  in the contrast  we try to give a content overview of
several videos aiming for providing a novel way to browse  search and create personal videos 
our system  figure    b   first describes frames by their color and spatial structure features 
and extract shots from videos if they remain consistent for certain amount of time  after
representing each shot by three of its frames and rescaling feature coordinates using trained
weighting vector  we then separate them into clusters where similar shots are supposed to fall
in the same cluster  since we have no idea what scenes would look like in feature space 
space and
how they would differ from each other  we apply clustering algorithm k
k means
means to data trying
to find appropriate boundaries  also
also  relying on the
he strong relation among frames from the
same shot  we can automatically decide the best number of cluster
clusters  finally  users
u
will be able
to browse clustered shots and select interesting ones  the
he resulting video is joined by users
preference and along with our smooth shot transition scheme 

 a 

 b 

figure    figure  a  illustrates the concept of the purpose of video abs
abstraction 
traction  by showing clustered
shots   users can have a big picture of video contents  figure  b  shows the flow chart of our system 
system
dash arrows are our future work

fi 

feature selection and distance computation

to automatically describe the video content  we need to employ low level features  different
types of features are reported to have good performance on the video abstraction task  such as
text  visual  audio and motion  these features are chosen according to the application  in this
project  we make our effort on global visual information  hsv color histogram and spatial
envelope 
  

hsv color histogram
hsv color histogram and other modified versions have been widely used in this area 
and could be discriminated in different scenes  to compute the histogram  the hsv
space is uniformly quantized into a total of     bins which includes    levels in h  four
levels in s  and four levels in v  in order to make them work with the other feature and
be more efficient  the values are rescaled and quantized nonlinearly  giving higher
significance to the small values with higher probability  figure   shows that if we only
scale the histogram values  the distance of two frames is still dominated mostly by color
difference  thus  we further apply nonlinear quantization to eliminate high variances of
color histogram 

 a 

 b 

figure    combined feature vector  first part is spatial feature and second part is color feature 
 a  scaled color histogram   b  scaled and nonlinear quantized color histogram 
  

spatial envelope
spatial envelope was originally developed for scene recognition  in      the authors use
it to distinguish different secne categories for both natural scenes and man made scenes 
it reveals information about openness  expansion  naturalness and roughness of space
which are very suitable for shots clustering  by applying different scale and oriented
gabor like filters onto images  the spatial envelope is represented by the relationship
between the outlines of the surfaces and their properties including the inner textured
pattern generated by windows  trees  cars  people etc  here we use three scales     filters
in total and partition filtered results into  x  blocks  the dimension of spatial envelope
is then      along with color histogram  each frame is represented by a     dimensions
vector 

distance computation
there exist several metrics to compute distance  in order to find the best solution for our
compound feature vectors  spatial envelope and color histogram  we try to measure the
correlation between the ground truth and distances computed by different methods  mainly
euclidean distance and cityblock distance  the ground truth here is generated by manually
labeling   or   indicating similarity  for example  a               b                  
groundtruth             we would conclude that sequence b has higher correlation with the
ground truth than a  based on this scenario  we found correlations are     and     respectively 

fiwhere cityblock metrics has stronger correlation with ground truth  this could be
understandable since the distance of histograms computed in this way makes more sense than
in euclidean distance while two metrics dont differ so much for spatial envelope 

 

shots generation and representation

videos are often composed of several chunks  and viewers may distinguish chunks from
chunks by detecting scene changes  based on this observation  we first split videos into shots
before clustering  intuitively  we record those frames with significant scene changes as salient
frames  and examine the duration between two salient frames to see how long the shot
remains consistent  if the duration passes the minimum length we set  the shot is kept 
otherwise its ignored 
the frames might be slightly different within one shot  but we still expect all frames should
fall in the same cluster  since using all frames to represent one shot will cost too much
computation  and using average of one shot will  on the other hand  lose information  we
therefore extract three frames from a shot  and because of the strong relation among those
three frames  it can later help in clustering 

 

clustering

the original distance d is computed as the sum of absolute differences  d sum  x  x     and its
correlation with ground truth is around     which is not high enough  and our experiments
also show that it can only achieve average     accuracy when clustering  thus we try to use
some labeled data and train the weighting vector which gives weights to features to improve
accuracy 
weighting vector learning
instead of directly using ground truth         where   indicates similar shots and thus distance
is zero  and dissimilar for    as our objective distance  we modified the distance by setting
d     d if x  and x  are similar  and d d otherwise  this will preserve original distance in
some sense while increase the correlation with ground truth to      using this modified
distance  we try to learn w such that  x  x   w is close to d for m frame pairs  with restriction
that wi must be greater or equal to zero  that is to solve aw d s t  w    or minimize
    wtataw  atd tw  s t  w    where



a   



r r
x   x   
r r 
x   x     a  r mn   w  r n    d   r m 

m


applying weighting vector is just like rescaling the coordinates of the feature space such that
data can be clustered better  given learned weighting vector  and since wi     x  wi  x  wi
   x  x   wi  we can directly multiply it into original feature space  and do clustering 
deciding the number of clusters k
in most cases of application  users dont know how many types of shots in the targeting
videos  so we develop a way trying to automatically choose the best k  at the beginning of
clustering  we set k to be a large number and separate shots into k clusters  and then using
the information mentioned in shots representation that three frames from one shot should fall
in the same cluster  we record the ratio of the number of shots that frames fall in cluster i also
fall in cluster j and the number of shots in cluster i in clusters relation table r i  j   if r i  j 
is high  we know that there is strong relation between them  and thus combine them into one
and set k k    same procedures continue until no more high r i  j  or reaching the minimal
k we set 

fiafter finishing iterative
erative clustering  for those shots that three frames still fall in different
clusters  we can keep the portion that most frames fall in the same cluster and ignore the other
or generally ignore entire shot
shot 

 

experiment and results discussion

database
in order to have videos that are close to reality  w
wee collect our videos from youtube by keying
the key words and randomly selecting
selecting  there are five categories of video in our database 
tennis  basketball  surfing  snowboard
snowboarding and news  each of them contains ten or more videos
and is totally around    minutes long  to train the weighting vector  we need to label frames 
however the videos we downloaded are polluted  meaning that although some frames appear
in a certain type of video  they
ey might not visually
sually belong to that category  for example  there
are people talking in snowboarding videos  tennis players walking to
toward their seat in tennis
video and so on  therefore  w
we manually join around    minutes clean shots and use first    
frames of them for training and remaining part for testing which contains around      frames 
frames
as shown in figure   

figure    black bars represent polluted shots  the bottom   manually extracted videos are used for
training and testing  while eventually it is videos with polluted shots that need to be clustered 
results and discussion
within      frames      from each category  we randomly select two frames and record their
difference  modified distance and ground truth for      to       times  and learn weighting
vector from those training data
data  the training and test error rates are shown in figure   
  where
dash lines represent clustering error for training set and test set without using weighting
vector  the error rate of clustering      frames used for training
g slowly decrease to nearly
     while the test error doesn
doesnt change too much as we increase training data  this might be
explained as the fact that there are obvious differences even within one clean video  for
instance  people ski both in mountain and artificial skiing resort  and tennis courts look very
different in u s  open and wimbledon  if they are not contained in training set  we
w can hardly
put them into the same cluster even we increase training data 
using the scenario described in section   and applying the learned weighting vector  we
classify original polluted videos and obtain   final clustered videos  for each cluster  although
misclassified shots exist  we can easily tell what the theme is as in table    for news cluster 
most of the     that dontt belong to news category are essentially close
close ups
ups of people  see
figure   a   so  even if there are only     that are exact news shots  the whole clustered
video is visually consistent  for surfing cluster  most of miscl
misclassified
assified shots are snowboarding
which share common low level ffeatures with surfing as we can see in figure   b  
  b  and cluster
  is what we didntt train with  but it automatically extracts all the shots displayed in
widescreen 

fi   
 

theme

correct  total

accuracy

 

snowboarding

          

   

   

 

widescreen  black bars at top and bottom 

   

 

basketball

            

   

 

 

news

            

   

 

tennis

            

   

 

surfing

            

   

error rate

   

 

 
 
 
  
training data size  k 

  

figure    red for training and blue for test error
rate  the dash lines are error rates of training
and test set without using weighting vector 

table    clustering results 

 a 
 b 
figure     a  the left shot  close up of people from snowboarding video  falls in the news
cluster  the right one   b  snowboarding and surfing images share common low level features 
thus in the surfing cluster  up to     are snowboarding shots 

 

conclusion and future work

so far  two kinds of features work well in current database  especially spatial envelope which
can distinguish major scene prospect  but there are still problems that current features cant
solve  to apply to more general videos  we will need more features like motion  audio
information and even object detectors  video abstraction is only the first step of many
applications  well use clustered results to further develop tools for video searching  browsing
and creating  figure   b  shows one of them 

 

acknowledgements

special thanks to media group leading by ashutosh saxena for providing precious opinion 
this project is cooperated with video montage by abhishek gupta and shakti sinha 

reference
    oliva  a     torralba  a          modeling the shape of the scene  a holistic
representation of the spatial envelope  international journal of computer vision         pp 
        
    truongtuba    venkateshsvetha          video abstraction  a systematic
review and classification  acm transactions on multimedia computing  communications
and applications    vol     no     article    

fi