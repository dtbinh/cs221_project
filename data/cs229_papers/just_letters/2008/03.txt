recovering the multitrack  semi supervised separation of
polyphonic recorded music
sasen cain and jennifer dolson
cs    final paper
december         

 

introduction

the goal of this project is the extraction of vocals and or specific instruments from a polyphonic cd quality
music recording  instrument separation is difficult because each song can be thought of as a weighted mixture
of different signals  each which vary based on the note being played  the timbre  characteristic sound  of
a given instrument  and the recording conditions  before we can assign parts of the signal to a particular
instrument  we must devise a generalized model of the instrument s  of interest  finding and using this
acoustic fingerprint involves both a clustering problem and a recognition problem  the acoustic model
that we use is called an average harmonic structure  or ahs 
the approaches of previous work in this area fall into the categories of supervised and unsupervised
methods  we hope to build upon the state of the art unsupervised method      altering a few key steps in
an effort to improve the final result  our method reduces error by prompting the user for input at crucial
stages  and also by replacing some complex and error prone procedures 

   

vocal separation

by convention in professional music recordings  vocals can be extracted by subtracting the left channel from
the right channel      this approach  while simple  has major shortfalls  other tracks  such as percussion and
bass  might also be removed because they share the center position of the mix with the vocals   any use
of reverb on the vocal track introduces artifacts  since the it causes vocals to bleed outside of the center
band  furthermore  differential loss of quality caused by algorithms that process the channels separately
 e g   mp  encoding  hampers perfect vocal cancellation 

   

instrument separation

there are many different ways to approach the problem of extracting a given instrument from a polyphonic
audio mix  our method builds directly on recent work by duan  et al      which achieves impressive results
using unsupervised learning  the authors showed that their methods produce cleaner extraction results than
other unsupervised methods such as pure ica for extracting all possible parts of an audio signal  however 
their algorithm cannot process chords or sources playing more than an octave above the lowest frequency
instrument 
a variety of supervised approaches have also been studied  some methods  such as      attempt to learn
statistical information about different instruments  using either hmms or other statistical models based on
priors from outside training sources  in      the authors use a training set of multiple similar songs in order to
extract vocals from a single song  and     takes user labeled signatures of drums  but only classifies portions
of the signal using svm as drums or not drums  in     and      models are learned based on example
recordings of each separate source independently  or on a database of isolated notes  the work of bay  et
  this

centering is the reason why the subtraction trick works at all 

 

fial      is most similar to what we aim to do  in that it uses harmonic information about a signal  however 
it requires that the fundamental frequency of each note be known  we would prefer to avoid complex
instrument modelling  note transcription  or special casing of vocals because of they lessen the generality of
the procedure 

 

our learning algorithms

in examining the composition of the mixed input signal  we rely on the knowledge that a note is not just a
single frequency  but rather excites a fundamental frequency  f    and its harmonics at integer multiples of
f    for a harmonic instrument  the normalized amplitude profile of overtones is fairly stable  independent
of pitch or volume  this amplitude profile  shown schematically in figure  b  defines the timbre  or general
tonal quality of an instrument      
our algorithm can be broken into two sections  ahs estimation and instrument extraction  in the ahs
estimation stage  we make the underlying assumption  following      that each harmonic instrument present
has a distinct and consistent timbre  instrument extraction uses these learned ahses as feature vectors to
classify and reconstruct each instrument 

   
     

ahs estimation
processing the initial query

in the first stage of the ahs estimation pipeline  we present the user with a small segment of the song  less
than   seconds  and ask the user to specify the number of harmonic instruments present  we then slice the
segment into smaller      sample windows and use the short time fourier transform     to perform peak
detection  as in            within each window  fig   a   each peak  is then treated as a potential f    and
its appropriate harmonic values are found  using the amplitude of the stft at the first    multiples of that
peaks frequency  to ensure volume invariance  these values are normalized by the amplitude of the first
peak  forming a harmonic structure vector  b  the ahs defined in     is simply the average over all windows
of b  if our assumption about the consistency and distinctness of each instruments harmonic structure
holds  then clusters should form in this r   space based around the ahs of each instrument  noise  or peaks
that were not actually f  s should scatter in the background  we use k means  with k as specified by the
user  to find these cluster centroids and they serve as our estimates of the ahses of the k instruments in
that window 
     

subsequent queries

the user is asked to report the number of instruments in various randomly selected regions of the song  in the
first query  all of the harmonic structures found by k means can be added to the list of unique instruments 
but in subsequent queries  fewer of the instruments will be novel  to avoid replicating instruments  we check
whether a set of ahses is linearly independent using svd  each ahs returned by k means is added to the
set of previous  unique  ahses  and the condition number     h  l is calculated  if the lowest singular
value is approximately   orders of magnitude smaller than the largest singular value  then this new ahs is
discarded because it represents an instrument whose ahs has already been found 
this procedure additionally serves to validate the first pipelines results  if repeated instruments are not
removed  then there is a failure within peak detection or k means  we find that  on average  instruments
repeated in subsequent queries are not added to the list of unique ahses  adding credence to the functionality
of the system weve described thus far 
  typically 

     peaks are identified in each window

 

fifigure    feature extraction from the frequency domain   a  the stft contains many peaks  some of which
are f  s   b  an ahs discards its frequency information  preserving only a ratio of amplitudes for the first
   harmonics 

   

instrument extraction

even given accurate ahses for each instrument  the problem of creating a signal containing only the extracted
instrument is still formidable  for each time window  we must decide which instruments are present and at
what frequences  i e   what notes are being played   and finally reconstruct the desired signal 
     

peak matching

in any time window  we assume that the stft is comprised of a linear combination of the ahses  plus
noise from non harmonic sources  intuitively  this means that some instruments are playing at particular
volumes  and some are silent  if we knew the f  s played by each instrument in that slice  then we could
easily recover the mixing volume levels using the standard least squares formulation  y   ax  y  r     is
the observed vector of amplitude peaks  for n source instruments  a  r    n would contain each ahs 
shifted to frequency space by its associated f    arranged in columns  it is known to be full rank because
of the conditioning check performed on the related ahs matrix  r  n   at the end of the first pipeline 
x  rn contains the volume level of each source 
the optimal f i must be found by matching the set of observed peaks  p   to the ahs of the ith instrument 
the optimization in equation   minimizes the resulting error between the observed and reconstructed peaks 
due to the combinatorial possibilities of the problem  it would take exponential time to exhaustively compute
f    so a heuristic is needed 
f    min   y  a f   x f      
f  p

   

gradient descent  a dead end
we considered using gradient descent to minimze the error function  unfortunately  this results in an update
rule for f t that doesnt make any sense because there is no constraint that the computed f t    p   in fact 
this points out that the optimization isnt even convex since p is a discontinuous set of empirically derived
points 

 

fifigure    time domain results  the first portion of the signal contains all instruments  while the second has
excluded the guitars 
ideas from reinforcement learning
to accelerate peak matching  we dropped the requirement that y   ax and chose instead to search through
the song  looking for time frames whose stft peaks were most consistent with the ahs of a particular
instrument  this greedy approach uses a reward function to determine consistency based on the stft
amplitudes at peak positions  for p  p   i               p    r                 


if  stft pi     br  f 
 
   
r       if  stft pi      br  f   tol


 
otherwise
simply put  if a frame has peaks of the right amplitude  or greater  due to contructive interference  at
the right places for a particular ahs given some f    then the value function is highly rewarded  if the peaks
are present but lower than expected  within some ratiometric tolerance  a moderate reward is received  if
the peaks are absent or too low amplitude  there is no reward  this idea  borrowed from reinforcement
learning  allows us to quickly estimate the f  of an instrument throughout the song by maximizing a value
function over possible f  s  frames with sufficiently high value are said to contain the instrument  and if the
instrument is not present during a particular segment  it should receive very little reward  this procedure
yields a list of frames that contain an instrument and the associated f   p  
     

final instrument extraction

the extraction algorithm was fairly simple and imperfect  but a quick and dirty method was needed so that
we could focus on the learning steps  for each instrument  the frame numbers that contained it were assigned
whatever values were at the right harmonics in the full signal stft  c  i e   not  stft   r    and the
rest was set to zero  the inverse stft of this set of frames is the extracted signal  the opposite actions
were applied for the removed signal  the relevant harmonics in the right frames were set to zero while the
rest of the original stft was preserved  the inverse stft of the signal with all harmonic sources removed
yields the non harmonic sources  including the vocals 

 

results and complicating factors

using k means in the clustering step makes our algorithm succeptible to local minima and maxima  therfore 
good results were only found after multiple trials of our algorithm  in our best result  the clustering and
cluster initialization worked such that we were able to estimate the mean of the undistorted guitar in one of
our test songs  and extract that instrument from the mix  fig      given the simplifications made for the
extraction steps  it was difficult not to extract other instruments or vocals that coincide with the frequencies
of the guitar  so our extraction was not as clean as we would have liked  however  our ability to model
and extract a multi string instrument playing chords without any specific training or priors represents an
improvement over previous work 

 

fi 

future work

a persistent complication in our work has been that various forms of learning and estimation are applied
at each step of the process  any errors in early stages  particularly peak detection or clustering  will be
compounded by later stages  making it difficult to track down errors or determine how best to modify our
approach  we are also unsure of how to quantify success rates  which would be helpful to know given that
different kinds of instruments have very different structures  one solution is to obtain multitrack recordings 
mix them into a single track using arbitrary parameters  and then quantify our ability to recover those
parameters  another possibility is to better label the presence or absence of an instrument throughout
the song based on user response  and to then ask the user to confirm correct labelling before attempting
extraction  furthermore  some sort of smoothing or kalman filtering technique that takes time dependence
into account would hugely improve the playback quality of the separated tracks 

 

conclusions

the applications of an instrument vocal extraction system could range from allowing musicians to better
hear the different parts of the music they must learn to creating tracks for karaoke or rhythm based video
games  e g   rock band   furthermore  artists would be able to remix  sample  or mash up songs without
having access to the original recording tracks  we recognize that  due to the supervision required of the
user  our system will not be useful for realtime  automatic instrument separation  nor will it be practical for
processing a library of thousands of songs  however  this is not a major limitation  in the context of these
applications  it is not too tedious for a user to have to answer questions about a target song because he or
she will be concerned with the quality of separation over the quantity of audio processed  also  our method
removes the need for having a large training database of individual instruments or similar songs 
a shortcoming of our method is that it requires the user to be sufficiently knowledgeable about the
music he or she is working on to correctly estimate the number of instruments present in each listening
segment  though our method is not perfect  we believe its ability to deal with string instruments represents
an improvement over previous work  its failings are reasonable  and may be overcome by pursuing some of
the avenues described in section   

references
    winer  e  the truth about vocal eliminators  prorec  the online audio magazine  august        available
online  http   www ethanwiner com novocals html 
    zhiyao duan  yungang zhang  changshui zhang  zhenwei shi  unsupervised single channel music source
separation by average harmonic structure modeling  audio  speech  and language processing  ieee transactions on   vol     no    pp         may      
    j  hersheya and m  casey  audio visual sound separation via hidden markov models  in proc  nips       
pp           
    s  vembu and s  baumann  separation of vocal from polyphonic audio recordings  in proc  ismir        pp 
       
    m  helen and t  virtanen  separation of drums from polyphonic music using non negative matrix factorization
and support vector machine  in proc  eusipco  istanbul  turkey        cd rom 
    l  benaroya  f  bimbot  and r  gribonval  audio source separation with a single sensor  ieee trans  audio 
speech  lang  process   vol      no     pp          jan       
    e  vincent  musical source separation using time frequency source priors  ieee trans  audio  speech  lang  
process   vol      no     pp        jan       
    m  bay and j  w  beauchamp  harmonic source separation using pre stored spectra  in proc  ica        pp 
       
    j o  smith  spectral audio signal processing  october      draft  http   ccrma stanford edu  jos sasp  
online book  accessed november       
     j o  smith and x  serra  parshl  an analysis synthesis program for non harmonic sounds based on a sinusoidal representation  in proc  icmc        pp         
     d j  levitin  this is your brain on music  the science of a human obsession  penguin books ltd        

 

fi