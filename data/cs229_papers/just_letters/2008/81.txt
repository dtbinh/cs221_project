semantic classification of email

data
the data is comprised of a pool of   million emails from the enron corporation that were
made available to the public  from these  n      emails were tagged with semantic
labels  the labels are in the domain of meetings 
each email can have multiple labels  this makes the classification problem more
interesting  the idea is to try to retrieve correctly all the labels assigned to a particular
email  most of the emails transcribed are labeled else  leaving relatively few classes with
enough training samples to produce significant results 
addagenda

addagendaitems

addattendees

attend

cancelactive

 

 

   

   

 

cancelpassive

  
table    number of emails containing each label 

else

    

organize

preparematerials

removeattendees

requestinfo

reschedule

  

 

  

 

 

i left out the classes with less than   training examples  add agenda  add agenda items 
cancel active  prepare materials and request info 
the data was split into train and test sets with     and     of the data respectively 
since there are some classes with few positive samples i used   fold cross validation on
the training data to tune parameters 

feature extraction
the first step is to extract relevant features from the      emails  the header of
the emails was removed  except for the subject  line which contains relevant
information about that can help in the classification problem  the rest of the header
contains information like from  that might improve training error but will not
generalize to a different test set 
the training feature is the frequency of the word in the email normalized by the total
number of words in the email  a dictionary of features is created on the training data  this
dictionary is used in both training and testing  oovs are ignored 
pre processing
the body of the email was pre processed to parse strings for dates  times numbers and
phone numbers into generic tags   date    time    month    year    number   and
 extension   all capitalization and punctuation was also removed 

training
i trained   svm classifiers  one for each label  the classifier used was svm light     

fisvm regularization parameter c was tuned  using   fold cross validation  on the training
set  individually for each class  on each new training run  as well as the threshold t  the
weight on the errors of positive samples j was set for each class individually 

to tune c i retrained the models on     of the training data and tested on the remaining
    for each of the three folds  and chose the c that maximized the average f score of
the three folds 
svm light computes a good default value for cd       avg x   x    i use a multiplier m
                                      on this default value  after that i fix c m cd and
tune t by computing the f score on score  t for t from    to    in increments of     
in the data set the classes are unbalanced  there are many more negative examples than
positive examples  svm light provides a parameter j that multiplies the sum of the errors
of the positive examples in the objective function     

min y   a   b

 
 

w

 
 

c

 

i  yi    

i   j i  y     i
i

 

s t  yi  wt xi   b       i    i
i      i
the j parameter can be set to j  

n
n



where n



is the number of negative samples 

 

testing
there are two metrics that i used to evaluate the performance of the system 
   the   best f score  harmonic mean of the precision and recall  
   the n best f score 
for   best i count as true false positives negatives the highest scoring class  i e   i sort
the classes in decreasing order according to the svm score and choose the first class  
for the n best f score i count all the positive scoring or positive label classes  for a
given email tp will be the sum of the classes that have positive labels and positive svm
score  fp the sum of the classes that have negative labels and positive scores  and fn the
sum of the classes that have positive labels and negative scores 
the n best score will give an idea of how many correct labels we are assigning to each
email  whereas the   best score will only look at the highest scoring class and can be
misleading regarding the performance of the system  since we are interested in retrieving
all the labels for a given email 

fiexperimental results
i will consider the baseline to be the case in which we classify all emails as else  i e   the
email doesn t contain any meeting information   for this case i set all the samples as else
in the test data  score     for else and     for the other classes  and compute the scores 
  best 
tp       fp     
precision 
      
f score      
fn     tn   
recall 
     
n best 

tp        fp     
fn       tn      

precision 
recall 

      
      

f score      

if i train the svm without tuning c or j and i don t tune t i get 
  best 
tp      fp    
precision 
      
fn    tn    
recall 
      

f score 

    

n best 

f score 

    

tp      fp     
fn      tn      

precision 
recall 

      
      

in the following i ll show only the f score measure 
after optimizing the threshold t for each class 
  best         
n best        
the n best score degrades by       absolute  this seems counter intuitive  after
examining the results i see that the class reschedule is producing a lot of fp with low
positive score  this class has only   positive samples so it s the most unbalanced class  so
tuning j would probably fix this problem 
after re training the models using j as described above for each class and re tuning t 
  best      
n best      
we get a nice improvement of      absolute for the   best score and       absolute for
the n best score  this seems to fix the problem with the reschedule class 
tuning c  as m  cd   re training the models and re tuning t afterwards i get 
  best      
n best      
tuning c doesn t seem to improve the score 
after doing some error analysis i observed that there are cases in which else and other
labels are selected as positive results  this is not logical since the class should be either
else or any other class  also there can be cases in which a sample doesn t get any positive
scores and is considered a tn so it doesn t belong to any class  but it must belong to a
class because else plus the other classes is the universe 

fichanging the approach to train svms only for the classes that are not else  and consider
an email to be else if all the scores are negative  that is if all the svms classified the
sample as negative  this solves the logic problems in the experimental setup 
after re tuning c and t i get 
  best      
n best      
i get a small improvement of      absolute for both scores 
for feature selection i tried removing features in the dictionary that have frequency   
these features could be considered noise  and removing them can decrease the feature
dimension significantly  after re tuning parameters and re training i get 
  best      
n best      
i observe a small improvement of      for the n best score and degradation of      for
the   best score 
one idea that improves performance considerably in speaker id systems that use svm
classifiers is to do variance normalization on the features  i computed the standard
deviation of the features on the training data and normalized both the train and test
features  it doesn t seem to improve performance 
  best      
n best      

system
baseline  all else 
default c  j    t  
default c  j    tune t
default c  tune j  t
tune c  j  t
remove else svm  tune c j t
remove freq    words
variance normalization
table    systems performance

  best f score
    
    
    
    
    
    
    
    

n best f score
    
    
    
    
    
    
    
    

conclusions
an email classification system was developed that tries to predict a semantic label in the
context of meetings  the system improves on the performance of a baseline that classifies
all emails as irrelevant  else   based on a metric  n best f score  that was defined to
asses performance in the case that multiple labels  with the same weight  are assigned to
a sample  the biggest improvements in performance are obtained by properly tuning
parameters j and the thresholds t 

fireferences
    thorsten joachims  learning to classify text using support vector machines 
dissertation  kluwer       
    k  morik  p  brockhausen  and t  joachims  combining statistical learning
with a knowledge based approach   a case study in intensive care
monitoring  international conference on machine learning  icml        

fi