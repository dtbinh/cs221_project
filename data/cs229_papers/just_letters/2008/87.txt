vertex classification for segmented images
evan rosen
with stephen gould and rick fulton
abstract  depth estimation is an essential component for scene
understanding  with this in mind  we take a novel approach to
 d depth reconstruction which leverages off of the highly structured
nature of our environment  we believe that high level structural
cues  such as planes  edges  and vertices play an important role in
resolving depth ambiguities and attempt to evaluate the significance
of such factors  this approach is partly inspired by the constraint
satisfaction of waltzs algorithm in understanding of line drawings with shadows         waltz was concerned with finding the
correct  d interpretation of a line drawing by using the geometric
constraints which connect edges place on each other  but in applying
this method to real images  a number of issues arise  it is not clear
where the  d discontinuities lie in the image and as a result we cannot use hard constraint satisfaction to solve for the true depth  we
attempt to distill the basic intuition and capture the geometric relationships between adjacent parts of an image probabilistically and
generate a global depth reconstruction using a crf  this project is
largely a theoretical exploration of this approach and here we will
specifically discuss the subtask of vertex classification 

 

model

one of the first questions for this model is how to represent the relationship between parts of the image and the corresponding  d patch which we are trying
to estimate  our initial approach was to subsample the image into a smaller
number of rectangular superpixels which we map to  d by assigned a depth 
each superpixel is assumed to be planar and parallel to the image plane  the
advantage of this parameterization is that we only need to estimate one variable
for each superpixel  the drawback is that we need to operate at a reasonably
high superpixel resolution in order to avoid the smoothing of edges which do
not fall on the superpixel grid  this is particularly detrimental to our attempts
to use edge relationships to constrain the depth of these superpixels because we
cannot even explicitly represent edges but rather have only indirect representation in the form of the depth differences between adjacent superpixels  though
this approach was a useful first attempt  we have settled upon a slightly different
model 
each superpixel is now parameterized by a normal vector to indicate orientation and an offset to indicate distance from the camera origin to the superpixel
centroid  and  instead of rigidly defining the superpixel shape beforehand  we
 

fiuse a segmentation algorithm by ren and malik      an important property
of this superpixel partitioning is that by oversegmenting the image we expect
to minimize the number of  d discontinuities which do not fall on superpixel
boundaries at the cost of including many segment boundaries which do not correspond to depth discontinuities  if we wish to use edges as an explicit part
of our model  the presence of these false positives is much less damaging than
that of false negatives  it is worth noting that such an approach assumes a
high degree of approximate planarity in the real world  for it does not capture
smooth depth discontinuities 
thus we have set of superpixels constrained in such a way to match the
notions of smoothness  colinearity  planarity  and the presence of corners  in
other words  we penalize a possible configuration of superpixel orientations and
offsets depending on whether the configuration respects our estimates on the
presence of edges and vertices  for example  if we had two adjacent superpixels
with identical uniform color and there was no intervening edge in the image  we
would like to constrain any global configuration to assign these two superpixels
similar normals  in the case of vertices  if we had three adjacent superpixels
which met at a perfect y  and which we had other reasons to believe looked like
a convex corner  we would like to constrain any global configuration to assign
each of the normals to be perpendicular to each other in the correct way  this
report focuses on finding exactly what does count as a good indicator of whether
there exists a certain type of vertex between adjacent segments 

 

vertices

in our attempt to learn to identify various vertex configurations we consider
only vertices formed by three adjacent superpixels  though there are many
cases of four intersecting edges and some vertices with even more  we address
the case of three superpixels because it is simpler and because it is not clear that
the predictive benefit of considering such vertices outweighs the computational
costs of introducing larger cliques in the crf  we also attempt to classify
each edge into one of the following categories  convex  concave  a obscures b 
b obscures a  and phantom  where a and b indicate order as one proceeds
clockwise around a vertex   we will number these vertex labels for notational
convenience so that the set of possible edge labels is                thus each each
vertex can be characterized by a combination of vertex types  with this view in
mind we try to learn how to classify each edge into one of the       permutations
of edge types 
this yields a simple graphical model for classifying vertices  a simple model
assumes that each edge label is chosen independently which then generates a set
of edge specific features with some probability distribution  p  fi  xi    thus  the
most likely assignment is just the set of labels which maximize each p  fi  xi  
or equivalently  the argmax of 
  f     exp            
p  x 
 

   

fiwhere i   ln p  xi  fi    however  assuming that the edge features alone dictate
the vertex classification  and that any combination of edges is equally possible 
as long as the edge features are explained by the labels  misses the inherent
constraints on which types of edges may join in a vertex  we know that the
edges do in fact constrain one another  as waltzs algorithm demonstrated  to
capture the constraint of consistency between edge labels  we will introduce a
  this yields the
set of vertex wide features  fv which is also generated by x 
graphical model  b   thus to classify a new vertex we calculate the posterior
probabilities  p  fi   xi   for each edge i and each possible label xi                 
  for all values of x
   i e  combinations
as well as the posterior probability p  fv  x 
 
of edge type  and find the value of x which maximizes their product  or
equivalently  the argmax of 
  f    fv    exp              
p  x 

   

  v    however  because it is not clear how much each of
where    lg p  x f
these i s and  should account for our final vertex classification  i e  how
important vertex wide features are  or how much more useful one edge estimate
is over another   we need to learn weights for these terms  to simplify the
problem  and because of the way in which edge indices get assigned  we have
no reason to believe that edges should be weighted differently and thus we
only need to find the weight for   which we will call   to do this  we use
  fold cross validation  to compute i   ln p  xi  fi   by bayes rule  we find

 a  naive model

 b  consistent edge model

ln p  fi  xi  p  xi   which are simply the result of tallies of our training set  and
  v   we find ln p  fv  x p
   x 
  which is also derived from
to compute    lg p  x f
  are given by a
tallies from our training set  specifically  p  fi  xi   and p  fv  x 
 
set of gaussian distributions for each labeling xi or x fit to the training vertices
  one issue arises due to the large
whose ground truth labeling matched xi or x 
number of vertex classes  we use smoothing to deal with the sparsity of our
 

fidata set and the fact that we do not have observed vertices for all of the possible
edge combinations  and nor should we  as some are geometrically impossible 

 

features

by the above model  we have two different feature sets  those corresponding
to each edge  fi s  and those corresponding to the vertices fv s  for the fv s we
just used coordinates of the vertex and estimates of the angles and lengths  in
number of edge pixels  of each edge  the edge angles were computed by taking
a weighted average of the angle that each edgel  pixel closest to the edge  made
with the vertical  for the fi s we used the edge angle and length as above  an
estimator of edge linearity  an estimator of edge strength  and the differences
and respective values for basic statistics of the adjacent superpixels  to estimate
linearity we used the absolute value of the covariance of the edgel coordinates 
to estimate the edge strength we took the mean color difference between adjacent pixels which belonged to different superpixels  we had initially used a large
set of randomly sampled templates with which we took the cross correlation for
various points associated with the edge  but these cross correlations were extremely time consuming to compute and we do not have sufficient labelled data
to test the features and have been left our in our initial model 

 

results

with respect to the vertex classification we do not have meaningful results for
their effect on the global depth estimation  nor do we have valuable results for
just the classification of vertices we are finding  the data set must be labelled
by hand and moreover must be reasonably well balanced across different edge
and vertex classes which has made getting enough data to meaningfully predict
a     class classifier exceedingly difficult  we are still largely predicting the
most frequently occurring vertex type across all actual vertex classes 

 

future work

for one  it has become apparent that using a data set with ground truth depth
maps might be valuable in allowing automatic labeling of much larger training
sets  additionally  the current data set is largely for outdoor images  which
makes certain vertex types very rare  such as convex convex convex   moreover  once we can evaluate the effect of vertex classification on the global depth
estimates  we can experiment with various segmentations will change the classification accuracy of vertices but which may improve the overall depth reconstructions  also  there is still a fair amount of work to be done in exactly how
to build potentials given information about vertex types  this will amount to
implementing some of the actual constraints used in waltzs algorithm 

 

fi