recognition of door handles to enable a robot to open doors
   introduction
in the last few decades the desire for robots to perform human like characteristics has led to the
development of numerous algorithms for robot vision  learning and navigation  many recent studies are
focused on generating a map of the environment and self exploration problem  these works enable to
obtain the pose of a robot as well as a map of its surroundings  yet  they do not consider the problem of
accessing new locations in an indoor structure by manipulating doors  since mobile robots have an
unlimited work space  their capabilities should be improved to let them access to new areas without any
human assistance  for example when a robot is sent into a facility  which may be potentially hazardous
for human health  it should be able to navigate throughout the building by itself      to increase the
circulation space  recognition of door handles is a key problem to be solved during robot navigation 
in this study the aim is to recognize and localize door handles by both using gray scale  d images and  d
point cloud images  visible light laser with a camera which is placed on a stanford artificial robot
 stair  is used as the data source  the laser and a camera operate in collaboration to provide  d
coordinates for each pixel of a  d image  svm  support vector machines  classification method is
applied to both  d and  d images so that   different classifiers are generated to identify the door
handles  the localization of a handle in a new given image is performed by a sliding window search  we
initially evaluate the decision boundary for the  d classifier on a regular lattice and obtain the possible
regions of the handle on the image  then we prune these results by evaluating the  d classifier on those
regions  in other words the initial estimates that are obtained by  d images are improved by using the
 d data  the results show that for opaque doors the localization accuracy is high  but false positives
prevent accurate localization for transparent doors 

   data collection and feature extraction
visible light laser with a camera  which is mounted on a stair  is used for collecting training and test
data  figure     the camera and visible light laser are coupled together so that for each pixel of an
image  x y z coordinates can be acquired  we have collected data from about    different doors and
formed a data set that contains a total of    gray scale images and their corresponding point clouds 
both opaque and transparent door images are taken in order to evaluate the performance of our
method realistically 
extraction of the positive and negative data samples is performed by the help of  d images since they
are easier to work with instead of the  d point clouds  the process is semi automatized by asking the
user to locate only the center of the region of interest on a     x     image  for example  while
generating a positive training data  once the center of the door handle is selected by the user  a fixed
sized window      x     is positioned around that point  pixel values and the corresponding x y z
coordinates for each pixel in the window are then lined up to generate our feature vectors for the  d
and  d classifiers respectively 

fifigure    visible light laser with a camera is used to obtain the image of a door handle and the corresponding  d point cloud
view  image pixel values and corresponding x y z coordinates are used as feature vectors 

   algorithm
we first obtain the decision regions for both  d and  d classifiers  svm with a linear kernel is used for
training both classifiers  to determine the location of a door handle on a given image we need to apply a
sliding window search on the entire image  direct application of this method on a point cloud data
would take   times more time than a gray scale image  so  initially the  d classifier is employed and
probable regions of the handle are obtained  then those regions are classified again by the  d classifier 
so that false positives of the  d classifier are pruned  only if the  d classifier cannot predict the position
of the handle at a particular case  i e  the handle may not be recognized when the image is dark   we
apply a complete sliding window search on the point cloud image 

   performance of support vector machines
prior to svm logistic regression and least squares classification methods are employed  but their error
rate was higher than svm  when we apply svm to a training set containing    positive and     negative
samples  the classification error for both  d and  d classifiers are given in table   and table  
respectively  test set contains    positive and     negative samples  instead of using equal number of
negative and positive training samples their ratio is selected as    since an input door image normally
contains only a single handle which covers a much smaller area then the negative data regions 
table    svm training and test errors for gray scale image data 

 d
training
test

false positives  
   
   

false negatives  
   
    

table    svm training and test errors for  d point cloud data 

 d
training
test

false positives  
   
   

false negatives  
   
   

fitable   gives us a general idea about the classification performance  the high rate of false negatives is
caused by the dark images that are used for testing  in these cases  d image is not informative enough
to recognize the handle  but instead point cloud data can be used  in fact  table   shows that all the test
samples are successfully recognized since false negative percentage for the  d classifier is       still
complicated point clouds of transparent doors cause the    false positive rate to be unavoidable 

   performance of sliding window search
we have extracted overlapping detection windows on a regular lattice  sliding windows  and evaluated
the decision boundaries given by svm  the difference between the consecutive search windows is set to
   pixels  taking into account the morphology of the robot and the height at which the camera is
mounted we have a priori information about the position of the robot arm in the images  this
knowledge is used to eliminate the false positives caused by the robot arm 
the results for an opaque and transparent door are shown in figure    although the handle is detected
in both cases  false positives could not be eliminated completely when the door is transparent  actually
having higher error rate with a glass door which yields a much more complicated  d point cloud image
compared to an opaque door that yields a mostly flat  d shape was expected  moreover the number of
training samples corresponding to the transparent regions might not be sufficient enough to enable an
accurate classification 
we define the localization error of a handle as the pixel wise difference between the ground truth  i e 
user selected  center of the handle  on  d image  and the center predicted by the sliding window
search  the average localization error is obtained as      pixels for the opaque doors in the test set  for
the transparent doors the localization error is obtained as       pixels  hence we can conclude that for
opaque doors the handle can be accurately localized  however we cannot obtain the true position of the
handle if the door is transparent  although the sequential evaluation of the  d  d classifiers reduces
the number of false positives significantly  accurate localization necessitates a complete elimination of
the false positives  for better accuracy a separate classifier for the transparent doors might be trained 

 a 

 b 

figure    sliding window results for  a  opaque and  b  glass doors

fihowever in that case we would also need a pre classifier which differentiates the types of the doors 
clustering the final predicted centers by k means classifier  with k      according to their depth values
may also help to eliminate false positives 

   conclusion and further work
in this study we have shown an implementation of svm for the recognition of door handles  we could
reliably obtain the center of a door handle for opaque doors by using the prior information about the
position of the robot arm and sequential usage of  d   d classifiers  although using both images and
point clouds helps to reduce the number of false positives  we cannot guarantee to eliminate them for
transparent doors  hence localization of the handle for those cases could not be achieved successfully 
a superior approach may be training separate classifiers for opaque and transparent doors  but that
would also necessitate training of a pre classifier that differentiates the types of doors  another way to
reduce the number of false positives might be adding k means clustering as a final step 
once the localization of the handle is performed  the axis of rotation and the orientation  left or right
turn  of the handle can also be determined  furthermore force and torque that should be applied by
the robot can be reduced by determining the position of the tip of the handle  this will not only increase
power efficiency  but also enhance success rate of opening the door 

   acknowledgement
special thanks to ellen klingbeil for inspiration and fruitful discussions 

   references
   ellen klingbeil  ashutosh saxena  andrew y  ng  learning to open new doors        

fi