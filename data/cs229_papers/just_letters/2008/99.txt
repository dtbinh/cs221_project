trying out machine learning tricks in a tactical game
hanhua yin
summary
this is my attempt of applying machine learning to replace pre defined ai of a tactical
game i created  instead of using reinforcement learning  i used simple linear regression
 with some extra tweaks   which works reasonably well 
game

it is a turn by turn tactical game  srpg  on a  d grid    sides take turns to try to
neutralize units of the other side  a unit undergoes   phases during its turn  in
moving phase  the unit can move to a location within range  if a unit moves to a
position occupied by a friendly unit  it will mount to the top of that unit  a mounted unit
cannot be attacked by others  in action phase  the unit can either choose to attack 
use item  or rest  see game manual for detailed explanation 
basic idea
let the learner  the entity controlled by machine learning algorithm  player and
learner are used interchangeably  try to learn to play better against some opponent  
control using machine learning
to play better against the opponent  the learner can make predictions about the
opponents response to learners action  instead of directly predicting how the opponent
will react  the learner predicts the value of a reward function if it is evaluated at the
beginning of learners next turn  that is  after the opponents response   the reward
function measures how well the learner is doing under a game state  big reward when
the learner is winning etc   by predicting the future reward  the learner indirectly
predicts the result of the opponents reaction  by seeking maximum predicted reward 
the learner seeks an optimal action 
  although it is trained against a deterministic ai  the learner does not know how the ai would respond  since the

point is to learn from some unknown opponent 

fiexample
to be more concrete  here is the example problem i use throughout the discussion and
the one i actually implemented and tested  player units are controlled by machine
learning algorithm  enemy units are controlled by a scripted ai 

simplifying assumptions used in implementation  to decrease coding complexity  are 
    mhp  mbl  and max   of items are the same across all units  this is used to simplify
reward function so that i can cancel a few common terms 
    mov  rngs  pows  atk  def  and str are the same across all enemy units  this is
used to decrease number of linear regression models to consider 
a close up on reward function

 
mhp

hp  
  of player units
 
   player
  
 
  of enemy units
mhp

enemy hp  

 
mbl

player bl  
  
 
mbl

enemy bl  

 
maximum   of items

  of items  
player
 
maximum   of items

  of items  
enemy

with positive weights              using assumption      this is just

 
 

hp  
  of player units
 
   player
  
 
  of enemy units
 

enemy hp  

 
 

player bl  
  
 
 

enemy bl  

 
 

   of items  

player

 
 
enemy   of items  



fithis reward function captures  roughly  how i make decisions while playing this game 
unit count is the most important factor to consider 

 

 hp  

grows quickly as hp of

player

any player unit approaches zero  decreasing

 
 

player hp  

and thus total reward

 denominator term is the opposite   other terms are similar  
a close up on training vector
since units come and go  and a unit with   hp is still active  so that a   hp unit is not
the same as a neutralized unit which is removed from the field   it seems natural to use
variable length training vectors  to handle this  i simply let the program switch among
different models automatically  using reward function specified in previous section 
reward scales properly among models  using assumption      there are   linear
regression models to fit for the example used  producing   sets of parameters  which are
loaded to the game 

outline of prediction process
for a unit under learner control  decide the course of action as follows 
   backup game state
   act according to ai  but write resulting game state to beststate  predict reward using
parameters obtained from linear regression and write to bestreward
     is used to avoid division by zero  when number of enemy units is zero  maximum reward is outputted 

fi   restore game state from backup
   loop over all possible actions 
 a  for each action  get resulting state and predict reward
 b  compare this reward with bestreward  if this reward is bigger  write current state
to beststate and current reward to bestreward
 c  restore game state from backup
   copy beststate to current game state  and go to next unit 
outline of data collection process

actual implementation issue and extra tweaks
actual training data have been obtained by     manually controlling players units    
letting  scripted  ai control players units     after enough data are obtained  letting
machine learning algorithm take control of players units 
since prediction can be noisy  i initially used a tolerance factor  tol  so that course of
action is altered only if theres some action with predicted reward tol bigger than the
action initially suggested by ai  as more data are obtained  i realized that it is actually

  this is a sequential algorithm  meaning each unit under learner control tries sequentially to minimize

accumulated relative damage  relative in a sense that this also takes into account damage learners unit inflicts to
opponents unit during current turn  the opponent could inflict to the learner in next turn  as if the opponent would
act immediately  a sequential algorithm is used since otherwise number of cases to consider can easily grow out of
bound   probably ok for current example  but for situation where each side has a lot of units  this is infeasible  

fihampering the performance  so i removed it  set it to     this means ml algorithm is
controlling the units without the help of ai at all  
to avoid getting stuck in a loop forever  unwinnable for both sides  a small positive
bonus is added to predictions resulting from aggressive actions  
the following tweaks are added to get around of an intrinsic problem of linear
regression  at least in current formulation   it cannot predict potential reward several
steps in the future 
i added a small bonus for predictions resulting from mounting  mounting is usually a
good approach to take  despite the fact that it may not yield immediate reward 
i also added a small penalty term proportional to distance between players units  so  if
predicted reward does not say much  the units prefer to stick together 
why it works
   from actual performance
as stated in previous section  by setting tol to zero after some training  ml algorithm
alone takes control of players units  it improves ai since there are many situations in
which ai controlled player cannot beat ai controlled enemy  while ml controlled player
beats ai controlled enemy nicely    set of stage data are attached for reference 
stagea txt  stage a txt  stage a txt  and stage a txt are ai versus ai versions  while
stage txt  stage  txt  stage  txt  and stage  txt are the same stages except that ml
algorithm takes control of the player side  winning the game unwinnable by
ai controlled player previously  
of course  improvement is expected only for matches between ml controlled player and
ai controlled enemy in a situation similar to the example  i e     player units    or fewer
enemy units  since training data are based on these assumptions  but it can be trained
to adapt to more general situations 
   from reasoning
unlike chess  one rarely needs to think a lot of steps ahead in this game  most of the
time one step ahead is sufficient  for situations requiring more steps of thinking to play
well  tweaks mentioned in previous section  which provide ml algorithm with some
general principles  partially fix this problem  achieving reasonable performance 

  it still uses ai suggested action as an initial condition  but any initial condition will produce the same result

since in practice virtually no predictions are identical after some training 
  by the way  a variable bonus  increasing with number of turns  turned out to make units insufficiently aggressive

at the beginning and overly aggressive towards the end 
  to load a stage into a game  simple run the game  dream exe  with the name of the stage file as parameter  for

example  to load stage a txt  start the game using dream stage a txt  once in the game  press any key to enter
the field  and press q to fast forward if you wish  press f   anytime to reset the game 

fi