learning to predict a student s performance on problem sets
josh taylor

fiproject goal
the goal of this project is to show that it is possible to create a system that can effectively
predict whether a student will get a problem right or wrong  given the performance of
other students on a set of problems which contains the target problem and given the
students performance on the same set of problems minus the target problem  the reason
i would find such a system useful is that i plan on making a web site that generates
dynamic problems sets for my senior project  the idea is that the web site would use
students history on problems to create customized optimal sets of problems for learning a
particular topic  predicting the probability that a student gets a problem right is key to
developing such a system 
data
for this project the grade   math problems from the california sat  exam were used as
the problem set  the data is composed of over         students  each of whom have
taken    problems 
choice of error measure
for the predictive system to be effective  it must accurately estimate the probability of a
students success on a problem  as opposed to predicting a binary value for success or
failure  this is because  it would be useful when creating dynamic problem sets  to be
able to specify how challenging a problem set should  thus rather than just calculating
the average number of times a prediction is wrong  error should measure the standard
deviation of the difference between the success and the predicted probability of success 
a measure of average absolute value of differences promotes a binary prediction while
using sum of squared errors promotes an accurate estimate of probability  the measure of
error therefore will be 
error   sqrt  i mj n  yhatij  yij    m n  
where yhat is the predicted probability of getting target problem right  and yij is success
on the ith problem on the jth test    if correct    if wrong   the error is summed over n 
because each of the n problems is used as a target and over m for each of the m students
in the sample  for frame of reference the error generated by simply guessing the mean
success of the target problem over all students in the training set is shown in the table
below 
 note  the fact that the generalized error was smaller for
the smallest training sets is idiosyncratic to the training set
used  if results were averaged over a large number of
training sets  this would not have been the case  

training
      
      
      
      

generalized
      
      
      
      

size
  
   
    
     

fichoice of model
the focus of this project is feature selection rather than model selection  gda  svm and
linear regression were considered for use as the standard model for the experiments 
gda was discarded since the data is not gaussian and gda is not recommended when
the independent variables are categorical  for svm with a linear kernel  and a gaussian
kernel  and a linear regression on the full set of problems the errors were comparable 
however  since some of the feature selection techniques required manipulating the
covariance matrix  linear regression was chosen as the standard model fore each feature
selection approach taken 
approach    construct covariance matrix using only k most correlated problems
motivation
the motivation behind using only a subset of problems to predict the success on a target
problem is two fold  first  when the sample size is low  such as    or     students  using
all other n   problems in the regression will over fit the data  secondly  without knowing
anything about the data it seems possible that some of the problems  even for large
sample sizes  would not have meaningful correlations to the target problem  and their
inclusion in the regression would likely increase the generalized error 
implementation
approach   was implemented by calculating the correlation matrix from the covariance
matrix using the formula  xx where x is the m by n matrix of zeros and ones of n
problems for m students after mean adjusting each of the n columns  a correlation
matrix was then constructed form the covariance matrix  and for each target problem  the
corresponding column of correlations was sorted to find the most correlated other
problems to the target  the covariance matrix was then constructed using only the k most
correlated problems and used to perform a linear regression 
results

num of min
students generalized
error
      
  
      
   
      
    
      
     

optimal
num of
problems
  
  
  
  

not surprisingly  the results show that the optimal number of problems to use increases
with the size of the training set  and of course the results improve as the training set size
increases 

fiapproach    interpolated binning
motivation
the motivation for binning is to try and gather as much information about a student into a
minimal number of features  the previous results appeared to have a substantial
difference in errors between the sample size of    and   k  the problem may be that to
get enough information to make an accurate prediction requires at least    to   
problems  but by this time the regression is already over fitting the data  grouping
students into bins according to the number successes they enjoyed on other problems has
the advantage of being able to express a lot of information with a small number of
features 
implementation
in the interpolated binning approach  the features of the training set are bins that
represent how many problems a student answered correctly  the bins receive weights
according to which one or two bins straddle the number of successes the students enjoyed
on the other problems  the weights always sum to one and no more than two will be nonzero 
results
bins             
training
      
      
      
      

generalized
      
      
      
      

bins       
size
  
   
    
     

training
      
      
      
      

generalized
      
      
      
      

size
  
   
    
     

the above data show good results even for small training sets  for larger training sets
more bin points are useful  while the smaller training sets benefit from fewer bins 
approach    eigenvector analysis
motivation 
thus far  i have discovered one approach that appears to work well for large sample sizes
and one that works well for small sample sizes  it would be nice  however to find an
approach that works well for both  a good starting point would be to get a better sense of
the factor structure of the data  and this can be done with an eigenvector analysis 
implementation 
an eigenvector analysis was performed on the n x n correlation matrix to get a sense of
the number of factors influencing correlation between problems 

firesults and analysis

fig   

fig   

from the data  fig     it would appear as though much of the substantive variance in the
data can be explained by a few eigenvectors and the rest of the eigenvectors correspond
to eigen values that are difficult to distinguish from noise 
to test this hypothesis another eigenvector analysis was performed on the results of a
randomly generated answer set that shared the same mean value of p for each problem as
the test data set but had no other structural correlation in common  for each student 
success on a problem was assumed to be uncorrelated to their performance on all other
problems  this was simulated by sampling from a binomial distribution with the
appropriate parameter p set to each problems mean  each student was assumed to be no
different from any other 
the graphs of the eigen values for the nave random sampling look very similar to the
tails of the eigen value graphs for the actual data  suggesting that many of the
eigenvectors with smaller eigen values may simply be capturing spurious correlation 
next a smart factor was added to the nave random data by making some randomly
generated students more likely to find success than others  this was accomplished by
shifting the mean value of all ps for each student independently  the shift was drawn
from a uniform distribution between     and     with the resulting probabilities for each
problem bounded by   and   
the graphs of the eigen values for the nave random distribution with the smart factor
added  fig     and the actual data are astonishingly similar  it thus appears that a single
factor is responsible for much of the predictability in a students performance  in any
event only the first few eigen values are distinguishable from those from the randomly
generated data  except for in the very largest data sets 

fiapproach  a  reconstructing covariance matrix from k largest valued
eigenvectors
motivation 
considering that many of the eigenvectors with small eigen values are difficult to
distinguish from random  it might be profitable to make predictions without incorporating
the eigenvectors with small eigen values into the model 
implementation 
excluding the small valued eigenvectors was accomplished by creating covariance
matrices using subsets of the eigen vectors in the following manner  first  find the
eigenvectors and corresponding eigen values of the covariance matrix x transpose x 
sorted in descending order of eigen values  next set the eigen values to zero for the k  
to the nth eigenvectors  and reconstruct the covariance matrix using the formula ce  
uut   where u is the matrix of eigenvectors and  is the diagonal matrix of modified
eigen values  finally  replace the diagonal of the matrix ce with the diagonal of the
original covariance matrix 
there are two reasons for this last step  first  with out doing so  ce would not be
invertible  because it was created with fewer that n eigenvectors and therefore could not
have full rank  by replacing the diagonal of ce with one that comes from a matrix that is
full rank  ce becomes invertible  it also makes sense to use the diagonal from the
covariance matrix  since we are relatively confident in the variances along the diagonal 
we can measure reasonably narrow confidence bounds on the diagonal  while off
diagonal elements are much more volatile  this makes it difficult to be confident in
resulting partial correlation coefficients without the use of large data sets  finally
standard linear regressions are performed using the resulting covariance matrices from
each training set while varying the number of included eigenvectors 
results 

num of
students
  
   
    
     

min
optimal
generalized num of
error
eigenvectors
        
        
        
        

the results show that a single eigenvector should be used for training sets of    or    
students while the first four eigenvectors are useful for larger training sets 

fiaside  comparing generalized error of       binning to generalized error of
regression using just the first eigenvector
after the eigenvector analysis i hypothesized that the first eigenvector may represent a
smart factor  we can essentially think of the       as a measure of intelligence  the closer
a student is to    the more intelligent they are  the minimal difference in errors suggests
that this hypothesis was not far off 
generalized
      
      
      
      

generalized
      
      
      
      

size
  
   
    
     

approach  b  eigen value multiplier and ridge 
motivation 
the later part of the graph of the errors for the reconstructed covariance matrix exhibits
seemingly strange behavior  in particular  in all cases with greater that    tests in the
training set  the regression preferred having all eigenvectors to having all but the last few 
but the optimal strategy is to just keep the largest one or several  this result suggested
that discontinuity in the treatment of eigenvectors of similar corresponding eigen values
was not favorable  it also suggested that even the last few eigenvectors had some useful
information content  to address this  two approaches were tried  demotion of eigen
values after the kth eigenvector using a multiplier and promotion of the diagonal using a
ridge multiplier  the ridge multiplier has a similar impact to demoting eigen values  but
was less discriminating in that it treats all components of off diagonal elements equally 
implementation 
in this approach  the full values of the first k eigen values are used  but a multiplier less
than one that depends on the sample size is applied to eigen values k   to n  again the
covariance matrix  ce  is constructed from the altered eigen values and their
corresponding vectors  finally  a ridge multiplier is applied to the diagonal of the matrix 

firesults

ridge     
num of
min
optimal
students  generalized num of
multiplier error
eigenvectors
      
        
 
        
         
        
         
        
         

using a single multiplier after the kth eigen vector and either using a ridge of     or no
ridge  generated comparable results and in both cases improved the results achieved by
zeroing out eigen values  notice that the optimal multiplier when the training set is
       is     meaning the only distortion of the original covariance is the ridging 
conclusion
the best approach for all training set sizes was reconstructing the covariance matrix
using one or several fully weighted eigenvectors and demoting the rest  the optimal
values for both the demotion multiplier and the number of full valued eigenvectors used 
makes sense in light of the eigenvector analysis  at small training set sizes the
eigenvectors corresponding to small eigen values are likely just capturing spurious
correlation  while the structure captured by all the eigenvectors is meaningful for large
sample sizes  it is not surprising then that the larger multipliers and larger number of fully
weighted eigenvectors were optimal for the larger training sizes  using this approach
seems to be a very robust strategy  because it has mechanisms to incorporate the optimal
amount of information  avoiding over fitting the data at low sample sizes and underfitting at large sample sizes  this approach would be especially useful for implementing
a dynamic problem set system  since the sample sizes for problems is likely to range
drastically in size 
the other two approaches were not nearly as robust  the sorted correlation approach was
nearly as effective using       students but was significantly worse at lower sizes  the
binning approach on the other hand was nearly as effective at small sizes and worse at
larger sizes  this makes sense since because the eigenvector approach essentially only
uses the first eigenvector at small sizes  and that single eigenvector appears to be
capturing the same information as the binning 

fithe results from this project are extremely promising  especially considering the difficult
nature of making predictions using test data  test problems are inherently orthogonal 
since the objective of a test is to test as many points of knowledge as possible with the
fewest test problems  thus the problems are unlikely to be highly correlated  making
accurate predictions harder  in fact  the tail of the eigenvector analysis  which looks very
similar to a random nave distribution  suggests that this is indeed the case  furthermore 
the sat  is a multiple choice test  and the ability of a student to randomly get a problem
right adds additional noise  thus i am hopeful that when i implement these approaches
on sets of math problems that are highly correlated and are not multiple choice  the errors
will be significantly lower  it may be  of course that with a richer correlation matrix 
other approaches would be even better  but the modified eigen value approach seems
likely to be a very good starting point 

fi