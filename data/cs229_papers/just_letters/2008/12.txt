cs    final project  audio query by gesture
by steinunn arnardottir  luke dahl and juhan nam
 steinunn lukedahl juhan  ccrma stanford edu
december         

 

introduction

in the field of music information retrieval  mir  researchers apply machine learning
techniques to systems that allow users to access a large database of music files by humming
a fragment of a melody  these  query by humming  systems are intended for use in
commercial settings such as music recommendation  machine learning techniques can
also have application in the creative domain of music creation and performance  we are
developing an audio  query by gesture  system to be used as part of a computer based
music performance instrument  the query by gesture system allows a user to make a
brief physical gesture with a pen and tablet interface which is then used to find and play
a particular audio recording from a database of recorded short musical gestures  this
paper describes how we apply machine learning techniques to determine which musical
snippet most closely matches a physical query gesture 

 

description of problem

our task is to construct a system that learns to choose an audio sample when queried by
a trackpad gesture  a user interacts with this system in two stages  during the training
stage the user is presented with audio samples from the database  and he or she responds
by making a gesture that they think matches the sound  we assume that the users
gestures correspond in some way to various aspects of the music  such as changes in
dynamics  pitch or timbre  according to principles which may be either intentional or
unconscious  these implicit principles can be considered as defining a mapping between
the space of possible gestures and the space of possible musical snippets  our system must
use the training examples to infer this mapping  and then use the same mapping during
the performance stage to respond to new gestures with appropriate musical samples 

 

audio and gesture data

we recorded thee hundred three second long musical gestures performed on the saxophone by musician adnan marquez bourbon  from these we chose sixty two samples
that had mostly contiguous pitched melodic material and relatively simple melodic shapes
 i e  not having too many changes of direction  
we define a gesture to be a three second long movement using a wacom bamboo
tablet  which measures pen position and pressure over a     by     area  in order to
gather the gesture data we created a program in the max msp jitter environment   which
plays a sample from the audio database  and records the first three seconds of gesture
data after the pen first touches the tablet  as the user makes a gesture the audio sample
is repeated to provide a reference  we found that this helped the user make gestures that
more closely matched the time features of the audio 

 

fifigure    training the query by gesture system

 

audio and gesture features

the selection of features is of critical importance since the features must encompass the
elements of both gesture and audio that allow us to associate a gesture with a sound 
after some experimentation we arrived at the following features  for audio we begin with
two base level features  the log frequency and the log rms energy in the signal  measured
from overlapping windowed segments occurring every    milliseconds  the pitch tracking
uses time based autocorrelation    our base level gesture features are the x and y pen
position and pressure measured every    milliseconds as well as the x and y velocities 
we then smooth each of these time series by approximating them with the first    discrete
cosine transform coefficients 
pitch
   

pitch

 
   
 
   

 

   

    

    
time  ms 

    

    

    

    

    

    

rms
   

rms value

   
 
   
   

 

   

    

    
time  ms 

figure    audio features   log rms energy and pitch  dashed blue  original feature  solid 
dct smoothed feature  dashed red  slopes  star  turning points 
we originally tried using as features some small number of dct coefficients for each
base level feature time series  these represented the basic shape of each base level feature 
but they could not account for the relative time warping of feature curves that occurred
between an audio samples features and the features of its corresponding gesture  to
capture the overall shape of our features and account for time warping  we modeled
 

fix position

position

 

   

 

 

   

    

    
time  ms 
x velocity

    

    

    

 

   

    

    
time  ms 
pressure

    

    

    

 

   

    

    
time  ms 

    

    

    

velocity

   

 

   

velocity

   

 

   

figure    gesture features   position  velocity and pressure  dashed  original feature 
solid  dct smoothed feature  red  slopes  star  turning points 

each dct smoothed time series as six sloped segments separated by five breakpoints 
figure   shows the base level audio features for a specifc audio sample as well as its dct
approximation and the slopes and breakpoints our algorithm found  figure   shows the
features for the associated gesture from the training set 
for each of our two base level audio features we get six slopes and five breakpoint
times  resulting in twenty two features that define our audio space  similarly  the slopes
and breakpoint times for each of our five base level gesture features result in fifty five
gesture features 

 

formulation as classification

if we group the feature vectors for all of the audio samples into a set of classes  we
can then formulate our mapping from gesture feature space to audio feature space as a
classification problem  the audio feature groups become the class labels for the associated
gesture feature vectors for the training examples  we can train a classifier to assign a
class label to new gesture feature vectors  and then choose an audio sample from the same
group 

figure    mapping from gesture space to clusters in audio space
we used k means clustering to group the elements of the audio database into six
groups  using an euclidean distance measure in the audio feature space defined above 
we found that the data fit easily into six categories  with more groups the separation
 

fiwas not as well defined and the algorithm would take much longer to converge  figure  
shows the number of members and confidence for each cluster 

k means clustering

cluster

 

 

 
 
 
 
 

   

   

   
confidence

   

 

figure    k means clustering
it is interesting to consider what characterizes each audio category  since the centroid of
each audio category does not necessarily correspond to an existing audio example we can
plot an approximate reproduction of the pitch and rms curves from the audio features of
each centroid  as in figure    if we listen to the audio files in each class we find that they
do seem to belong together  and members of different classes are different  notably  the
sole member of class six is the only audio snippet that accidentally made it into our data
set that has two short phrases within it 

audio feature curves for centroids     offset 

audio feature curves for centroids     offset 
    

    
pitch
rms

log freq rms

log freq rms

    

 

    

    

pitch
rms

    
 
    
    

 

   

   

   

   

   
   
relative time

   

   

   

    

 

 

   

audio feature curves for centroids     offset 

   
   
relative time

   

   

   

 

pitch
rms

    

log freq rms

log freq rms

   

   
pitch
rms

   
   
   

   
    

 

 

 

   

   

   

   

   
   
relative time

   

   

   

    

 

 

   

audio feature curves for centroids     offset 

   

   

   

   
   
relative time

   

   

   

 

audio feature curves for centroids     offset 
   

    
pitch
rms

 

pitch
rms
   

log freq rms

    

log freq rms

   

audio feature curves for centroids     offset 

   

   

   

   
    
   

 

   

    
   

 

   

   

   

   

   
   
relative time

   

   

   

   

 

 

   

   

   

   

   
   
relative time

   

figure    audio feature curves for cluster centroids

 

   

   

 

fi 

classification results

since our application is in a creative domain it is not crucial that a new gesture query
retrieves a specific audio file  but it is important that the retrieved audio file be similar
to the gesture in some way  for this reason we evaluate the success of our algorithm not
on whether it retrieves the audio file which matches a given test example exactly  rather
we check whether it retrieves a sample from the same class 
the six categories from the audio clustering were used as class labels for the corresponding gestures in the training set  we trained five support vector machines to classify
gesture vectors into one of these six classes  each svm estimates whether or not a test
vector is a member the first five classes  and if it is not it falls into the sixth class 
we performed k fold cross validation to evaulate the classifier  with our sixty two
examples we tried       training to test ratio        ratio  and leave one out cross validation  with all of these cases we achieved     to     accuracy in classifying test
vectors  achieving these accuracy results required tweaking the parameters of the smo
algorithm  we also tried using polynomial and gaussian kernals  but these did not result
in noticeable improvements 

 

conclusions and future work

it seems that the selection of features is crucial to successfully applying machine learning
techniques  designing a query by gesture system is especially difficult since we have to
find features for representing both gestures and music  by modelling our base features
 pitch and rms for audio  x and y positions and velocities and pressure for gesture  as
series of slopes with transition times between them  we hoped to create feature spaces in
which curves of similar shape lay close to each other  however there may be other choices
of base features or models of them which work better for this task 
if we were to model the audio data as gaussian clusters and estimate the parameters
using the em algorithm  given a classification of a gesture we may be able to choose
audio samples in a more interesting way  it may also be fruitful to model the time
varying properties of both gesture and audio using hidden markov models 
regardless of our choice of features or learning algorithms  we believe that a larger
training set will improve performance  and we plan to investigate that 
once we our system works well consistently we can adapt our system to work with
other gesture acquisition methods such as with accelerometers or video tracking  we
can also use the parameters of a fully trained query by gesture system to sythesize new
musical sounds rather than retrieving pre recorded sounds 

 

references

   http   cycling   com products max 
   garreth middleton  http   cnx org content m      latest 

 

fi