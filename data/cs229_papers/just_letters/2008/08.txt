classifying musical scores by composer 
a machine learning approach
justin lebar

gary chang

david yu

 jlebar  gwchang  yud  stanford edu

november         

abstract

scores 
we run each score through a   kern parser written
in java before applying our machine learning algorithms
to the scores  the parser transposes the score from its
original key to the key of c major and generates features
from each scores chords 

we apply various machine learning algorithms to classify
musical scores in the   kern format by their composer 
our algorithms were able to distinguish some composers
well  but had difficulty distinguishing between composers
from similar time periods  we conclude that categorization problems among certain sets of composers likely have
a higher inherent error rate than others 

 

 

data

figure   lists the composers we analyzed  for each composer except beethoven  we were only able to access significant numbers of pieces of one typeeither pieces for
a keyboard instrument or pieces for string quartets  to
maintain this symmetry  we consider beethovens string
quartets and piano sonatas as being written by two different composers  for pieces with multiple movements 
we analyzed each movement as a separate piece 

introduction

understanding the features that demarcate musical genres and distinguish the works of various composers is important for a number of applications  including organizing
musical databases and building music recommendation
engines  previous work has shown that classifying audio
recordings of music is a difficult machine learning probcomposer
lived
type
n
key
lem  not in the least because one must employ sophisticated audio processing techniques to extract features from
jsb
bach  j  s 
          kbd    
a noisy audio waveform  see examples of such complexity
sca
scarlatti  d 
          kbd
  
at      
hdn haydn
         
sqt
   
we avoided the challenges of audio processing by taking
moz mozart
         
sqt
  
a different approach  instead of analyzing audio files  we
lvbp beethoven  kbd            kbd
  
used musical scores in the plain text   kern format  full
lvbq beethoven  sqt 
         
sqt
  
specification available at      
cho chopin
          kbd
  
jop
joplin  scott
          kbd
  
classifying music by its waveform remains the primal
task in this field  but we hope that our analysis of features and algorithms for classifying scores may suggest figure    composers analyzed  type is either kbd for keywhich techniques for analyzing audio might be more ef  board music or sqt for string quartets  n is the number of
pieces by each composer in our training set 
fective and may hint at the inherent error associated with
distinguishing between certain composers 

 

 

process

   

we used the the humdrum projects library of   kernencoded classical scores  available at http   kern 
humdrum org  as our source of training and testing data 
  kern scores are essentially a lossless representation of
the original printed music and include information about
articulations  dynamics  and even note stem directions
in addition to the notes themselves  for simplicity  our
model ignores everything except pitches and their durations 
each line in a   kern file lists one or more notes
which begin simultaneously  we call each of these lines a
chord  and they form the basic token for our analysis of

methodologies
naive bayes

naive bayes showed little success when we considered two
chords equal only if they contained the same pitches for
the same durations  but we improved upon this result
by relaxing the conditions under which two chords were
considered equal  the best equality function we tested
is pitch count  two chords are equal under pitch count
if they both contain the same number of notes of each
pitch  ignoring octave and duration  that is  two chords
each containing two cs and a d in some octave have the
same pitch count  but a chord containing one c and two
ds has a different pitch count than the other two 
 

firequiring that two chords have similar rhythms in order to be considered equal decreased the efficacy of our
classifier dramatically  if we ignored the number of times
a note appeared in a chord and considered a chord containing three cs and a d to be equal to a chord containing
one c and four ds  our naive bayes implementation performed with only slightly less accuracy than pitch count 
if we created n chord tokens by grouping together the
first n chords into one token  chords   through n     into
a second token  etc   our models train error fell to nearly
zero for all composers and the testing error increased 
even for n      this indicates that even   chord tokens
cause our algorithm to overfit the data 

   

the same features as lda  to maintain the nonsingularity of the class specific covariance matrices  we classified
on the    largest principal components 

   

our implementations of naive bayes  svm  lda  and
qda include no metric for measuring the similarity of two
tokens aside from strict equality  as a result  we could
not train on full chordsinstead  we reduced chords to
their pitch count and learned the frequency that a composer wrote chords with various pitch count values  this
reduction throws out a great deal of informationin particular  it ignores the notes octaves and gives long and
short notes equal weight  we devised a scheme for classifying scores using nearest neighbor techniques which attempts to overcome these deficiencies 
our algorithm works as follows  for every measure m
in every score  we create the set nm containing the k
measures in other pieces which are most similar to m 
as measured by some distance function d  to classify a
score s  we compute for each measure m  s a function
c m  nm   of ms neighbors which classifies m as being
most similar to one composer  we then take a majority
vote of all the measures to classify s 
we tried a number of different parameters to this algorithm  in the end  we represented each measure as a
vector where each element of the vector was a weighted
sum of the notes sounding at a given pitch in that measure  longer notes received greater weight  in proportion
to their length  we found that taking octaves into account did improve the accuracy of our algorithm  as wed
hoped 
we found that the algorithm was not very sensitive to
the distance function used  but we had best performance
when using


 x
y 



d x  y    
kxk kyk 
as opposed to d x  y    kxyk or d x  y    kxyk  kxk 
kyk   our algorithm was somewhat sensitive to our choice
of c m  nm    the function mapping a measure m and
its neighbors nm to a composer  the knn algorithm
performed best overall for c m  nm   defined as


x
 
arg max
  c composed n  exp
 
d n  m  
composer c

support vector machines

our second approach to the problem was to apply a standard support vector machine using the same features we
had extracted for naive bayes  to this end  we used the
libsvm library  available at http   www csie ntu edu 
tw  cjlin libsvm  
the accuracy of our svm varied greatly depending on
the kernel we used and the parameters we passed to the
svm  we tried three kernels 
   linear  k u  v    ut v
   sigmoid  k u  v    tanh ut v   c   
   gaussian radial basis function 
 
k u  v    e uv 
and found that the last significantly outperformed the
first two  as a result  we spent most of our efforts tuning
and trying different features for the gaussian radial basis
function kernel 

   

k nearest neighbors

linear and quadratic discriminant
analysis

we performed lda classification using the same pitchcount features as naive bayes and svm since it was
shown it give better accuracy  lda assumes that the
input features are distributed according to a multivariate gaussian  if this assumption holds  then lda should
require fewer training samples to reach performance similar to nb and svm  although we didnt observe that
our data was closely distributed according to a multivariate gaussian  we hoped that our lda might nevertheless
perform well given a limited training set  to maintain
symmetry with our other algorithms  we trained on    
samples overall     per composer   and thus we rankreduced our data by selecting the     largest principal
components and performed lda on those 
supposing that the linearity of the classifiers in lda
might be a limitation to its performance  we also performed quadratic discriminant analysis on our data  using

nnm

this choice of c decreases very quickly as d m  n  increases  suggesting that our knn classification performs
best when c m  nm   outputs the single nearest neighbor
of m  using a majority vote when m has many neighbors
at approximately the same distance 

 

results

we see the following consistencies across classifiers  first 
all classification methods had difficulty distinguishing be 

fi   
nb
svm
lda
knn

jsb

sca

  

hdn

error    

composer

  

moz

lvbp

  
lvbq
jop
cho
lvbq
lvbp
moz
hdn
sca
jsb

cho

  
jop

 

jsb

sca

hdn

moz

lvbp

lvbq

cho

 

jop

  

  

  

  

  

  

  

   

pitch  the feature only reports a   if it appeared once
or more  or a   if it never appeared   for each set
of features  we performed a grid search to approximate
the optimal values for the svm parameters  we measured the performance of the classifier for values of the
slack constant c                        and the radial basis
function parameter                          by performing    fold cross validation on the training set  we found
that the pitch count feature set yielded the highest crossvalidation accuracy of        followed by pitch present at
      and note count at        it is interesting to note
that pitch count was also the feature set that yielded the
highest accuracy in the naive bayes classifier  however 
we still achieved a reasonably high level of accuracy when
given only boolean values for each pitch  in the case of
pitch present  or when given only the number of notes in
each chord  in the case of note count  
our last step was to perform a fine grained search for
c and  around the local maximum found previously 
with these parameters and the pitch count feature  our
cross validation accuracy increased from       to       
our overall accuracy on the test set using these optimal
parameters is shown in figure    and the confusion matrix
is plotted in figure   

tween composers belonging to the same musical period 
for example  in the svm confusion graph  figure    
we see that most pieces misclassified as bach were by
scarlatti and vice versa  this is unsurprising  since bach
and scarlatti were contemporaries and both wrote in the
baroque style  we see the same pattern can be across
classical  haydn  mozart  and beethoven  and romantic  beethoven piano and chopin  composers in all our
confusion plots 
second  all classifiers were able to distinguish joplins
works to a high degree of accuracy  this is consistent with
the fact that joplins ragtime style that was a distinct
departure from the european classical traditions 

naive bayes

we initially conducted our naive bayes modeling using
    of each composers scores for training and the remaining for cross validation testing  but we found that
this method biased the model towards composers with
more scores  to remove this bias  we trained on a constant
number of scores from each composer      and tested on
the remaining scores  doing so  we obtained much more
balanced results  we used this cross validation scheme
for all of our other classifiers as well 

   

  

figure    confusion plot for naive bayes classification 
the small bars represent the percentage of a composers
works which were misclassified as the composer listed on
the vertical axis  the large bars represent the sum of the
small bars they contain 

figure    test error for the four main algorithms used 
in each case  we trained on    scores from each composer
and tested on the remaining scores  for nb  svm  and
lda  we used the pitch count feature  for knn  we used
the measure count feature  see figure   for the full names
of the composers listed 

   

  

error    

composer

   

support vector machine

k nearest neighbors

even with a high degree of tuning  our knn algorithm
performed on par with lda and worse nb and svm  we
had hoped that the larger features we used would allow
us to extract more information from the scores  but this
turned out not to be the case  we suspect that the critical piece of information lost in the measure count feature

using the same training and cross validation scheme
as naive bayes  we tried features including pitch count 
note count  which reports only the number notes in a
chord and ignores the notes pitches  and pitch present
 which is similar to pitch count  except that for each
 

fijsb

sca

sca

hdn

hdn

moz

moz

composer

composer

jsb

lvbp

lvbq

lvbq
jop
cho
lvbq
lvbp
moz
hdn
sca
jsb

cho

jop

 

lvbp

  

  

  

  

  

  

  

  

  

jop
cho
lvbq
lvbp
moz
hdn
sca
jsb

cho

jop

   

 

  

  

error    

  

  

  

  

  

  

   

error    

figure    confusion plot for svm classification 

figure    confusion plot for lda classification 

meanwhile  qda had an error rate upwards of     
qda requires significantly more samples than lda to
classify data of the same dimension  so we had to run
qda on a lower dimensional vector space than lda  essentially giving qda less training data  furthermore  the
fact that our data is not multivariate gaussian  see figure
   made errors even more pronounced in qda 

jsb

sca

hdn

composer

  

moz

lvbp

 

lvbq
jop
cho
lvbq
lvbp
moz
hdn
sca
jsb

cho

jop

 

  

  

  

  

  

  

  

  

  

analysis

in order to understand how well our classification methods have performed  we need to estimate the bayes error that is inherent to our classification problem  the
bayes classifier is ambiguous when there is overlap in the
posterior probability distributions of two or more distinct
classes  therefore  we can estimate the bayes error by approximating the degree of overlap between the posterior
distributions  we do this through pca and a historical
analysis 

   

error    

figure    confusion plot for knn classification 

as compared to pitch count is the number of notes being
played at once  this information is likely key to success      principal component analysis
ful differentiation between string quartets and keyboard our first approach to estimating the degree of overlap
pieces  something which our knn algorithm had difficulty between features of distinct classes was to create a  doing  see figure    
dimensional visualization of the data  this was done by
    linear and quadratic discriminant mapping the pitch count features to their first two principal components  we observed that composers belonganalysis
ing to the same musical period exhibit significant overlap
we achieved training error of      and cross validation in the first two principal components  see     on the
accuracy of       using linear discriminant analysis  other hand  composers belonging to distinct musical pewhile this seems substantially lower than svm  we see riod show a high degree of separability  see     these
from figure   that lda is quite comparable to svm for observations help explain the patterns in confusion errors
all composers except for the romantics  beethoven and discussed above 
chopin   the difficulty lda had classifying the roman    historical analysis
tics is likely due to the linearity of its decision boundary
or and the fact that our data is not distributed according the degree of similarity in the music of distinct composers can also be traced historically  haydn and mozart
to a multivariate gaussian 
 

fi 

 

beethoven quartet
mozart
haydn

chopin
joplin
 
 
 
 
 
 

 

 

 

 
 
 
 
 

 
 

 

 

 

 

 

 

 

 

 

figure    pca of romantic vs  ragtime composers 

 

 

 

 

 

 

 

figure    pca of classical composers 

 

further work

were both in vienna from          and were admirers
of each others works  haydns opus    string quartets
are believed to have been inspired by mozarts k   
     while mozarts k                         and    
are widely known as the haydn quartets and were inspired by haydns opus         similarly  beethoven was
haydns pupil in vienna from          and chopins
late contemporary  beethovens later works  after      
are widely considered to be the beginning of the romantic
period  a canon to which chopin belonged 

in this paper  we laid a framework for applying machine
learning techniques to   kern scores  our analysis here
has been context free  our nb  svm  and lda algorithms consider only how many times a given chord appears in a score  and our knn algorithm classifies measures with no consideration given to the surrounding measures  future work might involve attempting to add more
context to these models  either by adding context to the
features used  or by using a context full model  such as
an hmm 

the similarities in haydn and mozart string quartets
have been quantitatively accessed by carig sapp and yiwen liu of stanford university  in an online quiz that
asks listeners to distinguish between movements from
mozart and haydn string quartets  the accuracy rates
ranged from    to     for self identified novices and
experts respectively      in light of the difficulty even
experts have with this classification problem  our algorithms difficulty here is not surprising 

 

 

acknowledgments

wed like to thank prof  jonathan berger for his valuable guidance at the beginning of this project  wed also
like to thank the humdrum project and the center for
computer assisted research in the humanities for making their collections of   kern scores available online for
free 

references
    mirex       international music information retrieval systems evaluation laboratory  university of illinois at urbanachampaign  http   www music ir org mirex      index php

conclusions

    everything you need to know about the humdrum   kern representation  ohio state university school of music  http   dactyl 
som ohio state edu humdrum representations kern html

we see that svm is consistently the more accurate classifier across all composers we analyzed  this is likely
because svm makes no assumptions on the probabilistic distribution of features  allows for nonlinear decision boundaries  and can train on a sparse yet highdimensional data  these properties make the svm the
superior classifier for this particular problem  although
our error rates for classifying some composers were large 
errors we saw are in line with what wed expect from a
historical analysis  pca  and human trials 

    saunders  c   hardoon  d  r   shawe taylor  j  and widmer  g 
       using string kernels to identify famous performers from
their playing style  in  the   th european conference on machine
learning  ecml  and the  th european conference on principles
and practice of knowledge discovery in databases  pkdd        
september  pisa  italy 
    rosen  charles  the classical style  haydn  mozart  beethoven 
w  w  norton company       
    sapp  craig and yi wen liu  the haydn mozart string quartet
quiz  stanford university  http   qq themefinder org  

 

fi