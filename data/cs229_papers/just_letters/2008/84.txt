prof  andrew y  ng
cs      machine learning
autumn          

advisor  prof  dan jurafsky  stanford university 
supervisor  dr  hiyan alshawi  google inc 

unsupervised learning of
hierarchical dependency parsing

 

valentin i  spitkovsky
final project
          

very brief introduction  background and motivation

in a sentence  a dependency is a link from one word  called a dependent or attachment  to another  the head  if we imagine
that the global head word itself connects to some root note  then a sentence of length n corresponds to a parse tree with
exactly n links  constituency parses  such as those in the wall street journal section of the penn tree bank  can be converted
to dependency parses using special head percolation rules  resulting references are then used for training and or testing
hierarchical dependency parsers  the standard quality metric is the fraction of correctly identified links  the problem of
training such parsers  and especially its unsupervised variant  thanks to an abundance of texts from specialized genres
and in exotic languages and due to the poverty of manually produced references   is still a very active area of research 
potential beneficiary applications include machine translation  information retrieval  web search and query refinement  to
name a few 
last quarter  in prof  mannings cs   n  natural language processing  we chose to implement klein   mannings
dependency model with valence for our final project  a generative model  the dmv lends itself to unsupervised learning  by
means of expectation maximization via inside outside re estimation  it actually took us the entire summer to debug our
derivations and code  but in the end our implementation actually out performed the advertised claims of the dmv  which is
itself considered state of the art  in this project  we proceeded to tease apart the inner workings of the dmv  in an attempt
to improve and or simplify wherever possible  making use of the various machine learning insights gleaned over the term 

 

just a few more necessary preliminaries

the dmv  to have any hope of success  instead of dealing with actual words  w   the model operates over part of speech
tags as lexical word classes  cw    of which there are only a few dozen  the generative story for a sub tree rooted at some
head ch rests on three independence assumptions     ch first decides whether to attach all of its left or all of its right children
first  using the probability porder  ch       it then generates children in a particular direction dir   l  r  until it stops  at
each point with probability pstop  ch   dir  adj   where adj   t  f   true iff this is the first or adjacent child   and    each
child slot is filled in with an attachment of class ca   according to pattach  ch   dir  ca    this disallows crossing dependencies
 non projective parse trees  and corresponds to a context free  grammar  in which all sentences are imagined to end with
the special token   whose class serves as the global head  attaching exactly one child on its left  and none on its right   all
probabilities are estimated without smoothing 
initialization  as is often the case  this learning algorithm is very sensitive to initialization of parameters  both porder and
pstop are set uniformly to one half  for pattach   however  the model is given a slight nudge in a linguistically sensible direction  within each sentence  the likelihood of attachment between two words is modeled as inversely proportional to their
distance  the resulting empirical distributions are then averaged over all head words from all sentences and used to seed pattach  
termination  we had to concoct our own terminating heuristic for em  so we chose to be conservative  bailing out once either
of the following conditions become true     the difference between current and best or last average per link cross entropy
drops below          number of iterations reaches       
data set  the dmv is trained tested on wsj    a modified subset of the ptb  annotated parse trees are stripped of all
empty elements and punctuation  tagged as           lrb  and  rrb    as well as items which arent pronounced   or at least
not pronounced where they appear  tagged   and     then thrown out if they still contain more than ten terminals  this
leaves       of the original        sentences 
metrics  requiring the parser to produce a single best parse for a given sentence  its directed score is then the simple
fraction of correct links  to facilitate comparison with earlier work  as well as to partially obscure the effects of alternate
  this isnt quite a probabilistic context free grammar in the strict sense  although the grammar aspect is itself context free  the rewrite
probabilities are not  since some are conditioned on adjacencies  and while the original authors may have needed this elaborate  p cfg  our
purposes are more modest  in particular  all of these independence assumptions imply that left children are entirely independent of the right 
thus  we drop porder altogether  making that choice deterministic  this reduces the state of the estimation problem and eliminates some noise and
rounding pollution   furthermore  we simplify the calculation of our inside probabilities  e g  that the word class in position p derives the sequence
of word classes in positions x  x      x              y        x  p   y  to be the product of probabilities associated with x  x      x              p  p    
and p  p      p              y     which peals away one layer of a very nested loop  thus speeding up our implementation  while also helping guard
against excessive noise and rounding issues 
  the original authors argue that unsupervised learning could shed light on language acquisition in children  and so include only whats audible 

fianalyses  e g  systematic choice between a modal and a main verb for the head of a sentence   the original authors also make
use of the more flattering undirected score  which ignores the directionality of each parent child relation 
scores  the adjacent word heuristic  which simply links each word with its immediate neighbor  either all going left or all
going right  yet shatters all previous unsupervised work  achieves a directed score of       on this data set  the dmv further
improves on its authors own baseline  scoring               directed and undirected 

 

alternative metrics

one problem with best parse scoring is that it does not lend itself to exact duplication by an independent implementation 
because of the many opportunities for idiosyncratic tie breaking in standard chart based algorithms  it can also over  or
under state a models performance  depending on how an implementations arbitrary but systematic preference among ties fits
with a particular language  e g  one from a left  or right branching family  being learned  rather than simply hard counting
each link as either right or wrong  we estimate its probability softly  as the mass of all parse trees in which it is present 
according to the model 
unlike best scoring  average scoring isnt used in the literature  most likely because it is harder to derive and more
difficult to compute  however  given the inside outside charts  it is actually marginally cheap and has the added benefit of
being entirely deterministic  modulo floating point rounding errors  and hence reproducible  furthermore  it can provide
extra insights into certain undifferentiated models  e g  those initialized uniformly  in which the number of ties can be
exponential 
we append these average scores to the ordinary best scores in square brackets  when available  in addition  we compute
the average per link cross entropy  in bits  and include this number with each set of scores as well  in the average case 
instead of using the probability of the best parse  we sum the probability of all parses to arrive at the probability of the
sentence  these are naturally higher  leading to lower perplexities   in this format  our own optimized  but otherwise vanilla 
implementation of the dmv framework achieves  on the standard data set  the following base lines 
d
       
       
       
       

u
         
         
         
         
best

x h
      
      
      
      

d
        
        
        
        

u
         
         
         
         
average

x h
       
       
       
       

model parameters
undifferentiated
ad hoc harmonic
unsupervised
supervised

these are  respectively  the undifferentiated  all uniform  model  the ad hoc harmonic  used to initialize unsupervised learning   the result of unsupervised learning  and a supervised  oracle  model  which simply reads off all probabilities from the
reference parse trees  whats even more surprising than the fact that our best directed score of       is significantly higher
than the reported       is that the supervised models perplexity is worse  task performance does not vary linearly with the
models goodness of fit 

 

ablative analysis

other red flags raised by our baselines include the facts that    even the system with perfect knowledge is quite far from
         undirected scores for initialization and its unsupervised result are quite close  at least on the best parse metric 
these inspired us to proceed with further supervised dissection of the dmv  to see how well various partial oracles perform 
without unsupervised training 
       
       
       
       
       
       
       
       
       

         
         
         
         
         
         
         
         
         

      
      
      
      
      
      
      
      
      

        
        
        
        
        
        
        
        
        

         
         
         
         
         
         
         
         
         

       
       
       
       
       
       
       
       
       

perfect knowledge
all attached knowledge
all left knowledge
all right knowledge
adjacent stop knowledge
all stop knowledge
non adjacent stop knowledge
ad hoc harmonic
zero knowledge

we continue to sort our results by the best directed score  and already some funny business begins to emerge  knowing the
full stopping probabilities appears worse than knowing just the adjacent ones  while knowing just the non adjacent stopping
probabilities appears worse than knowing nothing at all  however  average scores better reflect reality   still  it appears that
the model would perform better  as far as parsing is concerned  if the non adjacent stopping probabilities were eliminated
and replaced with the constant one half  or something even more appropriate   also worth noting is that this last one is
the only partial oracle thats worse than the full unsupervised model  there is much room for improvement  lastly  for
decently performing partial oracles  directed and undirected scores seem to be quite close  which doesnt appear to be the
case for badly trained models 

fi 

robustness and scaling exploration

to get a better feel for the dmvs properties  we trained it  in the standard  unsupervised fashion  on wsj  and wsj   as
well  keep in mind that these are rather weird  non comparable juxtapositions  since adding long sentences to an existing data
set significantly complicates the learning problem  the possible number of parses is exponential in the length of a sentence  
but also offers a non trivial amount of extra training material  each new sentence provides more data than ones before  
       
       
       

         
         
         

      
      
      

        
        
        

         
         
         

       
       
       

wsj 
wsj  
wsj  

       sentences 
       sentences 
        sentences 

as part of our earlier debugging efforts  we also implemented a viterbi approximation  instead of going through all the
error prone and computationally expensive trouble of inside outside re estimation  it learns from the single best parse of
each sentence  given the current model 
       
       
       

         
         
         

      
      
      

        
        
        

         
         
         

       
       
       

wsj 
wsj  
wsj  

 viterbi 



our other experiments suggest that the dmvs quality degrades gracefully as sentences grow past length     viterbi approximation continues to beat the exact solution on directed scores for longer sentences  though the gap trends downwards 
meant as a quick sanity checking hack  viterbi approximation offers a simpler  faster approach  which frequently beats its
slower  more complicated state of the art cousin  given the alarm bells from before  we strongly suspect that the dmv is not
the most appropriate model of natural language  it would appear that viterbi rushes in the direction thats approximately
right  while full fledged em io drills down towards a solution thats exactly wrong    consider what happens when we
initialize both algorithms with the supervised models perfect oracle  back on wsj   
       
       
       

         
         
         

      
      
      

        
        
        

         
         
         

       
       
       

perfect knowledge
perfectly seeded
perfectly seeded

 supervised 
 viterbi 
 em io 

viterbi manages to hang on to the good solution  while em io does not  however  we dont think that this is a bug in
our implementation of em io  since the more complex algorithm does manage to arrive at lower perplexity values than its
counterparts    nevertheless  since em io is numerically more intensive  we suspected that perhaps rounding and numerical
precision issues were muddying things  we managed to eliminate this possibility by testing both approaches nine ways    
using three different level of floating point precision in our numeric representation   long double  double  float   and   
using three different abstract implementations of arithmetic over probabilities  regular p space  p   log p  space  log  and
log p  space with sloppy math  for addition   subtraction  slop   starting each of the nine combinations with each of the
four set ups   viterbi  em io    ad hoc harmonic  perfectly seeded   we arrived at amazinlgy similar numbers  though
the number of iterations till convergence varied significantly between certain extreme arrangements 

 

some quick improvements

we already pointed out that the handling of non adjacent stopping probabilities could use some help  as is  the model is
quite elegant and recursive  lending itself to inside outside re estimation  but this is also its down fall  natural language is
not recursive  the probability of attaching a   th child is not the same as for the  nd    modeling the number of children
as a geometric distribution leaves way too much probability mass for the very long syntactic structures  but given that the
viterbi approximation is quite good anyway  we take another step away from context freedom by conditioning on the number
of children  which adds an extra layer to the nested loop of viterbis inside chart generation  
one elegant approach is to use a probability distribution that still supports arbitrarily long outcomes  even when whats
observed is bounded  as did the geometric  whose    p    we managed to find a suitable distribution  called the log series
distribution  whose existence follows from the fact that the taylor expansion of the natural logarithm around  x  x         
i
p i
p
 
  p
  we call this the ldmv 
is i   xi   the probability of landing on any partcular positive integer is pi and    ln  p 
another approach is to memorize non adjacent probabilities  conditioned on the number of children already attached  as is
done for the adjacent probability  which makes it impossible to have more children  on either side  than one minus the length
of the longest sentence  we call this dmvc  still on wsj    though now lacking average scores 
       
       
       
       
  for

         
         
         
         

      
      
      
      

        
        

         
         

       
       

 em io 
 viterbi 
 ldmv 
 dmvc 

instance  instead of exponentiating back to p space  addition is performed using the fact that


log x   y    max log x  log y    log     emin log x log y max log x log y   

fidmvc parses better  while the ldmv has lower perplexity than their predecessors  the gains may not appear very significant
at first glance  but these models really shine when a poor set of parameters requires rescuing  consider what happens when
the dmv is trained using undifferentiated seeds  rather than with ad hoc harmonic   also on wsj   
       
       
       
       

         
         
         
         

      
      
      
      

        
        

         
         

       
       

 em io 
 viterbi 
 ldmv 
 dmvc 

       
       

 em io 
 viterbi 
 ldmv 
 dmvc 

here is how they fare on wsj    this time seeded with ad hoc harmonic 
       
       
       
       

         
         
         
         

      
      
      
      

        
        

         
         

part of the reason why the dmv performs so well on wsj  and is still competitive on wsj   and wsj   is that the data
itself precludes it from realizing the very fertile heads that it seeks  as sentence length increases  the dmv gets increasingly
lost  excited by the possibilities  while other models could do better 

 

replacing the ad hoc harmonic

we tried out a cute idea of a more principled  yet still unsupervised approach to initializing pattach   clustering using an
svm  essentially  some attachments are likely  others are not  thus  we could pretend that there are exactly two classes 
furthermore  for every sentence of length n  of the  n    possible links  exactly n    will be correct  thus  we even
know the relative sizes of these clusters  we encode the same information given to the ad hoc harmonic  the head class  the
attachment class  direction and distance  using binary valued features and dump it all on svmlight without labeling a single
      
         using its transduction
example  but stating that the prior for the positive class is  in the case of wsj           
option  p and the basic linear kernel 
       
       
       
       

         
         
         
         

      
      
      
      

        
        
        
        

         
         
         
         

       
       
       
       

 ad hoc harmonic seed round 
 svmonic seed round 
 ad hoc harmonic viterbi result 
 svmonic viterbi result 

the numbers are in the same ball park  presumably we could augment our features with more rich information  e g  actual
intervening word classes  as well as play with more sophisticated kernel functions 

 

testing independence assumptions

from the point of view of statistical hypothesis testing  pretty much all relations gathered by the supervised oracle appear
significantly non independnet  even when divergence is tiny  and here  abundance of data back fires  as more data is
gathered  it provides extra evidence against the independence of less and less dependent events  as a thought experiment 
inspired by the kolmogorov smirnov test  we introduce the notion of external misallocation  one minus the total probability
assigned to jointly observed events  with each independent factor estimated by an mle  i e  the total probability mass
left over for unseen events  given that we arent explicitly aiming to smooth  we hope for this number to be small  as it
turns out  for some parts of speech  this number is empirically as high as         in particular for many verb types and the
coordinating conjunction  we also explored internal misallocation  the total probability mass assigned to observed events
in excess of their actual frequency  for certain types of verbs  these numbers too can be high  at        

 

a joint model

based on several pieces of insights already gathered  we introduce the jdmv  which attempts to model as much as possible
jointly  while preserving much of the elegance of the dmv  in particular  note that of n words in a sentence  only n   
become children  a priori  the expected number of children is less than one  fertile productions are rare  and we term
those with at most one child per side compact  also  recurion complicates things  recall that adjacency trumped recursive
non adjacency   so we try to do as much as possible in the infancy  while the head is small and still knows everything  rather
than when it matures and its world view is clouded by recursion  having lost track of what exactly went into making it the
way it finds itself  also  since the problem is constrained  it could be that taking care of the small nodes would help the
bigger ones take care of themselves   
for the jdmv  we fuse attachment probabilities with non adjacent stopping  yielding two probability distributions to be
estimated  pinit  ch   cl   cr   sl   sr   and precurse  ch   ca   s  dir   where the initial probability reflects the chances that ch attaches
cl to its left  cr to its right  and then stops  in either direction  based on sl   sr   t  f   recursive probability reflects the

fichances that it goes on to attaches ca on side dir   l  r   and then stops  based on s  given that it already has at least one
child on that side  this model could be augmented to condition on the number of children as well  but we wanted to derive
complete and efficient em io re estimation  in addition to the viterbi approximation  unfortunately  we are running out of
time and space to present these results   

  

a hint of impossibility  a glimpse of hope  and a wealth of data   

pereira and schabes showed that pure  unsupervised em fails to infer even the small artificial grammar

ac  w p      
c  sa 



bd  w p      
d  sb 
s
aa  w p      
a  a 



bb  w p      
b  b 
generating the two symbol palindrome language l    wwr   w   a  b      but manages to recover if given some partial
bracketing constraints  which enabled    faster chart generation     faster convergence  and    convergence to the true answer 
one of our hopes was that mark up  e g  hyper linked anchor text on the web  fonts like bold and italics  etc   may
frequently delimit linguistically sound constituents  if so  then there could be a  free  growing and multi lingual  wealth of
partial bracketing data to be mined on the web  to this end  we down loaded one news style blog in its entirety  hacked
around mxterminator to break clean  pretty printed html into sentences  then used charniaks parser for  supervised 
pos tagging and parsing  a la the ptb  excluding  useless  full sentence and single word annotations  we were able to extract
      bracketings from a total of        sentences  thats       yield         of these matched a constituent produced by the
parser perfectly  presumably others could still be helpful  since we are aiming for dependency and not constituency parsing  
       vast majority  of the extracted bracketings are derived by noun phrases  while        a non trivial minority  fall
under verb phrases  all of this suggests that we may be able to perform some very interesting lightly supervised learning
using these partial bracketings as valuable hints to the otherwise unsupervised algorithms 
well  we did this  but there is no room to include these results either    

  

references consulted

collins  m  head driven statistical models for natural language parsing       
ph d  thesis  mit 
collins  m  three generative  lexicalised models for statistical parsing  in acl           
charniak  e 
ftp   ftp cs brown edu pub nlparser parser  aug   tar gz
hardt  d 
http   www id cbs dk  dh corpus tools mxterminator html
jurafsky  d   and j h martin  speech and language processing 
an introduction to natural language processing  computational linguistics  and speech recognition 
       nd ed 
klein  d  the unsupervised learning of natural language structure       
ph d  thesis  stanford university 
klein  d   and c d manning  corpus based induction of syntactic structure 
models of dependency and constituency  in acl             
mcdonald  r  and j nivre  introduction to data driven dependency parsing 
http   dp esslli   googlepages com 
pereira  f   and y schabes  inside outside reestimation from partially bracketed corpora  in acl            
xia  f  inner loop of the inside outside algorithm 
http   courses washington edu ling    fei fall        summary pdf
xia  f  inside outside algorithm 
http   faculty washington edu fxia courses ling    inside outside ppt

fi