internet article comment classifier
matt jones  eric ma  prasanna vasudevan
stanford cs      professor andrew ng
december     

  introduction
    background
part of the web     revolution of the internet in the past few years has been the explosion of user comments on
articles  blogs  media  and other uploaded content on various websites  e g  slashdot  digg   many of these
comments are positive  facilitating discussion and adding humor to the webpage  however  there are also a
multitude of comments  including spam advertisements  blatantly offensive posts  trolls  and boring posts  that
detract from the ease of reading of a page and contribute nothing to the discussion around it 
to alleviate this issue  websites like slashdot have implemented a comment rating system where users can not only
post comments  but rate other users comments  this way  a user can look at a comments rating and quality
modifier  funny  insightful  etc   and immediately guess whether it will be worth reading  the site can even filter
out comments below a threshold so the user never has to see them  as slashdot does  

    goal
despite the power of crowdsourcing  ideally a website should be able to know how interesting or useless a
comment is as soon as it is posted  so it can be brought to users attention  via placement at the top of the
comments section  if it is interesting or it can be hidden otherwise 
so  our goal was to design a machine learning algorithm that trains itself on comments from multiple articles on
slashdot and then  given a sample test comment  predicts what the average score and modifier of that comment
would have ended up being if rated by other users 
specifically  we wanted to determine what makes online article comments a unique text classification problem and
which features best capture their essence 

  pre processing and setup
    scraping
we acquired the data by crawling slashdot daily index pages from june to present  noting each day s article urls 
and then downloading an ajax free version of each article page with all its comments displayed  we parsed each
comment page with a combination of dom manipulation and regular expression matching to extract the user 
subject  score  modifier  if any   and actual body text  these data were stored into a mysql database  and metafeatures were calculated later for each comment  and then stored back in mysql along with every other comment
attribute  the score and modifier distributions of the comments are illustrated in figure   and figure   
respectively  figure   depicts the proportion of each modifier type as well as the average score 

    formatting of input data
we took the following steps to convert this raw data to a form suitable for machine learning algorithms 
a  case folded and removed  stop words    these include  a    the   and other words that have little relation to
the meaning of a comment 
b  applied porter s stemmer   this algorithm converted each word to its stem  or root  reducing the feature
space and collapsing words with similar semantic meaning into one term  for example   presidents  and
 presidency  would both be converted to  presiden  
c  counted word frequencies   every stemmed word that existed in any of the comments in our data set was
given an individual word id  then  we counted the frequencies of words in each comment 

fithe result of these steps was  effectively  a matrix of data with dimensions  number of comments  x  number of
possible words   where the value of entry  i j  was the number of occurrences of word j in comment i 

    formatting of output variables
the   output variables we had to work with were 
   score   possible values are integers    to      classes 
   modifier   possible values are  none    insightful    interesting    informative    funny    redundant    off topic  
 troll   and  flamebait     classes 
in addition to these  we created   artificial output variables which we thought would be useful dependent variables
for our algorithms to predict 
      bad  for scores            good  for scores            classes 
      bad  for scores              good  for scores          classes 
     for negative modifiers  redundant  off topic  troll  and flamebait  
  for positive modifiers  insightful  interesting  informative  funny  
  for no modifier    classes 
     for negative or no modifier    for positive modifier    classes 
thus  we had   output classification types into which we wanted to classify comments 

  meta feature selection
    thoughts about comments
we made the following observations  based on our past experience  about the nature of different types of
comments 
 many spam messages are majority  or all capital letters 
 non spam messages that are majority  or all capital letters tend to be annoying 
 long comments are probably better thought out than very short ones  and are more likely to contain
insightful comments 
 users that have more experience posting comments on a moderated environment such as slashdot s are
less likely to purposefully post irritating comments 
 many spam messages contain urls 
 very few spam messages are very long  for the few that are long  it is usually because there are many
paragraphs with one sentence per paragraph 

    meta features
word frequencies alone are not enough to capture the above patterns  so  to better describe each comment  we
added the following   meta features to the word frequencies to form our new feature set 
 percent of characters that are upper case
 number of total characters
 number of paragraphs
 number of html tags
 number of comments previously made by the commenter
analyzing these meta features and their effectiveness for classification were our main points of interest in this
research  we wanted to determine what meta features would be best at capturing the essence of online
comments 

  algorithms
    nave bayes

fiwe used our own java implementation of nave bayes  adapted from the library used in cs      we experimented
with different training sets  including just comments from the one article with the most comments  m   number of
training samples          comments from the top    most commented articles  m           and comments from
the top     most commented articles  m          it ended up being infeasible given our implementation and
resources to run on comments from the top     articles  and we got better  and more useful  results by training on
the top    articles comments  this is naturally a more useful training set than just the   article set  since the end
application of this classifier would be given this comment  tell me if its good or bad  having a classifier that only
works for a given article wouldnt be very useful  thus a classifier thats general enough to do well for comments
from    articles should fulfill this end purpose more effectively that a classifier that only works on comments from
a given article 
while we initially tested on our entire training set  once we had chosen features we switched to    fold crossvalidation for more realistic accuracy  the partitioning into    blocks was done randomly  but in the same order
across all runs   all numbers reported are the mean of    fold cross validated accuracies 

    other
we adapted the smo implementation for the svm algorithm discussed in an earlier assignment to our data set  we
intended to compare these results with those from nave bayes  however the large sample size and high
dimensionality of feature space made this algorithm too slow to return useable results 
we also implemented the rocchio classification algorithm to test whether the centroid and variance of each class
comprised a good model for that class  however  this algorithm produced extremely inaccurate results  and often
predicted classes more poorly than random guessing  this indicated that our training examples were not oriented
in the spherical clusters assumed by rocchio classification  we did not include our results from these tests in this
paper and chose to focus on the nave bayes data 

  results
our prediction accuracies from the    fold cross validation tests using nave bayes are reported in figure    the
results have been grouped by the   classification types described above  and illustrate the different accuracies
achieved by using just word frequencies  just meta features  and a combination of both word and meta features 
for every classification type  we noticed a marked improvement when applying just the meta features compared
to using only word or a combination of features  the combined features led to inconsistent results  as they both
improved and worsened accuracy depending on the classification type  our graph clearly illustrates the improved
accuracy achieved when trying to predict between fewer classes  when predicting a comments modifier  all three
of our feature subsets did worse than the     accuracy achievable by random guessing  for classifying raw score 
all three of our tests beat random guessing accuracy of      with meta features leading at      but were still
much less accurate than the other types with   or   classes 
we also wanted to determine the isolated effects of the meta features  and ran a series of tests with nave bayes
with each individual feature  our results are displayed in figure    the graph represents the change in accuracy
achieved by each feature for the various classification types  rather than finding the difference from absolute
randomness  we realized that many of the classification types had one class with an overwhelming majority of
comments and calculated the difference from this classs percentage  when classifying by good  bad  and none
modifiers  for example      of the comments had no modifier  thus  simply labeling all comments as none would
have achieved     accuracy and simply measuring absolute accuracy would have reported a very deceptive  high
value 
the graph shows that the number of comments made by the user had the biggest impact out of all the metafeatures  but only helped when classifying raw score and good vs  bad with a score threshold of    when using just
meta features or a combination  the only improvements appeared for bad             this classification had two
classes of roughly even size  so our high accuracy numbers would be unachievable by simply classifying everything

fias the same  see below table for the confusion matrix   the results highlight what types of classification our
methods were effective in detecting 

confusion matrix for good vs  bad             using nave bayes   only meta features
actually
bad

actually
good

classified as bad           

    

    

classified as good

    

    

  conclusions
the results show that our chosen meta features are extremely powerful  as they consistently performed better by
themselves than word frequencies  this was a surprising discovery because we had originally expected the metafeatures to be a useful supplement to word frequencies  but not be able to classify so accurately alone  this has led
us to believe that so called good comments across articles may not share that many words  because each article
discusses a different topic  it makes sense that generalizations about shared words across all comments are hard to
make  our meta features probably performed so well because they capture fundamental differences between
commenting styles  as discussed above  we believe the unique nature of comments as an un edited forum for
publishing text leads to very useful  though specific features 
out of our tested meta features  we have also concluded that the number of previous comments made by the user
is the best indicator of comment quality  this could be attributed to the fact that returning users who consistently
participate in discussions have the intention of constructively contributing  users who infrequently use slashdot
have less incentive to post interesting comments because they wont care how their user rating is affected in the
future  these users would be more likely to leave offensive or purposefully instigating remarks 
what surprised us were the results of combining word and meta features  we had expected this to outperform
both of the individual feature groups  we believe this was a result of how we combined the feature groups in our
nave bayes implementation  our tests simply placed each meta feature value alongside the word frequencies
without accounting for weighting or normalization to make them more comparable  this would be useful to
explore in future research for developing a more effective algorithm to combine meta and word features 
in applying these meta features  we believe that the best method for classification is by labeling comments with
scores of   and above as good and those with scores less than   as bad  intuitively  such a division makes sense
as lower scores probably have more in common with each other as do higher scores  we were initially uncertain
whether a threshold of   or   would be best  but the results indicate that comments with score   have more in
common with higher scores than lower ones  we can apply this conclusion to slashdots automatic comment filter
and recommend hiding scores of less than    but displaying everything else 
our relative ineffectiveness at making specific score or modifier predictions emphasizes the importance of a
comments intrinsic meaning  simple metrics such as word frequency or number of previous posts failed to fully
capture this fact  and revealed that future research will have to attempt more sophisticated language processing 

  future research
the problem of classifying internet article comments is ripe for further research beyond what we have investigated
here  there are many possible avenues we can explore 
 don t make naive bayes assumption of word independence  our naive bayes  assumption that word
probabilities are independent is not completely true  to better capture the meaning of comments  we can
look at joint frequencies of noun adjective pairs  ignore nouns and look at only adjectives to extract mood 

fi








split up nouns and adjectives into different categories and then look at pairings  or try another  smart 
approach that tries to understand the meaning of a comment 
investigate other meta features  such as a user s history  e g  the average score of that user s previous
comments  his usual type of response  his ratings of other users   we can also use different weights for
different features 
take into account threads of comments  currently we are treating all comments as distinct  but some are
responses to others 
test additional algorithms  algorithms besides naive bayes were not useful because of the extremely high
dimensional feature space  we can look into reducing the dimensionality of the data or attempting to use
only meta features with other algorithms 
analyze other metrics besides accuracy  we can look at the entire confusion matrix for our results  instead
of only focusing on the overall error across classes  and use this as a metric for how well our algorithms
perform 
test other sites  many other sites besides slashdot have comments below articles  and it would be
interesting to see how our methods perform on those sites  especially the ones with different comment
rating systems 

these steps may help us more accurately predict the score and genre of a new comment  and at the same time
reveal some interesting patterns about crowdsourcing and comment posting on the internet 

figures

figure  

figure  

figure  

figure  

fi