ocr for mobile phones
kathryn hymes and john lewin

abstract
lighting  camera resolution and focus  skew  shear   and processing power all contribute to the
difficulties in developing effective ocr applications using mobile phones  we propose a novel algorithm
using pca  divergence  and a global neural network to develop an ocr package that levers user specific
parameters with a neural network trained on an extended data set 

 

introduction

the lack of accurate handwriting recognition  ocr  on mobile devices  using the on board digital camera  is
a barrier to the development of commercial applications such as document scanners  translators  and business
card readers  specific challenges for ocr on a mobile platform include
 low processing power
 low quality images
 tilt  skew  and rotations
 varied lighting and shadows
an unexplored advantage of mobile ocr is continuous generation of training data and the ability to maintain
local  user specific local training parameters as well as global  large scale training parameters 
a typical solution would apply variants of a   layer neural network  le cun et  all         however 
training a neural network on a mobile phone is computationally unfeasible  this approach does not lever the
advantages of user specific parameters that the mobile application offers  we present a novel three stage
algorithm that leverages dimension reduction  a global neural network trained externally  and a custom local
algorithm based on kl divergence as a package for mobile ocr 

 

data processing

pre processing is a sizable challenge that we only touch on here  we require images to be taken of handwriting
on a dual pad brand engineering pad  we have   working data sets 
 zipcode       a reference data set generated from       digits from zip code scans by the us postal
service  the data range is     
 iphone      symbols ranging from      the integral  and a few greek characters extracted and processed
from pictures taken with an iphone
 iphone    l  a data set of     pictures based on ten transformations each of fifty characters  the
data range is     
symbols are further normalized and adjusted for skew  the angle for adjusting for skew is the angle between
the top and rightmost line of the dual pad  these lines are obtained by extracting the dark points in that
region from the image  and fitting the line of highest variance using pca  after processing  each symbol is
represented as a grayscale        pixel matrix with values ranging from        or equivalently as a vector
with     parameters 
 

fifigure    iphone data pre processing

 

dimension reduction

we use principal component analysis to simplify the problem  we find the symmetry and high dimensionality
of our data lend to sizable dimension reductions while maintaining image quality  a reduction to n dimensions
is attained by projecting each data point onto the n greatest eigenvalues of the covariance matrix  e g  
let xi be a data point in the original high dimensional space  m the mean matrix  and v the matrix of n
greatest eigenvectors  then each is represented in the lower dimensional space as
yi   v t  xi  m  
figure   illustrates a reduction of a     dimension image to        and   dimensions  respectively along with
the mean zero image  we find that the  d projection is similar to the mean zero  with projections using   
and    eigenvectors are increasingly approaching the original zero image 
figure    dimension reduction using pca on a zero symbol

 
   

the kl divergence predictor
building the reference set

let xi be the ith element of a training set corresponding to symbol   for example  xi  is the ith image
corresponding to the number   in the training set   define x as
x   

  x
xn
n n

so that x   m  n  represents the estimated probability of a black pixel being found at the  m  n th coordinate
of a given symbol   the parameter  is the largest value in the image matrix  usually      
 

fi   

using x as a predictor

let y be an unknown symbol  normalize as above  y     y    for each symbol x    find the kl divergence
between x  and y given by
kl x    y    

  x
y  m  n 
y  m  n  log
mn m n
x m  n 

repeat the experiment for each   finally  predict y to be the symbol with the least divergence  e g  
y    arg min kl x    y  


   

testing and validation

we compare the kl divergence predictor to a vanilla  feed forward neural network with a single hidden layer
with    nodes  using the normalized post office data set and the processed iphone data set  the results are
show in table   
data set
zipcode     
iphone    
iphone    l

kl error rate
     
     
     

nnet error rate
     
    
     

table    kl testing versus vanilla neural network

   

analysis

table   is somewhat misleading  on larger data sets the vanilla neural network data has been shown to be
      effective  and multi layer neural networks represent the state of the art in character recognition with
success rates great than        le cun et al         furthermore  the iphone    l data set is quite limited
in size and interpretation of data should be weighed appropriately  that said  the kl approach does suggest
improved results for a specific persons handwriting  and in stark contrast to a multi layer neural network 
the computational requirements for training are feasible on an iphone or comprable hand held device 

 

   

the whole shebang  a global neural network with server generated parameters operating locally with a low dimension user
specific kl function as the decision function
overview

our system incorporates all of the above techniques  first  a custom global neural network is trained away of
the mobile device  each letter is evaluated locally with a kl divergence decision function  based on locally
generated parameters  in this way we lever the advantage of a neural network with use specific training
attainable in a computationally feasible way locally  in practice  we would would likely use a multi layer
nnet similar to what was described in le cun et al          except using a kl output function as follows 
each letter is extracted as in section    and reduced to    basis vectors as in section    each image is
furthermore subdivided into   quadrants  the hidden layer  as diagrammed in fig       the input layer is the

 

fifigure    left  symbol discretization k right  network diagram for the case of   symbols

number of symbols stored  and there are  x the number of symbols in the hidden layer as there are in the
input layer corresponding to   quadrants per symbol  this is calculated in much the same way as a regular
feed forward neural network  except the input is a matrix y   and the output function gn k corresponding to
the nth quadrant of the k th symbol is given by


kl y  kn  
gkn   tanh
c
where kn is an approximate distribution of the nth quadrant of the k th symbol corresponding to x from
section    y is a distribution corresponding to the nth quadrant of the unknown symbol  corresponding to
y in section    and tanh is the hyperbolic tangent  the constant is used to c adjust where the    boundary
lies  equivalently  c sets the degree of similarity required  measured by divergence  such that x and y are
classified as coming from the same symbol   this model has   major advantages 
 the power of the neural network is leveraged on an unlimited data set from as many people as possible
 the on phone calculations are limited to evaluation of the neural network and divergence calculations 
within the computational capabilities of a multimedia phone
 the on phone training data  the kn matrices  is user specific and therefore customized to his or her
precise writing style 
the final point is critical  we are generating a quasi customized algorithm for the phone user without
requiring him to train enormous amounts of data  or requiring heavy processing on his phone 

   

results

we trained the data using the zip code      data set and  independently  the iphone     data set  leaving
    out of the training data for testing  the weights were generated by training a single layer feed forward
neural network  using the gkn output function  the constant c was found before hand to be       for the zip
code      data set and       for the iphone     data set  coresponding to target divergence values of    
and          resulted in very few results being less than    on the iphone data set   our results can be seen
in table   

   

analysis

the results are encouraging  the network succeeds in obtaining the lowest error rate weve seen on this
data on a model customized for the user  furthermore  we have used a very simple neural network  a more
 

fidata set
zipcode     
iphone    

error rate
    
    

table    final error rates using the hybrid nnet
robust network such as that used in le cun et  all will likely generate even better results  as will improved
training data   however  there are two reasons to remain skeptical  first  the data set is quite small  second 
the nnet data is trained on the same data as the s are  so it is not clear how big of an advantage the
user specific gkn functions are  however  we believe the results are encouraging enough to warrant further
exploration 

 

references
 i  hastie  t   tibshirani  r   and friedman  j  the elements of statistical learning  springer     

 ii  ng  a  cs    lecture notes  fall     
 iii  suzuki et al  infty   an integrated ocr system for mathematical documents  doceng     grenoble 
france      
 iv  muller et al  an introduction to kernel based learning algorithms  ieee transactions on neural
networks  vol      no     march     
 v  barber  d  learning from data  dimensionality reduction      
 vi  shlens  j  a tutorial on principle component analysis  institute for nonlinear science  ucsd      
 vii  dailey  m  principal component analysis tutorial  asian institute of technology      
 viii  le cun  y   bottou  l   and haffner  p          gradient based learning applied to document regognition  proceedings of the ieee                  

 

fi