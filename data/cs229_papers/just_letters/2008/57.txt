synthesizing object background data for large   d datasets
david breeden  anuraag chigurupati
in collaboration with stephen gould  andrew ng
december         

 

introduction

in this project  we implement a   d data synthesis tool to generate a large number of object
recognition training examples by synthesis in various configurations of relatively few
background scenes and object models  inputs for our tool are     d background scenes and   d
object models  each background scene is comprised of a visible light image and corresponding
depth map  collected by stairs video camera and high resolution laser scanner  we
demonstrate a method to quickly and automatically generate scenes of the objects placed in the
background scenes  these scenes are intended to be representative of scenes stair would
encounter in the field containing real objects  and therefore good examples on which to train a
stair classifier for object recognition  we compare the performance of an object recognition
classifier trained on examples synthesized by our method against performance when trained on
actual scenes 

 

background

often  a learning algorithm s performance is dominated by the quality and quantity of available
training data  as seen in many learning scenarios  even simple algorithms with massive amounts
of data can substantially outperform complex learning methods with smaller datasets 
in the stair  stanford artificial intelligence robot  project  the goal of training a robot for
robust object recognition is hindered by this common problem of scarce data  the problem in
this context is exacerbated by the introduction of a new mode of input used in object recognition 
range data  while range data provides a wealth of valuable information about objects in a scene
     its integration demands greater time and expense in directly collecting training data 
sapp  saxena and ng        showed that synthetic data can be used to speed up data collection
and improve performance in image only object recognition      however  it remains to be seen
whether these methods can be ported to the new class of training data to achieve similar
significant results  the introduction of range data also presents new complexities in efficiently
synthesizing representative training examples 

 

methods

   

experimental setup

background scenes are captured using stairs imaging capabilities  stair is equipped with a
video camera used to capture     x     images and a high resolution laser scanner used to
determine range data  a     x     depth map to match the visual data is generated from the

firange data  object models to be synthesized with background scenes were obtained using a
commercial nextengine   d scanner  for each object  a dense point cloud was obtained for
rendering 

   

rendering synthetic data

our training set consisted of scene intensity and depth images with tight bounding boxes around
objects in the scene      the positive training set consisted of all such intensity and depth
patches containing the object  while the negative training set consisted of sufficiently nonoverlapping patches randomly sampled from the training set 
to generate a synthetic training example  we begin by collecting data for a background scene
using the stair imaging modalities  we run the greedy clustering algorithm from rabbani et
al  to find candidate points on a horizontal plane in the scene on which the object could be placed
in the background scene      a candidate point is chosen at random  and the object model is then
rendered into the scene at the candidate point by z buffering  z buffering simply requires
performing the following update on the scene for each vertex vi in the model 
vi   xi   y i   z i   ri   g i   bi

 

d proj   xi   yi     min d proj   xi   yi     z i
 ri   g i   bi
i proj   xi   yi     
 i proj   xi   yi  

 

d proj   xi   yi     z i
otherwise

for later use in anti aliasing  we actually render the object into an empty image  and also
maintain an object background mask  where each pixel of the scene is given value     if it was zbuffered from the object  or     if it remained from the background 
because the overall lighting intensity in scenes may vary significantly from the luminance of the
object when scanned  the rendered object must be color corrected to match the light intensity of
the surrounding elements of the scene  for each pixel in the object  its color is converted from
rgb space to l space and then the means and variances at each channel are matched with the
background means and variances      since our training images are grayscale  the intensity of
the pixel is given by the luminance channel 
in order to further reduce contrast from the surroundings and avoid jagged edges  we then antialias the rendered object  to do this  we smooth the mask with a gaussian filter and then
perform alpha blending between the object and background to obtain a final result with the
rendered object in the scene  corrected for lighting and ant aliased 

   

evaluating performance

to evaluate our approach  we looked at how gentle boost classifiers trained with our synthetic
data performed in comparison to classifiers trained with real data  real scenes were collected by
stair and each scene contained   objects  to generate our synthetic dataset  we randomly

fisampled    of these scenes as our backgrounds and created renderings from these backgrounds
and two   d stapler models 
obtaining the two   d stapler models took approximately   hours  and manually collecting a real
scene with the current configuration takes approximately   minutes  according to our timestamps
from data collection  since time spent synthesizing the data is negligible   the synthetic
approach significantly speeds up data collection  depending on the number of models  for
instance  for the same amount of effort it takes to collect a typical dataset of     scenes  we
could collect    stapler models and    background scenes  easily generating thousands of
positive training examples  where we would normally only be able to train on     

 

results

we trained four classifiers  training with    or    training examples of either real or synthetic
data  the following were our results 
maximum f  scores
   positive training examples
   positive training examples

real
     
     

synthetic
     
     

real vs  synthetic training      training examples
     

    

     

    
precision

real    
     
synthetic    
    

     

 
    

 

   

   

   

   

   

   

   

   

   

      
recall

 

execution time is dominated by clustering horizontal surfaces  which only takes a couple seconds per background
scene  however  the whole process can be automated and requires no actual attention from the researcher 

fireal vs  synthetic training      training examples
   

   

   

precision

   

   

real   

   

synthetic   

   

   

 
    

 

   

   

   

   

   

   

   

   

   

    
recall

surprisingly  synthetic training outperformed real training with    training examples  this can
largely be written off to the small training set size and resultingly erratic classifiers  but this does
imply that the two approaches are comparable  at    training examples  the real data is superior 

 

future directions

further evaluation and extension of the data synthesis tools might follow the following paths 
   testing with larger datasets  we have yet to see how the synthetic data compares to real
data at a reasonable volume  or see if it can generate a classifier that would actually be
useful  this is only a couple of experiments away  also  to truly test its efficacy versus
real data  we will train a classifier from a massive dataset as described above  generated
with effort comparable to a typical real dataset 
   testing with other object classes  our results show potential for scenes synthesized with
stapler object models  we are interested in whether these results can be generalized
across object classes  we have scanned object models of cups and mugs to create more
potential classes of synthetic training sets 
   recognizing objects positioned in non standard orientations  we currently train the
object recognition classifier only to recognize a single category of orientation  while the
placement of the object may be rotated about the normal to the support plane  no training
examples include objects tilted to a different orientation  e g   a stapler on its side  

fihandling such configurations will be necessary to build a sufficiently general object
classifier for stair  with the increased number of potential object configurations  the
massive number of training examples required for effective recognition further
strengthens the motivation for using an automated data synthesis tool for generating
training examples  additional care is needed  however  to ensure that nonsensical
synthesized orientations are not used for training 
   locally correcting for lighting  instead of relying on the statistics of the entire scene for
correcting the objects luminance  further realism could be obtained in the synthetic data
by responding to shadows and inffered light sources 

 

acknowledgements

we would like to thank siddharth batra  paul baumstarck  adam coates  and the members of
the stair vision group for their guidance 

 

references

    gould  s   baumstarck  p   quigley  m   ng  a  y   and koller  d  integrating visual and
range data for robotic object detection  in eccv workshop on multi camera and multi modal
sensor fusion algorithms and applications  m sfa         
    sapp  b   saxena  a   and ng  a  y  a fast data collection and augmentation procedure for
object recognition  association for the advancement of artificial intelligence  aaai        
    a  torralba  k  p  murphy  and w  t  freeman  sharing visual
features for multiclass and multiview object detection  in pami      
    t  rabbani  f  a  van den heuvel  and g  vosselman  segmentation
of point clouds using smoothness constraint  in isprs       
    e  reinhard  a  o  akyuz  m  colbert  and c  hughes  real time color blend of rendered
and captured video  in i itsec       

fi