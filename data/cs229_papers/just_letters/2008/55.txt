vision controlled autonomous indoor helicopter
sai prashanth soundararaj  arvind sujeeth
 saip  asujeeth  stanford edu
department of electrical engineering  stanford university
thanks to ashutosh saxena  asaxena stanford edu 
abstract  indoor navigation of aerial vehicles poses several
challenges in sustaining hover flights  the key challenges are the
high presence of obstacles in constrained environments  frugal
payload budget and the need for real time control response  in
this paper  we describe a successful application of computer
vision and machine learning techniques for autonomous panning
of rc helicopters in fixed indoor settings  our framework uses
images of the environment to learn its surrounding and
perform attitude estimation on live streaming data from an onboard wireless camera using the learned model  we then detail
the design of our controller  capable of hovering the helicopter in
place  sustaining orientation with respect to reference points and
following panning trajectories  finally  we present test results
based on simulations and actual autonomous test flights 

i  introduction
unmanned aerial vehicles have several indispensable
applications in surveillance  intelligence and information
relay  small radio controlled helicopters are particularly
suited for indoor navigation owing to their stable low speed
flight and in place hovering capabilities  the restricted nature
of indoor environments with walls and other obstacles 
however  pose unique challenges for autonomous navigation
and necessitate real time control response  frugal payload
budget and power restrictions of rc helicopters place
additional constraints on the use of on board sensors  these
make indoor autonomous navigation an interesting problem 
from both the vision and control aspect  we use monocular
vision and machine learning techniques to extract information
from images streamed from a light weight wireless camera
mounted on the helicopter and transmit back appropriate
control signals for autonomous panning 
several related work on autonomous helicopter flight in
outdoor settings          have led to terrific control algorithms
and are capable of performing extreme acrobatics  our
challenges are quite complimentary to these 

fig     diagram of helicopter panning

 indoor navigation is more of a perception and navigation
problem than a control problem  it is crucial to be able to
detect and avoid obstacles at close quarters in real time 
 the smaller rc helicopters are limited in their payload
capacities and suffer from air turbulence generated by its
own motion in closed spaces
 hazy images from the on board video capture are subject
to severe vibrations from helicopter motion 
to address these issues  our framework is composed of two
tightly coupled systems  the vision and the control system 
during the training phase  the framework forms a model of the
environment based on labeled images of its surrounding  this
model can be used to specify motion paths for flight  such as
stable hovering with respect to reference points or simple
panning trajectories  the vision system performs attitude and
motion estimation on images streamed from the onboard
camera  the control system uses these estimates to generate
panning control signals and interfaces with the helicopters
transmitter to efficiently control navigation for sustained
flight  fig    shows panning direction in a  d space 
ii  helicopter platform
our test platform is based on the eflite blade cx  coaxial
helicopter  fig      we started working with a dual rotor
model  exceed rc   but faced severe instability and lateral
drift issues  the coaxial model was chosen over this since they
provide stable flight and are considerably easier to control
than their dual rotors counterparts  a light weight wireless
camera is mounted on board  fig    inset  to stream images in
real time to the control algorithm  we use a commercial
interface  called the pctx      to link the transmitter to the
usb port of the laptop  via the trainer port  fig      this
enables the helicopter to be controlled via the pc using the
control system of our framework  we encountered several
compatibility issues in getting this to work since the
transmitter  tx  shipped with blade cx  was unable to accept

fig     the blade cx  helicopter

fig     the platform interfaces

fippm signals from the trainer port  which is what pctx uses to
communicate with the tx  we now use the spektrum dx i
tx  which is capable of translating ppm input to dsm signals
for controlling the helicopter 
iii  controller framework
we use techniques from computer vision and machine
learning to be able to quickly and reliably get an estimate of
our current position and velocity  we chose the k nearest
neighbors  knn  algorithm to classify a frame with respect to
its attitude because it is a fast  simple  data driven approach 
however  this alone is insufficient for fine grained navigation
and stability  we calculate optical flow to measure the relative
velocity from frame to frame  our goal is to combine these
estimates to produce sensitive control signals capable of
sustaining autonomous panning flight  fig    shows the block
diagram for the navigation system  the right side shows the
control path for live helicopter flight  while the left side shows
the control flow for the virtual controller  discussed in section
iv 
a  training the model
in the training phase we attempt to learn a model of the
environment  the choice of a constrained setting is an
important factor  an environment that is too dynamic  varying
from day to day tests  will perform poorly  we began with a
training set taken along an oval in the ai lab  a training video
is taken at each point by one of us holding the helicopter
facing a fixed reference point and rotating     degrees  each
frame in a video is then labeled using its angle w r t  a
reference point 








    

this procedure has some weaknesses  first  data collection
can be a slow process if there are many points in the
environment  second  it is difficult for a human to rotate at a
uniform velocity  lastly  the ai lab has many students and
many gadgets in a state of flux  to address some of these
problems  in the second half of the quarter we moved our
training environment to the  th floor of gates  just outside of
the elevators  fig     

fig     the training environment

fig     control flow diagram
this is an open space suitable for initial testing that does not
change much from day to day  we collected two sets of
training data along and within circles of varying radii  each at
a different height  for each point  we took a video rotating
first in the clockwise direction and then in the counterclockwise direction  we hypothesized that this would help
marginalize some of the human error involved from point to
point  we also experimented with methods of removing
human action from the process by rotating the helicopter on a
turntable  but found that we were unable to slow the rotation
to a satisfactory speed 
our final training dataset consisted of    videos and      
frames  each frame is     x       as this is a large amount of
data  we originally resized each frame to   x   to meet
memory and speed constraints for real time processing 
eventually  we found that we could improve our results by
using principal components analysis  pca  to reduce each
frame s dimensionality from        to     pca was
implemented using opencv s computationally efficient builtin eigen objects and eigen projections  instead of arbitrarily rescaling the image  using pca also helped us reduce the
classification s sensitivity to noise perturbations and height
variations  finally  we trained the k nearest neighbor s model
on the projected data using opencv s built in knn
implementation 
b  attitude and velocity estimation
when a new frame is received from the helicopter s wireless
camera  we extract the k nearest neighbors from its pca
projection  we incorporate the image history on the last five

fiiv  controller interface and simulator

fig     visualization of optical flow with attitude inset
classifications to select the closest prediction  i e  predictions
of the previous five frames are used to find the closest match
from among the k results   and that is output as the prediction
of current attitude 
optical flow is applied on the original full sized frame to
give us a measure of which direction the helicopter is moving
in  this velocity estimate will be fed into the control algorithm
and used to refine the magnitude of the turn command
generated  this provides a counter acting effect that helps
prevent the helicopter from turning too drastically  optical
flow is implemented using     features and opencv s
pyramidal lucas kanade  plk  algorithm      we calculate
the total optical flow as the mean over all the features for each
frame  with respect to the horizontal direction only  finally 
we apply a median mean filter to smooth the optical flow
values over time  fig    shows the calculated optical flow on a
frame  and the inset shows the attitude estimated by knn 
b  control algorithm
we propose an algorithm to combine the optical flow result
with the k nearest neighbor classification to produce an
updated attitude estimate for each frame  the update equation
is given as follows 
turncmd  max min  

   



   p th   n th 

a  controller interface
we have developed a functional gui framework for
remotely controlling the rc helicopter  our framework is built
on top of the opencv library and endurance s low level pctx
driver  it was designed to provide us flexibility in running
experiments  smooth control of navigation and effective realtime feedback  it also serves as the ui for training and running
the simulator 
the controller maps the transmitter controls to intuitive
keyboard presses  allowing the user to easily control the
helicopter in real time  it is multi threaded with non blocking
i o to ensure responsiveness  with a single key press  the user
can transfer control of the rudder  for panning  to the already
running vision control algorithm  while continuing to control
the throttle  aileron  and elevator from the keyboard  to
prevent the helicopter from receiving wild swings in control
signals which could damage its gears  the controller has a
configurable maximum change per channel per time step  the
frequency of updated channel values being transmitted to the
helicopter is also configurable from the interface  fig    shows
a screenshot of the controller during an actual flight  the
display presents several pertinent real time results to the user 
the current desired and actual channel values  the video stream
from helicopter s wireless camera  a radar displaying the
current desired panning angle and actual knn estimate  and
trackbars showing the knn estimate  optical flow values  and
turn command generated by the control algorithm 
b  simulator
we implemented a simple simulator  or virtual controller  to
verify our framework  the virtual controller forms a map of
the surrounding based on the test videos  it interacts with the
vision and control framework by accepting a turn control
command  retrieving the image frame that would have resulted
if the command was issued to the helicopter in the actual
setting  and passing that on to the controller  this allows for
simulating panning trajectories and visualizing the controller s
dynamics  the simulator was used extensively in the testing of
our system framework 

where
is the velocity estimate calculated by optical
is the desired angle we wish to navigate to 
flow 
is our current estimate from knn  and are constants used
as
to account for time and tune the result  we can alter
a function of time to make the helicopter follow a particular
trajectory at fixed altitude  turncmd is the control signal
generated by the algorithm  which is passed on to the
transmitter to autonomously control the helicopter panning 
the turncmd value is clipped to lie between the negative and
positive threshold  n th and p th  to avoid overcontrolling 

fig     screenshot of the controller interface

fiv  results
a  simulator reesults
f   and fig     show resullts of using the simulator onn a
fig 
traiining and a tesst video respecctively  while trrying to follow
wa
sim
mple trajectory   in these testss  the trajectoryy was defined as
conntinuous panninng back and fo
orth from   too      shown as
bluue lines in the figure  
f
the co
ontroller perforrms quite well on
botth training videeo and test vid
deos  the simuulated results are
a
bettter because alll of the fram
mes being classsified are draw
wn
from
m the videos taken
t
by us ro
otating in a cirrcle  this greaatly
redduces the num
mber of miscclassifications due to driftiing
outtside of the traiined area and altitude
a
variatioons  as comparred
to live
l test flight results 
r

fig     simulator
s
resullts for a traininng video

fig    
  simulator ressults for a test video
v

the figures
fi
do  however 
h
show
w some miscclassification
outlierss  these can bee attributed to issues
i
such as noisy
n
images
or lackk of features in
i the frame to classify it distinctly 
d
as
these misclassificatio
m
ons occur evenn in the idealizeed simulator 
they arre a contributiing factor for deviance from
m the desired
trajectoory in actual tesst flights 
b  teest flight resullts
the experiments
e
w
were
conductedd in our fixed environment
outsidee the elevators on the  th flooor of gates  each trial flight
was sem
mi autonomouus  we began by
b manually lifting off and
controllling the helicoopter using ouur controller innterface  we
adjusteed the values of each channell trim signal manually
m
until
a relativvely stable hovver was achieved  at this poinnt  we set the
rudder trim for the control algorithhm  this is the value of the
rudder the algorithm considers to be
b stable  we then press a
key to transfer
t
autonoomous control of the rudder to
t the visionbased control
c
algorithhm 
we chose a minimuum threshold tuurn command  n th 
 
of   
and poositive threshoold  p th  of
o     and exxperimentally
found
    
  and
     to correespond to an
approprriately scaled turn commandd  knn results for one trial
are shoown in fig     
  for this teest flight  we intended to
sustain orientation with
w respect to a point at       shown by
the straaight blue line    ideally  we should
s
be ablee to converge
to this position and hover
h
in its viicinity  our heelicopter was
able to correctly idenntify the generral direction of
o the desired
point  and
a pan towarrds that  the encircled secttion shows a
slice of time  about    seconds  where the heelicopter was
stably hovering at thhe desired anggle and knn was
w correctly
classifyying it  before deviating
d
awayy 
whilee these resultss are less thann exceptional   the general
trends are encouraginng  table   shows the actuual controller
output for a few fraames  as the helicopter appproaches the
desiredd angle         the magnituude of the turrn command
decreasses as we expecct 

fig      real
r results froom a trial flightt

fitable i
snippet from test flight
desired
angle
   
   
   
   
   
   
   
   
   

knn
estimate
     
     
     
     
      
      
      
      
      

optical flow

turncmd

     
    
    
    
    
     
     
     
     

     
     
     
     
     
     
     
     
     

unfortunately  we see a sudden discontinuity in classification 
which is a recurring problem  since the misclassification is
consistent over several frames  we believe that this problem
can be characterized as misclassification due to drift  and
occurs because as the helicopter turns it also drifts farther
away from the known environment  i e  outside the training
circle  
one way of mitigating this problem is to learn how changing
one trim affects another  trim correlations  and adjust
accordingly while turning to maintain a drift free hover 
another method is to collect more training videos at more
points in the fixed environment  our results also reveal that
the number of recorded frames is actually less than our
targeted frame rate     fps   this shows that we are not
actually processing in real time  and are dropping frames 
while it is unlikely that this has a significant impact on the
results  it is an issue that requires further investigation 
videos from our experiments are available online at
www stanford edu  saip heli html  they show both an
external view of the helicopter navigating as well as an
internal view of the controller and helicopter camera  it is
particularly striking to see the frequency of noise in the
camera stream  a consistent source of misclassifications and
poor optical flow values 
vi  conclusion and future work
our work demonstrates enough potential to conclude that a
vision based control algorithm  with appropriate training data
and tuning  can autonomously pan an rc helicopter to a
desired angle  we have developed a control framework for
running experiments that can be used as a foundation for
extending this work to other aspects of indoor helicopter
autonomous navigation  there are several key challenges
remaining that we hope to continue to pursue after this quarter
ends  first  learning trim correlations to combat the problem of
misclassification due to drift  second  being able to accurately
follow a live panning trajectory  third  extending autonomous
navigation capabilities to the aileron  elevator  and throttle
channels  and finally incorporating methods for obstacle
avoidance 
vii  acknowledgments
we would like to express our sincere gratitude to ashutosh
saxena and prof  andrew ng for their invaluable comments 

discussions and helpful insights throughout the course of this
project 
viii  references
   

   
   
   

g  eason  b  noble  an    andrew y  ng  h  jin kim  michael i  jordan
and shankar sastry  autonomous helicopter flight via reinforcement
learning  in the   th annual conference on neural information
processing systems  nips       
pieter abbeel  adam coates  morgan quigley and andrew y ng  an
application of reinforcement learning to acrobatic helicopter flight  in
proceedings of neural information processing systems     
www endurance rc com pctx html
optical flow implementation from opencv library and
template
code from david stavens  stanford ai lab  

fi