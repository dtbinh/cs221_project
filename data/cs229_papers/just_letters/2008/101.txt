channel selection for cognitive radio terminals
ling hung kung  suid          

 

introduction

due to the excessive need of wireless spectrum and the inefficiency in utilizing it  the technology of
cognitive radio  cr  addresses the issue of allowing unlicensed users to make use of the frequency
bands where licensed users is currently not active  from the hierarchical structure  cr users can
only grab resources under the premise of not interfering with the normal operation of the primary
system  ps   and this extra constraint complicates the original time varying wireless communication  by assuming bernoulli distribution for each channel and independence across channels  this
dynamic spectrum access scheme can be treated as a multi armed bandit  mab  problem for a
single cr user  where each channel is considered as a slot machine with some expected reward  and
this user is trying to get as much available bandwidth as possible  the key component of mab
problem is the tradeoff between exploitation and exploration  where the cr terminal tries to pick
the channel that has highest estimated reward from past history  and look for new channels that
might give even higher rewards at the same time 
there are different versions of mab formulation  in the case of stationary distribution  gittins
index is shown to be the optimal strategy for discounted mab in      and     apply it to cr  by
allowing channel distributions to change over time  whittles index is proved to be asymptotically
optimal under some constraints in       and it is shown in     that opportunistic spectrum access
is indexable and hence able to apply this strategy  however  the above approaches both assume
infinite horizon and maximize discounted reward  whereas in the wireless environment  we only
care about the reward obtained in a finite observation period  which leads us to the finite time
mab introduced in     and others  so far there is no optimal strategy to our knowledge  and we
would refer to different finite time algorithms with tuned parameters  in this paper we basically
follow the algorithms in     and       and proceeds as follows  in section   we describe the network
model in detail  and in section   we examine some common finite time mab algorithms  numerical
simulations are provided in section   to compare algorithms in different probability distributions 
and followed by the conclusion as well as possible extensions in section   

 

network model

consider a set of channels m            m   in a ps  and a cr terminal tries to use these channels
when they are free  or not occupied by the ps  the channels are temporally divided into discrete
time slots  and the cr terminal synchronizes to the ps such that the beginning and end of each time
slot is known  the probability that channel i is free is pi   i  m  in general we model the channels
using a stochastic process  but here we assume that pi is stationary to simplify the problem  the
terminal operates as follows  for each time slot t  the terminal chooses some channel i t    senses to
determine whether it is free  with probability pi t     and conducts its own transmission if it is  if
the channel turns out to be occupied  then the terminal needs to wait till the next time slot  and
choose some channel  maybe the same one  again  normally the terminal has no prior information
 

fiabout p    p         pm    and will learn some empirical distribution in the process of transmission 
 t 
let the reward of choosing channel i at time t be xi   then the goal of cr terminal is to maximize
p pt
 t 
the accumulated reward up to observation period t   i e  m
i  
t   xi   or to minimize the regret
p
p
 t 
of adopting this strategy  calculated by t p  i t xi   where p   maxim pi is the optimal
 t 
expected reward per time slot  from now on we simply assign   to xi if channel i is selected at
time t and not occupied  and   otherwise 

 

learning algorithms

   

static environment

most algorithms for finite time mab assumes stationary probability  as in            and the references therein  here we introduce some basic prototypes to compare their performance under our
network model 
     

upper confidence bound

this algorithm is derived from the index based policy developed in      where the index is the sum
of two terms  one is the current average reward  and the second term corresponds to the confidence
interval that both the true and average rewards fall in with high probability  the upper confidence
bound  ucb  algorithm first initializes by selecting each channel once  after that  for each time t 
ucb chooses channel i t  such that
 

s

log
t
 t 
i t    arg max xi  
 t 
im
ni
p
   
 t 
 t 
 t 
where ni is the number of times channel i has been chosen so far  xi   t    xi  ni is the
current average reward  and  is some parameter chosen to be   in      by letting         not only
it performs better in our simulation  but we also effectively reduce the upper bound of expected
regret from a factor of    an improved algorithm  ucb v  that considers the effect of the empirical
variance  is proposed in     and chooses channel i t  such that
v



u  t 
u x   x t      log t
  t  u i
i
c log t 
i t    arg max 
xi   t
   t  


 t 
im
ni
ni
where we are free to adjust  and c 
     

 greedy and its variants

the  greedy strategy consists of choosing a random channel with probability   and select the
channel with highest current average reward otherwise  here the choice of          is not specified 
however  this simple form of  greedy strategy is sub optimal for stationary probability distribution
because the constant  will prevent the terminal from choosing the optimal channel asymptotically 
a natural variant  greedyt  is to decrease  gradually by choosing t   min    t     we can also use
the decreasing factor log t  t instead of   t to get another strategy greedylogt  some discussion
on the regret bounds of the greedy family algorithms are given in     and     
 

fi     

softmax and its variants
 t 

recall that xi is the current average reward of channel i at time t  then the softmax strategy
 t 
chooses channel i at time t     with probability exp xi     z  t    where z  t  is the normalization
factor    r  is called the temperature and is free to users choice  similar to the case in  greedy 
we can gradually increase the probability that the channel with highest average reward being chosen
by setting t      t or t     log t  t  which we call them softmaxt and softmaxlogt 

   

stochastically changing environment

so far the algorithms above all use average reward as an index to compute which channel to choose 
however  in the time varying wireless channel  it is not reasonable to assign equal weights to all
observations no matter when we acquire them  one intuition is to forget old data and introduce
backward discounted reward by calculating the weighted average reward
 t 
xi

 

t
  x
 t 
ni    

   
it xi  

 t 
ni

 

t
x

it   i t    i 

   

where     i     is the discount factor for channel i that depends on how fast channel i changes 
and the weighting function decreases as t   increases  as in      now we can replace the average
reward used in ucb   greedy  and softmax with this weighted average reward  notice that this
new reward may not be applied directly to the variants of  greedy and softmax since exploration
is comparatively important in the dynamic environment  another possibility is to use sliding
window with width depending on how fast channel changes  which is proposed in     along with
some related regret bounds 

 

numerical simulation

in our simulation  we assume that one channel is either occupied by the ps  hence has a low free
probability  or not  and we test the stationary algorithms against the following three distributions 
distribution  
distribution  
distribution  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch 
  
  
  

ch  
  
  
  

the parameters adopted for our algorithms are ucb with         ucb v with        and
c        greedy with         greedyt with         greedylogt with        softmax with
         softmaxt with        and softmaxlogt with          after       iterations  the
results of average regret  variance of regret  and the percentage of time choosing the optimal channel
 ch   of different algorithms are shown in figure    these comparisons show that for  greedy
and softmax  gradually decreasing the percentage of exploration helps the algorithm to converge to
choosing the optimal channel  notice that in distribution    though the softmax family have small
average regret and high percentage of optimal choice  they exhibit extreme large variance in regret 
which is not a sign for good algorithm  besides  the regret bounds derived for these algorithms
may be too loose for smaller t   for instance  the modified bound for ucb in distribution   gives
        which is not only much larger than our empirical result  but also larger than the total
reward  based on the three indices that we tested  ucb v gives the best performance  but this
superiority may depend on the parameters that we choose  for the  greedy family  if we choose
 

fidistribution  

distribution  

   

  

average regret

  
  

 

  
variance of regret

   

  
  

distribution  

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

   

  best channel chosen

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

 

  

   

   

   

  
   

  
  

   
 

 

  

 

  

 

  

 

  

observation period

 

  

  

observation period

distribution  

observation period
distribution  

distribution  

   

average regret

   

  

variance of regret

   

  

    

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

 

  

 

  

   
    
  best channel chosen

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

 

  

   
    
ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

   
    
   

  

    
   

 

  

  
 

 

  

 

  

 

  

observation period

  

 

  

  
  

 

  

  

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

   
  best channel chosen

  

distribution  
   

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

variance of regret

average regret

  

  
observation period

distribution  

ucb
ucbt
greedy
greedyt
greedylogt
softmax
softmaxt
softmaxlogt

   

 

  

observation period

distribution  
   

 

  

   

   

   

  
  

   

  
 

 

  

  
observation period

 

 

  

  
observation period

 

 

  

  
observation period

figure    regrets and percentage of optimal action for algorithms under different distributions
larger   then in general we have larger mean and smaller variance  whereas the same thing holds
for larger  in softmax family  we demonstrate the relative variations of performance indices by
choosing different parameters  which is shown in figure    for greedyt  the optimal   s are     
     and    for these indices individually  and the actual choice of     if we decide to use greedyt 
depend on how the system evaluates these indices 

 

conclusion and future work

in this paper we transform channel selection in cr into an equivalent mab problem  examine several approaches and algorithms that deal with it  and run simulations to compare their performance
under stationary environment  there are several topics that we can keep working on  besides the
non stationarity of channels mentioned in section      we can study different channel models  such
as the gilbert elliot model used in       which treats one channel as a markov chain with two
states  busy and idle  yet preserve the independence across channels  more generally  channels can
be modeled as a partially observable markov decision process  pomdp  by introducing the correlation across channels  which is examined in       one other dimension is to introduce imperfect
sensing to make the scenario more realistic  as discussed in       finally  we can extend our topic
into multi agent system  where all terminals perform distributed learning without changing any
information explicitly  allowing the cr network to be effectively established in a simple manner 
 

figreedyt in distribution   for t       

softmaxlogt in distribution   for t       

   

   

   
average regret
variance of regret
percentage of optimal action

   

average regret
variance of regret
percentage of optimal action

   

  

  
 
 
  

 

  
 

 

 

  

  

 

 

  

figure    relative variation of performance indices versus parameter choice

references
    a  alaya feki  e  moulines  and a  lecornec  dynamic spectrum access with non stationary
multi armed bandit   in ieee  th workshop on signal processing advances in wireless communications  pp                
    j y  audibert  r  munos  and a  szepesvari  tuning bandit algorithms in stochastic environments  in algorithmic learning theory  pp                
    p  auer  n  cesa bianchi  and p  fischer  finite time analysis of the multiarmed bandit problem  in machine learning  vol      pp                
    n  cesa bianchi and p  fischer  finite time regret bounds for the multiarmed bandit problem 
in proceedings of the   th international conference on machine learning  pp                
    a  garivier and e  moulines  on upper confidence bound policies for non stationary bandit
problems        availble from http   arxiv org ps cache arxiv pdf               v  pdf
    j  gittins and d  jones  a dynamic allocation indices for the sequential design of experiments 
in progress in statistics  european meeting of statisticians  vol     pp                
    l  kocsis and c  szepesvari  discounted ucb  in  nd pascal challenges workshop       
    l  lai  h  el gamal  h  jiang  and h  v  poor  cognitive medium access  exploration  exploitation and competition  in ieee acm trans  on networking  oct        submitted 
    k  liu and q  zhao  a restless bandit formulation of opportunistic access  indexablity and
index policy  in  th ieee annual communications society conference on sensor  mesh and
ad hoc communications and networks workshops  pp            
     o  mehanna  a  sultan  and h  el gamal  blind cognitive mac protocols        available
from http   arxiv org ps cache arxiv pdf               v  pdf
     j  vermorel and m  mohri  multi armed bandit algorithms and empirical evaluation  in
proceedings of the   th european conference on machine learning  pp                  
     p  whittle restless bandits  activity allocation in a changing world  in journal of applied
probability  vol           
     q  zhao  l  tong  a  swami  and y  chen  decentralized cognitive mac for opportunistic
spectrum access in ad hoc networks  a pomdp framework  in ieee journal on selected
areas in communications  vol      no     pp                

 

fi