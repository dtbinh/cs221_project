learning business article sentiment based on stock
market performance
jeffrey schlosser and david li
introduction
the authors ultimate goal is to predict stock market trends based on a corpus of
business news articles published in real time  it is believed that stock brokers  as human agents 
will affect market prices by emotionally responding to financial news  as a result  articles that are
published on a certain day will have a strong correlation with financial events in the near future 
however  given that the scope of this problem is too large to face in a single quarter  the focus of
this project is on analyzing the sentiment of business articles published in the past 
identifying article sentiment can generally be a difficult and time consuming procedure 
fortunately in finance  the sentiment of an article can be reliably linked to the trend in relevant
stock market prices in the time period around publication  in the proposed algorithm  an article is
classified as positive when it occurs during a time period associated with a favorable market
response  since market response is quantitative and can be easily determined  our procedure
allows for labels to be applied automatically to a large corpus of training data  machine learning
is applied to learn a model of the training data  and to make predictions about the sentiment of
previously unread business articles 

methodology
our proposed business article classification system has three major aspects  data
collection  model training  and testing and evaluation 
data collection  several sources of training data were considered as the project matured  and
their progression is shown in the figure below 
rss feeds
 economy 

need more
articles

yahoo news need more yahoo news high bias
articles  multiple stocks 
 single stock 

cnn money
 economy 

figure    progression of data sources 
the data sources are shown in blocks  and the needs addressed by moving from one
source to the next are shown beside the arrows  shown in parenthesis below the sources are the
search criteria used to gather articles  rss feeds and cnn money were mined for general
articles about the economy  while yahoo news was searched for specific company s   overall 
cnn money was found to have a large volume of relevant financial articles concerning the
economy  so it was chosen as the preferred and only data source 
data harvesting is accomplished by searching cnnmoney com for business articles  then
filtering by date  a web interface is used to extract the title  summary  and or full text of each
article  and the porter stemming algorithm is used along with a specific vocabulary list  manually
or automatically specified  to generate a histogram of tokens  manual selection was performed
by picking the tokens which subjectively seemed most relevant to the authors  automatic
vocabulary selection was performed by eliminating all tokens with frequency less than a certain
threshold  then training on this data set  the most relevant positive words were those that had the
largest ratio between j y   and j y   after nave bayes training  and the reverse was true for the
most relevant negative words 

 

fifigure   summarizes the implemented data collection process  and its relationship to the
learning algorithm   

search
parameters

web
crawler

headlines 
summaries 
and or text

porter
stemmer

token
frequency

select
vocab

word frequency

evaluation

learning
algorithm

training and
testing sets

market
label

market
reader

figure    software flow chart 
training  both nave bayes and support vector machine  svm  algorithms were used to learn a
predictive model from the collected data  each article was labeled according to its date of
publication using the following metric based on the dow jones industrial average  dji  stock
quote  opening value the following day minus closing value the previous day  to ensure that a
close number of positive and negative training examples were used  thresholds for the dji metric
increase and decrease were implemented  articles that did not show an increase or decrease
greater than the thresholds were discarded  and the thresholds were adjusted until an even
number of positive and negative training examples were acquired  note that no causal
relationship was assumed between the article publication and the stock prices  some articles
could have caused the stock to rise or fall  while other articles could have described a stock rise
or fall in a given day  since the project goal was to classify the sentiment of financial articles and
not to predict future stock performance  this ambiguity in the causal relationship is acceptable 
testing and evaluation  nave bayes testing yields a measure between zero and one that a
single article is classified as positive  one way to act upon these results would be to place a
threshold at      classifying the article as positive if the output is greater than     and negative
otherwise  another choice would be to label an article as positive  negative  or uncertainthe
case where the measure is close to      using the latter metric  articles on which the algorithm is
uncertain are removed from the testing dataset  the remaining articles are labeled as confident
positive or confident negative  keeping in mind that an investor is interested in making decisions
using all articles published in a given time period  the arithmetic mean of all confident articles in a
given day yields information about a single day  a similar thresholding and prediction routine can
then be applied to data for each day  resulting in confident positive and confident negative days 
these predictions can then be validated from stock market data 
summary of terminology 
fraction of correct articles   the number of testing articles classified correctly divided by the
total number of testing articles
normalized confident days  confidence measure   number of days which are given a high
confidence value when testing with nave bayes divided by the total number of days considered
normalized correct confident days  accuracy measure   fraction of the days deemed confident
that have correct classifications
the fraction of correctly classified training examples  fraction of correct articles  is a
standard evaluation metric in machine learning  the two other metrics relate to the ultimate goal
of predicting future stock trends using business article sentiment  a human agent will only invest
in a stock if he or she is confident that a prediction will be correct  and will only profit on a correct
prediction  therefore a good investor is one that makes correct predictions on a high percentage
of days 

 

firesults

normalized confident days

nave bayes  the classification system proposed above was trained using nave bayes on
      articles  from                    and tested on      articles from                   
figure   represents the classification accuracy and confidence of a model trained on this data 

 
   
   
   
   
   
   
   
   
   
 

incorrect

     

correct
     

     

     

     

changing
search term

remove
weekends

remove
frequency
data

     

     

single and
fulltext

single and
summaries

pairs and
summaries

change
vocab
selection

figure    algorithm performance 
the vertical axis of figure   denotes the normalized confident day metric  which measures the
confidence of the model  the accuracy of the modelthe normalized correct confident day
metricis presented here as the ratio between the colors of the graphs  the percentage of
correct predictions made on confident days is presented above each bar  the horizontal axis
represents a progression in features and test parameters  the test parameters are detailed in
table   below  while exact weights on confidence and accuracy are decided by the user  the
data from these experiments indicates that certain test parameters yield clear performance
increases 
table    test parameters from figure   
label
single and
fulltext
single and
summaries
pairs and
summaries
change vocab
selection
changing
search term
remove
weekends
remove
frequency data

part of
article
full text

token
type
single

summaries

single

summaries

pairs

summaries

pairs

summaries

pairs

summaries

pairs

summaries

pairs

vocab
selection
frequency
based
frequency
based
frequency
based
human
based
human
based
human
based
human
based

search
terms
dow  economy 
stock
dow economy
stock
dow economy
stock
dow economy
stock
dow stock

removed
days
none

frequency
data used 
no

none

no

none

no

none

no

none

no

dow stock

smfs

yes

dow stock

smfs

no

 

fitotal article error

confident day error

   

   

training error

testing error

    

   

error  fraction 

error  fraction 

training error

testing error

   

   
   
   

   
    
   
    

 
 

    

     

     

m  articles in training set 

     

   
 

    

     

     

     

m  articles in training set 

figure    learning curves 
the learning curves presented in figure   on the predictions for articles  left  and confident days
 right  show that sufficient training data was collected  while the error may seem high in the
absolute sense  using thresholds yields good classification performance with the evaluation
metrics presented in the last section  the rate of decrease of the testing error has slowed for both
articles  left  and days  right  to the point where a larger training set would not yield substantial
returns  in addition  the shape of the curves suggests that bias could be further decreased  future
test would benefit from a larger set of features  such as those resulting from groups of three or
four words 
support vector machine  a series of tests employing the support vector machine method were
applied to the same data used in nave bayes in order to compare results from discriminative and
generative learning algorithms  different kernels  including radial basis  sigmoid  and linear 
yielded results that were within one percentage point of the nave bayes results 
future prediction  the previously described training metric  involving stock prices before and
after article publication  cannot be used to predict the future because it would require buying
stock before the article is published  a more practical technique would only consider price trends
on the day after the article was released  training on this modified metric yielded a model that
tended to make a high percentage of confident labels with low accuracy  for example  a typical
result using word pairs on all days of the week yielded a confidence of       in the past and
      in the future  with an accuracy of       in the past and       in the future 

discussion
test parameter selection  examination of the problem parameters can provide insight into this
machine learning classification problem  figure     testing shows that single token analysis on
articles with full text yields a high confidence  but low accuracy  a move to article summaries
drastically reduces the confidence  one explanation is that full text articles provide many words
that are not relevant to sentiment analysis and so accuracy drops  moving to examining token
pairs  instead of single tokens  and article summaries leads to a large performance gain  a word
pair offers inherently more information because it captures both words  as well as their
relationship  table   shows the   most relevant positive and negative word pairs and single
words  as determined by nave bayes training  clearly  the word pairs give more relevant
information than single words about economic article sentiment 

 

fitable    most relevant words and word pairs
negative pairs
stock slump
take profit
stock fell
stock slip
weak in

positive pairs
stock surge
stock gain
stock rally
edge higher
investor cheer

negative single
even
future
today
deal
into

positive single
tough
or
world
how
higher

analysis of the vocabulary selection shows that the single tokens are ambiguous because they
are taken out of context  while word pairs retain more context and more meaning  further
reducing the vocabulary with human input decreases confidence  but increases accuracy  the
intuition is that there is a smaller set of words that yield meaning  but these words are more
relevant  taking a subset of search terms  from economy stock dow to stock dow  yields
gains in accuracy because economy has weaker ties to the dow index  restricting
consideration to articles published on tuesday  wednesday  and thursday has a modest
increase in accuracy  since the dji only contains information on weekdays  trading is stopped
on weekends   the training metric for friday  saturday  sunday  and monday contains information
about days that are not adjacent  for example  news published on friday is labeled according to
the opening stock price monday minus the closing price thursday  news articles lose relevancy
after a certain time period  so accounting for only tuesday  wednesday  and thursday increases
accuracy  finally  analysis based on word presence in the article showed virtually no change
from analysis with word frequency  using article summaries  enough relevant information is
encoded in a short amount of text that repeated words are less common  and therefore less
significant to consider 
future training metric  the future training metric mentioned in the results section was not
expected to perform well in testing because of the ambiguity in the causal relationship between
articles and stock prices in the training set  if a training set was obtained that contained precise
times of article publication  the training articles could be labeled using stock market prices a short
time after article publication  this would ensure that stock prices were a reaction to article
publication  and testing results would likely improve 

acknowledgement
we would like to thank andrew ng  paul baumstarck  and ian goodfellow for their help and
insights in our project 

 

fi