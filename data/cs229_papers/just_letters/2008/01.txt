genotype prediction with svms
nicholas johnson
december         

 

summary

a tuned svm appears competitive with the fastphase hmm  stephens and
scheet         which is the current state of the art in genotype prediction  to
improve the performance of the svm  a large number of simulated training
examples must be created by randomly pairing haplotypes in the training set 
a window size parameter which determines how many adjacent snps act as
features was also investigated and it is coarsely tuned by cross validation 

 

introduction

a snp marker  single nucleotide polymorphism  is a position in the genome
in which a single nucleotide varies from individual to individual  the flanking
sequence is generally shared  so a microarray probe can be designed to detect
the presence of either state of the snp  for example one individual may have
a sequence aagtta and another will have the sequence aaggta  the microarray provides estimates of an individuals genotypes  and the haplotypes
must be inferred by statistical methods 
scheet and stephens        introduce a hidden markov model  named fastphase  for haplotype inference  they find that it gives worse performance than
the phase ii method  also proposed by stephens and scheet   but that surprisingly it is better at imputing genotypes at missing snp sites  they then argue
that imputing missing genotypes may ultimately be more important than accurate haplotype inference  servin and stephens        develop a methodology
for genotype association studies  the implementation available to practictioners uses fastphase and the markers measured by a microarray to infer the
genotypes at the much denser set of markers in the datasets provided by the
hapmap project 

 

svms for genotype imputation

this is a pure prediction problem  and the svm may provide better performance
despite making no use of the special structure of the data  the svm is a black
box model  but so is the hmm since its performance comes from averaging over
multiple restarts of the em algorithm 

 

fi   

simulation setup

the data in experiments to follow will be simulated from the cue  european  
yri  yoruban   chb  chinese  and jpt  japanese  panels of the hapmap
dataset  the hapmap consortium has posted inferred haplotypes at the set of
bi allelic markers for these populations and they have used the time consuming
phase ii model  the first two panels made use of parent child trios to phase
all but the triple heterozygous positions  so these haplotypes are very accurate 
i have taken the posted haplotypes from chromosome    and thinned to the set
of snps which are shared in all four panels  the reason for this thinning is
that the sample sizes are small and including the asian samples may improve
the predictive performance when inferring genotypes in european samples 
i then take a set of      adjacent snps and divide them into two groups 
three of four snps are marked as unobserved and will be missing in test samples  the remaining     of snps will be used to predict at the unobserved
locations       of snps are present in the training samples 

   

increasing training set sizes

if we are inferring genotypes for an individual of european descent  the hapmap
ceu panel only provides us with    samples  after removing children from the
trios   the asian panel can be used to augment the training set and double the
sample size  if we had many european training samples  adding samples from
a different ethnic population could hurt performance  in this situation we are
starved for training samples  so adding the asian panel improves performance
when predicting genotypes in samples of european descent 
the next way to increase the training set size is to simulate individuals based
on inferred haplotypes  the hapmap data is provided phased and so we can
random select two individuals from our training set  and randomly pair two
of the four haplotypes to produce a new set of genotypes not observed in the
training set  we repeat this to produce       simulated individuals to increase
the training set size  a graphic representing the process is shown in figure      
as an example  suppose we select the one haplotype from the   th training
sample and it is         it will actually be of length       in the experiments
to follow   we select another random individual  not excluding the   th  and
take one of their haplotypes           we then pair these to get a simulated
sequence of genotypes          and we add this into the training set  we allow
the possibility of selecting the same sample haplotype twice  so we could have
ended up with the homozygous sequence        

   

svm kernel and features

i have tried both the radial basis kernel k x  y    exp kx  yk    and the
polynomial kernel  x  y   c       the parameters  and c  are left untunedand
set to their default values of   dim x  and    the svm package i am using 
e     which is an r interface to libsvm  has a default value of the complexity
parameter c      the software also implements regression svms and they gave
similar performance 
a more important tuning parameter is the number of flanking snps to use as
features  if we are predicting the genotype at a position t  on the chromosome 

 

fithen a snp at position t  is less likely to be informative if  t   t    is large 
if we predict with too few flanking snps we will not have enough information 
whereas if we predict using too many  the distance kx  yk  will be random 
we define a window of size k to be the k nearest observed snps  distance
could be defined in terms of the index of the snp  i e              the physical
position on the chromosome  or the genetic distance  the genetic distance is
in fact estimated using an hmm  so rather than take this approach i have fit
a distance measure to the observed correlation matrix between markers instead
 next section  

   

snp distances

if we have n snps we can construct the n by n correlation matrix c whose
ijth element cij equals the correlation between the phased  binary snps with
pj 
indices i and j  we then model  cij   by exp  k i dk   for positive distances
dk      the idea is to create a distance function d i  j  which is increasing in
 i  j  and takes into account the correlation structure of the data yet is still
fast to compute 
by differentiating the squared error criteria f  d  we get a gradient descent
formula

f  d   

x

 cij    exp 

j 
x

i j

 f  d  ds    

   
dk

   

k i

x

exp 

ij is j

j 
x

 
dk

 cij    exp 

k i

j 
x

  
dk

   

k i

pj 
if we define a matrix a with elements aij     i   j  exp  k i dk    cij  
pj 
pi 
exp  k i dk     and a second matrix bij     i   j  k   aik   then we have
a formula allowing efficient computation
 
  
j 
j 
x
x
x
x
exp 
dk
 cij    exp 
dk
 
bjk
   
ij is j

k i

k i

jk

figure     shows an observed correlation matrix and the fit  these distances
allow the window of predictive snps to use more to the left than say the right
if those on the left are more correlated overall 

   

coding genotypes

if we arbitrarily label the three genotypes as aa  ab  bb  then the genotype g
of jth predicting snp can be used in a few different ways  but i will focus on
two  the first is two code genotypes aa  ab and bb as     and   respectively 
the second is two use to binary features  the first being   for and ab or a bb 
and the second being   for a bb  this coding yields a larger distance between
aa and bb than ab and bb in the gaussian kernel  the     coding seems a
reasonable choice  but in results shown later it can hurt the performance quite
a bit 

 

fip
figure    the left panel shows fitted positions pi    ji dj   the middle panel
shows the observed absolute
pj  correlation matrix  the right panel shows the fitted
matrix  cij     exp  k i dk   

   

comparison

test error rates will be measured on six ceu  european ancestry  samples at
    of       snps on chromosome    these six samples have   of   genotypes
masked as missing  figure     compares fastphase  trained with phased data 
predicting on unphased   to the svm at various settings  the asian panel
samples are added to the training set  but all test individuals are of european
ancestry 
the legend indicates whether voting amongst   classification svms was performed  class  or a regression svm was fitted  eps reg   all svms were fitted
using simulated training samples except for the points labeled n  the addition
of these extra training samples seems to have a bigger impact on performance
than the choice of kernel 
the x axis of the figure is the window size    flanking snps   in this case we
do not use the fitted distances to choose the nearest snps  but just look at the
marker index instead  small window sizes do not provide enough information
to predict the unobserved genotype  but performance does not appear to be too
sensitive to window size as long as it is large enough 
next i looked at a disjoint set of       snps and repeated the experiment
on   different test sets consisting of     of the ceu samples  now i use the
fitted distances described above and choose the window size at each marker
based on an inner loop of   fold cross validation  to reduce computation time
the choice is restricted to a window size of either    or     the fitted distances
were derived from all of the samples  so there may be some bias in their use 
figure     has five columns each showing   ratios of the svm error rate to
the fastphase error rate on each of the   test sets  unless otherwise noted 
the fitted distances were used and genotypes were coded as      or   in a single
feature  the svm cost and kernel parameters were left at default values  i
saw no benefit in attempts to tune these   the difference between the five
columns is that the  st uses a gaussian kernel rather than polynomial  the
second codes genotypes with two     valued features each as described earlier 
the third uses marker indices rather than the fitted distances  the fifth uses no
simulated genotypes in the training set 

 

fithe polynomial kernel with fitted distances and simulated samples dominates the hmm  but only slightly  the benefit to using distances fitted to the
correlation matrix is very minor and simply using the snp index would suffice 
on the othe hand  simulating extra samples by randomly pairing haplotypes improves performance significantly  overall the performance is competitive with
the hmm except in the case of column      i see no reason that this alternative
coding of the genotypes should worsen performance by this much 

 

conclusions

the svm appears to be competitive in the dataset considered  but i would not
recommend it over the hmm with such a small training set size  the reason is
that the hmm requires almost no tuning and can easily handle missing predictor
genotypes  so without a clear performance advantage  there is no reason to use
the svm  it is also worrisome that a reasonable change to the genotype coding
can result in such a large increase in error rate 
with a larger training set size  i believe the svm would dominate the hmm 
it is not clear when such a training set would become available  however 

 

bibliography
 scheet  p  and stephens  m          a fast and flexible statistical model
for large scale population genotype data  applications to inferring missing
genotypes and haplotypic phase american journal of human genetics 
         
 servin  b  and stephens  m          imputation based analysis of association studies  candidate regions and quantitative traits  plos
genetics 
 stephens  m  and scheet  p          accounting for decay of linkage disequilibrium in haplotype inference and missing data imputation american
journal of human genetics               

 

fi