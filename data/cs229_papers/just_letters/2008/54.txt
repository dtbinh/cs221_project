finding common opinions in user generated reviews
brett miller  brettm cs stanford edu
a large and growing body of user generated reviews is available on the internet  from product
reviews at sites like amazon com to restaurant reviews at sites like yelp com  for users making
a purchasing or dining decision  the opinions of others can be an important factor  although
some aggregate information    like average star ratings    for multiple reviews is sometimes
available  in general the only way to determine common views among users is by reading
through many reviews  as the number of reviews for a single product or restaurant becomes
large  on the order of hundreds   it becomes increasingly impractical to read every review 
some techniques are commonly employed to compensate for this  such as ranking reviews by
usefulness  as determined by readers  since readers are most likely to read only the topranked reviews  however  this approach likely leads to a reinforcement of existing useful
reviews  while relegating new  unread reviews to the bottom of the list 
a more sophisticated approach  and the focus of this paper  is to apply machine learning
techniques to the problem  the goal of reading multiple reviews is viewed to be determining
the most common specific opinions of reviewers  and informally  we wish to let a machine
exhaustively read every review for a product or restaurant and automatically find cohesive
groups of opinions that are both closely related and widespread  formally  we can break the
problem into two concrete machine learning tasks      apply supervised learning techniques to
classify each sentence in every review as either opinion or non opinion and     for all sentences
classified as opinions  apply unsupervised learning techniques to cluster those that are closely
related 
the remainder of this paper describes the system proposed to achieve this goal  following a
description of the corpus used to test the system  implementation details  test results  a
discussion of their meaning  and conclusions will be given 

corpus
for the purposes of this project  the corpus consisted of diner generated restaurant reviews
available at yelp com  reviews were collected exhaustively for   restaurants  reviewer names
were discarded  each review consists of a star rating    to    and the body of the review 
statistics are shown below in figure   
figure    corpus statistics
restaurant

total
reviews

total
sentences

length range
 words 

average length
 words 

coi

   

    

     

   

cortez

   

    

     

   

evvia

   

    

     

   

pesce

   

    

     

   

plouf

   

    

     

   

tamarine

   

    

     

   

training data for classification was obtained by sampling without replacement      sentences
 approximately     from the corpus and manually labeling each as opinion or non opinion 

implementation
in order to achieve the objective of finding common opinions across multiple reviewers  the
processing pipeline of figure   is proposed  each major stage of processing including sentence

fiparsing  naive bayes classification  k means opinion clustering  and cluster ranking  is
described in more detail below 
figure    processing pipeline
reviews

parser
k means
clustering

opinions
sentences

clusters of
opinions

naive bayes
classifier
nonopinions

ranking
algorithm

top ranked
clusters

review parsing
a sentence is considered to be a sequence of one or more words delimited by a period 
exclamation point  or question mark  sentences are tokenized on white space after casefolding and removal of non alphanumeric characters 
opinion classification
the first stage of the processing pipeline requires classification of sentences into either the
opinion or non opinion class  because it is relatively simple and often competitive with more
complex classifiers for text applications  a multinomial naive bayes classifier with laplace
smoothing was chosen for implementation  the input feature for each sentence is an n dimensional term vector  with n equal to the size of the dictionary over the entire training set 
thus  element i in an input vector contains the raw term count for term t i in the dictionary 
the training and testing algorithms are implemented as detailed in the lecture notes and they
 
will not be repeated here 

 

also   
in an attempt to compensate for our skewed training data  only     of examples are
labeled opinion   an alternative complement naive bayes  cnb  classifier  as described by
rennie et  al      has also been implemented 
opinion clustering and ranking
the next stage of processing requires finding common opinions among the set of opinions
output by the classifier  again  for simplicity and because it often produces good results  an
implementation of the k  means clustering algorithm was chosen  as usual  input is the set of
m opinion vectors  where each is normalized and of dimension equal to the size of the
dictionary over all reviews from a single reataurant 

 o         o m      where un normalized

 

o i 
j   occurrences of term t j in opinion i

the iterative algorithm to produce k clusters from the m input vectors is implemented exactly
as detailed in the lecture notes  and only the relevant notation is reproduced here  for clarity 

 

 

 

c  i    cluster assignment for opinion vector o i 
  centroid j
 
 j   cluster
 

 

 

 

 

fibecause we do not know  a priori  the optimal value of k   we need a way to evaluate the
quality of the clusters produced by the algorithm for a given value of k   and then choose k
such that the quality of the clusters is maximized over some reasonable range of k   for this
specific application a cluster of high quality is considered to be one that contains many
opinions  tightly grouped around the centroid  formally 
the quality of cluster j is measured as
 
 
  cluster
the product of inverse residual sum of squares  or rss  a standard
measure of internal
 
quality      and cluster cardinality 

q  j    

 o i    c  i    j 
 
  o i    j

 

i c   i     j

thus  large clusters of unrelated
opinions have low quality  as do very small clusters of closely
 
related opinions  having obtained cluster assignments for the optimal value of k   each cluster
q  j    
j is then ranked on the basis of its quality
 

results
 

 

first  results for the naive bayes classifier are presented  ten fold cross validation test set and
training set error as a function of the number of training examples  m   is shown in figure   
note that the alternative cnb classifier had nearly identical performance  so the results are
omitted 
its also informative to look at the top    words with the  highest predictive value for the
opinion class 
excellent  loved  those  atmosphere  yum  best  setting  slightly  leaving  client

there are two important means of evaluating the results of the clustering and ranking
algorithms  first  we can observe how the average quality measure changes as a function of k  
these results are shown in figure    second  we can manually inspect the top clusters  after
ranking  for optimum k produced by the algorithm to assess how well they represent common
opinions  we save this for the discussion 

 

figure       fold hold out cross validation error vs  training set size

 

fifigure    average cluster quality  q  vs  number of clusters  k 

discussion
as figure   makes clear  classifier performance is poor  even in the best case     fold crossvalidation test set error is        while training set error is around        because both
forms of error are relatively high  even for increasing training set size  the classifier likely
suffers from high bias  probably due to our choice of term frequencies alone as features  there
are several important observations to make about the nature of the classification problem that
might help explain the poor performance  first  unlike related sentiment classification problems
     we are trying to classify short sentences  consisting of few words  so there is very little
evidence for the sentence belonging to either class  second  it was probably unrealistic to
expect opinion to be highly correlated with word features alone  there are clearly cases where
strong words like excellent  loved  and yum  from our top    list  easily predict opinion  but in
general the expression of opinion is quite nuanced  and the true sentiment of a reviewer might
be implied rather than explicit  even hand labeling the training examples was sometimes
difficult  as the distinction between opinion and non opinion was unclear  taking into account
these and other challenges  such as the presence of sarcasm  it seems that a more
sophisticated set of features is needed for good classification 
in contrast to the difficulty of classifying opinion  the results of k  means clustering are very
promising  first  looking at figure    we see that for all restaurants  there exists a clear peak in
average cluster quality as a function of k   additionally  the shape of the plots makes sense 
with quality initially increasing as the model more closely fits true clusters in the data and then
sharply falling off as k becomes too large and good clusters
  start to fragment 
furthermore  when we examine
  top ranked clusters for each restaurant  many seem to be
useful in that they indeed contain closely related opinions that are expressed by many
reviewers  although
there are frequently non opinion sentences in the clusters as well  due to
 
the poor performance of the classifier  several examples of top ranked clusters are provided in
figure   

fifigure    examples of high ranking clusters
highly ranked cluster of    opinions
on sea bass dish from restaurant
evvia  k    

highly ranked cluster of   opinions on
mahi mahi dish from restaurant plouf
 k     

  sea bass   very light but had
the right amount of flavor
  we ordered the striped sea bass 
the moussaka  and the lamb chops
for our entrees
  the striped sea bass was served
on a bed of wilted greens and was
delicious
  the lavraki psito  sea bass  is
also a great entree if you re
looking for seafood
  the sea bass was fresh and light
in flavor  allowing the natural
qualities of the fish to shine
  the sea bass i ordered was
simply grilled and dressed with
lemon juice and oregano
  the sea bass is likewise
excellent
        more 

  the mahi mahi appetizer was great
  i ordered the mahi mahi atop
cranberry fusion cous cous and
grilled bok choy
  the mahi mahi was drizzled with
crushed olives  which was a bit
overpowering in taste
  mahi mahi with five spiced
couscous  baby bok choy and
cranberry onion compote
  however  the mussels  which is
their specialty  and mahi mahi was
delicious
  i could just sit there all day
munching on those mussels  and the
mahi mahi was soo moist
  afterwards i ate their mahi mahi
  the mahi mahi seemed undercooked

there are also many examples of high ranking clusters that meet all of our criteria for quality
but are probably less useful because the words they have in common are descriptive  but not
specific  for example  a cluster might form with opinions that all use the adjective excellent but
describe different aspects of a restaurant 

conclusions
to address the growing body of user generated reviews available on the internet  a system to
automatically extract clusters of widespread  common opinions using a combination of
supervised and unsupervised learning techniques has been proposed  a simple application of a
naive bayes classifier using words as features performs poorly at classifying short sentences as
either opinion or non opinion  likely suffering from high bias  this is probably due to the fact
that classification is at sentence level granularity and that words alone are insufficient as
features  however  k  means clustering  coupled with an application specific quality measure
both for finding optimum k and ranking the resulting clusters  performs well  many top ranked
clusters meet the subjective criteria initially proposed 

 
references

 

    rennie  jason d  m   shih  lawrence  teevan  jaime  and karger  david r  tackling the
poor assumptions of naive bayes text classifiers  proceedings of the twentieth international
conference on machine learning  icml        washington d c        
    manning  christopher d   raghavan  prabhakar  and schutze  hinrich  introduction to
information retrieval  cambridge  cambridge university press       
    popescu  ana maria and etzioni  oren  extracting product features and opinions from
reviews       

fi