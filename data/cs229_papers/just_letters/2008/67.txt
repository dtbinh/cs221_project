an automated microsurgery in drosophila
for high throughput brain imaging
matthew golub and bikiran goswami
cs    machine learning final project

   abstract
we demonstrate a supervised learning approach to tracking the location of live 
immobilized flies for purposes of performing automated laser microsurgery to prepare flies for
in vivo two photon imaging  we explored least squares estimation and multi class logistic
regression for classifying pixels in a digital image as either fly eye  fly skin or background  the
multi class logistic regression classifier outperformed the least squares estimator across nine
different experiments conducted with incremental training and testing image sets  we used kmeans clustering of machine labeled image pixels to estimate the location of fly eyes in test
images  when clustering pixels classified as eye point by the multi class logistic classifier  kmeans correctly located the fly eyes in      of our largest test set  which consisted of sixteen
images with varying amounts of defocusing 
   introduction
   

background
drosophila melanogaster  better known as the fruit fly  has long been studied as a model
organism  with a well characterized genome  drosophila genetics can be manipulated in the
laboratory with relative ease  understanding the fruit fly is tremendously relevant to solving
problems in human health  since roughly fifty percent of a flys protein sequences have an
analog in humans  from a neuroscience perspective  drosophila are particularly interesting due
to the simplicity of their neural circuits  which perform computation to guide behavior  a major
challenge to the drosophila brain research community has been in acquiring statistically
significant sets of neural data 
fly brain imaging requires an unobstructed optical path between brain and microscope 
in the past  highly trained biologists have manually dissected the fly head  removing areas of
tissue that protect the brain  this painstakingly slow process has severely limited the process of
decoding the circuitry of the fly brain in recent years  a high throughput imaging system must
utilize a fast  accurate and automated microsurgery to overcome the limitations associated with
manual dissection 
we have demonstrated a laser ablation strategy for removing protective tissues
surrounding the fly brain  a human operator mounts a fly in an acrylic restraint and uses a
commercially available laser microdissection device to ablate a    x    micron region above
the fly brain  we aim to automate this entire procedure  enabling imaging of tens or hundreds of
flies in a single session 
   

an automated laser microsurgery
to automate this microsurgery  we must teach our laser microdissection device to
intelligently locate the desired ablation region on each fly  from a low magnification image of a
mounted fly  the computer must first identify the position of the fly and navigate a motorized
stage such that the fly is placed under a set of microscope objectives  next  the computer must
analyze a   x magnification image of the fly to locate the    x    micron region that we are

fiinterested in ablating  once this region is located  the computer uses a   x magnification image
to autofocus the camera and laser on the fly head 
   

computer vision implementation and drawbacks
we have implemented computer vision techniques to carry out this three stage
positioning process  from the low magnification image  we are able to detect the approximate
location of the fly head using geometric cues from the acrylic mount  for more precise tracking
of the fly head  we use color information to identify the flys eyes  specifically  we apply color
intensity thresholding to choose eye seed points from a   x magnification image  region grow
over the seed points  and then dilate the region grown image to remove false positives  k means
clustering is used to identify the centroid location of each eye  finally  we apply edge based
autofocusing on a   x image centered between the fly eyes 
the low magnification detection and high magnification autofocus steps are
computationally inexpensive  and they are robust against color and lighting variation since they
operate only on grayscale intensity images  the computer vision based   x eye detection
algorithm is very computationally expensive  however  and thus not well suited for real time
application  further  the eye detection is highly dependent on color information  which must be
hard coded into the algorithm  these major drawbacks motivated a machine learning based
approach to detecting eyes 
   machine learning approach to fly eye detection
   

task definition
we employed supervised learning techniques toward
classifying image pixels as belonging to one of three
classes  fly eye  fly skin or background  two independent
methods were compared for classifying image pixels  leastsquares  ls  and multi class logistic regression  mclr  
each classifier was trained using human labeled images
regions of eye  skin and background were processed into
training examples 
   

algorithm definition
each pixels red green blue  rgb  and hue 
saturation value  hsv  color intensities were used as
training features for the ls and mclr classifiers  we
implemented a one versus all classification scheme for
mclr  prediction of a new images pixel labels are made
by multiplying each pixels    r g b h s v t vector by the
weights vector computed by ls or mclr  and thresholding
the result accordingly  across an entire image  this
prediction requires far fewer computations compared to the
region growing method previously described  after pixels
are classified  k means clustering is used to find the centroid
location of detected fly eyes  example input and output
images for ls and mclr are shown in figure   

figure    example classification
performance  in focus  a  and    um
defocused  b  images of a fly head under
  x magnification are classified using ls
 c and d  respectively  and mclr  e and
f  respectively   black points have been
classified as eye  gray points as skin  and
white as background  k means estimated
fly eye locations are shown in red  in both
of these examples  ls fails to correctly
locate the eyes  while mclr succeeds in
both examples 

fi   experimental evaluation
   

construction of training image sets
to evaluate the effect of image focus on training
efficiency  we constructed three training image sets consisting
of combinations of in focus     um defocused  and    um
defocused images  each training image set consisted of   x
magnification images with rectangular regions hand labeled
as either eye  skin or background  only the pixels within
these hand labeled rectangular regions were used as training
data  an example training image is shown in figure    we
s
chose a small training set  x train
  to include four in focus



m
s
images  a medium training set  x train
  to include x train
and
three additional    um defocused images  and a large training
l
m
set  x train
  to include
and three additional    um
x train

l
defocused images  note that x train
includes in focus     um


defocused  and    um defocused images  we trained each
classifier   ls  mclr  on each of the training sets 
s
m
l
in six weight vectors 
  x train
  x train
  x train
   resulting


figure    example hand labeled
training image  pixels within the
red  yellow and blue rectangles are
designated as eye  skin  and
background  respectively  pixels
outside of these rectangles are not
used for training 

   

construction of testing image sets
we evaluated machine labeling performance on three
testing image sets consisting of untrained images with identical
distribution to the training sets with respect to image focus  we
s
m
l
denote the test sets as   x test
  x test
  x test
    for the small  medium 



and large training sets  respectively  we applied each of the
nine weight vectors from the training phase to each of the three
testing image sets  resulting in eighteen machine labeled images 

machine
labeled images were compared against handfigure    example hand labeled
testing image  red  blue  and black labeled versions of the testing images  hand labeled testing
regions correspond to eye  skin 
images differed from the hand labeled training images  in that
and background labels 
the testing labels were applied to every pixel that could be
respectively  some points remain
unambiguously identified as eye  skin or background  some
unlabeled due to the ambiguity of
ambiguous pixels were left unlabeled  to prevent algorithm
their true class labels  these
unlabeled points are not considered performance penalization in regions that do not clearly belong to
while assessing machine labeling
one class or another  an example hand labeled testing image is
performance 
shown in figure   
   

results
we use a performance index  pi  as a metric for quantifying machine labeling performance
with respect to hand labeled testing images  we calculate pi according to equation   

fiequation   

pi  

    correct labelings    incorrect labelings 
total   hand labeled pixels

we calculate a three pis for each machine labeled image  corresponding to the three classes 
eye  skin  and background  the performance results  summarized in figure    show mclr
outperforming ls inall nine experiments  for all classifiers  performance degraded
s
l
monotonically as the testing set grew from x test
to x test
  physically  this means that the
classifiers had more trouble labeling pixels from out of focus images than pixels from in focus
l
images  for practical purposes  we are most interested in each classifiers performance on x test
 
l
since we would like our algorithm to work regardless of image focus  on x test   the mclr

 s
classifier performed optimally when trained on x train
  physically  this means that the in focus
images provided the best information for classifying pixels in variable focus images 

following machine labeling  we apply k means clustering to eye pixels to determine the

centroid location of each eye  in all experiments  k means clustering on mclr labeled correctly
detected the location of the fly eyes  
the ls classifier did not prove nearly as successful  on
l
s
m
  k means succeeded on           and     of images when trained on x train
  x train
  and
x test
l
x train   respectively 








figure    summary of performance results of the least squares  ls  and multi class logistic regression
 mclr  classifiers  plots across rows correspond to classifier performance when trained on the same image
set  but evaluated with respect to different testing image sets  plots down columns correspond to classifier
performance on the same testing image set  after training on different image sets 

fi   future work
while we are not ready to present results  we are currently working on a multi class
support vector machine implementation  mcsvm  to solve our pixel classification problem 
we are interested in using a quadratic kernel according to a         feature mapping  from
our experimental results  we found mclr to yield the best performance  depending on the
success of our mcsvm progress  we will integrate either mclr or mcsvm directly into the
software that controls our laser microdissection device 

we would like to allow for online training  to enable researchers to bring in fly lines that may
have radically different appearances compared to the ok    flies that our classifiers were
trained on  other fly lines may have genetic modifications  such as white eyesclearly the
classifiers would need to be retrained at the beginning of a microsurgery session 
   conclusion
we have successfully demonstrated the use of supervised machine learning algorithms to
classify regions in high magnification images of the fly head  we characterized least squares
estimation to multi class logistic regression classification for our fly tracking purposes  we have
shown that mclr trained on in focus images results in optimal classification performance on infocus and out of focus images  using k means clustering of mclr labeled points  we achieved
a      success rate in locating the position of fly eyes  these results suggest that we will be
able to perform high throughput microsurgeries to prepare live flies for two photon brain
imaging 

fi