cs    final project
one click  object removal
ming jiang

nicolas meunier

december         

 

introduction

in this project  our goal is to come up with an algorithm that can automatically detect the contour
of an object selected by the user and remove this object from the image by replacing it with a
plausible estimate of the background  the algorithm mainly consists of two steps of tasks  the
first one is to detect the contour of an object in an image  given a pixel in the image chosen by the
user  the algorithm should be able to define a mask surrounding the entire object containing the
selected pixel  we do not need to find the boundaries of all objects in the image but just to find
a mask that contains at least all the pixels of an object that we would like to remove  the second
important task is to remove properly this object from the image  for this task  given an entire
mask containing pixels to be replaced  we aim to produce an algorithm that can precisely replace
the object by a visually pleasant estimate of the background behind it 

 

foreground detection

our entire goal being to remove an object from an image  we first need to be able to define a
mask that represent our object and that we would like to remove  in other terms  given a simple
click by the user  we need to figure out what object he wants to remove and automatically define
a mask that surrounds this object  as we want to work on the widest possible class of objects 
we decided not to implement any kind of object detection that would limit our algorithm to the
learnt objects  we finally came up with the idea to define the mask by preprocessing our image in
superpixels  cluster of coherent pixels  and then asking the user to choose one of them that is inside
the object he wants to remove  it then remains the task to define all the superpixels that are inside
the object  for the superpixel extraction  we used the method of ren and malik with the available
code at http   www cs sfu ca  mori research superpixels code  and we then developed the
following algorithm based on a simplified segmentation classifier 

   

principles of the algorithm

in order to compute the entire mask  we decided to build a classifier that learns if a superpixel
is part of the foreground  so is potentially part of an object we would like to remove  or part of
the background  and then we dont want to add this superpixel to the mask as it could contain
important information for the image inpainting task   this is a simple segmentation problem where
we only have to distinguish between two possibilities for a superpixel 
to build this classifier  we first need to extract features from the superpixel that will help us
distinguish between background and foreground  the features we used in our algorithm contain
some color statistics over the superpixels  texture information based on different filterings  wavelet
filters   geometry information about the superixels and their spatial location in the image  then
 

fiwe implemented two learning methods based on these features to classify the superpixels  logistic
regression and svm  we used groundtruth information about the foreground and background of
images as our training data  around       groundtruth superpixels taken from     images   after
training  we evaluate our algorithm on a separate set of testing data  around       superpixels 
and produce the estimated foreground mask for each image  first the mask contains all predicted
foreground superpixels  then based on the users selection and adjacency information of the superpixels  we create the final mask which contains only the connected components around the user
selection  figure   illustrates the algorithm process  you can find another example in figure   

figure    process of generating foreground mask

   

comparison and use of the different classifiers

we used two classifiers for learning  logistic regression and svm  with different kernels   to compare
the performance of these two methods  we used the following three measures 
   accuracy  the rate of correct predictions made by the model over a data set 
   precision  the number of true positives  i e  the number of items correctly labeled as belonging
to the class  divided by the total number of elements labeled as belonging to the class  i e 
the sum of true positives and false positives  
   recall  the number of true positives divided by the total number of elements that actually
belong to the class  i e  the sum of true positives and false negatives  
there are two big issues that are worth noticing for our algorithm  one is that there will be
many more negative data  i e  the superpixels belonging to background  than positive data  i e 
the superpixels belonging to foreground   that means we cannot judge the performance of our
classifiers only by looking at its accuracy  as our goal is to detect the foreground objects  it is more
important to focus on the precision and recall measures than accuracy  table   gives the comparison
result between logistic regression and svm with a linear kernel for a non biased decision threshold 

table    comparison between logistic regression and svm
logistic svm
accuracy              
precision
             
recall
             
the other important issue is that for the image inpainting algorithm  we can tolerate a mask
bigger than the original object  that means we can sacrifice the accuracy and precision to some
extent for a higher recall  to change the recall  we can adjust the threshold parameter t of
 

fiboundary decision  we will label foreground superpixels with a probability margin larger than t
and background otherwise  figure   illustrates the precision recall curve 
precisionrecall curve
   
        
    
   

   

    

    

precision

   

   

    

    

   
    

   

   
    

    

   

   
    
    
   
   

   
   

   

   

   
recall

   

   

   

   

figure    precision recall curve
finally we choose a threshold parameter     as opposed to the normal     for testing in our
algorithm  which produces a precision of        and a recall of        

 

image inpainting

for the inpainting part itself  we now have a mask that has been filled in by the previous algorithm
and that we want to remove from the image  the mask is considered as a missing region of the
image that we want to fill in such a way that the result will seem visually natural  we do not intend
to restore any hidden object in our project  just to replace an object by a plausible background 
many algorithms have been developed over the last years  using 
   statistical based methods   given the texture surrounding the mask  this approach tries to
extract interesting statistics for those texture and then reproduce them inside the mask area 
the typical problem of this kind of approach is that it works well when we are only working
with textures 
   pde based methods   given information surrounding the mask  we try to propagate it inside
the mask using a diffusion process  this process is generally simulated by solving a partial
differential equation  pde   a problem of this approach is that the mask generally has to be
quite thin and the information quite smooth to give a visual natural solution  so  when the
missing region is highly textured or corresponds to a large object  those methods would lead
to a blurry solution 
   examplar based methods   this is the most known successful techniques for inpainting purpose  these methods fill the mask by copying content from the known part of the image 
thus considering that the missing information inside the mask can be found elsewhere in the
image 
for this project  we decided to use an examplar based inpainting approach and to improve it by
using machine learning to define optimal metrics used in the algorithm  the challenge here is to be
 

fiable to fill the removed object by a plausible background that fits well with the rest of the image 
we want it to preserve edges inside the region and we want it to use appropriate information from
the background to do so  for this purpose  we implemented the algorithm in     

figure    principle of the algorithm
we have a mask  that we want to remove from the image  we will progressively fill the mask
with information coming from the rest of the image  to do so  we will use a template window that
will find the closest patch in the image  outside the mask  to a given patch p on the border of
our mask  this closest patch in the image will be used to fill the missing information of the border
patch inside the mask  to compute this closest patch  we will define our own metric that is defined
in the next section 
for every patch p around the mask we define a priority order in which we will fill the mask
based on the parameters defined in figure    we modified the original priority function in    
using different features  like presence of edges around a patch  and presence of different kind of
segmentation  using the algorithm in     and the stair vision library     

   

comparing patch and propagating information inside the mask

if we want to choose a correct patch to replace part of the mask  we need a way to compare patches 
in the original algorithm  only color difference is used to compute distinction between different
patches  we purpose here to use a learning algorithm to define a new metric to pick up patches
for replacing  for two given patches  we can define a small set of different features that are some
characteristics of their difference 
   the sum of absolute difference of each color channel
   the sum of square difference of each color channel
   the number of pixels with a different segmentation
   the difference of entropy in the segmentation
we then have a set of features s    fi   in and we can define a function that permits us to
compare two patches as follows  
p
f  p    p      ni   wi fi  p    p   
for a given starting patch around the mask we can select a potential replacing patch and run
our inpainting algorithm starting with this patch  we then score the final result comparing its
color and segmentation with the original image  for each patch tried as a potential solution we
can then attribute a score  by this method  we can then create a training database on which to
run a learning algorithm  here a logistic regression  to maximize our final metric  the results of
this approach gives almost equal importance to the different features and with the learnt metric 
it permits to avoid reconstructing unwanted objects inside the mask as it could sometimes happen
with the original algorithm  this is why using the segmentation features to compare patches was
useful in our approach  you can see some examples of our inpainting algorithm in figure   and   
 

fi 

figure    original image

figure    mask

figure    inpainted image

figure    original image

figure    mask

figure    inpainted image

conclusion

we have presented an algorithm to detect the contour of an object selected by the user and remove
it from the image in a visually pleasant way  for foreground detection  we applied and compared
logistic regression and svm for the learning process  and also adjusted the threshold parameter
for decision boundary to trade off between the precision and recall rate  for image inpainting  our
results always do at least as good as the original algorithm and improves it in some difficult cases
where the original algorithm fails  we think that improving the results can not be done without
using a more global approach  or without having access to higher level information  concerning
context or objects surrounding the mask 

acknowledgements
we would like to thank john bauer  geremy heitz  stephen gould and daphne koller for their
contribution to the inpainting algorithm  we would also like to thank professor andrew ng and
the tas for their advice and suggestions 

references
    a criminisi and p perez  object removal by exemplar based inpainting  computer vision and
pattern recognition       
    stephen gould  andrew y  ng  and daphne koller 
http   ai stanford edu  sgould svl       

the stair vision library 

    stephen gould  jim rodgers  david cohen  gal elidan  and daphne koller  multi class segmentation with relative location prior  international journal of computer vision  ijcv        

 

fi