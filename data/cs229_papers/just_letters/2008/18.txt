adaptive ai for fighting games
antonio ricciardi and patrick thill
december         
 aricciardi  kpthill  stanford edu

 

introduction

traditionally  ai research for games has focused on developing static strategiesfixed maps from the game state to
a set of actionswhich maximize the probability of victory  this works well for discovering facts about the game
itself  and can be very successfully applied to combinatorial games like chess and checkers  where the personality
and play style of the opponent takes a backseat to the mathematical problem of the game  in addition  this is the
most widely used sort of ai among commercial video games today      while these algorithms can often fight
effectively  they tend to become repetitive and transparent to the players of the game  because their strategies are
fixed  even the most advanced ai algorithms for complex games often have a hole in their programming  a simple
strategy which can be repeated over and over to remove the challenge of the ai opponent  because their algorithms
are static and inflexible  these ais are incapable of adapting to these strategies and restoring the balance of the game 
in addition  many games simply do not take well to these sorts of algorithms  fighting games  such as street
fighter and virtua fighter  often have no objectively correct move in any given situation  rather  any of the
available moves could be good or bad  depending on the opponents reaction  these games reward intuition  good
guessing  and the ability to get into your opponents mind  rather than mastery of abstract game concepts  hence 
the static ai algorithms that are most prevalent tend to do even worse in these games than in others 
these two problems are perfect for applying the techniques of machine learning  a better ai would have to be able
to adapt to the player and model his or her actions  in order to predict and react to them  in this paper  we discuss
how we developed such an ai and tested its effectiveness in a fighting game 

 

goals

our original goal was to produce an ai which learned online  allowing it to adapt its fighting style mid game if a
player decided to try a new technique  obviously  this is preferable to an offline algorithm that can only adjust its
behavior between games using accumulated data from previous play sessions  however  such an ai would need to
be able to learn a new strategy with a very small number of training samples  and it would need to do so fast enough
to avoid causing the game to lag  thus  our first challenge was to find an algorithm with the learning speed and
computational efficiency necessary to adapt online 
in addition  there are several other general requirements typical for online learning algorithms in video games
     for example  since the goal of a video game is to entertain  an ai opponent should exhibit a variety of different
behaviors  so as to avoid being too repetitive  furthermore  the ai should not be too difficult to beat  ideally it
would adjust its difficulty according to the perceived skill level of the player  however  such modifications can be
made relatively easily once the algorithm is capable of winning      so we chose not to focus on difficulty scaling for
this project 
finally  because players tend to have unique play styles in fighting games  it makes sense to develop separate
models for each player  a game should be able to recognize a player and swap in the old model for that player 
rather than learn from scratch each game  ideally  when a player is found who cannot be confidently classified as
one particular stereotype  the game would be able to merge the most similar models  allowing the opponent to begin
the game with a strategy much better than that of a generic ai 

fi 

early implementations

because implementing an algorithm for a complicated fighting game is very difficult  given the number of possible
moves and game states at any point in time  we designed our first algorithms for the game rock paper scissors  this
game abstracts all of the complexity of a fighting game down to just the question of predicting your opponent based
on previous moves  as such  it was a perfect way to test a wide variety of algorithms quickly  and get a feel for
which applied best to this problem 
   

naive bayes

for our implementation of naive bayes  we used the multinomial event model  treating a series of n rounds as an ndimensional feature vector  where each feature xj contains the moves  r  p  or s  made by the human and computer
in the jth round  for instance  if both the computer and human choose r on round j  then xj    r  r   we classify a
feature vector as r  p  or s according to the opponents next move  in round n       we then calculate the maximum
likelihood estimates of    r  r     r      r  p     r      s  r     r   etc    as well as r   p   and s 
   

softmax regression

for softmax regression  we used the same feature vectors as in naive bayes  with the same scheme for labeling  we
derived the partial derivative of maximum likelihood function with respect to the ith  jth element of the k by n matrix
 and used it to update the matrix via gradient ascent  due to the computational complexity of updating the  matrix
each iteration  we chose to run this algorithm offline 
   

markov decision process

to apply reinforcement learning to rock paper scissors  we defined a state to be the outcomes of the previous
n rounds  the same way we defined a feature vector in the other algorithms   the reward function evaluated at a
state was equal to   if the nth round was a win    if it was a tie  and    if it was a loss  the set of actions  obviously 
consisted of r  p and s  we estimated the probability of transitioning from one state to another on a given action by
keeping counts of all previous transitions  the optimal policy was calculated using value iteration 
   

n gram

to make a decision  our n gram algorithm considers the last n moves made by each player and searches for that
string of n moves in the history of moves  if it has seen those moves before  it makes its prediction based on the
move that most frequently followed that sequence 
   

algorithm comparison

to test our algorithms  we wrote a java program which simulated a game of rock paper scissors between two
opponent ais  each learning ai was paired up with several static ais that we designed to simulate real
players  these static ais would execute randomly chosen patterns whenever they saw a new state  where states
were defined similarly to those of the mdp   for example  given the previous   rounds  one such ai might have a
    chance of following with the pattern r r p  then executing a different pattern based on whichever new state it
arrived in  since our simulation allowed us to generate an arbitrary amount of data  we performed a simple cross
validation on all the algorithms to see which performed best  we found that naive bayes had a slightly better
prediction rate than the other three algorithms but had a slower learning speed than reinforcement learning  we
rejected softmax regression due to its slow computational speed and the fact that it did not outperform the other
algorithms in any other regard  in the end  we went with the mdp  since we believed its learning speed gave it the
most promise as an online algorithm 

 

final implementation

for the actual fighting game  we decided to implement our own game in java rather than attempt to develop an
algorithm for an existing open source game  we did this for two main reasons  first  finding an appropriate opensource game and learning its code base would have taken significantly longer than designing a simple game of our
own  second  developing our own game gave us complete control over both the number of possible game states and

fithe set of available actions  which made constructing the mdp much easier  of course  the cost of this decision is
that our game is much more rudimentary than a commercial fighting game  however  we believe our findings are
still very applicable to larger scale games 
in our simplified fighting game  players have access to a total of   moves  left  right  punch  kick  block  throw  and
wait  each move is characterized by up to   parameters  strength  range  lag  and hit stun the moves interact in a
similar fashion to those of rock paper scissors  blocks beat attacks  attacks beat throws  and throws beat blocks  but
the additional parameters make these relationships more complicated  the objective is to use these moves to bring
your opponent s life total to    while keeping yours above    though most commercial fighting games tend to have
a wider range of attacks available  typically around      as well as a second or third dimension for movement  their
underlying game mechanics are generally very similar  in making the game  we tried to keep it simple enough to
avoid spending too much time on aspects of the game unrelated to machine learning  such as balancing the attacks
and finding appropriate animations for all the moves   while keeping it complex enough to make the problem
challenging and make the algorithm applicable to commercial games 
the most difficult decision in porting our reinforcement learning algorithm to an actual fighting game was how to
define a state  our first idea was to include a history of the most recent moves performed by each player  as we did
with rock paper scissors  however  since each player now has   available moves each game frame  this resulted in
      memory possible states  where memory is the number of previous frames taken into account  having this
many states caused a very slow learning speed  since an mdp needs to visit a state many times before it can
accurately estimate the transition probability distributions for that state  in addition  we found that very similar
states  whose histories differed in only one move by a single player  were treated as completely different states  so
that the mdp would often behave inconsistently  of course  when we tried decreasing memory to reduce the
total number of states  we found that the states did not contain enough information to find the correct action 
to minimize the number of states without losing too much information  we decided to ignore the specific moves
made by each player and instead focus on the game statistics which resulted from those moves  in particular  we
added information about the life totals  lag and position of each player  to eliminate redundant states  we extracted
only the relevant information from these numbers  specifically  we only kept track of the change in life totals from
the previous frame  the distance between the two players  and the opponent s remaining lag  we ignored the ai s
lag  since the ai can only perform actions during frames where its lag is    we defined the reward function for a
state as the change in life totals since the last frame  for example  if the ai s life falls from    to     while the
opponent s life falls from    to     the reward is                           
to test the idea of recognizing a player based on his or her play style  we tried a few simple algorithms which rated
the similarity of two mdps  we compared the value function  transition probabilities  and number of visits for each
state in the first mdp to those of the corresponding state in the second mdp  our hope was that a new mdp
playing against a particular opponent would resemble an mdp for the same player from a previous play session  and
that this resemblance would be noticeable relatively early in the new play session 
   

improvements to the standard mdp

since a cunning human player may vary his strategy mid game  it is important give the most recent state transitions
the most weight  since they are most indicative of the opponent s current play style  for example  suppose that the
mdp has been playing against a player for several rounds  and in state s  it has taken action a      times      of
which resulted in a transition to state s   yielding an estimated transition probability of pa  s         now suppose the
opponent shifts to a new strategy such that the probability of transitioning to s  on a is      clearly it will take
many more visits to s  before the estimated probability converges to      to address this  our mdp continually
shrinks its store of previous transitions to a constant size  so that data contributed by very old transitions grows less
and less significant with each rescaling  in our example  this might mean shrinking our count of action a down to
   and our counts of transitions to s  on a down to    maintaining the old probability of       this way  it will take
far fewer new transitions to increase the probability to     
in addition to increasing the weight of the most recent transitions  we needed to increase the rate at which the
transition probability estimates were recalculated from the stored counts  so that the change in the ai s behavior
would be noticeable quickly  this was particularly important for preventing the opponent from exploiting a newly

fidiscovered weakness in the ai s strategy for more than a few frames  after an initial exploratory phase  our
algorithm performs value iteration on every frame  to encourage exploration of new moves after this point  it
chooses a move at random from those within a tolerance of the value of the best move  also  to prevent the
algorithm from getting stuck in a local maximum  as well as to prevent repetitive behavior  we temporarily remove a
move from the list of actions if it has been used three times in a row 
since our goal was to develop a player model  rather than a general strategy for playing the game  we hard coded a
few rules that would always be correct  regardless of the opponent s strategy  for example  if your opponent is more
than two spaces away  it is useless to block  since no attack has a reach greater than    these rules sped up the
learning process by preventing the ai from wasting time exploring useless moves  to decrease the value of these
actions  we created a dummy state with a very low reward  and gave each useless action a      chance of
transitioning to this state 

 

results

we tested the mdp ai at first against other static ais  and then against human players  the static ais were modeled
by state machines that would execute random patterns associated with each state  as described in section     after
making the aforementioned improvements to our algorithm  the mdp ai beat the static ais nearly every game  of
course  this is not surprising  since the static ais ran on the same states as the mdp and thus were easy for it to
predict 
against human players  the mdp was very strong  winning most games and never getting shut out  however  the
only experienced players it faced were the researchers themselves  also  while any ai has the advantages of perfect
control  timing  and perception against human players  those advantages may have been magnified in this case by
the games unpolished animations and slow frame rate  nevertheless  this is a good result  and the success of the ai
can be confirmed by other observations 
one goal of the project was to create an ai which could find its own way out of the simple patterns that players
might use to exploit the ai  for example  once a player had backed the ai into the corner  he could simply use the
kick attack over and over  with its long range preventing a jab or throw from being effective  the only way for the
ai to recover from this situation is to step forward before jabbing or throwing  over the course of user testing  the
ai had to recover from a number of such exploits  on the average  it made a correct choice on the around sixth
attempt  but it was subsequently very quick to counter if it saw the same tactic later on  in addition  we occasionally
saw it develop more complex counters  for example  if the opponent was simply repeating an attack over and over 
the mdp would sometimes learn to block and then counter while the opponent was in lag  since these behaviors
were not preprogrammed and had to arise from random exploration  we viewed this as a significant achievement  if
the ai were able to learn for many games against the same player  rather than just the few rounds it got against our
participants  these behaviors may develop more fully and even interact with each other 
opponent recognition worked very well for the static ais against which we first trained our mdp  indeed  after   
rounds the similarity between two instances of the ai trained against the same static ai was infinity  that is  the two
instances had identical transition matrices  up to java s rounding error  however  recognition grew spottier when
applied to human players after   rounds of play  some players consistently produced similar mdps  whereas the
mdps of other players varied widely  it is unclear whether this is due to a flaw in the comparison algorithm  the
relatively short play time  or the players themselves modifying their strategy between games  although it should be
noted that their inexperience with the game makes the last option the most likely 

 

conclusion

the ai algorithm presented here demonstrates the viability of online learning in fighting games  we programmed
an ai using a simple mdp model  and taught it only a few negative rules about the game  eliminating the largest
empty area of search space but giving no further directions  under these conditions the ai was able to learn the
game rules in a reasonable amount of time and adapt to its opponent 

fithis project also demonstrates several ways in which the domain of fighting games poses a unique problem  the
extremely quick learning speeds and computational efficiency required for game ais make several standard machine
learning techniques  such as naive bayes  impractical  and forced us to make modifications to the standard mdp
algorithm  indeed  pattern matching in general is a poor metaphor for this domain  as fighting games often require
one to recognize and adapt to a new tactic as soon as it appears  rather than when it came be recognized distinctly as
a statistical pattern  one might expect that an ai with this property needs to be written specially for fighting games 
but we have demonstrated that it can be achieved by adjusting a standard reinforcement learning algorithm to have a
short memory and a rapid update rate 
regarding player recognition  it is unclear based on the results whether it is feasible to recognize a player based on
previous play sessions and swap in an old ai faster than it takes the new ai to learn from scratch  perhaps player
recognition would be more successful using a separate learning algorithm  rather than comparing the new and old
mdps with a formula  of course  it may also be acceptable to have a player directly identify him or herself with a
profile  so that the recognition step is unnecessary 

 

future work

an immediate improvement to the algorithm might be made by experimenting more with possible state
compositions  for example  perhaps the mdp could learn to play more conservatively if it knew when its life total
was near    in addition  it might be fruitful to separate the game playing and player modeling aspects of the project
into two separate algorithms  so that general game knowledge gained in one encounter might more easily be
transferred to another against a different player  a more significant next step would be to generalize the algorithm
to be used in more complex games and see what problems result from the larger scale  finally  since the algorithm
presented here is only somewhat capable of recognizing players  it would be interesting to further explore the
feasibility of reusing old mdps  in addition to swapping in an old mdp for a returning player  this might allow a
new players strategy to be classified as being a combination of the strategies of previous players  whose respective
mdps could then be merged for the new player 

acknowledgements
we would like to thank professor pieter spronck of tilburg university for offering advice during the early stages of
this project 

references
    charles  d et al  player centered game design  player modeling and adaptive digital games 
http   info    infc ulst ac uk  darryl papers digra   digra   pdf
    spronck  pieter  adaptive game ai  http   ticc uvt nl  pspronck pubs thesisspronck pdf
    spronck  pieter  personal interview  october          

fi