a machine learning approach to developing rigid body dynamics
simulators for quadruped trot gaits
jin wook lee
leepc   stanford edu

for complex robot systems      various characteristics of a

abstract i present a machine learning based rigid body
 

dynamics simulator for trot gaits of the quadruped littledog  

robot such as a complex body shape  joint motors and other

my contribution can be divided into two parts  the rst part

specications  which can signicantly affect its dynamics  are

for the reduction of one time step prediction error  and the

far too complicated to be input to an ode type simulator 

second part for a more accurate prediction over longer time
scales  first  in order to reduce the one step prediction error  i

in order to resolve these challenges  i propose a learning

compared three regression methods   st order linear regression

based approach to developing an accurate simulator  here 

 lr  

after  an ml simulator  that does not require any specic

linear

weight

projection

regression

 lwpr 

   

and

gaussian process regression  gpr       although gpr shows the
 
highest accuracy  its cost for computation and storage  o n  
 
and o n    respectively  is too high to handle a large amount

information  but learns from data for a robot of interest  i
focus on learning dynamics of trot gaits of the quadruped
littledog on a at terrain 

of training data which are required in my problem  therefore 
i developed a sparse gpr  called local gpr  lgpr   in

b  approach in this research

lgpr  training data are divided into k clusters by k means 
and a locally full gpr model is constructed for each cluster 

denote by

the prediction is done with the local model closest to a test

step

data point  in terms of the normalized euclidean distance 
 
the cost for computation is tractable  approximately o m  

m n k 

where

t 

st

a state of the robot at a certain discrete time 

then our goal is to predict

st  

for

t                 t

like the following 

making it possible to use a large amount of

st     predict st   ut   

training data  i showed that lgpr signicantly reduces the
one step prediction error  second  to reduce the accumulation

main sources of such error turned out to be in some predicted

s is  rn and an action u is  rm   where n
     and m      for the problem i deal with  the initial
state s  and actions ut for    t  t are given for each
time step  s consists of the body s position  linear velocity 

 

orientation  angle of joints and time derivatives for them  such

of prediction error  i proposed a projection method  called

p roj  

a state

 

i found that using lgpr as it is accumulates the

prediction error too much over longer time scales  one of the
states that are not within the dynamically feasible region 

p roj

projects a predicted state to such region  i dened the

that it can solely represent the state of the robot at time

region without using any specic information of littledog  e g 
max min joint angles   it is only dened by the training data 

ml simulator is challenging for two reasons  first  the ml

thus  this projection method can be applied to other robot
systems without modication 

 p roj

simulator should predict multi dimensional outputs for every

highly improved the

prediction over longer time scales  the empirical results show

element of

that these key ideas led to the signicant reduction of prediction
error and better performance than the ode

 

simulator 

st    

years 

the

useful

controllers 
to

have

it

regression  lwpr  and gaussian process regression  gpr  
gpr was the best among them 

robot s

an

is

however  gpr suffers from extremely high cost for computation

very

accurate

deviates from the dynamically

 st order linear regression  lr   linear weighted projection

dynamics  when developing
such

st

step prediction error  i compared various regression methods 

velopment of a robotic conlearn

this means that once

largely two goals  first  i focused on reducing the one 

machine

extensively applied to the deto

st  

or diverge  in order to resolve these difculties  i constructed

learning algorithms have been

troller

accurately  second  a predicted state

feasible region  the prediction error will quickly accumulate

a  simulator for quadruped trot gaits
recent

st  

must be used as a feature set to predict its successive state

i  introduction

in

t 

unlike ordinary regression problems  developing this kind of

o n   

and storage

o n   

such that only thousands

of training data points are actually used for a prediction  thus 
fig    

the littledog robot

i adopted a sparse gpr  called subset of regressors  sor      

simulator in order to test the controller s performance before

          for this kind of sparse gprs  it is very important

applying it to a real robot  especially for the ground robots 
there have been several rigid body dynamics engines  the
most widely used one being open dynamics engine  ode  

m support data
n   m  training data

to choose a good set of

points which can

represent the entire

points  i proposed

an efcient method that nds such support data points by a

although ode is used in wide ranging applications not only

k fold cv test in the training data pool 

for robotics but also for games and animations  it is not

this selection method outperformed over the ordinary ran 

accurate enough for engineering purposes  furthermore  in

dom sampling  however  the accuracy was not good enough

general  it is very hard to get an accurate dynamics model

to be used as a simulator again  thus  i developed a local

 

designed and built by boston dynamics

gpr  lgpr  that divides training data points into k clusters

 

open dynamics engine  www ode org

by k means  and predict with only one local model closest to

 

fit

z

x y z      ji ji

t  

z

x y z      ji ji

error  and the computational cost also reduced to a tractable
level 

o m   

m n k  

where

action
 given 
 i

state

a test data point  this further reduced the one step prediction
while exploiting all data points

j

in the training data pool 
the second part of my contribution is on the reduction



of the prediction error over longer time scales  hereafter  the
sequential prediction error   the sequential prediction error

 

ji

accumulates as time step increases because each one step
prediction error is not zero  this accumulation can cause a
predicted state

st  t 

fig    

to deviate from the feasible data region

and so doesn t the yaw angle

 fdr   in which robot s dynamic constraints are satised  i
proposed a projection method called

 p roj

that prevents

section vi  for the similar reason 

section ii describes data collection and feature selection 

   

empirical results  and section viii contains concluding rean action

marks 

ij

ii  data and feature selection

 st   ut  

u                       r  

where

denotes the desired joint angle for the next time step 

for the one step prediction  the problem i consider has a

st     as illustrated in fig    i
st and an action ut to predict each element of
the response st     i applied the same regression method to
i
i
predict each st    s denotes the i th element of a state s  
multi dimensional response

unit variance  i split the collected data into two categories 

used a state

all the training data
data from

is simply

iii  comparison of regression methods

the feature part and

has its own scale  so each element is normalized to have a
and t estp ool  such that
t rainp ool and all the test

u

therefore  there are            features and    responses 

i collected data from littledog s trot gaits  each data

come from

should

s    zw   xb   yb   zb                              
 
                            r  

that signicantly

reduces the sequential prediction error  section vii presents

t rainp ool

zw

xb  yb  zb     and  rather than the velocity terms
xb   yb   zb      and    finally  a state s can be rewritten as 

section v explains the proposed novel lgpr  section vi

the response part  each element of the feature part

and

the time step is xed to be     s  i used the difference terms

tion iv proposes the method of choosing support data points 

  st   ut    st     

xw  yw

be rewritten to be originated from the body frame  because

in section iii  various regression methods are compared  sec 

point is encoded as

excluding these features

because fdr must be bounded  i will discuss fdr later in

decreased 

 p roj

 

is very important to dene the feasible data region  fdr 

this  i show that the sequential prediction error signicantly

presents the projection method

one step prediction for a multi dimensional response

t estp ool 

the time step was      second and one trot move lasted

to reduce the one step prediction error rst  i compared

for   seconds so that each move has about     data points 

three regression methods   st order linear regression  lr 

there were    types of moves 

f  f l f r turning or moving

b  bl br

 turning or moving straight

straight forward  
backward  

l r

 moving sidewise  

nally forward  and

s

dl dr

as a baseline approach  linear weighted projection regression
 lwpr      and gaussian process regression  gpr      

 moving diago 

lr

there were    moves     for

t rainp ool and   for t estp ool 
s

lwpr manages

here in more

ceptive

detail  there are    rigid bodies including the robot body 

the system to     so  i have    dimensional conguration
i denote by

x

and

with

k
a

locally linear models  called a recenter

ck  

the

prediction

can

be

pk
pk
y  
k   wk y 
k   wk   each model s weight is wk  
exp  xck    dk  xck      where dk is a distance metric

these rigid bodies restrict the degrees of freedom  dof  of

q    x  y  z                          

eld 

response
y
 
y are a design matrix
x is a test feature set 

multi dimensional
where

done by computing the weighted mean of each model 

eight leg parts and the ground  the joint constraints among

space 

a

and a response matrix  respectively 

now  prior to the discussion of the selection of a proper
feature set  let me dene a robot s state

predicts

x  x   x   x   y  

 standing   for each type of trot gaits 

in a gaussian kernel  lwpr learns training data one by

 x  y  z 

one to update its center

the position of the center of gravity  cog  for the robot body 

ck

and parameters  and increase or

decrease the number of receptive elds if necessary  lwpr

        means the roll pitch yaw orientation of the body  ij

is parametric  so it can predict quickly and handle a large

represents the j th joint angle for the i th leg  where each leg

number of training data  however  the major drawback of

has three joint angles  additionally  i consider the rst order

lwpr is the required manual tuning of many highly data 

time derivatives  i e  the linear velocity and the angular rate

dependent parameters     

of the orientation joint angles  one might think that a state

   

y   f  x     with additive gaussian
 with variance n    it predicts a response f x    
k   k   n  i   y with variance v  f     k x   x   
k   k   n  i   k   where k    x    p   k xp   xq    
 xp    p  xq   and k   p   gpr requires less data 

means

specic tuning of hyperparameters of its covariance func 

denotes the angular rate of each joint 

tion      and can be easily optimized by common gradient

however  taking into account the fact that features should

ascent algorithms  however  it suffers from extremely high

gpr assumes

should be written as the following 

noise

s    xw   yw   zw   xw   yw   zw                                
 
                            r  
where the superscript
the body frame  and

w

ij

stands for the world frame 

b

be relevant to predicting a response  three features  xw  
and

 

o n    and memory usage o n    for
 
inverting  k   n i   various sparse approximations were
applied to full gpr for resolving this drawback      i will

yw

computational cost

should be removed from the feature set  a robot s x 

y position  xw  yw   does not actually affect robot s behavior 

 

fin training data points gave the  separateness distribution 

discuss this in the following section  for gpr  i chose the

over

automatic relevance determination  ard  a squared exponen 

over the entire training data pool 

tial kernel with individual distance metric for each feature  

because i applied individual regression model for each

s 

then i optimized its hyperparameters by using a conjugate

element of a state

gradient descent that minimizes the negative marginal log 

computed for each element separately 

likelihood  these optimized hyperparameters were also used

 separateness distribution  should be

v  local gpr

to tune the distance metric of each feature for lwpr 
i evaluated these three regressers by a hold out cross 

sor gave less prediction error  however  the one step

validation test with the identical test data set  i used normal 

prediction should be as accurate as possible because the se 

ized mean squared error  nmse  as an error measurement

quential prediction error can be greatly accumulated through

metric such that i can easily compare the error of each

a number of one step predictions  thus  i invented a novel

element of a state  in terms of test error  gpr outperformed

sparse gpr called local gpr  lgpr   lgpr can take

over other two regressors as shown in table i  therefore  i

advantage of using the entire training data points  it divides

t rainp ool

decided to use gpr for implementing the ml simulator 

into

k

clusters by k means  and construct

k

locally full gpr models  a prediction for a test data point
is done with the closest local model only  this is similar

iv  sample selection for sparse gpr

to the block diagonal approximation of a covariance matrix 

due to gpr s high cost of computation and memory  i

however  it is different in that only one local model is

considered sparse gpr methods that are designed for han 

activated for a prediction  so computationally less expensive 

dling a larger set of training data  i tried one of them  called

of course  the hyperparameters are re optimized for each

subset of regressors  sor                 the key idea for sor s
approximation is that it chooses

m

can represent the entire training data pool
sor  the rest

 n m 

local model 

 support data points  that

t rainp ool 

each local model has approximately

in

training data points are assumed to be

the overhead to nd the closest local model is ignored  lgpr

independent with one another and they only communicate
through

m

can be a better alternative for high dimensional problems

support data points  sor s cost for computation

and memory is

o m  n 

and

o mn  

data points out of the entire

which require a very large set of training data for an accurate

respectively  needless

to say  it is very important for sor to choose

prediction 

m good support

n training data points  these data

vi  projection to training data pool

points should be well distributed so that it can cover up most
of

t rainp ool 

in the previous sections i focused on the reduction of the

there are several approaches that jointly nd

optimal hyperparameters and support data points where

m

one step prediction error  one might think that the sequential

is

prediction will be done well through a number of one step

given      however  especially for the high dimensional and

predictions in a row  however  a predicted state

data rich problem i consider  i couldn t obtain the solution in

required to deal with such deviation 

support data points by doing k fold cv test in the training

in the real world or the ode system  a state

because i used a squared exponential

angle should be bounded  i will denote such constrained

point is not predicted well in a leave one out cross validation

region by the feasible data region  fdr   fdr is truly

 loocv  test  the point is likely to be separated from other

bounded since i excluded some features  xw and

these

predicted state

method is measuring  separateness  of each training data

usually deviates from fdr a little at

st  

uses such

st  

as

features  the prediction error will be further amplied 

support data points from this distribution prevents choosing

thus  we need to adjust the deviated

too many from clustered data points  such that selected ones

st  

to be in fdr 

however  i cannot explicitly dene fdr  e g  letting each

t rainp ool 

foot s z position over zero  because the goal of this research is

loocv test is the ideal one for my sampling method 

on the development of an ml simulator that does not consider

however  the problem is that full gpr can hold only thou 

any specic information about the robot 

sands of training data points for each time of cv test  thus 

because the problem i deal with is data rich  i can collect

i applied k fold cv test such that a small number of training

a very large

data points are involved in each cv test  first  i randomly

t rainp ool

covering most of fdr  if i collect

t rainp ool will be equal
t rainp ool to adjust
i
a deviated state st   is giving the upperbound u and the
i
i
lowerbound l to each element of the state st   like the
innite number of training data 

t rainp ool into k groups 
full gpr  n k training data

data points in

and did k times of cv test by

st  

least  because the prediction for

point  and giving them  separateness distribution   sampling

n

that

for most one step predictions by an ml simulator  a

points set as many as possible  the key idea of my sampling

divided the entire

yw  

are not bounded in section ii 

separated data points should be included in the support data

can be well distributed in

always

position of each foot should be over zero and each joint s

their normalized euclidean distances  therefore  if a data

t rainp ool 

st  

satises the robot s dynamic constraints  for example  a z 

kernel  covariances among data points are closely related to

data points  to cover up a wider range of

will

accumulation of one step prediction errors  therefore  it is

instead  i invented a simple but efcient method that nds

t rainp ool 

st  t 

greatly deviate from the dynamically feasible region by the

a reasonable time 

data pool

m   n k training data
o m    

points  so the computational cost of lgpr will be

to fdr  the simplest way of utilizing

n k     k test data points are involved in each time
t rainp ool was tested
k    times  and hence had k    number of prediction errors 
points and

of the cv test  each data point in

following 

st   i    min  u i   max  st   i   li      

summing up these error for each data point and normalizing it

 

fitable i
c omparison

of n mse for various regression methods

lr  lwpr  gpr  s o r

lgpr

and

method

zw

xb

yb

zb











  

  

  

  

lr

      

      

      

      

      

      

      

      

      

      

      

      

      

lwpr

      

      

      

      

      

      

      

      

      

      

      

      

      

gpr

      

      

      

      

      

      

      

      

      

      

      

      

      

sor

      

      

      

      

      

      

      

      

      

      

      

      

      

lgpr

      

      

      

      

      

      

      

      

      

      

      

      

      

    

deviated prediction  st   

x 

   

 proj

bound
mbr

    

    

nmse

q
fdr

    

    

training data points

   

x 

gpr
    

fig    

comparison of two adjusting methods 

 p roj

and

of course  the boundaries are those observed in

sor  random 
sor  cv test 

bou n d

l gpr
    
    

t rainp ool

bou n d  however  there
 p roj as shown in
fig     let me denote by q   minpt rainp ool kp  st   k 
the training data point closest to the deviated prediction st    
then  p roj can be written as the following adjustment 
and this method is called hereafter

fig    

can be a tighter adjustment called

    

     
  training data

     

     

nmse of various regression methods for predicting

for full gpr and sor are

o n   

o mn  

and



respectively 

therefore  full gpr couldn t work with more than     
training data due to lack of memory  and sor couldn t

st           st     q          

hold

mn

 

over                  for the same reason 

i carefully optimized each regressor s hyperparameters and
i used nmse for each element as an error measurement

determined the size of training data set such that it yields

metric for the one step prediction error  however  for the

the best performance considering the limitation of the system

sequential prediction error  i used two other different error

resource  all of       training data were used for lr  lwpr 

metrics  the non cumulative error and the cumulative error 

and lgpr       and       training data were used for full

the non cumulative sequential prediction error is measured

gpr and sor  where 

for each element as

i
 si
t  st    

where

element of the actual state at time

t 

si
t

denotes the i th

the reason why i

call it  non cumulative  is that every element of a state
element except the z position

zw

fig    shows nmse for predicting

state  instead  only the hardest element to be predicted  the
roll angle difference

to observe a cumulative behavior of the sequential prediction

s

p

and used

s  instead  i computed  xw  
    as the
 xw  xw       yw  yw

p roj

with various

 

bou n d

and

will be compared  fig    describes

size of the training data set  however  some regressors suffer
from dealing with such large training set  as i mentioned

cumulative sequential prediction error 
the empirical results for no adjustment 

 

that the accuracy of most regressors roughly increases as the

error with these elements of
from

for each gpr

for simplicity  i will not show nmse for every element of a

does not

accumulate due to the gravity  therefore  it is not possible

yw  

 

method  with respect to different size of training data set 

is not cumulative because

zw

respectively  as shown in

poorer than expected  among gprs  lgpr was the best 

s

is non cumulative  as i already discussed in section ii  each
it is originated from the body frame  also 

m       

table i  gprs outperformed over lr and lwpr  lwpr was

previously gpr couldn t take advantage of training data over

 

      although its performance was the best with training data

are given in the following section 

less than       i tried to x the number of support data points

m

vii  empirical results

of sor to be      for its best performance  however 

mn

couldn t be over           so it was unavoidable to decrease

a  results of one step prediction

with intel core   duo l        ghz cpu and  gb ram 

n      
n        this is why sor s performance got worse
with n       and n        for lgpr  different number of
clusters k was used for each case  i set the size of training data

table i shows the comparison of performance for all re 

in a single local gpr model to be approximately       so that

t rainp ool

and

t estp ool

m

have       and       data

from      to      and     for the case of

and

points  respectively  computation was performed on a laptop

gression methods i discussed above  different size of training

k

data set was used for each regression method  this is because

all other regression methods  we can see that only lgpr can

some of regressors  full gpr and sor  suffer from very

handle very large training data set and the accuracy is the best 

high cost for computation and memory as the size of training

fig    also shows that sor with k fold cv test outperformed

data increases  i mentioned in section iv that storage costs

over sor with random selection of

 

is              and    for each case  lgpr excelled over

m

support data points 

fi 

  
no adjust
bound
 proj        
 proj        
ode

 

no adjust
bound
 proj        
 proj        
 proj        
 proj        
ode

  

 

non cumulative sequential error

cumulative sequential prediction error

   

x   

   

 

   

 

 

 

 

 

   

 

 

  

   

   

   
time step

   

   

   

   

 

 

  

  

  

  

  

element

fig    

cumulative sequential error for the robot moving straight forward

fig    

element wise non cumulative sequential error

viii  conclusion
   

 

cumulative sequential prediction error

i developed a simulator that learns and predicts quadruped

no adjust
bound
 proj        
 proj        
ode

trot gaits without explicitly considering the dynamics over it 
i invented the local gaussian process regression  lgpr  that
signicantly reduces the one time step prediction error by

   

taking advantage of a large set of training data  it has k locally
full gpr models and predicts with the local model closest to

 

each test data point  i also proposed the projection method
called

   

 p roj

that can prevent the prediction over longer

time scales from diverging  by projecting a predicted state to
the feasible data region  fdr  in which dynamics constraints

 

are satised 

 p roj

greatly reduces the prediction error

over longer time scales  empirical results validate that my

   

approach is more efcient for simulating quadruped trot gaits
 

than the ode simulator and other methods i tried 
 

  

   

   

   
time step

   

   

   

   

this research can be extended to problems with harder
motions such as galloping and jumping  adding height slope

fig    

cumulative sequential error for the robot turning left backward

map of a terrain to a feature set will allow the ml simulator to
deal with a bumpy terrain  local gpr actually approximates

b  results of sequential prediction
i will compare the two adjusting methods

bou n d

 p roj

the original covariance matrix as the block diagonal one 
allowing the blocks to overlap  one data point is belong to
and

multiple local clusters  can improve the prediction  multi task

in terms of two different error measurement met 

learning can also be applied to reduce the prediction error 

rics  the cumulative error and the non cumulative error  as

acknowledgement 

explained in section vi  first  i will take a look at the

i thank zico kolter for his helpful

advice on this project 

cumulative behavior of the sequential error and how two

r eferences

methods relieve it  second  i will check the non cumulative

    s  vijayakumar  a  d souza  and s  schaal  incremental online learning

error and see if they actually make the sequential prediction

in high dimensions  neural computation  vol      no      pp      

converge to fdr 

           

fig    and fig    show the cumulative

    c  e  rasmussen and c  k  i  williams  gaussian processes for machine

sequential error for littledog s two kinds of move  f and

bl  see section ii   no
bou n d and  p roj

learning 

cambridge  ma  the mit press       

adjustment gave the worst result 

    j  p  duy nguyen tuong  local gaussian process regression for real 

are indeed helpful to reduce the

time model based robot control  in ieee rsj international conference
on intelligent robots and systems  sept       

 p roj           gave
the best result  i omitted the result of         and   
     because they yielded similar result as          at last 
the ml simulator with  p roj defeats ode  in the   d
visualization of these results   p roj          looks very
sequential prediction error  but

    m  seeger  some aspects of the spline smoothing approach to nonparametric regression curve tting  in j  roy  stat  soc   vol  b        
springer verlag        pp      
    g  wahba  g  wahba  x  lin  x  lin  f  gao  f  gao  d  xiang  d  xiang 
r  klein  and b  klein  the bias variance trade off and the randomized
gacv  in advances in neural information processing systems 

mit

press        pp         

similar to the real robot motion rather than ode  note that

    a  j  smola and p  bartlett  sparse greedy gaussian process regression 

ode simulator s parameters are very carefully tuned to make

in advances in neural information processing systems    

its motion look similar to the real littledog  fig    shows the

mit press 

      pp         

pt

i
i
t    st  st   
this represents the non cumulative sequential error  the result


    j  quinonero
candela and c  e  rasmussen  a unifying view of sparse

implies the convergence criterion for fdr  for this specic

    m  seeger  fast forward selection to speed up sparse gaussian process

element wise error sum for all time steps 

problem 

    

approximate gaussian process regression  journal of machine learning
research  vol     pp                 
regression  in workshop on articial intelligence and statistics         

is enough for the prediction to converge 

 

fi