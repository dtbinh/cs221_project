classifying hand shapes on a multitouch device
daniel gibson  aaron rosekind  ashley wellman
stanford university  stanford  ca      
email   gibsnson  rosekind  wellman  stanford edu

  introduction
as modern computer systems become more complex  the latest tablet laptops  pdas  and
handheld gaming devices are all moving towards more intuitive and dynamic touch interfaces 
multitouch has sparked significant interest amongst academic researchers and hobbyists alike 
however  standard open source libraries  the most popular being touchlib by nui group  do not
distinguish between different hand shapes  the ability to distinguish between different types of
contact would allow such intuitive gestures as collecting objects by cupping with the sides of the
hand  or erasing by wiping the screen with the palm of the hand  our goal  then  is to create a
system that will correctly distinguish between different hand shapes  this is a difficult problem 
because the size and orientation of a given hand shape can vary greatly 
our project solves this problem by extracting relevant features from a preprocessed hand image
and applying machine learning algorithms to identify different kinds of hand contacts  our
system can identify the fingertip  the side of the hand  and the palm of the hand  despite widely
varying hand sizes and orientations 
   hardware
the first stage of our project was to build a light based multitouch surface  light based
multitouch devices involve three main components  a camera with an ir filter  an ir light
source  and a touch surface  contact with the touch surface scatters ir light towards the camera 
causing the camera to register a bright spot at the point of contact  this technique can be
implemented using a variety of methods  which are distinguished primarily by their method of
illumination  we tested three such methods  rear diffused illumination  rear di   frustrated total
internal reflection  ftir   and led light plane  led lp   rear di uses ir led panels to bathe
the touch surface with light from below such that the light is reflected back by objects above the
surface  ftir uses ir leds around the perimeter of the touch surface  the light from these
side mounted leds is directed into the surface from the sides  and reflects internally until a

ficontact scatters light towards the camera  led lp uses the same hardware setup as ftir  but a
thinner surface is used  creating a light plane above the surface which scatters upon contact 
the question of which method to choose depends heavily on the intended use of the device  we
identified three primary criteria for comparing the different methods  overall brightness of ir
image  image contrast between touched and untouched areas  and whether a part of the hand
hovering just above the surface is incorrectly registered as touch  rear di has the high
brightness  low contrast  and worst presence of hover  ftir has very low brightness  high
contrast  and no hover  led lp lies between these two extremes  sample raw camera images
from the three methods are shown in figure   

after testing all three  we found the method that worked best for this project to be led lp  since
the hover range for rear di was too high and the brightness of ftir was too low  the led lp
we built only had a narrow band in which hover was an issue  and had much higher brightness
than our ftir 
   image processing
substantial pre processing is neccesary in order to transform the raw data from the multi touch
device into useful input for our classifier  first  we use background subtraction to remove the
light gradient caused by light leakage around the touch surface  next  we apply a gaussian blur
to remove noise  followed by a threshold filter that transforms the gray scale image into a binary
image  noise is further reduced by removing all blobs with area less than    pixels  finally  we
use a standard blob detection algorithm to divide the filtered image into zero or more distinct
blobs 
in order for our classifier to act on hand shapes rather than individual blobs  we aggregate related
blobs into groups  the importance of this aggregation is illustrated by the palm  which is
composed of several distinct blobs but must be classified as a single image  our blob
aggregation algorithm begins by considering each of the blobs to be separate  and groups any
two blobs whose convex hulls intersect  the algorithm then proceeds recursively  with two
groups being combined whenever their convex hulls intersect  and runs until a steady state is
reached  this algorithm is exhibited in figure   

fiour final pre processing step standardizes the orientation of the grouped blobs  we perform
linear regression on the image to find the line of best fit for the image  and then rotate the image
so that this line is vertical  this final image is used as input for our classifier 
   classification
we used the spider machine learning matlab toolbox to train an svm that attempts to classify
the processed images into one of three classes  fingertip  side of the hand  and palm  several
distinctive features of the image  outlined in table    were used as input to this classifier  we
also experimented with performing naive bayes on the pixel data  but abandoned it after our
svm gave us such good results 
we created a training data collection program that captures camera
images  extracts the desired features  and saves them with a specified
label  using this program  we created a training set comprised of    
raw images  with one example contact in each image  we collected
   finger examples      side examples  and    palm examples  with
data collected from   different users  additional side examples with a
variety of orientations were used  because initial results suggested
that the classifier had the most difficulty identifying sides 
we optimized our svm by performing model selection on kernel
parameters and selecting the kernel with the lowest cross validation
error  model selection significantly decreased our error  for the
polynomial kernel  the degree parameter was tested between      and
      with an error range from     to      for the gaussian kernel 
the sigma parameter was tested between      and       with an error
range from    to      we chose the gaussian kernel with a sigma
value of       since the optimized gaussian kernel achieves a    fold
cross validation error of    compared to the optimized polynomial kernel error of      a plot
of the cross validation error for the gaussian and polynomial kernels as the model parameters
vary is shown in figure   
we found that performing backwards search feature selection allowed us to eliminate some
extraneous features while still maintaining    cross validation error  using a gaussian kernel
with sigma         we found five extraneous features  equiv  diameter  minor axis length  the xcoordinate of the centroid  extent  and eccentricity  figure   shows the error as each feature

fiidentified by backwards search is added back to the classifier  the most useful feature is major
axis length  which reduces error to     even when it is the only feature 

fi   conclusion and future work
our svm allowed us to successfully classify hand shapes on our custom built multitouch device 
as our classifier achieved       fold cross validation error on our training data set of    
images  we also created a demo program that allows users to touch the screen and have their
hand shape classified  tests of this tool during a live demo of the project were very successful 
while we did not collect hard data during our live demo  most hand gestures were classified
correctly  the main source of error during our demo was with users wearing long sleeves or
having hands bigger than any of our training set  additional training data with sleeves and a
wider variety of hand sizes would likely help to prevent this problem 
despite the fact that we built several iterations on our hardware  there remains much room for
future improvement in this area  for example  a refined ftir or rear di set up with high
touch hover contrast would yield filled shapes of contacts  rather than just the outlines of
contacts from led lp  with these filled shapes  our blob grouping algorithm would no longer
be necessary and classification would require less image processing time 
additionally  while our system can perform classifications in a few seconds  it would become
much more useful for most applications if classification could be done in real time  moving the
system from matlab to c   opencv would increase our computational speed and facilitate
integration with future projects involving multitouch classification 
once classification can be done in real time  the problems of contact tracking and gesture
classification should be addressed  contact tracking requires maintaining a list of contact
locations  shapes  and heading  and can likely be accomplished with only minor extensions to
currently existing blob tracking algorithms  as are currently used by touchlib  such algorithms
compare successive images  determining whether a contact is a new contact or a translation of a
current contact and removing old contacts from the list  gesture classification would involve not
only detecting hand shapes  but also identifying different hand motions upon the touchscreen 
   references
j  weston  a  elisseeff  g  bakir  and f  sinz   spider  general purpose machine learning
toolbox in matlab   the spider     july          dec      
 http   www kyb mpg de bs people spider   
n  motamedi  et al   lcd ftir   first results   touchlib forum     nov        nui group    
dec        http   nuigroup com forums viewthread        

fi