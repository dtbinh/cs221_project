prediction of double gene knockout measurements
sofia kyriazopoulou panagiotopoulou
sofiakp stanford edu
december         
abstract
one way to get an insight into the potential interaction between a pair
of genes is to compare the phenotype of the double knockout organism to
the phenotype we would expect if the genes were independent  however 
since gene knockouts are expensive and time consuming  we would like
to be able to predict gene interactions  we used a dataset consisting
of     yeast genes for which only a few double knockout measurements
were known  and applied regression algorithms to predict the unknown
measurements  in this article  we describe the algorithms we used and
compare their results 

 

introduction

one way to get an insight into the interaction of two genes is to knock them
out and compare the phenotype of the double knockout organism to the phenotypes of the two single knockout organisms  this can be done by comparing the
measurement of a reporter protein in the double knockout organism to what
we would expect if the genes were independent  based on their single knockout measurements  for instance  if two genes are in the same pathway  then
knocking out both may have a less severe phenotype than expected 
we consider the case where only a small subset  the query genes  of the genes
we want to study have been knocked out in pairs with all the other genes  and
for the rest of the genes  the array genes  we only have their double knockout
measurements against the genes of the first group  we would therefore like to
predict the unknown measurements 
for our experiments we used a dataset of     yeast genes  there are    query
genes measured against almost all the genes of the dataset and     array genes
measured only against query genes  our goal is to predict the double knockout
measurements of the array genes against other array genes  since the expected
double knockout measurements are known for all pairs  predicting the difference
between observed and expected double knockout measurements is equivalent
to predicting the true double knockout measurements  the data we used for
predictions is in the form of a symmetric          matrix d  d i  j  is the
difference between the true double knockout measurement of genes i and j and
 

fifigure    a simplified version of our
problem  genes   to   are query
genes  and genes   to   are array
genes  the entries in light blue
are known  these are the differences between the observed and the
expected double knockout measurements for the corresponding pairs of
genes  all the other values are missing  we are interested in predicting
the values in yellow 
their expected double knockout measurement  if there is no double knockout
measurement available for the pair of genes  i  j   then d i  j  is treated as a
missing value 
figure   presents a simplified version of the problem  in this example genes
  to   are query genes and genes   to   are array genes  the light blue entries
are the differences between the observed and the expected double knockout
measurements for the corresponding pairs of genes  these are the values we
know  all the other entries are unknown  notice that query genes are measured
against most but not all the other genes  there might be a few missing values
even for query genes  these may have been caused by errors in the experiments
and are shown in red  of course we could try to predict these values too  and
in fact we do   but we are mostly interested in predicting the values shown in
yellow  i e  the measurements of the array genes against other array genes 
in section   we describe the cross validation scheme we used for evaluating
our predictions  in section   we explain how we can make relatively crude
predictions by clustering  later  in sections   and   we see how these predictions can be refined using regularized linear regression and gaussian processes
respectively  finally  in section   we conclude and discuss the directions of
future work 

 

evaluating our predictions

we used a    fold cross validation scheme to evaluate our predictions  at each
fold we selected a subset of the query genes and treated them as array genes 
i e  we treated their measurements against array genes as missing  then we
computed the root mean squared error  rmse  of our predictions for these
genes 
we assume that most of the pairs of genes are independent  so in most cases
the difference between the true and expected double knockout measurement
should be close to zero  therefore  our baseline algorithm always predicts zero 
the baseline rmse is        

 

fi 

solution    k means clustering

if we treat each row  or equivalently each column  of d as a training example
with     features  then we can apply k means to cluster the genes  of course 
the values of some of the features are missing for some examples  there are
several ways to handle these missing values  for example  we could replace
them with zero  based on our assumption that most pairs are independent  or
with the row or column average  however  it turned out that the best approach
in terms of performance was to completely ignore the missing values  i e  to
compute the distance between a gene and a centroid taking into account only
the common non missing features 
after clustering  we can use the values of the cluster centroids to predict the
missing values of the genes in the cluster  remember that d is symmetric  so
to predict d i  j  we can use either the cluster of gene i or the cluster of gene j 
to see why this is true  assume that we want to predict d       in the example
of figure    we could find the cluster of gene   and take the value of the  th
feature of the centroid of that cluster  this would be the average measurement
of the genes of the cluster against gene    alternatively  we could find the cluster
of gene   and take the value of the  th feature of the centroid of that cluster 
this would be the average measurement of the genes of the cluster against gene
   for that reason  our prediction for d i  j  is the average of the value of the
ith feature of the centroid of the cluster of gene j and the j th feature of the
centroid of the cluster of gene i 
when running k means  we initialized the centroids   times and returned
the assignment to clusters with the smallest sum of within cluster distances 
the    fold cross validation experiment was repeated    times  the mean of
the rmses we obtained was        with a standard deviation of        

 

solution    regularized linear regression

consider the simplified example of figure   and assume we want to predict
the unknown measurements for gene    i e  we want to predict the last   values of column    we can use the first three rows of d  for which  almost  all
the values are known  as the training data for a linear regression model  our
dataset consists therefore of three training examples with   features each  the
corresponding outputs are the three first values of column    using the parameters learned by linear regression  we can predict the values of column   that
correspond to the sets of features of the last   rows of d 
however  not all the features of the last   rows are known  for that reason 
we first apply k means and replace missing values with the corresponding predictions  this implies that we follow a two step prediction procedure  where we
first find some crude predictions using k means and then refine them using
regression 
note that the number of features used for linear regression is much larger
than the number of training examples  going back to the original problem 

 

fithere are    query genes which will be our training examples  each having    
features  as a result  linear regression doesnt work  unless we use some form
of regularization  such as l  and l  
l  regularized linear regression minimizes kx   y k    kkk    where x is
the matrix of the training data with one training example per row   is the
column vector of the parameters of linear regression   y is the column vector of
the target values  and k is a parameter of the model  the closed form solution
for  is then  x t x   ki   x t  y   where i is the identity matrix 
l  regularized linear regression minimizes kx   y k    kkk    there is
no closed form solution to this minimization problem  so we used an existing
matlab implementation     
for both l  and l  regularization  the parameter k was chosen by comparing different values using the cross validation scheme described in section   
since we apply k means before clustering  there is a random component in our
predictions  so again we ran the cross validation several times and computed
the mean and standard deviation of the rmses obtained  the mean rmse
for l  regression was        with a standard deviation of         while for l 
regression the mean was        and the standard deviation        

 

solution    gaussian processes

we briefly explain how gaussian processes can be applied in the case we study
using the example of figure   and following the same naming conventions as
in     
assume that we want to predict the missing values of gene   and that y  i   
f  x i       i    i                    where   x i    y  i     are our training examples  as
in the previous section  and  i   n          are i i d  noise random variables 
assume also that x is the      matrix of training examples as explained before 
x is the      matrix of test examples  with missing values replaced as in the
previous section    y is the vector of known outputs for the examples of x  and
 y are the target values we want to predict  then  assuming a zero mean prior
over functions f     gp     k       where k is a kernel function  it can be shown
           that  y   y   x  x  n         where
   k x   x  k x  x       i    y
   k x   x        i  k x   x  k x  x       i   k x  x  
k x  x   r     such that  k x  x  ij    k x i    x j      and k x  x   
k x   x   k x   x   are defined similarly 
we used a linear kernel k x  x      x  x    and chose the noise variance by
cross validation  the mean rmse for          found after    repeats of the
experiment  was         with a standard deviation of         it is worth noting
that we experimented with other kernels too  the predictions of the squared
exponential kernel were always very close to zero  so its performance was almost
the same as the baseline performance  and the polynomial kernels of order higher
than   performed worse than the linear 
 

fi 

conclusions and future work

table   summarizes the results of the previous sections 

mean rmse
standard deviation
iterations

k means
      
      
  

l  regression
      
      
  

l  regression
      
      
  

gp
      
      
  

table    mean and standard deviation of the rmses for the algorithms of the
previous sections  the last row contains the number of iterations that were used
to obtain the statistics in each case  the baseline rmse is        
although l  linear regression has the best performance  we believe that
gaussian processes are a promising approach due to the flexibility introduced
by the kernel function  evaluating more kernels is an important direction of
future work  we are currently working on using l  linear regression to learn a
bayesian network of the genes and to create a covariance matrix that could be
used as a kernel  additionally  we are studying ways of incorporating the single
knockout measurements into our predictions 

acknowledgements
i am deeply grateful to ph d  student alexis battle for her invaluable guidance
and advice throughout this project 

references
    c  e  rasmussen  c  williams  gaussian processes for machine learning 
mit press        http   www gaussianprocess org 
    chuong b  do  honglak lee  gaussian processes      
    p  carbonetto  matlab implementation for l  regression  http   www 
cs ubc ca  pcarbo 
    u  n  lerner  hybrid bayesian networks for reasoning about complex
systems  ph d  thesis      

 

fi