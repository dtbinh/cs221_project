holey ship  bilging by reinforcement learning
jesse rodriguez  tiffany chen  and jason turner maier
december         

 

introduction

off between increasing the size of the break and difficulty of producing the larger break is whats called a
 bingo   or a  x x  
this type of break can be made often enough such
that it is feasible to work towards it as a strategy 
while also giving a large number of points for completion  due to this point efficiency  we trained our
bilging player to prefer building bingos over other
break types  if larger breaks are readily available 
our player will take them  but bingos are the break
type it actively works towards  as a point of notation  we will refer to a  one away bingo  as a bingo
that can be made making one move  for example on
the left we have a matched bingo   x x    and on the
right we have a one away bingo 

we constructed a markov decision process  mdp 
player of a bejeweled like game called  bilging  this
game is part of a massively multiplayer online game
based on puzzles  in this game  a player is presented
with a grid of pieces  the board   and for each move he
is allowed to swap horizontally adjacent pieces  if  after a move  three or more adjacent pieces of the same
type are aligned in a row or column  the player gains
some number of points based on the number and configuration of these pieces  immediately  the pieces are
then  broken  and the pieces below them are shifted
up to fill the space  random pieces are added at
the bottom to fill up the resulting empty space  the
scoring is such that breaking more pieces in a single
move is worth significantly more than breaking them
separately  i e  breaking six pieces at once is worth
significantly more than breaking three pieces twice 
there are several different possible types of breaks
based on the number of distinct horizontal vertical
matches made with one move  the players overall
score is determined by their average score per move 
there are several interesting features of the game
that make it a compelling problem  first  the state
space is large with  e   possible boards  so we cannot
explicitly represent all possible boards  second  a
greedy strategy yields a poor average score  since the
scoring function increases much faster than a linear
function of number of pieces broken  third  since the
game can progress for an indefinite amount of time 
we are solving for a problem that has no explicit final
goal state 

blocker pieces

one problem that humans experience when making
bingos is that they will move a piece towards completing a bingo  but the move will cause an unexpected
break to occur near the bingo which destroys it  this
is because nearby pieces are aligned to form a match
and the move completes it  because they hinder the
player from making the intended bingo  we call these
nearby pieces blockers  here is an example where a
the  bingo 
player is trying to complete a smaller match called a
there are    types of matches  also called breaks   x  but is hindered by blockers 
with large rewards for the larger break sizes  however  larger breaks are far less likely to occur by
chance so they usually require several moves to reconfigure the board to make them  human players
are prone to making an irreversible mistake during
these reconfiguration moves  it is generally agreed
among the best bilging players that the ideal trade 

fithe player would like to move piece a over to the right  we call it a right bingo  if its on the left its a
right to complete the  x  with the c pieces  however  left bingo 
if it does so in a naive fashion  it will form a break
with the b pieces on the way and will be unable to
complete the  x  since the a and b pieces will be
destroyed  it needs to move one of the b pieces out
of alignment with the other before moving a if it
wants to successfully complete the  x   therefore 
the b pieces are blockers in this situation 

 
   

methods
then we set the anchor to be the position reached
when walking one step from these two pieces in the
direction of the  rd 

features

since the state space of the problem is so large  in order for the learning process to be tractable we reduce
our state space by representing boards as features
of potential bingos on the board  we can easily detect if a bingo is possible by counting the piece colors
present in each row  let us make a few definitions before fully describing the features  for these purposes 
assume a one away bingo is structured as follows 

here are the bingo features we use to describe our
boards 
 sum of the distances from the two closest vxs
not in the same row to the anchor
 sum of the distances from the two closest vos
here  the two color types are denoted as x and o 
not in the same row to one away from the anchor
v and h refer to the row in which the piece appears 
in the direction of the bingo orientation
by convention  x refers to the more abundant color
in the h row  our features for a possible bingo mea sum of the distances from the three closest hxs
sure how far from the one away configuration it is 
to the anchor
these features are defined as distances from the rele distance from the ho to the anchor
vant pieces to a reference  anchor  point which is an
estimate of where the ho piece should end up in the
 number of pieces which could not be moved
one away bingo configuration  we use the following
naively into position due to blockers
heurisic to guess where ho should be  first  we find
the two closest x pieces in the h row 
these are features are depicted graphically here 

if there is no bingo possible on the board  all the
now  find we find a  rd x piece in the h row  that features we describe above are set to their maxiis closest to the first two pieces  this  rd piece de  mum possible values  eg the largest distance any piece
termines the orientation of the bingo  if it is on the could be from the anchor  
 

fietc  in the vector c such that ck   score for match k
with  c        so  if action a on state s creates a
match k  then
r s  a    ck

as an aside  we also tried many iterations of
many different types of features before settling on
these  they involved loose patterns and distribution of pieces on the board without specific breaks
in mind  we found that training on these features
either produced greedy behavior  or pursuit of the
encoded patterns rather than matches that yielded
long term high scores 

   

value function
since we use the state action version of the reward
function  we have to modify our value function from
the state only reward function described in the cs   
lecture notes    we train our value function such
t
that
maxa r s  a   
  s      s  approximates
 
  v 
k
 
t

j    succ s  a    
k

mdp definition

states
each state is a full board configuration which is a
matrix with    rows and   columns with each element containing an integer value representing one of
  colors  formally  each state s is defined by 
   

sz

   

fitted value iteration

training

si j                

we created a training set of      boards where a
bingo could be made with one move  and      boards
we do not consider boards with matching values where a bingo could be made after several moves  we
to be valid states  both of these statements hold 
used a value of        originaly but saw no behavioral difference from       to estimate the optimal
   i  j  such that si j   si j     si j  
value function  we use standard fitted value iteration
described in the cs    lecture notes  except that we
   i  j  such that si j   si   j   si   j
use the version that uses a reward function given by
a state action pairing such that at at iteration we
actions
calculate for each training example s i 


each action swaps two horizontally adjacent pieces
k
 
 
such that taking action ai j     i     on state s
y  i    max r s i    a    t 
succ s i    a 
a
causes exchange of values of si j and si j    
k j  
 

successor function

playing

our successor function succ s  ai j     s  is deterministic if ai j does not cause a match of   or more
in a row such that s  is identical to s except that
s i j   si j   and s i j     si j  
if ai j causes a match  then s  is sampled randomly from distribution of successors states decribed
in the dynamics above whereby matching values are
removed  values are shifted vertically to fill their
place  and the remaining values at the bottom are
sampled with a uniform distribution over all   color
values such that the newly sampled pieces do not
cause any matches 

after training we have learned the optimal value function v   and in order to choose the best move to make
at each state  the mdp player evaluates the expected
value of each action by sampling each action k times 
thus it chooses to make the following action for state
s 


k
 
  
arg max v
succ s  a 
a
k j  

this corresponds to a   move lookahead to optimize
over our value function that guides it towards completing a bingo with each move 

reward function

   

we use a reward function r s  a  based on the current
state  s  and the action a that is taken on this state 
any action that does not cause a match is given a
reward of    and any action that makes a match is
given a positive reward  we store the score given by
each of the    of the match types   x    x    x x  

software

we
used
the
aima java
package
 http   code google com p aima java   for linear
algebra subroutines for performing normal equationbased linear regression during mdp learning  all
  http   www stanford edu class cs    notes lecture   pdf

 

fiother software was written by us  including the game  
discussion
simulator  the gui  the mdp  model training routines
we attribute the good  but less than perfect  perforand  and game playing routines 
mance to a couple factors  first  our mdp players
somewhat low bingo completion rate signficantly lowers its score  it seems to have the most trouble with
  results
very spread out bingos which have many pieces far
by the numbers
from their correct position are difficult to assemble
without falling into a local optima  as previously dewhen playing    moves per game  our mdp player
scribed   these same bingos tend to be difficult for
obtains an average score of   points per move  this
humans as well  having a one move look ahead limis much higher than a greedy implementation  which
its our value function in this regard since it is insufaverages a score of     points per move  it obtains
ficiently expressive to create value function that is a
scores similar to above average players  better than
strictly increasing while progressing towards a bingo
the authors of this work   but is significantly short of
in all possible cases  to address this problem  we may
the best best human players  it is rumored that the
add a small search routine or a two move lookahead
top human players average approximately      points
which would help our player overcome these one move
per move 
local optima  secondly  our mdp is not trained to
the mdp player completes      of the possible optimally identify special cases where it can gain exbingos it encounters  while making moves towards tra points by leveraging its knowledge of the scoring
bingo completion  it sometimes enters a local optima function 
where it will decide not to make any more moves to
overall  however  the player does very well in concomplete the bingo  we consider this a failure of structing bingos and exhibits desirable behavior  in
the mdp player since we trained it to complete as conclusion  we have produced a strong mdp bilging
many bingos as possible  however  by comparison  player  and future work will need to focus to work on
a greedy player implementation achieved a    bingo reducing the number of missed bingos to make it play
completion rate over the same number of moves and optimally  eventually  we aim to compete our player
games 
against a broader audience of human players 
behavior

acknowledgements

this refusal to complete the bingo results from local
optima in the estimated value function where it views
any further moves around the bingo as worse than
playing at any other position on the board  this situation tends to occur when the mdp player must swap
two pieces where it moves one of them into a better
position but the other into a worse one  futher  it
usually occurs when it is very close to the completion
of a bingo 
it exhibits a couple interesting  intelligent  maneuvers that humans perform  and also some that
humans are rarely capable of performing  because
we added features that take into account pieces that
block the construction of a bingo  the mdp is able to
recognize and move these blocking pieces out of the
way  as mentioned before  human players can accidentally break a bingo before its completion  but it
turns out that our player will actively destroy a posible bingo in order to optimize the rest of the board
for another better bingo elsewhere which improves its
overall score 

we would like to thank tom do  zico kolter  and
ian goodfellow for helpful comments  suggestions 
and guidence in this project  the original bilging game is made and owned by three rings
software as a part of their puzzle pirates product
 http   www puzzlepirates com   

  computed as number bingos completed divided by number
of possible bingos encountered during    games of    moves per
game

 

fi