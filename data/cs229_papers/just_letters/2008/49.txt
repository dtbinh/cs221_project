character set encoding detection using a support
vector machine
sean r  sullivan
seans google com

abstract
i propose a method to automatically detect the character set encoding for an
html text document using a support vector machine  using only the stream
of bytes within the document  this algorithm learns the patterns of byte n gram
frequencies to classify the document  the algorithm will currently detect the following popular character set encodings  us ascii  utf    utf     iso        iso         iso         big   gb      gb       shift jis  euc jp  euckr  koi  r 

 

introduction  the problem 

have you ever received a web page containing strange question mark characters  or other garbled
text  unable to decode bytes into text characters  the browser substitutes these question marks 
or believing erroneously it can decode the bytes into text  it outputs gibberish instead  without
knowledge of the scheme used to encode text into bytes  the character set encoding  it is not possible
to make sense of a stream of bytes  these encodings come in several different flavors  singlebyte  multi byte  and variable length  additionally  some encodings are proper supersets of other
encodings  e g  utf   is a superset of us ascii  
table    character set encodings
encoding
common languages encoded
us ascii
english
utf  
all
iso         latin    spanish  french  german
gb     
chinese  japanese  korean
gb    
simplified chinese
shift jis
japanese

num bytes
 
   
 
      or  
 
   

the webs hypertext transfer protocol  http      specifies the character set encoding for a web
page should be declared in the content type response header  example 
content type 

text html  charset iso       

but if this content type header is wrong or missing  how can we decode the web page  is
it possible to detect the character set encoding of a text document without this metadata  using
features generated from the raw bytes of html text documents  this experiment attempts to use a
support vector machine  svm  to determine the documents character set encoding 
   

digresion  unicode

after the initial proliferation of incompatible character set encodings  the unicode consortium was
created to unify the representation of all languages  toward that end  the consortium has attempted to
 

fidefine a standard for the characters of all the worlds languages  separately  it has defined methods to
encode these characters  several of these encodings include utf    utf     and utf     because
the variable length utf   encoding is compatible with us ascii  and because it is a compact
representation of some the webs most common languages  it is fast becoming the current standard
for encoding text documents 

 

features  byte n gram frequency 

since almost all character set encodings require between one to four sequential bytes to encode a
character  i define a byte n gram as a sequential concatenation of n bytes  for example  a byte
  gram  trigram  is the concatenation of three adjacent bytes within a document  while a   gram
 unigram  is simply a single byte  consequently  there are   n possible values for each type of
n gram  these n grams are generated by scanning the document sequentially  shifting the n gram
window by one byte at each iteration  the frequency  by percentage  within the document of each
of the n gram values defines a feature  for example  a document encoded in us ascii could have
a frequency of        for the unigram value      which is character d in this encoding   while a
chinese language document encoded in gb      could have a significantly different frequency for
this unigram value  if we include all n grams up to    the feature space size is 
                                    
   

feature selection

since this feature space is so large  this experiment only calculates features for unigrams  bigrams 
and trigrams  but since even this space is too large  approximately    million   we need a method to
choose good features from this large set  in order to find features which discriminate well between
the positive and negative class  this experiment uses the kullback leibler  kl  divergence 
r
p   x 
kl pkq    x p x  log q x 
assuming the distribution of the byte n gram frequency is gaussian for each encoding  this experiment calculates the frequency mean and variance for the set of positive and negative examples for
each feature  since the kl divergence is an asymmetric measure of the distance between two probability distributions  we can measure how much a feature differs between the positive and negative
examples  a high kl divergence indicates a feature which discriminates well between the positive
and negative examples  because it indicates the positive label has high probability and the negative
label has low probability  in order to have some confidence in the gaussian parameter estimation 
this experiment only includes a feature if a minimum threshold of number of frequencies was present
for both the positive and negative examples  not all features correspond to a character in the positive
label encoding  but the following table illustrates examples when the chosen feature is a character 

encoding
utf  
utf  
utf  
iso       
iso       

 
   

table    feature kl divergence examples
language
byte value num bytes
simplified chinese      
 
korean
     
 
traditional chinese      
 
russian
   
 
portuguese
   
 

kl divergence
      
      
      
     
    

experiments
data set

in order to generate html documents with different character set encodings  a set of web pages
from wikipedia     was downloaded for some of the worlds most common languages  these web
pages were all encoded in utf    these pages were then converted into encodings common to
 

fithese languages  using the charsetencoder     class included in the standard edition   of the java
development kit  additionally  every example was encoded into utf    
table    languages and their character set encodings
language
character set encodings
english
us ascii
spanish
iso       
french
iso       
german
iso       
italian
iso       
portuguese iso       
polish
iso       
czech
iso       
hungarian iso       
russian
iso         koi  r
chinese
big   gb      gb     
japanese
shift jis  euc jp
korean
euc kr
   

support vector machine

this experiment used thorsten joachims svm light software     which implements a support
vector machine  the software defines four built in kernels  linear  polynomial  radial basis function 
and sigmoid hyperbolic tangent  having found no difference between the standard kernels  the linear
kernel was used in all of the experiments 
   

the procedure

for each character set encoding  this experiment generated a random set of html documents  where
approximately half were drawn from this encoding as the positive examples  the negative examples
were drawn from the remaining encodings  this generated set was divided into a training set      
and a test       set  once the training set was created  this experiment ran feature selection on this
training set to determine the salient features  after generating the features for the training set  the
support vector machine was run against the training set to learn a model 
svm learn  x   train utf   dat model utf  
the  x   flag calculates the leave one out cross validation  loocv  error  once the model was
created  it was run against the features generated from the test set 
svm classify test utf   dat model utf   results utf   txt
   

classifier comparison

as a final check  we compared the accuracy of the svm classifier against jchardet      the java
port of mozillas charset detector  for three popular encodings  we generated another random set of
     documents  and asked the jchardet classifier to identify the character set encoding  using
our previously generated svm model  we attempted to classify these same examples  since the
jchardet software returns multiple labels if it is unsure  this comparison is inexact  whenever
jchardet returned multiple labels  we considered this a misclassification if the correct label was
not among the returned labels 

 

results

for classifying some encodings  the svm classifier appears to exceed the accuracy of the
jchardet classifier  since the loocv error appears similiar to the test set error  it appears we
are not overfitting the data  the number of support vectors for each model gives us an idea of how
 

fipositive label
utf  
utf   
us ascii
iso       
iso       
iso       
koi  r
gb    
gb     
big  
shift jis
euc jp
euc kr

table    svm with linear kernel
num training num test num support vectors
    
    
   
    
    
   
    
    
   
    
    
   
    
    
   
    
    
  
    
    
  
    
    
   
    
    
   
    
    
   
    
    
   
    
    
   
    
    
   

loocv
     
     
     
     
     
     
     
     
     
     
     
     
     

accuracy
      
      
      
      
      
      
      
      
      
       
       
      
      

table    svm classifier compared to mozilla jchardet
encoding
svm classifier jchardet classifier
utf  
     
      
iso             
     
gb     
     
     

easily the svm found a separating hyperplane  unsurprisingly  the models with the fewest support
vectors produced the best accuracy 

 

conclusions and future directions

while this experiment produced relatively good classification accuracy  there are several improvements that can be made  the current one versus all classification will not scale well  as examples
from more encodings in different languages are added to the negative examples  it will become
harder to find a separating hyperplane  a better approach might be similiar to a decision tree  an
initial classification could determine the number of bytes for the encoding  single  double  variable  
which would subsequently provide a more accurate way to generate the byte n grams 
the feature selection can be improved on several fronts  we could incorporate the confidence interval for the gaussian parameters in our kl divergence calculation  we could filter out kl divergence
measures that do not meet a confidence threshold  additionally  we can incorporate features which
measure the difference in frequency readings between the positive and negative examples  for example  if there are numerous frequencies for the unigram value    within iso        encoded documents  but none for all other encodings  then we should use this disparity to identify iso       
encoded documents  currently  we are only incorporating features for which we can estimate gaussian parameters for both positive and negative examples  since we are trying to reduce the number
of features by selecting those which discriminate the best  it might be worthwhile to explore pca 
finally  we can incorporate knowledge of specific encodings into the classification  by adding features particular to that encoding  for example  we could add a feature which defined the percentage
of n grams values with the high bit set  since us ascii is a   bit encoding  none of the bytes would
have the high bit set 
references
    r  fielding 
et al  
hypertext transfer
hhttp   www w  org protocols rfc     rfc     sec   htmli

protocolhttp     

    http   en wikipedia org
    http   java sun com javase   docs api java nio charset charsetencoder html

 

rfc

     

     

fi    t  joachims     in  making large scale svm learning practical  advances in kernel methods   support
vector learning  b  scholkopf and c  burges and a  smola  ed    mit press       
    http   sourceforge net projects jchardet

 

fi