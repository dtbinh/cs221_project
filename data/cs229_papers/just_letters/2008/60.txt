cs    machinelearning stanforduniversity















fall    

querysegmentationusingsupervisedlearning
asif makhani
asifm stanford edu

  introduction
improving both recall and precision to return the specific
results that the user originally intended to find is an
important aspect of search engine improvements  one
component of this problem is related to translating the
users intention to the best query sent to a search engine in
order to get the most relevant results  for example  if a
user is looking for the cheapest   gb flash drive  she may
enter the keywords   cheap   gb flash drive  or  sale   gb
usb drive  into the search box  a search engine attempts
to derive the query intent from the keywords as well as any
user behavior information such as the previous query and
could construct a new structured query in order to retrieve
the most relevant results 
there are many techniques to translate the original query to
reflect the users intent such as query expansion and query
segmentation  in the case of query expansion  new terms
are generated  as additions or replacements  to construct a
revised query based on the context of the original query  in
the case of query segmentation  the query terms are divided
into individual phrases and semantic units from the
sequence of user s query terms 
the focus of our research is to develop a novel way using
supervised machine learning to segment the original
queries into a set of phrases  for example  the query  
cheap   gb flash drive  can be segmented into   distinct
segments as    cheap     gb   flash drive     generating
the segments allows us to provide the necessary hints to a
search engine to improve its relevance by ranking
documents  eg  web pages  with the segments higher than
the documents in which the terms of a segment are not
adjacent 
one way to achieve that is to translate the query to specify
phrases in order to increase precision  that is   cheap  
gb flash drive  becomes  cheap   gb flash drive  
alternatively  if the above reduces recall  one can build a
search engine where the segmentation is provided as
additional hints used only during ranking by boosting
documents containing a closer proximity of the segment
terms 
there are also other potential applications of query
segmentation such as providing search suggestions and

takahiro aoyama
taoyama stanford edu
spelling correction  by computing and storing a collection
of known segments from previous user queries  a search
engine can find and suggest the segment closest to the
current query  for eg   the term harry can be mapped to
harry potter as a suggestion  similarly  the query britany
spears can be mapped to a spelling correction of britney
spears  using edit distance as a method of defining
closeness  

  related work
interestingly  many researchers have worked on the query
segmentation problems in different ways  risvik et
al        approached the problem by combining the
frequency count of a segment and the mutual information
 mi  between pairs of words in the segment in a heuristic
scoring function  in other nlp based approaches to query
segmentation  tan and peng        used a generative
query model using em optimization to estimate n gram
frequencies in order to recover a query s underlying
concepts that compose its original segmented form 
bergsma and wang        went beyond simple n gram
count based features by building upon previous nlp work in
noun compound bracketing 

  approach
we addressed the query segmentation problem as a binary
classification problem  for each query q    t  t   tn  where
ti is a query term  we extract all possible segments s    s  
s   sn  where si   tk  tk    tp is a segment limited to
adjacent terms  for this study  we limited the set to
segments of size two  for example  for q    cheap   gb
flash drive   s    cheap      gb  gb flash  flash drive  
each input segment si is characterized by a set of feature
x  i     and has a binary output y i          such that the
segment si is a relevant segment if and only if y  i        so in
the case of the above example    gb and flash drive
would be classified as relevant segments  and cheap  
and gb flash as not 
we used queries sampled from aol query logs  discussed
in detail in section    with the relevant segments identified 
as training data to produce a model using linear regression
to predict which segment of a test query is relevant  for

fi   pmi based on   results using web search   order
not preserved
   pmi based on   of pair occurrences in a document
corpus  query order preserved
   pmi based on   of pair occurrences in a document
corpus  order not preserved
   pmi based on   of pair occurrences using a query
corpus  query order preserved
   pmi based on   of pair occurrences using a query
corpus order not preserved
   proximity of terms in the document corpus

each segment  we computed the input x is a list of features
and the output y is the binary classification 
using logistic regression  we have the following loglikelihood function 
m

l        y   i   log h   x   i           y   i     log     h   x   i         
i   

we want to maximize likelihood function l      
where h             x          x               k x

 k  

  we

features       are pmi using the different data sources
 web search engine  document corpus  query corpus  with
consideration on the order of the terms in the query  in
addition to pmi features  we compute the proximity feature
    using the distance between terms in a document based
corpus  since the distance between terms is different in
different pages  we assume that distance of terms in the
document corpus is given as average of the minimum
distance in each document in the corpus 

estimated the parameters using the newton raphson
algorithm for optimizing the likelihood function l       the

 s we learned are applied to find the decision boundary to
classify si 

      h    l      where h ij  

   l   
 i  j

   

prox   t     t      

table    examples of inputs and outputs
query

pmi
web

pmi
wiki

pmi
aol

proximity



y

ohio
state

       

       

       

 



 

  data

ohio
movie

       

 

       

  



 

   

  features
we used multiple input features to characterize a segment
si  where each feature represented the correlation of the
terms in a segment  e g  distance between two terms in a
document  adjacency of terms  and mutual information
between terms   as a primary association features  we
used pointwise mutual information  pmi  and found it to be
both powerful and simple to compute 

 smallest

distance b w t     t   in i th doc    
  of documents containing t     t  

training and testing

training and test data is a collection of         queries
adapted from the aol query dataset  bergsma et al        
with the relevant segments identified to train and test our
system  data contains set of segmented queries and clickurl as seen in figure    for each query with relevant
segments identified  we generated both relevant and nonrelevant segments  for example  schools blue in figure  
would be extracted as a non relevant segment 

figure    segmented query logs and click url
p t  t    
pmi t    t       log
p t     p t    
where p t i    

p t t      

   

  of occurence of t i
and
  of words in the documents
  of occurence of t  t  
  of all pairs in the documents

the following distinct features were computed for each
segment 
   pmi based on   results using web search  query
order preserved

since our study was limited to segments of size    we
dropped the data involving larger relevant segments  that
is  in the example in figure    a relevant segment west
virginia state university does not necessarily imply that all
adjacent segments of size   are relevant  eg  virginia state  

   

data sources

we used the following data sources to compute our
features 
a  web search index  using alexa web search 
we used web search using the alexa web search service to
compute pmi based on the number of the search results 

fip t t      

  of web search results for   t t     n    t t      
 
total   of web pages
t

where n q      of results for the query q and t is the total
  of web pages  estimated at   billion in the case of alexas
search index   thus  we have

pmi t t       log

n    t t         t
n    t        t  n    t         t

  evaluation
our evaluation criteria is based on standard information
retrieval metrics of recall  precision and f measure  for
each test run  we compare the segments classified as
relevant with the actual total relevant segments 

recall  

   relevant segments    classified relevant segments   
   relevant segments   

  log n    t t         log t  log n    t      log n    t      
precision  

   relevantsegments   classified relevantsegments  
   classified relevantsegments  

in the cases of features where order is not preserved  we
consider both possible orders 

p t t      

n    t t        n    t  t     
t

b  wikipedia release version
in addition to the web search index  wikipedia release
version          is used as a raw document corpus 
wikipedia release version is a collection of good quality
titles       articles     million words   the motivation of
using document type corpus is to extract the context
information  such as words sequence and distance between
words  not readily available when using an external search
index 

f   measure     

  precision   recall 
 precision   recall 

  results
we experimented to build an optimal model by focusing on
the computed features 

we used the document corpus to compute pmi based on
the number of pair occurrences of terms in a segment as
well as proximity function based on the distance between
terms 
c  aol query logs

figure    individual features
the set of aol query database  pass et al         was also
utilized in order to capture the users behavior for searching 
which is not captured in document based corpus  this data
source includes    m search queries from    k real users 
containing users anonymous id  query issued by users 
time when query was provided  the rank and url of a
website that user clicked  for the purpose of computing
pmi based on the number of pair occurrences of terms in a
segment  users query is only used 
we decided to use both a document based corpus and a
query based corpus as both has their own tradeoffs  a
document corpus such as the entire web provides high
coverage but can be costly to compute advance features  a
query based corpus  on the other hand  is more
manageable but provides less coverage due to its
sparseness  however  a query based corpus is much more
representative of a users intention  in general  we believe
that a combination of data sources to get high coverage as
well as representation 

as shown in figure    we first built the model with a single
feature and attempted optimizations through various feature
combinations as shown in figure    see appendix   for the
results   
in the case of individual features  the feature with both the
highest f measure          and highest precision
         was based on web search with query order
preserved  this is mostly related to a high coverage
available in a web search index  the feature with the
highest recall         was based on proximity score using
the wikipedia corpus  the high recall was due to the high
coverage achieved when looking at the distance between
the terms as a metric and not limiting to adjacent terms
only  however  while recall was highest in this feature 
precision was the lowest         as two terms in the same
document may be related but cannot always be considered
a relevant segment 

fi  future work
the current approach can potentially be improved
significantly by using a larger and more representative
document corpus for term pair and proximity features  one
approach could be to limit the training and feature data to a
particular domain or subject  eg  electronics or history  in
order to have a representative yet manageable corpus 

figure    feature combinations
combining features together resulted in improvements in
several different ways  limiting to features that preserve
query order resulted in a higher precision          and a
higher f measure          than features that dont
preserve query order  precision           f measure  
         this emphasizes the point that order matters in
user query segmentation  eg  new york is a relevant
segment but not york new  
due to the sparseness of data in the query logs  document
based features were superior to query based features  at
the same time  features computed using the wikipedia
document corpus had the lowest recall           this is
mostly due to limited data but also because wikipedia
corpus may not be representative of web search queries
which include a significant number of entertainment and
shopping related searches that are not captured by
wikipedia 
web search features had a higher f measure than other
features from other data sources due to the high coverage
   billion web pages vs      wikipedia articles  as well as
additional search engine features such as stopping 
stemming  synonyms and punctuation handling that a
typical web search engine provides to improve relevance 
since the wikipedia and query log based features were
computed using basic string matching without any
sophisticated normalization  this lowered recall as variations
of a term present in the corpus were not matched 

also  future experiments would need to normalize queries
and corpus using standard features such as stemming 
stopping  punctuation handling in order to increase recall 
other areas for future work include developing a model for
segments of any size  not just    as well as considering
non adjacent segments which could help towards query
reordering to increase precision  furthermore  developing a
segmentation model that takes into account query context
would be beneficial as effective segmentation can be
obtained by incorporating the search domain as well as
looking at other terms in the current query as well as
previous queries 

  acknowledgement
we would like to acknowledge shane bergsma  university
of alberta  for making available segmented aol query logs
used for training and testing 

   references
   tan and peng         unsupervised query
segmentation using generative language models and
wikipedia

   bergsma and wang         learning noun phrase
query segmentation  query and feature data used in
experiments

   wikipedia release version 
http   en wikipedia org wiki wikipedia release version

  
the best result  f measure           precision         
recall          was achieved by combining the web
search features with the proximity score feature based on
distance between terms in the wikipedia corpus  this
outperforms query segmentation algorithms using mi  fmeasure          as well as other language modeling
approaches shown by tan and peng        resulting in an
f measure of       and       with em optimization  our
result does underperform when compared with other more
sophisticated algorithms such as language modeling with
em optimization augmented with wikipedia knowledge   fmeasure          

alexa web search web service 
 http   aws amazon com alexawebsearch  

fi  

appendix


a  classificationresultsusingindividualfeatures


 
 
 
 
 
 
 

feature

recall

pmi using alexa web search  order matters
pmi using alexa web search  order does not matter
proximity score using wikipedia corpus
pmi  term pairs  using wikipedia  order doesnt
matter
pmi  term pairs  using wikipedia  order matters
pmi  term pairs  using aol query logs  order
matters
pmi  term pairs  using aol query logs  order doesnt
matter

precision

fmeasure

      
      
     
     

      
      
     
      

      
      
      
      

      
      

      
      

      
      

      

      

      

table    individual features

b  classificationresultsusingfeaturecombinations


features

recall

precision

fmeasure

 

all features that preserve order

      

      

      

 

all features that dont preserve order

      

      

      

 

all document based features

      

      

     

 

all query based features

      

      

      

 

search engine  alexa  based features

      

      

      

 
 

raw corpus  wikipedia  based features
best recall feature   best precision feature

      
      

      
     

      
      

 

top   features         from table  

     

      

      

table    feature combinations

fi