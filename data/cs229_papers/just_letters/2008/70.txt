predicting protein interactions with motifs
jessica long
chetan sharma
lekan wang

december         

 

background

proteins are essential to almost all living organisms  they are comprised of a long  tangled chain of amino
acids  of which there are twenty of the organic molecules  proteins also tend to fold into distinctive shapes
with certain idiosyncratic structures  with names such as    sheets    helix  and    hairpin  which we
typically call secondary structures  combinations of these structures give rise to motifs in proteins  and
some of these motifs have functional properties  such as mineral binding  protein protein interactions  and
enzymatic eects 

the property we will consider is protein protein interactions 

understanding which

proteins interact is an important part of understanding biological processes  as many bodily functions are
triggered by protein signaling pathwaysseries of proteins that interact  each causing the subsequent protein
to re fold in a slightly dierent way that allows it to interact with the next protein  and that protein will
re fold and interact with the next  and so on until the nal protein is activated to do something biologically
useful 

 

data

in our problem  we deal with two matrices 
the matrix indicating the presence of each motif in each protein which is a subset of

n

  number of proteins

m

  number of motifs

this matrix has values

aij    

if protein

i

interacts with protein

j 

b 

were extremely sparse matrices 

where

  otherwise 

we used a dataset of      proteins and      motifs  both the associations matrix 
matrix 

rmn  

a 

and the interaction

most proteins interact meaningfully with few other proteins 

and most proteins have few of the motifs  the proteins  on average  interact with      other proteins  and
contain       motifs  this made it dicult to nd a relevant subset of the data to reduce the features for
classication or to take repeated samples for bagging  a relevant set of proteins and motifs means that each
protein shares a motif with one othe protein  and also that the set of proteins have multiple interactions 

 

fithis diculty with nding a smaller relevant bootstrap sample  as well as the sheer size of the matrices
made our bagging algorithm require prohibitively large amounts of computation time 

 

boosted decision trees

finding a complex rule in classication is often extremely computationally dicult 

however  it is often

easier to nd many simple rules of thumb and let them work in tandem rather than nding a single complex
rule  this is the inspiration behind boosting  in which many simple rules are found in multiple rounds  and
in which the samples that are classied incorrectly in each round are then given more weight in the next
round 

   

basic boosting algorith

the basic boosting algorithm is as follows  schapire        
initialize
for

d   i      m

t              t  


train base learner using distribution



get base classier



choose an



update

dt

ht   x   

t   

dt    i   

dt  i  exp t yi ht  xi   
where
qt

qt

dt   to

normalizes

once trained  the classier can output prediction with

h x    sign

t
x

a distribution 

 
t ht  x 

t  
with the adaboost algorith  we choose

w



t  

 
 

ln



w 
wt



  where

w  is

the number of positive samples  and

is the number of negative samples 

since we are using boosted decision trees  we need to determine the relative position of the new node  we
will do this by nding

   


zi   w       w    w   

upper bound using

zpure

one of the innovative parts of our procedure is the idea of
algorithm  calculating

zi

runs in

o n   

zpure

 kunjade  anshul   as captured by our

time  where n is the number of samples  to make the algorithm

more ecient  we calculate a best possible

z

at each leaf  called

zpure   zpure is

the value that

when a particular rule classies all negative examples  or all positive examples correctly 
we know that 
    in the case where the rule classies all negative samples correctly 

w     w    w        w    w
 

zi

takes on

fi    in the case where the rule classies all positive samples correctly 

w     w    w   w     w   w     
so calculating

zpure

is computationally quite simple as compare to calculating all other values of z i 

ordinarily  it is necessary to calculate
the current

zmin

is lower than

zpure  

leaf cannot be less than our current

zmin

at every leaf to nd the absolute minimum

then we know that the lowest possible value of

zmin  

z

this means that we do not need to calculate

zmin  

however  if

at that particular

zmin

at the

zpure

leaf  thus saving computational time 

   

adaboost results

using our adaboost algorithm with     iterations  we found the training loss to be
loss to be

          

          

and the test

which means all but a few of the positive protein interactions were misclassied  in

each iteration  the optimal motif pair is chosen and its weighted classications added to the tree  in some
iterations  even the optimal motif pair results in a loss in training error because of the weightings of the
respective positive and negative labels 

in this case  the weight of the positive interactions for the motif

pair overweigh the negative interactions  and make the training loss increase  however  the error generally
decreases with iterations of the algorithm  however  due to the sparsity of the matrices  the model takes a
long time to converge  as each motif pair covers few protein combinations 
we ran the algorithm again with a reduced set of features  instead of using all      motifs  we used the    
motifs that occurred in two or more dierent proteins  and re classied with those  this required much less
computational power  and didn t signicantly increase the error  this is expected  as the boosted decision
tree would have only used the more commonly associated motif pairs to predict  resulting in the same motifs
being chosen  if we had the computational power to run many iterations  then the error would be dierent
as the algorithm reaches motif pairs that occur in fewer and fewer proteins 
the adaboost algorithm will likely work better if other features were used in addition to motifs  secondary
structures  surface charge  etc   or if we could nd a data set with more motif commonalities  more training data  in the form of more proteins or more motifs  would also help the boosted algorithm nd more
commonalities between the proteins on which to predict 
see gures at the end of the document for further information 

 
   

naive bayes
the algorithm

because boosting with so many rounds took a signicant amount of time  we also implemented naive bayes
as a way to get some quick results  given a distribution of whether or not two proteins interact  conditioned
on the the joint distribution of the two vectors of motifs associated with the two proteins  we have 

 

fip im 
   n 
  n  m 

p m 
   n im  
  n  p im  
  n 
p m 
   n im  
   n im  
  n        p m 
  n     
p m    n   im  
 p m
 
n
 
   im  
  n
  n      p mj   nk   im  
  n  p mj   nk  im  
  n      p im  
  n 
 p m    n   im  
  n        p m    n   im  
  n        p mj   nk  im  
  n        p mj   nk  im  
  n           

 
 

where

 m    j

and

 n    k  

the second line is from the naive bayes assumption that individual features

 the motifs  are independent 
we built up a matrix of counts  counting the presence of each motif in each pair of motifs present at each
interaction or non interaction  then  we take the margins of every row and column of counts  and  after
applying laplace smoothing  nd the joint distributions of all
simply calculate both

p im 
   n 
  n     m 

and

p im 
   n  
  n     m 

m 
   n  

and each

im  
  n 

then  to predict  we

and compare which one is larger  because the

products in both the numerator and denominator risk underow  we use the log probability 

log p im 
   n  
  n   m 

in the prediction phase 

   

results

because naive bayes required calculating over each motif pairing of each protein pair  the problem was

o m  n    

using the denitions of

m

and

n

given in section    with nearly      proteins and       motifs 

we couldn t eciently process the entire data set  so as in boosting  we selected some of the proteins and
motifs that had the least sparse data  and ran naive bayes over those data 
adaboost  with a

      train

were classied correctly 

error and a

      test

the result was similar to

error  so very few of the positive protein interactions

again  we attribute this to the overwhelming majority of non interactions over

interactions 

 

conclusions

in the end  we tried several dierent algorithms  but had limited success due to the sparseness of our data 
ideally  we would have liked to make the data more concentrated 

we tried to limit our data set  but it

was dicult to nd sparse subsets within the data and more dicult to predict whether these would be
representative subsets of proteins and motifs 
our primary algorithm was moderately successful  yielding a test loss that was almost half as much as random
chance  it s possible that running more iterations of the algorithm would have given us better results  but
throughout the rst fty iterations  our test loss and train loss remained completely stable  concerned by
the lack of updating on successive iterations  we turned to nave bayes and adaboost to give us a better
sense of how traditional machine learning algorithms handled our particular data set  unsurprisingly  the
results from both algorithms were fairly poor  given the small number of interactions on which to condition
future predictions 
we believe that better results could be attained in the future by using data with more of the protein
interactions already lled in 

 

fi 

references

jothi  raja  and przytycka  teresa m   computational approaches to predict protein protein and domaindomain interactions  national center for biotechnology information 
ng  andrew  cs     notes        http   www stanford edu class cs    materials html
schapire  robert e   the boosting approach to machine learning  an overview  nonlinear estimation
and classication  springer  att labs       
acknowledgment to anshul kundaje  postdoc under serm batzoglou and arend sidow  department of
computer science  stanford university 

 

fifull feature selection training error       motifs      iterations

full feature selection test error       motifs      iterations

fismaller feature selection training error      motifs     iterations

smaller feature selection test error      motifs     iterations

fi