recommendation system based on collaborative filtering
zheng wen
december         

 

introduction

recommendation system is a specific type of information filtering technique that attempts to present
information items  such as movies  music  web sites  news  that are likely of interest to the user 
it is of great importance for the success of e commerce and it industry nowadays  and gradually
gains popularity in various applications  e g  netflix project  google news  amazon   intuitively  a
recommendation system builds up a users profile based on his her past records  and compares it
with some reference characteristics  and seeks to predict the rating that a user would give to an
item he she had not yet evaluated  in most cases  the recommendation system corresponds to a
large scale data mining problem 
based on the choice of reference characteristics  a recommendation system could be based on
content based approach or collaborative filtering  cf  approach  see      or both  as their names
indicate  content based approach is based on the matching of user profile and some specific characteristics of an item  e g the occurrence of specific words in a document  while collaborative filtering
approach is a process of filtering information or pattern based on the collaboration of users  or the
similarity between items  in this project  we build a recommendation system based on multiple
collaborative filtering  cf  approaches and their mixture  using part of netflix project data as an
example 
the remaining part of this report is organized as follows  in section    we reformulate the netflix
project and use it to test the proposed algorithm in section    in section    we propose various
cf algorithms to solve this problem  the experimental results are demonstrated in section    we
conclude the current results and propose future work at last 

 

problem formulation

we use the netflix movie recommendation system as a specific example of recommendation system 
specifically  assume there are nu users and nm movies  given a set of training examples  i e  a set
of triples  user  movie  rating    define the user movie matrix a   nm nu as 
 

amu  

rm u   user us rating on movie m
 

if such rating exists
if no such rating

   

our task is to predict all the users ratings on all the movies based on the training set  i e  existent
ratings   in other words  we try to replace all the question marks in a by some optimal guesses 
in this problem  each rating is an integer between   and    and the goal is to minimize the rmse
 root mean square error  when predicting the ratings on a test set  which is of course unknown

 

fiimplementation
rmse

naive
around     

cinematch
      

top in the current leaderboard of the contest
      

table    performance of benchmarks
during the training phase   that is  to minimize
v
u
rmse   u
t

 

x

 stest    m u s
test

 rm u  pm u     

   

where  m  u   stest if user u rates movie m in the test set   stest   is its cardinality  rm u is the
true rating and pm u is the prediction based on the recommendation system 
due to the recommendation systems importance in improving service quality  netflix has started
a contest with a grand prize to attract the researchers worldwide to work on this problem  currently  many researchers  including many experts  are focusing on this problem and have made great
progress  the performances of some important implementations can serve as the measures  or
benchmarks  for the quality of different algorithms  as shown in table    where naive implementation corresponds to using a constant rating  such as the average of all the available ratings  
cinematch is netflixs original recommendation system  baseline  
due to the limited computation power of pc and matlab  we only use part of the available
data to build the recommendation system  specifically  we use a data set include        users  and
      movies 

 
   

collaborative filtering algorithms
item based k nearest neighbor  knn  algorithm

the first approach is the item based k nearest neighbor  knn  algorithm  its philosophy is as
follows  in order to determine the rating of user u on movie m  we can find other movies that are
similar to movie m  and based on user us ratings on those similar movies we infer his rating on
movie m  see     for more detail  in order to determine which movies are similar  we need to
define a similarity function  similarity metric   as in      we use the adjusted cosine similarity
between movie a and b 
p

sim a  b   

uu  a 

r
p

uu  a 

t

u  b 

t

u  b 

ra u  ru

ra u  ru



  p

rb u  ru

uu  a 

t

u  b 



rb u  ru

 

   

 

where ra u is user us rating on movie a  ru is user us average rating  u  a  is the set of users that
t
have rated movie a and hence u  a  u  b  is the set of users that have rated both movie a and b 
the advantage of the above defined adjusted cosine similarity over standard similarity is that the
differences in the rating scale between different users are taken into consideration 
as its name indicates  knn finds the nearest k neighbors of each movie under the abovedefined similarity function  and use the weighted means to predict the rating  for example  the
knn algorithm for movies leads to the following formula 
p

pm u  

jnuk  m  sim m  j rj u
jnuk  m   sim m  j  

p

 

 

   

fiwhere nuk  m     j   j belongs to the k most similar movies to movie m and user u has rated j  
and sim m  j  is the adjusted cosine similarity defined in      rj u are the existent ratings  of user
u on movie j  and pm u is the prediction 
it should be pointed out that there also exists a user based knn algorithm  however  in most
cases of the netflix project  its performance is poorer than item based knn algorithm  this is
because the user based data appear to be sparser  i e  it is very unlikely that a movie has only been
rated by   or   users  and highly possible that a user only rates   or   movies   in addition  since
typically there are much more users than movies  the user based knn also leads to a computational
challenge 

   

item based em algorithm

an alternative approach to solve this problem is item based em algorithm  in this algorithm  we
classify movies into g groups  and each movie m belongs to group g with probability qm  g   for
a given group g of movies  we assume the ratings of different users are independent and gaussian 
     the conditional independence is known as naive bayes
that is  p  ru m  m  g   n  g u   g u
assumption in machine learning literature and is widely used in many different applications  in this
project  intuitively  if we know movie   is a horror movie  then whether user   likes this movie
should be independent of whether user   likes this movie  hence their ratings on this movie are
conditionally independent  the gaussian assumption is assumed to simplify the calculation in the
m  step  the unboundedness issue related to the gaussian assumption can be solved by truncation 
to formulate the em algorithm formula in this case  let latent random variable gm  qm   
denotes the group of movie m  define u  m  as the set of users that have rated movie m  thus 
the e step formula is described as follows 
 
q t   
 g    p  g t   
  g ru m   g u   g u
 
m
m



 t 

 
qm  g uu  m   

g u

 

exp 

 t   
  
g   qm  g  uu  m   

p

g    u

 ru m g u   
 
 g u



exp 



 ru m g   u   
 g    u

 

   

where superscripts  t  and  t      are used to denote distributions of latent random variables in
different iterations  for m step  we need to solve
 

max

 
g u  g u

xx
m

g

 

 
 ru m  g u   
qm  g  log uu  m  
exp 
 
 g u
 g u

  

 

 log  qm  g    

which is equivalent to
 

max

 
g u  g u

x

x

qm  g 

m g

uu  m 

 

 ru m  g u   
log g u   
 
 
 g u

define m  u  as the set of movies user u has rated  by changing the order of summation and a little
bit of algebra  we have
p

g u  

mm  u  qm  g ru m

p

mm  u  qm  g 

 

fip
 
g u

 

 g u   
mm  u  qm  g 

mm  u  qm  g   ru m

p

   

we repeat e step and m step until convergence  and the final prediction is given by
pu m  

x

qm  g g u  

g

similarly  there also exists a user based em algorithm  due to the reasons discussed in the
previous subsection about the advantage of item based knn over user based knn  for this project 
the item based em will work better than user based em in most cases 

   

sparse svd algorithm

another algorithm to solve this problem is based upon sparse matrix svd  this approach models
both users and movies by giving them coordinates in a low dimensional feature space i e  each user
and each movie has a feature vector  and each rating  known or unknown  is modeled as the inner
product of the corresponding user and movie feature vectors  in other words  we assume there exist
a small number of  unknown  factors that determine  or dominate  ratings  and try to determine
the values  instead of their meanings  of these factors based on training data  mathematically 
based on the training data  sparse data of a huge matrix   we try to find a low rank approximation
of the user movie matrix a  this approach is called sparse svd algorithm 
let ui   nf   i              nu be all the users feature vectors  and mj   nf   j              nm
be all the movies feature vectors  this problem can be formulated as the following optimization
problem 

min

ui  mj

x



x
x
 ri j  ui t mj        nui kui k   
nmj kmj k    
i

 i j i

   

j

where i     i  j  if user i has rated movie j   nui is the number of movies user i has rated  and nmj
is the number of users that have rated movie j  we notice the weighted l  regularization term is
added to avoid potential overfitting 
one should notice that to solve     directly is nontrivial  as has been proposed in      this
problem can be solved as follows  first  we fix the users feature vectors ui   and solve for the movies
feature vectors mj   then we fix movies feature vectors mj   and solve for users feature vectors ui  
we repeat this process until convergence  notice in each step  we are solving a regularized least
square problem  for which efficient algorithms exist and are handy to use 

   

tricks in postprocessing

in addition to the collaborative filtering algorithms discussed in the previous subsections  multiple
tricks in data postprocessing could also be applied simultaneously to enhance the performance
of the recommendation system  some of the tricks we have used in our implementation include
newcomer prediction  prediction truncation  item based correction and near integer round off 

 

finewcomer prediction
when there exists a newcomer  i e  a user without any existent ratings  it is very difficult to predict
his her rating on any item  in order to minimize the rmse in this case  we use the item mean as
his her prediction  that is  the prediction of user us rating on movie m is the average value of
existent ratings on movie m given by other users 
pm u  

x
 
rm u   
 u  m   u  u  m 

as defined above  u  m  is the set of users that have rated movie m 
prediction truncation
for some algorithms such as sparse svd  it is possible that the prediction pm u is above   or
below    in this case  we truncate the prediction  that is  we set pm u     if pm u     and
pm u     if pm u      of course  prediction truncation will strictly improve the performance of the
recommendation system and reduce the rmse 
item based correction
the third and most important trick is item base correction  specifically  define the item based
rating mean as
sm  

x
 
rm u   
 u  m   u  u  m 

where u  m  is defined above  for a given collaborative filtering algorithm  define the item based
prediction mean as
sm  

  x
pm u   
nu u 

where nu is the number of users  we correct the prediction by m   sm  sm   that is
pm u    pm u   m  

   

the above equation gives the formula for item based correction  based on our experiment  the
item based correction improves the rmse by about      
near integer round off
for netflix project  one interesting question is that whether rounding off the prediction will reduce
the rmse  while reducing the errors of some predictions to    this approach will also increase the
errors of other predictions  in practice  it is observed that in most cases the naive round off will
increase the rmse 
in this project  we use an approach called near integer round off  specifically  we round off
the prediction if it is close enough to an integer  during the implementation  we round off the
prediction if its distance to the nearest integer is less than or equal to      it has been observed
that this approach can improve the performance of the recommendation system 
 

fisequence of implementation
in our implementation  given predictions as outcomes of some collaborative filtering algorithms  we
further enhance the performance by first applying the newcomer prediction algorithm  then carry
out the item based correction  then prediction truncation  and at last near integer round off  it has
been observed that with this sequence of implementation of postprocessing tricks  the rmse of the
recommendation system can be reduced by approximate      

   

parameter adjustment and algorithm mixture

as is classical in machine learning and collaborative filtering  we need to adjust the parameters
in order to achieve the optimal performance of the recommendation system  for example  in the
item based knn algorithm  we need to adjust the number of neighbors  i e  k   in the item based
em algorithm  we need to adjust the number of groups of movies  in the sparse svd algorithm  we
need to adjust the dimension of the feature vectors and the regularization weight  all the parameter
adjustment is carried out through k fold cross validation 
it is also noticed that as expected  blending the algorithms could improve the performance 
for blending  we mean that we use the convex combination of the predictions from different colkn n is the prediction
laborative filtering algorithms as the final predictions  for example  assume pm u
sv d is the prediction from the sparse svd algorithm 
from the item based knn algorithm  and pm u
then the final prediction is
f inal
sv d
kn n
pm u
  pm u
       pm u
 

   

where          is the weight of sparse svd algorithm in the blending 

   

our new contribution

it should be noticed that most of the algorithms used in this project have been presented in previous
literatures  e g          in this field  to the best of our knowledge  our new contributions include 
   we use the item based correction and near integer roundoff in both sparse svd and em
algorithm 
   we change the em algorithm from user based to item based 

 

experimental results

we carry out the algorithms proposed in section   on part of netflix data with         users
and        movies  for each cf algorithm and their mixture  we use the postprocessing tricks
presented above to enhance the performance and use k fold cross validation to choose the best
parameters  models   in particular  we choose k      in our implementation  the results are
listed as follows 

   

item based knn algorithm

for the item based knn algorithm  we use the approach described in subsection      the crossvalidation rmse is shown in figure    from the cross validation result  we notice the optimal
number of nearest neighbors is k        with this particular choice of k  the rmse of test data
is        
 

fifigure    cross validation for item based knn algorithm
although the performance of the item based knn algorithm is not as good as alternative algorithms  such as the sparse svd and item based em algorithm  it does reach a performance similar
to cinematch  in addition  the prediction results of the knn algorithm can be used to initialize
other algorithms  which will usually lead to a much better performance and faster convergence than
random initialization 

   

item based em algorithm

for the item based em algorithm  we apply the algorithm described in subsection     and use the
cross validation approach to choose the optimal number of groups of movies  the cross validation
indicates the optimal number of groups is     and the resulting rmse for test data is        
during the experiment  we notice that item based em algorithm has a better performance than
user based em algorithm  however  its performance is poorer than the sparse svd algorithm 

   

sparse svd algorithm

for the sparse svd algorithm  we implement the algorithm as described in subsection      the
cross validation rmse for different feature vector dimensions nf and l  regularization weights  is
shown in figure     and    from the cross validation  we notice the optimal feature space dimension
is nf       and the associated optimal weight is           with these parameters  the rmse on
test data is         
we notice that sparse svd has a better performance than item based knn algorithm and itembased em algorithm  this is primarily due to      the sparse svd simultaneously treat both user
and item features  while other algorithms always concentrate on one aspect  hence can be classified
as user based or item based       l  regularization is implemented to avoid overfitting 

 

fifigure    cross validation for sparse svd algorithm  nf               

figure    cross validation for sparse svd algorithm  nf                      

 

fifigure    cross validation for sparse svd algorithm nf                     
algorithm
rmse

item based knn
      

item based em
      

sparse svd
       

blending of item based em and sparse svd
      

table    performance of different collaborative filtering algorithms

   

blend item based knn and sparse svd

at last  we blend the predictions of item based knn and sparse svd  as described in equation
     we use the cross validation to choose the optimal weight   and the cross validation rmse is
shown in figure    from the cross validation  the optimal blending weight is          with this
optimal weight  the rmse for test data is        
in summary  the performance of different algorithms are illustrated in table   

 

conclusion and future work

in this project  we present several collaborative filtering algorithms for recommendation system and
test the performance of each algorithm and their mixtures on part of the netflix data  at last 
a rmse of        is achieved  which corresponds to a       improvement of the performance of
cinematch  baseline  
as to the future work  we plan to
   try other important algorithms on this project  such as bayes network 
   work more on the algorithm blending  since different algorithms might characterize different aspects of the problem  and cooperation between multiple algorithms should lead to a
better performance  of course  model selection approaches such as cross validation will be
widely used in this scenario 

 

fifigure    cross validation for blending weight of item based knn and sparse svd

acknowledgment
we are grateful for prof  ng and tas help during this course and project  we also thank mr 
robbie yan for helping us get started on this project 

references
    j  breese  d  heckerman and c  kadie  empirical analysis of predictive algorithms for collaborative filtering  technical report of microsoft research       
    b  sarwar  g  karypis  j  konstan and john riedl  item based collaborative filtering recommendation algorithms  proceedings of the   th international conference on world wide
web               
    y  zhou  d  wilkinson  r  schreiber  r  pan  large scale parallel collaborative filtering for
the netflix prize  aaim               
    andrew ng  cs    lecture notes 

  

fi