automatic playlist generation
xingting gong and xu chen
stanford university
gongx stanford edu
xchen   stanford edu

i 

introduction

digital music applications have become an increasingly
popular means of listening to music  applications such
as spotify allows the user to add songs to his her
playlist without downloading the song to his her computer  the user can also be recommended songs by
spotify through spotifys discover  option  pandora
an online radiogenerates a radio station based on
a single user inputted artist  genre  or composer  for
these types of applications  applying algorithms to
learn user preferences is extremely important  in this
report  we explore two different methods to generate
automatic playlists based on user input 
   gaussian process regression  gpr  
this method takes in a set of seed songs  which can
contain as little as a single song  inputted by the user to
train a preference function that predicts the user preference for a new song to be considered for the playlist 
   svm   hmm 
for this method  we assume that the user has generated a large number of seed songs  i e  a user has liked
hundreds of songs are pandora throughout the course
of a year   for this method we also require a set of
low preference songs  i e  the user has skipped hundreds of songs on pandora over a year   with a large
training set of labelled data  we can apply classification
algorithms such as svm to determine if a new song
will be liked or disliked by the user 
because we believe timbre to be an important predictor for music  we combine the svm with an hmm to
model the timbre spectra 
these methods are described in much greater detail in
section iv 

ii 

related work

automatic playlist generation can be a difficult task
due to the fact that the user will often provide only
a few seed songs  with such a small training set  it
can be difficult to train a sensible similarity metric  a
paper by zheng et  al  at microsoft corporation devised a novel  kernel meta training   kmt  method to

mitigate the problem of a small training set  instead of
designing a machine learning problem to train only on
the user provided seeds  the microsoft group gathered
        songs from        albums as a kernel  meta training set  the idea is that songs placed on the same
album are similar  and so it is appropriate to train the
similarity metric on the set of         songs  however 
their selection of musical features were mostly qualitative  and consisted of  genre  i e  jazz  rap   subgenera
 i e  heavy metal   mood  i e  angry  happy   style  i e 
east coast rap  gangsta rap    rhythm type  i e swing 
disco   rhythmic description  i e  funky  lazy   and vocal code  i e  duet  instrumental   we felt that some of
these features were not very well defined and seemed
redundant in the musical qualities they were trying to
capture  the example features classified under genre 
subgenera  and style  for example  seem extremely interchangeable amongst the   categories  for our first
method using gpr  we aim to expand upon the microsoft groups work by applying kmt to data from the
million song dataset  described in section iii   briefly 
the million song dataset provides quantitative features
such as tempo in beats per minute  or loudness in decibels  which we will instead use as features to train the
similarity metric 
our second method falls into the more standard category of binary classification problems  more notably 
we want to expand upon a standard svm by modeling
the timbre sequence as a hmm  there has been a lot
of research on using timbre to analyze complex instrumental textures and even rhythmic styles        due
to the importance of timbre in characterizing sound 
we believe that an svm armed with hmm can be an
effective classifier on large training sets 

iii 

dataset and features

we obtained our data from the million song dataset  a
dataset compiled by columbia universitys laboratory
for the recognition and organization of speech and
audio and the echo nest  the entirety of this dataset
consists of audio features and metadata for a million
popular songs  for the sake of practicality  we down 

filoaded the        song subset 
the dataset contains approximately    features per
track  of this set we hand selected a subset of   features upon which to perform our analysis 
   genre  each track can consist of anywhere from  
to multiple user supplied genre tags from the musicbrainz website  to keep our analysis simple  we
randomly assigned each track to one its genre tags 
each track is therefore labelled by an integer that
corresponds to a particular genre 
   tempo  the estimated tempo in bmp  to discretize
this feature  we binned the tempo values by increments of    
   loudness  the average loudness of the song in db 
we binned these values by increments of   db 
   decade  we included the decade in which the song
was released as a feature  the motivation behind
this is that songs produced in the same decade
sound similar in style 
   timbre  timbre is represented as a     n matrix
of mel frequency cepstral coefficients  where n is
the number of segments  each column of the matrix
is thus a    dimensional vector representing the   
mfccs of a particular time segment  we processed
timbre differently for each of our two methods 
gpr  for this method  we limited ourselves to tracks
with n      and randomly selected     rows of
the timbre matrix for these tracks 
svm   hmm  since each column of the timbre matrix is obtained from a different time segment  it
makes more sense to represent the timbre matrix as
a hidden markov model  we trained an hmm for
each of the two song sets representing songs  liked 
and  disliked  by the user  i e  added or not added
to the users spotify playlist from the applications
list of recommendations   the loglikelihoods of each
track given each of the hmm models are then used
as features for the svm 

iv 

gaussian process regression  the first of our main
methods makes use of gaussian process regression  in
a gaussian process  gp   any finite subset of the points
in our domain space satisfies a multivariate gaussian
distribution  for automatic playlist generation  our
domain space consists of the possible user preferences
f for some song which we wish to predict  to simplify
calculations  the mean of the gp is often assumed to be
   which makes sense in our case since it is reasonable
to assume that in the space of all songs  a user will
probably not want to listen to most of them 
let seedsongs    xi  in   be the set of user inputted
songs that serve as the  seed  for which we will generate a playlist around  where xi denotes the feature
vector for seed song i  let f i denote the true user preference for these songs  though f i can in principle take
on any real value  for simplicity we assume it is approximately   with noise  if the user selects the song as a
seed   let f  denote the user preference for some song
x that we want to predict  then the joint distribution
  f i   f    is gaussian 


fi
  n     k  
   
f
where k is the covariance matrix 
since   f i   f    is jointly gaussian  the conditional distribution p  f    f i   is therefore also a gaussian with
parameters 

features

example raw values

discretized processed values

genre

rock  indie  pop  hip hop
country  jazz  metal  folk
rap  dance
                        
                
                        
    n matrix of mfccs
    n matrix of mfccs

integer values   to  

tempo
year
avg  loudness
timbre  gpr 
timbre  svm hm 

 

       
             
          
randomly selected     rows
loglikelihoods under each hmm

p  f    f i    n     

  a 

   k   xi   x    k   xi   xi   f i

  b 

   k   x   x    k   x  x  k   xi   xi  

 

k   x    xi  

  c 

the preference function f  is then obtained by taking the posterior mean of this conditional distribution 
resulting in 
n

 i k   xi   x   

  a 

  k xi   x j       ij   

  b 

f  

i   
n

i  
table    feature vector examples

methods

j   

lastly  since  is the noise in our gp  it is obtained by
maximizing the log likelihood of obtaining the set of
seed songs 
 
 
n
log p  f        f t k   f  log  k    log      
 
 
 
thus  in order to learn the preference function we must
obtain a kernel k   x  y  that will serve as our similarity metric between two songs x and y  once we have

fithe preference function  selecting a playlist becomes as
easy as computing the preference of each song under
consideration and ranking the top m 
for our project we tried two kernels  the first of
which must be learned  a method called  kernel meta
training   and the second is a simple hamming kernel
to compare our results with 
kernel meta training  kmt   the idea behind kmt
is to use pre defined playlists to learn a similarity metric k between any two songs  in our case  we separated
the        songs from the million song subset into a
kernel meta training set  a seed set  and a test set  the
test set consists of the set of  new  songs that will be
considered for the playlist generation  the kmt set
constitutes the largest fraction of our dataset and is
divided further into subsets based on some feature in
order to train k  in our case  we chose to paritition
the kmt set by genre  in contrast  the seed set is
usually very small  containing as little as one song 
thus  the advantage to kernel meta training is that we
can effectively meta train on a much larger sample 
allowing us to obtain a more accurate similarity metric 
the kernel we chose to learn was inspired by the
one used in zheng et  al       and defined as follows 
n

k   x  y   

 n n  x  y 

   

n   

where n is a family of mercer kernels indexed by n 
and n is defined later  in order to describe n in the
least confusing manner  we first make an observation
about the timbre feature  for the first   features  a
direct comparison can be made between any of these
features  i e  two songs have similar tempos if they fall
within the same increment of    bpm   however  the
timbre feature was processed into a still fairly large
matrix of dimensions          to compare two timbre
features x  and y    we take the frobeniums norm of the
difference between the two timbre matrices 
norm  x    y      frobenius norm  x   y   

   

two timbre matrices are then declared similar if their
norm is below a threshold value  which we computed
by taking the average norm of all tuples in the premade playlists  now we are ready to define n  

   if anl     or xl   yl l     and
norm  x    y      threshold
n   x  y   

 
otherwise
   
where the vector a is the binary representation of the
integer n  to understand this more intuitively  a can be

thought of as a mask that allows us to compare a subset
of features at a time  in other words  n evaluates to  
only when the components of x and y are exactly equal
 or less than a threshold  whenever the component of a
is    since we have   features  we therefore have a total
of    subsets of features to consider  and so n ranges
from   to n       
finally  to train k we want to solve for the coefficients  n   we do so by minimizing the cost function 
n


  n
 n   arg min   kij    n n   xi   x j    
  i j  
n   

   

where kij is the empirical covariance given by
 
kij  

 
  genres

 

 

 
 

if songs xi   x j are in same genre
otherwise
   

generating the seed  test  and kmt sets 
our kmt set consists of       randomly picked songs
from the million song subset  to generate a seed set
of size n from the remaining       songs  we first randomly selected a single track to serve as the initial seed 
we then compared its features to other songs in the remaining set  if the initial seed song s  either shares an
artist with a second song s  has   identical  non timbre
features  or has   identical non timbre features and the
timbre features are  similar   i e  norm   threshold  
then s  and s are declared  similar   however  in order
to accurately mimic user behavior  we also introduce
randomness into our seed generation  for every song
s we compare to s    we also generate a random real
number random          if s and s  are similar and
random       s is added to the seed set  if s and s 
are dissimilar and random        s is also added to
the seed set  finally  the remaining         n songs
comprise the test set 
hamming kernel  to compare the results we obtain with the kmt method  we also applied a simple
hamming kernel  no training required  to gpr 
 

kham   x  y   

    xi   yi        norm x    y      threshold 

i   

    
this kernel simply computes the number of matches
between the feature vectors  and for timbre  it compares whether the two matrices are similar  

 

fisvm   hmm  our desire to give a more reasonable treatment to the timbre feature led us to explore
a combined svm and hmm model  in the case of
having a very small seed set  training an hmm on a set
that can contain as little as a single song does not make
much sense  however  it is reasonable to suppose that
over time a user may have accumulated a large playlist
of preferential songs and also rejects other songs in the
process  i e  from a recommended playlist on spotify 
the user only chooses a fraction of the songs  or the
user either likes or skips a song on pandora   in this
case we have two training sets  a set of  likes  and a
set of  dislikes   we can then train an hmm on each
of these sets and the log likelihood of a training song
under each hmm model become the timbre features
for the svm 
generating the training sets  in order to apply svm
we must first separate our training data into two sets 
one with the label  like  and the other with  dislike  
we denote these sets as s l and sd   respectively 
to obtain these playlists  we selected an initial seed
song s  and compared its features to other songs in
the dataset  if a song s has   non timbre features in
common with s    then s is similar to s    we also added
the same random component as with generating the
seed set for gpr  thus the same rules apply for adding
s to s l or sd as for gpr 
pre processing timbre  to make the size of the timbre
matrices uniform across all training songs  we averaged
consecutive columns to obtain a resulting  compressed 
timbre matrix of dimension          i e  if the original
track had n       segments  then the first     columns
of the new matrix will consist of averaging      int     
   
consecutive columns  the last column will contain the
average of the last    columns 
gaussian hmm  since mfccs do not take on discrete
values  we model the emission probability at each state
by a multivariate gaussian  where  has dimensions
      and the covariance matrix has dimension        
we used a hmm toolkit for matlab to train the
hmm      we experimented with different numbers of
hidden states  and picked the state numbers lhidden    
and dhidden     that maximized the log likelihoods of
the training sets s l and sd   we then obtained the
timbre features for a song x by computing the log
likelihoods log p  x   hmml   and log p  x   hmmd   

v 

results

gpr  we tested both the kmt and hamming kernel
on two seed sets  one containing a single seed and the
other containing   seeds  the results are tabulated
 

below 
table    top   songs for gpr   kmt using   seed
seed

playlist

title

artist

genre

decade

tempo

loudness

house of pain

van halen

rock

    

   

   

tell your momma come
october
goin to the river
dont waste my time
on the road again

black eyed peas
u 
alice cooper
john mayall
the sonics

pop
rock
rock
pop
pop

    
    
    
    
    

   
  
   
   
   

  
  
   
   
   

table    top   songs for gpr   kmt using   seeds

seeds

playlist

title

artist

genre

decade

tempo

loudness

house of pain
harajuku girls
ease

van halen
gwen stefani
public image ltd

rock
pop
rock

    
    
    

   
   
   

   
  
   

performance
laser show
looking for a kiss
the end
till we aint strangers anymore

happy mondays
fountains of wayne
sex pistols
my chemical romance
bon jovi

rock
rock
rock
pop
metal

    
    
    
    
    

   
   
   
  
   

  
  
  
  
  

table    top   songs for gpr   hamming using   seed
seed

playlist

title

artist

genre

decade

tempo

loudness

house of pain

van halen

rock

    

   

   

out of this world
dont waste my time
stumble
still a fool
goin to the river

black flag
john mayall
r e m 
groundhogs
alice cooper

rock
pop
rock
pop
rock

    
    
    
    
    

   
   
   
   
   

   
   
   
   
   

table    top   songs for gpr   hamming using   seeds

seeds

playlist

title

artist

genre

decade

tempo

loudness

house of pain
harajuku girls
ease

van halen
gwen stefani
public image ltd

rock
pop
rock

    
    
    

   
   
   

   
  
   

you
start me up
true nature
armageddons raid
final straw

radiohead
the rolling stones
janes addiction
belphegor
r e m 

rock
pop
rock
metal
rock

    
    
    
    
    

   
   
  
   
   

   
 
 
 
  

svm   hmm in order to generates results with
svm hmm that can be comparable to those using
gpr  we generated the sets s l and sd based on the
same initial seed s  used to generate the seed sets for
gpr  thus  although s l constitutes a much larger set 
the idea is that most of the songs in s l are  similar  to
s    and therefore to all the songs in the seed sets 
we trained our svm and hmm on a set s l of
    songs and a set sd of     songs  we then applied our trained svm   hmm model to classify
the top    ranking songs in each of the four gpr
runs  gpr kmt and gpr hamming with   seed 
gpr kmt and gpr hamming with   seeds   omitting overlaps  this meant testing on a set of    songs 
of these  only   were labelled as  like   and these songs
are listed in the table below 
table    classification using svm hmm

fititle

artist

genre

decade

tempo

loudness

performance
darling  i want to destroy you
stumble

happy mondays
afi
r e m 

rock
rock
rock

    
    
    

   
   
   

  
 
   

vi 

discussion

gpr  to compare the performance of different learning algorithms  we used standard collaborative filtering
metric to score the generated playlist in each trial  the
score of the playlist from trial j is defined as 
nj

rj  

tij

    i           

    

i   

where tij     if ith song in the generated playlist is
in pre made playlist in trial j    otherwise  beta determines how fast user interest decays  and is set to     nj
is the number of songs in the generated playlist  the
score is then summed and normalized as
    

r      



j   

rj

     



rmax
j

hamming kernel  on the other hand  treating songs
from the same artist as pre defined playlists results in
much better predictions of the user prefenrece  based
on the observation above  kmt works well only with
well designed pre defined playlists  and can constantly
outperform gpr hamming 
svm   hmm  although we did not have enough time
to write a scoring algorithm for svm  we note that   of
the   songs in table   were ranked at top    by gpr 
kmt and gpr  hamming  thus the two methods do
appear to overlap in terms of which songs they predict
the user might like  in general  however  svm and gpr
are very different approaches to playlist generation and
therefore difficult to compare  gpr is good for very
few seeds  but has an extremely simplistic way of dealing with timbre  svm depends on their being two large
training sets s l and sd   allowing us to train an hmm
for timbre for each of these sets  thus each method has
their own unique advantages and determining which
method is best depends on the situation 

    

vii 

j   

where rmax
is the perfect score in trial j  r      
j
corresponds to perfect prediction and larger r values
indicate better performance 
figure    histogram of scores of the various methods

conclusion future work

there are a lot of interesting avenues to explore from
here  we could  for example  train our svm   hmm
model using the more intricate kernel defined in equation   rather than the linear kernel which we used for
this project  we could also expand upon the feature set
by including lyrical featuresi e  judging the meaning content of the music rather than only its acoustic
features 

references

all variations of gpr significantly outperform the
randomly generated playlist  however  gpr kmt
does not consistently win gpr hamming  how we
make the pre defined playlists is important  treating all songs in the same genre as similar tends to
over generalize and results in worse predictions than

   

zheng et  al          learning a gaussian process
prior for automatically generating music playlists

   

sandler et  al          timbre models for analysis
and retrieval of music signals
ieee transactions on multimedia  vol    no   

   

zhang  x   ras  z   dardzinska  a          discriminant feature analysis for music timbre recognition and automatic indexing

   

platt  j  c          auto playlist generation with
multiple seed songs

   

ebden  m          gaussian processes for regression  a quick introduction

   

murphy  k          hidden markov model  hmm 
toolbox for matlab

 

fi