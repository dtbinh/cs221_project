gpi contrast prediction  december       cs     final project

predicting contrast performance for
the gemini planet imager
victoria borish  katherine sytwu  jean baptiste ruffio
stanford university
vborish stanford edu ksytwu stanford edu jruffio stanford edu
project ta  junjie qin jqin stanford edu

i 

introduction

the gemini planet imager  gpi  is a new
exoplanet hunting instrument mounted on the
gemini south telescope in chile  exoplanets 
or planets orbiting stars other than the sun 
are detected by masking out the light of the
star at the center of the image  allowing one to
detect objects in its neighborhood  detection
of orbiting planets is often inhibited by noise
coming from the turbulence in the atmosphere 
which allows light to escape the mask  the
instrument tries to actively correct for these
perturbations but it is still imperfect  making
it difficult to detect the faint exoplanets  there
are nights when the atmospheric conditions are
such that it is impossible to obtain any usable
data  so trying to observe then is a waste of
valuable telescope time  there currently is no
reliable method to predict the performance of
the instrument   
a quantitative measure of whether or not
an exoplanet is detectable is given by the contrast  or the ratio of the brightness of the exoplanet to that of the star it orbits  the target
variable for each image is the threshold for the
lowest possible contrast of a planet that could
be viewed on a given night  therefore  we
want these values to be as low as possible to
allow for viewing of faint exoplanets 
we can approach our prediction in two
ways  one way is to predict a numerical contrast value using regression and then allow the
researchers to decide whether it is worth observing or not  in this case  we decided to use

locally weighted linear regression  lwlr  to
predict the actual contrast value  the other
way is to treat this as a classification problem
by predicting if an image will be good or bad
on a given night  for this  we used a support vector machine  svm  algorithm with a
gaussian kernel to solve for a target labeled as
either good or bad  in both cases the features
are the environmental variables recorded by
the telescope for each observation    it includes
variables such as but not limited to the star
brightness  wind direction and turbulence 
the advantage of predicting a numerical
contrast value is that it also provides us with a
typical performance for the instrument under
current conditions  this information can be
used to determine if the instrument is doing
better or worse than usual in similar conditions  the classification approach might seem
less powerful  but it has the advantage of being
a simpler problem and therefore may give a
more accurate answer 
previously  our collaborators have examined correlations between the contrast and individual features  determining that performance
is mainly driven by the star brightness and the
adaptive optics wavefront error            however  a prediction approach utilizing all of the
features in a multi dimensional problem has
not been implemented or studied 
in section ii  we describe our data set and
preprocessing steps  then in sections iii and
iv  we describe our prediction methods and
results for a classification and regression problem  respectively  finally  in section v  we

  there exist indicators of the current observing conditions but they are not trusted for gpi  indeed it is not clear what
drives gpi performance because gpi utilizes active correction of the turbulence 
  the variables are saved as metadata in each image 

 

figpi contrast prediction  december       cs     final project

iii 

compare both methods  and in vi  we present
ideas for future work on the project 

ii 

training sets

our training set was obtained from the gpi
database    after filtering out training examples with missing data  two types of training
sets were extracted  the first raw data  contains      training examples in which an example corresponds to a single    second exposure  the second training set  processed data 
considers processed images  images resulting
from combining many single exposures  and
consists of only     training examples  for the
latter  both the mean and the standard deviation of the raw features define the new set
of features  the processed data represents the
data from the astronomers final images  and
thus is what they actually care about  even
though it yields a much smaller training set 
we considered the following features   o  
and      refer to the raw and processed dataset
respectively  
 the seeingo    a measure of the fuzziness
of the image 
 the characteristic time scaleo of the turbulence 
 the airmasso    the distance travelled by
a light ray through the earths atmosphere 
 the wind directiono  and its standard
deviation   
 the wind velocityo  and its standard
deviation   
 the wavefront erroro    a measure of the
perturbation of the incoming light  and
its standard deviation   
 the apparent magnitudeo    the star
brightness using a logarithmic scale 
the previous features were suggested as potentially interesting by our collaborators  then
a forward search  described in section iii was
applied to extract the optimal subset of these
features for the given training set 

classification

for the classification problem  we first need to
set a threshold for the contrast  above which
we know the conditions wont allow for a
research quality image  from previous observations  we know that this threshold lies
around          this choice led to a skewed
dataset with around     of the training examples classified as  good  
we decided to use a    soft margin support vector machine which calculates a decision boundary whose shape is determined by
a kernel and a small number of training examples close to the boundary  since our data is
not linearly  or polynomially  seperable due
to the high amount of noise  we chose a soft
margin algorithm  given by equation   
min
w b

s t 

m
 
k w k    c   i
 
i   

 i  
t  i  
y w x   b     i  i           m

 i     i           m
   
here  w and b are parameters that define the
decision boundary    x  i    y i    are the m training examples where x is the feature vector and
y the target  the   i   are error parameters that
allow some of the data points to be slightly
across the decision boundary  something that
is necessary with our noisy data  the penalty
coefficient  c  accounts for the noise in the data
by determining the relative weight given to
each of the two objectives  thus c is related to
the number of training examples we allow the
model to classify wrong 
the first three parameters are computed by
the svm  but c is chosen before the svm is
run  we optimized over c by running the svm
on the training set with various values of c
ranging from     to     for each value of c  we
averaged over many runs  each with a different
random test set  and the remaining data as the
training set   we then chose the value of c that
minimized the average generalization error 

  the features and targets were obtained either through a sql query to the database or by reading manually the images
metadata 

 

figpi contrast prediction  december       cs     final project

in addition to optimizing c  we needed
to optimize the features we used to train the
model  we used forward feature selection  beginning by iterating through the features and
training the data on a set consisting of only
a single feature  we then chose the feature
that produced the smallest generalization error
and added that to the list of features to keep 
continuing this for another iteration through
the features meant we trained the data on sets
consisting of all pairs of features in which one
of the features was the good feature found in
the first loop  we repeated this process until
we had gone through all the features  adding
them one by one  and plotted the generalization error vs  feature  for each run of the svm 
we optimized over the c value 
figure   shows the result of the forward
feature selection  where we decided to keep
the features selected until the generalization
error began to increase  this left us with an
optimum training set consisting of   features 
star magnitude  seeing  wavefront error  and
wind direction 

figure    forward seach feature selection for the processed data 

we chose a svm because its one of the best
off the shelf classifiers which would give us
a decent preliminary result  using pythons
scikit learn     library  we ran our svm with
a gaussian kernel because it allows for nonlinear decision boundaries and high flexibility 
since our features have unknown relationships
to the contrast  the gaussian kernel would not
make any strong assumptions 
we divided our processed  raw  data set

into a test set of          examples and a training set of variable size from                  
examples  we then studied how generalization
and training error depend on training set size
to better understand our model and error  figure   shows both the test and training error for
the svm as a function of training set size for
the processed data 

figure    generalization and training errors of the processed data for the svm vs  training set size 

in figure    we see that the generalization
error has not quite leveled off at the largest
available training set size  this leads us to believe that the best generalization error        
could still be improved upon with a larger set
of data  the training error  however  begins to
level off at a value of around      the asymptote of this being non zero implies that this
svm cannot perfectly classify our entire data
set  we plot here the error for the processed
data set because that is ultimately what is important to the astronomers  however  when
we initially tested the data on the raw data
set  we found an error as low as        our
error is still larger than we would like it to be 
but lower than the error achieved when using
regression  see next section  
to understand our error  we calculated the
confusion matrix  see figure    and found that
svm did extremely well for the good contrast
values  classifying     correctly for the processed data   but had a large percentage of
false positives        the raw data did better
on classifying the bad nights  and understanding this and trying to correct for it is still an
open problem 
 

figpi contrast prediction  december       cs     final project

figure    confusion matrix for svm  left  processed
training set  right  raw training set     
      of the good nights were correctly classified as opposed to only           of the bad
nights for the processed  raw  case 

iv 

regression

since our data is extremely noisy  linear regression did very poorly and did not improve
upon the addition of higher and lower power
features  instead  we want a model that is just
as flexible as svm and makes few assumptions
about our data  locally weighted linear regression satisfies both  as it only assumes two
things  local linear dependence of the features
and little dependence on training examples far
away from the test example in feature space 
we decided to use gaussian weights for our
model for the same reasons as stated for the
svm model  for any test example x  the weight
of each training example x  i  is given by
wi   exp  k x  x  i  k 



   

where  is a parameter we choose that controls the relative weighting of nearby training
examples 
we can directly solve for the predicted contrast y given the test example x using a modified normal equation 
y  



x t wx

  

x t wy

t

x

   

where x is a mxn matrix of the training examples  n is the number of features   w is a
diagonal matrix with the weights defined in
equation    and y is a vector of the training
target values 
 

we split up the data into the test and training sets as described in section iii and determined the optimal  value in the same way
that we determined c in the svm model  we
then similarly compared the generalization and
training error as a function of training set size 
since our contrast values are small  we calculated percentage error  and found that lwlr 
on average  with the processed data set gives a
     error in the contrast value 

figure    generalization and training errors of the raw
data for the  classified  locally weighted linear
regression vs  training set size 

we can compare lwlr with our svm
model from before by converting our predictions into binary  good  and  bad  values using
the same cutoff as we used for the svm  the
resulting generalization error for the processed
data leveled off at around      which is worse
than the svm model  however  we initially ran
it on the raw data  achieving an error of around
     as seen in figure     slightly larger than
that of the svm  lwlr relies on nearby data
points  so it isnt surprising that the raw data
does better than the processed data because it
containes    times as many training examples 
for both data sets  the generalization error leveled off  so more training examples probably
would not improve our prediction 

figpi contrast prediction  december       cs     final project

tween each misclassified point and the decision boundary for the svm and the absolute
error for the regression  from our histogram 
we saw that most of the misclassified points
are close to the decision boundary and are not
outliers  but that was all the useful information
we were able to obtain 

figure    confusion matrix for lwlr  left  processed
training set  right  raw training set 

figure   shows the confusion matrix for
the regression on the processed and raw data 
lwlr does a better job classifying the  bad 
data     than svm  however  it does worse at
classifying the  good  data      the improvement in classifying the bad data is most likely
due to the extra information on the contrast
values during regression 

v 

discussion and future work

in addition to svm producing a lower generalization error than lwlr for both the raw
and processed data sets  it also performs better
for our application because it has fewer false
negatives  false negatives carry a much larger
penalty than false positives  as false negatives
would imply astronomers are not collecting
data on a night when they could have gotten
good data  the svm currently predicts    
of good nights correctly and still saves the astronomers from collecting data on almost    
of the bad nights 
in order to try to improve the performance
of both the algorithms  we characterized the
test examples that were being misclassified 
we first used principle component analysis 
which projects our high dimensional data onto
three orthogonal axes that capture the most
variance  our pca decomposition  however 
only captured about     of the variance  and
the boundaries were hard to distinguish  we
also plotted a histogram of the distance be 

feature selection was our most successful
strategy  we confirmed  as expected  that the
features with the most predicting power are
the star brightness and the wavefront error 
however  the timescale of the turbulence was
thought to better correlate with the contrast
than the seeing which is contradicted by the
present study    we also found that the standard deviations of the wind speed and wind
direction were important features in our prediction  these parameters hadnt been considered
in previous studies 

vi 

conclusion

in conclusion  we have demonstrated the first
machine learning approach to predict the
worth of observing on a given night  support
vector machine classification gave better results
than locally weighted linear regression  particularly with the processed data set  the data that
reflect the final image the astronomers will use  
although the locally weighted linear regression did better at predicting the contrast on the
bad nights  the svm gave    false negatives 
which is the most important figure  because astronomers never want to miss an opportunity
to observe 
before beginning  we thought the classification problem would work better than the
regression problem because its simpler  and
that proved to be the case  with more time  or
a rotation student   we hope to use the framework we have created and the knowledge of
which features are the most helpful to apply
this problem to models designed for more complicated systems  such as neural networks 

  the more accurate conclusion might be that the dimm instrument that measures the seeing does a better job than the
mass instrument 

 

figpi contrast prediction  december       cs     final project

references
   

   

 

rajan  a   patience  j   nielsen  e  l  
wang  j  j   de rosa  r  j   macintosh  b  
graham  j  r  et  al          gemini planet
imager exoplanet survey  a study of the
contrast and planet detection sensitivity
poster at spirit of lyot conference
rajan  a   nielsen  e  l   wang  j  j   de
rosa  r  j   macintosh  b   graham  j  r  et 

al          gpies planet yield simulation
update poster at spirit of lyot conference
   

poyneer  l  a  et  al          performance
of the gemini planet imagers adaptive
optics system submitted

   

pedregosa  f  et  al          scikit learn 
machine learning in python jmlr     pp 
         

fi