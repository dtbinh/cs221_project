automatic rhythmic notation from single voice audio sources
jack oreilly  shashwat udit
introduction
in this project we used machine learning technique to make estimations of rhythmic notation of
a sung melody from its raw audio waveform  notating music by hand is a tedious and time consuming
task  and the benefits from automation in allowing musicians to easily collaborate and transfer their
ideas would be substantial  however  of the various instruments employed to produce melodies in
music  the human voice is among one of the most difficult instruments in which to determine note
onsets  while some instruments which mechanically produce consistent pitches corresponding to each
note  the human singing voice exhibits continuous variation in pitch even during the middle of a note 
and these variations are frequently more pronounced towards the beginning and end of a note  there is
no absolute physical or mathematical definition for note onset with a human voice  instead  listeners are
frequently able to discern the onset of new note through the use of contextual clues  with our machine
learning techniques  we attempted to match what a skilled human listener would transcribe if they were
listening to a performance  the data input for are algorithm is the waveform itself and the output is the
rhythmic notation that indicates the note onsets and durations  to accomplish this task  we divided the
problem into two sub problems  the first is estimation of note start and stop times from the audio
waveform  the second sub problem is to fit this result  given in a set of onset times and note durations 
to the likeliest musical notation 

requirements of the dataset
before searching for an appropriate data set  we determined the type of sample data and
ground truth data that would be necessary for tackling the problem  an appropriate data set would
need to include raw audio recordings of a single instrument or singer  it would need to have note onset
and offset times in order to be appropriate for the first problem  to be useful for the notation problem 
it would need to include musical notation for the monophonic melody 
we found two data sets that fulfilled some of these requirements  the first is the svnote data
set  which contains monophonic melodies and provides onset and offset times as well as estimated note
pitch values for ground truth  the melodies are sung without accompaniment by non professional
singers  the tonas data set provides a similar set of samples and ground truth values for
unaccompanied vocal performances of flamenco music by trained flamenco singers  these data sets
have an unfortunate shortcoming  the ground truth values for note onset and duration come from a
computer aided manual transcription process 

dataset generation
after spending some time working with both the tonas and the svnote datasets  we became
increasingly suspicious that the nature by which the datasets were generated was resulting in relatively
noisy ground truth data for training our algorithms  here  we decided to investigate generating our own
dataset  taking inspiration from the maps dataset  which used computer controlled acoustic pianos to
generate extremely accurate onset and offset data  we decided to use a professional virtual software
instrument to generate monophonic voice signals and accurate ground truth data simultaneously  using
sibelius  we generated a set of musical notation files for single voices  using the sibelius manuscript
scripting language  we then generated synthesized audio files via playback through the voxos epic
virtual choir plugin and simultaneously created metadata files describing precise onset  offset  and
fundamental frequency values  in an attempt to reduce overfitting to a particular sound  we employed

fithe full functionality of the virtual instrument  i e   the dataset included staccato and legato passages 
and every available syllable was represented 
for algorithm training and evaluation  we reserved     of the dataset for training and     for
evaluation  in order to ensure that both portions of the dataset were representative of the signals likely
to be encountered throughout  each portion was randomly shuffled  the random seed used for
shuffling was fixed for all feature types and learning strategies 

feature selection
qualities of audio signals and singing
before diving into particular machine learning algorithms  it was important to understand what
features are typically considered significant when analyzing audio data  particularly with regard to note
onset and offset  immediately rising to mind were the energy envelope  the instantaneous pitch  and
the autocorrelation of the waveform  in particular  a high rate of change in the signal energy for a given
signal window may signify a note onset or offset  likewise  a change in the estimated fundamental
frequency for a given signal window could indicate the same 

nave windowing
the first approach we took was one we came to call the nave windowing method  in this
method  our feature vectors are simply the raw waveform values of a subsection of the audio signal 
intuitively  this is unlikely to be an optimal approach to the problem for a few reasons  by creating an nsample sequence into an n dimensional vector  we may be throwing away information that adjacent
samples are likely to be highly correlated  the results from this approach were poor  as expected  and
they do not appear in the results section of this document 

windowed dft
the next approach we took was to window the signal sample and compute the discrete time
fourier transform  we used the kaiser window and a sample length of    ms  intuitively  this
corresponds somewhat closer to the musical features of an audio waveform than the nave windowing
method  as it approximately corresponds to energy represented in frequency bins  and therefore
musical pitch   however  this approach still produces a vector of high dimensionality proportional to the
sample length  and it suffers from many of the same problems as nave windowing 

mfcc with derivatives
one approach to feature selection was inspired by classical audio processing techniques  as
discussed in      here  we took windows of the audio signal and computed the mel frequency cepstral
coefficients  mfcc   these coefficients can intuitively be thought of as representing the energy
represented within a signal sample mapped to discrete frequency bins on the log frequency scale  this
approach is psychoacoustically motivated  as humans perceive musical pitch on a logarithmic basis  we
used    cepstral coefficients  in addition  we appended the feature vector with approximations the first
and second derivatives of these coefficients  resulting in a feature dimension of    
for computing the cepstral coefficients  we concerned ourselves with two frequency ranges 
the choice of the first was motivated by the fundamental frequency range we saw in the data set 
approximately     hz to      hz  for the second range  we doubled the upper end in order to
incorporate greater harmonic content and effects of signal transients 

fifigure     mel frequency cepstrum for a sample file

pca
for some feature selection strategies  we reduced the feature dimension by employing principal
component analysis  in particular  we performed analysis using pca on feature vectors produced by the
windowed dft approach as well as the mfcc approach  the number of principal components used was
determined experimentally by searching through powers of two up to the original feature vector
dimension  and better performing values were selected 

algorithmic approaches
onset detection
svm
for each feature selection approach  we trained svms  using libsvm  with three different
kernels  the linear kernel  polynomial kernel with degree    and the radial basis kernel  the data set
was normalized to have zero mean and values bounded within          feature vectors were classified
into two classes  those corresponding to signal windows containing a note onset and those without 
however  almost every single feature selection type produced a severely optimistic or pessimistic
model  either classifying every signal window as containing an onset or classifying no windows as such 
the only exception was using the linear kernel with the dft   pca  k       feature set  which  although it
produced a model that successfully classified the test data into more than a single class  otherwise
performed poorly 
em with gaussian mixture model
this approach  which proved the most successful  involved training two gaussian mixture
models  gmms  with the em algorithm  each corresponding to a classification type  onset containing or
non onset containing   given a test vector input  we compared prior probabilities calculated according
to each of the two models in order to decide the likelihood of an onsets presence 

fibest musical notation fit with nave bayes
to generate rhythmic notation  determining the note onset and offset is the only the first part of
the problem  we must also turn the raw intervals into note lengths  this is difficult  howeverin
addition to varying pitches during a note  vocalists often slightly vary the length of a note  when we
transcribe the notation we aim to convey the intended length  not the note that is actually sung 
one potential advantage we had as we attempted to generate rhythm notation is that there are
certain psychological and cultural bases of music that come into play  as music assigns high probability
to certain note durations  in particular  notes in western music almost always have a duration which is a
linear combination of products of reciprocals of   or   where we define a measure to have duration of
exactly one  if a note as marked by the onset and offset times appears to be between a quarter and a
fifth of measure  it is almost certainly a quarter note    quint notes are rarely used in western music  the
machine learning technique used to take advantage of our knowledge about the prior likelihood of note
lengths is the nave bayes algorithm which is shown below 

here y   is the chance that the note is question is equivalent to a particular note value  say a
quarter note  and x is observed interval between note start and stop  use of the nave bayes algorithm
requires knowledge of the tempo of the melody in beats per minute or equivalent  this is a reasonable
assumption and necessary for precise transcription  since theres no other way to distinguish between
half notes at    beats per minute and quarter notes at     beats per minute  if we used correct note
onset and offset times and a reasonable training set of note probabilities  the nave bayes classifier was
relatively accurate in accurately classifying notes with around     of notes merged together and
roughly double of that split  this was encouraging because it reinforced that the main challenge in
transcription was detecting note onset and offset  not converting those into notation  further  this
created interest in whether the nave bayes classifier could be used to detect note onset and offset  i e  
whether a certain interval was likely to be two eighth notes  a quarter note  part of a longer note etc 
there the results largely did not exceed random guessing  but with the intriguing exception that
accuracy was higher if both the training set and the sample being analyzed where very similar in style 

results and discussion
feature selection

algorithm

onset class error

dft  pca with k     
mfcc     ms window  no pca
mfcc      ms window  no pca
mfcc with high frequencies     ms
window  no pca
mfcc with high frequencies      ms
window  no pca
mfcc     ms window  pca with k  
  

em   mixtures     
em    mixtures     
em   mixtures    
em    mixtures     

      
      
      
      

non onset class
error
      
      
      
      

em    mixtures    

      

      

em    mixtures     

      

      

these results indicate that there is a significant tradeoff between sensitivity and precision for
the feature types and learning models that we employed  although we were not able to train a model
that simultaneously achieves high precision and recall  we noted that further filtering of the prior
probability functions  when computed continuously  using a sliding window for signal samples  may

fiimprove onset detection after further empirical observations  indeed  many current algorithms use
feature fusion and non trivial detection functions as a computation layer after the gmm model to
produce high performing detection 

conclusion 
generally  the best results were obtained when we could make strong  accurate assumptions as
in our nave bayes classifier for certain audio samples  or when we had sophisticated feature vectors
such as the mel frequency cepstral coefficients  this points to the twofold path forward towards more
accurate transcription  for the first and more uncertain approach  it may be possible to combine
machine learning algorithms of the type used in this project with other machine learning algorithms that
endeavor to take the entire sample and classify the style of music of piece in the melody of the whole 
such algorithms would need to have considerable power to break down music into much finer pieces
than commonly recognized genres the and tell when a piece has influence of multiple styles before it is
possible to make confident assumptions about the distribution of likelihood of various note lengths  but
the possibility cannot be ruled out  it is not unlikely that when human listeners transcribe music  their
expectations are affected by knowledge of the style of music 
the second path is what we would put the bulk of our efforts into if we had additional resources
in the form of team members  time  or computational resources  refinement of the feature vector 
from our experience in this project  a feature vector that can reliably distinguish between a frame in
which a note onset occurs and a frame in which a transition did not occur is unlikely to simple  but we
have demonstrated that there are feature vectors that contain information about the likelihood of
onsets and that some feature vectors produce more accurate results than others  the challenge that
remains in order to achieve accurate transcription and commercialization is one of optimization 

citations
    c  c  toh   b  zhang and y  wang  multiple feature fusion based onset detection for solo singing
voice   proc  int  soc  music inf  retrieval conf   ismir       
    chih chung chang   chih jen lin  libsvm  a library for support vector machines  acm transactions
on intelligent systems and technology  tist   v   n    p       april     
    viitaniemi  timo  anssi klapuri  and antti eronen   a probabilistic model for the transcription of
single voice melodies   proceedings of the      finnish signal processing symposium  finsig         

fi