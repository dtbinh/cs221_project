 

the lowest form of wit  identifying sarcasm in social media
saachi jain  vivian hsu

abstractsarcasm detection is an important problem in text
classification and has many applications in areas such as security 
health  and sales  previous works have tried various features
such as sentiment  capitalization  and n grams to classify sarcastic
texts  downloading a corpus of tweets through twitter archiver 
we used multinomial naive bayes  logistic regression  and support
vector machine to classify tweets as sarcastic or not sarcastic 
we found that unigrams and bigrams were the most indicative
features of sarcastic tweets  and we achieved an accuracy of
       using logistic regression on a data set with oversampling
on sarcastic tweets 

i 

i ntroduction

defined by merriam webster as the use of words that
mean the opposite of what you really want to say  sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike  often cited as an exception in sentiment
classification strategies  it reverses the intention of a concept
or idea by relying on grammatical structure  hyperbolical
vocabulary  and context 
sarcasm can be difficult to spot  especially when expressed
through written text  humans often mistake the true sentiment
that others convey in emails  messages  or posts on social
media  this mistake proves to be a problem in many different
contexts  for example  security agencies have trouble
differentiating false comments about planning terrorist attacks
from real ones  people also need to distinguish sarcastic
ratings and reviews from non sarcastic ones on websites
such as amazon and yelp  furthermore  some find it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help  a sarcasm detector can not only
help people interpret others writings  but can also assist the
writers themselves in avoiding being misunderstood 
an efficient sarcasm detector has proved to be difficult
to implement  as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success  in this paper  we use a corpus of sarcastic and
non sarcastic tweets  isolating features such as n grams 
capitalization  sentiment split  and subjectivity  to classify a
tweet as sarcastic or not sarcastic using multinomial naive
bayes  logistic regression  and linear support vector machine
 svm  classifiers 

ii 

r elated w ork

due to the significance of sarcasm in text classification 
several groups have already explored using machine learning
techniques to detect sarcasm in text  throughout many of
these studies  social media  especially twitter  is often the

primary data source for sarcastic and non sarcastic texts 
a significant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand  dr  mathieu cliche from cornell
university separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag  sarcasm  arguing
that tweets with  sarcasm are likely to be truly sarcastic
tweets  and tweets without the tag  although they may contain
sarcastic tweets  have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise  liebrecht et  al from radboud university nijmegen and
dr  david bamman and dr  noah a  smith from carnegie
mellon university employed similar techniques to create their
datasets  gonzalez ibanez et  al from rutgers university also
used  sarcasm to identify sarcastic tweets  but rather than
choosing non sarcastic tweets as tweets lacking sarcastic
hashtags  he used tweets presenting positive or negative
tags   happy   sadness   angry  etc  under the hypothesis
that tweets with tags representing pure emotions are less
likely to be sarcastic  although this approach caused the
non sarcastic dataset to be less representative of general
tweets  we thought that it is a better choice because it reduces
the noise associated with the non sarcastic set if it were
obtained by simply choosing tweets without  sarcasm 
dr  cliche  liebrecht et  al  bammam et  al  and gonzalezibanez et  al all had n grams as a critical feature in their
classifiers  in addition  bammam et  al counted the number of
words in all caps in a tweet as a feature  riloff et  al from the
university of utah explored another feature involving a split
in sentiment using a bootstrapping algorithm  specifically 
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state  i e 
i love taking exams   their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets 
several of the groups focused on logistic regression as a
promising classifier for sarcastic tweets  cliche  gonzalezibanez et  al  bammam et  al   dr  cliche also employed
multinomial naive bayes and a linear svm  and achieved an
f score of       liebrecht et  al from radboud university
nijmegen used a balanced winnow classifier  based off of
the perceptron algorithm  and obtained an accuracy of     
gonzalez ibanez et  al had a     accuracy with logistic regression and a     accuracy with sequential minimal optimization
algorithm  smo   bammam et  al used logistic regression to
get an accuracy rate of        overall  it seems that logistic
regression was the most popular and consistent classifier  while
unigrams and bigrams were the more effective features 

fi 

iii 

data and f eature e xtraction

our dataset consisted of english tweets obtained from
twitter through twitter archiver  a google add on that
downloads tweets into a google spreadsheet based on filters
such as hashtags and language  we downloaded tweets
from november   th to december  rd        based on
the assumption that the writers of the tweets are the best
people to judge whether their tweets are sarcastic or not 
we used hashtags to annotate sarcastic tweets  we obtained
sarcastic tweets by getting tweets with the hashtag  sarcastic 
we further assumed that any tweets with emotional
hashtags such as happy  joy  lucky  sad  angry  and
disappointed were non sarcastic tweets expressing positive
or negative sentiment 
to clean the data  we filtered out symbols and strings that
did not contribute to the overall meaning of the tweets  we
took out all words followed by a hashtag      all links to
other websites  tokens beginning with http   and all tags to
other accounts  tokens beginning with     if a tweet has
fewer than three tokens left after cleaning  we took it out of
our data set  we were left with        sarcastic tweets and
        non sarcastic tweets in our dataset 
our feature set consisted of unigrams  bigrams 
capitalization  sentiment split  and subjectivity  each unigram
and bigram was its own feature  we created the unigrams
by parsing the tweets into lemmatized individual words and
punctuation marks        and     and counted the overall
occurrences of each in sarcastic texts and non sarcastic texts 
we created the bigrams by parsing the tweets into pairs
of consecutive words  and also counted their frequencies
in sarcastic and non sarcastic texts  however  we took out
punctuation marks in the bigrams because we wanted to
analyze the impact that two adjacent words have on whether
or not a tweet is sarcastic  rather than whether or not a
punctuation mark precedes or follows a word  we then further
reduced our data set by eliminating unigrams and bigrams that
appear fewer than    times  in total  we had       unigrams
and        bigrams 
aside from unigrams and bigrams  we extrapolated three
more features based on the overall context of the tweets  one
of the features was the number of words in all caps  greater
than one letter   as suggested by bammam et  al  another was
sentiment split  which captures the difference in the sentiment
between the part of the tweet before the verb phrase and the
part of the tweet after the verb phrase  for example  if the tweet
was i hate christmas presents  we used the library pattern en
to split the tweet into two chunks  i hate and christmas
presents  using the nlp library textblob  we calculated the
sentiment score for both chunks  a score of from    to   
where    is very negative and   is very positive   and found
the difference between the two scores as the tweets sentimentsplit score  finally  our last feature was the subjectivity score
 fact or opinion  of the entire tweet  which we also calculated
using textblob 

iv  m ethods
after collecting the data and extracting the features  we used
three classifiers  all from the library scikit learn   multinomial
naive bayes  logistic regression  and svm  to predict whether
a tweet was sarcastic or not sarcastic 
a  multinomial naive bayes
bayes theorem states that  for feature vector
 x    x         xm   and resulting class y  the following
relationship holds 
p  y x    x         xm    

p  y p  x    x       xm  y 
p  x    x         xm  

   

under a naive bayes classifier  we model p  x    x         xm  y 
with the assumption that each xi is conditionally independent
on y  thus  we can simplify     to be 
m
q
p  y 
p xi  y 
i  
p  y x    x         xm    
   
p  x    x         xm  
after finding the prior distributions using maximum likelihood
estimates  we simply choose the class y that gives the higher
posterior probability in     
multinomial naive bayes  a variation on the naive bayes
algorithm above  is commonly used for text classification  it
is parameterized by  y    y    y         yn   where yi is the
probability of feature i appearing in the class y  specifically 
scikit learns version of multinomial naive bayes that we used
further employed laplace smoothing  such that the parameters
were calculated as 
nyi    
   
yi  
ny   n
where nyi is the number of times feature i appeared in class
y and ny is the total count of features in y 
b  logistic regression
under logistic regression  given a feature vector x  we use
the following hypothesis function 
h  x    g t x   

 
    et x

   

where g is the sigmoid function 
following most linear classifiers  we set the hypothesis
function as 
p y x       h  x  y     h  x   y

   

we then find the likelihood of the parameters  as 
l    
 

m
y
i  
m
y
i  

p y  i   x i     
   
 i 

 h  x i    y     h  x i     y

 i 

fi 

we can then maximize the likelihood by maximizing the log
likelihood 
      log l  
m
x
   
 
y  i  log h x i          y  i    log    h x i    
i  

in order to maximize the log likelihood  we use gradient ascent
over a period of updates  our gradient ascent update rule will
then be given by 
           
m
x
      
 y  i   h  x i    x i 

   

i  

then gradient ascent is performed until convergence to create
the parameter vector   after training  tests can be performed
by calculating the value of the hypothesis h  x    g t x 
c  support vector machine  svm 
we used scikit learns linear support vector classification
algorithm  which is an implementation of linear support
vector machine that scales better to large numbers of samples 
in support vector machine  we denote the class that each
point xi belongs to by yi   which is either   or    svm works
by finding the maximum margin hyperplane that divides the
xi s for which yi     from the xi s for which yi      a
hyperplane is a set of points x such that w  x  b      in
which w         n  t and b       we need to solve the dual
optimization problem 
m
m
x
  x  i   j 
y y i j hx i    x j  i     
max w     
i 

 
i j  
i  
m
p

the ai s are lagrange multipliers and c controls the relative
weighing between the goals of making the   w    small and of
ensuring that most examples have functional margins at least
  in    regularization  in our case  c      the default set by
scikit learn 
we then use the sequential minimal optimization  smo 
algorithm to solve the dual problem 
repeat until convergence  
   select some pair i and j to update 
   reoptimize w    with respect to i and j   while
holding the other k s fixed 
 
the decision function used by scikit learn is 
m
x
sgn 
yi i hxi   xi    

where  is the intercept term 

a  original runs  unbalanced data set
we partitioned     of our tweets into a training set  and
    into a testing set  we then ran each of the three models
on the two sets  to see that our models have learned from
the features  we also ran them under the condition that each
tweet had only one feature  randomly assigned to be   or   
model name
multinomial naive bayes
logistic regression
linear svm

accuracy
      
      
      

random
      
      
      

table    model ccuracy when splitting     of the samples
into the training set and     into the test set
here multinomial naive bayes and logistic regression were
more effective than svm  however  because the dataset was
heavily unbalanced with far more non sarcastic samples than
sarcastic samples  the random classification rate was close to
the model classification rates  to more closely evaluate the
differences in the three models  we found the confusion matrix
to report the false positive and false negative rates 

i y  i       where

i  

i  

r esults and d iscussion

   

     simiplifies to 

s t     i  c  i           m and

v 

    

figure    false positive and negative rates for multinomial
naive bayes  logistic regression  and svm
for all three models  the false positive rate  non sarcastic
samples classified as sarcastic  was lower than the false
negative rate  sarcastic samples labelled as non sarcastic   we
believe this may be due to the fact that the dataset was
unbalanced  and took measures to address this issue later in
the report  see oversampling  undersampling  
furthermore  we found the precision recall curves for each
of the three models  multinomial naive bayes performed the
best in terms of the precision recall tradeoff  while svm performed the worst  this result matches the accuracies presented
in table   
model name
multinomial naive bayes
logistic regression
linear svm

average precision
      
      
      

table    average precision accuracy  auc for pr curve  for
each model

fi 

the false negative rate was significantly lower  which
indicates that the training phase may be more effective with
a balanced data set because the classifier was no longer
automatically classifying the samples as not sarcastic  the
false positive rate was slightly higher  because more samples
were now being classified as sarcastic 
finally  we found the precision recall curves under the new
data set  both the precision and the recall improved when
training on the oversampled set  among the three curves 
multinomial naive bayes continued to have the highest average
precision while svm performed relatively poorly 

figure    precision and recall curves
we further performed   fold cross validation using scikitlearns cross validation module on each of the three models 
here logistic regression performed better than multinomial
naive bayes  but the accuracies were relatively close 

model name
average precision
multinomial naive bayes
      
logistic regression
      
linear svm
      
table    average precision accuracy  auc for pr curve  for
each model with an oversampled data set

model name
accuracy
multinomial naive bayes
      
logistic regression
      
linear svm
      
table    accuracy with   fold cross validation
b  oversampling
in order to achieve a more balanced data set  we implemented oversampling and undersampling  in oversampling  the
samples belonging to the more scarce class are overrepresented
in the final data set  thus  we had each sarcastic tweet occur
  times in the data set  so that both sarcastic and nonsarcastic sets had about         samples  thus  the random
classification rate was closer to      we again split the final
data set into     train and     test  under the oversampled
set  logistic regression had the highest accuracy 
model name
multinomial naive bayes
logistic regression
linear svm
table    accuracy with an

accuracy random
      
      
      
      
      
      
oversampled data set

we again found the confusion matrix to analyze false
positive and false negative rates 

figure    false positive negative rates for oversampled data

figure    precision recall curves for oversampled data set
c  undersampling
undersampling involved taking only    th of the non sarcastic
tweets for the testing and training sets each time so that both
sarcastic and non sarcastic sets had around        tweets  we
performed undersampling   times  each with a different quarter
of the non sarcastic set  and averaged the accuracies in table
  
model name
accuracy random
multinomial naive bayes
      
      
      
      
logistic regression
linear svm
      
      
table    accuracy with an undersampled data set
similar to the unbalanced data set  multinomial naive
bayes and logistic regression were most effective while svm
performed more poorly  overall  comparing the unbalanced 
oversampled  and undersampled sets  the oversampled set
had the greatest improvement in classification accuracy over
the random classifier  figure     the oversampled set may
have performed better than the undersampled set because it

fi 

encompassed more of the non sarcastic examples during the
training phase 

based on emotion related hashtags  we have no non sarcastic
tweets that express neutral emotion  in the future  we can
try the liebrecht et  als proposed method of obtaining nonsarcastic tweets  which involves simply taking tweets that do
not have the tag  sarcastic and accepting the resulting noise 
in order to provide a reasonable classifier  we created a data
set with comparable numbers of sarcastic and non sarcastic
tweets  however  sarcasm is relatively rare  so these sets do
not represent the proportion of sarcastic tweets in real life 
in the future  we can explore other classifiers that handle
unbalanced sets more appropriately 

figure    difference between random and model accuracies
for unbalanced  oversampled  and undersampled data sets
d  feature analysis
to analyze the impact of each feature on the classifier  we
found the accuracy of the classifiers depending on each feature
alone when trained and tested on the oversampled data set 
table   contains the average single feature accuracies over the
three models
feature name accuracy
unigram
      
bigram
      
capitalization
      
sentiment split
      
      
subjectivity
random
      
table    accuracy of classifier with single feature averaged
over multinomial naive bayes  logistic regression  and svm
as shown  unigrams and bigrams were the most effective
in classifying a tweet as sarcastic or not sarcastic  while
capitalization and sentiment split achieved results that were
only slightly better than those obtained by random feature 
vi  c onclusion and f uture w ork
of the three models  logistic regression was the most
effective at classifying sarcastic tweets  with an accuracy of
       on an oversampled set  svm generally performed
more poorly  this may be because there is not a large
enough margin between the two classes for a sufficient
linear hyperplane  multinomial naive bayes performed well
overall  and even performed better than logistic regression in
the unbalanced and undersampled sets  however  it makes
the underlying assumption that each of the features are
conditionally independent  but unigrams and bigrams are
dependent by definition  thus  multinomial naive bayes may
be overweighting the importance of certain unigrams and
bigrams  of our features  bigrams were the most significant in
performing a correct estimate  with a single feature accuracy
of        on an oversampled set 
currently  there are some limitations to our data set 
due to our methodology of selecting non sarcastic tweets

finally  sarcasm is often based on current events  we polled
sarcastic and non sarcastic tweets over a period of four weeks 
ideally  however  tweets should be pulled over a longer period
of time to provide a larger and more unbiased corpus of tweets 
r eferences
   

bamman  david and noah a  smith  contextualized sarcasm detection
on twitter  association for the advancement of artificial intelligence
                

   

cliche  mathieu  ph d  the sarcasm detector  n p   n d  web 

   

de smedt  tom  and walter daelemans  pattern for python  journal
of machine learning research                  

   

gonzalez ibanez  roberto  smaranda muresan and nina wacholder 
identifying sarcasm in twitter  a closer look  proceedings of the
  th annual meeting of the association for computational linguistics
                

   

liebrecht  christine  florian kunneman  and antal van den bosch  the
perfect solution for detecting sarcasm in tweets  not  proceedings
of the  th workshop on computational approaches to subjectivity 
sentiment and social media analysis               

   

loria  steven  pete keen  matthew honnibal  roman yankovsky  david
karesh  evan dempsey  wesley childs  jeff schnurr  adel qalieh  lage
ragnarsson  and jonathon coe  textblob  version v        available at
https   textblob readthedocs org

   

pedregosa  fabian  gal varoquaux  alexandre gramfort  vincent michel 
bertrand thirion  olivier grisel  mathieu blondel  peter prettenhofer 
ron weiss  vincent dubourg  jake vanderplas  alexandre passos 
david cournapeau  matthieu brucher  matthieu perrot  and douard
duchesnay  scikit learn  machine learning in python  journal of
machine learning research                   

   

riloff  ellen  ashequl qadir  prafulla surve  lalindra de silva  nathan
gilbert  and ruihong huang  sarcasm as contrast between positive
sentiment and negative situation  proceedings of the      conference
on empirical methods in natural language processing                 

fi