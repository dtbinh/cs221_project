drug and chemical compound named entity
recognition using convolutional networks

mark j  berger
department of computer science
stanford university
stanford  ca      
mjberger stanford edu

 

introduction

as biomedical literature continues to grow at an explosive rate  researchers are unable to process the
vast amounts of information generated by one another  in order to account for this  text mining and
information extraction systems have been developed in order to help researchers find information
that is relevant to their respective research  however  text mining systems have also been developed
to infer new knowledge  and further biological understanding  examples include inferring proteinprotein interactions  and gene disease interactions  from publicly available literature in order to
further the understanding of systems biology  and ultimately  to better combat disease  krallinger et
al        
in order for all of these systems to produce strong results  each system must be able to accurately
determine the name of a gene  protein  or any other item of interest  therefore  strong named
entity recognition  ner  systems must be developed in order to prevent errors from propagating
through the system  and negatively affecting performance  in order to strengthen the ability for text
mining systems to infer novel biological knowledge  we focus on the task of improving named entity
recognition for drugs and chemical compounds 
specifically  in this work  we explore ner systems which do not incorporate linguistic or domaindependent information  instead opting to explore how text representation affects performance  for
all of the models discussed in this paper  the input to our algorithm is the abstract of a biomedical
paper  we then use a convolutional neural network to predict whether a given token is part of a
drug or chemical compound  given this restriction  we show that a convolutional neural network
outperforms a strong baseline  however  future work is necessary in order to rival the performance
of existing systems 

 

related work

with the induction of the chemical mention recognition task to biocreative iv  krallinger et al 
       multiple systems have addressed this task  leaman  lu    munkhdalai   despite a variety of systems  the top performing systems all use conditional random fields  which is a form of
undirected graphical model  the model derives its power from being able to account for the probability of neighboring samples  therefore making them a natural fit for named entity recognition tasks
 mccallum et al   
for input and processing  the models differ  but all of them follow a general trend  lu et al  focus
on representing the documents in high dimensional spaces by generating vector representations with
brown clustering  brown et al   and the skip gram model described in word vec  mikolov et al   
they supply both of these representations to their crf model to determine the entities  in contrast 
leaman et al  opt to use an n gram representation with linguistically informed features  such as
lemmatization and part of speech tags  they also incorporate a variety of domain specific knowl 

fiedge  such as amino acids and common chemical elements  finally  munkhdalai et al  combine both
approaches by using brown clustering and word vec to generate high dimensional representations 
while also supplying domain specific features to the crf model  despite their differences  all of
these models produce comparable results when measuring performance with micro f  
however  when we examine these crf model closely  we notice an underlying limitation  crfs are
linear models  and distributed representations have relationships which cannot be linearly modeled
in a low dimensional space  wang   manning   therefore  we explore whether a deep architectures 
which address these two limitations  can produce superior performance 

 
   

data set and preprocessing
data set

for our data set  we use the chemdner corpus of chemicals and drugs from the biocreative iv
chemical mention recognition task  krallinger et al         overall  the data set contains a collection
of        randomly selected pubmed abstracts  which contain a total of        chemical entity
mentions  the annotations were hand labeled by experts in the field  and therefore are considered to
be reliable  while the data set has two sub tasks  we specifically focus on the chemical document
indexing  cdi  task  which requires the system to return all unique chemical entities within a given
abstract 

figure    an excerpt of a biomedical abstract from the chemdner data set  where the first instance of each chemical compound is highlighted 
officially  the data set has the following split       articles for training       articles for development evaluation  and      articles for the held out test set  since neural network models are known
to require a lot of training data  we rearranged the data set by randomly selecting      articles from
the development set to become our new development set  we then used the remaining      articles
as training data  the test set was not modified  since the number of named entities is rather small
compared to the overall number of tokens in the set  we also duplicate all named entity data tokens
in order to combat class imbalance 
   

preprocessing

before training any models  we tokenize the data by splitting on the symbols in figure   using the
following regular expression    symbols    this symbol table was previously used by lu et al 
on the same data set  while this approach is considered to be rather trivial compared to using a
statistical parser  for chemical compounds  this offers superior performance  since statistical parsers
are trained on more general text  they will often split domain specific words  such as n    hydroxy  mercaptonaphthalen   yl amides  in odd or unpredictable ways  this can cause the named entities
to be combined with other words  which ruins our ability to accurately return the entities within the
text  therefore  using a naive regular expression is more suitable for the task at hand 

figure    symbols used to split the text  lu et al        
 

fifor input to our neural network models  we opted to train word vectors for each token using the
popular word vec system with the default parameters and a vector length of      mikolov et al    as
training data  we used the      version of the bioasq data set  which contains roughly      million
biomedical abstracts from the pubmed database  tsatsaronis et al    we removed any abstracts from
the bioasq data set which were also in our development and test sets 

 
   

models
baseline

as a baseline  we use logistic regression with l  regularization and a bag of words character based
ngram representation  each token in the abstract is classified  and consecutive positively classified
tokens are joined to form an entire entity  these final entities are then evaluated to determine an f 
score 
   

convolutional neural network

for our experiments we explore the use of a convolutional neural network  cnn  model  krizhevsky
et al   lecun et al    which has been used for ner in the past  collobert              we use a
slight variation of the cnn architecture presented by kim  and we briefly describe it here  in order
to represent a token  we construct an n by k matrix  where n is the number of tokens and k is the
dimension of the word vectors  for each row in the matrix  row i contains the word vector for the
ith word in the document  tokens which occur at the beginning and end of the document are zero
padded to have a length of        w where w is the window size 
after constructing our matrix  we conduct a convolutional operation over each documents respective
matrix  specifically  we apply a weight matrix  known as a filter  w  rhk to a window of size h
by k  where h is the length of an n gram  to produce a scalar ci   specifically  this is calculated by 
ci   f  w xi i h    b c   
where b is the bias term and f is a chosen non linearity  during convolution  we apply this filter
over all possible n  h     windows to produce the set of features  c    c         cn h      we then
apply a max over time pooling operation  collobert et al         to the set of n  h     features to
produce the feature c 
c   max c    c         cn h    
therefore  we extract a single feature from the set of features produced by the filter  we repeat this
same process for multiple feature maps of size h by k  as well as for filters with different sizes of h 
we then concatenate all of these features into a single vector  creating the vector c    and supply c  to
a set of fully connected layers  in order to prevent feature adaption  we apply the popular dropout
method with a probability of p to the penultimate layer 
for the final layer  we add a fully connected output layer with a sigmoid activation function  which
produces a scalar  we treat this scalar as the probability that the center token is contained within a
named entity 
y    u c    b o   
intuitively  we believe this model has the ability to offer superior performance to crfs because of its
filter based approach  the character composition of chemical compounds tend to differ from other
english words  and therefore  we believe that the cnn model will learn filters which are activated
by their unusual token compositions 
as in the baseline  each token in the abstract is classified  and consecutive positively classified tokens
are joined to form an entire entity  we use a threshold    to determine whether a token is positively
classified 
 

fifigure    our model inspired by kim        

 
   

experiments   results
baseline

for our logistic regression model  we performed a grid search over the window sizes   through    and
the ngram sizes   through    our model performed best on the development set with a window size
of   and an ngram size of   and was implemented using the popular scikit learn library  pedregosa
et al   
   

convolutional neural network

for our cnn model  we evaluated window sizes of   and   with the following combinations of
filters      unigram      bigram      trigram filters  and     trigram filters  our model performed
best on     trigram filters  for all experiments  we fixed the fully connected layers to a single layer
with a size of      a leaky relu non linearity with alpha of      mass et al    a dropout probability
of      hinton et al    and constrained the column norm to        the convolutional weight matrices
are initialized via drawing from a gaussian distribution with mean   and standard deviation of     
the fully connected layers were initialized using he initialization  he et al    and the model was
trained with stochastic gradient descent with momentum  nesterov  and a batch size of      we
performed two experiments  one with fixed word vectors and one with those that were updated  our
implementation was built using theano  bergstra et al  

 

results and analysis

in order to evaluate our models  and compare them to existing models  we use the popular micro
precision  micro recall  and micro f  score metrics  overall  our baseline model is able to achieve
an f  score of       on the test set  our two cnn models performed nearly identically producing
a f  score of       and        respectively  however  these results do not rival the best performing
systems in this task 
after analysing our results  we believe our model has a variety of problems  as seen in figure   
the precision and recall of our models on the token level is significantly better than the entity level 
with a f  score of        however  once these tokens are joined to form entities  the f  significantly
decreases to        by evaluating these results and examining the systems output by hand  we
noticed that our model will often drop a middle token in an entity  which will cause it to form two
entities  in a similar vein  additional  incorrect  tokens will be placed at the beginning or end of
a correct entity  we believe this is caused by the short nature of the tokens due to our aggressive
tokenization technique  which in turn creates similar context windows for tokens which do  and do
not  belong in an entity  additionally  unlike the conditional random field models used in other
systems  the cnn cannot condition on the likelihood of the surrounding tokens also being positive
 

fimodel
our baseline
our model   static
our model   non static
lu et al 
tmchem  leaman et al  
banner chemdner  munkhdalai et al  

precision
     
     
     
      
     
     

recall
     
     
     
      
     
     

f 
     
     
     
      
     
     

table    micro f  results on the chemdner test set  in the case where other papers contained
multiple models  we chose the model with the highest f  score 

instances  while it should be able to learn a similar output from the surrounding window  it is not
able to do joint inference like in the crf models 
finally  we believe our model is suffering from high bias after evaluating the error produced on the
training and evaluation data sets  throughout the course of the entire experiment  the error on the
training and evaluation sets stay roughly the same  and the model never begins to overfit the test
set  therefore  the model may not be expressive enough to capture the underlying function we are
attempting to model 

figure    the precision recall curve for our best model at the token level 

 

future work

in order to address these concerns  we wish to explore a variety of options  first  we wish to
significantly increase the number of convolutional filters  and the size of the fully connected layers 
in order to make our model more expressive  this will hopefully combat the high bias problem
we are experiencing  similarly  we can also add multiple convolutional layers  instead of using
the aggressive pooling scheme in kim  additionally  we would like to experiment with using onehot character representations  and using convolutional layers to extract features from that sparse
representation  santos   zadrozny   finally  we would like to experiment with other models which
combine aspects of conditional random fields and convolutional networks  such as neural conditional
random fields  do   artieres  
acknowledgements
the author would like to acknowledge mitch sanford for his help in conceiving the project and parsing the chemdner data set  the author would also like to acknowledge professor gill bejerano
for access to his labs computing resources 
 

fireferences
bergstra  j   breuleux  o   bastien  f   lamblin  p   pascanu  r   desjardins  g         bengio  y         june  
theano  a cpu and gpu math expression compiler  in proceedings of the python for scientific computing
conference  scipy   vol     p     
brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram models of
natural language  computational linguistics                 
collobert  r     weston  j         july   a unified architecture for natural language processing  deep neural
networks with multitask learning  in proceedings of the   th international conference on machine learning  pp 
          acm  chicago
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k    kuksa  p          natural language
processing  almost  from scratch  the journal of machine learning research                
do  t     arti  t          neural conditional random fields  in international conference on artificial intelligence and statistics  pp           
he  k   zhang  x   ren  s     sun  j          delving deep into rectifiers  surpassing human level performance
on imagenet classification  arxiv preprint arxiv            
hinton  g  e   srivastava  n   krizhevsky  a   sutskever  i     salakhutdinov  r  r          improving neural
networks by preventing co adaptation of feature detectors  arxiv preprint arxiv           
kim  yoon  convolutional neural networks for sentence classification  arxiv preprint arxiv          
       
krallinger  m   leitner  f     valencia  a          analysis of biological processes and diseases using text
mining approaches  in bioinformatics methods in clinical research  pp            humana press 
krallinger  m   rabal  o   leitner  f   vazquez  m   salgado  d   lu  z         segura bedmar  i          the
chemdner corpus of chemicals and drugs and its annotation principles  journal of cheminformatics    suppl
    s  
krizhevsky  a   sutskever  i     hinton  g  e          imagenet classification with deep convolutional neural
networks  in advances in neural information processing systems  pp             
leaman  r   wei  c  h     lu  z          tmchem  a high performance approach for chemical named entity
recognition and normalization  journal of cheminformatics    supplement    
lecun  y   boser  b   denker  j  s   henderson  d   howard  r  e   hubbard  w     jackel  l  d         
backpropagation applied to handwritten zip code recognition  neural computation                
lu  y   ji  d   yao  x   wei  x     liang  x          chemdner system with mixed conditional random
fields and multi scale word clustering  journal of cheminformatics    suppl     s  
maas  a  l   hannun  a  y     ng  a  y         june   rectifier nonlinearities improve neural network acoustic
models  in proc  icml  vol      
mccallum  a     li  w         may   early results for named entity recognition with conditional random
fields  feature induction and web enhanced lexicons  in proceedings of the seventh conference on natural
language learning at hlt naacl      volume    pp            association for computational linguistics 
mikolov  t   sutskever  i   chen  k   corrado  g  s     dean  j          distributed representations of words
and phrases and their compositionality  in advances in neural information processing systems  pp             
munkhdalai  t   li  m   batsuren  k   park  h  a   choi  n  h     ryu  k  h          incorporating domain
knowledge in chemical and biomedical named entity recognition with word representations  journal of cheminformatics    suppl     s  
nesterov  y         february   a method of solving a convex programming problem with convergence rate o
   k    in soviet mathematics doklady  vol      no     pp           
pedregosa  f   varoquaux  g   gramfort  a   michel  v   thirion  b   grisel  o         duchesnay  e         
scikit learn  machine learning in python  the journal of machine learning research                
santos  c  d     zadrozny  b          learning character level representations for part of speech tagging  in
proceedings of the   st international conference on machine learning  icml      pp             
tang  b   feng  y   wang  x   wu  y   zhang  y   jiang  m         xu  h          a comparison of conditional
random fields and structured support vector machines for chemical entity recognition in biomedical literature 
journal of cheminformatics    supplement    

 

fitsatsaronis  g   schroeder  m   paliouras  g   almirantis  y   androutsopoulos  i   gaussier  e         ngomo 
a  c  n         october   bioasq  a challenge on large scale biomedical semantic indexing and question
answering  in aaai fall symposium  information retrieval and knowledge discovery in biomedical text 
wang  m     manning  c  d          effect of non linear deep architecture in sequence labeling  in proceedings of the  th international joint conference on natural language processing  ijcnlp  

 

fi