deep reinforcement learning with pomdps
maxim egorov
december         

 

introduction

recent work has shown that deep q networks  dqns  are capable of learning human level control
policies on a variety of different atari      games      other work has looked at treating the atari
problem as a partially observable markov decision process  pomdp  by adding imperfect state
information through image flickering      however  these approaches leverage a convolutional network
structure     for the dqn  and require the state space to be represented as a two dimensional
grid  this approach works well on problems where the state space is naturally two dimensional
 i e  atari screen   but does not generalize to other problems  this project aims to extend dqns
to reinforcement learning with pomdps without the limitations of a two dimensional state space
structure  in this project we develop a novel approach to solving pomdps that can learn policies
from a model based representation by using a dqn to map pomdp beliefs to an optimal action 
we also introduce a reinforcement learning approach for pomdps that maps an action observation
history to an optimal action using a dqn 

 

partial observability

in many real world applications  the complete state of the environment is not known to the agent 
in these cases  the agent receives an observation that is conditioned on the current state of the
system  and must make a decision based on the observations it receives as it acts in the environment 
these types of problems can be modeled as pomdps  a pomdp can be formalized as a   tuple
 s  a  t  r  z  o   where s  a  t  r are the states  actions  transitions and rewards as in a markov
decision process  the z  o are the observation space and the observation model respectively 
when the pomdp model is known  the agent can update its belief b s  as it interacts with the
environments the belief defines the probability of being in state s according to its history of actions
and observations  systems with discrete beliefs can be updated exactly using
b   s     o s    a  o 

x

t  s  a  s   b s  

   

ss

many approaches exist for approximating optimal pomdp policies when the model of the
environment is known  the state of the art solver known as sarsop     attempts to sample
the optimally reachable belief space in order to efficiently compute a pomdp policy  in this
work  sarsop will be used as a benchmark for testing the dqn policies  a pomdp policy
can be represented as a collection of alpha vectors denoted   associated with each of the alphavectors is one of the actions  the optimal value function for a pomdp can be approximated by a
piecewise linear convex function that takes the form
v  b    max   b  


   

if an action associated with an alpha vector  maximizes the inner product   b  then that action is
optimal  reinforcement learning has also been used in the context of pomdps by using function
approximation to represent a stochastic policy      we use a similar approach to compare against
the dqn approach to reinforcement learning with pomdps and refer to it as pomdp rl 
 

fiabkmhiq 
o

a
   t
  bm q   k mi
g  mbm 

j kq v

g  m  

expereince

figure    information flow in deep reinforcement learning with a generalized problem simulator
r
fully observable variables

five fully connected layers

x 
approximate
q values

x 

q a   

x 
q a   

belief vector

b 
q a   

b 

b 
b 

figure    five layer fully connected network that maps the concatenated fully observable variable and belief
vectors to q values

 

deep reinforcement learning

in reinforcement learning  an agent interacting with its environment is attempting to learn an
optimal control policy  at each time step  the agent observes a state s  chooses an action a  receives
a reward r  and transitions to a new state s    q learning estimates the utility values of executing
an action from a given state by continuously updating the q values using the following rule 
q s  a    q s  a     r    max
q s    a     q s  a   
 
a

   

learning the q values for pomdps using the approach above is intractable  because a q value
would be needed for each possible belief or for arbitrary long action observation histories  however  a
function approximator  such as a neural network  can be used to approximate the pomdp q values 
in deep q learning  a neural network is used to approximate the q values in a decision process 
for a pomdp  the q values are parameterized by either the belief and the action q b  a  or an
action observation history h and an action q h  a   these modified q values can be learned by a
neural network that is characterized by weights and biases collectively denoted as   these q values
can be denoted by q b  a   and q h  a    instead of updating each q value  we can now update
the parameters of the neural network by minimizing the loss function 

 

fifigure    the value function surfaces for the tiger problem for the sarsop alpha vectors  left   dqn
converged policy  middle   and dqn non converged policy  right 

l b  a i      r    max q b    a i    q b  a i      
a

   

where i     i    l i    however  this approach to updating the q values can lead to divergence 
there are three techniques used to stabilize the learning  first  experience replay tuples  b  a  r  b   
are recorded in a reply memory  and are then sampled uniformly  second  a separate target network
is used to provide state update targets to the main network  lastly  an adaptive learning method
known as rmsprop is used to regulate the parameter adjustment rate of the network  this
framework is decomposed into three components  the simulator  memory and learner  fig      the
simulator can be a pomdp model or an atari emulator  while the learner contains a function
approximator which is a dqn in this project 
the architecture of the dqn is shown in fig     we adopt a similar framework to      where
a simulator is used to populate and experience replay dataset  in the figure the input layer to
the network consists of the belief of the agent  and the fully observable state variables  the fully
observable state variables generalize this representation to problems where the agent may have
some knowledge of the system state  known as mixed observability mdps  mdomps   a similar
architecture was used for training the dqn on action observation histories as well  once again 
if the problem has fully observable state variables  those are also used as inputs to the network 
the current formulation uses a fully connected network that either takes the fully observable state
variables x and the belief b or just the belief b and outputs a value approximation for each action
available to the agent  the dqn hyper parameters used in this project can be found in section   

 

evaluation and results

this famework was tested on two popular benchmark problems found in the literature  tiger and
rock sample  in the tiger problem  the agent has to choose between opening a door on the left or
on the right  if the agent opens the door with tiger  it receives a reward of       if it opens the other
door it receives a reward of    and escapes  the agent can listen  and receive a noisy measurement
of where the tiger may be located  the true alpha vectors for this problem were obtained using
sarsop  and are shown in figure    the dqn for this problem was evaluated at various belief
points to construct a value function surface  the value function surface for the converged dqn
closely resembles the surface formed by the alpha vectors  the value surface for a non converged

 

fifigure    policy and q value convergence for the tiger  left  and rock sample  right  problems

dqn is shown for reference  while the difference between the converged and non converged value
surfaces are small  the primary difference in policy comes from the listen action dominating the
open right action at small belief values 
in the rock sample problem  a rover is tasked with sampling a set of eight rocks on a     
gridded map  the goal is to sample rocks that are good and to avoid sampling bad rocks  the
rover can make a noisy measurement of the rock quality  with the accuracy of the measurement
parametrized by the distance between the rock and the rover  rock sample is a momdp  because
the rover knows its own position exactly  but does not know the state of the rocks  this is a fairly
large problem with          states     actions  and   observations  nearly optimal policies took
about   minutes of training for the tiger problem and about   hours of training for the rock sample
problem  note that it takes sarsop less than   second to compute the tiger policy and about  
minutes to compute the rock sample policy  however  sarsop needs explicit knowledge of the
model to determine the alpha vectors  while the dqn can be trained using just action observation
histories 
one of the more significant results from this project was identifying the differences in convergence
behavior of the q values and the policies  these are demonstrated in fig     where the average
q values of the network are evaluated at each episode  and the policy is evaluated as well  and
is scored using an average reward   the figure shows that for both of the problems the q values
converge  while the policies do not  for the tiger problem  the jumps in the policy evaluations can
be attributed to the small difference in value function surfaces between two actions as seen in fig    
despite the lack of policy convergence  the dqn can still learn very good policies as indicated
by the upward spikes in the average reward values  for both the belief approach and the history
approach  these policies were evaluated and compared to the sarsop policies and the pomdp
rl approach from      the results of the evaluations are shown in table    for the tiger problem 
the dqn approaches are able to perform just as well as the optimal sarsop policy  for the rock
sample problem  the belief dqn performs better than the history dqn  both the dqn approaches
outperform the pomdp rl approach 

 

conclusion and future work

this project introduced a novel approach of solving pomdps using a dqn  we demonstrated
that dqns can learn good policies  but require significantly more computational power  we also
showed that while the q values converge  the policies are sensitive to small perturbations  and do
not converge even after long training cycles 

 

fitable    average rewards per time step for the tiger and the rock sample problems using the sarsop 
dqn and pomdp rl policies  the rewards ere averaged over     trials 

tiger
rock sample

belief dqn

history dqn

sarsop

pomdp rl

         
         

         
         

         
         

         
         

the first area of future work would be to determine if the two problems examined in this project
have structure that leads to policy instabilities  or if these instabilities are present in all pomdp
problems  an approach to dealing with unstable policies is to try and stabilize the alpha vectors of
the pomdp  this can be done by using a policy gradient approach as opposed to using q learning 
another area of future work would attempt to improve the q value convergence rate by first creating
a two dimensional representation of the belief space or of the action observation history space using
a self organizing map  som   the output of the som would be used in a convolutional neural
network  which would take advantage of locality information in belief space 

 

acknowledgments

i would like to thank yegor tkachenko without whom this project would not have been possible 
he implemented the deep learning backend for this project and ran many of the experiments 

references
    v  mnih  k  kavukcuoglu  d  silver  a  a  rusu et al   human level control through deep reinforcement learning 
nature  vol       no        pp                    online   available  http   dx doi org         nature     
    m  hausknecht and p  stone  deep recurrent q learning for partially observable mdps  arxiv e prints  jul 
       online   available  http   arxiv org pdf           v  pdf
    j  schmidhuber  deep learning in neural networks  an overview  neural networks  vol      pp              
published online       based on tr arxiv            cs ne  
    h  kurniawati  d  hsu  and w  s  lee  sarsop  efficient point based pomdp planning by approximating
optimally reachable belief spaces  in robotics  science and systems  zurich  switzerland  june      
    j  baxter and p  l  bartlett  reinforcement learning in pomdps via direct gradient ascent  in in proc    th
international conf  on machine learning  morgan kaufmann        pp       

 

appendix  deep q network hyperparameters

the hyperparameters used for training the dqn can be found in table  
table    deep q network hyperparameters
hyperparameter

value

decription

max train iterations
minimbatch size
target network update
replay size
learning rate
initial exploration
 decay
max history

      
  
    
      
     
   
      
  

the maximum number of training samples generated
nnumber of training samples per update in stochastic gradient descent
frequency of updating the target network
size of the experience replay dataset
rate used by rmsprop
initial  value in  greedy exploration policy
rate at which  decreases
the maximum number of samples kept for action observation histories

 

fi