matching handwriting with its author
ziran jiang  aditya r  mundada

abstracthandwriting matching is a useful feature to identify
individuals and is used for many purposes including bank check
authentication and forensic investigation  this paper implements 
compares  and optimizes handwriting matching using two
algorithms  nave bayes and support vector machine  svm  
handwriting samples are collected using the inkredible app    
to obtain realistic samples similar to handwritings on paper     
writing samples are collected from each of the three authors
 authors a  b  and c   and each sample is scaled to four different
image resolutions  a preprocessing algorithm is developed using
matlab to convert the collected samples to black and white
and normalize the size of the handwriting  nave bayes and svm
algorithms are implemented using matlab to distinguish
between samples from authors a and b  svm is also expanded to
distinguish samples between all three authors  performance of
nave bayes and svm is compared  and the effect of image
resolution and preprocessing is also analyzed 

i  introduction
ach person has a unique handwriting  and this makes
handwriting a useful feature to identify individuals      
handwriting matching is used by banks for check writing
and signature authentication  in forensic science  handwriting
matching algorithm can aid handwriting analysis experts
predict the author with more accuracy  the goal of this project
is to manually implement and optimize handwriting matching 
including     data acquisition     image preprocessing    
algorithm implementation  nave bayes  svm  and svm for
three authors   and    algorithm comparison and
optimization  the input to the naive bayes and svm
algorithms are all the pixel values    for white  and   for
black  from the handwriting sample image  the output of the
algorithms is the prediction of the corresponding author 

e

ii 

algorithms  unlike typing in computer  handwritings requires
manual work  and generating a large number of handwriting
samples is very time consuming  a number of studies have
been done to create algorithms that will automatically generate
many artificial handwriting samples based on some original
authentic samples written by human                        and
     only require one original sample  and use a deformation
model to deform the original sample to generate many new
samples  this method however does not always generate
natural looking handwritings  the study done by      tries to
learn the natural variation from multiple authentic samples 
and create a distribution that describes the variation  it then
synthesizes new samples from this distribution  this method
requirs many original handwritings to have an accurate
distribution 
iii 

data acquisition

we considered using the mnist database     for
handwriting samples  however the mnist database samples
are not associated with the corresponding authors  for
handwriting matching  the algorithm needs to know the author
for each training samples  so the mnist database could not
be used as training or test samples  instead  as a temporary
measure at the initial stage of algorithm implementation  ms
paint was used to generate handwriting samples  later we
switched to using an app called inkredible      which allows
directly writing on a tablet screen using stylus finger  the
inkredible app enables collecting realistic handwriting
samples similar to the samples written on paper  each
handwriting sample was scaled to the following four
resolutions using matlab    x   pixels    x   pixels 
  x   pixels  and  x  pixels 
  x  

  x  

  x  

 x 

related work

although intuitively we know that handwriting is different
for every individual  the uniqueness of each persons
handwriting was studied and objectively validated by         
through a machine learning approach  handwriting features
can be divided into two categories  document examiner
features  and computational features       document examiner
features  such as handwriting embellishments  are often used
by forensic handwriting examiners and are difficult to model
using computers       used these document examiner features
and obtained promising results  computational features can be
easily modeled by machine learning algorithms  and are used
in          
another challenge of handwriting matching is generating a
large number of training samples for machine learning

fig     handwriting samples at different resolutions

    handwriting samples were collected from each of the
three authors  authors a  b  and c   and each sample was
scaled to the four different resolutions mentioned above  as
shown in fig     each author has different writing style  and
the algorithms attempt to predict the author based on these
differences  fig     only shows one sample from each author 
but within the     samples from the same author  there is also
certain variations from sample to sample  this sample tosample variation reflects the real world situation where a
person writes slightly differently each time  the features for

fithe algorithms are all the pixel values    for white  and  
for black  from the handwriting sample image 
author b

author a

author c

fig     handwriting samples from authors a  b  and c

iv 

image preprocessing

while there are many image pre processing techniques
depending on the condition of scanned handwriting  we
identified the following three preprocessing techniques as
crucial to the functionality of handwriting matching algorithm 
  
  
  

conversion of image to b w  each pixel will either be
   for white  or    for black  
handwriting size normalization 
background removal

we have implemented part   and part   of the preprocessing
algorithm in matlab  conversion to black and white is
achieved using the in built matlab function called
rgb gray  this function uses the luminance equation to
convert rgb pixels to grayscale numbers  the equation
used is 
      r         g         b     
the r  g and b in the above equation are respective color
channel values for any given pixel  once the image is
converted to grayscale  we use the bounding box method to
normalize our image  in this  the algorithm first finds a
lower and upper  row and column bounds to fit the image in
the smallest possible rectangle  the rectangle bounding box
is converted to a square by expanding the smaller side to
make it equal to the larger side  the new pixels that get
added as a result of this operation are initialized to white
color  note that the smaller side is expanded on either side
to automatically center the image  this bounded box image
can now be scaled to any pixel resolution using the
matlab function imresize  the default algorithm used
by imresize to scale the image is bi cubic interpolation 
there are other options available as well  such as  nearest
neighbor and bi linear interpolation  bi cubic interpolation
performs a weighted average computation in a  x 
neighborhood of the pixel thus resulting in a more
smoothened edge outputs for higher scaling factors as
compared to bilinear interpolation which works in a  x 
neighborhood  thus  it was our choice of algorithm 

fig    bounding box based size normalization

v  nave bayes algorithm
nave bayes algorithm was implemented to distinguish the
handwriting samples from author a and author b  we varied
the number of training samples  half and half from authors a
and b  and used     test samples     from author a and   
from author b   at each number of training examples  the
average generalization error was obtained by averaging the
generalization error collected over     runs  where each run
used different randomly picked training samples 
to make a prediction on a new test sample  we compare the
following equations     and     
p y     x   

         

p y     x   

         

  

  

    
    

in the equations above  y     means the author is a  and y    
means the author is b  since we always used half from author
a and half from author b for the training and test samples 
p y      p y           also 
p x y                        
p x y                        
here  n is the total number of pixels in an image sample 
therefore  in a sample of   x   pixels image  n         
      which implies  is the value of   pixel  to calculate
each          and           laplace smoothing was
used 
  

          

          

  

                
  

           
  
  

                
  

           

    

    

in equations     and      m denotes the total number of
  
training samples  so  means the value of  pixel in the  
training sample 
to model p x y   the nave bayes assumption assumes that
the  s are conditionally independent given y      this means
that for example we are assuming given the author is a  or b  
knowing the value of pixel i has no effect of our beliefs
about the values of pixel j  this nave bayes assumption
does not entirely hold true in this case  because we know that
for any handwriting  especially for the high resolution images 
if a pixel is black so that its part of the letter number  then the
adjacent pixels are also likely to be part of the letter number 
with this in mind  the nave bayes algorithm was
implemented and the generalization error results were
obtained 

finaive bayes generalization error
using size normalization

  error   x  

  error   x  

  error   x  

  error   x  

  error   x  

  error  x 

  error   x  

  error  x 

  

  

  

  
generalization error    

generalization error    

naive bayes generalization error
without size  normalization

  

  
  
  
  
 

  
  
  
  
  
  
 
 

 
 

  

  

  

  

 

   

  

  

  

  

   

   

number of training samples

number of training samples

fig     nave bayes generalization error without size normalization and centering fig     nave bayes generalization error using size normalization and centering

refer to fig    for the generalization error of nave bayes
using size normalization and centering  the results were
worse than without using size normalization and centering for
all image resolutions  size normalization and centering did not
improve the performance of nave bayes  we think this is
because size normalization and centering actually reduced the
difference of the writing samples from the two authors  for
example  if one author generally writes big and the other
author have smaller handwriting  this difference will be
reflected in the raw image  nave bayes algorithm then picks
up this difference in the          and         
calculation to make the prediction  however if the images are
size normalized  then the writing samples from the two
authors will be scaled to similar sizes  and they will become
more similar compared to the raw images  with more
similarity  the prediction will become less accurate  and this is
why the generalization of nave bayes algorithm increased
when using the size normalization and centering 
naive bayes effect of size normalization and centering

average generalization error    

refer to fig    for the generalization error of nave bayes
without size normalization and centering  the   x   images
resulted in the lowest generalization error of     followed by
  x           x          x        images  the higher
resolution samples    x   and   x    had higher
generalization error compared to the   x   images because
the number of training samples were not enough to generate
an accurate result  for example a   x   image has     
features  pixels   and     training samples were not sufficient
compared to the number of features in each image  for the
  x   training images  the number of feature is            
which is comparable to the     samples  as for the  x 
samples  the performance was worse than   x   images
because they lost too much of the original characteristics of
the handwriting  however  note that even at the lowest
resolution of  x  pixels  the algorithm still achieved    
generalization error  this is impressive considering that an
 x  resolution image has such a low resolution that it is hard
to recognize even by human eyes  as shown in fig    

without size normalization and centering
using size normalization and centering

  
  
  
  
 
 

  

  
 

  x  

 

  

 

  x  

 

  x  

 x 

different resolutions using     training samples

fig     nave bayes effect of size normalization and centering

fivi 

support vector machine  svm  algorithm

since we are evaluating supervised learning algorithms  a
discussion without svm is incomplete  we used an off the
shelf svm training algorithm provided by matlab to
understand if we can classify images to their respective
writers  svm is modelled using the primal optimization
problem     


 
 
          
    
  

  

   

       

                    

fig    shows the generalization error for svm without sizenormalization and centering  the   x   images had the lowest
generalization error of     followed by   x            x  
        x         the   x   images had the lowest
generalization error at     training samples  but the   x   and
  x   images had results flattening between    to     training
samples  for the   x   and  x  images  the generalization
error was too high and the results were not useful to make a
meaningful prediction  this result is intuitive as well since we
lose too much information as we reduce the image resolution 
at  x  resolution  the information loss is so much that it is
impossible to distinguish between two images 

              





 
max                              

 
  

   

                
    



svm generalization error using size normalization

average generalization error    

here   denotes the weight matrix      are our samples
 images       is the class label  c is a parameter that does
relative weighting of the twin goals of minimizing the
functional margin and ensuring that all samples have
functional margin of at least    and lastly   is the quantity by
which the functional margin for a sample may be less than  
which results in an extra cost of c   after writing the
lagrangian for the above problem and setting the partial
derivatives of lagrangian w r t  and b to zero  we get our
dual optimization problem     

  error   x  

  error   x  

  error  x 

  
  
  
  

  
  
 
 

         

  error   x  

  

  

  

  

   

number of training samples

  

the svmtrain function from matlab solves the above
optimization problem and calculates the values of all the
parameters  we used a linear kernel for our svm classifier 
again  we had     samples from each author which were split
into    training samples and    test samples  we have
compared the performance of the algorithm over various
image resolutions 

average generalization error    

svm generalization error without
size normalization
  error   x  

  error   x  

  error   x  

  error  x 

  

  

  
  
  
  
  
  
 

 

  

  

   

number of training samples

fig    svm generalization error without size normalization and centering

fig     svm generalization error with size normalization and centering

fig    shows the generalization error for svm using sizenormalization and centering  the performance was improved 
especially for the low resolution images    x   and  x    the
  x   and   x   images showed slight improvement as well 
the results may seem odd but the key point to note here is that
size normalization and centering brings uniformity amongst
chaos in the low resolution images  when compressing the
original image  there is no control over how the pixel
information is truncated  the compression may lead to loss of
information at different areas in the training and test image
which will make it difficult for the test image to be identified 
when size normalization is applied  this indeterminate loss of
information is curbed to a certain extent since whatever the
dimensions of the text  we center it and then scale it  this
effect will be more pronounced in the samples where the text
is present near one of the corners or edge of the image 

fiaverage generalization error    

svm effect of size normalization and centering

  

  
  
  

  
   

    

 

  x  

  x  

  

 

  x  

 x 

different resolutions using     training samples

without size normalization and centering
using size normalization and centering
fig     svm effect of size normalization and centering

  of training
samples
 
 
 
 
  
  
  
  
  
  
  
  
  
   

  x  

  x  

  x  

 x 

legend
nave bayes  no size normalization and centering
nave bayes  with size normalization and centering
svm  no size normalization and centering
svm  with size normalization and centering

fig     the best algorithm at different number of training samples
and image resolutions 

vii 

svm algorithm for three authors

svms typically classify data in two classes using a
separating hyperplane  however  the handwriting recognition
is a multi class problem  to solve this  we extend the svm
algorithm to a multi class algorithm     by computing one
classifier for each class by pitting that class against all other
classes  therefore  if we have k classes in our sample space 
we would need to compute k classifiers  to classify a new
sample  we evaluate the new sample against each classifier
and choose the class whose corresponding classifier labels it
as    this is otherwise called one against all approach 

generalization error    

generalization error    

  

      

  
  

at  x  resolution  between    and    training samples 
svm performs slightly better than nave bayes  but the
difference not significant  if the training samples have high
resolution similar to   x   pixels  if there are very few
training samples         using nave bayes without sizenormalization and centering is still the best option  only for
  x   images with    or more training samples  svm
outperforms nave bayes  this result is synonymous with the
results obtained in the class  nave bayes is quicker to learn
while performance of svm improves as the number of
training samples increases  although svm had a narrower
range of good performance compared to nave bayes  svm
achieved the absolute lowest generalization error of    with
  x   training samples and using size normalization and
centering 
ix 

 

 

     

 
 
  x  

  x  

  x  

 x 

image resolution

fig     multi class svm generalization error with size normalization and
centering

viii 

conclusion

we compared the performance of the four combinations 
nave bayes svm  with without size normalization and
centering  at each image resolution and training sample size 
the combination that gives the lowest generalization error is
illustrated in fig      based on this result  if the handwriting
image resolution is   x   pixels or lower  using nave bayes
without size normalization and centering generally has the
best performance 

future work

handwriting matching algorithms  such as this  are typically
used by banks and law firms for signature matching to detect
potential fraud or establish authenticity  our signatures
typically do not reflect our actual handwriting  i e  there is a
significant difference between the handwriting style of a
written paragraph and a signature from the same person  in
addition to this  the size of the signature varies from document
to document which adds to the complexity of problem  as
future work  the challenge would be to extend this algorithm
to signatures  the size normalization algorithm introduced in
this project needs to be modified and extended to normalize
the size of signatures not only from the same person  but
across different people as well  another challenge would be to
increase the learning rate  that is lower generalization error
for lesser number of samples  this is an important aspect
because typically organizations have to work with very few
samples to determine if the signature is authentic or not 
in order to work with very few samples  it will be helpful to
automatically synthesize a large number of training samples
based on the real signature samples  as explored in      an
algorithm can be developed to naturally deform the original
samples to generate many more training samples 

fireferences
   
   
   
   

   
   
   
   

   

    
    

    
    

    

    

    

    

    

inkredible  online   available  http   inkredibleapp com 
the mnist database of handwritten digits  online   available 
http   yann lecun com exdb mnist 
literature
on
multi class
support
vector
machines
http   nlp stanford edu ir book html htmledition multiclass svms   html
ideas on how to implement multi class support vector machines http   www codeproject com articles        handwriting recognitionrevisited kernel support v
https   en wikipedia org wiki edge detection
image
boundary
detection and isolation 
background subtraction techniques  a review  massimo piccardi
a  ng  cs    lecture notes  in cs     machine learning  class
zheng  y     doermann  d         august   handwriting matching and
its application to handwriting synthesis  in document analysis and
recognition        proceedings  eighth international conference on  pp 
          ieee 
srihari  s   zhang  b   tomai  c   lee  s   shi  z     shin  y  c        
april   a system for handwriting matching and recognition  in proc 
symposium on document image understanding technology  pp        
srihari  s  n   cha  s  h   arora  h     lee  s          individuality of
handwriting  journal of forensic sciences                 
wang  j   wu  c   xu  y  q     shum  h  y          combining shape
and physical modelsfor online cursive handwriting synthesis 
international journal of document analysis and recognition  ijdar  
              
bunke  h         august   generation of synthetic training data for an
hmm based handwriting recognition system  in null  p        ieee 
mori  m   suzuki  a   shio  a   ohtsuka  s   schomaker  l  r  b    
vuurpijl  l  g         september   generating new samples from
handwritten numerals based on point correspondence  in proc   th int 
workshop on frontiers in handwriting recognition  pp           
chui  h     rangarajan  a          a new point matching algorithm for
non rigid registration  computer vision and image understanding 
               
pervouchine  v     leedham  g          extraction and analysis of
forensic document examiner features used for writer identification 
pattern recognition                   
srikantan  g   lam  s  w     srihari  s  n          gradient based
contour encoding for character recognition  pattern recognition        
          
cha  s  h     srihari  s         september   multiple feature integration
for writer verification  in proc   th int  workshop on frontiers in
handwriting recognition  pp           
statistics and machine learning toolbox  matlab 

fi