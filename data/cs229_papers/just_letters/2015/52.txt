machine learning in automatic music
chords generation
ziheng chen

jie qi

yifei zhou

department of music
zihengc stanford edu

department of electrical engineering
qijie stanford edu

department of statistics
yfzhou stanford edu

i  i ntroduction
melodies and chords compose a basic music piece 
assigning chords for a music melody is an important step
in music composition  this project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces 
in real music chord assignment process  to choose
which chord to use for a measure  musicians normally
consider the notes in this measure and how chords
are progressed  so our aim is to learn the relationship
between notes and chords  as well as the relationship
between adjacent chords  and use that to assign chords
for a new melody 
the input to our algorithm is a music piece with
several measures  we then try different models to output a predicted chord for each measure  we will use
basic models taught in the class  logistic regression 
naive bayes  support vector machine  as well as some
advanced models  random forest  boosting  hidden
markov model   and compare their performance for our
problem 
ii  r elated w ork
there have been some previous works using machine
learning techniques to generate music chords  cunha and
ramalho    set a neural network and combined it with
a rule based approach to select accompanying chords
for a melody  while legaspi et al     use a genetic
algorithm to build chords  chuan and chew    use a
data driven hmm combined with a series of musical
rules to generate chord progression  simon et al     use
hmm and    types of chords to made an interactive
product  which would generate chord accompaniment for
human voice input  paiement et al     use a multilevel
graphical model to generate chord progressions for a
given melody 

iii  dataset
a  data source
we collected    lead sheets as our dataset  the chosen
lead sheets have several properties  a  virtually each
measure in the dataset has and has only one chord  b  the
chords in the dataset are mainly scale tone chords  which
are the most basic and commonly used chords in music
pieces  the data we use are in musicxml format  which
is a digital sheet music format for common western
music notation  measure information can be extracted
from this format by matlab 
b  data preprocessing
to simplify our further analysis  we did some preprocessing of the dataset before training 
   the key of each song is shifted to key c  the
key of a song can determine the note and a set of
common chord types the song uses  a song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally  without affecting its
subjective character  therefore  without any loss
of music information  we can shift the key of each
song to key c to make the dataset more organized
as well as decrease the number of class types
 chord types  in training process 
   the chord types  class types  are restricted only
to scale tone chords in key c  which are   types 
c major  d minor  e minor  f major  g major  a
minor  and g dominant  other types of chords  like
c dominant  g suspended  etc   are transformed to
their most similar scale tone chords 
   some measures in the dataset have no chords or no
notes  like rest measure   we simply delete these
measures from the dataset 
   a small number of measures in our dataset are
assigned two chords continuously to accompany

fidifferent notes in the measure  to simplify the
learning process  we regard the second chord as
the chord of this measure and delete the first one 
from an audiences perspective  it will sound better
than deleting the second chord 
after preprocessing  the    lead sheets we chose have
    measures in total 
the   chord labels are shown in table i  they are
the labels we are trying to assign  now  our project turns
into a multi class classification problem 
label
chord

 
c

 
dm

 
em

 
f

 
g

 
am

fig     forward feature selection result

 
g 

we can see the accuracy does not improve much after
  features  finally  we selected the next   features in our
following analysis  the note pitches on the   beats and
the   longest note pitches 

table i  labels for different chord types
iv  f eature s election
we started by extracting some indicative features  the
basic units for prediction are the notes inside a measure 
then we chose our initial features from the following
three aspects 
   note itself  whether a note is present in the measure  take value of   or    a chord is influenced by
the notes that appear to create a sense of harmony 
   note vs  beat  the notes on the beat in the measure
since chords should accompany the beat tones 
   note vs  duration  the longest notes in the measure
since the long notes need to be satisfied by the
assigned chords 
to represent a note in a measure  we quantified it as
shown in table ii  it is labeled   to     note that
we dont consider which octave the music note lies in 
since the octave basically will not affect the chord type
we choose  e g  c  and c  are both labeled    
label
note
label
note

 
c
 
f  gb

 
c  db
 
g

 
d
 
g  ab

 
d  eb
  
a

 
e
  
a  bb

 
f
  
b

table ii  labels for different note types
we chose notes on the     beats and   longest notes 
in addition to the    type     features  we initially have
   features 
to get the true effective features  we ran forward
feature selection based on random forest  which will
be discussed further in the next section   by adding one
feature at a time and selecting the current optimal one 
the result is shown in figure   

v  m odels
a  logistic regression
we used multinomial logistic regression  i e   softmax
regression in r  with the log likelihood as
     

m
x

log

i  

k
y

t

x i 

el
pk

j   e

l  

   y i   l 

   

lt x i 

we could achieve the maximum likelihood estimate
by using newtons method 
      h       

   

since our feature size is small  n      and it is easy to
compute the inverse of the hessian 
b  naive bayes
we applied naive bayes by using the statistics and
machine learning toolbox in matlab  we used the
multinomial event model with multiple classes  for any
class c  the maximum likelihood estimate gives
 i 
 i 
j     xj   k  y
 i    c n
i
i     y

pm pni
k y c  

i  

pm

pm
y c  

i     y

 i 

m

  c 

  c 

   

   

we then made the prediction on the posterior computed with the probabilities above 

fic  support vector machine  svm 

e  boosting

svm is one of the supervised learning algorithms
which is to maximize the distance between training
example and classification hyperplane by finding  use
hard margin as an example  

boosting is a meta algorithm to learn slowly to fit
the new model from the residual of the current model 
its parameters include the number of trees to split  the
shrinkage parameter and the depth of the tree     we also
applied this model in r 
specifically  we tune these parameters using cross validation to make sure the model has the best performance 
as a result  the number of trees is      the shrinkage
parameter is     and the depth is   

min

 w b

 
  w   
 
s t  y  i   wt x i    b      i           m

   

by using kernel trick  svm can efficiently perform
non linear classification or data with high dimensional
features  in this problem  we tried the following three
kernels in r 
   linear kernel
k xi   xj    

p
x

xip xjp

   

k  

   polynomial kernel
k xi   xj    

  

p
x

 d
xip xjp

   

 xip xjp   

   

k  

   radial kernel
k xi   xj     e

pp

k  

d  random forest
random forest is based on bagging which is a kind
of decision tree with bootstrapping and can decrease
variance  for a classification problems with p features 

p features are used in each split in order to decrease
the correlation of the trees     we applied this model
using r package 
after applying the model  we can generate the importance plot for the features as the figure below  all the
features we are using have the similar importance level 
which reinforces the conclusion from feature selection 

f  hidden markov model  hmm 
up till now  these five models above make prediction
based on the information of a single measure  we wanted
to incorporate the relationship between measures  so  we
also tried hmm to make prediction based on a sequence
of measures 
in hmm  the system is being modeled to be a
markov process which has a series of observed states
x    x    x         xt   and a series of related unobserved
 hidden  states z    z    z         zt    t is the number
of the states   suppose there are s types of observed
states and z types of hidden states  an s  s transition matrix denotes the transition probabilities between
adjacent hidden states  and an s  z emission matrix
denotes the probabilities of each hidden state emit each
observed state  given an observed series of outputs  if
we know the transition matrix and emission matrix 
we can compute the most likely hidden series using the
viterbi algorithm    
in our chord assignment problem  the first note of each
measure was considered as an observed state  and the
chord of each measure was considered as a hidden state 
using our dataset  we computed the transition probability
and emission probability and formed transition matrix
and emission matrix  and then we computed the most
likely chord progression using hmmviterbi function in
statistics and machine learning toolbox in matlab 
vi  r esults   a nalysis
a  cross validation

fig     variable importance

we started at comparing different machine learning
models to indicate how they perform  we used hold out
cross validation and     of the data as the validation
set  in addition  we also tried k  fold cross validation 
the cross validation results are used to represent the test
accuracy and evaluate how our models perform 

fib  prediction on a single measure
the cross validation accuracy for the five different
models we used is shown in table iii   k        
for k  fold 
model
logistic regression
naive bayes
svm linear
svm poly
svm radial
random forest
boosting

       
      
      
      
      
      
      
      

   fold
      
      
      
      
      
      
      

  fold
      
      
      
      
      
      
      

bias low variance classifiers  e g   naive bayes  could
have a better performance  however  naive bayes also
assumes that the features are conditionally independent 
in reality  the four notes on the beat could influence each
other  so such strong assumption could limit the performance of naive bayes  random forest and boosting
perform best in our case since they are both complex
and have a reduction in model variance 
c  prediction on sequential measures
the following is the song msilent night  that is
assigned chords by our hmm model 

table iii  accuracy for different models
to get an insight of the results above  we can have
a look at the confusion matrix  take svm with radial
kernel as an example 


              
              


              


               
   


              


              
             
in the confusion matrix  the rows represent the prediction results  the columns represent the true labels and the
diagonal values are the correct predictions  we can see
the data is highly imbalanced  i e   most of the labels are
  or    in fact  for key c  the frequent chords are exactly
c major     and f major      this data distribution will
cause our models to suffer 
now lets visualize the accuracy results in the figure
below 
fig     generated lead sheet based on hmm predictions

fig     visualization of accuracy for different models
as the figure shows  random forest and boosting are
the most solid models  apart from the imbalanced data 
logistic regression and svm with linear kernel have bad
performance  mainly because the relation is highly nonlinear and these models cannot fit this dataset while high

the result we get for hmm varies greatly for different
songs  the overall accuracy of hmm is        but for
some pieces  it can achieve an accuracy over      this
could be caused by the limited information provided by
the first note pitch observed  to add more pitches and
regard a group of notes in the measure as an observation 
the result can be improved but it will greatly complicate
the model with a much larger state space 
the second reason is that  assigning chord is a more
subjective than objective process  two composers may
choose different chord types for the same measure and
both of them can sound pleasant  there is no single norm
to decide if the chord is assigned correct or not 

fivii  c onclusion   f uture w ork
from the results above  we can conclude that random
forest and boosting perform best in prediction with
single measure  hmm could also achieve a good result
if we include more information as our observation states 
but the highest accuracy we can achieve is only about
     this is caused by the subjectivity in our dataset 
this can be improved by choosing data from one
composer with one genre  however  it is typically hard
to achieve  a better way is to design an experiment to
use human judgements to evaluate the performance of
our model instead of only relying on the given labels 
r eferences
    cunha  u  s     ramalho  g          an intelligent hybrid model
for chord prediction  organised sound                 
    legaspi  r   hashimoto  y   moriyama  k   kurihara  s    
numao  m         january   music compositional intelligence
with an affective flavor  in proceedings of the   th international
conference on intelligent user interfaces  pp            acm 
    chuan  c  h     chew  e         june   a hybrid system for
automatic generation of style specific accompaniment  in  th intl
joint workshop on computational creativity 
    simon  i   morris  d     basu  s         april   mysong 
automatic accompaniment generation for vocal melodies  in
proceedings of the sigchi conference on human factors in
computing systems  pp            acm 
    paiement  j  f   eck  d     bengio  s          probabilistic
melodic harmonization  in advances in artificial intelligence
 pp            springer berlin heidelberg 
    james  g   witten  d   hastie  t     tibshirani  r          an
introduction to statistical learning  p      new york  springer 
    viterbi  a  j          error bounds for convolutional codes
and an asymptotically optimum decoding algorithm  information
theory  ieee transactions on                 

fi