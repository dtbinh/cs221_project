predicting yelp star ratings based on text
analysis of user reviews
junyi wang
stanford university
junyiw stanford edu
abstract
we perform sentiment analysis based on yelp user reviews  we treat a yelp star rating of   or   as a positive
sentiment and a rating of      or   as a negative one  various language models are used to obtain feature vectors
and we implement three different algorithms  namely perceptron learning algorithm  naive bayes and svm to
predict sentiment  the performances of these three algorithms on this binary classification problem are discussed
in this paper 

i 

introduction

input    text    what a cool bar restaurant   i will
no doubt be visiting again  the service and prices were
great   and the restrooms were clean  i had the buffalo
chicken sandwich and it was delicious  the menu consists of typical bar food  however  theres a few different
items on there which stand out on the menu  a cool
bar off the beaten path that is a worth a trip  cheers  
   

elp has been one of the most popular sites to
find great local business over recent years  people write reviews and give a star rating ranging
from   to   based on the service quality and their
own personal preferences  the average star rating is a
very important indicator of the quality and popularity
of each business  on the other hand  text data are
ubiquitous and play an essential role in a variety of
applications  fortunately  yelp has provided a large
dataset containing information about users and businesses  we use the user reviews in the given dataset
and build prediction models for ratings  specifically 
we want to predict whether a user is conveying a positive or negative feedback simply by looking at the
review texts alone 

y

ii 

output   sentiment      

iii 

related work

kou and wu tried building models to predict yelp star
rating and used the root mean square error  rmse  as
the error metric      the best prediction performance
with a        rmse is produced by ridge regression
with tf idf feature selection method  li and zhang
worked on both binary and   class classification problems to predict the star rating      they used the meansquare error  mse  for evaluation and concluded that
the best model is to use support vector regression
removing stop word with a test mse of        on the
binary classification problem  xu  wu and wang also
looked into the   class classification problem and did
binary sentiment analysis      their results showed that
the perceptron algorithm has better performance than
multi class svm and nearest neighbor on precision
and recall  and the prediction results are good for a
star rating of   and    but relatively poor for a rating
of       and   

task definition

the goal of the project is to apply machine learning
algorithms to predict the yelp star rating based on the
user review  since the star ratings are not linear  classification results are expected to perform better than
regression  to simplify the problem and the implementations of algorithms  we treat a star rating of   or  
as a positive feedback and a rating of      or   as a
negative one  basically the input is some text of user
review  and we want to output a binary value  with
   indicating a positive feedback and    denoting a
negative feedback  an example would be as follows 
 

fiiv 
i 

dataset and preprocessing

preprocessing

the dataset used in this project is provided by the yelp
dataset challenge      the dataset includes five json
files containing information about business  checkin 
review  tip and user  specifically  we use the review
dataset and extract the user review and star rating information  as the original review dataset is quite large
with         reviews  we collect a random sample of
size        then we split the       examples by randomly selecting      of them to be the training data
and the rest      as the test set 

ii 

figure    procedure of getting feature vectors

the perceptron learning algorithm and naive bayes
are implemented in python  and we use the scikit learn
library to implement svm      in terms of common
symbols  we define the set to be                stop words
and word stems are obtained by using the natural
language toolkit library in python     

error metric

we use precision and recall as the error metric 
precision  
recall  

tp
tp   fp

i 

let h be a threshold function defined as
 
 
if  t x   
t
h  x    
  if  t x    

tp
tp   fn

where tp  fp  fn stand for the number of true positives  false positives and false negatives 
in terms of training error and test error  they are
defined similarly as

training  test error  

where x is a vector of features and  is the corresponding weight vector 
we apply perceptron learning algorithm and run
stochastic gradient descent on the weight vector  using
hinge loss until convergence 

im     y i     yi  
m

       max           x  y  

where m is the total size of the training test data 
y i  and yi stand for the predicted sentiment and the
true sentiment on the i th example  respectively 

v 

perceptron learning algorithm

table    results of perceptron learning algorithm for various language models

methods

we apply three supervised learning algorithms for prediction  namely perceptron learning algorithm  naive
bayes and svm  to obtain feature vectors  we use different language models  including basic unigram  basic
bigram and character based n gram  we can run cross
validation on different sizes of n  and find that the optimal n is    as a result  for examining the performance
of character based models  we use the character based
  gram  we have also tried removing common symbols 
removing stop words and stemming  the high level
procedure is shown in figure   

language model

training test ererror
ror

precision

recall

basic unigram
basic bigram

     
     
     

     
     
     

     
     
     

     
     
     

unigram
removing
stop
words
and
common symbols

     

     

     

     

stemming with stop
words and common
symbols removed

     

     

     

     

character based
  gram

     

     

     

     

unigram
removing
common symbols

 

fitable    results of naive bayes for various language models
language model

training test ererror
ror

precision recall

basic unigram
basic bigram

     
     
     

     
     
     

     
     
     

     
     
     

unigram
removing
stop
words
and
common symbols

     

     

     

     

stemming with stop
words and common
symbols removed

     

     

     

     

character based
  gram

     

     

     

     

unigram
removing
common symbols

figure    comparison of different language models for
perceptron learning algorithm

specifically  we run cross validation on the learning
rate   and use          since it gives the lowest test
error 

ii 

naive bayes

once we get the feature vectors from the training examples  we can compute the overall probability of a positive review p y      and the probability of a certain
feature xi appearing in a positive review p  xi  y      
similarly for the negative case  suppose the total feature size is n  then we can compute the posterior
probability by bayes rule

figure    comparison of different language models for
naive bayes

p   y      x    

iii 
 in  

svm

we define a hyperplane by

 in   p  xi  y       p y     
p  xi  y       p y         in   p  xi  y       p y     

  x   f   x     x t           
where           and x is the feature vector  the
optimization problem of svm can be expressed as

we check whether p y      x   is larger than      and
give the corresponding prediction result  for the implementation of naive bayes  we take the logarithm of
the likelihood function and use summation for computation to avoid very small number close to   produced
by the product of a series of conditional probabilities 
we also use laplace smoothing on the prior probability
for features that never appear in the training set  the
prediction results are summarized in table   

n
 
min          c  ei
     
i   

subject to
ei     yi   xit             ei i
 

fiwhere yi is the response variable indicating the sentiment of the i th example in the training set  and c is
the penalty parameter of the error term 
we use the svc function in sklearn library with a
linear kernel and experiment with various values of c 
we find that the test error for c       is the smallest 
so we use c       throughout the process of trying
different language models and get the following result 

prediction results  the precision for perceptron learning algorithm typically ranges from     to      and the
recall from      to       for comparison  the precision
for svm is around      and the recall varies from     
to      
notice that the recall for using naive bayes is really
low  while the precision is very high  this means that
the total number of false negatives is high  one possible reason can be that p y      is high  meaning that
the overall probability of getting a negative feedback
is higher than the probability of getting a positive one 
another possible explanation is that people who give
a negative feedback tend to write longer reviews  so
for each feature in the test set  the probability that it
appears in a negative review is higher than the chance
that it appears in a positive one 
in terms of all the language models that we have experimented  stemming with stop words and common
symbols removed gives the best prediction result  it
produces the smallest test error of       if we apply
the perceptron learning algorithm  the result is expected as stemming and removing common symbols
reduce dimension and the chances of multicollinearity by removing sets of similar features  removing
stop words improves the model by getting rid of nonsignificant features  in addition to perceptron learning
algorithm  svm with features generated by characterbased   gram also produces a reasonably good test
error of       
one thing worth pointing out is that for all the
three algorithms that we have discussed  the training
error is much lower than the test error  this suggests
that we might have overfitted the data  to further improve the performance on the test set  we need to add
regularization terms and run cross validation on the
regularization parameter 

table    results of svm for various language models
language model

training test ererror
ror

precision recall

basic unigram
basic bigram

     
     
             
              

     
     
     

     
     
     

unigram
removing
stop
words
and
common symbols

              

     

     

stemming with stop
words and common
symbols removed

    

      

     

     

character based
  gram

 

     

     

     

unigram
removing
common symbols

figure    comparison of different language models for
svm

vi 

vii 

conclusion and future work

in this paper we experimented perceptron learning algorithm  naive bayes and svm to predict sentiment
as reflected by the yelp star rating  various language
models have been used to extract the features from
the text of yelp user reviews  to sum up  the perceptron learning algorithm generally produces the best
prediction results and the best language model is using stemming and removing stop words and common
symbols 

results and discussion

we can compare the results in table      and    generally the perceptron learning algorithm has the best
performance on various language models  svm has
comparable performance with perceptron learning algorithm  while naive bayes does not produce good
 

fi    chen li  and jin zhang  prediction of yelp review
star rating using sentiment analysis       

for future work  we would try regularized perceptron learning algorithm and naive bayes to see if that
will reduce the test error  we would also explore the
  class classification problem  other classification methods and more advanced language models  as mentioned in the section of related work  xu  wu and wang
did not get high precision and recall for the   class classification as it is a much more complicated problem
than binary classification that we have discussed in this
paper  as a result  it is difficult to compare the our
results and theirs 

    yun xu  xinhui wu  and qinxia wang  sentiment
analysis of yelps ratings based on text reviews 
     
    data source 
challenge

http   www yelp com dataset 

    scikit learn library documentation 
  scikit learn org stable modules 
generated sklearn svm svc html

references
    jiyou kou  and luyan wu  building prediction models on yelp dataset       

http 

    nltk library documentation  http   www nltk 
org 

 

fi