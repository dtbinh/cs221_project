 

multifaceted predictive algorithms in
commodity markets
blake jennings  hunter ash  vineet ahluwalia

i 

i ntroduction

ii 

data and f eatures

a  feature sets
commodity price prediction is a notoriously difficult
task  and fluctuations in this space have large social 
economic  and political consequences  as the recent
precipitous drop in oil prices has demonstrated  the
ability to predict the movements of commodity prices is
highly consequential  a successful prediction algorithm
could inform a company on how to hedge against risk
or form the basis for a profitable algorithmic trading
strategy 
in this project  we employ logistic regression  support vector machine  and naive bayes classification
algorithms  the input to each model is a collection of
time series including technical indicators  macro market
features  and other commodities  each model outputs
a vector v        n   in which the nth coordinate
indicates whether a given commodity index will have
achieved net positive return n           n   days in the
future 
we begin our exploration with the findings of jennings commodity etf prediction project  stanford
ms e      which demonstrate that models constructed
on feature spaces consisting of solely price data and
corresponding momentum indicators are insufficient for
predicting price movements  this agrees with the conclusions of kinlay  who ran           different machine
learning algorithms using only price data and derivative
features and found that none of the models predictions
were distinguishable from noise on out of sample data
    
kinlay published his           models paper in response to research like that published by ticlavilca et al
    and huang     which seems to indicate that machine
learning on technical indicators can produce useful and
reliable results  kinlay suspected that the success of
many of these researchers is due to sampling bias  and
thus performed his exhaustive           model test 
the research by kase     and jegadeesh     demonstrating the profitability of momentum based trading
strategies may have inspired recent attempts by machine
learning researchers to use feature spaces consisting
only of technical indicators 
mindful of this prior work  we conduct an iterative
process of feature engineering and exploratory analysis to construct an improved input feature space  and
show that our model implementations can successfully
generalize to out of sample data 

in jennings project  he pulled data on over   
commodity exchange traded funds from yahoo finance
from      to       and constructed a feature set for
each commodity consisting solely of technical indicators on that commodity   primarily simple  volumeweighted and exponential moving averages  performing
recursive feature elimination with logistic regression on
these data sets yielded the following average testing and
training accuracies 

seeing that the model was unable to generalize  we
decided to gather daily data comprised of both technical
and macro market features ranging back to       for all
trading days   this data set  obtained from bloomberg
     contains      samples of    features  including the
kc  coffee futures index  which we attempt to predict 
the macro market features include indices such as the
s p     us treasury yields  and a number of different commodities  the technical features are various
momentum indicators which quantify the relationship
between recent price changes in a given window and
the long term trend of the price of an instrument 
our findings  presented in the following sections  are
obtained using this feature space and the direction of
the kc  index as the response variable 

b  principle component analysis
methodology  to explore our expanded feature space 
we perform principle component analysis and look at
the top   components  for each of these components 
we take the   most heavily weighted features  highest
absolute value of the coefficients  

fi 

results  we found that the first   components
explained     of the total variance  the   highestweighted features for each of them are listed below 

the net change t k   k days in the future is then given
by

t k  
feature  
feature  
feature  

pc 
usgg yr
usgg  yr
fdfd

pc 
ussp  
lei bp
conssent

k
x

t i

i  

the response variable on day t is an indicator function
on t k being positive 


the variance explained per component  as well as the
cumulative variance explained  are plotted below 

directionk  t  

 
 

t k    
t k   

a  logistic regression
we first investigate logistic regression  in which we
fit a hypothesis of the form
h  x   

 
    e t x

to our data by minimizing the cost function
m
 
  x   i 
x  h  x i   
m i  

j    

using the coordinate descent algorithm  which iteratively
updates each coordinate of  according to the rule
k    k 

j
 
k

b  support vector machine
we also implement a support vector machine binary
classifier  which is the solution of the optimization
problem
 
  w   
 w b  
s t  y  i   wt x i    b    
min

iii  m ethods
we now present the definitions and specifications of
the models we employ  first  we show the how we
calculate our response variables 
computing response variables
to compute our response variables  we take the first
difference of the kc  commodity column 


  

    
  
  
   
 
 p 

 

 t 



pt    pt  t   






p
 t   
pt    pt    t   






pt   
  
  


  
 
 
 

for i              m   this is a convex optimization
problem solvable using quadratic programming
methods  since this problem is kernalizable  we try
classifying with the following two kernels 
rbf
exp   x i   x j        
sigmoid


tanh  x i   t x j 

fi 

c  naive bayes
for our naive bayes model  we find the parameters
which maximize
m
y
l    
p x i    y  i     
i  

averaging over all windows  we have the following
confusion matrix for this test 

actual negative
actual positive

predict negative
   
   

predict positive
   
   

or equivalently
     

m
x

log p x i   y  i    x     log p y  i    y  

i  

for our implementation  we assume a bernoulli distribution for p x y  
p xj  y    p j y xj       p j y      xj  
where x is a training example and j is the index of one
of our features 
iv  r esults
we train    models for each of our four classification
algorithms  corresponding to every prediction window
from   to    days in the future  we test each of our    
models using both a     training       testing method
and a time series version of   fold cross validation 

from which we compute
precision        
recall        
accuracy        
we also obtain an auc score of         we see that
our models gain their predictive edge primarily from
correctly identifying upward price movements  we also
see that models trying to predict windows of less than   
days do not reliably generalize  and so going forward we
restrict ourselves to predicting between    and    days
in the future    fold cross validation for these windows
yields the following accuracies 

the prediction windows are the number of days
into the future for which we predict a net positive or
negative change in the kc  index  our time series
  fold cross validation method involves first splitting
the data into tenths  then training on the first     of
the data  chronologically ordered  and predicting the
next      then training on the first     and testing on
the next      etc  therefore  we have   training testing
pairs  and the reported accuracy is the average over
all pairs  for        we simply train on the first    
of the data  again ordered chronologically  and test on
the remaining     
a  logistic regression
we first test our logistic regression model over all
prediction windows  using the       method 
the accuracy generally increases with window size 
up to a size of approximately       days  this general pattern makes sense  increasing the window size
smooths out some of the volatility in the price  but
eventually starts rendering the information accessible to
the algorithm obsolete 

b  support vector machine
we consistently obtain above     testing accuracy
results         for our support vector machine with an
rbf kernel 

fi 

our accuracies under   fold cross validation are
similarly distributed 
d  summary
the following two graphs summarize the mean prediction accuracies for       and   fold cross validation 
respectively 

similarly to logistic regression  there is a clear 
roughly linear relationship between prediction window
size and accuracy  especially with cross validation  this
trend is more pronounced with the svm  we also ran
our svm with a sigmoid kernel  obtaining a similar
trend  but lower average accuracy  we experimented
with a linear kernel  but the increase in run time was so
dramatic that we abandoned this route 

c  naive bayes
like our logistic regression and svm models  our
naive bayes model consistently attains over     accuracy over all windows from       days  it also has peak
accuracy at       day windows similarly to logistic
regression  however  its average accuracy is noticeably
lower than our other models  the run time for naive
bayes was substantially lower than any other model that
we experimented with over the course of our analysis 
the following graph shows the results of performing
  fold cross validation on this model 

v  c onclusion
our analysis confirms that the bloomberg data set
with multiple commodities  technical indicators  and

fi 

macro market features has substantially more predictive
power for the kc  coffee futures index than a feature
space consisting only of technical indicators  we find
that logistic regression gives us the best average testing
accuracy  followed by the support vector machine with
an rbf kernel  although naive bayes was our least
successful model  it still attained consistently above
    accuracy on windows    through     with an average accuracy of approximately            method  
in all models  predictions for larger windows tended to
be more accurate 
going forward  we are looking to establish a relationship between the variance and the reliability of different
features  additionally  we hope to employ ensemble
methods to improve our accuracy  we are specifically
interested in devising a trading strategy to test the
profitability of our prediction algorithms 
r eferences
   
   
   

   

   
   
   

   
   
    

kase  cynthia a   how well do traditional momentum indicators work   kase and company  inc   cta      
becedillas  gabriel  pyalgotrade  http   gbeced github io 
pyalgotrade            
jegadeesh  n  titman s  returns to buying winners and selling
losers  implications for stock market efficiency  journal of
finance           
kinlay  jonathan  can machine learning techniques be used
to detect market direction    the           model test 
systematic strategies llc 
huang  d  jiang  f  tu  j  zhou  g  mean reversion  momentum and return predictability      
bloomberg business  http   www bloomberg com 
ticlavilca  a  m   dillon m  feuz and mac mckee       
forecasting agricultural commodity prices using multivariate
bayesian machine learning regression  proceedings of the
nccc     conference on applied commodity price analysis 
forecasting  and market risk management  st  louis  mo 
scikit learn  http   scikit learn org stable 
scipy  scipy org
waskom  michael  http   stanford edu mwaskom software 
seaborn 

fi