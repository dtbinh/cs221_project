using decision tree to predict repeat customers
jia en nicholette li

jing rong lim

 

abstract
we focus on using feature engineering and
decision trees to perform classification and
feature selection on the data from kaggles
acquire valued shoppers challenge 

   introduction
customer retention is important to many
businesses as it is cheaper to build loyal
relationships with a customer than to source for
new customers     a study by bain   company
stated     to     increase in profits can be
made just by increasing    of customer
retention rates and a     rise in company value
with an increase of     of customer retention  
from marketing to offering discounts to loyalty
programs  companies have been continually
innovating in order to increase customer
retention  albeit at an initial cost to themselves 
a good marketing strategy to look into would be
product offers  product offers aim to attract new
and old customers alike with attractive product
deals as an incentive to continue buying from
them  however  this comes at the expense of
businesses as these deals equates to lower
revenue  hence it is important that these costs
translate to loyal customers that repeat product
purchase from them within and outside of
product offer periods 

 e g  amount spent  and discrete  e g  id of
company  category  brand   according to a
general purpose separability criterion by
grabczewski and duch  the construction of
decision trees is a natural application of the
separability criterion  a criterion based on the
idea that the best split is the one that splits the
largest number of pairs from different classes    
as the cut off point for continuous features can
be a real number subset in a range of values for a
feature  decision tree algorithms are suitable to
handle it as they inherently try to find the best
cut off point during its splits  this applies to
discrete features as well 

   

whether a customer decides to repeat a
purchase is dependent on a myriad of factors 
these can range from loyalty and trust to a
particular company or brand  or maybe the
product is a necessity  such as toothpaste  as
decision tree algorithms inherently estimate the
suitability of features during separation of
classes and can handle both categorical and
numerical features  we are interested in finding
out the ranking of features in retaining customers
with the use of product offers 
   related work   motivation
our motivation to use decision tree algorithm
to predict repeat customers comes from trying to
find an algorithm that can handle both
continuous and discrete variables as customer
transactions have properties that are continuous

 

an added benefit of utilizing decision trees is
that decision trees implicitly perform feature
selection while performing classification  in
feature selection with decision tree criterion 
grabczewski and jankowski states that decision
tree algorithms inherently estimate the suitability
of features for separation of objects representing
different classes     this makes it an interesting
endeavor to find out what makes customers repeat
a purchase and it can possibly give new insights
to how customers tick 
a similar work to our project would be
predicting customer shopping lists from pointof sale purchase data by cumby  fano  ghani
and krema     instead of predicting whether a
customer would repeat a purchase with an offer 
this research predicts what a customer would
want to purchase from their past transactions
using decision trees  specifically c         linear
methods  such as perceptron  winnow and naive
bayes  as well as hybrids of different algorithms 
they accounted their research results by the
accuracy  precision and coverage of how well the
algorithms do in predicting a customers potential
shopping list  of which c    have the highest
precision  the number of true positive
predictions     at     and second highest
accuracy  the total number of correct predictions
over the total number of examples  at     
promising results from the use of decision trees to
predict customers shopping list have led us to
look into using it to predict whether a customer
will repeat a purchase 

  

fi   methodology
    aim
with an input of number of items bought  total
amount spent in    days     days     days     
days  overall transactions from a product
company  product category  product brand  find
out the usefulness of decision tree algorithm to
predict repeat customers and determine the
important features for predicting repeat customers 

    data
the dataset is acquired from the kaggle
competition  acquire valued shoppers
challenge containing    customers pre offer
transactions     training history containing a
product the customer bought and whether a
repeat purchase was made     testing history
containing the predicted repeat success 
failure for a product and    a list of offers 
data type

properties

past
transactions

customer id  store  product
department  product company 
product category  product brand 
date of purchase  product size 
product size  product measure 
purchase quantity  purchase
amount

training
history

customer id  store  offer id 
geographical region  number of
repeat trips  repeater  offer date

testing
history

customer id  store  offer id 
geographical region  number of
repeat trips  repeater  offer date

offers

offer id  offer category  offer
quantity  offer company  offer
value  offer brand

table   showing the raw data acquired from the kaggle
competition  acquire valued shoppers challenge

the pre offer transactions contains nearly
    million rows of past customers
transactions but we are using only      
million rows for this project 
    feature engineering
due to the nature of the data  such as one
customer having multiple transactions from
buying multiple products  it is necessary to
merge the transactions of each customer into
a row and engineer new features to make

 

sense of the pre offer transactions file  this is
done through summing the quantities and
amount spent for each product company 
product category and product brand for all
pre offer transactions  for example 





quantity bought from company      
 to company l 
quantity bought from category      
 to category m 
quantity bought from brand     
 to brand n 

upon considering the importance of the time
proximity from the offer date  the amount
spent for each company  category and brand
within    days     days     days      days
before the offer date are also included as
features  a feature to keep track of whether a
customer has bought from a company 
category  brand during the pre offer
transactions was also added for convenience 
the collated list of features engineered is as
follows 
   total quantities bought from particular
company  category  brand
   total amount spent on particular
company  category  brand
   total amount spent on particular
company  category  brand within    days
before offer date
   total amount spent on particular
company  category  brand within    days
before offer date
   total amount spent on particular
company  category  brand within    days
before offer date
   total amount spent on particular
company  category  brand within    
days before offer date
   never bought from particular company 
category  brand

 

    decision tree algorithm
our model was implemented using apache
sparks machine learning library  v       that
uses distributed cart  classification and
regression trees  to construct the tree based
on numerical splitting criterion recursively
applied to the training data  the decision tree
model implemented in apache spark is a
greedy algorithm that performs a recursive

  

fibinary partitioning of the feature space by
selecting the best split from a set of possible
splits to maximize the information gain at a
tree node from the set 

 

   results   discussion
    classification
building the decision tree model

      
          
 

legend 
a   company           spent  
b   company            spenttotal
c   company           spent  
d   brand      spent   
e   company           spent  
f   category      neverbuy
g   company           neverbuy
 

the above described algorithm is also better
known as hunts algorithm  used in many
existing and popular decision tree induction
algorithms including id   c     and cart 
in this algorithm  a decision tree is grown
recursively by partitioning the training dataset
into successively purer subsets  let dt be the
set of training records that are associated with
node t and y    y   y           yc  be the class
labels  we can then hunts algorithm as
such    
step    if the records in dt belong to the same
class yt  then t is a leaf node yt 

the above decision tree model was built
using entropy impurity measure  with a
maximum depth of     which is the maximum
depth the library allows  the decision to select
the maximum depth possible was made after
taking into careful consideration the amount of
features the training dataset contains  close to
      and it is thus unlikely for the decision
tree model to overfit the data  when training
the model  a       split on the data was
implemented to perform cross validation 
which attained a test error of      

step    if records in dt belong to more than
one class  an attribute test condition is used
to partition the records into smaller groups 
for each outcome of the test condition  a
child node will be created  the records in dt
are also distributed to the children according
to these outcomes  this is then done
recursively to each child node 
node impurity and information gain
the node impurity is a measure of the
homogeneity of the labels at the node  the
current implementation of apache spark
provides two impurity measures for
classification   gini impurity 
 
   

 

figure   shows the illustration of the first   features 
depth    with       nodes

submission to kaggle
we ran the results from our decision
trees against the kaggle competition  acquire
valued shoppers challenge to see where we
stood amongst the submissions and we had
achieved a         accuracy 

            

and entropy 
 
   

 

            

  

figure   showing the possible ranking in acquire valued
shoppers challenge 

 

our prediction accuracy ranked us at     
outperforming hundreds of other submissions 
however  with an accuracy of          we
felt that our model could have performed
better 

 

  

fithe top team of the submission achieved
accuracy of        which had outperform our
model by a significant margin  given this
result  theres a possibility that our decision
tree model had either underfit or overfit our
training dataset 

 

using other classification algorithms
in an attempt to attain better results  our team
tried tuning parameters and explored several
other classification algorithms as well 
algorithm   parameters

results

decision trees   max depth   

      

gradient boosted trees with
   iterations

      

random forest   max depth   
     trees

      

decision trees   max depth   

      

logistic regression

      

table   showing comparison of various classification
algorithms ran on our engineered training data 

as shown in the above table  given that a
decision tree of depth much shallower than
our model had perform worse than our
current model  it is unlikely that our decision
tree model had overfitted the data  it is more
likely that the model had underfitted the data
due to insufficient depth of the tree 
while our decision tree model performed
the best out of the classification algorithms
we tried  most of the classification models
performed within    accuracy of our
decision tree model with a max depth of    
as such  it is likely that the results are
relatively algorithm agnostic  and the
problem might be unrelated to the algorithm
we picked 
evaluation of our classification model
our team have identified a few possible
reasons on our results performing below
expectations 
firstly  we hypothesized of a possibility that
our decision tree model had underfitted and
was unable to perform a good representation

 

of the training data  the library we used 
apache spark  allows for the maximum depth
of the tree to only     given that we have
approximately     features  it is possible that
the decision tree model was incapable of
fully capturing the features available to build
the best possible model  thereby underfitting
the dataset 
approaching this from another perspective 
there is also a high possibility that our
engineered features are not representative of
the customers behavior  as shown in table  
above  implementing various different
classification algorithms to our training data
did not improve our accuracy when evaluated
against kaggles test data  therefore  we
hypothesized that the engineered features
could have been better improved to capture the
essence of the massive training data we were
presented with 
lastly  there is also a possibility that we had
lost valuable information during the data
reduction process  the data reduction process
involves filtering out transactions of products
which did not appear in the offer data 
performing this filtering allowed for us to
reduce the data set from   gb to about  gb 
or from     m to      m rows of data  while
this assumption is relatively safe and widely
used online  our training model might have
benefitted from these additional data to attain a
more accurate result 
    feature selection
decision trees implicitly applies feature
selection whilst performing classification 
when fitting a decision tree to training data 
the top few nodes of which the tree is split are
regarded as the important variables within the
dataset and feature selection is thus completed
automatically as the features are sorted top
down by information gain  we used the
heuristic  separability of split value  ssv 
criterion     for feature selection as one of the
basic advantage of using ssv is that it can be
applied to both continuous and discrete
features  as well as compare the estimates of
separability despite the substantial difference
in the types of data 

 
the nodes found at the top of a decision tree
are regarded as the most important variables

  

fiin feature selection  and features that
appeared infrequently are pruned  thus
simplifying the decision tree model  in this
case      out of     features which had not
appeared at all     features which have a
frequency of less or equal to    occurrences 
which are pruned from the decision tree and
be ignored when performing classification 
top    recurring features
feature label

count

company           itemstotal

     

company           spenttotal

    

company           neverbuy

    

company           spent  

    

company            neverbuy

    

company           neverbuy

    

company           itemstotal

    

company           spenttotal

    

company           spenttotal

    

company           spent  

    

  

conclusion   future work

although our decision tree model did not match
up the top performers from kaggle  the model
still attained a test error of     upon cross
validation and is therefore a relatively good
model to predict customer repeats  as an
additional benefit to using decision trees  it
allowed us to perform implicit feature selection 
through the top    features  we discovered that a
particular company has a greater impact in
predicting customer repeats 
the project can be further improved by using
more pre offer transactions to provide a more
holistic picture of how many items and how much
a customer spent on a company  category or
brand  this would likely increase our accuracy
when tested on the test data  however 
performing training and testing on such a large
data set would take a much longer time and hence
we were unable to do so for this project in a short
time frame 
another possible future work would be to train
the data against winnow as per the work done in
predicting customer shopping lists from pointof sale purchase data  in the work done by
cumby and his team  the winnow algorithm had
a    higher accuracy but    lower precision than
their c    decision trees  as this kaggle
competition is looking for accuracy in predicting
whether a customer repeats a purchase  winnow
algorithm may perform better for this 

table   showing the top    recurring features in our
decision tree model 

we picked the top    recurring features of
our decision tree model and noticed a
surprising trend that the first two features 
company           itemstotal and
company           spenttotal had a huge
difference from the  rd feature  the third feature
had relatively the same count as the  th to   th
features  this is an interesting find as it means that
the company with id           had the most
customers repeating their purchase from it  even
though we do not know what company it
represents  we can hypothesize that it is a
company that caters to the basic needs of the
shopper 
another insight from the top    recurring features
list is that some companies are producing products
that are unpopular such that not buying from the
company is one of the main features in our
decision tree model 

 

references
   

reichheld  f          prescription for cutting costs  bain
  company  boston  harvard business school publishing 
   
k  grabczewski and w  duch  a general purpose
separability criterion for classification systems  in
proceedings of the  th conference on neural networks and
their applications  pages         zakopane  poland  june
     
   
k  grabczewski and norbert jankowski  feature selection
with decision tree criterion  dept  of comput  methods 
nicolaus copernicus univ   torun  poland  nov     
   
cumby c   fano a   ghani r     krema m         
predicting customer shopping from point of sale purchase
data  kdd     proceedings of the tenth acm sigkdd
international conference on knowledge discovery and data
mining   pp            new york 
   
j r  quinlan  c     programs for machine learning 
morgan kaufmann       
classification  basic concepts  decision trees and model
evaluation  in introduction to data mining 
   
tan p  n   steinbach m   kumar v  classification  basic
concepts  decision trees and model evaluation  in
introduction to data mining 

  

fi