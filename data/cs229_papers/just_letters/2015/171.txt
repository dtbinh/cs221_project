predicting momentum shifts in nba games
whitney larow  brian mittl  vijay singh
stanford university  december         

 

introduction

the inherent difficulty of this problem is tied to the
high amount of variance in between basketball games 
every game has its own unique set of players  coaches 
and referees  and so many different events can occur in a
given game at any point in time  our goal is to find features that transcend this high amount of variance and can
serve as useful predictors over several unique basketball
games 

during the first round of the      nba playoffs  the
eventual champions  the golden state warriors  found
themselves pitted against the new orleans pelicans  in
game   of their matchup  the pelicans opened the second
quarter with a stretch in which they scored    points and
the warriors scored none  they opened the third quarter with a similar      run  and  despite comeback efforts
by the warriors  the pelicans were up    points by the
start of the fourth quarter  against all odds  the warriors
managed to take momentum at this point  outscoring the
pelicans       in the quarter and eventually winning the
game in overtime 
these types of momentum shifts are not atypical in
basketball  the media often attributes these momentum
shifts and streaks to players getting distracted or not
wanting it enough  our project hopes to delve past this
rhetoric and determine effective methods to predict momentum shifts in these games  the ability to predict momentum shifts is valuable as it grants coaches the ability
to counter momentum shifts in real time  we postulate
that streaks are the factors that make or break games 
thus  we define a streak as a point where one team scores
  or more points while limiting its opponent to   points 
we decided on this definition of a streak after an in depth
literature review  see references   there are several useful data points that can be used as features to reasonably predict the likelihood of a team going on a streak
at any given point in an nba game  while traditional
nba stats like fouls  turnovers  and rebounds are good
features  other non traditional stats  such as frequency
and recency of timeouts  changes in lineups  and overall
team offensive rating  have proven insightful in predicting
streaks as well 
for our project  we use a window approach to predict whether or not a streak is about to occur  in other
words  we look at the previous    events in a game  timeouts  turnovers  shots made or missed  rebounds  etc   to
predict whether or not a streak will occur in the next   
events  we found that on average about     events occur
in one game  the input to our algorithm is then a combination of statistics from the past    events along with
some metadata about the teams  we then use two models  logistic regression and an svm  to classify whether
or not a streak will occur over the course of the next   
events 

 

related work

related work falls primarily in a similar yet separate
category  assessing the validity of momentum as an important factor in sports games  these studies typically
analyze the relationship between successive outcomes in
sports games  and many attempt to debunk the notion
that momentum is influential in the outcome of a game 
two studies in particular evaluate winning streaks in
sports with different outcomes  first  a study published
in the journal of sport behavior attempts to debunk the
myth of momentum in sports and shows that both participants and observers place an unjustified level of importance on momentum in the result of games      this
study  done by hand  assesses the psychological elements
of how momentum influences a game  however  another
study published in the journal of quantitative analysis
in sports uses neural networks to predict the outcomes
of nba games      this state of the art study shows that
there are predictable factors including momentum that
influence the outcome of games  allowing for a successful
prediction rate        of the time  over    higher than
experts 
still  other studies attempt to analyze the effect of momentum on in game statistics  which is closer to what this
project strives to predict  a study done jointly at cornell and stanford university found that there is no positive correlation between the outcome of successive shots 
which it attributes to the misperception that short random sequences are indicative of the ultimate generating
process      thus  the notion of a hot hand or streak
shooting in basketball is false according to this study 
along these same lines  a second study done at harvard
university compares the idea of a hot hand  or the belief that the success of one shot in a game will lead to
the success of another  with the idea of gamblers fallacy  or the belief that recent success must soon end     
 

fithis study ultimately shows that it depends on the perceived intentionality of the streaks agent  in other words 
the prediction of a streak depends on whether or not the
predictor believes the streak is intentional 
from a more psychological viewpoint  another study
considers exactly what constitutes a streak in the predictors mind and finds that three repeated events must
occur to solidify the subjective belief of a streak      ultimately  most of these studies only consider the psychological effects of momentum and the resulting impact on
belief  although it has been argued that shooting streaks
for a particular player do not occur  this is not the only
way momentum can influence the outcome of a game 
rather  factors such as rebounds  assists  timeouts  substitutions  free throws  and more all combine to influence
a streak of team points in a basketball game  hence  this
study intends to show that momentum is a factor in basketball games that can be predicted via machine learning
techniques through a multitude of factors 

 
   

playtype   turnover   
 awayscore       
homescore       
team   home  
time           
points      
playtype   madeshot   
   
 
 
this scraped data then becomes our database from
which we derive our train and test data  we process one
game at a time  first extracting metadata features like
both teams offensive rating and record  then using a window of the previous    in game events to collect statistics like count of made shots  made free throws  offensive rebounds  team timeouts  turnovers  substitutions 
etc  this window of only    events helped us to emphasize the importance of recency of events in our analysis 
we combine these metadata features with the count features from the event window to create a feature vector
for every in game event we see  also including an intercept term x       we then use our streak definition to
label our dataset by determining  at the given in game
event  whether or not a streak is about to occur  i e  will
the given team have outscored their opponent by   or
more points at any moment in a window of    plays into
the future   we output our feature vectors  along with
these associated labels  to a csv file  which we then used
to train and test our models 
we ended up only using the game data from the        nba season because this had over       games  which
translated into over           in game events  which we
decided would be enough data for both our train and
test examples  we split our dataset        using about
        examples for our training set  which we also broke
up       for our validation set  and         examples for
the final test set  the one on which our error rates are
reported  

dataset and features
data extraction

we use the python library scrapy to scrape our data
from basketball reference com      which contains playby play data for every nba game from the         season
through the         season  we start our crawlers at the
schedule   results page for each of these    seasons     
follow links through each of the game dates      and then
follow links through the play by play pages for each of
the games that occurred on that date      we perform
our actual data scraping from each of these play by play
pages 

figure    sample unstructured play by play data 
we first collect metadata about the game  such as
date  team names  final score  amount of time spent tied 
number of lead changes  and other useful metadata  we
then collect a list of plays  which has information about
every play that occurred in the game  such as time  players involved  play type  shot  rebound  foul  timeout 
turnover  substitution  etc    score  and other details relevant to that specific type of play  our scrapy client then
creates a json file with all of this information  where
each game is represented as a dict type structure 

   

feature extraction

after examining our play by plays and observing
streaks that occurred in actual nba games  we built an
exhaustive list of    features from our json data    
home or away team     record difference     difference
in field goal percentage     difference in offensive rating     difference in turnover percentage     difference
in free throw percentage     consecutive points scored 
   recent points differential     points scored by leading scorer      teams made shots      teams missed
shots      teams made three pointers      teams
missed three pointers      teams made free throws 
    teams missed free throws      teams offensive rebounds      teams defensive rebounds      teams assists      teams personal fouls      teams turnovers 

  awayteam    los angeles lakers  
 hometeam    golden state warriors  
 date    november          
   
 plays     awayscore       
homescore       
team   away  
time           
 

fi    teams timeouts      teams substitutions      opponents made shots      opponents missed shots     
opponents made three pointers      opponents missed
three pointers      opponents made free throws     
opponents missed free throws      opponents offensive rebounds      opponents defensive rebounds     
opponents assists      opponents personal fouls     
opponents turnovers      opponents timeouts     
opponents substitutions
we realized that including this many features could
cause overfitting  and that many of the features were
probably not useful  so we had to determine which features were the most effective in predicting streaks  to
do this  we utilized the recursive feature elimination tool
from the python scikit learn library  which works essentially the same way as backward search  backward
search works by starting with the entire feature set and
removing one at a time  tracking which of the resulting
sets produces the best generalization error  it then repeats the process until it reaches the desired number of
features 
we applied backward search to evaluate our feature
set with respect to our logistic regression model  this
produced a set of    optimal features  using the previous
enumeration                                             
we also evaluated our features using our svm  which
produced an optimal feature set of    features             
                                              

 
   

     to       with    steps  this gave us an optimal decision boundary of      as it maximized the sum of the true
positive and true negative rate  see graph below  

figure    graph showing the effect of changing the logistic regression decision boundary on test performance
through the use of   fold cross fold validation 
to find the weight vector    we fit the logistic regression model with training data  the model runs an
algorithm like stochastic gradient ascent to maximize the
likelihood of   using the update rule 
 i 

j    j    y  i    h  x i    xj

we implemented our logistic regression model in
python using the pandas library to handle and operate
directly on our data and the statsmodels library to create
our actual logistic regression model 

methods
logistic regression

the problem at hand is essentially a binary classification problem  at any given moment during a basketball
game  we want to be able to predict whether or not a
streak will occur  given that our outcome variable is categorical  and our predictor variables may be either continuous or categorical  logistic regression is a natural
starting point 
logistic regression models work by producing a
weight vector    which defines a hypothesis  h  x   that
is used to produce a probability score between   and   
for a given feature vector  x  the hypothesis of the model
produces the value 

   

support vector machine

we also modeled our problem with a support vector
machine to predict whether or not a streak will occur 
to date  few supervised learning algorithms have outperformed svms  perhaps the most alluring property of
svms is that we can utilize their symbiosis with kernels 
this lets us create high or infinite dimensional feature
vectors  allowing us to capture the subtle interactions between features  which is particularly important for our
dataset because it contains so many intertwined and interdependent features  thus  the svm presents an ideal
classification model that does not limit our ability to uti 
h  x    g t x   
lize a dense  high dimensional feature vector to determine
    et x
the state of the game 
one disadvantage of svms  however  is that they are
this value necessarily falls between the values   and   
which values are classified as a streak and which are clas  easily susceptible to overfitting because they work in such
sified as a non streak is determined by the decision  cut  a high dimensional feature space  since it is likely that
off  boundary  given a decision boundary  b  all predic  the extracted data will become linearly separable  we run
tion values     p  b are classified as a non streak and the danger of the model learning hidden attributes of our
all prediction values b   p     are classified as a streak  data rather than the more general trends we are looking
to determine the optimal decision boundary  we im  for 
on the other hand  mapping features to a high dimenplemented and ran   fold cross fold validation  splitting
our test set       each time  on the range of values from sional space doesnt necessarily guarantee that the data
 

fibecomes separable  we also dont know  in our case  if
we want to find a separating hyperplane because it is
likely that our data contains outliers  due to its highly
variable nature   and we dont want to fit our optimal
margin classifier to these confounding outliers  to make
an effort to account for these issues  we also use    regularization  which incorporates an error term that allows
for the disregard of outliers 
we optimize the resulting primal problem to find the
optimal margin classifier  which defines our svm 
m

x
 
i
min kwk    c
 w b  
i  
s t  y  i   wt x i    b      i   i              m
i     i              m

figure    final performance rates for our svm and logistic regression 

svms constructed in this manner work to maximize
the geometric margin for every point by constructing a
separating hyperplane in high dimensional space  as most
data sets are not linearly separable  that classifies as
many points correctly with the largest margin possible 
intuitively  geometric margin can be understood as the
functional margin

while both our logistic regression and svm models performed significantly well  the logistic regression
model still outperformed the svm  this could be for a
variety of reasons  while svms can be very powerful and
outperform most other models when appropriate  they are
known to perform poorly under certain conditions  for
 i 
 i 
t  i 
   y  w x   b 
example  if the dataset is susceptible to outliers  which
ours may be  or if the high dimensionality of the examscaled by kwk  the result is a classifier with the lowest
ples causes the svm to pick up trends seen only in the
possible generalization error  which is the ultimate goal
training data  the svm could generalize poorly  logistic
of any classifier 
regression models  which are typically simpler  are not
for our svm  we used the python library  scikit
susceptible to these same pitfalls  it could be that the
learn  we found that a linear kernel worked best on our
data followed trends which are more easily picked up by
dataset and features  we also implemented   fold cross
logistic regression models than svms  in the end  the
fold validation  splitting our test set       each time  to
results show that testing a variety of models helps prodetermine other optimum values like c              and
duce the best results  as different models have different
    maximum iterations 
strengths and weaknesses  which arent always apparent
until tested on the data 

 

results and discussion

   

interestingly enough  with our most successful model 
logistic regression  not many traditional nba statistics remained as useful features  for instance  both rebounding and assists  conventionally considered important stats  were not used in the predictions  this is probably because these events occur so frequently regardless
of whether or not a streak about to occur  to illustrate 
a rebound occurs at least once in every possession with a
missed shot  and an assist usually occurs in a possession
with a made shot 

final results

our final models produced the following success rates 

the most important numbers are the sums of the true
positive and true negative rates for the test set for both
the svm and the logistic regression model  we achieved
an overall success rate of       for our svm and      
for our logistic regression model  because these numbers are significantly higher than      the success rate of
a model that guesses randomly  we claim that our methods were successful in predicting scoring streaks in nba
games  a visual representation of the success of our models can be seen below 

instead  the relevant features included stats about the
team in general that did not have to do with the specific
game  such as record difference and offensive rating   current momentum  such as recent points differential   and
rarer in game events  such as timeouts and substitutions  
despite the high variance in nba games  these features
consistently proved to be effective predictors across our
entire large dataset  showing that there are similarities in
how streaks occur across the nba 
 

fi   

look ahead window variation

the above four features were some of the most important predictors in our logistic regression model  based
we thought it would be interesting to look more into on how our rates dropped when they were removed  by a
our original design ideas for our model  so we experi  wide margin  recent points differential was the most immented with changing the look ahead window size in con  portant predictor  this feature captures the difference in
sideration when training and testing on our dataset  orig  scoring between the two games over the last    events 
inally  we trained on the previous    events to predict indicating that momentum in nba games builds on itself
whether or not a streak will occur in the next    events  and can lead to streaks 
what if we predict whether or not a streak will occur in
while the effect of the points scored by the leading
the next   events instead  the next    events  this win  scorer was not nearly as dramatic as the previous two feadow controls how far into the future we are attempting tures  it was still non trivial given the size of our dataset 
to predict  how does changing this window size  making the importance of this feature as a predictor suggests
it larger and smaller  influence our results 
that a teams leading scorer may often be the primary
agent driving a streak  from a basketball perspective 
star players often dominate the game  as bench players
and role players usually score at a dramatically lower rate
and are unlikely to produce a streak on their own 

 

our results showed that momentum shifts  as modeled
by our streak definition  can be predicted with relatively
high accuracy and are determined by a variety of factors 
ultimately  logistic regression produced better classification results as a model  but our svm model was still
able to correctly identify momentum streaks at around a
    true positive and true negative rate  this shows that
momentum is a factor of not just scoring but the overall
state of the game  given more time and resources  we
would like to explore neural networks as a potential model
for predicting momentum streaks in nba games  neural
networks are promising for this project because of their
ability to learn features  since momentum is defined by
a complex system of factors that are difficult to define by
hand  neural networks could prove even more successful
at predicting momentum shifts than logistic regression
and svms 
looking at other potential features  such as injuries
on a team or data regarding the referees  could also prove
to be useful predictors  outside the scope of this project 
future research into how coaches could use this data to
change their in game strategies would be very useful  for
instance  if an opposing team seems likely to go on a
streak  a coach could call a timeout or make substitutions to change the dynamic of the game  looking more
in depth into the events that cause streaks to end would
also be useful in this context 
overall  this project successfully shows that nba
games are influenced by various factors that increase the
likelihood of momentum shifts  this breakthrough is important as it could be used to influence in game decision making to optimize performance at the professional
basketball level through machine learning and statistical
analysis 

figure    the effect of varying the look ahead window
size on the performance of our logistic regression model 
the results of this experiment make sense  our logistic regression model performs poorly when we ask it to
look far into the future and well when we ask it to only
predict the outcome of the next few plays  this shows
that the recency of events in a game is important in predicting a scoring streak  events happening now have more
effect on events in the near future than on events in the
far future  

   

conclusions   future work

single feature removal

figure    the effect of removing individual features from
our feature set on the performance of our logistic regression model  as measured by decrease in true positive and
true negative rates 

 

fireferences
    basketball reference com homepage  http   www basketball reference com  accessed  nov       
    game date example page for october          http   www basketball reference com boxscores 
index cgi month    day    year       accessed  nov       
    game play by play page for charlotte hornets at atlanta hawks  http   www basketball reference 
com boxscores pbp          atl html  accessed  nov       
    schedule   results example page for the         nba season  http   www basketball reference com 
leagues nba      games html  accessed  nov       
    kurt a carlson and suzanne b shu  the rule of three  how the third event signals the emergence of a streak 
organizational behavior and human decision processes                      
    eugene m caruso and nicholas epley  hot hands and cool machines  perceived intentionality in the prediction of
streaks  in poster session presented at the  th annual meeting of the society for personality and social psychology 
austin  tx  usa       
    thomas gilovich  robert vallone  and amos tversky  the hot hand in basketball  on the misperception of
random sequences  cognitive psychology                     
    bernard loeffelholz  earl bednar  and kenneth w bauer  predicting nba games using neural networks  journal
of quantitative analysis in sports             
    r vergin  winning streaks in sports and the mispreception of momentum  journal of sport behaviour           
          
note  we used this projects data and infrastructure for another class project  with the permission of the instructor 

 

fi