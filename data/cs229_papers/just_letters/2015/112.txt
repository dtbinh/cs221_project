machine learning in network intrusion detection
liru long  xilei wang and xiaoxi zhu

i  i ntroduction
network security is of great importance to individuals and
organizations  advanced technologies have been developed
to protect both incoming and outgoing traffic  e g  encryption
of sensitive information  firewalls to block risky traffic  however  traditional firewalls and intrusion detection system
 ids  identify and block suspicious traffic based on preconfigured rules  traffic signatures as well as patterns of
known attacks stored in its database  given the exponential
growth in the size and complexity of network  this legacy
approach turns out to be inefficient  on one hand  the
increasing amount of attack patterns requires large database
and long processing time  on the other hand  numerous
unknown attack patterns pop up everyday  which makes it
next to impossible to keep the firewall ids up to date 
in response of the challenges  an advanced firewall and or
ids should possess the following characteristics  firstly  it
should be able to detect network attacks efficiently  secondly 
it should be able to assess unknown traffic patterns  both
academia and industry have been developing ids implemented with machine learning algorithm  a properly trained
ids learns the general pattern of normal against malicious
traffic  thus able to process incoming traffic much faster than
those iterating through static rules  in addition  the pattern
generalization also allows ids to make a better judgement
on unforeseen traffic patterns 
this project aims to build an effective ids using machine learning algorithms  the pre processing engine in
ids extracts relevant features  such as transport protocol 
application service  connection duration  payload size  etc  
from each network connection  the core engine uses the
extracted features as learning input and outputs the binary
classification result  i e   normal vs attack 
the rest of the paper is organized as follows  section  
summarizes a few related works  section   discusses the data
set and input features in more details  section   establishes
the performance baseline using different single stage classifier  section   proposes an extension with more sophisticated
multi stage classification algorithms  section   focuses on
optimization with feature reduction  section   concludes the
project with a comparison of various implementations and
proposes directions of future improvement 
liru long lirul stanford edu   xilei wang  xileiw stanford edu  and
xiaoxi zhu  zxx    stanford edu  are with the department of electrical
engineering  stanford university  palo alto ca     

ii  r elated w ork
given the fact that data in production network is hard to
obtain  the kddcup       data set has been widely studied
and applied in related field of network security 
as clean data set is the first step in developing good
machine learning algorithms  study in     focuses on techniques of raw data processing and normalization  sabhnani
proposes an effective feature reduction scheme that results
in significant improvement of the classification results 
siddiqui et  al      analyzed the dataset with k means
clustering  targeting to characterize the correlation between
various types of attacks and the transport layer protocol 
researchers have also attempted to develop more advanced
learning algorithm for ids  the algorithm proposed in    
optimizes svm with ga techniques so that the most relavent
feature sets and optimal parameters of svm could be identified quickly  chandrasekhar et  al      proposes a cascaded
technique using k means clustering  fuzzy neural networks
and svm 
iii  data s et and f eatures
training and test data sets used in this project are downloaded from kddcup   website   an annual data mining
and knowledge discovery competition organized by acm
special interest group  raw tcp dump data in a simulated
local area network  lan  are processed to generate network connection records  each constitutes of    feature fields
and a class field  the full training set is generated from
seven weeks of network traffic  which results in five million
records  the full test set  which is generated from two weeks
of traffic  leads to two million connection records  a    
subset of training and test data was used in this project  the
package includes a training set of         samples and a test
set of         samples 
each data sample consists of    features  which can be
further categorized as basic features  content features  timebased features  the network connection features of downloaded data set are of inconsistent format  some features
are thus converted  from text to numerical value  and or
normalized to the same order  conversion approaches are
listed as below 
 text to numerical  feature fields labeled with text message  e g  protocol  service  etc  are converted to integer
values between   to    
 medium numerical values  feature fields where
the range is of medium magnitude      e g 
num file creations  are normalized to the range
       w r t  its max value

filarge numerical values  feature fields where the numerical range is large      e g  dst bytes  are converted to
the range        using base    logarithm 
each training sample is labeled with a text indicating
either normal or attack  with an exact description for type of
attack  the specific attack types can be further categorized
into four generic categories  the label is thus converged to
integer values with the correspondence map   normal   
probing    dos    u r    r l    


in this section  three single stage classifiers     naive
bayes  decision tree  and k means clustering   are trained
and tested  hold out cross validation is used to select model
parameters where applicable  multi class classification is
implemented for each model  however  the result is only
evaluated based on the confusion matrix of normal positive 
vs attack negative  defined in table i  in other words 
misclassification across different types of attacks are ignored
as it does not raise any concern in real application 
truenormal predictedasnormal
truenormal
trueattackpredictedasnormal
trueattack
truenormal predictedasattack
truenormal
trueattackpredictedasattack
trueattack

table i  confusion matrix

the confusion matrix of classification result using the
above multi class gaussian naive bayes classifier is presented in table ii
tpr         
fnr         

fpr         
tnr         

b  decision tree
decision tree is another supervised learning algorithm
commonly used for classification  the classification process
can be represented as a tree structure  where root node and
each intermediate node contains an attribute classification
criteria  a given test sample starts from the root node and
traverses through a selected path to a leaf node based on
the decision made at each node  each leaf node is labeled
during training process and the test sample is assigned the
same label as the leaf node 
the classification criterion at each node is formulated
during training process by maximizing information gain 
ig y  x  for a given node with attribute x is defined as
entropy difference before and after partition     
ig y  x    h y    h y  x 

a  naive bayes
classification with naive bayers algorithm is simple and
efficient  yet gives satisfactory results for most classification
problems  therefore  it is implemented as the baseline for
other more advanced algorithms 
a trained naive bayes classifier uses maximum a posterior estimation to predict the class of test data p y  with
known features x    x         xn by maximizing the conditional
probability of p y x    x         xn    bayes theorem defines that 
p y p x    x         xn  y 
p y x    x         xn    
p x    x         xn  

   

under the assumption that all features are independent 
eq    can be further reduced to 
p y   p xi  y 
p x    x         xn  

   

p y x    x         xn    p y   p xi  y 

   

p y x    x         xn    

   

table ii  confusion matrix for naive bayes

iv  s ingle   stage c lassifier

true positive rate  tpr 
false positive rate  fpr 
false negative rate  fnr 
true negative rate  tnr 

  x      
 
i
i
p xi  y    
exp 
 
 y
 y

   

h y   is entropy before partition  which measures the
homogeneity of a given set of data as follows 
h y       pi logpi

   

i

h y  x  is the weighted sum of entropy for all subsets
after partition based on attribute x  defined as below 
h y  x      p x   xi  h y  x   xi  

   

i

thus each node partitions the data set using the most distinguishable attribute by maximizing the information gain 

therefore  the map can be expressed as 
y   arg maxp y   p xi  y 

   

y

in addition  it is assumed that each feature follows a
gaussian distribution as in eq    where the parameters y
and y are estimated using maximum likelihood 
  all

machine learning algorithms are implemented with scikit learn
library downloaded from http   scikit learn org stable 

fig     decision tree performance vs tree depth
decision tree has several advantages over naive bayes 
first  it selects only one attribute as classification criteria
at each node  thus the process prioritizes the feature set 

fiwhich achieves similar outcome as a weighted cost function 
second  by limiting the tree depth  it filters out attributes that
are less indicative  hence prevent overfitting and reducing
unwanted noise of high dimensional feature set 
the key parameter in decision tree algorithm is the
maximum tree depth  test runs are conducted to select the
optimal value  initial runs are conducted over the depths
range of   to    with a step of    second iteration of runs
uses finer step  with the depth ranging from   to    
fig    presents the trend of precision and recall for normal
class with varying tree depth  a decision tree classifier with
maximum depth of   is selected based on the results  which
gives the optimal combination of precision and recall 
the confusion matrix of classification results using the
trained decision tree is given in table iii below 
tpr         
fnr         

order to deal with data skew as well as non linear separable
data set  however  an arbitrarily large k value may result in
over fitting 
test runs are conducted with k value varying from     to
     to identify the best performing classifier  each model 
with a given k value  is also trained multiple times as the
centroids can be different due to randomly assigned initial
value  fig    shows the performance comparison of different
models based on precision and recall computed from cross
validation result 

fpr         
tfr         

table iii  confusion matrix for decision tree
c  k means clustering
k means clustering  as indicated by its name  groups given
data set into a fixed number of clusters according to their
geometric locations in the space spanned by the features 
the algorithm follows the steps below 
   k cluster centroids are initialized  typically randomly 
   assign each sample to the a cluster whose centroid is
closest to the sample using eq   
c i     arg min  x i    j    

   

i

where x i  is the geometric location of the ith sample 
 j is the geometric location of the jth centroid and c i 
is the assigned centroid 
   reset the centroids of each cluster to the center of all
samples in the same cluster using eq    
 j   

 i 
 i 
m
i     c   j x
 i 
m
i     c   j 

fig     k means performance vs number of clusters
the     cluster model is selected since it generates most
satisfactory results in cross validation  it is also observed that
the performance of this model is most stable across multiple
runs  however  no cluster of class    r l attack  can be
formed even for such a large k value 
the confusion matrix resulting from a modified    cluster k means algorithm is given in table iv
tpr         
fnr         

fpr         
tfr         

table iv  confusion matrix for k means
d  discussion

    

classification of test data is simply achieved by selecting
the nearest centroids based on euclidean distance  it is noted
that generated clusters are not labeled  while the test data
needs to be classified as either normal or attack  therefore 
an additional step is added  where each centroid is assigned
to one of class   to   based on majority vote of all samples
in this cluster  test data is then labeled the same class as its
nearest centroid 
viewing the data set in a high dimensional space  they
should form five clusters in the ideal situation  however 
considering the fact that each class can be further divided into
different sub classes  it is likely to have many more clusters 
even worse  two clusters belonging to the same class may
not be located next to each other  indicating that the data set
is not linearly separable  to address this issue  number of
cluster centroids needs to be selected wisely  as a common
practice  number of centroids is set to a large number in

the confusion matrices for all three single stage classifiers
show similar results  it is noted that although true positive
rate tpr  is high         which minimize the occurrence
of false alarm  however the false positive rate fpr  is
relatively high  which implies potential security risks arising
from undetected attacks 
a possible root cause for the inferior performance is
that some attack traffic have distinctive feature signature
compared to normal class while others may have similar
feature patterns  a simple classifier trained with the complete
set is thus biased towards the most distinguishable features 
in the next section  multi stage classifier models are proposed
to overcome this limitation 
v  m ulti   stage c lassifier
the three single stage classifiers give satisfactory recall
value for normal class  however  the relatively low precision
value indicates a large number of undetected attacks  one
of the possible reasons is that those misclassified traffic

fihas similar feature signature compared to normal class  the
proposed solution is to implement multiple classifier that
performs finer classification  two different approaches are
discussed in this section  random forest constructs parallel
classifiers while the decision tree with gmm model builds
a cascaded classifier model 
a  random forest
random forest  in its simplest form  is a collection of
decision trees  each decision tree is trained separately with
only a subset of training samples  as a result  trees are
constructed with different classification criteria at each level 
the randomness helps to reduce the variance  a given test
sample is thus classified using all different trees and the label
is assigned based on majority vote     
there is a natural trade off in determining the number of
decision trees in random forest algorithm  increased number
of trees may improve the classification performance  however
resulting in longer runtime  test runs are conducted with
number of dts varying from    to     the performance
difference is not significant across different runs  therefore 
the number of trees is limited to     which generates more
stable results across multiple test runs 
the confusion matrix of classification result using the
random forest algorithm is given in table v 
tpr         
fnr         

fpr         
tfr         

table v  confusion matrix for random forest
b  decision tree with gmm
the major performance concern is precision value of
normal class  thus a cascaded classifier is proposed where
the second stage classifier only acts upon the normal class
labeled by first classifier  the second stage classifier is thus
trained to identify finer difference between normal and attack
class  decision tree is selected as the first classifier as it
gives the best performance out of the three algorithms in
section    the second stage classifier is implemented with
gaussian mixture model  gmm  
gmm is a probabilistic model with the underlying assumption that all data points are generated from a mixture
of a finite number of gaussian distributions with unknown
parameters  test sample is assigned to the class that it most
likely belongs to  it is clear that gmm is a unsupervised
learning algorithm  unsupervised learning is preferred in the
second stage as it is misclassification mostly come from
subcategories that are not present in training set  similar
as k means clustering  gmm is able to handle non linear
boundary in the data sets 
gmm implements four sub models  each corresponding to
a different covariance matrix  as with other learning algorithms  test runs are conducted to select optimal parameter
setting  tied covariance matrix generates best result in crossvalidation  the confusion matrix of classification result with
the cascaded classifier is presented in table vi 

tpr         
fnr         

fpr         
tfr         

table vi  confusion matrix for cascaded

c  discussion
it is observed that both multi stage classifiers gives
marginal performance improvement  the results trigger more
in depth analysis into the data set  it is identified that most
mis classification arises from class    u r attack  and class  
 r l attack   a detailed study reveals the following insights 
 class   attack has extremely small sample size in both
training set and test set compared to other class  thus 
the unique feature signature pattern is not properly
learned by the classifiers 
 in addition  training set and test set does not follow
similar distribution across different classes  only      
of training samples are of class   type  however 
there are      of them in the test set  the uneven
distribution results in bias of trained classifier  thus poor
performance in identifying this shadowed class 
 lastly  class   can be further split into different subclasses of attacks  examining the original feature data
shows that class   samples in training set and class  
samples in test set belong to different sub classes  it is
highly likely that those sub classes have different feature
signature patterns  thus the trained classifier is not able
to identify the unknown pattern in test set 
in conclusion  a cleaner training set should be re generated
in order to build classifier with high performance 
vi  f eature r eduction
the optimal depth for best performing decision tree
based classifier is    indicating that there are redundant
or unimportant features out of the    dimension feature
space  therefore  feature reduction is exercised as a further
optimization for the ids with machine learning algorithm 

fig     partial correlation matrix
the approach for feature reduction is highlighted below 
   generate correlation matrix for complete feature set
   identify feature groups with high pair wise correlation
   apply principal component analysis to reduce the
dimension of each feature groups

firi j   p

 yki  i   yk j   j  
  yki  i     yk j   j  

    

a partial correlation matrix is given in fig     where
the correlation is computed using widely adopted pearson r
coefficient in eq      highlighted fields are selected groups
of features with high correlation  given as follows 
 group          
 group                   
 group                    
 group                       
each group is then reduced to its principal component
with orthogonal linear transformation defined in eq      
the feature space dimension is reduced from    to    
w   arg max   x i   w  

    

  w    

the base line single stage classifiers are then re trained
with the new feature set  however  the performance improvement is marginal  detailed performance comparison for all
models in this paper is presented in the conclusion 
a further analysis on feature correlation is conducted
to further investigate this counter intuitive result  in the
analysis  feature reduction is applied to individual classes
of traffic  re applying steps         on a single class of
samples  the result is summarized in table vii 
class
probe

dos
u r
r l

correlated features
                    
                 
            
                         
                 
                          
                 
                 
            
                         

table vii  feature correlation for individual class
it is shown that each class has different correlated feature
groups  on one hand  it is a strong proof of the assumption
that each class has a unique feature signature pattern  on
the other hand  it reveals that the generic feature reduction
approach is too aggressive  it does help to compress redundant features  however  it blends some of the distinctive
features from different classes as well  therefore  a onevs all classifier with class specific feature reduction may
give better results  due to time constraints  this approach
is not implemented  the concluding remarks highlights this
approach as a future extension of this project 
vii  c onclusion and f uture w ork
fig    below presents a complete view of different machine
learning algorithms tested for a network intrusion detection
system  the key performance metrics used for evaluation
is precision  true positive against predicted positive  and

fig     comparison of classifier

recall  true positive against condition positive  as target is
to classify network traffic into normal against attack 
the results show consistent performance across different
approaches  recall value ranges from     to      the high
percentage indicates a low rate of false alarm  however  the
precision value varies in between     to      leading to
potential risks of undetected attacks 
further analysis reveals that the root cause lies in the data
sets  first of all  the raw features are of different format 
the pre processing and normalization step may shadow some
unique patterns  secondly  five classes are not evenly distributed in the training set  data skew leads to bias in trained
classifier and thus degraded accuracy while classifying test
data  thirdly  training data and test data in the same class
does not necessarily possess similar feature signature pattern
as each class consists of multiple sub classes  lastly  features are correlated but the correlation pattern varies across
different classes  hence pca across entire training set does
not improve the performance 
in view of the above characteristics inherent to the data
set  future work should focus on developing customized data
processing techniques prior to implementing more sophisticated learning algorithms  a few prospective directions are
briefly discussed here 
   analyze the value distribution of individual features to
select more suitable normalization function
   re generate training data set with a even distribution
across different classes
   finer categorization of the general attack class
   conduct class specific feature reduction  followed by
one vs all classification with the reduced feature set

fir eferences
    kdd cup      data  http   kdd ics uci edu databases kddcup   
kddcup   html  web  oct      
    m  sabhnani and g  serpen  application of machine learning algorithms to kdd intrusion detection dataset within misuse detection
context  in proceedings of the international conference on machine
learning  models  technologies  and applications  pp                
    m  siddiqui and s  naahid  analysis of kdd cup    dataset using
clustering based data mining ijdta  vol     no     pp              
    d  s  kim  h  n  nguyen  and j  s  park  genetic algorithm to
improve svm based network intrusion detection system  in advanced
information networking and applications  aina    th international
conference  pp                
    a  m  chandrasekhar and k  raghuveer  intrusion detection technique
by using k means  fuzzy neural network and svm classifiers  in
proceedings of ieee international conference on networking  sensing
and control        pp      
    scikit learn org  scikit learn  machine learning in python   scikitlearn      documentation         online   available  http   scikitlearn org stable    accessed  nov       
    quinlan j  induction of decision trees  mach learn               
    v  svetnik  a  liaw  c  tong  j  culberson  r  sheridan and b 
feuston  random forest  a classification and regression tool for
compound classification and qsar modeling  journal of chemical
information and modeling  vol      no     pp                  

fi