classification of higgs boson tau tau decays using
gpu accelerated neural networks
mohit shridhar
stanford university
mohits stanford edu  mohit u nus edu

abstract
in particle physics  higgs boson to tau tau decay signals are notoriously difficult
to identify due to the presence of severe background noise generated by other
decaying particles  our approach uses neural networks to classify events as signals
or background noise 

 

introduction

the discovery of the infamous higgs boson in      was one of the most significant scientific
milestones of the past century  the particles existence was predicted nearly    years ago  but
verifying it experimentally became a monumental challenge  the search brought together more
than    countries and cost nearly       billion dollars 
the existence of the particle spawned a number of other challenges including the study of how
it decays into other particles  an essential characterization in particle physics   physicists are
specifically interested in the tau tau decays of higgs bosons  but unfortunately  these events
are deeply buried in severe background noise  the objective of this study is to apply supervised
learning techniques to carryout simple binary classification  tau tau decay vs  noise  primarily  we
will focus on using neural networks to methodically classify input measurements and benchmark
their performance against other learning models 
prior work in the field mainly revolves around the use of gradient boosting     or random forests  p 
sadowski et al  demonstrated that deep neural networks are well suited for improving the discovery
significance of tau tau decays  this classification problem was posed as a kaggle challenge     in
     

 

dataset

as the decay phenomenon is still a relatively new area of research  we didnt use any real data
from particle accelerators  but instead  we used simulated datasets generated by cern through the
atlas experiments  these simulations were based on our current understandings of the standard
model and empirical observations from various experiments 
the input vector consists of    features  which represent conditional parameters and measurements taken from various instruments  typically the raw data from accelerators is heavily
skewed  i e  the occurrence of signal events  tau tau decay  is vastly overshadowed by background events  the simulated dataset has a higher ratio of signal to background events in order to ease the process of training models with limited samples  so to compensate for this 
the dataset provides a relative weighting factor for each datum that specifies the significance
 

fiof the event  formally  given a training dataset d     x    y    w           xn   yn   wn     where
xi  r   is a vector of input features  yi   s  b  is the label  signal or background 
and wi  r  isx
a weighting factor  the weights wi also have
xa physical significance      
wi s   ns
   
wi b   nb
   
is

ib

where ns and nb are the expected occurrences of the each event  these values were computed
by taking the conditional densities p xi   of the input features and dividing it by the instrumental
densities q xi   

wi 

ps  xi   qs  xi    if yi   s 
pb  xi   qb  xi    if yi   b 

   

the performance of the system was quantified using a metric provided by the kaggle challenge
called the ams objective function     
s 

amsc      s   b   breg   ln    

s
b   breg




s

   

where s and b are the expected number of signal events and background events respectively  selected
by the learning model  generally  higher ams scores correspond to better classifications  the ams
score of the winning kaggle submission was         in      

 

methods

our approach to the classification problem involves the use of dropout neural networks to statistically
model the data  due to the complex relationship between the input and output of the system  neural
networks were deemed to be the appropriate learning model to tackle this problem  the fact that
the input is heavily biased with background events makes the model prone to over fitting issues     
dropout neural networks have been proven to reduce over fitting problems faced by standard neural
networks      consider a standard feed forward neural network with perceptron nodes 
 l   

zi
 l   
yi

 
 

 l   

 l   

wi
y l   bi
 l   
f  zi
 

figure    standard feed forward network

 

figure    dropout network

 

   

fi l 

where l           l  is a hidden layer among l layers  yl is the output from layer l  wi are the
 l 
weights  and bi are the biases  the nodes of a dropout neural network have a distinct probability
of being dropped during training 
 l 

rj
yl

 l   

zi
 l   
yi


 
 
 

bernoulli p  
r l   y l 
 l    l
 l   
wi
y   bi
 
 l   
f  zi
 

   

 l 

where rj is a bernoulli random variable with probability p of being    during every epoch  we are
essentially sampling a sub network from the larger deep network  in a way  the dropout technique
is performing stochastic regularization to reduce over training  i e  the weights are being scaled
 l 
 l 
using p as a factor  wi  pwi   when using the model to predict labels using real data  we
set p       to include all the nodes  n  srivastava et al  showed that this process results in a
huge performance improvement across various neural architectures without using hyper parameters
tailored specifically for the architecture 

 

experiments   results

figure    random distribution

figure    gradient boosting

figure    xgboost

figure    dropout neural networks

we implemented a bag of dropout neural networks using googles tensorflow deep learning library 
the performance of the proposed model was compared with other popular learning models used
 

fifigure    over training

figure    accuracy benchmark

by the kaggle challange participants  xgboost   gradient boosting  we decided to train an ensemble of neural networks instead of just a single complex network to further reduce over fitting     
from the input vector of    features    were removed via manual selection to prevent over fitting 
the input vector was normalized to have a mean      stddev     the final learning model was
an ensemble of    dropout neural networks with   hidden layers consisting of     neurons each 
  x   x   x   x   the outputs were two softmax neurons  which individually predicted the
probability of a sample being a signal or an event  an input was classified as a signal if the output
of the softmax signal neuron was greater than         results from each network  from the bag 
were simply amalgamated by averaging the output probabilities  the model was trained with
  fold stratified cross validation with random shuffling to prevent overfitting  additionally  the cost
function was set to reduce the mean of the cross entropy loss  each neuron in the hidden layer had
a     probability of being dropped 
figure   shows the output probability distribution of a model that randomly classifies an event with
an even probability of      such a purely random classifier results in an ams score of     on average 
the output of the other three models show a distinct segregation between the signal and background
events  which correspond to the accuracy of the process  in each instance  if the output probability of
the event being a signal was greater than      the event was classified as a signal  figure   shows that
our bag of dropout neural networks perform marginally better than xgboost and gradient boosting
      ams higher   the final score of the network ensemble averaged out to       ams  the
winning kaggle submission          prize  had a score of       our score roughly corresponds to a
lb score  l  norm  of     accuracy  the model was also validated on a separate dataset of sample
size          

figure    cpu vs gpu  time taken to complete     epochs

 

fiin terms of scalability  our dropout neural networks definitely under performs in relation to
xgboost and gradient boosting  the xgboost model took about   minutes to train on a quad core
cpu  whereas a single neural network took nearly    hours to achieve similar performance  the
training dataset was composed of          samples  but the atlas experiment generates petabytes
of data  which could be computationally very expensive to train neural networks with 
gpu acceleration was used to speed up the training process  we setup our dropout neural networks
on an amazon ec  instance with   gpus each containing        cuda cores and  gb of memory 
as shown in figure    the gpu cluster was nearly    times faster than a standard quad core cpu 

 

conclusion

our proposed solution to the higgs boson classification problem had an accuracy of       although  the testing dataset was considerably small compared to the amount of actual data produced
by cern  so it is still possible that the model was over trained  in terms of improvements  we could
consider boosting the dropout neural networks     instead of simply averaging results from the bag 
another neglected aspect was feature engineering  understanding the physical significance of the
input measurements could help identify the features that are over training the model  furthermore 
pca can be used to reduce the dimensionality of the input vector to improve performance and reduce
the time taken to train the model 

 

references

    atlas collaboration         dataset from the atlas higgs boson machine learning
challenge       cern open data portal  doi          opendata atlas zbp  m t  
    jerome friedman  greedy function approximation  a gradient boosting machine       
    nitish srivastava  geoffrey hinton  alex krizhevsky  ilya sutskever ruslan salakhutdinov 
dropout  a simple way to prevent neural networks from overfitting  journal of machine
learning research       pages           
    ury naftaly  nathan intrator and david horn  optimal ensemble averaging of neural networks 
comput  neural syst        pages         
    holger schwenk  yoshua bengio  boosting neural networks  comput  neural syst       
    rupesh kumar srivastava  jonathan masci  sohrob kazerounian  faustino gomez  jrgen
schmidhuber  compete to compute  nips      
    peter sadowski  pierre baldi  daniel whiteson  searching for higgs boson decay modes with
deep learning  advances in neural information processing systems       pages           

 

fi