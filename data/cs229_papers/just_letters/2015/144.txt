gradient boosting  this link gave a useful explanation of how gradient boosting works 
   microsoft time series algorithm   msdn
another alternate approaching to sales data is microsoft time series which is based on a core algorithm called autoregressive integrated moving average  arima   one component of arima is the
autoregressive model  which models a predicted
value x t  as a linear combination of values at previous times  for example a    x t      a    x t   plus a random error term  the other component
of arima is the moving average model  which
only depends on past errors  errors before time t  
   different approaches for sales time series
forecasting   bohdan pavlyshenko this post on
kaggle compared the performance of linear regression with regularization  arima  conditional
inference trees  and gradient boosting  from this
resource  we ultimately determined that no single
approach behaves the best on all stores  convinced
us to focus our efforts on algorithms that assume
the data points are i i d  instead of using time series algorithms  which would be harded to work
with and optimize 
   we also gained a lot of insight from the    
poster session  as there were several groups working on the rossmann competition  this included
insights such the importance of parameter tuning
and an idea to train on individual stores 

rossmann store sales
david beam and mark schramm
december     

 

introduction

the objective of this project is to forecast sales in
euros at      stores owned by rossmann  a european pharmaceutical company  the input to our
algorithm is a feature vector  discussed in section
   of a single day of data for that store  we tried
using a number of algorithms  but mainly gradient boosting  to output the predicted total sales
in euros for the given store that day  rossmann
provides a massive     million example data set
as part of a sponsored  by rossmann  contest to
see who can best predict their sales  over     
groups have entered the contest  with the best
group achieving      root mean squared error 
this project is also being used for cs      and
use different parts of the project for that class
that includes k means to group the data and an
markov decision process  mdp  to model store
output over a period of time  for this class  our algorithm focused around regressive algorithms and
decision tree based regression algorithms like gra    datasets and features
dient boosting  as a baseline algorithm for both
classes we used support vector regression  svr 
store
day of week
date
over an unprocessed feature set  however  to imcustomers 
open
promo
prove performance we used an augmented feature
state holiday school holiday
set  we also experimented with linear regression
and neural nets with less success 
table    features   customer data was provided 
and was the most useful feature  however it is im  related work
portant to be able to predict sales without using
   exploratory analysis rossman   posted by customer data  as future customer numbers are unchristian thiele on the kaggle competition board  known  while some initial trials used customers as
this resource useful insights for augmenting our a fetaure final results and tuning were done withfeatures such as using the log of the distance be  out it as discussed below 
tween stores and competitors instead of the actual
the full dataset provided by rossmann indistance 
cludes approximately     million training exam   introduction to boosted trees   tianqi chen ples consisting of the features shown in figure   
university of washington we used gradient boost  there was also meta data for each store  some ening because many competitions used and endorsed tries were incomplete  however every entry listed
xgboost  a python library for decision tree based the store type  assortment  inventory  and distance
 

fito closest competitor  two stores were missing
this  so we filled them in with a large value to approximate a very far competition distance   these
three meta data features were used as well  the
open feature was a special case  if the store was
closed  sales were always    we did not train on
any examples where open was   and we always
predicted   sales when we saw this at test time 
some data pre processing was necessary  positive entries for the holiday fields  as well as the
store type were given as a  b  c  or d  we
converted these values to numbers      for compatibility with our algorithms  the date field was
mapped to the day of the year         and the year
was turned into a separate feature  not used in our
final experiments as it ended up slightly hurting
performance   lastly  competition distance was
converted to the log distance as recommended by
exploratory analysis rossmann  and this helped
performance 
for almost all training and testing  we did not
use the full     million point data set  but a smaller
set of        training examples         development samples  and        test samples  all randomly selected once  then saved into excel for
consistency  we employed simple cross validation  tuning on the dev set  then finally testing on
the test set  for some final testing  we did ran
on the full     million entry dataset  here we randomly split off     of the data to use for training
and the remaining     for testing  no dev set was
used as there was no tuning done at this point  and
in general we saw very little variance between dev
and test results  most likely due to the similarity of
the data 

restrict ourselves to a linear svr kernel because
the features seemed only to relate linearly to sales 
the linear program of our svr was therefore minimize 
l

x
 
  w      c
 i   i    s t
 
i  
yi    w  xi   b     i
  w  xi    b  yi     i  i   i    

   

feed forward neural net

we briefly tried using a feed forward neural net 
neural nets used a series of  weight matrices as
well as a nonlinear function  we settled on tanh 
to take an input feature vector and output a prediction  in our case a regression sales value  the
feed forward hypothesis function of a neural net
with weight matrices u and w   an input vector x 
and two bias vector b  b  would be calculated as
follows  note this is based off an assignment from
cs     
p  x i      h    u  tanh w x   b      b   
where h   is the sigmoid function  we can then
use the least squared loss function 
m

j    

 x
 p  x i     y  i    
  i  

and minimize this loss with sgd to optimize all
of our parameters  we were not able to get good
results though using neural nets  despite trying a
few different hyper parameter values  our best rms
was a disappointing      or       with customer
data included  this is likely due to the low dimensionality of the data set  neural nets our generally
  methods
at their best with large input vectors     or     at
in total we tried four algorithms  we used svr as a least  to tune and influence weight matrices  neubaseline  gradient boosting as our main algorithm  ral nets had not really been connected to this proband also tried linear regression and a feed forward lem at all  but it was still interesting to try a deep
neural net  the last two algorithms showed poor learning approach 
initial results and were abandoned  all four how    linear regression
ever  will be discussed in some detail below 
we also briefly tried linear regression to see how it
    svr
compared to svr  despite running quickly it persvr is just the regression version svms which formed worse than svr on initial runs  so we dewe studied in class  after looking at visualizations cide to stick with boosting as the main algorithm 
of the sales over time on kaggle  and determining regression played a small part in our project and
the effects of the features on sales  we opted to was studied in class so it is only discussed very
 

fip
where gi
 
and gi
 
iij gi  
y t   l yi   y  t   that is the sum over all training
instances i contained in leaf j of a tree  the ws are
the weights of each leaf and  is some constant of
the l  norm of the weights  the optimal weight
of each leaf and resulting objective value are
pt
g  
gj
and obj       j   hj j   t  
wj     hj  
in practice  the tree is grown greedily using the
change in the objective function value based on a
 
 
gl  gr  
   the terms
split  hgll    hgrr   h
l  hr 
here are the score of the left child  right child 
score if we dont split  and complexity cost of an
additional leaf  respectively  we can do a linear
scan over a sorted instance  calculating the above
expression each time to greedily determine the
optimal split over our data 
a key step to improving the performance of
boosting was hyper parameter tuning  to do this
we used a greedy version of sklearns gird search 
which given a grid of hyper parameter options
exhaustively tries each combination to see which
performs best  to save a massive amount of run
time  we only tuned one or two parameters at a
time  greedily assumed that this would be the best
value  and then moved on to the next parameter  adding in our newly tuned values  of course
the greedy method does not work perfectly  and
some parameters were re tuned downstream  for
example initial tuning led us to a large learn rate
of    however this was later decreased to     despite having to occasionally re tune greedy tuning
saved dozens of hours of run time and still reduced
our rms by almost      

briefly  it uses the same least squares cost function as neural nets  with the difference being rather
than using the hypothesis function p  x i     we
just use the sigmoid function 

   

gradient boosting

our primary algorithm was gradient boosting 
gradient boosting is a very powerful supervised
learning algorithm that uses regression trees  each
regression tree splits the data at each non leaf node
based on a constraint on a feature  i e  distancetocompetitor         the leaf node value of a
data point determines its score  and given multiple regression trees  a data point is predicted by
summing the scores of the leaf node it belongs to
across all of the regression trees  the functions
generated by these regression trees are actually
step functions the jump where the data splits  assuming we have k trees  the generic model for the
regression tree is
yi  

k
x

fk  xi  

k  

and the objective function we wish to minimize is
n
x

l yi   yi    

i  

k
x

 fk  

k  

where l is our loss function and  is the measure
of function complexity  gradient regression uses
square loss for l but a number of different function complexity measures that typically incorporate the number of leaves and the l  norm of the
leaf scores weights gradient boosting  or additive
training  expresses a function in terms of a sum of
the previously attempted functions  so

 
   

results and analysis
general results

most of our development was done working with
gradient boosting  as previously mentioned  our
yi t   
fk  xi  
boosting trials will be thoroughly discussed in seck  
tion      that said there were still some general
substituting into the objective function  using trends and results discovered from the other alsquare loss  using a taylor series expansion  and gorithms  in general our baseline svr  out perusing a standard definition of function complex  formed linear regression and neural nets in all
head to head trials  it is entirely possible that
ity  our new objective function becomes
we could have gotten results close to or as good
t
as those of boosting had we decided to focus on
x
 
svr  however boostings much faster run time
 gj wj  hj    wj       t
 
j  
t
x

 

fi    minutes  svr  vs   seconds  un tuned gb 
  min  tuned   and slightly better preliminary results made it a more desirable choice  the comparison of performances with and without customer data showed just how useful of a feature
it was  its inclusion with no other changes doubled the performance of svr and boosting  as
seen in table     while  again  we couldnt use
it on final trials as future customer data is of
course unknown  it was a good indicator of how
well an algorithm would work  for example the
fact that we could not get a good prediction with
neural nets using customer data told us that its
prospects for final testing without customer data
were not high  figure    reinforces the importance
of customer data by offering a visualisation showing the correlation between customer data  promotions  and sales  we chose the customer and
promotions features for this graph as they were
shown to be the two best by the sklearns select
k best feature selector   the sales data is more
or less linear with respect to customers  while an
active promotion increases the slope of the line 
the customer feature was so influential that we
even tried first predicting the number of customers
for a given day  then including that as a feature to
predict sales  however this gave us slightly worse
boosting performance  again shown in table    
dataset size was one variable that had varying impacts on different algorithms  regression for example converged quickly  and showed the same
results for a   k set as the full    k set  boosting however showed a       performance gain
algortihm
svr with meta
svr no meta
nueral net
linear regression
linear regression
boosting no tuning or meta
boosting no tuning
boosting no tuning
boosting no tuning
boosting tuned store by store
boosting tuned
boosting tuned

when increasing the amount of data  we did not
run the full data on svr due to the long run time 
and neural nets showed no improvement moving
from  k train to  k train so we stopped there  

figure    data visualisation  a plot of customers
and promotions vs sales

figure    tuning  this graph shows the effect
of hyper parameter tuning on rms using the   k
dataset

train data size
  k
  k
 k
  k
   k
  k
  k
  k
  k
  k
  k
   k

test data size
  k
  k
 k
  k
   k
  k
  k
  k
  k
  k
  k
   k

cust used
yes
no
yes
yes
yes
yes
yes
no
predicted
no
no
no

  rms
    
    
    
    
    
    
    
    
    
    
    
    

table    full results  test error is reported  test and dev error never varied by more than      boosting
 

fialways used store meta data unless otherwise specified    rms was calculated by taking total rms and
dividing by the average sale amount       euros

   

boosting

of data  meaning its should contain a large variety on its own  its still not guaranteed to maintain low variance on unseen future results  one
failed experiment was an attempt to train an individual boosting model for each store  instead of
one large one  other work on this problem as well
as the poster session implied that this should improve performance  however  we had a large error
using un tuned values about      and a staggering       error using values tuned for one large
booster  this high amount of error leads us to believe there may have been some subtle implementation error in our code  however no bug was apparent upon inspection  regardless we were still
happy with our final       number and probably
could have pushed it even lower if we had continued increasing the number of estimators 

we spent the most time focusing on optimizing
our boosting  early on  even when working with
svr  we saw that the addition of store meta data
as features improved performance  and indeed we
saw a    rms gain when we added this  other
small gains came from the aforementioned data
set expansion  as well as taking the log of the
competition distance rather than the full distance
 this was used in tuned trials  and on its own provided roughly a    increase   by far the biggest
gain however  came from hyper parameter tuning 
once we started tuning we got our rms down from
       to      and finally        using the grid
search method discussed in section     we settled on a learn rate of     max recursion depth of
   on minimum split of    and      estimators 
the effects of tuning can be visualized in figure
   it is important to note that this graph is somewhat biased  the tuning gain versus parameters
tuned trade off was more of a function of some
parameters leading to much larger gains than others  had we made this graph in a different order
the data would look somewhat different  however
it serves its purpose in showing that initially we
saw large performance gains that tapered off as we
got further into tuning  we found that the more
estimators we added the longer the run time and
the better the performance  as far as we know this
trend would continue for quite some time  we ultimately stopped pushing the number estimators
higher due not only to the increasing run time  but
more importantly a fear of over fitting to the train
data  while our validation still showed low variance  the train dev and test data are all very similar as they were taken from the same     million
data set  and while this large represents   years

 

 

conclusion

we were able to achieve       rms error using
gradient boosting  our main algorithm  to predict rossmann sales without using customer data 
boosting worked very well for this data set  most
likely due to the fact that this data is extremely
dense  which is known to be ideal for boosting 
we also got decent baseline results using svr 
given next to no tuning was done for this algorithm  given more time we could explore svr in
more depth  other future work would include continuing to push the upper bound on the number of
estimators used on the data  and trying to get better results using the individual store method  we
could also more finely tune hyper parameters using a more algorithmic method such as bayesian
optimization  rather than the brutish grid search 
any of these paths could push our error down to
current state of the art levels of    rms error 

bibliography

   microsoft time series algorithm  microsoft time series algorithm  microsoft developer network  june       web     dec       
   chen  tianqi  introduction to boosted trees          n  pag 
https   homes cs washington edu  tqchen pdf boostedtree pdf  university of washington     oct       
web     dec       
 

fi   thiele  christian  dashboard  rossmann store sales  kaggle    oct        web     dec 
     
   from sklearn we used linearregression  svr  and gradientboostingregressor  we used pybrain
for our forward feed neural net 
   manning  christopher  neural networks for named entity recognition  stanford university  n d 
web     dec       
  pavlyshenko  bohdan  rossmann store sales  different approaches for sales time series forecasting    kaggle     nov        web     dec       

 

fi