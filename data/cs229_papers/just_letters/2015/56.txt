vision based hand hygiene monitoring in hospitals

cs    fall      project report
zelun luo  boya peng  zuozhen liu

abstract

performs the hand hygiene action using various
computer vision and machine learning methods 
this action is defined by a person placing his or
her hand under a hand hygiene dispenser and receiving soap  therefore  this problem can be formulated as a supervised binary classification task
with raw visual image inputs 

hand hygiene has been shown to be
an effective intervention to reduce transmission and infections in many studies 
this project focuses on interpreting visual clinical data for hand hygiene monitoring  we propose two distinct deep
learning approaches to detect hand hygiene action on manually collected and
labeled data  specifically  we investigate
a fixed window and a pose based hand
detector using convolutional neural network  cnn   we show both approaches
are able to achieve high accuracy and outperform our baseline model using linear
support vector machine  svm  classifier 

 

provided that deep learning methods have
achieved state of the art performance in various
computer vision tasks in recent years  we want to
apply cnn to solving this classification task  we
are also interested in exploring pose based approaches that may be easily extended to monitoring clinical activities in the healthcare settings 

 

dataset

for an initial pilot study  we collect both depth
and rgb data from a depth sensor mounted in
a lab environment  we collect a dataset of  hour depth and rgb signals  from which we extract        frames containing     positive hand
hygiene frames  examples of depth images are
shown in fig    these frames are then used
to train and evaluate our svm baseline model 
cnn based hand hygiene detection model  and
the pose based model  since the dataset is highly
imbalanced as we have far more negative frames
than positive ones  we use cross validation to
tune the ratio of positive and negative frames in
the training set 
for our pose based approach  we use the
hand dataset found at oxford visual geometric
groups repository      we use        hand images with        synthesized negative examples
to train the hand classifier 

introduction

with recent success of deep learning in computer vision  visual clinical data can be exploited
to improve the understanding of patient experience and environment during hospital stays 
such data can contain rich information about patient condition such as the appearance of distress 
which has been described as the  th vital      as
well as details about the occurrence and nature
of clinical activities ranging from patient care
to bundle compliance and hand hygiene  however  visual clinical data still remains an underexplored source of information in the healthcare
settings 
in this project  we want to make use of valuable
visual clinical data in the hand hygiene setting
where the objective is to detect when a person

 

fifigure    examples of depth images from our dataset  from left to right  the first two are positive
instances of hand hygiene  and the last two are challenging negative instances 

 
   

approach

video frame  we select the fixed window to
be a cropped region containing the dispenser  a
     pixles region near the dispenser  

svm baseline model

the network architecture consists of two convolutional layers  each followed by a max pooling layer  and two fully connected layers  since
the input images have relative small dimension 
we decide that two convolution steps are enough
to extract important high level feature representations for classification  the output from the
fully connected layer is a binary classification
of whether the hand hygiene action is occurring 
and we optimize a logistic loss function using
stochastic gradient descent  we then use cross
validation to tune all hyperparameters to determine the final architecture of our cnn based
model shown in fig   

for each image  we first crop a   x   region
that contains the dispenser  this region encodes
rich information regarding the hand hygiene action and we can train a classifier using features
directly based on raw pixel values  our classifier is a linear svm model with hinge loss function  in order to combat overfitting  we also incorporate l  regularization into our svm model
which is equivalent to the optimization problem
expressed below 
m

min w b
s t 

   

x
   
i
kw k   c
 
i  

in our experiments  we compare this model
against a similar cnn model trained on the full
frame image         pixels   in addition 
we also compare the accuracy between models
trained on rgb images and depth images respectively 

y  i   wt x i    b      i  
i    
i              m

fixed window hand hygiene detection

in this approach  we train a cnn model to detect whether hand hygiene action occurs in a

figure    cnn based hand hygiene detection model architecture
   

pose based approach

proach  we first train a cnn based hand detector that detects hands in a      pixels region 
the model consists of two convolutional layers 
each followed by a max pooling layer and the
training procedure is almost identical to the first
cnn model 

we train a cnn based detector for hand joint
in each detected human region  since the hand
is what performs the hand hygiene action  we
then consider hand hygiene to be performed if
a hand is detected in the physical space immediately under the hand hygiene dispenser  in
such scenario  the hand placement is most likely
to trigger the soap to be dispensed  in this ap 

in the classification phase  for each input image 
we use the sliding window method to extract regions of size      pixels with a stride of   pix 

 

fithe distance between the closest hand and the
dispenser is smaller than a fixed threshold  fig  
shows the architecture of our pose based model 

els  we then feed each extracted region into our
hand detector and predict if a hand is detected 
hand hygiene is considered to be performed if

figure    pose based model architecture

 

experiments

mance in vision tasks  however  the results obtained from cnn over the entire image are less
satisfactory due to undesired noise in the rest part
of the image  fig    shows the detection results
on cropped depth images  we observe that the
cnn model over dispenser region correctly detects hand hygiene action when an arm is stretching out to the dispenser  however  from the false
negative results shown in the second row  this
model fails to detect the action when a person
comes too close to the dispenser  this limitation
is caused by partial occlusions due to a top down
viewing angle of depth sensor 

using the dataset described in section    we perform evaluations on all of the approaches discussed in previous section on both rgb and
depth images  taking the target class imbalance into account  we use mean average precision map  as our metric to reflect the performance of our different detectors  the results for
these detectors are displayed in table    for privacy reasons  depth images are preferred in realistic settings and we want to make sure that our
models are able to adapt to this challenge  note
that our hand detector in pose based approach is
trained only on rgb images  therefore  this approach is currently only limited to rgb images 

the pose based approach has a worse performance than fixed window cnn  one challenge
is that peoples hands are often occluded by the
dispenser when they are performing hand hygiene and this method depends on hand detection to make final prediction  such input frames
are likely to be classified as negative as the hand
detector is not able to detect any hands  on the
other hand  false positive results sometimes occur when peoples hands are within the distance
threshold from the dispenser but are not performing hand hygiene actions  however  this method
has a potential benefit of being able to tie the
action to its performer  overall speaking  this
model achieves decent performance considering
its simplicity  more complex pose based models
can be further explored to resolve the challenges
mentioned above  fig    shows different detection results using the pose base approach 

from table    we observe that both approaches
outperform linear svm baseline on rgb images  the intuition is that linear svm baseline
imposes strong bias on the hypothesis class in
rgb space and the model is underfitting due to
lack of complexity  one promising solution is
to train svm with various kernels to learn nonlinear boundaries in the rgb space  however 
the svm baseline performs surprisingly well on
depth images  we believe that the reduction in
dimension from depth images gives rise to a simpler decision boundary that can be well represented by a linear boundary 
the cnn model over dispenser region is able
to achieve strong performance on both rgb and
depth images  the results also validate the success of cnns achieving state of the art perfor 

svm baseline over dispenser region
cnn over full image
cnn over dispenser region
pose base approach

rgb
     
     
     
     

depth
     
     
     


table    average precision of hand hygiene action detection 

 

fifigure    examples of detection results using cnn approach over dispenser region  green for
correct labelings  red for incorrect labelings 

figure    examples of detection results using pose based approach  green for correct labelings  red
for incorrect labelings 

 

conclusion

from depth images  but they still fail to address
the challenges of handling different camera view
points  e g  top view  and occlusions 

we believe that the recent success of using machine learning techniques over depth signals to
perceive the world could have an unprecedented
impact in health care  we have shown that it is
possible to detect hand hygiene compliance  an
important component of reducing the cost associated with hospital acquired infections 
for future work  we plan to work on a poseestimation model which is invariant to different
view points and can handle self occlusion  a
pose estimation model allows more general applications such as person identification  activity
recognition and characterization  which can be
broadly applied in a healthcare setting  although
there has been various work focusing on  d  d
pose estimation from rgb images          work
on  d pose estimation from depth images are
relatively scarce  shotton et al     appear to
represent the state of the art for pose estimation

references
    d  howell and k  olsen  distress the  th vital
sign  current oncology                  
    a  mittal  a  zisserman  and p  torr  hand detection using multiple proposals 
    j  shotton  a  fitzgibbon  m  cook  t  sharp 
m  finocchio  r  moore  a  kipman  and
a  blake  real time human pose recognition in
parts from a single depth image  in proc  cvpr 
ieee       
    j  tompson  a  jain  y  lecun  and c  bregler 
joint training of a convolutional network and a
graphical model for human pose estimation 
    c  wang  y  wang  z  lin  a  l  yuille  and
w  gao  robust estimation of  d human poses
from a single image  in ieee conference on computer vision and pattern recognition       

 

fi