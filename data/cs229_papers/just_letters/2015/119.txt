rossmann store sales quantity prediction
cs    course project

vladimir sazontyev  stanford university
wolod stanford edu
abstract
my project aimed to solve machine learning problem for rossmann inc  i use rossmann dataset that contains data since     
year till       rossmann is challenges to predict   weeks of daily sales for       stores located across germany  end to end
workflow of data science was applied in the project  from data retrieval  data processing  statistical modelling  machine learning 
and data visualization and data analysis  in my project i implement various machine learning algorithms  with focus on showing
the evolution of machine learning algorithms  that i use to solve the problem 
introduction
motivation
rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance  store sales are influenced
by many factors  including promotions  competition  school and state holidays  seasonality  and locality  with thousands of
individual managers predicting sales based on their unique circumstances  the accuracy of results can be quite varied 
problem definition
we are given dataset from rossman inc  they ask us to write an algorithm that will predict quantity of sales in each of next   
days  the evaluation metric for this problem is rsmpe  the rsmpe calculated as follows 

 where yi   denotes the sales of a single store on a single day and i   denotes the corresponding prediction  any day and store
with   sales is ignored in scoring  we challenged to predict continuous variable  to be explicit  we should predict field sales 
related work
as a related work i found useful article by prof d yakonov      in his article he suggested to use different weighting schemes and
probability of coming to store in specific day of week  this is a very clever approach  but unfortunately this approach can t be
applied directly to our problem  but i made a try in my project    unfortunately it doesn t improve my algorithm at all 
also i was impressed by havard rue and oyvind salvesen     who used a bayesian linear model to predict soccer results  they
used a time dependent model using relative strength of attack and defense of each team 
i found interesting work by pierre geurts     his approach helped me how to move to dicision trees  but unfortunately his
techniques are hard to implement in short time  and they highly depends of timeseries that you really have  and as i see it won t
improve more than i can extract from dates analytically 
one more notable book by ian h  witten  eibe frank     especially interesting chapters was about decision trees  this helped me
to get into xgboost approach  as well as paper by jerome h  friedman     related to this library  it is very deep and good
explanation how approximation accuracy of gradient boosting can be substantially improved by incorporating randomization
into the procedure  and also explains based on what xgboost library works so well 

dataset
dataset is provided by rossman  list of available columns presented in table   

table    columns by files of given dataset

fidata visualization and analysis
i start my analysis with table   that shows amount of unique values  nans  and unique values if there less than    of them 

table    simple data information
from this table we see that there    nans in  open  on test  also in test there only    a  value in  stateholiday  field it s
different from what we see in training set   on the next figure   we see distribution among all sales  left part of figure     i took
logarithm of it and it become even more bell shaped  right part of figure    

figure    distribution among all sales  left side    logarithm of it  right side 
also from table    we clearly see  that we have field sales and customers  that are presented in train csv  but not presented in
test csv   sales are not presented since we should predict the values of this field for whole this test csv  customers are not
presented since they highly correlate with sales field  i plotted graph date days in training set by week and over all stores and
years 

figure    amount of days by weeks and by years among all stores 

fifrom figure   we clearly see we see  that in the second year        there is a clearly less days registered since    th week and
amount of days restored since new year        it happens  because statistics for     store ids are missing since    th week up
to    week  but they present in test set  we should predict for them   one more point  that i found  that last   days of month
greatly changes the behavior of sales curve 
machine learning algorithms and approaches
in my work i tried to follow prof  andrew ng workflow      unfortunately  his approach to analyze learning curve  is not clearly
useful  because we deal with timeseries date  and older data not always train our model better  and we can say that old data is
equally useful as recent one 
i started with linear regression  because it is a really simplest and fasted  in terms of implementation time  model  i used this
model as a baseline  just to comment algorithm   this is an approach for modeling the relationship between a scalar
dependent variable y and one or more explanatory variables  or independent variables  
next model   is a locally constant solution  this was not taught during this course  but i think this is a good model compared to
our baseline  because it is obviously separate our decision space better than our baseline  by separating i mean  that you
separate space and assign some fixed value  as a fixed value i choose simple mean  i made experiments with median  but it
worked worse  also i tried to multiply fixed value by constant  but since this is not a final model  i left that idea 
third model  is based on previous one  i just substituted fixed value   mean by group  with linear regression by group 
one more step that i tried to do  is to incorporate knowledge of related works  especially a lot of effort was done to implement
weighting schemes that was proposed by prof  d yakonov s paper      those schemes are 

unfortunately  no one of them even barely decreased my rmspe  as well as trying to use month year as feature    to
make sure that i implemented it correctly  i tried this on toy problem  and it worked  but here it doesn t  
and the final model is based on random forests   xgboost       this is a library that is designed  and optimized for boosted  tree 
algorithms  it simply choose features and build decision trees on that features  and after that it averages the result  also   i must
mention that we can increase number of trees to as much as we want and make it as large as we want  only computational time
really matters in this case  figure   perfectly describes the decision tree in case of regression problem 

figure    decision tree of xgboost library for regression problem
and final thing that i used   i tried ensemble different algorithms  even if we train only models based on xgboost  it turns out
that if we ensemble different number of trees  iterations  it used to give better results  one more approach that i used is based
on bell shaped distribution of number of sales   is to take logarithm of number of sales  then use exponent of predictions as
solutions 
i tried to justify based only on learning curve  but best justification to move in that direction is that each generation of models
that i used is simply separate better our space  and from this point of view turns our better for us 
machine learning algorithms and approaches  and findings
the first step was to implement very simple model  just to have a baseline i split my training set on two parts  first part for
training   it contains all dates from very first date            till             and second part for test training   since           
till             the intuition behind these dates  that for local evaluation i used same exact amount of days  as we asked to

fipredict in problem  i can t truncate     of training set and set this as training set  and     as test set  since this problem
involves timeseries  and old data might produce non relevant prediction 
i tried very simple model  as a baseline   sklearn linear model linearregression        without any regularization  since we
clearly do not overfit our data  as features i used  store and dayofweek  i got        on train set and        on test set  based
on rmspe  on local cv evaluation 
from figure    we see that obviously  such simple model can t represent good our data  since it lack of features and what is
really meaningful that it separates space badly   see prev  paragraph  

figure   learning curve for the first step model  training and test set error  x axis   amount of training examples  yaxis rmspe error 
the second step that i made was to take mean of sales grouped by store and dayofweek  and report that mean as a prediction
that gave me on my local cv evaluation           rmspe   the intuition behind it  that we made more complex model  with same
features  the learning curve is on figure   left side  
when i added promo   schoolholiday and open feature in this model my model showed me on my local cv evaluation        
and on public leader board          also i tried to incorporate my finding  that last   days significantly changes curve of sales
amount  on figure    right side    you can see learning curve  where   means   use   month as training                              use   month for training                             the greater value means we use more older data  all other learning
curves are given based on same scheme 

figure    left side  learning curve for second step  right side   same step with   new features  training and test set
error  x axis   month that used on training  y axis rmspe error
the third step   i substituted simple mean  with linear regression just like in first step  but now i have a lot of such linear
regressions for each group separated by store  dayofweek  promo  schoolholiday  open  this gave me on my local control 
        and i used last   month for training                           learning curve is provided on figure   

fifigure   learning curve of third step  training and test set error  x axis   month that used on training  y axis rmspe
error
the forth step   i used random forests extreme gradient boosting   xgboost  this is a library that is designed  and optimized for
boosted  tree  algorithms  the good explanation how it works is given on their page       based on this explanation i see that it
does pretty similar work as i did in the second and third steps  but it splits space even more  i used these features
store competitiondistance competitionopensincemonth competitionopensinceyear promo promo  promo sinceweek promo
 sinceyear schoolholiday dayofweek month year day year storetype assortment that gave me         on public leaderboard
 number of iterations       
as a fifth experiment i tried to mix the solutions    ensemble   simply add them with some coefficient  obviously  coefficients
summed up to one   i mixed models with different numbers of iterations                   of trees and one model  where
solutions where i took logarithm of solutions  and took exponent of prediction   but this approach worked only for this case  it
did not worked with previous models also i added feature isinlast days  i made it to show it to help algorithm  to understand
that it must react differently if data is in range of last   days of month   on my local control  this approach achieved        but
on public leaderboard it might show greater value  since this method take a lot of time   and i did not planed to improve after
this method  i did not made a plot  because it will took days of computation 
results 
model
local control
linear regression
      
locally constant solution
      
locally constant solution  with   more features 
       
linear regressions by groups
     
xgboost random forests with     iterations
ensemble of xgboosts models
     
table    results of algorithms

public leaderboard

       
       

for the final mode l used same configuration for all xgboost models in ensemble  except number of trees  
 objective    reg linear    eta          max depth       subsample          colsample bytree          i used random search to
find these parameters  in simple grid search we go over fixed points in space and simply try our model  in grid search we only
specify range in which parameters can vary and then chose points with random  coordinates   by point i mean set of
parameters  by coordinate i mean parameter that we try to find   this page efficiently searching for optimal tuning parameters
    explains the advantages of this approach 
further works
as further work i suggest to use external weather data  and actually i tried to use it  and find that difference between
temperature of current day and previous one significantly influential on curve of number of sales  if you add features like that to
your model you probably we get even better score 
contributions based on course
since this course was a homework for other course  i must mention  that data visualization  to be more specific   the second
part  where i discuss about lost data for     stores and that   last days matters  was done for that course and cs     also
xgboost  only forth step  was done for both courses  all other findings was done after homework submission   specifically for
cs    

fireferences
    a  d yakonov  supermarkes clients behavior forecasting by weighted methods of probability and density estimations 
business informatics            msu       
    h  rue and o  salvesen  prediction and retrospective analysis of soccer matches in a league  journal of the royal statistical
society  series d  the statistician                      
    p geurts  pattern extraction for time series classification principles of data mining and knowledge discovery  springer berlin
heidelberg      
    ian h  witten  eibe frank  data mining  practical machine learning tools and techniques  second edition  morgan kaufmann
series in data management systems  morgan kaufmann publishers inc  san francisco  ca  usa      
    jerome h  friedman   stochastic gradient boosting   computational statistics   data analysis                     
    andrew y  ng  advice for applying machine learning  stanford university  http   cs    stanford edu materials ml advice pdf
   tianqi chen  http   xgboost readthedocs org en latest model html      
    pedregosa et al  scikit learn  machine learning in python  jmlr     pp                  
    kevin markham http   blog kaggle com            scikit learn video   efficiently searching for optimal tuningparameters      

fi