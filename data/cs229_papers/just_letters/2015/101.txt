predicting student earnings after college

miranda strand
tommy truong

   introduction
many students see college as an investment to help
them earn more and live better lives after graduation 
while it is true that college graduates earn more on
average than those without a degree  large numbers of
students today are graduating with worrying amounts
of debt  calling into question the assumption that attending college is always the wisest investment  it has
become more important then to understand the factors that contribute to post graduation earnings and
the ability to repay student loans 
a common belief is that the prestige of a university
affects the future income of college students  but prestige is likely not the only factor  we looked more
closely at some of the other variables associated with
a college that could potentially predict the financial
future of its students 
our goal was to create a model that would accurately
predict the earnings of a colleges graduates given specific features of the college  such as its acceptance rate 
average test scores  student body demographics  and
the student loans needed to attend  the insights provided by such a model could give incoming college students greater knowledge about the features to consider
when choosing a college  the results also yield interesting insights about the american higher education
system on the whole 

   related work
there has been interest in the relationship between
college education and earnings after college for many
decades  in our exploration of the previous literature on this topic  we came across several papers that
looked at the effect of college selectivity and quality to
earnings of students 
in their paper  brewer  eide  and ehrenberg  brewer 
      built a choice model to determine a students
earnings  using the assumption that a student would
pick a certain type of college based on their individual characteristics  we thought that using individual characteristics to build this choice model  instead

mstrand stanford edu
tommyt stanford edu
of solely relying on college specific data  was clever 
but we felt that their reduction of colleges into only
six classes potentially lost subtle but significant differences between colleges 
rumberger and thomas  rumberger        similarly
considered both individual and college features in
studying the impact of three variables on earnings after college  college major  school quality  and student
academic performance  we thought that they were
astute in using hierarchical linear modeling to address
the fact that their data was composed of nested samples of students all in the same few colleges 
in another similar paper  loury and garman  loury 
      take a more economics minded approach to
building their model by assuming that students would
try to maximize net earnings by picking a college where
the marginal product of attending the college would
equal its marginal cost  we liked that they considered
several potential costs of attending a selective college 
such as higher tuition and increased likelihood of failing to graduate  but we felt that their study was limited by their data  which only looked at male students
who were either white or black 
oddly enough  james  alsalam  conaty  and to
 james        similarly limited their paper on this issue to male students only  however  we thought they
had a clever approach of incrementally building their
model by selectively adding more feature sets 
in a different approach  wachtel  wachtel        focused on looking at the effects of increased investment
in college in relation to earnings instead of considering a variety of both individual and college features 
we thought his concentration on just two expenditure
categories  the amount of time spent in college and
the amount of money spent per year in college  helped
make his paper more targeted and focused  however 
wachtels data was even more limited than the previous papers   it only had information for white  male
volunteers for army training tests 
the major limitation common to all of these papers is
that they relied on data collected almost twenty years
before their publication  and the scope of the college

fipredicting student earnings after college

data they collected is considerably smaller than that
of the college scorecard dataset that we used in our
project  this scorecard dataset has only recently been
made publicly available by the government  which explains why we did not find literature making use of
this dataset  we thus have the privilege of working
with a large  up to date  comprehensive dataset  with
many more features than any dataset in the literature
we researched 

graph  we found that the first principal component of
the features  which captured about one third of the
overall variance in the data  also seemed to capture
some of the variation in post grad earnings  as shown
in figure   

   dataset and features
in       the us department of education matched information from the college financial aid system with
federal tax returns of the graduates of those colleges 
creating the college scorecard dataset  a wealth of
information intended to help students and families
make the best decisions about where to attend college  for the almost       colleges included  there are
over a thousand fields  including demographics about
the students at each college  the degrees and majors
offered  the cost and average loans taken out  students
test scores  admission rates  and more  matched with
statistics for rates of repayment of student loans  and
the distributions of graduates incomes over the course
of the ten years following graduation 
not all of the data was relevant to our task  we
chose the mean income    years after graduation as
our response variable  and eliminated the many other
fields pertaining to post graduation income  as well
as those describing the loan repayment patterns and
death rates of graduates  we then focused primarily
on a set of    features provided by the us department
of treasury  including gender  age  ethnic  and income
demographics of students 
to gain an understanding of the data  we ran principal components analysis  pca  on the scaled and
centered features of each college to reduce them to
a visualizable number of dimensions  pca works by
projecting the data onto a k dimensional subspace in
which the basis vectors for the subspace are the top k
eigenvectors of the original data  this serves to maximize the variance of the projections onto the subspace 
preserving as much as possible of the datas original
variance 
after performing pca  we plotted our reduced data
points in two and three dimensions  to visualize the
relationship between the reduced features  and postgraduation earnings  we scaled each colleges point by
the mean income of its graduates and colored it according to whether that mean income was above or below
the average for all colleges  looking at the resulting

figure    colleges with mean graduate income above the
average are shown in blue  those below the average are in
red  points are scaled according to the magnitude of the
mean graduate income 

along with the features from the treasury data  we
then added the admission rates of the schools  and the
midpoint sat scores of their students  this reduced
the size of the dataset even further  but running pca
on the augmented set of features  we found again that
the first principal component captured much of the
variation in future earnings 

   methods
we sought to perform a regression on students mean
income ten years after graduation  to do so  we began
with linear regression  which fits a coefficient vector 
so as to minimize the residual sum of squares
 
 

pm

i    

t

x i   y  i    

where each x i  is a training example  vector of college
features  with x      for the intercept   and y  i  is
its response  mean post graduate income   by viewing each data point as a row of a matrix x  linear
regression can also be solved using the normal equations      x t x   x t  y   which correspond to setting
the derivative of the original least squares cost function to    but from this equation  we can see that least
squares will suffer when the features are collinear  in
the case of perfect collinearity  x t x is not even invert 

fipredicting student earnings after college

ible  a nearly singular x t x will still cause increased
variance in the model 
we knew that many of our features were likely to have
collinearities  for example  pell grants are awarded
based on family income  so the percentage of students
receiving pell grants would undoubtedly be correlated
with the mean household income of students families  it is almost certain  too  that less obvious correlations exist among the different demographic statistics
of schools 
to make the model more robust to collinearity  we introduced a degree of bias to the regression  imposing
a penalty term constraining the norm of the coefficient vector  ridge regression penalizes the squared
l  norm  with the cost function
 
 

pm

i    

t

x i   y  i             

the solution to the normal equations then becomes
    x t x   i   x t  y   resolving the previous need
to invert a singular matrix 
the lasso  similarly  introduces a penalty term  but it
uses instead the l  norm
 
 

pm

i    

t

x i   y  i            

which has the advantage of performing a type of feature selection by forcing coefficients to be    giving a
sparse solution 
in general  given the multicollinearity of our data  we
found greater success with models that perform an inherent feature selection  in addition to the lasso  we
also tried random forest regression  random forests
work by building a series of decision trees on the training data 
a decision tree is formed by partitioning the data one
variable at a time  these partitions are made by choosing a region  a prediction  and splitting point in order
to produce the largest decrease in the residual sum of
squares  to make a prediction on a new datapoint 
we find the the partition that the point lands in  and
predict the mean value of training points in that space 
in a random forest  we make a series of decision trees 
to predict  we take the mean prediction from all of
them  in forming each tree  we also choose a random
subset of features to consider at each step  the result
is that we build uncorrelated trees  making the model
more robust to multicollinearity in the data 

table    an example of mse using the treasury  admission 
and sat features 

model
baseline
linear regr 
ridge regr 
lasso
random forest

training set mse

cv set mse

            
           
           
           
          

            
           
           
           
           

   experiments
     regression models
to measure accuracy of our models  we used hold out
cross validation  we set aside a random     of our
data and calculated the cross validation set error on
this data as an estimate of the generalization error  we
compared this error to the baseline of computing the
average post graduate mean income  and predicting
that for every college 
as we looked at the financial aid data from the department of treasury  combined with the sat score
and admission rate statisticsa set of    featureswe
faced a substantial problem of missing data  many
colleges were lacking a large number of fields  either
due to unavailable data or privacy concerns  to start 
we removed these data points  but in doing so  we
reduced the size of our data set immensely from     
to     colleges 
even a simple linear regression on the treasury  admission and sat data fared significantly better than
the baseline  as shown in table    for context  note
that a mean squared error  mse  of             is a
mean difference of          between the predicted and
actual mean incomes  which is about     of the average mean income  as expected from the multicollinear
features  though  the linear regression model appeared
to have very high variance  the mse of the training
set tended to be about ten million dollars lower than
that of the cross validation set  re running the model
with different choices of training and cross validation
sets also resulted in changes to the mse on the order
of ten million 
ridge regression and the lasso both improved on the
cross validation set error  to choose values for the
penalty term multipliers  we ran many trials  for the
lasso  the best multipliers seemed to be around    
for ridge regression  they were about one half  in
particular  the lassos ability to perform feature selection seemed helpful  with the best choice of hyper 

fipredicting student earnings after college
table    an example of mse using just the treasury features 

model
baseline
ridge regr 
lasso
random forest

training set mse
            
           
           
          

who sent fafsa applications five or more schools 

the random forest regressor also performed well in
predicting earnings  and it also conveniently assigns
importances to features automatically  based on which
cv set mse features were used to make splits in the decision trees 
the five highest ranked features here were percentage
             of students who received a federal loan for college  the
           
            midpoint sat scores of the college for each of reading 
            math  and writing  and the colleges admission rate 

parameter    out of the    features were eliminated  including the age of students upon college entry  percentages of their marital and veteran statuses  and some
logarithmic transformations of family income 
we still seemed to face a problem of variance  though 
even in the penalized models  except for with lasso 
there was a high discrepancy between training set and
cross validation error  and changing these sets still resulted in substantial changes to the mse  to address
the variance issue  we needed a smaller set of features 
or a larger set of training examples  given the number
of null and privacysuppressed data points that we
had removed  these two goals could actually go hand
in hand sometimes 

we then plotted each of these individual features
against mean earnings after graduation  a few of the
results were as expected  there was an obvious positive correlation between sat scores and mean earnings  students with higher sat scores are higherachieving and can attend more selective and distinguished schools  and thus earn more after graduation 
additionally  there were negative correlations between
admission rate  percentage of pell recipients  and firstgeneration students  schools with lower admission
rates can be more selective and admit high achieving
students  students who receive pell grants and firstgeneration students typically come from poorer or lesseducated family backgrounds  and thus will tend to
earn less after graduation due to the challenges of moving out of an economic class 

since the sat and admission rate data were missing
for a majority of schools  removing those features allowed us to expand the size of our dataset from     to
      colleges  on the larger training set with fewer
features  all of the models performed better  though
still with some variance  the random forest regressor stood out in paticular with the best results and the
least variance between trials 
     feature selection
one of the more interesting aspects of our project was
identifying the most important features of a college
that determine student earnings  to achieve this  we
used two feature selection methods to see which features were the most important predictors in our model 
since the lasso performed so well  we decided to use
it in conjunction with recursive feature elimination
to identify its five most important features  recursive feature elimination first trains the lasso on all
features  prunes the features with the lowest learned
weights  then recursively trains and prunes on the
smaller set of features until only a few are left  the
top five features here were percentage of students who
received a pell grant  percentage of dependent students  percentage of female students  percentage of
first generation students  and percentage of students

figure    the colleges with highest student earnings typically had a roughly even split between the two genders 

however  there were interesting results that we did not
expect  we found that the schools with higher postgraduate earnings typically had a nearly even split between male and female students  see figure     we
believe that this is the case because more prestigious
schools will have many applicants and are thus more
able to admit an evenly split class of qualified students 

fipredicting student earnings after college

in addition  schools with many students who submitted more than five fafsa applications tended to have
higher post graduate earnings  see figure     which at
first glance seems to contradict the pell grant trend we
observed  we then reasoned that students who took
the time to apply to many colleges tend to be more
ambitious and high achieving  and schools with many
of these students must be attractive enough to convince students to attend their school instead of the
other schools they applied to 

table    performance without and with imputation 

model
baseline
ridge regr 
lasso
random forest

mse w o imp 

mse with imp 

            
           
           
           

            
            
            
           

examples would both be given the same value for the
same missing feature 
performing imputation increased our sample size back
to around      schools  we still left out schools missing
values for our response variable  but reduced the performance of our regression models  as shown in table
   this is understandable given that some features 
such as sat midpoint scores  were missing values for
over      schools  for these features  the estimated imputed values overwhelmed the actual observed values 
however  the model can still make a decent prediction on a new example with missing values  whereas
without imputation this would not have been possible 

   conclusion and future work
figure    colleges with many students who sent fafsa
apps to many colleges tended to have higher earnings 

     imputation
one of the most challenging aspects of working with
the scorecard data was handling the missing  null or
privacysuppressed  values  one common and simple
strategy to handle this is to throw out examples with
missing values  but this could potentially cause models to miss out on valuable information available from
the non missing values in these examples  in our case 
many colleges were missing at least one feature  so performing this strategy reduced the number of training
examples available from around      to around      
we thus tried to use imputation to substitute missing values with estimated values  we replaced missing
values with the mean of the present values for that
particular feature  the benefit of doing this is that it
preserves the sample mean for each feature  and more
importantly  it allows the model to train and make
predictions on examples that are missing features  increasing our sample size and making our model more
robust to incomplete data  however  imputation adds
noise and makes it harder to observe correlations between variables because significantly different training

for our project  we used the college scorecard dataset
to build a model that could predict the earnings of a
colleges students after graduation  we also gained insight into what characteristics of a college are important in determining the earnings of their students 
we used a few different regression algorithms and
found that lasso and random forests yielded the lowest mean squared errors  we believe that these two
algorithms performed the best because they both perform a type of feature selection  which reduces high
variance   lasso uses regularization to force the coefficients of the least useful features to    while random forests assigns importances to features when using them to make splits in decision trees  this property allows these two in particular to perform well on
our large dataset that contains hundreds of features 
if we had more time for future work  we would like
to develop better ways of visualizing the dataset  the
sheer number of features and colleges contained in the
data makes it hard to grasp  and it would be worthwhile to create an application that can project the data
onto custom features or components to yield visible
insights on the relationship between college and earnings  we might also try more unsupervised approaches
to group similar colleges together  thereby providing
possible alternatives to attending a specific college 

fipredicting student earnings after college

references
brewer  d  j   eide e  r  ehrenberg r  g  does it pay
to attend an elite private college  cross cohort evidence on the effects of college type on earnings  the
journal of human resources                     
james  e   alsalam n  conaty j  c  to d  l  college quality and future earnings  where should you
send your child to college  the american economic
review                     
loury  l  d   garman d  college selectivity and earnings  journal of labor economics               
     
pedregosa  f   varoquaux  g   gramfort  a   michel 
v   thirion  b   grisel  o   blondel  m   prettenhofer  p   weiss  r   dubourg  v   vanderplas  j  
passos  a   cournapeau  d   brucher  m   perrot 
m   and duchesnay  e  scikit learn  machine learning in python  journal of machine learning research                    
rumberger  r  w   thomas s  l  the economic returns to college major  quality and performance  a
multilevel analysis of recent graduates  economics
of education review                  
wachtel  p  the effect on earnings of school and college
investment expenditures                     

fi