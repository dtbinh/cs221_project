sign language recognition using temporal classification
hardie cate  ccate stanford edu 
fahim dalvi  fdalvi cs stanford edu 
zeshan hussain  zeshanmh stanford edu 
december         

 

introduction

extracted included motion of the hands  location of the
sign being performed  and handshapes used  these data
are collected temporally  i e  at multiple time steps   the
authors use an spm method in order to address common
issues with other models that have been used for sign language classification and translation  including unrefined
feature selection and non discriminatory learning  in general  there has been significant work done in using sequential pattern mining methods to analyze temporal data
        other authors use hidden markov models  hmm 
on features extracted from video frames to perform the
same task  starner et  al  hypothesize that the fine
movements of a signers hand are actually not required 
and that coarse position and orientation of the hand are
discriminative enough to classify signs       they also
try two camera perspectives  one directly in front of the
signer and the other on a cap that the person is wearing 

in the us alone  there are approximately         hearing impaired people whose primary mode of conversation
is sign language  for these people  communication with
non signers is a daily struggle  and they are often disadvantaged when it comes to finding a job  accessing health
care  etc  there are a few emerging technologies aimed at
overcoming these communication barriers  but most existing solutions rely on cameras to translate sign language
into vocal language  while these solutions are promising 
they require the hearing impaired person to carry the
technology with him her or for a proper environment to
be set up for translation 
one alternative is to move the technology onto the persons body  devices like the myo armband available in the
market today enable us to collect data about the position
of the users hands and fingers over time  since each sign
is roughly a combination of gestures across time  we can
use these technologies for sign language translation  for
our project  we utilize a dataset collected by a group at
the university of south wales  which contains parameters  such as hand position  hand rotation  and finger
bend  for    unique signs  for each input stream representing a sign  we predict which sign class this stream falls
into  we begin by implementing baseline svm and logistic regression models  which perform reasonably well on
high quality data  lower quality data requires a more sophisticated approach  so we explore different methods in
temporal classification  including long short term memory architectures and sequential pattern mining methods 

 

other techniques that do not involve visual input have
also been studied  these techniques normally rely on
data from sensors that measure features like hand positions and orientations  finger bend measures  etc  for
example  kadous uses a novel technique in his paper to
improve classic machine learning algorithms by creating
meta features      these meta features are derived from
the raw features by looking at important events in the
time series data  an example kadous uses in his paper is
the vertical maxima that a persons wrist reaches while
signing  a meta feature like this gives meaning to the
y axis in the data  and helps create better features  he
also looks at other automatic techniques to generate these
meta features by looking for variations across time in a
given dataset  another study by mehdi et al  proposes
the use of neural networks to classify signs correctly     
they focus on signs that have distinct static shapes rather
than analyzing the signs over time  some studies have
also suggested using hmms to detect the gestures being
performed  liang et  al  employ this technique  along
with a sign language to english language model to predict a particular sign      finally  a study by graves et
al  uses strong classification to predict a sequence of labels given time series data  rather than a single label     
they use a recurrent neural network  rnn  integrated
with a softmax classifier to achieve this prediction 

related work

several techniques have been used to tackle the problem
of sign language to natural language translation  conventionally  sign language translation involves taking an
input of video sequences  extracting motion features that
reflect sign language linguistic terms  and then using machine learning or pattern mining techniques on the training data  for example  ong et al  propose a novel method
called sequential pattern mining  spm  that utilizes tree
structures to classify signs      their data was captured
using a mobile camera system and the motion features
 

fi    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
 

raw x

resampled x

raw y

resampled y

raw z

  

  

  

of the important information is retained  which supports our choice of normalization  this also helps
nullify the affects of the speed at which a sign is performed  as some signers sign at a higher speed than
others  temporally scaling the data would normalize
all signals to be at roughly the same speed 
   spatial scaling  each sign in our datasets  especially in the low quality dataset  was performed at
different times under different conditions  hence  the
signs were performed at varying relative positions
and orientations  for example  a sign that involves
moving the wrist in a parabolic motion may have its
vertex at varying heights between runs  to normalize these variances  we spatially scale all the signals
to be between   and   

resampled z

  

    

  

  

  

  

  

  

figure    signal resampling

 

   time series flattening  for our datasets  each
sample consists of a num featurestime steps matrix  where each row represents one of the distinct
motion parameters and each column is a particular frame  we process this data and store it in a
  d matrix  whose dimensions are num examples 
num featurestime steps  for the algorithms that
cannot take into account the temporal nature of the
data  we transform this matrix into a flattened  d matrix of size num examples   num features 
time steps  such that the first num features features in each row are the reading at time t     
the next num features features correspond to time
t      etc 

datasets

we are primarily using two datasets from the research
project by kadous      the first dataset is a high quality dataset  with data recorded at a frequency of     hz 
this dataset was recorded using two   dimensional flock
gloves on each hand  we have access to   bit precision
finger bend measures for each finger  as well as    bit precision orientation and position of each wrist in space  the
second dataset is a low quality dataset  with data available for only one hand  the data was recorded at a much
lower frequency of    hz using the nintendo powerglove 
we only have   bit precision for the finger bend measures 
and data was recorded for only   fingers  the position of
the wrist also has a lower precision of   bits  in addition 
we have access to only one degree of rotation  which was
recorded at roughly   bit precision  both datasets consist
of time series data for    signs  in the high quality data 
we have    instances of each sign  while in the low quality
data  we have    instances of each sign  these instances
were recorded across various sessions  the source of the
high quality data was a single professional signer  while
the low quality data was recorded using several signers of
varying levels of proficiencies 

 
   

technical approach
baseline

we begin with several baseline implementations using
svm and logistic regression models  since both of these
techniques require single dimensional features  we use
temporal scaling and time series flattening  we use both
linear and rbf kernels for our svm  but the additional
complexity of the rbf kernel did not improve our results
significantly  for both of these models  we use one vsrest strategy to build classifiers for each sign  we train
both of these models on     of the data  and use the
    preprocessing
remaining     as test instances for each of the    signs 
we perform some of the following preprocessing steps on we use the scikit learn library for our svm and logistic
the dataset  depending on which algorithm we are using  regression implementation     
   temporal scaling  the average number of frames
for each sign is    frames  so we normalize all signs to
this length by resampling using a fast fourier transform  in figure    the graphs on the left display
readings for a single motion parameter over the lifetime of the sign  while the corresponding graphs on
the right depict the resampled version  in general  we
notice that there are not many differences between
the original data and the resampled version  most

   

long short term memory

the first complex model that we work with is a recurrent
neural network  specifically the long short term memory
architecture  we choose this model because it takes into
account the temporal features of our data  unlike the
baseline models  we start out by trying to mimic the results of logistic regression using a simple neural network
with a hidden layer that used sigmoid activation  once we
 

fihave sufficient performance  we use the same architecture
for each time step  and then connect the hidden layers so
that we can perform backpropagation with time  other
architectures with more layers  both fully connected and
partially connected layers  were also considered  the final architecture is a three layer network  the first layer
is a time connected layer  the second layer is a fully connected dense layer for each time step  and the final layer
is a dense layer that outputs a    dimensional vector  we
utilize mean squared error as our loss function  we use
the keras library for our lstm implementation     

   

at least as often as the pattern itself  generating
patterns of length k     involves considering all patterns of length k that have the same k    states as
their prefix 
   binary vector creation  finally  having a set of
patterns that remain after our candidate generation 
we use a chi square test to rank all the patterns by
their ability to distinguish between the signs  after
ranking  we choose the top max patterns patterns 
where max patterns is a hyperparameter we tune 
once we have max patterns patterns  for each instance in our dataset  we build a binary feature vector  where a   in position i indicates that pattern i
occurs in that instance 

sequential pattern mining

the second technique that we try sequential pattern mining  batal et  al  have described an algorithm to perform
multi variate time series classification      their method
is primarily a feature engineering technique that looks at
the combination of the signals in an instance and outputs
a binary feature vector  we can then train a standard
svm over these binary feature vectors to perform our
classification  the algorithm tries to find a set of patterns in the signals that serve as a fingerprint for the
class of that signal  after preprocessing  temporal and
spatial scaling   spm primarily has three steps 

 

results and analysis

   

experiments

with each of our models  we try several experiments 
specifically  for our baseline svm model  we try various kernels  we also try regularizing both our baseline
svm and logistic regression models to prevent overfitting
on the data  for the lstm architecture  we experiment
with several activation functions for each layer and try
adding intermediate layers like dropout to prevent overfitting  finally  for our spm approach  we have several
hyperparameters to tune such as the window size  length
of patterns generated during candidate generation and
the minimum support required for each candidate pattern  we also tune parameters such as the regularization
and feature vector length on which we train the svm 
finally  since spm involves discretizing the signal  we try
two different implementations  discretizing the raw signal
itself and discretizing the rate of change in the signal 

   discretization  the first step in the algorithm is
to discretize the input signals into discrete values  in
our implementation  we try two different sets  high 
middle  low and very high  high  middle  low  very
low   because of our spatial scaling  the signal values
are between   and    we set thresholds for each of
the discrete values depending on the set we are using  thus  the discretized signal might look like the
following  hmllhhlmmmmlh  where h is high  m is
middle  and l is low  subsequently  we combine all
consecutive values that are equal  hence  our example would transform into hmlhlmlh 

   

   candidate pattern generation  the next step of
the algorithm is generating patterns  a pattern is
defined as a list of states  where each pair of consecutive states is connected by a relation  in our
case  the states are h    which indicates that signal
  was high  we also consider only two relations  before and overlap  hence  h   b l   implies that the
state with high value in signal   occurs before another state in signal   that has a low value  generating candidate patterns alternates between generating
all possible patterns of a certain length k and pruning them  we start with all patterns of length    i e 
k      then proceed to k     etc  to prune patterns 
we see if the pattern appears in any of the instances
of a given class  if it appears a minimum number
of times  denoted by support   we keep the pattern 
thus  we follow the approach of the apriori algorithm for frequent item set mining which relies on
the fact that any of a patterns subpatterns appear

results

the baseline models perform very well on the high quality
data  both the svm and the logistic regression models
give us a test error of      on the high quality dataset 
since we already have good performance on the high quality dataset  we focus on the low quality dataset for the
remainder of the project  the results on the low quality
dataset are shown in table   
table   
dataset

algorithm performance on the low quality

precision
recall
f 
training error
testing error

 

svm
     
     
     
     
     

log  reg 
     
     
     
     
     

lstm
     
     
     
     
     

spm
     
     
     
     
     

ficonfusion matrix of sign multiclassification

  

  

  

  

  

  

  

  

 

  

  
  
predicted label

  

 a  high quality

confusion matrix of sign multiclassification

 

true label

true label

 

 

  

  
  
predicted label

poorer performance  we think that using much longer patterns  window size      maximum pattern length     
would give us a better result  unfortunately  our implementation runtime increases exponentially as we increase
these parameters  and hence we were limited on the maximum length up to which we could increase our generated
patterns  another hypothesis we have for the generally
poor performance is that the spm algorithm gives us a set
of patterns it thinks are most distinguishing  after this 
we check if each of the patterns occur in the instances to
build our feature vector  but we do not take into account
where or how many times each of these patterns occur in
the instances  thus losing some more information in this
process 

  

 b  low quality

figure    confusion matrix for svm

first  we note that both our training and testing errors
using svm are very low  so our model is not suffering from
overfitting  see table     additionally  all other metrics 
including precision  recall  and f  score are very high 
suggesting a high  but also precise  level of performance 
this theory is substantiated by the confusion matrix for
the svm  see fig   a   which shows that the classifier is
not confusing a sign with some other sign  the clear blue
line down the diagonal is evidence of this claim  note
that the confusion matrix as well as the metrics on the
low quality dataset are much worse than those on the high
quality dataset  in general  our classifier confuses similar
signs more often  which is expected because there might
not have been enough features to distinguish these signs 
unlike the baseline models  the lstm results are poor
on the low quality dataset  although we initially expect
the lstm to perform better  as it takes into account
the temporal nature of the data  the performance does
not change much even after varying architectures for the
lstm  we hypothesize that this is because of some key
assumptions that the standard lstm model makes that
do not apply to our data  in the standard lstm model 
backpropagation through time is done at every time step 
this is usually acceptable for time series data since we
normally want to predict the value at the next consecutive
time step  however  in our case  backpropagating at each
step leads to a poorer model  since we are not trying
to predict the next value in the signal  eg  next hand
position  orientation etc    but rather we want to classify
the entire signal as one unit  hence  we would like to only
backpropagate at the final timestep to achieve a better
model  since building a custom lstm model would be
time consuming  we pursue spm as it is more promising
for our particular task 
the spm model performs slightly better than the
lstm model  however  with around     accuracy  the
model is not very strong  for the best result  we had a
window size of    minimum support of    and a maximum
pattern length of    we posit that because of such limiting
values  the patterns we generate are not very long  hence 
we are losing a lot of distinguishing information by not
having longer patterns  although slightly longer patterns
 window size      maximum pattern length      give us

   

analysis

 
 
 
 
 
 
 

   
   

 

 

 

 

 
 
    

 

 

 

 
  
 

figure    feature space reduced to three dimensions
to explain the variances in out results between the different models  we decide to analyze the feature space 
importance of each feature and the effect of various hyperparameters on our models 
given that the baseline models perform better than the
other models  we decide to analyze the flattened feature
space and see if it was truly separable in the high dimensional space  we hypothesize that each class has its
own cluster in the high dimensional space and is far from
other classes  one way to confirm this is to use pca
to reduce the feature space to   dimensions and plot the
data  we see in figure   that examples from the same
class  indicated by similar colors  cluster together  which
indicates the presence of the clusters in the higher dimensional space 
next  we want to see which of the features are most discriminative  so that we could restrict the features we were
using the the more complex models to save on runtime 
we run two rounds of ablation tests on the svm to determine which features were the most significant contrib 

fiutors to the overall performance  the first round removes
each feature independently and measures the results on
the data without that one feature  while the second round
removes an increasing number of features  all of these
tests are run on the low quality dataset  from the results
of these tests  we see that removing the position features
of the hand results in a significant increase in test error
 see table     additionally  the largest jump in error for
the second set of ablation tests occurs when we remove
the position and rotation features  removing the finger
features does not have a significant impact on the performance of the model  suggesting that the position and
rotation features are the most distinguishing features between the signs 

as our final analysis  we also try concatenating the feature vectors we get from the spm algorithm along with
the raw flattened features  using a small subset of these
features       we found that we get a    bump in accuracy over our baseline svm 

table    ablation test results
removed features
none
pos
rot
f 
f 
f 
f 
pos  rot
pos  rot  f 
pos  rot  f   f 
pos  rot  f   f   f 

figure    performance of spm with varying dataset sizes

test error
     
     
     
     
     
     
     
     
     
     
     

 

conclusions and future work

in this paper  we study and apply machine learning techniques for temporal classification  specifically the multivariate case  although the results obtained are not very
high  we believe that a more efficient implementation of
the algorithms can yield bigger and more complex models
that will perform well 
in the future  we plan to improve the implementation
behind spm to build better models  we may also consider implementing a custom lstm model that removes
the assumptions of the technique that do not apply to our
data  finally  we would also like to use a device available
in the market today  namely the myo armband  to record
our data  and try our models on this data  even though
the data that will be collected will not be exactly the
same as our current data  we believe that the techniques
we have tried and implemented are general enough for
them to work well on the new data 
most importantly  we have shown that at least with
high quality data  it is indeed possible for us to translate
sign language into text  we hope that some day this
will enable hearing impaired people to communicate more
effortlessly with the rest of the society 

here  pos and rot refer to the position and orientation of the
right wrist respectively  f  refers to the fingers on the hand 
the ordering of the fingers is thumb  index  middle  ring 

furthermore  we perform extensive hyperparameter
tuning for our spm model  since our hyperparameters
space is quite large  we use a procedure akin to coordinate ascent to find the optimal set of hyperparameters 
for the larger models  we also start with a small number of signs to build an intuition on the affect of varying
each hyperparameter  and then slowly expand our training test datasets to include more signs  as shown in figure    the test error increases as we include more signs in
our analysis  we started with   signs and progressively
added sets of signs  and for each set we computed the hyperparameters that gave us the best accuracy  as we can
see  the window size reduces as we increase the number
of signs  one reason for this may be as we increase the
number of signs  the probability of us seeing a pattern
again increases if the window size is held constant  since
we want to choose the patterns that are most discriminative  a lower window size leads to better performance
with a high number of examples  we also notice that the
optimal window size and maximum pattern lengths are
quite small        for all sets of signs we tried  indicating that the instantaneous motions in each sign serve as
discriminative features  rather than longer patterns 
 

fi 

references

   

iyad batal et al  multivariate time series classification with temporal abstractions  in  flairs
conference       

   

francois chollet  keras  https       github   com  
fchollet keras       

   

alex graves et al  connectionist temporal classification  labelling unsegmented sequence data with
recurrent neural networks  in  proceedings of the
  rd international conference on machine learning 
acm        pp         

   

mohammed waleed kadous  temporal classification  extending the classification paradigm to multivariate time series  phd thesis  the university
of new south wales       

   

rung huei liang and ming ouhyoung  a realtime continuous gesture recognition system for sign
language  in  automatic face and gesture recognition        proceedings  third ieee international conference on  ieee        pp         

   

syed atif mehdi and yasir niaz khan  sign language recognition using sensor gloves  in  neural information processing        iconip    proceedings of the  th international conference on 
vol     ieee        pp           

   

eng jon ong et al  sign language recognition using sequential pattern trees  in  computer vision
and pattern recognition  cvpr        ieee conference on  ieee        pp           

   

panagiotis papapetrou  constraint based mining
of frequent arrangements of temporal intervals 
phd thesis  boston university       

   

f  pedregosa et al  scikit learn  machine learning in python  in  journal of machine learning
research            pp           

    

thad starner  joshua weaver  and alex pentland 
real time american sign language recognition using desk and wearable computer based video  in 
pattern analysis and machine intelligence  ieee
transactions on               pp           

 

fi