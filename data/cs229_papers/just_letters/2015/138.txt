subspace clustering
weiwei feng
december         

abstract
data structure analysis is an important basis of machine learning and data science  which
is now widely used in computational visualization problems  e g  facial recognition  image
classification  and motion segmentation  in this project  i would like to deal with a set of
small classification problems and use methods like pca  spectral analysis  kmanifold  etc  by
exploring different methods  i would try to prove both mathematically and in application that
different problems need to be treated differently  and use their case specific methods 

keywords  independent subspace  nonlinear subspace  pca  clustering analysis  kmanifold
algorithm  knn ssc lsc

 

problem background and analysis

a successful analysis on manifold data analysis is based on appropriately dealt with its data structure  and design of specific algorithm  as one of the main topics on manifold learning  subspace
clustering has bright prospect in application  according to different assumptions  it is classified as
sparse subspace clustering ssc  and low rank subspace clustering  lsc   first of all  the two
methods both assume that the observed data satisfy
x   x    e  rdn  
there are n d dimensional data points  x  is clean  non noised  data  e is noise or extreme value
indispensable in the data collection process  the first assumption of subspace clustering is to
assume that data points can be expressed using a few vector bases  mathematically  we can write
the assumption as
x    az 
here a is a matrix  z is sparse matrix  in the following algorithm  we use a   xo   which implies
that the data is auto representative  the difference between two methods is that ssc assumes the
matrix z is sparse  i e  there are only a few effective representative vectors  while lsc assumes
that the matrix z is low rank  so for ssc  we have
minimize    kx  xzk f   kzk 
subjet to diag z      
and for lsc  we have
minimize kzk    kx  xzk   
 

fithe kmanifold algorithm  which i used a lot in this project  is based on isomap and em algorithm 
here is the realization of kmanifold algorithm 
algorithm   kmanifold
require  a set of images  x    x            xn   and a desired label set size  k
   calculate the euclidean distance between any of the two points in the data set
   create a graph g with a node for each image  and an edge between pairs of neighboring images
and set the edge weight to that distance
   compute all pairs shortest path distances on g  let d i  j  be the length of the shortest path
between node i and j 
   initialization  create a k n matrix w where wc i is the probability of xi belonging to manifold
c  w can be inititalized randomly unless domain specific priors are available 
   m step  for each class  c  supply wc and d and use node weighted mds to embed the points
in the desired dimension 
   e step  for each point  estimate the distance to the manifolds implied by the output of step
  and re weight accordingly 
   go to step   until convergence 

 

problem solving

here i first tried to dig into the data structure of each problem  and then applied methods which
are relatively robust  easy  and computationally feasible 

   

problem i

data of problem i are from two independent subspaces  from the geometric point of view  linear
subspace are points  lines  planes etc through the origin  perpendicular linear subspaces imply that
the two spaces are orthogonal  hence  from the linear point of view  independent subspace are
easy to differentiate  common method like principle component analysis  pca  are suitable for
solving this kind of problems  besides  pca requires few computation  after obtaining the principle
components  using simple clustering method  like k means method  we are capable to separate data
fro different subspaces 
i did pca on the data  and projected it onto the two dimensional space  see graph    a  
obviously  data are well classified to two classes  and using k means method  i got a result presented
in graph   b  

   

problem ii

for this problem  i used four different dataset which are all no longer simple independent linear
classification problems  hence here  pca is not suitable  we analyze respectively these problems
and then introduce proper algorithm to realize the clustering objective 
     

problem ii  a 

the difficulty of this problem is that both of the two lines do not go through the origin  therefore
its not a subspace  use subspace clustering method  will add one more affine  and reduce the
 

fifigure    problem i    original data pca analysis      k means method

algorithm efficiency  but we notice that these two lines are both one dimensional manifold  and
perpendicularly intercept  manifold clustering  kmanifold  is proper for this kind of problem  the
basic idea is to first construct the manifold measurement using isomap algorithm  then assume the
observed data are from low rank dimensioanl manifold plus noise  construct the probability model 
finally get the result using classic em algorithm  particularly  due to the perpendicularity  the
manifold separation is very clear  so the result of kmanifold is good 

figure    problem ii a     original data      kmanifold method

     

problem ii  b 

data for this problem are from manifolds in three dimensional spaces  it looks harder then the
previous problem  but all the spaces go through origin  so it perfectly satisfies the assumption of
subspace clustering  we have found that lack of independency  ssc has more robust performance
than lsc  so here i used ssc 
i took regular parameter         similarity matrix w    z       z      results are as follows 

 

fifigure    problem ii b     original data      ssc method

     

problem ii  c 

we see that the two parabolas are nonlinear  one of the two not passes the origin  and the two
do not intersect  this implies two points     this problem doesnt satisfy the subspace clustering
assumption     the two one dimensional manifold do not intersect  so kmanifold doesnt apply 
considering these two insights  we first use knn to construct an affinity matrix  then use spectrum
clustering method 
i took k      one thing to notice is that k cannot be too large  otherwise we violate the fact
that the two lines do not intersect  results are as follows 

figure    problem ii c     original data      lsc method

 

conclusion

from the previous parts  we can see that kmanifold  with the realization of ssc and lsc  can
be applied to  d and  d data separation and classification  with smaller data set and simple
structure  methods like pca and knn are both sufficient for solving our problems  however  with
 

fibigger data set and previously unknown data structure  kmanifold is an efficient way of dealing
with these data sets  taking into consideration of the curse of dimensionality  lsc is more robust
in multi dimensional data problems    this interesting observation can be compared to qda and
lda in their application with different data sets  

 

fireferences
    r  basri and d  w  jacobs  lambertian reflectance and linear subspaces  ieee transactions
on pattern analysis and machine intelligence                     
    r  vidal  subspace clustering  ieee signal processing magazine                   
    j  shi and j  malik  normalized cuts and image segmentation  ieee transactions pattern
analysis machine intelligence                     
    g  liu  z  lin  s  yan  j  sun  y  yu  and y  ma  robust recovery of subspace structures
by low rank representation  ieee transactions on pattern analysis and machine intelligence 
                   
    e  elhamifar and r  vidal  sparse subspace clustering  algorithm  theory  and applications 
ieee transactions on pattern analysis and machine intelligence                        

 

fi