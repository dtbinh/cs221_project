gaussian process regression with k means clustering for very
short term load forecasting of individual buildings at stanford

 

carol hsin

abstractthe objective of this project is to return
expected electricity load in kilowatts  kw  given a building
and a time interval  the raw data consists the mean kw
of    minute intervals for two years from     meters
corresponding to     buildings at stanford resulting in a
data set of       observations  the data was cleaned and
then split into training and test sets  k means clustering
was used to split the training set into more similar
sets  test dates and test buildings was clustered clusters
and clusters were used to make the prediction on test
dates from given clusters  the baseline model is an auto
regressive model  gpr models with   days prior and gpr
models based on clustering was also completed  error
analysis was done using root mean squared error 

i  i ntroduction
unlike other products  electricity isnt easily stored 
so producers must anticipate and meet the maximum
demand  also known as peak load  at any given time or
outages will ensue  since consuming fuel and running
electrical power generators isnt free  producers also
need to reduce operational costs by keeping the generation reserve at the minimum required to meet demand 
while it would be suboptimal to be producing at peak
if most of the electricity generated will be wasted  it
would also be problematic for outages to occur due
to inadequate supply  thus  accurate electrical forecasts are essential to energy management  production 
transmission  and distribution because these forecasts
allow dispatchers to make decisions on the optimal  realtime scheduling of electricity production between power
plants or generation units 
a  gaussian process regression  gpr 
a gaussian process is a collection of random variables of which the joint distribution of any subset
of the variables are gaussian  it can be viewed as a
multivariate gaussian in which we have infinitely many
variables  which basically corresponds to a function
because we can think of a function as being an infinitely
long vector that returns a value based on the input x 
thus  a gaussian process can be used to describe a
distribution over functions    
for clarity  let us explicitly state the differences
between a gaussian distribution and a gaussian process 
a gaussian distribution is fully specified by a mean
vector    and a covariance matrix  
f    f            f      n      for indexes i              n

in contrast  a gaussian process is fully specified by
a mean function m x  and covariance function k x  x   
with x as the indexes  we can think of the mean function
as an infinitely long vector and the covariance function
as an infinite by infinite dimensional matrix  gaussian
processes with finite index sets would just be gaussian
distributions 
f  x   gp m x   k x  x     for indexes x
gaussian distributions are nice to work with because
of their special properties  particularly the marginalization property because that allows us to get finite
dimensional answers to finite dimensional questions
from the infinite dimensional process     recall that
the marginal of a joint gaussian is gaussian and that
formulating the marginal distribution is just taking the
corresponding sub blocks from the mean and covariance
matrix of the joint distribution 
 
  

xa
a
aa ab
 
 
 n 
xb
b
ba bb
z
p xa    
p xa   xb       dx
xb rn

xa  n  a   aa  
this property means that to make a prediction with
on a test point x   so one can just take the relevant
sub block of the covariance matrix of the  n     dimensional joint distribution  n training points and  
test point  
another useful key property is that the covariance
matrix corresponding to a multivariate gaussian distribution is positive semidefinite  which is also the
necessary and sufficient condition for a matrix k to
be a mercer kernel  thus  any valid kernel function
can be used as a covariance function and we can apply
the kernel trick to reduce computation time  generally 
the kernel is the most important aspect of a gaussian
process and choosing the right kernel to capture the
correlations in the data is where most of the work will
be  specifically  if we assume zero mean  which is a
commonly used assumption  then the gaussian process
is completely defined by the covariance function 
in this project  we are only interested in the case of
predicting using noisy observations y   f  x     where
f  x  is unknown and noise is modeled as   n     n    
thus the covariance of y is cov y    k x  x    n  i

fifig     plot of a rejected building

fig     comparison to non rejected data

in the data cleaning process  meters servicing the
same buildings were merged and any meters serviced
more than one building was removed  this process
f  x  y  x  n  f   cov f   
reduced the data from     to    variables 
all    remaining were plotted and analyzed to ref   e f  x  y  x     k x   x  k   n  i   y
move anomalies  especially buildings with missing data
cov f     k x   x  k x   x  k n  i   k x  x   that was replaced with a number as a stand in  this
reduced the data to    variables for a      x   data
matrix      days year      hours day        mins   ii  data d escription
mins     years          
the raw data obtained from stanford facilities consists of the mean kw of    minute intervals for two
b  dividing the data
years from     meters corresponding to     buildings
the data was divided such that only the last half
at stanford resulting in a data set of       observations 
would be part of the test set  the trick here was that
each building may be serviced by multiple meters and
since the plan was to see how well similar buildings
each meter may be servicing multiple buildings  the
could help predict the load of one building  there needed
data ranges from october          at          am to
to be a way to group the buildings  k means would
october          at          pm 
do this  but personal computers do not have enough
memory required for k means on a      x           
          to solve this  the dates were clustered  more
a  data exploration and cleaning
in k mean section  
preliminary data exploration revealed there are  
for each experiment  new models were trained per
missing data points within the period being examined 
test
date per test building and only data from past wrt the
this was discovered while formatting the time  after
test
date time was used to obtain predictions  basically 
looking at the data file  missing points occurred on
for
one
building chosen from each of the building cluster
march         and march         between         
and one day from each of the date clusters  we got a total
pst and          pdt  so for about one hour starting
of   test buildings and   test dates  so    models per
at  am  this corresponds to daylight savings time  so
experiment  there are   experiments  see experiments
the data for that period is missing because the hours
section  
dont actually exist and this was confirmed because nov
and after defining k   k x  x  to simplify notation 
the key predictive equations for gpr is    

        and nov         has extra data points  the
missing points around march were extrapolated using
the mean of the points before and after while the extra
points were merged by averaging every consecutive two
points 

iii  k  means c lustering
k means clustering was performed to get similar
dates with which to cluster the buildings  rs kmeans
and plotting functions were used in this section 
 

fia  clustering dates
to cluster dates  the      x   matrix was reduced to
a      x  mean vector  mean across the    buildings  
then the vector was reshaped so that each row was a
date         and the columns were the time intervals
      for    hours        min    min  




 
 
   
    
         x       x              x    
   
     


the resulting   x    matrix was clustered by rows
and the following cluster dendrogram was used to
determine the appropriate k value  see figure on page  
for building cluster dendrogram  a k value of   was
chosen by the usual distance metric because it was the
highest number of clusters that was reasonable given
the data 

iv 

e xperiments and m odels

a  baseline model  autoregressive linear model
for the    baseline models  the data point is predicted
based only on data from the past  an ar model was
completed for each building and for each test date based
on    days prior to the test date  rs auto arima was
used to process each data vector  an input vector of
           points prior to a test date was used to
predict the output vector of    for the next    time
points in the test date  this was done using a for loop 
so only one time point  of the     was predicted per
run and the prediction saved into a vector  then the next
test point would be predicted with the same    days but
also with the previous time point from the data 

the resulting matrix was fed into rs kmeans function
to cluster the rows using the euclidean distance  each
point seen as a    dimensional vector   the resulting
hierarchical cluster dendrogram revealed   possible kvalues based on euclidean distances calculated  while
each point is in    dimensional space  a visualization
using usual discriminant coordinates was created for
better analysis  for some k values  it was clear which
days were being clustered  e g  winter vacation in k  
was cluster    a k value of   was picked because the
distances were reasonable and the goal was to get as
many clusters as the data would allow 

b  clustering buildings
the dates clusters were used to cluster the buildings 
the clusters were used to turn the      x   data matrix
into a   x    matrix by splitting dates of the building
vector       x   into the cluster groups and then taking
the mean of each group and then reshapping that into
a vector to be inserted into the k means clustering
building matrix 
 

date
   
   
   
   
   
   
   
   

ar building   
trainrmse
testrmse
                                 
                                 
                                 
                                 
                                 
                                 
                                 
                                 

date
   
   
   
   
   
   
   
   

ar building   
trainrmse
testrmse
                                 
                                 
                                 
                                 
                                 
                                 
                                 
                                 

fidate
   
   
   
   
   
   
   
   

ar building   
trainrmse
testrmse
                                 
                                 
                                 
                                 
                                 
                                 
                                 
                                 

date
   
   
   
   
   
   
   
   

ar building   
trainrmse
testrmse
                                 
                                 
                                 
                                 
                                 
                                 
                                 
                                 

b  gpr with   days prior
the    models used an input vector of size       
    to predict the output vector of size     like in ar  a
for loop was used and a vector was used to keep track
of each time point predicted  for this and all models
using gpr  the covariance matrix used was squared
error and the hyperparameters were tuned in part by
using the gpml matlab toolkit     
 

date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
       
       
       
       
       
       
       
       
      
       
      
       

date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

fidate
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
       
       
      
       
      
       
       
       
       
       
      
       
      
       
      
       

date
   
   
   
   
   
   
   
   

v  a nalysis
the models using the clustering did not perform as
well as expected compared to the models using just the
historical data of that particular building being analyzed 
this may be that a buildings profile is quite unique
and surround buildings may not be good features for
predictions  it might also be the case that the clustering
could be better 
further plots and analysis of the clustered dates were
completed  the data to suggests that the days being
clustered do follow a logic in that each has a pattern 
the plots of cluster k   with cluster   is displayed 
the mean kw corresponds to the mean vector of the
matrix from the first half of the data  as described in
the k means section 

c  gpr with similar building
the    models used an input matrix of size      
      x number of buildings in cluster  to predict the
output vector of size     each of the test buildings
were from a building cluster  so their respective building
clusters were used as the input  thus  a vector of
the time period       from each building in the test
buildings cluster that is not the test building was used
and the input matrices sizes differed depending on the
number of buildings in the cluster 
date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
      
      
      
       
      
      
      
      
      
      
      
      

date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
      
       
      
       
      
       
      
       
      
       
      
       

date
   
   
   
   
   
   
   
   

building  errs csv
trainrmse testrmse
      
      
      
      
      
      
      
       
      
      
      
      
      
      
      
       

building  errs csv
trainrmse testrmse
       
       
      
       
      
       
       
       
      
       
      
      
      
       
      
       

errors might also be due to having only   days  the
author wanted to use more than   days  but gpr is a
non parameteric model  which means it is very memory
intensive and matlab gave errors that the array exceeds
maximum array size preference  more data points also
slows the for loop from      that retrains a model per
run to be slow and given    models to per experiment 
vi  c onclusion
while the results from the experiments seem to
suggest that similar days are not quite that important 
it might be due to the clustering of the buildings  so
more work needs to be done to understand why similar
buildings do not have a high predictive value 

   
   

 

r eferences
carl edward rasmussen and chris williams 
gaussian processes for machine learning  cambridge  massachusetts  the mit press       
carl edward rasmussen and chris williams  the
gpml toolbox version      manual for gpml
matlab code version      july      

fi