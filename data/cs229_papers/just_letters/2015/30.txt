plead or pitch  predicting the performance of kickstarter projects
nihit desai and raghav gupta and karen truong
department of computer science  stanford university
 nihit rgupta   truongk  stanford edu

abstract
in this cs    project  using   k project proposals
from crowdfunding platform kickstarter  we evaluate
the performance of different models  logistic regression  svms among others  on predicting whether a
project will meet its funding goal or not  with a particular focus on features derived from the language used
in project pitches  we then contrast the performance
of our best model when using data from the projects
launch day against the last day of the funding period 

 

introduction

the recent advent of web based crowdfunding has connected
project creators to backers around the world  while recent
work has discussed patterns in factors correlating well with
the success failure of crowdfunding projects  not much has
been made of formulating what constitutes a successful project
pitch  while project metadata is important in determining
project success  the language used in the pitch is also a major
determinant of success as one of the few variables creators
can tinker with pre launch to maximize their chances of success 
briefly  in this work  we explore the problem of forming
attractive project pitches  focusing on the language used to
describe the project and associated risks   the precise question
we try to answer is  given a snapshot of a kickstarter project
 note that projects span multiple days  as our input  consisting
of text such as project description  risks and rewards as well as
non text metadata such as project category  creator bio etc  how
accurately can we predict the final success of campaigns using
different learning models  svm  logistic regression  decision
trees etc   in addition  we also perform a qualitative analysis of
the characteristics of language used in successful projects 
all three authors are crediting part of this project for
cs   n  natural language processing  in particular  some
parts of sections      and    and very little from sections   
  was included in our cs   n submission  our central focus
as regards the cs   n project was a rigorous explanation of
linguistic techniques used in feature extraction  and qualitative
analysis of the linguistic features performance  which we
touch upon very briefly in this submission   section      which
provides the mathematical explanation of our model choices 
and our discussion of model performance  contained in sections
         and     are not part of our cs   n submission 

 

project  and present results on prediction based on the time
series of early funding obtained  focusing on kickstarter dynamics   mollick        finds higher funding goals and longer
project duration lead to lower chances of success  while including a video and frequent campaign updates increase the chances 
while the above work presents statistics about the determinant features for success  there has also been work on
analyzing the language in project descriptions  mitra and
gilbert         however  this only looks at projects when the
campaign is no longer live  here  we instead examine the first
day of a campaign when the project creator still has the ability
to make changes that increase their likelihood of success 
   

text classification and analysis for social behavior

text classification as a general problem has existed in various
domains  and a variety of machine learning algorithms  including the ones we tried  have performed relatively well  aggarwal and zhai         additionally  in recent years  scientists
have been investigating the relationship between social behavior and language in online settings  for instance  studying linguistic aspects of politeness   danescu niculescu mizil et al  
      show that polite wikipedia editors are more likely to be
promoted  in another work   althoff et al         analyze social
features in online communications to determine relations predicting whether an altruistic request will be accepted 

 

domain and dataset

we describe here the kickstarter portal and funding model  followed by a description of our dataset 
   

kickstarter

kickstarter is an online crowdsourcing portal    with projects in
diverse domains like technology  film and fashion  kickstarters
all or nothing funding model makes it suitable for a prediction
task  a typical project page has many parts relevant for our
 

http   www forbes com sites chancebarnett            top   crowdfunding sites for fundraising 

related work

in this section  we give an overview of related work on crowdfunding and social analysis of text 
   

crowdfunding dynamics

 etter et al         analyze kickstarter projects by constructing
a projects backers graph and extracting tweets mentioning the

figure    data processing workflow

fimetric
  campaigns
goal  avg 
  sentences  avg 
  reward types  avg 

successful
    
     
    
    

failed
     
      
    
    

table    kickstarter corpus statistics
      psycholinguistic features
we use liwc  linguistic inquiry and word count  pennebaker
et al         to extract psycholinguistic features  liwc is a lexical database of words relating to different psycholinguistic categories  examples being   s denote stems figure    a sample kickstarter project page showing metadata
task  project description  can contain text  images and video  
potential risks  goal amount  pledge levels and user comments 
   

data

we use a dataset of        kickstarter campaigns whose final
outcome  success or failure  is known  we obtained this data
from rob voigt  linguistics dept  stanford   for each project 
the dataset comprises one snapshot per day of the projects kickstarter webpage  html   while its live  the crawl  which was
run daily for a period of about a year starting june       contains a daily snapshot of all projects active on that day  note
that for our precise problem definition  we are interested in the
project pages snapshots on the first and last days of the projects
funding period  after cleaning  we parse the html  amounting to   gb of raw html  using beautifulsoup    to extract
four types of information  textual description  metadata  including but not limited to the title  start and end date  funding goal 
image   video urls   pledge information and the class label 
the overall processing flow is outlined in figure    and some
important statistics about the data are presented in table   

 

experimental setup

we now detail our experimental setup   problem definition  features and models  we model our task as a binary classification
task  treating each project as a training  or testing  example  and
with labels being success and fail  depending on whether
the project met its funding goal or fell short 
   

features

in this section  we describe our set of features  results from
feature selection experiments and a brief analysis of usefulness
of various features is given in later sections 
      metadata
this feature set includes includes  project category  no  of
videos  no  of images  no  of comments  no  of projects previously created backed by creator and the no  of pledge levels
and goal amount  we illustrate some of these in figure   
      n grams
these are tf idf matrices of n grams  n        from the
project description and risks  we filter out phrases very specific to particular categories  e g  arduino for technology  to
avoid feature correlation with project category feature  we tried
using the plain counts matrix of the n grams  but that led to a
drop in performance 
 

http   www crummy com software beautifulsoup 

 certainty  invariab   factual   absolutely
 leisure  horseback  dvd   celebrat 
 achievement  effect   persever   founded
our features are word counts in each category from description 
      sentiment from user comments
we use stanford corenlps sentiment analyzer  manning et al  
      to annotate comments on the project webpage  the number of sentences with different sentiment scores                
most positive    most negative  form our feature values 
   

feature transformation

many elements used in the objective function of our estimators
 such as the rbf kernel of svms or the l  regularizers  assume
that all features are centered around zero and have variance in
the same order  before training these models  we transform our
data so that features have zero mean  unit variance 
   

models

      k nearest neighbors  knn 
these methods find a predefined number of training samples
closest in distance to the new point  predicting the label from
these  in k nearest neighbors  a query point is assigned the class
which has most representatives within the nearest k neighbors
of the point 
      multinomial naive bayes  nb 
naive bayes is a popular supervised learning algorithm in text
classification  bayesian classifiers use the maximum a priori
 map  decision rule to assign a label y as follows 
y   argmaxy p  y   ni   p  xi  y 
   
with multinomial naive bayes  the data distribution is
parametrized by vectors y for each class y  where each yi  
p xi  y  has a multinomial distribution  the maximum likelihood estimate for y is basically relative frequency counting 
p
xi
yi   pt xt
   
p
i  
xt xi
where the numerator is the number of times feature i appears
in a sample of class y in the training set t  and the denominator
is the total count of all features for class y 
      decision trees  dt 
a decision tree classifier is non parametric supervised learning
method that poses a series of learned questions about the features of the training examples  each time time it receives an
answer  a follow up question is asked until a conclusion about
the class label of the record is reached  equivalent to the rootto leaf traversal of a tree with the next child decided by certain
feature values  and the leaves indicating the class labels 

fi     

stochastic gradient descent  sgd 

stochastic gradient descent is an optimization algorithm for
minimizing an objective function  the sgd classifier is a linear
model that updates and computes the gradient of the parameters
using only a single training example  xi   yi   at time 
     j   xi   yi  

   

we experiment with a hinge loss cost function
     

support vector machines  svm 

svms are a popular discriminative algorithm  which has previously been used for text classification problems  the svm attempts to find the max margin hyperplane  defined by w and b 
inference on sample x is y   sgn k w  x    b   k defined below  separating the dataset based on class  in a high dimension
feature space  finding this optimal margin reduces to solving this convex optimization problem  reducable to quadratic
program  
   
argminw b   w      cm
i   i
 
subject to constraints
y  i   k w  x i      b      i   i    i           m 

   

where i s are slack variables to account for non separability of
the data  and kernel k x  y  is the inner product  in the highdimension feature space  of vectors x and y  we try out two
kernels   gaussian rbf  and polynomial with degree   
k x  y rbf   exp 

  x  y   
 
   

k x  y poly    xt y   c d
     

   
   

logistic regression  lr 

this model has the hypothesis parametrized by  
h  x    sigmoid t x   

 
    exp t x 

   

value of h  x  is interpreted as probability of class   for example x  inference is by y     h  x        for training  we
try logistic regression with l  penalty  c     being the l 
regularization hyperparameter  
 
   argmin m
 h  x i     y  i       c      
  i  

 
   

   

experiments
procedure outline and evaluation

in experiments  we divide our data into training and test sets
     and     respectively   we then use the training set
to perform feature selection  model selection and parameter
tuning  all experiments we performed using   fold cross validation on the training set  finally  we report the performance of
our best model on the test set  our implementation for feature
extraction  training and testing primarily uses scikit learn
 pedregosa et al          a popular ml library in python 
the metrics we focused on in our results was f   score 
with added attention to the successful class precision  for
notational convenience  we use p for precision  r for recall  f 
for f   score  tp for true positive  tn for true negative  fp
for false positive  fn for false negative and the subscripts s for
label successful  f for label failed and o for overall 
p  

tp
t p  f p

r 

tp
t p  f n

r
f       pp r

figure    performance of different feature sets with l   logistic
regression with balanced class weights 

   

feature selection

we do a forward search for feature selection  with the
small change that we iterate over classes of features and not
individual features   starting with a simple model based only
on the project category feature  we progressively add more
features types and evaluate the performance of these features
sets using the average f  score of   fold cross validation  we
use l   regularized logistic regression models with balanced
class weights  for details see next section  for this exercise  our
choice of model for this experiment is driven by the popularity
of logistic regression for text classification in general  plus its
small running time 
figure   shows the f  increase as features are added  we
perform feature selection experiments on project snapshots on
the first and last day  owing to the fact that most predictive
features at the campaigns start might be different from those
more predictive as the campaign progresses  we observe that
metadata is more important for predicting success at the end of
a campaign compared to the start  additionally  we find that
user comment sentiment is only helpful for predicting success
on the last day  which is expected since there are hardly any
comments on the first day 
   

model selection

knn
nb
sgd
dt
lr
svmpoly
svmrbf

ps
   
   
   
   
   
   
   

rs
   
   
   
   
   
   
   

pf
   
   
   
   
   
   
   

rf
   
   
   
   
   
   
   

po
   
   
   
   
   
   
   

ro
   
   
   
   
   
   
   

f o
   
   
   
   
   
   
   

table    performance of various models with the full feature set
next  we train various models using the entire feature set
and evaluate the performance of these models on snapshots on
the first day of the campaign  using   fold cross validation 
something to note is that we balance class weights in our
estimators since our dataset is skewed towards unsuccessful
campaigns  after balancing  a most common label baseline
would have an accuracy of       
results  avgd over   fold cross validation  comparing the
performance of various models are shown in table   to denote
different models we use knn for k nearest neighbor  nb for
multinomial naive bayes  dt for decision tree  sgd for

filinear classifier trained using sgd and hinge loss function 
lr for l  regularized logistic regression  svmpoly for svm
with polynomial kernel and svmrbf for svm with rbf
 gaussian  kernel  in addition  to the overall f  score  we also
consider the successful class precision  ps   of the models in
our evaluation  this is because if the end goal is to help project
creators improve their project pitches  then we would prefer
our model to have a reasonable ps   ensuring that our model
recommends useful changes to project pitches  we find that
logistic regression  with l  regularization  and svm  with
gaussian kernels  give the best results during cross validation 
   

parable on the training and test set  for lr  however  for
sv mrbf   the training error is much less than the test error
for both the first day and the last day  this leads us to conclude
that sv mrbf is overfitting the data while lr isnt  this is
despite our efforts to prevent it  we tuned parameter c  and also
tried setting a bound on number of support vectors  

testf irst
testlast
trainf irst
trainlast

parameter tuning

c    
c    
c  
c  
c   

ps
   
   
   
   
   

rs
   
   
   
   
   

pf
   
   
   
   
   

rf
   
   
   
   
   

po
   
   
   
   
   

ro
   
   
   
   
   

f o
   
   
   
   
   

testf irst
testlast
trainf irst
trainlast

 

po
   
   
   
   

ro
   
   
   
   

f o
   
   
   
   

ps
   
   
   
   

rs
   
   
   
   

pf
   
   
   
   

rf
   
   
   
   

po
   
   
   
   

ro
   
   
   
   

f o
   
   
   
   

relying only on linguistic features for classification is tricky
since all projects aspire to succeed  making the task hard even
for humans  unlike a problem like spam classification  where
the spam language is distinguishable from normal language 
for some campaigns  description is entirely through images  since open source frameworks for extracting text from
images  we tried tesseract    are not robust enough for our
needs  projects falling in this category are often misclassified as
unsuccessful  owing to the datas inherent skew  
during parsing we observed that some html pages did
not confirm to the normal structure  as a result  some attributes
from these pages werent parsed correctly  leading to missing
feature values 

   

   

      overfitting
to see if sv mrbf and lr are overfitting  we evaluated their
error on the training set  comparing with the test set error as
shown in tables   and    we observe that the model performance for either the first day or the last day data is very com 

rf
   
   
   
   

challenges

 

      first day and last day
having experimented with feature sets and different models 
we use these results to see how well can we predict success
of a campaign as it progresses  using our two best models 
sv mrbf and logistic regression  lr   in tables   and    we
report the performance of our system on campaign snapshots on
the first day  as a lower bound  and on campaign snapshots on
the last day  as an upper bound   this is our final experiment for
the prediction task and we use the test set which has thus far not
been touched  for notational convenience  we use abbreviations
as explained in section      we observe that logistic regression
performs better on the test set compared to sv mrbf likely as
a result of sv mrbf overfitting  see next section  

pf
   
   
   
   

table    logistic regression performance on test set

table    predictive performance of svm  with rbf kernel  for
varying c  regularization  parameter
results on test set

rs
   
   
   
   

table    sv mrbf performance on test set

we come to the problem of selecting parameters for our best
performing class of models 
 we performed   fold cross validation on the training set for
selecting regularization parameter c for svm  because we
observed disparity between the training and validation error   results are shown in table    we observe that very
small values of c seek only to maximize the margin  allowing for more examples to be misclassified  for c    
and larger  overall f   score remains fairly constant  however for large values of c  precision of successful label decreases and that of failed label increases  model is forced to
make less training errors  overfitting   since we care about
successful class precision  ps   of the models  which is why
we report our final numbers on the test set for c     
 for logistic regression  we observed that training and validation errors were very close  so we just used the default
parameter value c      for l  regularization  
 for n gram features  we selected the top     ngrams in an
unsupervised fashion  using svd on the tf idf matrix 
to help speed up training of our models  especially svm  

ps
   
   
   
   

discussion
linguistic features

interestingly  we find that most of our highest positive and negative weighted features are in fact n grams  in this section  we analyze linguistic features highly predictive of success of a kickstarter campaign  refer to table   for a more extensive list  and
try to offer some insights in this regard 
 reciprocity  reciprocity is the tendency to return a favor
in exchange for receiving one  we find many predictive
phrases that are often used to offer a reward or a gift in
return for donation funds 
 social relationship  we find that phrases indicative of success employ social dynamics and provide context in which
their projects  if successful  will have a positive impact 
 emotional appeal  successful campaigns are seemingly
emotionally appealing to the readers  and both negative and
positive emotion are well represented 
 gratitude  successful campaign text often conveys gratitude towards the backers 
 

https   code google com p tesseract ocr 

figroup
reciprocity
social
emotion
thankful
pitch
collective

phrase list
free shipping  you receive  early bird  be the first  your reward
friends  friendship  community  and family  his family  people
passion  dream  inspired to start  believe that  impact  volunteer
thank you  so thankful  thanks  thanks so much  grateful  grateful for your
why support  funds will cover  will be used  aiming to  aim to raise
help us  we can  we raise  we plan to  we need  we found  we created

table    phrases in project descriptions most predictive of successful kickstart projects  grouped
 collective phrasing  we find that project descriptions
making use of the singular first person pronoun i tend
to belong to unsuccessful projects  while those using the
plural pronoun we are generally successful 
feature weights for liwc categories corroborate these observations  categories representing emotion  positive emotion 
negative emotion   social processes  categories friend  family 
are strong positive predictors  at the same time  membership in
liwc categories like death  sad  anger is indicative of failure 
   

sgd classifier

the sgd classifier presented interesting results  during   fold
cross validation  the f  score for the successful class fluctuates
wildly  from     to     and the overall f  score from      to     
without converging within    epochs  default value for scikitlearn is     we hypothesize this is because of sgd being unstable with non separable data  a largely continuous presence
of outliers during training would guide sgd towards fluctuating directions  this is supported by the linear kernel svm not
converging within   k iterations  discussed in section        

metadata features

when training and testing on the first day project descriptions 
only the feature on goal amount  with a high  ve weight  features among the top    features  surprisingly  the features on the
number of images and videos  while having a moderate positive
weight  did not affect classification performance drastically  but
when training and testing on the last day data  our features on
the number of backers and the number of comments gain prominence  and feature among the top    most predictive features 
   

     

models

we discuss possible interpretations of the performance of various models on our dataset  here  wed like to point out that
our prediction task is more naturally modeled using a discriminative approach rather than a generative one  this is because
all campaigns aspire to succeed  therefore  it is better to model
conditional probability of the label given the data as opposed to
their joint probability 
      k nearest neighbors
given the dataset size  we expected knn to provide a decent
baseline for our task  however  with ps       and f    
      it does not perform well  we reason this is because campaigns fairly similar in terms of the language may have different
outcomes  only a handful succeed  biasing knn towards predicting failure most of the times 
      multinomial naive bayes
notwithstanding being a generative model where our problem
is better modeled discriminatively  multinomial naive bayes
performs commendably on this task  however  it is a simple
model with strong assumptions and often a high bias  as discussed in the lectures   it performs very consistently during
cross validation  ps        f  o        helped by the size
of the data  and provides a competitive baseline for the task 
      decision trees
decision trees are not too useful in problems with highdimensional feature spaces  e g  text classification  where decisive features may not exist or be hard to find  due to the sparsity
of the feature set  particularly the n gram features in our case  
in addition  two projects with fairly similar feature values may
still have different labels outcomes  which is a disadvantage for
decision trees  hereby unable to distinguish such examples 

     

svms

svms provide good results here on our test set  only slightly
inferior to lr on ps   the radial basis function kernel outperforms the polynomial kernel significantly  while the linear kernel fails to converge in reasonable time  see section         we
performed   fold cross validation for selecting the parameter c
 details in section      and our reported results are with c     
which offers the best tradeoff  one limitation we observed in
case of svms was the overfitting on training set 
     

logistic regression

one of the two most popular linear models  the other being
svms  in the text classification domain  our logistic regression
models provides good performance  with precision for the successful class  ps       and f  o       for snapshots on
first day of the campaign  our l   regularized logistic regression
model comes up with feature weights that are explainable  it
forms the basis for our feature search  another advantage with
logistic regression is that its output can be interpreted as probability of a campaign being successful 

 

conclusion and future work

we present results on kickstarter project success prediction
using mainly the initial description  indicating a departure from
prior work relying on post launch data  we identify logistic
regression and sv mrbf as our top models  the former
avoiding overfitting to boot  and also illustrate the efficacy of
metadata and linguistic features for the task  obtaining good
results on the initial description and identifying stylistic patterns
in the language used in successful campaigns 
apart from the feature set extensions discussed in section
   we discuss directions for future research in this domain  one
interesting extension to this work would be to use the obtained
feature weights themselves to suggest to project creators which
portions of a project description are lacking  a thorough
analysis of backers preferences for different kinds of rewards 
and its impact on project success  would be interesting to
explore  finally  we would like to get to the root of the svms
overfitting problem  given more time 

fireferences
charu c  aggarwal and chengxiang zhai       a survey of
text classification algorithms  mining text data  springer us 
       
christopher manning  surdeanu mihai  john bauer  jenny
finkel  steven bethard and david mcclosky        the
stanford corenlp natural language processing toolkit 
proceedings of   nd annual meeting of the association for
computational linguistics  system demonstrations 
cristian danescu niculescu mizil  moritz sudhof  dan jurafsky  jure leskovec and christopher potts        a computational approach to politeness with application to social factors  proceedings of the conference of the association for
computational linguistics       
ethan mollick        the dynamics of crowdfunding  an exploratory study  journal of business venturing           
james w  pennebaker  martha e  francis  and roger j  booth  
      linguistic inquiry and word count  liwc       mahway  lawrence erlbaum associates    
joyce berg  john dickhaut and kevin mccabe        trust 
reciprocity  and social history  games and economic behavior               
tanushree mitra and eric gilbert        the language that
gets people to give  phrases that predict success on kickstarter  proceedings of acm conference on computersupported cooperative work and social computing 
tim althoff  cristian danescu niculescu mizil  dan jurafsky 
      how to ask for a favor  a case study on the success of altruistic requests  proceedings of the international
conference on weblogs and social media 
vincent etter  matthias grossglauser  and patrick thiran      
launch hard or go home  predicting the success of kickstarter campaigns  proceedings of the first acm conference
on online social networks 
fabian pedregosa et al        scikit learn  machine learning in
python  the journal of machine learning research  jlmr 

fi