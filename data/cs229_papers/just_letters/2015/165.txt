contextual code completion using machine learning
subhasis das  chinmayee shah
 subhasis  chshah  stanford edu
mentor  junjie qin

abstract
large projects such as kernels  drivers and libraries follow a code
style  and have recurring patterns  in this project  we explore
learning based code recommendation  to use the project context
and give meaningful suggestions  using word vectors to model
code tokens  and neural network based learning techniques  we
are able to capture interesting patterns  and predict code that that
cannot be predicted by a simple grammar and syntax based approach as in conventional ides  we achieve a total prediction
accuracy of       on linux kernel  a c project  and       on
twisted  a python networking library 

 

introduction

code ides make the task of writing code in new languages easier
by auto completing key words and phrases  for example  when
writing code in c    an ide may automatically close an opening
parenthesis  and suggest else immediately following an if block 
such code completion has several problems 
   grammar based code completion requires writing down an
exhaustive set of rules  this is tedious and repetitive when
done for every new language 
   predictions do not consider the category of code  for instance  driver codes and math libraries may exhibit different
patterns 
   predictions do not consider context such as class definition 
function definition  and tabs and spaces 
   recommendations are often ordered lexicographically 
which may not be very useful 
this report explores a learning based approach to code completion  instead of writing grammar based rules  we use machine
learning to learn structure and patterns in code  we can then not
only automate code predictors for different languages  but also
specialize them for different kinds of projects  we can also consider the enclosing context and rank predictions 

 

related work

we use natural language techniques for predicting code  in this
section  we review some literature on language models and neural
network based language models 

   

traditional language models

statistical models of languages have been used extensively in various natural language processing tasks  one of the simplest such
models proposed is the n gram model  where the frequencies of
consecutive n tokens are used to predict the probability of occurence of a sequence of tokens  the frequencies of such n grams
are obtained from a corpus  and are smoothed by various algorithms such as kneser ney smoothing      to account for sparsity

of n grams  such models can be easily extended to code completion  by modeling each token as a word and doing n gram analysis
over a codebase  however  such models do not capture the high
fraction of unknown tokens from variable and function names very
well 

   

word vector embeddings and neural network language models  nnlm 

in their seminal work  bengio et al      first propose a vector embedding space for words for constructing a language model  that
significantly outperforms traditional n gram based approaches 
their proposal maps each word to a vector and expresses the probability of a token as the result of a multi layer neural network on
the vectors of a limited window of neighboring words  t  mikolov
et al       show that such models can capture word pair relationships  such as king is to queen as man is to woman  in the
form of vector operations  i e   vking  vqueen   vman  vwoman   
to distinguish such models from recurrent models  we call them
feed forward neural network language models  we use the same
concept in our token embedding  we found singificant clustering
of similar tokens  such as uint  t and uint   t  but were
unable to fund vector relationships 
later  such feed forward nnlms have been extended by
mikolov et al       where they propose the continuous bag ofwords  cbow   and the skip gram  in cbow  the probability of
a token is obtained from the average of vectors of the neighboring tokens  whereas in skip gram model the probabilities of the
neighboring tokens is obtained from the vector of a single token 
the main advantage of such models is their simplicity  which enables them to train faster on billion word corpuses  however  in
our case  the size of data was not so huge  and we could train even
a   layer model in   days 

   

rnn language models

a significant drawback of feed forward neural network based language models is their inability to consider dependencies longer
than the window size  to fix this shortcoming  mikolov et al 
proposed a recurrent neural network based language model      
which associates a cell with a hidden state at each position  and
updates the state with each token  subsequently  more advanced
memory cells such as lstm     and gru     have been used for
language modeling tasks  more sophsticated models such as treebased lstms      have also been proposed  we experimented
with using grus in our setup  but surprisingly did not find them
to be competitive with feed forward networks 

   

attention based models

recently  neural network models with attention  i e   models that
weigh different parts of the input differently have received a lot
of attention  pun intended  in areas such as image captioning     
and language translation          in such models  the actual task

fialgorithm to autocomplete new variable names  once they occur in
a file  for example  given many examples of the form for  int
token      token   n  token     all with different
values of token   our algorithm should be able to autocomplete
for  int myitername      with myitername  this
capability can not be achieved by merely trying to autocomplete
between a set of pre defined key tokens  hence  we also define
positional tokens as follows 
given a sequence of tokens  an incomplete piece of code  we
replace each token which is not a key token by a string of the form
pos tok ii  where ii is the position of that token within
that sequence  a non keyword token which is repeated multiple
times in a sequence is assigned the index corresponding to its first
appearance  for example  given the sequence of tokens  for 
int  myvarname           myvarname     n 
   myvarname       where myvarname is not a key token 
we construct the new sequence  for  int  pos tok   
         pos tok       n     pos tok        
in case the token to be predicted is not a key token but has
appeared in the window  it is also replaced by the corresponding
positional token string  if the target has not appeared before in the
window  it is assumed to be a special token unknown  however 
as we describe in section    we ignore such windows since in
many such cases the token is actually a hereto unseen token 
such an encoding is advantageous since now the set of prediction targets to choose from is simply the union of the key tokens
and the positional token strings  of the form pos token ii  
in case of a fixed window size of w and a fixed number of key
tokens k  it can be seen that the total number of prediction targets
is k  w     k key tokens  w positional tokens  and   unknown
token   i e   a constant  this immediately opens up the possibility
of applying simple models such as logistic neural network based
classifiers 

of prediction is separated into two parts  the attention mechanism which selects the part of input that is important  and the
actual prediction mechanism which predicts the output given the
weighted input  we found an attention based feed forward model
to be the best performing model among the ones we considered 

   

code completion

frequency  association and matching neighbor based approaches     are used to improve predictions of ides such as
eclipse  our learning approach  on the other hand  attempts to
automatically learn such rules and patterns 

 

dataset and features

we use two different projects  linux source      a c project  and
twisted      a python networking library  to train and test our
methods  in each case  we use half of all files for training  and
the remaining half for testing  given a sequence of tokens in an
incomplete piece of code  we predict the next token  we pick
these incomplete code pieces randomly from the complete codes
in the training set  tokens in these incomplete codes constitute the
features  and the next token occuring in the actual complete code
is the  truth   the next section describes how we model these
tokens 

 

modeling tokens

one of our objectives of learning based code prediction is to do
away with the tedious process of building grammar based rules
for different languages  we treat codes in the training set in a
language agnostic way  the first step is to build a dictionary of
tokens or words that can occur in the code  that we can take as
input to predict the next token  we build this dictionary by reading all code  and treating each consecutive set of alphanumeric
characters and      or each consecutive set of special characters
other than alphanumeric       space and newline as one token 
thus  for example  the code for  my var      my var
  foo  my var      will be tokenized into eleven tokens 
for  my var           my var     foo     my var        
given a sequence of tokens  we then want to predict the next token  note that these tokens do not correspond exactly to language
level tokens  e g         is a single token despite containing three
different language level tokens        and   
the dictionary of tokens constructed as above is not complete 
since new code may contain new tokens that are not present in
the training data  moreover  many of the tokens in the training
data may be specific to a few files and may never occur again  to
address these issues  we divide the tokens into two categories 
a  key tokens  a subset of k most frequent tokens are categorized as key tokens  not surprisingly  many of these frequent
tokens are keywords for the language the project is written in  or
words that tend to occur often in that particular project  for example  some of the frequent tokens in linux kernel are  struct 
   and dev  out of which the first and second are keywords in c 
and the third is a token frequently used to denote device objects
in linux  note that while several language keywords do end up
being part of the set of key tokens  we do not manually curate
the list of key tokens to ensure that they contain only language
specific keywords 
b  positional tokens  while key tokens occurrences constitute
a major fraction of all token occurrences       with      key
tokens   the remainder of tokens are rarely seen  such as names
of variables  macros etc    however  we would like our learning

 

methods

in this model  we simply take the set of k   w positional and
key tokens  and the set of k   w     output tokens  and fit a
model similar to word vectors       the details of the model are
described below 
we first assign a d dimensional vector to each one of k  
w different key tokens and positional tokens  we denote such
token vectors by vi   where    i  k   w   next  given a fixed
size window of tokens  t    t            tw    we compute a score for
each possible output j  sj as a function of the token vectors of the
tokens in the window  i e  
sj   fj  vt    vt          vtw  
the final loss function for this particular example is given by
 
esto
l   log p sj
je
where to is the output token  this is the cross entropy loss between softmax based probabilities for each output and the actual
observed output  according to the actual form of the function fj  
we get different models  a few of these models are described below 
for each model  we use a dag rad optimizer to minimize the
total loss function with respect to the parameters of that model 
a dag rad was chosen because it gave the best performance in
our case among other alternatives such as vanilla sgd and momentum based sgd 
 

fi   

fixed window weight model

   

motivated by the recent successes of attention based models  we
explore an attention mechanism for our problem as well  we experiment with a soft attention model  i e   a weight ai between  
and   is assigned to each position i  the attentions are assumed to
be the result of a sigmoid function on a linear combination of the
concatenated word vectors  subsequently  the word vector for the
ith token is weighted by ai   and the aforementioned nl k model
is applied on the concatenation of these weighted word vectors
instead  thus  for example  an attention based nl   model takes
the form of 

in the spirit of continuous bag of word  cbow  model       in
this model we assume that a token at position i has a weight
of wi   and combine the token vectors of the window according to
these weights  the final score is assumed to be a linear function
of this weighted token vectors  thus  the overall model is
u 
sj  

w
x

wi vti

   

i  
ptj u

   
esto
p s
j
je

l   log

 

u    vt    vt    vt            vtw  
a    au 
z    a  vt    a  vt    a  vt            aw vtw  
sj   nl   z  q    q    q    pj  
 
esto
l   log p sj
je

   

the parameters in this model are the weights wi   the word vectors vi   and the prediction vectors pj   the gradients of the loss
w r t  the parameters are obtained by backpropagation  the details
of which are omitted here for space 

   

in this case  we do not introduce any averaging as in the case of
cbow  but instead simply concatenate the token vectors to create
a larger vector which is then used to create the scores  formally 
the model is
u    vt    vt    vt            vtw  
ptj u

l   log

   

   
   

esto
p s
j
je

   

u    vt    vt    vt            vtw  
z    relu q  u 
z    relu q  z   
z    relu q  z   

   
   
   
    

sj   ptj z 

    
e
p

j

esj

 g    g    g            gw     gru  vt    vt    vt            vtw   

    

ptj  g 

    

sj  

l   log

esto
p s
j
je

 
    

here  gi is the output of the ith gru cell  and we have a dense
layer after that to get the scores for each output token  as we did
not achieve competitive performance with this model as compared
to nl    we did not experiment further with deeper gru models 

this case differs from the matrix vector model by addition of
one or more non linear transformations between the concatenated
token vectors and the final scores  we denote a specific instance of
this model by nl k  where k is the number of non linearity layers
it contains  thus  for example  nl   is equivalent to the matrixvector model described above  while for nl   the probabilities
are obtained as 

sto

gru based recurrent model

 

feed forward neural network model

l   log

    

recurrent neural network models based on cells like lstm    
and gru     have recently been shown to achieve state of theart performance in language models  inspired by these results 
we also attempted to use a gru based recurrent model for our
prediction task  specifically  our model was the following 

note that since here u is dw dimensional instead of being d
dimensional as in the case of fixed window weight model  thus 
the prediction vectors pj are much higher dimensional as well 
which means this model has a higher number of parameters as
compared to fixed window weight model 

   

    
    
    
    

here nl   x  q    q    q    pj   is the function going from u to sj
in the case of a nl   model  described in section     

matrix vector model

sj  

feed forward model with soft attention

 

experiments and results

in this section  we evaluate the methods outlined in section    on
linux kernel and twisted library  we first present the accuracy of
predictions on the test set  and then present some interesting predictions that come out from recurring patterns  we implemented
the matrix vector model in python  and the feed forward and recurrent models in keras      our code is available on github     

   

accuracy

method
win   keys
nl            
nl            
attn          
bestkw randpos

 
    

here relu    is the rectified linear unit  i e   relu x    x if x    
and   otherwise  in this work  we specifically experimented with
nl    nl    nl    and nl   

known
acc 
    
    
    
    

abs
acc 
    
    
    
    

top  
acc 
    
    
    


key
acc 
    
    
    
    

pos
acc 
    
    
    
   

table    test accuracy     of predictions for linux project
 

fimethod
win   keys
nl           
nl           
attn         
gru         
bestkw randpos

known
acc 
    
    
    
    
    

abs
acc 
    
    
    
    
    

top  
acc 
    
    
    
    


key
acc 
    
    
    
    
    

pos
acc 
    
    
    
   
   

from the report for space  unlike natural language texts  however 
we did not observe any significant vector relationships between
different tokens 

   

examples

another experiment we ran was to evaluate the accuracy of the
classifier in a setting where  if the correct token is not within the
top few predictions  the user types in one or more characters to
home in on the correct prediction  figure   shows the results for
this experiment  in this experiment  a character is colored in green
if the token is correctly predicted  i e   top prediction  before that
character is typed  it is colored in yellow if the token is among
the top   before typing that character  and colored in red if the
token is not among the top    so  for example  in the first line
in the token struct  the character s is in yellow since before
typing s  the token struct was among the top    after typing s 
the top prediction was struct  and thus the characters truct
are all in green  we can see that  for example  entire sequences
of code such as   pci vdevice sundance  are predicted
correctly without typing anything 

table    test accuracy     of predictions for twisted project

table   shows results for linux source  a c project  and table  
for twisted  a python networking library  as mentioned in section    we use half the files for training and half for testing  column   lists the learning method  window size and number of key
words  nl   is a feed forward model with   non linearity layers  and nl   is a feed forward model with   non linearity layers  attn  computes a weighing parameter for each input token in
the window  and uses   non linearity layers  gru is a recurrent
model  we do not report numbers for gru in the linux codebase
since it gave poor results in the smaller twisted codebase  additionally  we report numbers for a fictional random predictor that
achieves the accuracy of the best predictor in case of keywords 
but in case of a non keyword token predicts one randomly from
the non keyword tokens in the window  to show the significance
of positional tokens  we name this bestkw randpos 
known acc  is the accuracy for predictions for the cases where
the next token is a key token  or a positional token from the window being considered  abs acc  is the accuracy for predictions
for all cases  including cases where the next token is neither a key
word nor a seen positional token  these are often new function
names or variable names  or variables outside of our window  top
  is the percentage of cases where the next token is within top
  predictions  ignoring the unknown cases  unseen variable and
function names   we report this number  because generally  we
are interested in the top few suggestions as opposed to the top
suggestion with a code recommendation system  key acc  is the
accuracy with which we predict key tokens correctly  given the
next token is a key token  and pos  acc  is the accuracy with
which we predict positional tokens correctly  given the next token is a positional token  these give us insight into how well our
methods predict tokens that are not key tokens 
we found that attention gives the best results among all the
methods we tried  this makes sense because it is also the most
general of all models  it directly connects all inputs to the output   unlike gru where computed states are chained   and it has
an additional attention parameter  that is just a constant in the simple feed forward case  we also found that the feed forward model
with   non linearity layers tends to overfit the training data  giving
a slightly lower accuracy on test data  increasing the word vector
dimensions also tends to result in overfitting  though regularization can help with overfitting  we found that reducing vector dimensions and number of layers was also sufficient to reduce high
variance from overfitting 
the accuracy of predicting non key words  that is  positional
tokens is quite high for linux kernel  a c project  indicating there
is a lot of redundancy  recurring patterns  the positional token
accuracy is lower for python  we think this is because python is a
dynamically typed  high level language with less redundancy  and
more terse syntax  however  in both cases  the top   prediction
accuracy is still much higher than a random predictor  indicating
that the model does extract some patterns out of the code 
a plot of the word vectors reveals a significant clustering of
similar words  uint  t and uint   t  different locking
and unlocking calls  and integers  we have omitted these plots

figure    code example with per character predictions

     

importance of positional tokens

from figure   we can also see the importance of positional tokens  the tokens pci vdevice and sundance are both not
keywords  i e   not frequently observed in the codebase   and yet
were predicted correctly by our method the second time they were
used  in line     if we chose to ignore such tokens instead  we
would not have been able to predict such tokens correctly in a
majority of cases 
     

learning frequently occurring patterns

we also found several interesting patterns that are predicted correctly by our system  an example of such a pattern is given in
figures  a    b  in these figures  each character is color coded as
before  and some additional information is also shown  the top
  predictions at the position of interest  shown in red border   are
shown in a blue box  also  the value of attention for each token in
the prediction window are shown color coded in sky blue  where
deeper blue is indicative of more attention  for reference  the first
if tokens in both of these two cases have an attention value of 
   
here  a variable is assigned to right before an if statement 
and the prediction is that the variable will be immediately used
inside the if condition  this is a common pattern in linux code 
where if the condition inside if is too long  it is broken up into
an assignment and a subsequent conditional 
also  in the figure the top   predictions for each case is given in
the blue box  we can see that unlikely is a common prediction 
since if  unlikely condition   is a common pattern in
 

fi a  example  

 a  mutex lock and mutex unlock pair on same lock variable
 b  example  

figure    definition before if pattern
linux code to check for unlikely edge cases such as error conditions 
we can also see that the attention of the models is relatively
high at the correct predictions  skb and framelen in the first
and second cases respectively   this shows that even a single layer
model can be expected to fairly reliably predict the next token
 since the attention is derived from a single non linear layer  
a few more interesting examples are shown in figure    a
description of why each of the cases are interesting are given in
the captions 

 

 b  duplicate code after function definition predicted correctly

conclusions and future work

we found that a learning based approach extracts some interesting patterns  that cannot be captured by language rules  we model
tokens using dense word vectors and use natural language processing methods such as a simple matrix vector model  feed forward neural network models  attention models and recurrent models  these models do not use any information about the language
grammar or syntax  the attention based model performs the best 
because it directly connects each input token to output  and also
computes weights for important tokens  it is the most general of
all models  with the most parameters  we found that by adjusting the vector dimensions and number of layers  we could reduce
overfitting  and get close accuracies for training and test data  our
top   predicted tokens give an accuracy of over     for the cases
where the next token is known  a key or a postitional token   for
linux  and over     for twisted  a study of these vectors reveals
a significant clustering of related tokens 
though we did not use any information about language grammar and syntax  it would be interesting to combine language rules
with our learning based approach  for example  we could prune
our predictions to those that are only syntactically valid  we could
also increase our window size  and jointly process  h and  c files 
to improve the context  such large contexts may not scale well
with feed forward model  but recurrent models are known to perform well with large inputs  we could also try to use language
rules to list out all possible options for next token  and then chain
that to a learning based model to improve the predictions  we
could also try to learn these language rules instead of listing them 
by processing a large number of different projects 

 c  member variables predicted correctly

 d  case token predicted correctly after each break 

figure    interesting code patterns predicted correctly
    keras  http   keras io  
    linux source code  https   cdn kernel org pub 
linux kernel v  x linux       tar xz 
    twisted source code 
https       github   com  
twisted twisted git 
    d  bahdanau  k  cho  and y  bengio  neural machine
translation by jointly learning to align and translate  arxiv
preprint arxiv                 
    y  bengio  r  ducharme  p  vincent  and c  janvin  a neural probabilistic language model  the journal of machine
learning research                   

references

    m  bruch  m  monperrus  and m  mezini  learning from
examples to improve code completion systems  in proceed 

    contextual code completion  https   github com 
subhasis    ml code completion 
 

fiings of the the  th joint meeting of the european software
engineering conference and the acm sigsoft symposium
on the foundations of software engineering  pages        
acm       

in interspeech         th annual conference of the international speech communication association  makuhari 
chiba  japan  september              pages          
     

    k  cho  b  van merrinboer  d  bahdanau  and y  bengio 
on the properties of neural machine translation  encoderdecoder approaches  arxiv preprint arxiv                 

     t  mikolov  i  sutskever  k  chen  g  s  corrado  and
j  dean  distributed representations of words and phrases
and their compositionality  in advances in neural information processing systems  pages                

    s  hochreiter and j  schmidhuber  long short term memory 
neural computation                      

     t  mikolov  w  t  yih  and g  zweig  linguistic regularities
in continuous space word representations  in hlt naacl 
pages              

     r  kneser and h  ney  improved backing off for m gram
language modeling  in acoustics  speech  and signal processing        icassp           international conference
on  volume    pages         ieee       

     k  s  tai  r  socher  and c  d  manning  improved semantic
representations from tree structured long short term memory networks  arxiv preprint arxiv                  

     m  t  luong  h  pham  and c  d  manning  effective
approaches to attention based neural machine translation 
arxiv preprint arxiv                  

     k  xu  j  ba  r  kiros  a  courville  r  salakhutdinov 
r  zemel  and y  bengio  show  attend and tell  neural image caption generation with visual attention  arxiv preprint
arxiv                  

     t  mikolov  m  karafit  l  burget  j  cernocky  and s  khudanpur  recurrent neural network based language model 

 

fi