 

algorithmic trading of cryptocurrency
based on twitter sentiment analysis
stuart colianni  stephanie rosales  and michael signorotti

f

 

a bstract

p

ast research has shown that real time twitter data can
be used to predict market movement of securities and
other financial instruments      the goal of this paper is
to prove whether twitter data relating to cryptocurrencies can be utilized to develop advantageous crypto coin
trading strategies  by way of supervised machine learning
techniques  our team will outline several machine learning
pipelines with the objective of identifying cryptocurrency
market movement  the prominent alternative currency examined in this paper is bitcoin  btc   our approach to
cleaning data and applying supervised learning algorithms
such as logistic regression  naive bayes  and support vector machines leads to a final hour to hour and day to day
prediction accuracy exceeding      in order to achieve this
result  rigorous error analysis is employed in order to ensure
that accurate inputs are utilized at each step of the model 
this analysis yields a     accuracy increase on average 

 

i ntroduction

cryptocurrency is an alternative medium of exchange
consisting of numerous decentralized crypto coin types 
the essence of each crypto coin is in its cryptographic
foundation  secure peer to peer transactions are enabled
through cryptography in this secure and decentralized
exchange network  since its inception in       the bitcoin
has become a digital commodity of interest as some believe
the crypto coins worth is comparable to that of traditional
fiat currency 
considering the exchange rates of cryptocurrencies are
notorious for being volatile  our team strives to develop an
effective trading strategy that can be applied to a variety of
cryptocurrencies  our method for determining the optimal
time to trade involves correlating prices with one of todays
most popular social media sources  twitter  the advantages
of using twitter include having access to some of the
earliest and fastest news updates in a concise format as well
as being able to extract data from this social media platform
with relative ease 
our trading strategy applies supervised machine learning
algorithms including support vector machines  logistic regression  and naive bayes to determine whether the price of
a particular digital currency will increase or decrease within
a predetermined time interval  the two approaches for

training these classifiers involve using direct text  otherwise
known as tweets  from twitter users and using third party
open source sentiment analysis apis to rate the positivity
and negativity of words within each post  both of these
training methods prove to be effective in estimating the trajectory of cryptocurrency prices  in order to predict market
movement to a particular granularity  a time series of tweets
equal in length to the trading period is required one cycle
beforehand  this time series of twitter posts is used as an
input to the classifiers 

 

r elated w ork

applying machine learning to cryptocurrency is a relatively
new field with limited research efforts  using bayesian
regression  shah et al  achieved an     return on investment
over fifty days of buying and selling bitcoins      another
approach predicted the price change of bitcoin using
random forests with       accuracy      these approaches
fail to consider the feelings of individuals about bitcoin 
and therefore  fail to harness these potential features in their
learning algorithms  twitter sentiment analysis has been
widely researched  bollen et al  utilized the profile of mood
states  poms  to predict the movement of the dow jones
industrial average with       accuracy  go et  al focused
only on classifying tweets and used several approaches
to achieve an accuracy of       with multinomial naive
bayes        with maximum entropy  and       using a
support vector machine      this paper will expand on the
approaches of researches in the past and apply twitter
text classification and sentiment analysis to cryptocurrency
markets 

 

data

in order to create a training and testing data set for the
learning algorithms  we utilize tweepy   an open source
python library for accessing the twitter api       the
keyword bitcoin  is searched in real time and tweets
containing this token is placed into a text file  additional
data being collected for each post containing the keyword
includes the user id  a unique identifier which cannot
be changed  and a time stamp  in addition  the prices
of the cryptocurrency is collected every hour via the
cryptonator com api and placed into text files to create a

fi 

price history     
while tweets are collected in real time  excess white space
is removed and the text is changed to lowercase  to clean
the data  the following procedure is carried out  the
first step is to remove all non alphabetic characters  the
second step is to remove duplicates  the reason for doing
this is because of the prevalence of twitter bots  many
of which instantaneously disseminate tweets containing
particular keywords  not removing these tweets will cause
the distribution of words in our training set to be skewed 
invalid english words which remain are identified and
removed based on not having membership in the words
corpus of the natural language toolkit      stop words are
subsequently removed from tweets based on membership
in the stopwords corpus of the natural language toolkit 
we then create two data sets  one with stemming and
one without  in the stemming set  all remaining words
are stemmed using the porter stemming algorithm  then 
entries which no longer contain words are removed from
the data set 
the duration of the data collection process was     hours
spanning over twenty one days  during this period  over  
million tweets pertaining solely to bitcoin were collected 
the processed data set consists of over         individuals
who posted at least once about bitcoin  the distribution of
the number of bitcoin related tweets per user is exhibited in
figure    please note that the x axis is in log scale due to the
large variance in tweets per user 

in order to ensure accurate prediction results across all days 
it is important to consider whether the number of data
points each day is fairly consistent  figure    which displays
the number of tweets that mention bitcoin per day  proves
the posting frequency for this particular class of tweet is
relatively uniform 

fig     the above chart shows the number of bitcoin related tweets per
day from november          to december          the dates included
are limited to days with continuous data collection 

 

in order to determine digital currency market movement
with the twitter data set  text classification and sentiment
analysis algorithms are utilized  the goal of each algorithm
is to predict whether the price of bitcoin will increase or
decrease over a set time frame  for the text classification
approach  the implementations of naive bayes  logistic
regression  and support vector machines in the scikit python
library are utilized      training and testing on sentiment
analysis data requires the same implementation of support
vector machines and logistic regression  although both
types of algorithms are trained on the same data set  the
fundamental approaches to formatting each models feature
vector is quite different 

   

fig     the number of bitcoin related tweets per user is displayed above 
the x axis is in log scale  the data was collected from november    
     to december         

m ethod

feature vectors

for the examples below  suppose that we wish to reconstruct
the processed version of the following tweet into a feature
vector  without the removal of stop words  
bitcoin has a bright future in the worlds economy 

as displayed in figure    the majority of individuals
contributed only a few times throughout the data collection
period  although some users tweet far more than others 
the individuals with the most posts only contribute to a
fraction of the   million tweet data set  half of all users who
posted at least one tweet about bitcoin only tweeted once
about the digital currency 

the features for text classification consist of a vector of
all unique words in the data set lexicon  since the vector
encompasses all possible unique entries  it is sparse even
for the longest of tweets  suppose that there are n words in
the learning algorithms vocabulary  an example of a text
classification feature vector for this model with each entry x
       n is as follows 

fi 


  
 
 

     
  

  
 

  
   bright 

  

     
  

  
 

  
  economy 

  

     
  

  
 

  



x        f uture 


     
  

  
 

  

  
 

  

  
  

     
 

  
   worlds 

  

  
  
 

  
 
 
 
when a particular word is observed at least once  a binary
value of one is recorded in the position of that word in
the feature vector  when the total count of each word is
represented in the same format of feature vector  the input
is modeled as multinomial rather than bernoulli  therefore 
the entries in a multinomial feature vector will take on
values x            k n  
training and testing feature vectors for sentiment analysis
models are fundamentally different  in order to generate
feature vectors of this structure  preprocessed tweets are
analyzed word by word in the text processing com api     
this api returns scores between zero and one for words
positivity  negativity  and neutrality  these scores are aggregated into a single vector similar to the one below 



    
x       
   

   

naive bayes

the naive bayes is a generative learning algorithm which
is commonly applied to text classification and sentiment
analysis machine learning problems  this approach to text
classification utilizes the first format of feature vector where
the appearance of a word is modeled by either the bernoulli
or multinomial distribution  in both versions of this algorithm  we assume the xi variable given y in the naive bayes
mathematical program below to be conditionally independent of one another 

argmax p  y   yj  
yj

m
y

p  xi  y   yj  

built for both positive and negative variation in the market 
for each observation in the training set  the above product
of probabilities is calculated assuming each market trend 
and the results are compared  the classification resulting
in the higher probability is assumed true and subsequently
assigned to that particular post 
   

logistic regression

discriminative learning algorithms such as logistic regression are also useful in the field of text classification and
sentiment analysis  unlike generative learning algorithms 
this model examines two classes in the training set and
determines the best separation  the logistic regression learning algorithm can be derived by maximizing the following
likelihood function 

l    

m
y

 i 

 h  x i    y     h  x i     y

 i 

i  

in this likelihood function  x  i  takes the form of either
of the previously mentioned feature vectors  the index  i 
maps the feature vector to one of the observations in the
training set of size m  the exponent  y  i    represents the
state of the market for feature vector i  the function h is
the sigmoid function below 

g z   

 
    expz

in order to determine an update rule for the parameters 
theta  the log likelihood function can be formulated  this
function can then be differentiated in order to derive the
stochastic gradient ascent formula below 
 i 

j    j    y  i   h  x i    xj

after reaching convergence  the parameters  theta  are utilized in the sigmoid function in order to classify the state of
the digital currency market 
   

support vector machines

support vector machines are supervised learning algorithms
that can perform nonlinear classifications by mapping data
to higher dimensions through the use of the kernel trick 
support vector machines are an effective tool in sentiment
analysis as proven by go      the l  norm soft margin
model below can be trained with either the text classification
feature vector or the sentiment analysis feature vector 

i  
m

min
in this formulation  yj represents the classification of
whether the bitcoin price is increasing or decreasing over
a predetermined time interval  the variable xi is the feature
vector for tweet i where a total of m tweets are collected 
since this is a generative learning algorithm  a model can be

x
 
kwk     c
i
 
i  

s t  y  i   wt x i    b      i
i   

i           m
i           m

fi 

fig     accuracy using sentiment as feature vector

this support vector machine has the l  norm soft margin
formulation which includes a penalty term for points which
are not linearly separable  this penalty term for this model
is below 
m
x
c
i
i  

the term  i   is a slack variable  the penalty equation acts as
a trade off between having a large separation between terms
and incorrectly classifying observations  the feature vector 
x  i   is used with both the text classification and sentiment
analysis forms outlined earlier in the section  the variable 
y  i    is the observed class for a particular observation 
   

fig     confusion matrix   bernoulli nb day to day

training and testing

training and testing the models for the text classification
and sentiment analysis problems involves nearly identical
steps  we partitioned the training set in a       split where
    of the data is reserved for training and     is marked
for testing  when performing this random sampling  we
assure there is an equal representation of tweets from both
classes while maintaining an accurate timeseries representation  after training the model on this balanced training set 
we record the rate at which samples are classified correctly
with respect to the test set data points  we repeat this
procedure ten times and aggregate the correct classification
rates over all iterations 

 

classifier  logistic regression using sentiment scores  and
the most accurate day to day classifier  bernoulli naive
bayes and multinomial naive bayes using the tweet as a
feature vector  

r esults

we tested classifiers using two data sets  one where the
tweet had been stemmed using porter stemming and one
without  to assess the performance of different classifiers 
we computed the accuracy of each  in every case  the accuracy of the classifier was on par or significantly better when
operating on the unstemmed data 
in our first classification attempts  we treated the words
in a tweet as elements of the feature vector for each classifier 
bernoulli naive bayes performed the best out of all text
classification algorithms by achieving a day to day accuracy
of        and an hour to hour accuracy of        
fig     accuracy using tweet as feature vector

in our second classification attempts  we used the textprocessing com api to calculate negativity  neutrality  and
positivity scores for each tweet  the api also returns a positive  neutral  or negative label  these labels were used as the
feature vectors for naive bayes  bernoulli and multinomial  
the return scores were used as the feature vectors for the
classifiers  logistic regression performed the best using this
feature vector achieving a day to day accuracy of       
and an hour to hour accuracy of        
a confusion matrix along with the precision  recall  and
f score were calculated for the most accurate hour to hour

fig     confusion matrix   logistic regression hour to hour

the precision matrix for day to day results has a precision of    a recall of       an accuracy of      and an fscore of       the negative predicted value calculated to
      and the true negative rate calculated to    the precision
matrix for hour to hour results has a precision of       a
recall of       an accuracy of       and an f score of      
the negative predicted value calculated to       and the true
negative rate calculated to      

 

e rror a nalysis

our initial accuracy for predicting the hour to hour sign
change of bitcoin using the bernoulli naive bayes classifier
was        in order to determine the possible error locations within the machine learning pipeline  we thoroughly
inspected inputs at each step  during one of the data processing steps  we noticed many near duplicate posts that
differed based on numerical id values within the tweets
text  in order to remove these automated postings  we
adjusted the data cleaning procedure by removing all nonalphabetic characters before removing duplicate tweets  in
order to ensure these automated postings were removed 
we computed the levenshtein distance from each tweet
to every other tweet  the levenshtein distance is the edit
distance between two strings  this value can be leveraged
to prove that any pair of tweets is dissimilar by at least
some threshold value  after cleaning  the new data set
was roughly     smaller  but yielded a significantly better
classification accuracy of        

 

f uture w ork

in order to further improve the accuracy of the learning
algorithms  additional research can be performed in the area
of error analysis  an improvement that can be explored
for text classification algorithms involves accounting for

fi 

negation as outlined by jurafsky et al      according to
jurafsky  an efficient method of accounting for negation in
text analysis is to prepend the prefix  not   after a negated
word  an additional modification that can be made to
the training set is to ensure that the training set has an
equal number of words associated with each classification 
although our sets are relatively equal       words in the
price decrease set and      in the price increase set   creating
a training set that is completely unskewed could result in
lower classification error  in addition  we can formulate a
set of words where each element has a high correlation with
cryptocurrency market movement and use this as a basis for
training the learning algorithms  this adjustment will result
in sparser feature vectors for text classification and possibly
more accurate predictions 

r eferences
    bollen  johan  and huina mao  twitter mood predicts the stock
market  http   arxiv org pdf                 oct        web    
nov       
    go  alec  lei huang  and richa bhayani  twitter sentiment analysis 
entropy           
    jurafsky  daniel  classification  naive bayes  logistics regression  sentiment         web     dec       
    madan  isaac  saluja  shaurya  and aojia zhao  automated bitcoin
trading via machine learning algorithms  department of computer
science  stanford university 
    natural language toolkit  natural language toolkit nltk    
documentation        web     dec      
    online bitcoin wallet  cryptonator        web     dec       
    scikit learn    machine learning in python      documentation 
web     dec       
    shah  devavrat and kang zhang bayesian regression and bitcoin 
http   arxiv org pdf          v  pdf    oct        web     nov 
     
    text processing com  api documentation for text processing com
text processing com api     documentation  web     dec       
     tweepy  tweepy  web     dec       

fi