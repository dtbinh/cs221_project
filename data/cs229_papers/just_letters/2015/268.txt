assessing and implementing automated news
classification
francisco romero

zahra koochak

department of electrical engineering
stanford university
stanford  california
faromero stanford edu

department of electrical engineering
stanford university
stanford  california
zahraa stanford edu

abstractnewsfeed websites sort articles by
subject to make it easier for readers to search for
articles in their preferred category  when
uploading a new article  authors are usually tasked
with selecting the most pertinent category so the
new addition can then be grouped with similar
articles  we are interested in further developing
the framework to automatize the classification of
news articles using machine learning and natural
language processing  nlp  techniques  we
explore three classification methods  support
vector machine  svm   nave bayes  and softmax
regression  and evaluate each classifiers ability to
select the appropriate category given an articles
title and a brief article description  our results
show softmax regression to be the best classifier
among the three we evaluated 

description as the input to our classifier  we then
evaluate the capability of our classifier using a
minimal amount of information about the articles
subject  finally  we used three supervised learning
classifier to output a predicted article category  nave
bayes  support vector machine  svm   and softmax
regression  our data spans over seven categories 
sports  us  science and technology  business 
world  entertainment  and health  based on the
lexical features of each article  it was the job of each
classifier to select the most appropriate category for
the article 

   relevant work
previous work has focused on developing algorithms
and software to automate the process of accurate text
classification  young and jeong implemented a new
feature scaling method that uses the nave bayes
classifier  the feature scaling method was tested on a
news group dataset and outperformed other popular
ranking schemes  such as information gain while
noting nave bayes as being a suitable classifier for
news articles       wang et al  developed an optimal
text categorization algorithm that is based on the
svm algorithm used in this paper       using a news
article corpus similar to ours  they found their
algorithm to outperform other classifiers such as the
decision tree algorithm and the k nearest neighbor
algorithm  hakim et al  evaluated the term
frequency inverse document frequency  tf idf 
algorithms ability to be used in text classification for
news articles in bahasa indonesia       however 
their approach did not focus on any machine learning
techniques  only on the tf idf algorithm  looking to
our future work  one of the first frameworks for
neural networks was developed by ruiz and
srinivasan       using about       documents  they
showed the ability of neural networks to accurately
categorize text  do and ng explored text classification
using a modified softmax regression algorithm      

index termsnews  articles  nlp  svm  nave
bayes  softmax  classification  tf idf

   introduction
when visiting a newsfeed website  we are often
interested in reading articles in a specific category 
based on their content  articles are sorted by subject 
which allows readers to effortlessly find articles in
their preferred category  to determine the articles
category  most newsfeed websites ask the author to
select the best fit category for their article  selecting
an articles category is not only based on the authors
opinion  but can also be tedious when several articles
are simultaneously being added to a newsfeed
website  since the vocabulary and terminology used
by an articles author is indicative of the target
audience and  more generally  of the articles
category  we believe this process can be effectively
automated 
for our project  we are interested in assessing three
classification methods to determine the feasibility of
automatically classifying news articles  we selected
to use the articles title and a     sentence article

 

fitable    lexical feature extraction
top    words for each category
sports

us

nfl
first
game
win
over
new
players
season
coach
mets

new
us
state
texas
police
states
tuesday
over
wednesday
court

science  
technology
new
apple
google
space
facebook
us
online
internet
sony
ipad

business

world

us
billion
new
prices
oil
bank
sales
may
stocks
up

president
us
killed
forces
nuclear
government
police
people
bin
libya

entertainment
new
theater
show
star
idol
wedding
american
up
first
film

health
study
new
health
cancer
us
may
drug
risk
heart
people

the classifier outperformed one against all svm and
multi class svm 

necessarily reflect the classifiers ability to determine
the category of an article from the testing set 

our work deviates from the aforementioned studies in
that we used only the title and a short description of
each article for our lexical feature extraction and we
focused on evaluating all three classifiers rather than
trying to optimize the performance of a particular one 
in addition  we are using a variant of the softmax
regression algorithm presented by do and ng to
perform our text classification 

pre processing the news article data involved three
steps  first  we separated each articles title 
description  and pre labeled category into a separate
text file  since the corpus is formatted into a single
file  second  we removed all punctuation from the
title and description  third  we capitalized all letters
in the title and description  the latter two steps are
necessary for performing a lexical feature extraction
using the vocabulary of the title and the article
description 

   methodology
to test the three classifiers  we divided our data into
training news articles and testing news articles     
of our data         articles  were designated as the
training articles and the remaining          
articles  were designated as the testing articles 
since an author writes an article with an intended
category or subject in mind  we believe the
vocabulary can be used as our features for our
classifiers  thus  our objective for our feature
extraction was to obtain the f most salient words for
each category  and count how many times each word
appeared in a given article  we tested feature sizes of
f                   and     to obtain the best accuracy
possible for each classifier  to obtain the f most
salient words for each category  we used the tf idf
algorithm  which we explain in the next section  the
extracted features were then passed to each of the
three classifiers 

figure    distribution of the tagmynews data

   dataset and pre processing
to perform our classification evaluation  we used the
tagmynews dataset      the corpus includes       
training examples of news articles  each training
example has a structure including a title  a description 
a news article link  an id  the date of publication  the
news article source  and a subject category  of interest
to us were the articles title  the brief article
description  and the pre labeled category  figure  
shows the distribution of news articles for each
category  the majority of the training examples were
from the sports category  while we had the fewest
training examples from the health category 
however  as we show in section    the number of
training examples for each category did not

    extracting salient words with tf idf
the tf idf weighting scheme will assign each term 
t  a given weight in a document as follows 
           log   

   

where n is the number of documents  the weight is
assigned by the product of       the term frequency 
and log      the inverse document frequency  for

 

fieach category  we computed the tf idf of each term
and obtained the f most salient words from the sorted
list of tf idf rankings  while the goal of using tfidf was to extract the most meaningful and indicative
words for each category  we needed to further filter
the results of the algorithm to exclude words such as
the  or  him  which carry no significance  thus 
we implemented a stop word list to remove these
meaningless words from our feature set based on      
table   shows a list of the top    words extracted for
each category using tf idf 

the libsvm library offers different options that
allow a user to set the svm type and the kernel type 
as well as values for the different parameters  for our
svm  we tested multiple kernel functions  including a
linear  polynomial  and radial basis function  rbf  
we found the rbf to have the best performance  and
subsequently used it for our implementation of the csvc  the rbf kernel is given by              where 
is a weighting parameter and  is a query point     
in order to maximize the performance of the svm  we
needed to optimize the parameters  and   to do so 
we performed a parameter grid search by
implementing internal cross validation      we
selected exponentially growing ranges for both  and
                        and                          
using   fold internal cross validation on only the
training data  we iterated through the different  and
 options  noting the values that gave the highest
accuracy during each fold  we did not include any of
the test data in the internal cross validation  since
optimizing for a given test set would be incorrect 

we also investigated a variation of the tf idf
algorithm called sublinear tf scaling     
           log   

   

where     is given by 
   
    log      
        
  

each terms frequency is now assigned a weight 
which may represent a terms significance more
accurately than just counting the number of
occurrences  however  we found the classic tf idf
to have better performance on all three models over
this modified version of equation    using equation
   the top f words for each category were similar to
those of table    but not exactly the same  since the
classic tf idf performed better for all three
classifiers  we did not use equation   in our final
implementation 
     

libsvm implements a one against all training model
for the svm when using a multi class dataset  for
each label value                 a different svm
model is trained  thus  we make k different binary
models  we then test each model on the testing data
and determine the model from which the highest
prediction confidence is returned in order to classify
the data 

    implementing the classifiers

      nave bayes

we selected to evaluate nave bayes  svm  and
softmax regression due to their ability to perform
supervised learning on multi class datasets  in the
following sections  we describe each algorithm and
how it pertains to our goal of news classification 

nave bayes was used as our baseline text classifier
because it could be quickly implemented for analysis 
since our news classification framework has been
dened for multiple classes  we have developed the
appropriate algorithms for this case as 

      support vector machine

            

the svm algorithm requires the solution to the
following optimization problem     
 
min

       
    

 

 

 

 
 

 

 

   

 
   

            

   

         

   
         

from       these parameters have a natural
interpretation  for example         is the fraction of
the category i in which word j appears  having fit all
the parameters  we calculate 
           
   
        
  
     

   
 

 
 
       
 
   

        

    
where  is a regularization weighting parameter  the
goal of the svm is to find a linear separating
hyperplane that has the maximal margin in the higher
dimensional space that   is mapped to      for our
project  we used the libsvm matlab library to
implement our support vector classifier  svc      
 

fifigure    nave bayes performance measurements

figure    softmax performance measurements

figure    svm performance measurements

figure    average performance measurements for each
classifier

since we are applying the classifier over a large
vocabulary  we implemented laplace smoothing to
avoid having    end up as zeros 

      softmax regression
we selected to use softmax regression  also known
as the multinomial logistic regression  as opposed to
  binary classifiers because our seven classes are
mutually exclusive  i e  a news article will be a part of
at most one category   for this classifier  the class
probabilities      are modeled as 
   

   
 
  
    

 

 

   

 
    

  

   

      

figure    classifier accuracy as feature size varies

performance out of the three classifiers  with a
maximum accuracy of        at     features  nave
bayes performed significantly worse than the other
two  achieving a maximum accuracy of        for
    features  which we attribute to its simplicity and
weak scalability  svms best performance was not
much worse than softmax regression  with a
maximum accuracy of        for     features  both

where the  parameters are learned from the training
set by maximizing the conditional log likelihood of
the data       in this approach  a total of k parameters
are trained jointly using numerical optimization 

   results and discussion
the weighted accuracy of each classifier is presented
in figure    softmax regression achieved the best
 

finave bayes and svm ran into overfitting issues at
    features  i e  performance began to decrease   we
attribute the performance drop to the minimal overlap
in words between the different categories  especially
as the feature size increased  however  the optimized
svm performed much better than the non optimized
svm  which ran into overfitting issues after   
features and had a maximum accuracy of        
since softmax regression did not run into overfitting
issues for our evaluations maximum feature size 
future work will continue to increase the feature size
and further evaluate the performance of the classifier 

often being mentioned in a commercial setting  even
more interesting  the number of false positives for the
health category according to nave bayes is very
high  evident by the very low precision in figure    
using nave bayes  the majority of the falsenegatives for every category was health  which is the
source of the low weighted accuracy 

   future work
although the sublinear tf scaling modification to
the tf idf algorithm did not outperform the classic
tf idf  we would like to look into more tf idf
variants and other methods to improve the feature
selection process  other methods might include
information gain or conditional mutual information
     to explore the optimal feature size  we might
want to try a forward or backward search procedure
with a reduced dataset  we are also interested in
testing other classification methods  such as recurrent
neural networks  to compare them against the three
implemented in this work  in particular  classifying
articles into more specific categories  such as
computing instead of just science and technology 
may lead to classifier performance differences
compared to the results from this work 

to further evaluate each classifiers performance on
individual classes  we used the three measurements
derived from the confusion matrix  precision  recall 
and f  score  the latter three measurements reflect
the importance of retrieval of positive examples in our
text classification       precision is the class
agreement of the data labels with the positive labels
given by the classifier  while recall is the
effectiveness of a classifier to identify positive labels
      the f  score is the harmonic mean of precision
and recall and is given by 
     
   
   
figures     show each classifiers precision  recall 
and f  score for each category  the sports category
had the highest precision and recall  and
subsequently the highest f  score for all three
classifiers  this is due to the esoteric and overly
specific terms used in the sports categories lexical
features  as seen in table    for softmax regression
and svm  the science and technology category had
the lowest f  score  while for nave bayes  health
had the lowest f  score  while the plots of these three
metrics for svm and softmax regression strongly
resembled each other  nave bayes had precision
higher than recall for all categories except for health 
the balance between precision and recall for svm
and softmax regression is visible in figure    the
average precision and recall are almost identical 
demonstrating the two classifiers ability to perform
better on a per category basis  especially for softmax
regression 
   

beyond trying to classify news articles into a specific
category  we would like to explore the application of
our developed framework towards detecting emotions
or bias in a news article  work has been done to
explore using svms and semi supervised learning
models for political bias by      in addition       
investigated emotional classification from both the
writers and readers perspective using svms 
effectively  we want to see how well our framework
generalizes to other underlying aspects of news
articles 

   conclusion
based on the f  scores in figure    our best classifier 
softmax regression  performed        worse than
the proposed methods of wang et al  for a similar
dataset  nevertheless  we have shown the ability of
three different classifiers to automatically classify
news articles into their subject category  nave bayes
performance was adversely affected by the high
number of false negatives from the health category 
the well known svm algorithm  although not the top
performer  was also found to be highly suitable for
classifying news article into their subject category 

the low f  scores of both the health and science and
technology categories are due to the inability of the
classifier to distinguish one category from another  in
particular  for science and technology  the majority
of the false negatives came from the business
category  this is most likely attributed to technology

   references

    a  lab  tagmynews dataset 
http   acube di unipi it tmn dataset 

 

fi    c  chang and c  lin  libsvm  a library for
support vector machines  in acm trans  on
intelligent syst  and tech   vol     no     pp       
      software available at
http   www csie ntu edu tw  cjlin libsvm

classifier for text datamining  in pattern
recognition letters  vol      no     pp          
     
    
z  wang et al   an optimal svm based text
classification algorithm  in      international
conf  on machine learning and cybernetics  pp 
                

    c  hsu et al   a practical guide to support
vector classification      
    a  ng  cs      class lecture  topic  support
vector machines  stanford university  stanford 
ca  october          

    
m  ruiz and p  srinivasan  automatic text
categorization using neural networks  in proc 
of the  th asis sig cr workshop on
classification research  pp              

    a  khan et al   a review of machine learning
algorithms for text document classification  in
journal of advances in inform  tech   vol     no 
   pp             

    
a  hakim et al   automated document
classification for news articles in bahasa
indonesia based on term frequency inverse
document frequency  tf idf  approach  in  th
international conf  on information tech  and
elec  eng  pp            

    c  manning et al   scoring  term weighting and
vector space model  in introduction to
information retrieval  cambridge university
press        ch     sec       pp          

    
m  sokolova and g lapalme  a systematic
analysis of performance measures for
classification tasks  in information processing
and management  vol      no     pp          
     

    d  zhou et al   classifying the political leaning
of news articles and users from user votes  in
proc  of the  th international aaai conf  on
weblogs and social media       

    
a  ng  cs      class lecture  topic 
supervised learning  stanford university 
stanford  ca  september          

    k  lin et al   emotion classification of online
news articles from readers perspective  in
proc  of the      ieee wic acm int  conf  on
web intelligence and intelligent agent tech  
     

    
a  ng  cs      class lecture  topic 
generative learning algorithms  stanford
university  stanford  ca  october         

    d  bracewell et al   determining the emotion of
news articles  in computational intelligence 
springer berlin heidelberg  pp                

    
c  do and a  ng  transfer learning for text
classification  in nips       

    
e  young and m  jeong  class dependent
feature scaling method using nave bayes

    
ranks  stopwords list 
http   www ranks nl stopwords

 

fi