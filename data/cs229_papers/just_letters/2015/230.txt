incorporating nesterov momentum into adam

timothy dozat

 

introduction

when attempting to improve the performance of a
deep learning system  there are more or less three
approaches one can take  the first is to improve the
structure of the model  perhaps adding another layer 
switching from simple recurrent units to lstm cells
     orin the realm of nlptaking advantage of
syntactic parses  e g  as in      et seq     another approach is to improve the initialization of the model 
guaranteeing that the early stage gradients have certain beneficial properties      or building in large
amounts of sparsity      or taking advantage of principles of linear algebra       the final approach is to
try a more powerful learning algorithm  such as including a decaying sum over the previous gradients
in the update       by dividing each parameter update by the l  norm of the previous updates for that
parameter      or even by foregoing first order algorithms for more powerful but more computationally
costly second order algorithms      this paper has
as its goal the third optionimproving the quality
of the final solution by using a faster  more powerful
learning algorithm 

 
   

related work
momentum based algorithms

gradient descent is a simple  well known  and generally very robust optimization algorithm where the
gradient of the function to be minimized with respect to the parameters   f   t     is computed  and
a portion  of that gradient is subtracted off of the
parameters 
classical momentum      accumulates a decaying

algorithm   gradient descent
gt   t  f   t   
 t   t   gt
sum  with decay constant   of the previous gradients into a momentum vector m  and using that instead of the true gradient  this has the advantage of
accelerating gradient descent learning along dimensions where the gradient remains relatively consistent across training steps and slowing it along turbulent dimensions where the gradient is significantly
oscillating 
algorithm   classical momentum
gt   t  f   t   
mt  mt    gt
 t   t   mt
     show that nesterovs accelerated gradient
 nag      which has a provably better bound than
gradient descentcan be rewritten as a kind of improved momentum  if we can substitute the definition for mt in place of the symbol mt in the parameter update as in    
 t  t   mt

   

 t  t    mt   gt

   

we can see that the term mt  doesnt depend on
the current gradient gt so in principle  we can get
a superior step direction by applying the momentum
vector to the parameters before computing the gradient  the authors provide empirical evidence that this
algorithm is superior to the gradient descent  classi 

fialgorithm   nesterovs accelerated gradient
gt   t  f   t    mt   
mt  mt    gt
 t   t   mt

cal momentum  and hessian free     algorithms for
conventionally difficult optimization objectives 
    l  norm based algorithms
    present adaptive subgradient descent  adagrad   which divides  of every step by the l  norm
of all previous gradients  this slows down learning
along dimensions that have already changed significantly and speeds up learning along dimensions that
have only changed slightly  stabilizing the models
representation of common features and allowing it
to rapidly catch up its representation of rare features  
algorithm   adagrad
gt   t  f   t   
nt  nt    gt 
 t   t    ngtt 
one notable problem with adagrad is that the
norm vector n eventually becomes so large that
training slows to a halt  preventing the model from
reaching the local minimum       go on to motivate
rmsprop  an alternative to adagrad that replaces
the sum in nt with a decaying mean parameterized
here by   this allows the model to continue to learn
indefinitely 
algorithm   rmsprop
gt   t  f   t   
nt  nt         gt 
 t   t    ngtt 

   

combination

one might ask if combining the momentum based
and norm based methods might provide the advantages of both  in fact      successfully do so
  most

implementations of this kind of algorithm include an
 parameter to keep the denominator from being too small and
resulting in an irrecoverably large step

with adaptive moment estimation  adam   combining classical momentum  using a decaying mean instead of a decaying sum  with rmsprop to improve
performance on a number of benchmarks  in their
algorithm  they include initialization bias correction
terms  which offset some of the instability that initializing m and n to   can create 
algorithm   adam
gt   t  f   t   
mt  mt         gt
mt
mt   
t
nt  nt         gt 
nt
nt   
t
t
 t   t    nm 
t

    also include an algorithm adamax that replaces the l  norm with the l norm  removing the
need for nt and replacing nt and  t with the following updates 
nt  max nt     gt   
t
 t   t    nm
t  
we can generalize this to rmsprop as well  using
the l norm in the denominator instead of the l 
norm  giving what might be called the maxaprop
algorithm 

 
   

methods
nag revisited

adam combines rmsprop with classical momentum  but as      show  nag is in general superior to classical momentumso how would we go
about modifying adam to use nag instead  first 
we rewrite the nag algorithm to be more straightforward and efficient to implement at the cost of
some intuitive readability  momentum is most effective with a warming schedule  so for completeness we parameterize  by t as well  here  the
algorithm   nag rewritten
gt   t  f   t   
mt  t mt    gt
mt  gt   t   mt
 t   t    mt
vector m contains the gradient update for the cur 

firent timestep gt in addition to the momentum vector
update for the next timestep t   mt   which needs
to be applied before taking the gradient at the next
timestep  we dont need to apply the momentum
vector for the current timestep anymore because we
already applied it in the last update of the parameters  at timestep t    
   

algorithm   nesterov accelerated adaptive moment
estimation
gt   t  f   t   
g   gtt 
i   i
mt  mt         gt
t
mt    mt  

i  

applying nag to adam

ignoring the initialization bias correction terms for
the moment  adams update rule can be written in
terms of the previous momentum norm vectors and
current gradient update as in     
mt 

 t  t    p
nt         gt    
     gt
 p
nt         gt    

   

in rewritten nag  we would take the first part of the
step and apply it before taking the gradient of the
cost function f however  the denominator depends
on gt   so we cant take advantage of the trick used
in nag for this equation  however   is generally
chosen to be very large  normally        so the difference between nt  and nt will in general be very
small  we can then replace nt with nt  without losing too much accuracy 
mt 
 t  t    
nt    
     gt
 p
nt         gt    

   

the the first term in the expression in     no longer
depends on gt   meaning here we can use the nesterov trick  this give us the following expressions for
mt and  t  
mt      t  gt   t   mt
t
 t   t    vmt  
all thats left is to determine how to include the
initialization bias correction terms  taking into consideration that gt comes from the current timestep
but mt comes from the subsequent timestep  this
gives us the following  final form of the nesterovaccelerated adaptive moment estimation  nadam 
algorithm  adamax can make use of the nesterov
acceleration trick identically  nadamax  

i

nt  nt         gt 
nt
nt   
t
mt      t  gt   t   mt
t
 t   t    nm 
t

 

experiments

to test this algorithm  we compared the performance of nine algorithmsgd  momentum  nag 
rmsprop  adam  nadam  maxaprop  adamax 
and nadamaxon three benchmarksword vec      
mnist image classification      and lstm language models       all algorithms used        
and     e  as suggested in      with a moment
tum schedule given by t                     with
        similar to the recommendation in      
only the learning rate  differed across algorithms
and experiments  the algorithms were all coded using googles tensorflow     api and the experiments were done using the built in tensorflow models  making only small edits to the default settings 
all algorithms used initialization bias correction 
   

word vec

word vec      word embeddings were trained using each of the nine algorithms  approximately
   mb of cleaned text  from wikipedia were used
as the source text  and any word not in the top      
words was replaced with unk      dimensional vectors with a left and right context size of   were
trained using noise contrastive estimation with   
negative samples  validation was done using the
word analogy task  we report the average cosine differene    cossim x y 
  between the analogy vector and
 
the embedding of the correct answer to the analogy 
the best results were achieved when  each column
of the embedding vector had a     chance to be
dropped during training  the results are in figure   
  http   mattmahoney net dc text  zip

fitest loss
test loss
test loss

gd
    
rms
    
maxa
    

mom
    
adam
    
a max
    

nag
    
nadam
    
n max
    

test loss
test loss
test loss

gd
     
rms
     
maxa
     

mom
     
adam
     
a max
     

nag
     
nadam
     
n max
     

figure    training of word vec word embeddings

figure    training of handwritten digit recognition

the methods with rmsprop produced word vectors that represented relationships between words
significantly better than the other methods  but rmsprop with nesterov momentum  nadam  clearly
outperformed rmsprop with no momentum and
with classical momentum  adam   notice also that
the algorithms with nag consistently outperform
the algorithms witn classical momentum 

the global learning rate   and in other training runs
nadam performed noticeably better with a higher
value of  

   

image recognition

mnist     is a classic benchmark for testing algorithms  we train a cnn with two convolutional layers of      filters of depth    and depth    alternating with      max pooling layers with a stride of
   followed by a fully connected layer of     nodes
and a softmax layer on top of thatthis is the default
model in tensorflows basic mnist cnn model 
the annealing schedule was modified to decay faster
and the model was trained for more epochs in order
to smooth out the error graphs  the results are given
in figure   
in this benchmark  nadam receives the lowest development erroras predictedbut on the test set  all
momentum based methods underperform their simple counterparts without momentum  however     
use a larger cnn and find that adam achieves the
lowest performance  furthermore  the hyperparameters here have not been tuned for this task  except

   

lstm language model

the final benchmark we present is the task of predicting the next word of a sentence given the previous words in a sentence  the dataset used here
comes from the penn treebank      and the model
uses a two layer stacked lstm with     cells at
each layer  reproducing      on a smaller scale  all
default settings in the small tensorflow implementation were retained  with the exception that the
model included l  regularization in order to offset
the lack of dropout  the results of training are presented in figure   
in this benchmark  algorithms that incorporated
rmsprop or maxaprop did noticeably worse than
gradient descent and those that only use momentum  and tuning hyperparameters makes little difference  its unclear why this isits conceivable that
the assumptions rms maxaprop make about how
features are represented dont hold in the case of
language models  lstms  or recurrent nns more
broadly  its also curious that classical momentum
outperformed nesterov momentum  although that
may be due to the relatively small size of the models  crucially  even though nadam wasnt the op 

fitest perp
test perp
test perp

gd
     
rms
     
maxa
     

mom
    
adam
     
a max
     

nag
    
nadam
     
n max
     

figure    training of a penn treebank language model

timal algorithm in this case  it significantly outperformed adam  supporting our hypothesis that using
nesterov momentum will improve adam 

 

discussion

while the results are a little messy  we can infer a number of things  the first is that the hitherto untested maxaprop based methods  including
adamax and nadamax  generally do worse than at
least some of the other algorithms  in the word vec
task  the algorithms with nesterov momentum consistently achieve better results than the ones with
only classical momentum  supporting the intuition
behind the hypothesis that nadam should be better
than adam  we also see that nadam produces by
far the best word vectors for this task when dropout
is included  which is not normally done but in our
experiments categorically improves results   with
the mnist task  we find that including rmsprop
makes a significant difference in the quality of the
final classifier  likely owing to rmsprops ability to
adapt to different depths  nadam performed best on
the development set  but contra      rmsprop surpassed it on the test set  this may be due to the
smaller size of our model  the relative easiness of the
task  cifar    might make a better benchmark in
a less time sensitive setting   our choice to only use

default settings  random fluctuations that arise from
dropout  or possibly even slight overfitting  which
can be clearly seen in nadambut not in rmsprop
when hyperparameters are tuned   finally  in the
lstm lm  we find that using rmsprop hinders
performance  even when hyperparameters are extensively tuned  but when it is included anyway  adding
nesterov momentum to it as in nadam improves it
over not using any momentum or only using classical momentum 
in two of the three benchmarks  we found a rather
significant difference between the performance of
models trained with adam and that of those trained
with nadamwhy might this be  when the denominator of rmsprop is small  the effective step
size of a parameter update becomes very large  and
when the denominator is large  the model requires
a lot of work to change what its learnedso ensuring the quality of the step direction and reducing as
much noise as possible becomes critical  we can
understand classical momentum as being a version
of nesterov momentum that applies an old  outdated
momentum vector that only uses past gradients to
the parameter updates  rather than the most recent 
up to date momentum vector computed using the
current gradient as well  it logically follows then
that nesterov momentum would produce a gradient
update superior to classical momentum  which rmsprop can then take full advantage of 

 

conclusion

adam essentially combines two algorithms known
to work well for different reasonsmomentum 
which points the model in a better direction  and
rmsprop  which adapts how far the model goes in
that direction on a per parameter basis  however 
nesterov momentum is theoretically and often empirically superior to momentumso it makes sense
to see how this affects the result of combining the
two approaches  we compared adam and nadam
 adam with nesterov momentum   in addition to a
slew of related algorithms  and find the in most cases
the improvement of nadam over adam is fairly dramatic  while nadam wasnt always the best algorithm to chose  it seems like if one is going to use
momentum with rmsprop to train their model  they
may as well use nesterov momentum 

fireferences
    martn abadi  ashish agarwal  paul barham 
eugene brevdo  zhifeng chen  craig citro 
greg s  corrado  andy davis  jeffrey dean 
matthieu devin  sanjay ghemawat  ian
goodfellow  andrew harp  geoffrey irving 
michael isard  yangqing jia  rafal jozefowicz  lukasz kaiser  manjunath kudlur  josh
levenberg  dan mane  rajat monga  sherry
moore  derek murray  chris olah  mike
schuster  jonathon shlens  benoit steiner  ilya
sutskever  kunal talwar  paul tucker  vincent vanhoucke  vijay vasudevan  fernanda
viegas  oriol vinyals  pete warden  martin
wattenberg  martin wicke  yuan yu  and xiaoqiang zheng  tensorflow  large scale
machine learning on heterogeneous systems 
      software available from tensorflow org 
    john duchi  elad hazan  and yoram singer 
adaptive subgradient methods for online learning and stochastic optimization  the journal
of machine learning research              
     
    xavier glorot  antoine bordes  and yoshua
bengio  deep sparse rectifier neural networks 
in international conference on artificial intelligence and statistics  pages              
    sepp hochreiter and jurgen schmidhuber 
long short term memory  neural computation 
                    
    diederik kingma and jimmy ba  adam  a
method for stochastic optimization  arxiv
preprint arxiv                 
    quoc v  le  navdeep jaitly  and geoffrey e 
hinton  a simple way to initialize recurrent networks of rectified linear units  corr 
abs                  
    yann lecun  corinna cortes  and christopher jc burges  the mnist database of handwritten digits       
    mitchell p marcus  mary ann marcinkiewicz 
and beatrice santorini  building a large annotated corpus of english  the penn treebank  computational linguistics           
          

    james martens  deep learning via hessian free
optimization  in proceedings of the   th international conference on machine learning
 icml      pages              
     tomas mikolov  ilya sutskever  kai chen 
greg s corrado  and jeff dean  distributed
representations of words and phrases and their
compositionality  in advances in neural information processing systems  pages          
     
     yurii nesterov  a method of solving a convex
programming problem with convergence rate
o   k     in soviet mathematics doklady  volume     pages              
     boris teodorovich polyak  some methods of
speeding up the convergence of iteration methods  ussr computational mathematics and
mathematical physics                 
     richard socher  cliff c lin  andrew y ng 
and chris manning  parsing natural scenes
and natural language with recursive neural networks  in proceedings of the   th international conference on machine learning  icml     pages              
     ilya sutskever  james martens  george dahl 
and geoffrey hinton  on the importance of initialization and momentum in deep learning  in
proceedings of the   th international conference on machine learning  icml      pages
               
     sachin s talathi and aniket vartak  improving performance of recurrent neural network with relu nonlinearity  arxiv preprint
arxiv                  
     tijmen tieleman and geoffrey hinton  lecture     rmsprop  divide the gradient by a running average of its recent magnitude  coursera  neural networks for machine learning 
        
     wojciech zaremba  ilya sutskever  and oriol
vinyals  recurrent neural network regularization  corr  abs                 

fi