spectral transition based playlist prediction
nipun agarwala  chris billovits  rahul prabala
 nipuna   cjbillov  rprabala   stanford edu
december         

abstract

to reduce playlist perplexity  which becomes an issue because of data sparsity 

since the advent of the radio  and in the
last decade  pandora and apple music  curated playlists have become a crucial player
in the music industry  current techniques
for playlist recommendation rely on supervised learning or collaborative filtering on
metadata  though such techniques may
work well sometimes  they do not capture
the musical semantics which attracts listeners to one song or makes them desire
a certain song after the current one plays
in a playlist  the paper proposes a transition model on spectral data to learn musical tastes and patterns of playlists  we
achieve      f  using a small shallow neural
network on supervised transition prediction
task  formulated below 

 

   

motivation

the end goal of automated playlist generation can
be derived directly from a binary transition based
classifier  given n songs  the objective would be to
craft a k length playlist that maximizes the joint
probability under markov assumptions  this is
a linear chain graphical model  upon which inference is tractable  the local probability distributions p si  si     can then be derived from the label
confidence of a binary transition classifier on the
window  si     si   
we present this binary transition based classifier
and analyze its direct performance  and briefly assess its suitability for the end goal task 

 

background

data collection

in order to draw deep  high order connections on
the relative ordering of songs within a playlist  we
begin by selecting curated playlists employed by
djs at edm music festivals in order to ensure
that playlists which emphasize good transitions
between songs  we follow the general formulation of     in developing a semi supervised binary
classification algorithm  treating consecutive songs
as positive examples  to generate benign negative examples  we crawl soundcloud and took    
popular edm genre music streams and select random transitions between these songs to be negative  with the assumption that random pairs of
songs will not possess strong transitions  this allows simple binary classification without introducing a very large bias  we generated     examples 
of which     were positive support  and stratified
them into a         train   test set split 

playlist generation is motivated by several dierent lines of thought  traditional radio stations
seek to play popular songs  other djs may select playlists in order to match prior interests with
new  unheard artists      much of the real world
implementation of playlist recommendation systems are reliant on relevance feedback to mediate between this priorities  we seek to classify
playlists which were built on transition based criteria     which is a generally unexplored approach 
the majority of playlist recommendation
schemes rely entirely on metadata  rather than audio  while some approaches have included temporal structure of songs  tempo  and other user based
constraints that may be imposed on a generated
playlist      there have been relatively few investigations into factoring the spectral component into
playlist generation  those approaches that rely on
spectral content more often than not use it solely
as a similarity metric      chen et al      have examined metric embeddings as a smoothing model
 

fi 
   

feature selection

the change in mfccs over the time scale given by
each frame  this enhanced set of features allows
for greater performance in many speech and music recognition tasks      for a given frame t  the
delta coefficient dt is given by

fast fourier transforms  fft 

the unsupervised methodology proposed to derive
musical semantics of the inputs involves the fast
fourier transform fft   computed using the discrete time fourier transform by
x     

 
x

x n e

dt  

   

ct i  

 

n
p

i 

i  

where ct is the mfcc at frame t  the data and
feature selection comprises a live pipeline that will
be used to continually refine negative example generation 

each music input of total time t seconds is divided
into t strips  where  is the configurable length of
each musical strip  an fft is performed for each
strip  generating frequencies based on  and the
sample rate of the audio file  these frequencies are
then binned according to the frequencies of notes
for a standard piano tuning a       hz  the separation in frequencies between the notes is divided
equally and assigned to each note  respectively  so
that the value of the fft for a given frequency
will be assigned to the bin with the least dierence  the values  in hz  for the bins are given
by
n

i ct i

i  

i n

n   

f  n          

n
p

 

models

we implement   primary binary classifiers from
scratch using numpy and scipy  benchmarking
them against scikit learn when applicable 

   

logistic regression

a naive first algorithm used was logistic regression  where   was assigned for bad transitions of
songs and   for good transitions  it runs the following optimization objective 

  
  

min j     ky

mel frequency cephstral
coefficients  mfcc 

h  x k    kk 

where h  x  is the sigmoid function  since the
model is trained on more adverserial than positive
examples  the model is hopefully selective in its
choosing of good transitions  as seen in the results 
since it can be noticed that the fft amplitudes
are generally very small  of the order         l  
regularization was added to keep the coefficients
small  this addition also prevented overfitting of
the model on the training set  considering that
there were much more features  either      or even
         than training examples 

an additional featurization was selected using the
mel frequency cephstral coefficients  mfccs 
and their delta coefficients  the mfccs more
closely approximate the way that humans perceive
audio by emphasizing the lower frequencies in the
audible range      rather than simply capture the
frequency component at a given time  these coefficients capture a scaled version of the energy
present in each filterbank  which is a binned collection of frequencies  the filterbanks are calculated
by the mel scale  which is given by

   

m  f          ln     f      

hinge loss regression

a more complex model used the hinge loss to allow for stronger classification than logistic regression through a max margin classifier  the optimization problem is 

the mfccs are computed in a similar fashion to
the ffts  in that the sample is first split into
frames  upon which the mel filterbanks are applied
by using a triangular filter centered at each filterbank value  the filtered energies are then logscaled  and the coefficients are extracted from the
log of the discrete cosine transform of the logscaled filtered energies 
additionally  the delta coefficients can be computed on a given set of mfccs that characterize

min j     max     

y  t x    kk 

where t x is our predictions  this loss function linearly increases with the absolute continuous value of t x  another interesting note about
this loss is the max operator  if y and t x are of
the same sign  then the max operator chooses   as
 

fithe loss  showing that the prediction is correct and
no loss needs to be added to the objective  like
above  l   normalization is added to keep the 
values in check and prevent over fitting due to a
large feature size 

   

well because it has tight regret bounds  individual features which suer high gradient losses become more cautious  but rare informative features
are not penalized  hence  adagrad resists overtraining  especially on small datasets  by annealing
the learning rate for common features  this makes
it more robust to hyperparameters and achieve
better convergence 
adagrad did not perform markedly better with
the max margin classifier  we hypothesize this is
because the gradient is linear  so over confidence
in the hypothesis function does not as readily induce large gradient errors 
l bfgs   for logistic regression and the neural networks  we use the quasi newton method
l bfgs  which maintains a linear gradient history to approximate the inverse hessian  and line
search in order to approximate the function minimum  because it uses storage space linear in the
size of   it is tractable to use on nontrivial feature
sets where the inverse hessian is computationally
intractable 

neural networks

we used shallow feedforward neural networks with
small hidden layer sizes and tanh activation  the
intuition was that shallow neural networks are sufficient to model arbitrarily complex functions  in
eect  the neural network trains a tan h    affine
kernel from the full input vectors to h features 
under the top layer objective  we used a least
squares shallow neural network with the objective
function
  x   i 
y
  i  
m

j w  u    

 

 

u   f  w  x i   

 

   w         u      

 

the activation function is f   tanh  in our
notation here  we use the bias trick  so u has dimension    h     and w has dimension h     
numf eat      note that y  i  is a one hot label
vector 
secondly  we implemented a sigmoid top layer 
with which to get confidences on our classification
decision 

j w  u    

m
x
i  

 

 

y  i t  log

 

u  f  w  x i   

the initial feature vector size was on the order
of         features  since getting datasets was
hard and ultimately only     song transitions were
used  there was a tremendous need for regularization  on manual inspection of the weights on the
features  it was observed that the weights on the
features on either size of the middle      features
were considerably smaller  almost negligible  as
compared to the middle       this indicated that
most of the information was encoded in the middle      features  which corresponded to roughly
  second in the transition  the rest of the features
seemed to be highly correlated  thus  the feature
set was reduced to prevent over fitting and allow
regularization to be more eective 



   w         u      

we used the latter network with sigmoid top
layer to achieve our best results 

   

feature reduction

 

optimization

   

we utilized two optimization techniques not mentioned in class  so we summarize them here 
adagrad   for all three classifiers  we optimize the objective function with stochastic gradient descent and annealed learning rates  for logistic regression and neural networks  we use the
diagonal adagrad method      our adagrad implementation provides a per feature learning rate
that monotonically decreases according to the l 
norm of the gradient history  adagrad works

results
fft   quantitative

our results can be summarized in figures   and   
the metric being used is the positive f  scores 
which gives a method of measuring the models accuracy  it takes into account the precision of the
model  i e  the number of transitions correctly
predicted out of the total number of songs predicted  and the recall  the number of transitions
relevant that were correctly fetched out of all pos 

 

fistill does better than our grid searched learning
rates  but adagrad is too conservative in its later
iterations in the size of its updates compared to lbfgs  a momentum based subgradient method
might not have this limitation 

   

fft   qualitative
wave embeddings of artist stratified
playlist transitions

figure    results table using f  scores
sible correct transitions  
as seen from the table  the reduced feature
model for logistic regression gave the the least
performance of all possible models  it is obvious
that it gave less performance than the full model 
which overfit due to the extremely large feature
vector  hinge loss  as expected  does better due
to the max margin nature of the classifier  the
reduced model again has lower performance  for
both these cases  adagrad allows for an appropriate learning rate to maximize performance  though
not preventing overfitting as much 
the particularly interesting models were the
neural networks  the results for neural networks
for dierent hidden layers and iterations can be
seen in figure   

figure    pca   tsne projection of unweighted linear fft transition vectors      x   

figure      d projection of feature vectors using
pca   tsne
figure   shows an embedding of the fft features into r  for positive transitions of a few
artists  when we added the artist name as a label 
we discover that waveforms for song transitions are
very close for tiesto and knifeparty  but a minority of djs  nicky romero  have wide spanning
embeddings  meaning that the transition methodologies are not consistent between artists  despite
this  we seem to predict each artists transitions
fingerprint precisely 
listening to the mispredicted transitions  it appears that the transitions from progressive high
frequencies to a series of low frequencies  i e  a
drop  are the main sources of error  this is
likely due to many curated playlists incorporating several of these types of transitions into their
setlists  which in turn complicates the classification of these transitions  the characteristics of
mispredicted transitions were entirely composed
of second songs that contained a strong percussive
element within the first few seconds 
despite the great results and relatively few predictions  the p values indicate a lot of relatively insignificant features  the null hypothesis was that
the features are insignificant  accordingly  there
were     significant features  as measured by pvalues        they may be statistically insignificant  but that does not imply that there is no
predictive eect of those features  for the remain 

figure    neural net results using f  scores
as seen from the figure     we have that the
l bfgs nn with    hidden layers gives the best
test and train error  in that order      iterations 
in combination with adagrad  was seen to give
the sweet spot in training  adagrad additonally
prevented overfitting the nn by choosing appropriate learning rates  we can also observe that as
the number of hidden layers increases from    to
    the f  score growth decrease and converges 
we take the score having the larger train f  score
in case the test f  scores are the same 
we hypothesize that l bfgs does better in
training the same network architecture because
   the dataset is small  so stochastic methods do
not oer a large speedup  and    we run l bfgs
with a narrower convergence criteria  adagrad
 

fithat there are certain frequency bins in every sample that are heavily predictive  other features in
conjuncture with these frequency bins can be used
to make better predictive models 

references
    g  bonnin and d  jannach  automated generation of music playlists  survey and experiments  acm comput  surv   vol     
pp             nov       

figure    feature significance using p values
ing  there is only a mild predictive eect  thus 
our hypothesis of using only the middle      features was justified due to the highest significance
for those values 

   

    j  j  aucouturier and f  pachet  scaling
up music playlist generation  in multimedia
and expo        icme     proceedings      
ieee international conference on  vol    
pp         vol         

mfcc   quantitative

    a  flexer  d  schnitzer  m  gasser  and
g  widmer  playlist generation using start
and end songs   in ismir  j  p  bello 
e  chew  and d  turnbull  eds    pp         
     
    s  chen  j  l  moore  d  turnbull  and
t  joachims  playlist prediction via metric
embedding  in proceedings of the   th acm
sigkdd international conference on knowledge discovery and data mining  kdd    
 new york  ny  usa   pp          acm 
     

figure    mfcc transition features results using f  scores
the mfcc feature transitions did not give results even close to those from the fft  the best
result came from the neural network with lbfgs with    hidden layers  which had a score of
      despite supposedly being a better distance
metric for music in general  it failed to be a better
predictor than the fft for good playlists transitions  to get an idea of the feature significance 
p values were tried to be found for the mfcc features  but their varied nature on either side of the
y axis  mfccs can be negative  made it impossible to use the   distribution for p values  nevertheless  it can be deduced that fewer features were
important and the significance of those features
was lower than those of the ffts 

 

    f  maillet  d  eck  g  desjardins  and
p  lamere  steerable playlist generation by
learning song similarity from radio station
playlists  in proceedings of the   th international conference on music information retrieval       
    s  b  davis and p  mermelstein  comparison of parametric representations for monosyllabic word recognition in continuously spoken
sentences  acoustics  speech and signal processing  ieee transactions on  vol      no    
pp               
    j  duchi  e  hazan  and y  singer  adaptive subgradient methods for online learning and stochastic optimization  tech  rep 
ucb eecs          eecs department  university of california  berkeley  mar      

conclusion

the models and results presented in this paper
prove that transition based features are strongly
predictive of curated music playlists and hence can
be used for the same  at least for edm music 
it is also shown that transition based mfcc features are not as good as their fft counterparts for
strong predictive schemes  finally  based on the
p value statistics of the features  it can be deduced
 

fi