melody extraction from generic audio clips
thaminda edirisooriya  hansohl kim  connie zeng

introduction

amplitudes  and that of tachibana et al      
which separates out sustained harmonies by using varying time windows  and then separates
out aperiodic percussion by using varying frequency windows  these methods are more mathematically straightforward but rely on simplified
assumptions 
our work is similar to that of salamons  but
instead of manually specifying the salience function  we want to see if an rnn can learn to
identify the notes from an audio sample  there
are many interactions between sound frequencies
that may be hard to model mathematically  so a
neural network might be able to perform better
on this task  johnson     has done related work 
using a bidirectional rnn to compose music 

in this project we were interested in extracting
the melody from generic audio files  due to the
context dependent nature of music  particularly
the melodic voice  we decided to use a recurrent
neural network  rnn   which keeps a memory of
previous inputs  the input layer to the network
captures a time step of a song  containing the distribution of frequencies  the main chord  and the
single loudest frequency in that time step  which
are all preprocessed from raw audio  the output
layer is a softmax classification of the note predicted to be the melody  we implemented this
in python  with several modules from numpy    
and scipy     

related work
dataset

one of the most accurate methods to date  from
salamon  et al       computes the harmonic
sum of each pitch as a salience function to estimate its likelihood in an audio signal  the
most salient pitches are then grouped into continuous contours  and the contours are filtered
based on characteristics such as average pitch
and pitch deviation  this is well suited for vocal
melodies  but performs poorly on instrumental
music  which has more overlapping voices and
sudden changes 
bosch and gomez     created a variation of
this method  designed to improve performance
on an orchestral dataset  instead of using harmonic summing  the audio signal is modeled as a
lead plus accompaniment  with the lead further
approximated as a source filter model  the pitch
salience is then calculated using maximum likelihood  as expected  this method yielded better
results on orchestral music  while still maintaining moderate accuracy on vocal music 
other approaches include that of arora and
behera      which finds harmonic clusters and
thresholds them based on their summed squared

our primary dataset is a collection of bach
chorale harmonizations  from jsbchorales net
     these chorales have harmonic patterns that
are still widely used and can help make inferences
about the melody  they are in midi form  with
one track per piece containing the melody  we
converted each raw midi file into a wav file as a
sum of sawtooth waves  and also created a wav
file containing the isolated melody as simple sine
waves 
we supplemented our dataset with clips used
in the annual music information retrieval evaluation exchange  mirex  melody extraction
task      they are drawn from a variety of genres 
including pop  jazz  and classical  these already
come as wav audio files  along with a reference
file containing the annotated melody 
since melody extraction is a complex task  we
started by focusing on the chorale melodies  we
used a training set of    chorales  from which
    was randomly selected for the validation set 
and a test set of    chorales  then to start learning more general melodies  we added    mirex
 

ficlips to the training set and   mirex clips to
the test set 

feature extraction
we took the fast fourier transform  fft  of the
full audio wav files to obtain a matrix of all constituent frequencies over      second time steps 
for the chorales  we similarly took the transform
of the melody wav files to identify the main frequency of the melody at each time step  for the
mirex clips  we simply read the frequency values of the melody from the reference file  as
a simplification  we only considered frequencies
from        hz  based on the typical range of
music 
the full audio fft was fed to a chordrecognition svm to identify the main chord per
time step  each input vector to the neural network then consisted of the fft       features  
the predicted chord from the svm     features  
and the frequency with the highest amplitude   
feature  for a given time step  all time steps of
a song were combined into a single matrix and
stored with the target melody frequencies in a
csv file 

to generate data for the svm  we created
chords by summing sawtooth waves and taking
the fft  for each base note and tonality  we
considered the three notes that make up the corresponding chord and  at each octave  included
each note with a random chance  we used a
span of   octaves  from   below middle c to  
above middle c  as a reasonable range for chords
in actual music  this process essentially creates
a random sampling of all the permutations and
multiplicities of a chord 
kernel
rbf
linear
polynomial

accuracy
   
   
   

table    comparison of kernels for the chordrecognition svm 

we used a training set of     samples per
chord and a validation set of    samples per
as part of our feature extraction  we created an chord to select the kernel for the svm  as shown
svm to predict the most likely chord in an audio in table    the linear kernel resulted in the best
sample  the svm we used operates by solving classification  we further tested it on a set of
more complete chords  containing at least one
the optimization problem 
instance of the base note and the middle note 
the svm achieved     accuracy on a set of   
samples per chord  finally  we tested a small
sample of bach chorales  with     accuracy 
chord recognition svm

percussion processing

which is can be expressed as the dual optimization problem 
implemented using the scikit learn svc class
      the svm takes an fft as input  and outputs an indicator vector with one of    chords
chosen  the    possible chords come from   
half step base notes and   tonalities  major or
minor 

as part of the preprocessing on pop audio samples in particular  we tried to remove the spikes
in frequency that occur when loud percussion
such as a kick or snare drum occurs  a kick
or snare often has high power levels in the frequency ranges also shared by vocal and other
instruments  and can thus obscure the melody
 

fiinformation that we want to extract  this is
a problem because the chorales did not contain
percussion  and thus our algorithm did not generalize too well to percussion heavy music 
in an attempt to correct this issue  we created
a model of the fft of a kick drum and a snare
drum by taking the means of the ffts of    
different kick and snare samples respectively  we
used this to detect percussion in audio samples
by taking the fft of      second time steps of
the audio  and computing the dot product with
the kick or snare model as a similarity measure 
the higher the value  the more likely it was to
contain a kick or snare sample  we then chose
the median of every   time slices  in terms of
similarity  as a representative of the entire     second time step  to avoid spikes due to loud
percussion 
however  when we tested this method  we
found no improvement on our accuracy  we noticed that the heuristic we used to remove percussion also removed important melody information  future work could seek to refine this process as a means of improving the generalization
of the algorithm as a whole 

figure    rnn structure

that control when an input will be remembered 
when an input should be forgotten  and when
the memorized value should be output  the
frequency information in music can fluctuate
wildly  so these lstm recurrent modules provide
the potential to judge when a change in inputs
should be ignored and when a change is significant enough to warrant changing the note of the
predicted melody 
recurrent neural network
the    output nodes represent    possible outthe      features from preprocessing are input put notes  and they use the softmax equation 
into our rnn  implemented with pybrain      
with a    node hidden layer and an output layer
exp wt xj  
softmax x    p
of    classification nodes  see figure     the
exp wt xk  
k
hidden layer is split into two sections based on
connectivity  the   node octave layer only this is a multi class generalization of the sigconnects to the      frequency inputs  and each moid classifier  and all nodes sum to    repreoctave node only outputs to    of the    out  senting the estimated probabilities of each class 
put notes  corresponding to an octave  the     the maximum value output node is taken as the
node note layer is fully connected to the in  output note 
put and output  when activated  the output
the neural network is trained with resilient
node with the maximum value is taken as the backpropagation  which is variant of the stanpredicted melody note 
dard backpropagation algorithm  the weights
hidden layer nodes are implemented as long are updated by multiplying by one of two conshort term memory  lstm  recurrent modules  stants decided by the relative signs of the gradithese miniature networks can store input val  ent in the current and previous iterations  while
ues for near arbitrary lengths of time  lstm the magnitude of the gradient is ignored  rearchitectures vary  but all include gate nodes silient backpropagation was chosen for its speed
 

fiand its ability to store individual learning rates on many of the chorales  but also fails to grasp
that can adapt for each weight 
certain anomalous patterns 
note that the classification error rate alone
does
not take into account similarities between
results
certain notes  the note c is more different from
we trained the neural network for     iterations  a d than it is from a c one octave above  examusing a validation set to estimate convergence  ining a sample predicted melody  we note that
the training curve  in figure    shows that con  the prediction is generally accurate  including
vergence was reached near     iterations 
the absence of melody at the start  but a closer
look at some of the errors reveals a pattern to
some of the missed predictions 

figure    training curve

figure    predicted chorale melody
here  the note    in the melody is often  and in
a regular repeating pattern  wrongly classified as
note     this is in fact the same note  transposed
one octave up  this provides further indication
that our trained network is learning harmonic
patterns  examination of the wrongly classified
notes often reveals a shift of       or   half steps 
which correspond to the minor and major third
as well as the dominant fifth  all of which play
important roles in established harmonic rules 
the results up to this point have been from
testing on chorales  one of our motivations was
to use the chorales and harmonic training as a
base for later extension to other genres and styles
of music  while we do not expect the classification to perform as well on other styles of music 
we are interested in testing how useful the harmonic patterns in chorales are for melody extrac 

figure    error rate histogram
the average classification error on the test set
was      this relatively low error rate likely
demonstrates that the neural network has found
and taken advantage of patterns in the frequency
and chord data to aid in predicting the melodic
pitch  looking beyond the average of the error rate  we note that     of the test chorales
are clustered below     classification error  with
four outliers accounting for much of the error 
this indicates that the trained net is accurate
 

fition  below is a predicted melody for a mirex generalizing the system to a wider range of mutrack  from an rnn trained on the dataset con  sic  as seen with the test on mirex samples 
the harmony based system can be effective betaining    mirex clips 
yond the chorales  improvements to the percussion processing system or more powerful chord
classifiers that account for chord beyond simple
majors and minors are possible future works that
could extend the effectiveness of this system to
a more general body of music 

references
    stefan van der walt  s  chris colbert and
gal varoquaux  the numpy array  a
structure for efficient numerical computation  computing in science   engineering
                
figure    predicted mirex melody

    jones e  oliphant e  peterson p  et al  scipy 
open source scientific tools for python 
from the results we see that the neural net       http   www scipy org   online  acwork is capable of generalizing some of its precessed             
dictive power to music other than chorales  the
predictions are not only able to predict the pre      salamon  justin  et al  melody extraction
dominant tones in the melody  but also capture
from polyphonic music signals  approaches 
several nuances in the melody  this is an encourapplications  and challenges  signal processaging result in terms of future work  and may be
ing magazine  ieee                      
coupled with other methods such as a more de    bosch  j   and emilia gmez  melody exveloped percussion processor in the future 
traction by means of a source filter model
and pitch contour characterization  mirex
conclusions and future work
       music inform  retrieval evaluation
from our results  we conclude that the use of
exchange  mirex         
harmonic data in addition to raw frequency data 
fed through a recurrent neural network  can be     arora  vipul  and laxmidhar behera  oneffective at predicting the melody of harmonyline melody extraction  mirex       exrich pieces  the bach chorales represent a relatended abstract submission to the music
tively clean audio track  without the added cominformation retrieval evaluation exchange
plication of atonal elements such as percussion 
 mirex         
in addition  the bach chorales adhere to established harmonic rules strongly  but this is also     tachibana  hideyuki  et al  melody line
estimation in homophonic music audio sigtrue of much of modern popular music 
nals based on temporal variability of melodic
the harmonic patterns learned and recognized
source  acoustics speech and signal processby our neural network  and the substantial preing  icassp        ieee international conferprocessing system used to extract valuable feaence on  ieee       
tures  represent a foundation for future work in
 

fi    composing music with recurrent neural
networks  hexahedria     aug         online  accessed            
    https   web archive org web          
http   www jsbchorales net sets shtml
 online  accessed            
    http   labrosa ee columbia edu projects melody 
 online  accessed            
     pedregosa et al   scikit learn  machine
learning in python  jmlr                     
     schaul et al   pybrain  jmlr           
        

 

fi