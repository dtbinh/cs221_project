on the relation between solar flares and corona
mass ejections  from a machine learning view

ruizhu chen
department of physics
stanford university
rzchen stanford edu

xin zheng
department of electrical engineering
stanford university
xzheng  stanford edu

abstract
we apply machine learning techniques to predict whether a solar flare is associated
with any corona mass ejections  cmes  given the features of solar flares  including
direct observables and derived quantities using satellite data from sdo and goes 
nave bayes  logistic regression and support vector machine are explored  with
positive cases upweight  feature selection by correlation and feature dimension
degeneration by pca  promising results are achieved with generalization error  
    and k score        on a small training set of     data points with class label
confirmed  efforts are also made on a    times larger train set with less confident
class labels 

 

i n trod u cti on   rel a ted w ork  

solar flares and corona mass ejections  cmes  are both energetic events with eruptions  solar flares
are explosions in solar atmosphere  while cmes are eruptions that blow high energy particles from
the sun to the space  which have significant impact to human activities when faced towards earth 
some of the solar flares and cmes are associated with each other  especially the strong ones 
although the mechanisms of both events and their relation have been heavily studied during the past
decades  kahler       review      the relation between solar flares and cmes is still unclear 
most of the previous works are case by case studies on the physics process  zhang et al  
     etc       it is indicated that flares and cmes may be two type of phenomena due to the same
process  recently  two works draw large samples of flare cme pairs and study the statistical relation
on one or two characteristic properties  yashiro           youssef            it is found that flares
with larger flux or longer duration tend to have associated cmes 
in this project  we try to model the flare cme association from machine learning view  we use
logistic regression  nave bayes  and support vector machines  svm   with input of properties of
flares to predict whether the flare is cme associated 
there are not many studies using machine learning techniques in this field  on the same topic 
qahwaji          used cascade correlation neural networks  ccnn  and svm with four features
of flare  intensity  flare duration  and duration of decline and duration of growth to predict
association with cme  the training class labels are determined automatically by event detection
times  we think we can improve in two aspects  first  we use case by case confirmed association of
flare cme pairs as class label  at the cost of a smaller data size  the automatic determination of
association by detection time turns out to be very noisy  in section   we build a better criteria using
both the location and event starting time  also counting propagation time before detected   and even
that is not robust enough compared to the case by case confirmed pairing  second  we use    features
including more physical quantities  from continues time full disk satellite data from sdo hmi and
sdo aia 
on similar topics  al omari           used svm to associate filaments and cmes  qahwaji         
used adaboost to predict cmes by filaments  qu          used multi layer perceptron  mlp   radial
 

fibasis function  rbf   svm to predict solar flares  bobra          used svm to predict solar flares 
of all these work  bobra       is the first to use data from sdo hmi  the first instrument to
continuously map the full disk photospheric vector magnetic field from space  we also include those
features  svm algorithm is used most in the existing literature  and many claim it to be working best 
in this work we will also use svm  together with logistic regression and nave bayes 

 

datas et an d featu res

   

d a t a g e n e r a t i o n a n d p re p ro c e s s

we used the solar flare events catalog from goes satellite and the corresponding cme associations
from solarham catalog if available  an example is shown in table    the class labels  yes  
and no     are confirmed by researchers by manually check in a case by case way  the features
are generated in the following way 


we used duration  end time  peak time  and peak flux as features directly 



info like time  ar region  the index number of the active region where the flare originated  
and location are not good features for the study  instead we used them as indexes to quote for
measurements and properties of the same events in sdo aia and sdo aia databases 



from sdo aia database we get filtered image  figure    of the sun at   different
wavelengths at the peak time of a flare  for simplicity we use the mean value of each image as
a feature 



from sdo hmi database we get    sharp  spaceweather hmi active region patch 
quantities or segments  figure    and use the mean value within the active region as a feature 

flare
class

date

start
time

end
time

peak
time

ar
region

location

cme

x   

      

     

     

     

    

 

yes

m   

      

    

    

    

    

n  e  

no

m   

      

     

     

     

    

s  w  

no

table    a example of solar flare catalog  goes   solarham 

figure   an example of sharp segments  left
panel shows area of active region  used as a mask
applying on middle panel  magnetic field  the
average of masked field  right panel  is used as a
feature
figure   an image at     from aia database
   

f e a t u re a n d d a t a s u m m a r y

below is a summary of feature and class 
column  

column  

duration

peak flux

column     
intensity of  
wavelengths

column      
   host active region field
parameters from sharp

table    summary of generated feature and class
 

class
y      
 as factor 

fiwe have a total of     data points      of which are positive class  the relatively small data size is
limited by the availability of flare cme associations confirmed      data points have all    features 
    of which are positive  others only have the first    features 
we cleaned outliers and scale each feature so that the histogram is close to normal or uniform  or
double bell distribution  additional preprocess methods are introduced in section   to improve
machine learning performance under different circumstances 

 

meth od s

   

metrics

for examination  we use generalization error  recall  precession and k score as the metrics  k score is
defined as
  
   



 

   

   

algorithm

          l o g i s t i c r e g re s s i o n   l r  
in lr  decision boundary      is found by maximizing the likelihood on training set  l    

                  where we assume a model with probability
               

 

  exp    

 

                

   

we use leave one out cross validation to achieve metrics values 
      nave bayes  nb 

nb is to predict class through comparing          and          where we assume the features
are conditionally independent so that
       
       

and

     

   
  

 

 
         

  
         

   

the performance of nb depends on the difference between posterior distribution given y   and y   
in our case  its hard to model some of the feature distributions using continuous models  so for
simplicity we then discretize each feature uniformly into the same number of bins  we choose the
number of bins as tuning parameter from   to      and select the optimal number of bins by
evaluating performance through leave one out cross validation 
      svm
support vector machine finds a separating hyperplane that maximizes the margin between the
separating hyperplane and training data  we apply both linear  and radial kernel svm with
regularization  using a series of regularization parameter c and search for the optimal value  figure
    the optimal c is decided by best generalization error and k score  estimated by k fold
cross validation  k     

figure   an example of finding optimal parameter through performance  train set is significantly
overfitting as c increases  the optimal c is chosen at     where error is smallest and k score is
largest 
 

fi   

upweight

to solve the unbalanced class problem  the ratio of positive class        we upweight the positive
class in training sets by a factor of   
      f e a t u re p ro c e s s


we first use the    direct observable features for training  this is of interest because we could
have real time prediction without the hassles of inversion problems to get other features 



the whole    features are then used for training 



feature selection  fs  methods are used to find the most correlated features  calculate the
correlation between y and each feature and eliminate the least correlated feature one by one
until the best performance 



pca is applied to decrease redundancy in feature dimension  the final feature size is around
   

    res u l ts
error

precision

recall

k score

error

logistic regression

precision

recall

k score

nave bayes

      

    

    

    

    

    

    

    

    

  uw

    

    

    

    

    

    

    

    

     

    

    

    

    

    

    

    

    

 uw

    

    

    

    

    

    

    

    

 uw   fs

    

    

    

    

    

    

    

    

 uw  fs
 pca

    

    

    

    

    

    

    

    

table    logistic regression and nave bayes main results        stand for data set with   
features and     samples  uw stands for upweight  fs stand for feature selection 

error

precision

recall

k score

error

svm linear 

precision

recall

k score

svm radial 

     

    

    

    

    

    

    

    

    

  uw

    

    

    

    

    

    

    

    

     

    

    

    

    

    

    

    

    

 uw

    

    

    

    

    

    

    

    

 uw   fs

    

    

    

    

    

    

    

    

 uw  fs
 pca

    

    

    

    

    

    

    

    

table    svm main results
table   and table   summarize the machine learning performance 
upweight improves performance significantly for all models  this is not surprising considering more
balanced class will give more balanced description of both class during training and thus give better
predictions 
feature selection works best for svm when removing   insignificant features  but it doesnt improve
performance for lr and nb  there is a tradeoff between removing unrelated information and
removing useful information  when we keep the features with large correlation to class labels  we
assume the features and classes are linearly related  however  real system based on physics law may
be non linear  and thus our feature selection may also remove useful information 
 

fipca improves performance for nb significantly  while doesnt work for lr and nb  pca removes
redundancy in feature space  for nave bayes  this works because pca forms orthogonal basis for
data and thus help to satisfy the conditional independence assumption  for the other methods  some
useful information may be lost after pca  thus cause slightly worse performance  however pca will
decrease the computation demand significantly 
best performance is achieved by nave bayes after upweight  feature selection and pca  the highest
k score is      
nn

    gen e rate a l arg e r d ata s et an d trai n  
the results on the small set are very promising  we then decide to try generating a larger set of a
typical machine learning size  there are far more solar flare and cme events recorded by goes and
lasco  but to determine the association of solar flare and cme pairs is hard  a robust but too
expensive way is by manually check the processes of events  like how its done for the small set 
instead  we try an empirical paring criterion that requires spatial and temporal coincidence  the same
method used in yashiro           youssef          
the method is not robust because it cannot correctly classify the given pairs in the     data points we
have  the limitation comes from     the start time of cme is derived by a simplified model using
cme detection time  average cme velocity  flare center location and geometry  which doesnt count
in flare size  shape  and cme velocity change     the universal location and time window used do
not work for all sizes of flare and cme  however this is the best we found in literature 
we still generate a set of      data points  the sampling of this large set is more general  including
weaker events  while the small sets are sampled unevenly favoring strong or interesting events  the
overall positive rate is about      and positive rate for flare m class and above  same as small set  is
about      comparable to the small set  the positive class is then upweighted by a factor of   
we used either    direct observables or    downsized features by feature selection and pca  using
all    features are not affordable but its proved in sector   that the feature downsizing only cost a
little bit performance to dramatically save the computation demand 
the optimal results shown below in table    the results are very poor compared to the small set 
results are limited by the robustness of train set class labels  we found the probability distribution of
features on both class  figure    are so similar that no feature can significantly distinguish the two
classes 

method

error

precision

recall

k score

      

    

    

    

    

pca  uw

    

    

    

    

table    summary of svm results using large data set 
figure    probability distributions of features on both class 

  co n cl u s i on and fu tu re wo rk
we applied algorithms of lr  nb and svm on a small data set of     data points and    features 
with upweight and feature manipulations  and got results with generalization error      and
k score      for all algorithms  our best result is achieved using nb  with generalization error      
and k score       
these results are promising given the small size of training data  while we also generate and try on a
   times larger set  the empirical class labels are not robust enough for a good study  however  most
existing work with large data sets used similar or simpler empirical criteria for pairing solar flares
and cmes  and here we learn that the true association should be more complicated than this apparent
temporal and special coincidence  when more confirmed pairs of flare and cmes are published and
accumulate to a considerable amount  the model will be improved 
 

fir e f e re n c e s
    kahler  s  w   solar flares and coronal mass ejections   annual review of astronomy and astrophysics   
                
    zhang  j   et al   on the temporal relationship between coronal mass ejections and flares   the
astrophysical journal                   
    yashiro  seiji  and nat gopalswamy   statistical relationship between solar flares and coronal mass
ejections   proceedings of the international astronomical union   s                    
    youssef  m   on the relation between the cmes and the solar flares   nriag journal of astronomy and
geophysics                     
    qahwaji  r   et al   automated prediction of cmes using machine learning of cmeflare
associations   solar physics                       
   al omari  m   et al   machine leaning based investigation of the associations between cmes and
filaments   solar physics                       
    qahwaji  r   et al   using the real  gentle and modest adaboost learning algorithms to investigate the
computerised associations between coronal mass ejections and filaments   communications  computers and
applications        mic cca       mosharaka international conference on  ieee       
   qu  ming  et al   automatic solar flare detection using mlp  rbf  and svm  solar physics              
        
   bobra  monica g   and sebastien couvidat   solar flare prediction using sdo hmi vector magnetic field
data with a machine learning algorithm  the astrophysical journal                   

 

fi