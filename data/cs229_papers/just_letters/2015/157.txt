machine learning   cs   

landslide susceptibility mapping in nepal using spatial feature vectors
markus zechner  muhammad almajid  kuy hun koh yoo

section    introduction
according to the usgs  u s  geological survey   the earthquake of april      in nepal and its aftershocks claimed
the life of almost       people  many of the deaths were caused by landslides which also blocked important routes
for emergency response and evacuations  these kinds of disasters are not uncommon in other parts of the world
and vastly underestimated as recent research shows  landslide prediction can be a valuable tool for planning first
response  where to send first response helpers if there is no communication after an earthquake   and emergency
evacuations 
unlike the us  countries like nepal do not have extensive infrastructure to measure and monitor seismic
occurrences that could help predict and locate disasters  for this project we try to identify high risk zones prone
to landslides based on features that have been studied by research institutions  vanwesten et al         
we use expert labeled gps locations  which occurred in nepal as a consequence of the earthquake  as our
training set  figure   shows a zoomed in image of some of nepals landslides taken from google earth 

figure    a close up image of   landslide locations  unnamed yellow pins  and one village  named yellow pin 

section    related work
landslide hazard assessment is an important step towards landslide hazard and risk management  several
methods of landslide hazard zonation  lhz  like heuristic  semi quantitative  quantitative  probabilistic and multicriteria decision making process are applied to predict landslides 
in last few years a change from a heuristic  knowledge based  approach to a data driven approach
 statistical approach  was observed with the goal of minimizing subjectivity and increasing reproducibility  the
statistical methods for lhz are grouped into two     bi variate statistical analysis and    multi variate statistical
analysis  the bi variate statistical analysis for lhz compares each data layer of features to the existing landslide
distribution  kanungo et al         multi variate statistical analysis for lhz considers relative contribution of each
thematic data layer to the total landslide susceptibility  kanungo et al         these methods calculate the
percentage of landslide area for each pixel and the landslide absence  logistic regression model  discriminant
analysis  multiple regression models  conditional analysis  artificial neural networks are commonly used
methods for lhz mapping  wang and sassa       ayalew and yamagishi       guzzetti et al        ercanglu      
catani et al        pradhan and lee       pradhan and lee       bui et al       
although current research is promising in assessing critical features for the prediction of landslides  we
believe there is room for improvement  since in most situations the data is highly unbalanced  it is important to
study machine learning model selection and to evaluate the prediction performance with relevant metrics 

section    dataset and features
the locations of the landslides triggered by the earthquake that occurred in april of      with a magnitude of    
after richter were obtained from the usgs  it is important to note that the reported locations are the only
 

fimachine learning   cs   
locations where people reported landslides  it is more than possible that there were other landslides that have
not been reported  it is therefore not surprising that most observations were made near populated areas 
elevation  aspect ratio  slope  wetness index  ndvi  vegetation index   ndwi  water index  and curvature
were used as features and were obtained from google earth engine  the selection of these features was based on
previous studies on landslide susceptibility based on spatial data  vanwesten et al          nevertheless  other
critical features that were considered relevant were not readily available for the area of interest such as drainage
networks and distance to main faults  figure   shows individual color maps for some features  the location of the
landslides are denoted by blue dots on each figure 

figure    post processed feature maps obtained from google earth engine

our main challenge was cleaning and preparing the data for our chosen algorithm  in a first step  we
mapped our landslide coordinates onto a   km grid we established  landslide grid   a grid cell that contains a
landslide was assigned a value of    conversely  a grid cell with value of   is assigned when no landslides are
reported  since all the features have different grid resolution  our main task in the second step was to cut the
feature map obtained from google earth engine so that it matched the area covered by our landslide grid  given
that the grid resolutions between the feature map and landslide grid are different  the last and final step was to
interpolate feature properties onto our landslide grid  nevertheless  such up down scaling of features did involve
some kind of spatial averaging which could eventually affect the descriptive strength of the feature itself  i e  high
peaks with steep slopes would appear to be average after feature post processing  
it is important to note that for our given dataset and landslide grid  the percentage of yes grid cells is
only       this will become important in the selection and implementation of our learning algorithm 
for all our algorithms we used cross validation to evaluate the performance of prediction  the training
data set was     from our data  the     left were used for testing 

section    methods
in this section we present a short description of the learning algorithms used in this project 

nave bayes  nb 
in the nb model  the features are assumed to be conditionally independent given the responses  this is a strong
assumption  given our training set                            we can fit our parameters                       
and  by maximizing the joint likelihood of the data  using the fitted parameters  we make a prediction on a new
example by picking the class which gives us the higher posterior probability 

logistic regression
logistic regression is a method used to predict probabilities for binary classification  the algorithm is defined as 
repeat until convergence  

  

  
      
          
    

 for every j 

 

where         is the sigmoid function  we predict one class if the probability is above a certain threshold
and the other class otherwise  the threshold can be changed based on how stringent we want to be on the data 
 

fimachine learning   cs   

support vector machine  svm 
we used the usual svm algorithm with regularization to take into account non separable cases  in svm  we are
essentially trying to find the weights that should be multiplied by our example  the primal problem that the
optimization algorithm solves is 


  
min       
    
  

                    
              

tree based methods  cart  random forest  boosting 
the classification and regression trees algorithm  cart  is based on splitting the feature space recursively into
binary partitions  in other words  they are piecewise constant models  in a node   representing a region  with
 observations  icme ml workshop        
      arg max 
k

  

 
      

 

random forest is a large collection of uncorrelated trees that are built and then averaged  the idea is to
reduce variance because the bias of averaged bagged trees is the same as that of an individual tree  hastie et al  
       theyre similar to cart in that they capture high order interactions between variables and handle mixed
predictors  random forests are better than cart in that theyre less prone to overfitting and because of using outof bag samples  cross validation is already built in 
finally  boosting fits additional predictors to residuals from initial predictions  icme ml workshop        
               
   

section    experiments results discussion
defining error metrics for imbalanced data sets
many performance metrics have been proposed for imbalanced data sets but all of them are based on the
confusion matrix  when data sets have classes with significantly different sample sizes  only      of our data
indicate landslides  the standard definition of accuracy is not applicable any more  for example a classifier which
predicts all grid cells with no landslide would have a very good accuracy but obviously a very bad prediction
performance 
a much better performance indicator are sensitivity and specificity which are often called true positive
 tp  rate and true negative  tn  rate  respectively 
furthermore  the area under the receiver operating characteristic curve  auc  is also a very indicative
measure for the prediction capabilities in an imbalanced data set  the receiver operating characteristic  roc 
plots the true positive rate versus the false positive rate with a varying classification threshold  by default it is
      the higher auc is the better the performance of the algorithm  a high slope in the beginning indicates a
very good prediction performance  in other words  by changing the threshold we can gain many more true
positives without scarifying false positives  when the curve bends over we still can increase the true positives
but also need to except more false positives 
we use the auc and the roc as the error metrics for our project as we believe they are the most intuitive
ones 
 

fimachine learning   cs   

learning algorithms results  base case
we began our project testing the simplest algorithms  nave bayes and logistic regression  with two features
 elevation and slope  to establish a baseline  given our imbalanced data set  we thought that nave bayes would
be a good match as it is known to converge quicker than discriminative models and it requires less training data
to converge  furthermore  the nave bayes conditional independence assumptions apply very well for our
problem statement since a landslide is unlikely to have an influence on its surroundings since the grid block size
is one square kilometer 
both  logistic regression and nave bayes have the advantage of a probabilistic interpretation  unlike
decision trees or svms  this probabilistic framework makes it easy to adjust the classification thresholds which
are crucial in imbalanced data sets  in the first trial with two features  nave bayes showed a good performance
whereas logistic regression performed rather poorly  figure    

figure    roc curves from implementation of logistic regression  left  and nave bayes  right 

surprisingly the svm algorithm did not give a good result  since svm  like logistic regression  is based on
a linear separator we expected the linear svm to have a similar performance as nave bayes  but the results did
not support our initial hypothesis  after testing with the linear separator we decided to apply the kernel trick to
allow for non linear separators  we tested the polynomial  gaussian and the exponential kernel on our training
test but without success  the area under the curve  auc  stayed constant at      
furthermore  we tested for the regularization parameter   i e  a penalty parameter of the error term  in
the range of      to      the upper bound showed a very small        improvement with respect to auc  since
our dataset is highly imbalanced  we assigned weights inversely proportional to class frequencies in the input data 
nevertheless  this tuning did not improve our results with respect to auc  we would have liked to perform further
testing on the hyper parameters of the svm but due to enormous runtimes we could only test for lower and upper
bounds without considering any possible interactions 
finally  we explored tree based methods for our data set  mainly for two reasons  decision trees are easy
to interpret and it is possible to identify critical features for the problem  they can often provide good insights
into the problem and enhance the understanding of the problem itself  decision trees also often perform well on
imbalanced datasets  the splitting rules that look at the class variable used in the creation of the trees  can force
both classes to be addressed  a major disadvantage is that they easily over fit  but thats where ensemble methods
like random forests or boosted trees come in 
in this baseline case we used elevation and slope as features  furthermore  gradient boosting performed
very well  auc       whereas the forest tree algorithm performed poorly with an auc of     

learning algorithms results  full feature set
when the full set of features were considered  the best results were obtained using logistic regression
and nave bayes  in both cases  we used thresholds to be more stringent on deciding whether there is landslide
or not  figure   shows the roc curve  the confusion matrix and the auc vs  training set size for both algorithms 
in both auc vs  training set size plots  we observe that the performance increases with increasing training set size 
equivalent to a decrease in error in the standard learning curve  for large training set sizes a decrease in
performance is noticed  we believe that this is due to the fact that if we have a large portion of training example
 

fimachine learning   cs   
only a few positive example are left for prediction  only a few false predictions could lead to a large error  this
also explains the large variance on the auc 
the confusion matrix shows that for both algorithms  we were able to predict the presence of a landslide
with an accuracy of about     
logistic regression

nave bayes

figure    results of logistic regression and nave bayes using   features  from left to right  roc curve  confusion matrix and area
under curve vs  training set size  standard deviation shown in red 

section    conclusion   future work
in conclusion we can say that it is very hard to debug a problem with a highly imbalanced data set and
strategically improve it with respect to variance and bias  testing different algorithms showed that nave bayes
and logistic regression perform the best when having all available features included and adjusting the threshold
for probability in the case of nave bayes  we were able to predict      of the landslides 
finally  it is very hard to predict the exact location at which a future landslide will occur given the
unbalanced data set  nevertheless  the probabilistic interpretation provides a useful tool to generate spatial
probabilistic hazard maps  e g  what areas are landslides most likely to occur   
in terms of future work  we believe that including more critical features such as the distance to faults 
distance to rivers  and formation deformation from insar satellite data is worth exploring  other unsupervised
learning algorithms can also be tested such as anomaly detection because our data set is highly imbalanced 

 

fimachine learning   cs   

section    references
    ayalew  lulseged  and hiromitsu yamagishi   the application of gis based logistic regression for landslide
susceptibility mapping in the kakuda yahiko mountains  central japan   geomorphology                    
    bui  dieu tien  et al   landslide susceptibility assessment in the hoa binh province of vietnam  a comparison of
the levenbergmarquardt and bayesian regularized neural networks   geomorphology                   
    catani  f   et al   landslide hazard and risk mapping at catchment scale in the arno river basin   landslides    
                
    ercanoglu  m   landslide susceptibility assessment of se bartin  west black sea region  turkey  by artificial
neural networks   natural hazards and earth system science                     
    guzzetti  fausto  et al   landslide hazard evaluation  a review of current techniques and their application in a
multi scale study  central italy   geomorphology                      
    hastie  trevor  robert tibshirani  and j  h  friedman  the elements of statistical learning  data mining 
inference  and prediction  new york  springer        print 
    kanungo  d  p   et al   landslide susceptibility zonation  lsz  mappinga review   j south asia disaster stud
                   
    pradhan  biswajeet  and saro lee   landslide risk analysis using artificial neural network model focusing on
different training sites   int j phys sci                   
    pradhan  biswajeet  and saro lee   regional landslide susceptibility analysis using back propagation neural
network model at cameron highland  malaysia   landslides                   
      precision and recall   wikipedia  wikimedia foundation  n d  web     dec       
     vanwesten  c  j   castellanos  e   and kuriakose  s  l          spatial data for landslide susceptibility  hazard 
and vulnerability assessment  an overview  engineering geology                
     wang  h  b   and k  sassa   comparative evaluation of landslide susceptibility in minamata area  japan  
environmental geology                      
     icme machine learning workshop  august           

 

fi