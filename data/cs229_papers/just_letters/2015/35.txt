machine learning for thyroid cancer diagnosis

rajiv krishnakumar
department of applied physics  stanford university  ca       usa

rajk   stanford   edu

raghu mahajan
department of physics  stanford university  ca       usa

rm     stanford   edu

akash v  maharaj
department of physics  stanford university  ca       usa

amaharaj   stanford   edu

abstract
we investigate the use of high throughput gene
expression data in the diagnosis of thyroid cancers  using logistic regression and support vector machines  svms   we develop a classifier
which gives similar performance      sensitivity and     specificity  to the currently bestknown classifier  but uses significantly fewer features  we used two different techniques  principal components analysis and mutual information
score  to select features  the results do not depend significantly on which method is used for
feature selection 

   introduction and related work
high throughput gene expression data is now readily available for many diseases and has been used extensively to develop classifiers to help physicians diagnose and treat these
diseases  thyroid cancer is one of those diseases 
currently  a technique known as fine needle aspiration
 fna  is used to determine whether a thyroid nodule is malignant or benign  even though over     of the total cases
end up being benign  howlader et al          many more
are diagnosed as malignant or indeterminate  i e  unclear
diagnosis  after performing fna  patients with indeterminate diagnoses  as well as those with malignant diagnoses 
then undergo diagnostic surgery to remove the tumor or benign thyroid lesion  only about     are subsequently found
to be malignant post operation  welker   orlov        
this imperfect system leads to many unnecessary surgical
operations  enhancing risk for the patient and an increase
in healthcare costs 
cs    final project report 
december         

a recent study  alexander et al         has shown the potential for diagnosing thyroid cancer using gene expression
data  the data consists of     samples  each having about
o       features  genes  and each classified as malignant
or benign by post operation inspection  using support vector machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of o      
features  while the diagnostic test has a reasonable false
negative rate of     it has a high       false positive rate 
in addition  the use of     genes with just     patients renders any classifier liable to overfitting  thus  our principal
goal is to investigate novel machine learning approaches
for development of a classifier that outperforms the current
state of the art  namely higher specificity while maintaining
or improving the current sensitivity  with the use of fewer
features 
in sec    we discuss the nature of the data  in sec   we discuss feature selection based on mutual information scores 
and also via principal components analysis  in sec   
we implement logistic regression and support vector machines with linear kernels  we discuss the tuning of parameters such as relative weights assigned to the benign
and malignant samples  and varying    regularizations for
the svm  we end with a summary of our major results in
sec    and discussion of future work in sec    

   gene expression data
the dataset which forms the basis of this project was first
employed by  alexander et al         in a study on the use
of gene expression data for pre operative thyroid cancer diagnosis  a full description of the clinical procedures employed in acquiring the gene expression data is beyond the
scope of this work  interested readers can consult  alexander et al          basic attributes of the dataset  onlineref 
      are summarized in table    the relevant part of
dataset consists of m       patients  or more callously 

fimachine learning for thyroid cancer diagnosis
table    key attributes of the dataset 

result of biopsy  cytology 
benign
malignant
indeterminate
post operative diagnosis of
indeterminate tumors  ground truths 
benign
malignant

   

  

  
 

  
  
   

   
  

 

   

patient sample

number of patients in dataset

  

 

 

   

 

number of features  gene expressions 
raw dataset
pre normalized and selected

      
   

   
  

  

   
  

  

  

  

   

   

   

   

gene feature

training examples  with indeterminate biopsy results  of
which the ground truth classifications of tumors are   
malignant and     benign  a further m        patients
whose biopsies were not indeterminate     benign     malignant  are also included in the primary dataset and these
will later form our validation set    each training example
consists of n       features  corresponding to a select set
of gene expressions obtained from the biopsy and subsequent microarray assay of a single thyroid nodule 
     pre processing of data set
as part of the publicly available dataset  onlineref        
we were able to obtain the normalized gene expression data
for     genes for all     patients  these     were chosen
by the authors of  alexander et al          based on a sequential procedure involving limma analysis and is detailed in the supplemental material of their paper  we used
only this subset of features in our analysis 
we further normalize each gene expression to zero mean
and unit standard deviation  as mentioned previously  the
relative intensities of different genes is likely to be meaningless  an artifact of the microarray procedure   and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise  figure  
provides a color map of these n       features for the
m       patients in our dataset  we note here that an attempt at quantile normalization severely decreased the performance of our classifiers  so we have not employed this
additional pre processing step 
 

the fact that biopsy results are based on the personal opinion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs  pre determined
cytology diagnoses  this is our justification for using the predetermined samples as a validation set 

figure    color map visualization of gene expression data for a reduced feature set of     features  because relative magnitudes of
different genes are  in principle  meaningless  each feature  gene 
has been normalized to zero mean and a standard deviation of
   the black line at patient number     separates benign  above
line  from malignant samples  below line  

   feature selection
with     features and     samples  smart feature selection
is crucial to avoid overfitting  we have adopted two complementary approaches in this work  mutual information
 mi  and principal components analysis  pca   happily 
as we will see in subsequent sections  the predictive power
of the resulting classifier is similar regardless of whether
we use the mutual information statistic or pca to select
the subset of genes 
     mutual information
our first method of feature selection involved computation
of mutual information scores  this has the advantage of
hand picking the the most informative genes  opening the
possibility of building a classifier which uses a few select
genes that can easily be sequenced in clinical tests  we
computed the mutual information score m i xj   y  of each
gene xi defined as
m i xj   y   

xx
xj

y

p xj   y  log

p xj   y 
p xj  p y 

   

where p xi   y  is the joint distribution of gene xj and diagnosis y  and we binned the gene expression levels into   
bins to make xj a discrete variable  the names and tcids
 taken from the supplemental material in  alexander et al  
       of the top ten genes are listed in table    a histogram is shown in figure   

fimachine learning for thyroid cancer diagnosis
mi rank
  
  
  
  

gene names
liph
mdk
pros 
lrp b

tcid
       
       
       
       

gene description
lipase  member h
midkine  neurite growth promoting factor   
protein s  alpha 
low density lipoprotein receptor related protein  b

  

mapk 
pkhd l 

       

mitogen activated protein kinase  
polycystic kidney and hepatic disease    autosomal recessive  like  

  
  
  
  
   

gabrb 
cldn  
dpp 
timp 
mpped 

       
       
       
       
      

gamma aminobutyric acid  gaba  a receptor  beta  
claudin   
dipeptidyl peptidase  
timp metallopeptidase inhibitor  
metallophosphoesterase domain containing  

  or    i e  benign or malignant respectively   here   
rn   is a vector of coefficients which can be determined by
maximum likelihood estimation  although h  x  takes on
continuous values between   and    the algorithm sets the
prediction to   if h  x        and   otherwise  given the
training examples x i    and their corresponding outcomes
y  i    we can write down the likelihood

table    list of the top ten highest correlated genes ranked via
their mutual information  mi  score 

l    

     principal components analysis

 

   models
     logistic regression
given the binary classification problem with inputs x 
rn a simple first classification algorithm is logistic regression  dobson   barnett         logistic regression uses
the function
 
   
h  x   
    et x

    

    

   

    

   

mutual information

   

   

    

 

test error
training error

   

   

  

    

   

  

  

  

  

  

  

  

 

  

number of features

  

    

  

  

  

  

  

  

number of features
   

test error
training error

test error
training error

    

   

 
 
  

 

    

test error
training error

   

  

  

 

 

 

log  

figure    histograms from mi  left  and pca  right   showing the
number of genes  principal components  with a given mutual information score  eigenvalue   the features on the right side of
these histograms are the most important  note the logarithmic
scale for pca rankings 

    

error

 
 

 y  i 

we implement logistic regression using matlabs multinomial logistic regression function  mnrfit  to assess the
success of this algorithm  we employ hold k fold cross validation  with k       in fig    we plot the training error and
empirical test error as the number of features is varied  we
trained different classifiers by tweaking the number of features kept and also the relative weights      for benign vs 
malignant samples  assigned to the samples  it is important
to correctly diagnose the patients who have a malignant tumor  so we impose a big penalty for misclassifying the malignant samples  we do this for features selected both using
mutual information and pca 

error

 

    h  x  

where w i  is a weight which is dependent on whether the
sample was malignant or benign  the  log  likelihood can
now be maximized using gradient ascent  the weights allow us to impose a higher penalty when predicting a malignant sample incorrectly compared to when predicting a
benign sample incorrectly 

error

  

number of eigenvalues

number of genes

  

y  i 

w i   h  x  

   

i  

to determine whether a sample of with a vector of features
x  rn    in our case the n different gene expressions 
with an extra constant feature x       has an outcome of
  

i  
m
y

w i  p y  i   x i     

error

to have more confidence in our feature selection  we also
did feature selection using a completely different method 
principal components analysis  pca   this method complements mi by selecting linear subspace of all n      
genes with the highest variances  and hence largest amount
of information   in pca we compute the eigenvectors of
pm
t
the covariance matrix    i   x i  x i  where x i  
rn is a vector containing all genes of patient i  selecting
the top k principal components corresponds to transforming each x i  by a matrix w    u    u            uk   where
u   rn is the principal eigenvector of  etc   a histogram
of the eigenvalues of  is shown in figure   

m
y

   

    
   
    

    
   
 

   
  

  

  

  

  

number of features

  

  

    
 

  

  

  

  

  

  

  

number of features

figure    learning curves for logistic regression  unweighted  upper row   and weighted     benign malignant samples  lower
row   mi on left column and pca on right column 

fimachine learning for thyroid cancer diagnosis
   

    

test error
training error

    

test error
training error

   
 

    

gene    midkine   tcid          

    

error

error

    
    
   

   
    

    
    
 

b b
i b
b m
i m
m m
weighted boundary
unweighted boundary

   

  

  

   
 

  

number of features

  

  

  

number of features

figure    learning curves for svm  weighted  mi on left and
pca on right  each with a    parameter of c        the weighted
decision boundary uses relative weighting of      malignant samples are assigned a higher weight  

   
 
   
 
   
 
    
  
    
    

  

    

 

   

 

   

 

   

gene    lipase  member h   tcid          

 

the second learning algorithm we have implemented is a
support vector machine  svm   using a linear kernel with
   regularization  use of    regularization is necessary because the data is  empirically  not strictly linearly separable 
p
svms work by finding the optimal hyperplane j wj xj  b
that separates classifies the data  here  b is a constant offset
and wj is a coefficient for each gene feature j in our data 
solving for the optimal wj and b amounts to a constrained
convex optimization problem  where we must minimize
m

x
 
  w      c
i
 
i  

   

w r t  w and b  subject to the constraints y  i   wt x i    b  
   i and i    i  i are known as slack variables 
and c is the constraint parameter which penalizes incorrectly classified data   solution of this problem is simpler
if we consider a dual optimization problem where a series
of standard manipulations  bishop        leads to the dualized optimization problem 
max w     


m
x
i  

i 

m
x

t

y  i  y  j  i j x i  x j 

   

i j  

pm
subject to the constraints    i  c and i   i y  i   
   we implement the svm with this linear kernel  using the
matlab routine fitcsvm which proceeds via the sequential minimal optimization  smo  algorithm  platt        
to estimate our errors  we again employ k fold cross validation with k      and impose a big penalty for misclassifying the malignant samples  in figure    we show the
learning curves for the svm using a relative weighting of
    for the benign vs  malignant samples and an    parameter of c        which we optimized for by considering cs
over   orders of magnitude   it is clear that optimal performance for both mi and pca selected features is achieved
for fewer than    features kept 

principal component   

 

     support vector machines

 
 
 
  
  

b b
i b
b m
i m
m m
weighted boundary
unweighted boundary

  
  
   

   

  

 

 

  

  

principal component   

figure    decision boundaries for svm  using only the best two
genes selected using mutual information  top figure  and top two
principal components pca  bottom figure   we used a relative
weighting of      higher weights for malignant samples  and the
   parameter c        data points are labelled by their pre  and
post operative diagnosis  b   benign  i   indeterminate  m   malignant   so for example  i m  indicates a pre operative diagnosis
of indeterminate and a post operative diagnosis of malignant  the
clustering of malignant samples is obvious in both mi and pca
variables  it is also clear that unweighted boundary achieves excellent classification of benign samples  leading to low false positive errors for unweighted classifiers in table    

while the learning curves of fig    suggest that     features should be used in our final classifier  we have found
that a two feature scatter plot is an informative visualization
of the structure of our data demonstrating the lack of linear
separability which persists in higher dimensions  plotted
in figure   is the full data set  test and training sets  along
with the validation set   in addition to the decision boundaries obtained by keeping the top two features 

   final results
after exploring the parameter space of the models described in section    we decided to work with a subset of   
features  since its in that ballpark where the test and training errors seem comparable and small  having used the
training curves to select these    features  both genes from
mi and then components from pca   we then train on the

fimachine learning for thyroid cancer diagnosis
table    false positive and false negative rates as well as total errors for the various classifiers used in our study  both logistic regression and svms were trained on the top    features obtained by feature selection using mutual information  mi  scores and principal
components analysis  pca   errors reported here are obtained by training only on the full set of     training examples  i e  the test
  training set used in plotting learning curves of sec      and       while we have introduced a validation set comprising an extra
    patients whose cytology was determined before hand  weighted classifiers have significantly lower false negative rates  higher
sensitivity  and so are the preferred classifiers in our work 
classifier

false positive percentage

false negative percentage

total misclassification percentage

train  test set

validation set

total

train   test set

validation set

total

train   test set

validation set

total

logistic unweighted
logistic weighted

   
  

 
 

   
  

  
   

  
   

  
  

  
  

   
   

  
  

svm unweighted
svm weighted

  
  

 
  

   
  

  
  

   
   

  
  

  
  

   
   

  
  

logistic unweighted
logistic weighted

   
  

 
   

   
  

  
  

  
   

  
  

  
  

   
   

  
  

svm unweighted
svm weighted

   
  

 
  

   
  

  
  

   
   

  
  

  
  

   
   

  
  

mi

pca

full m       indeterminate patients  finally  we include
the previously unexposed m        samples in the validation set for the first time  and predict on all     samples 
we report on final error estimates  for both logistic regression and svms in table    note that for the svm  we use
a linear kernel with an    parameter c        and wherever weighted classifiers were used the weights were      
benign to malignant 
this study presents a unique challenge  while it is crucial
that a cancer classifier accurately predict malignant samples  i e  low false negative rates or high sensitivity   recall
that most patients with indeterminate biopsies end up having unnecessary surgery  thus a parallel goal of our work is
to reduce unnecessary surgeries for patients with benign tumors  requiring a low false positive rate  high specificity  
an optimal balance of these competing interests is perhaps
achieved by the weighted svm classifiers  with a total
false negative rate of      i e      sensitivity   and a false
positive rate of       i e      specificity  this is clearly
a competitive classifier 

   summary and future avenues
the primary battle in this project was high variance  and
so we have been very careful in feature selection  we have
used a small subset of the curated list of     genes  based
on their mutual information score and pca  we selected
our model after a thorough analysis of the learning curves 
while varying the relative weights and the    parameter for
the svm  our classifier gives similar performance to the
currently best known classifier  but uses significantly fewer
features     features as opposed to      this narrowing of
 
the results are presented in a manner that is consistent with
cancer biology research  and hence we have not shown p  r and
f  values or confusion matrices  these can be inferred from table
  

the list of contributing genes can possibly allow for a more
targeted approach to investigating the genetic characteristics of thyroid cancer 
a natural next step is to test the robustness of our gene
selection by implementing other feature selecting methods
 such as random forests  forward search  and seeing if they
pick the same genes  in addition  it is worth noting that
we have only had access to     genes out of        and
an analysis of the full set of genes from the experiment
would make our feature selection process more complete 
unfortunately this goal faces an administrative hurdle  we
were told by dr  giulia c  kennedy  one of the authors of
 alexander et al          that this data is in fact proprietary 
it is also interesting to note that the errors of our classifiers
do not depend on whether we use pca or mutual information to select our features  however this result is not trivial 
especially because the top    genes do not have significant
weights in the top principle components  this curious result warrants further investigation 
finally  in addition to strengthening our feature selection 
we plan to see if more advanced classification techniques 
such as neural networks  may give better performance 
however it is possible that this may require larger data sets
and more extensive clinical trials 

   acknowledgements
we are thankful to kevin brennan and olivier gevaert at
the stanford center for biomedical informatics research
for suggesting this project  providing us with the data  and
helping us try to unlock information in the cel files  we
also thank peter li for insightful comments and the genesis
of this project  finally we would like to thank our project
ta irene kaplow for helpful comments on the midterm
project report and during the poster session 

fimachine learning for thyroid cancer diagnosis

references
alexander  erik k  kennedy  giulia c  baloch  zubair w 
cibas  edmund s  chudova  darya  diggans  james 
friedman  lyssa  kloos  richard t  livolsi  virginia a 
mandel  susan j  et al  preoperative diagnosis of benign
thyroid nodules with indeterminate cytology  new england journal of medicine                      
bishop  christopher m  pattern recognition and machine
learning  information science and statistics  springer 
new york        isbn                   
dobson  annette j  and barnett  adrian g  an introduction
to generalized linear models  number    in texts in
statistical science series  crc press  boca raton  fla  
   ed edition        isbn                   
howlader  n  noone  am  krapcho  m  neyman  n 
aminou  r  waldron  w  altekruse  sf  kosary  cl 
ruhl  j  tatalovich  z  et al  seer cancer statistics review 
          bethesda  md  national cancer institute 
         
onlineref       
url http   www ncbi 
nlm nih gov geo query acc cgi acc 
gse      
platt  john c  sequential minimal optimization  a fast algorithm for training support vector machines  technical
report  advances in kernel methods   support vector learning       
welker  mary jo and orlov  diane  thyroid nodules  american family physician                     

fi