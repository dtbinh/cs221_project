cs     final project  fall     

 

risk assessment  an art or science 
predicting recidivism at the time of sentencing
miguel camacho horvitz  mathematical and computational sciences
jeremy kim  electrical engineering
stanford university

abstractmotivated by recent legislation that bases sentencing
of criminals on their likelihood to recommit a crime  we developed
models that would make a prediction about whether or not individual criminals would commit another crime post release  using
various supervised learning techniques  we looked at features
of relevant criminal  locational  and demographic information
to make predictions based on the known outcomes of  in some
ways  similar individuals  we also created a balanced set with
equal numbers of positive and negative examples and trained
and tested on that set  from there  we moved on to a hybrid
unsupervised supervised model  in this approach  we classified
the data we had using k means clustering before training our
supervised learning algorithms on each data cluster and looking
at the average predictive error between clusters  finally  we
calculated mutual information statistics to infer which features
were most informative of recidivism in an effort to decompose our
hypothesized biases  in the end  we found that in the full data
set the supervised learning models on their own could barely
perform better than a null hypothesis that no one recommits 
but when paired with the k means clustering  we saw slight but
significant improvement in the prediction accuracy 

i  i ntroduction
as a society  we seek to reduce crime  one important
facet of this complex goal is the minimization of prisoner
recidivism  or a relapse in criminal behavior following release 
to this end there have traditionally been several strategies including rehabilitation services and prisoner vocational training 
recently  however  quite a different strategy has arisen in the
national conversation  in       pennsylvania adopted into its
state legislature the idea of using a risk assessment metric
as a factor in determining the most appropriate incarceration
sentences  by title     section        
 a  general rule the commission shall adopt a sentence
risk assessment instrument for the sentencing court to use
to help determine the appropriate sentence within the limits
established by law for defendants who plead guilty or nolo
contendere to or who were found guilty of felonies and
misdemeanors  the risk assessment instrument may be used
as an aide in evaluating the relative risk that an offender will
reoffend and be a threat to public safety     
what if at the time of sentencing we could predict whether
an offender in question  still awaiting his her sentence  would
commit another crime following his her ultimate release from
our penal system  given such a metric  many have advocated
that we could reduce the burden on our prison system by
providing low risk offenders early parole as well as enhancing public safety by keeping tabs on high risk offenders 
however  there has been significant push back  with critics

raising both moral and practical questions surrounding the
usage of risk assessment instruments  indeed  former attorney
general eric holder commented that by basing sentencing
decisions on static factors and immutable characteristics  like
the defendants education level  socioeconomic background  or
neighborhood  they may exacerbate unwarranted and unjust
disparities that are already far too common in our criminal
justice system and in our society 
interesting as it may be  in this study we avoid any moral or
ethical debating and focus on numbers  in particular  we hope
to address perhaps the most obvious and practical question 
given the apparent consequential nature of a risk assessment
instrument  how well could such a model truly predict prisoner
recidivism 

ii  r elated w ork
we originally came across this topic through a collaboration
between well known statistician nate silvers fivethirtyeight
and the marshall project  a non profit organization studying
criminal justice  just months ago  in august of       they copublished an in depth piece titled should prison sentences
be based on crimes that havent been committed yet 
    in which they brought up the idea of risk assessment as
a tool for criminal sentencing  though acknowledging the
controversies  they pointed to several counties across the us
in which initial implementations of risk assessment tools used
in criminal sentencing has led to less severe sentences for
low risk offenders and a reduction or flat line in recidivism 
intrigued by this idea  we looked into how such counties 
states  or outside organizations have constructed or determined
the best risk assessment instruments  we found that multiple
other studies                      published statistics about rates
of recidivism  in particular  we found that previous studies
focused their analyses on identifying significant features  this
is one reasonable application of such analysis on prisoner
recidivism as there are so many human biases at play that
there is a natural goal of demystifying which features are
in fact most predictive as well as identifying the correlations
between features  unfortunately  these studies do not publish
their predictive models nor discuss their methodologies 
thus  the main difference between our work and these previous studies is that we will go beyond feature inference and
resulting predictive error percentages and report specifically
which models do best as well as interpreting our results 

fics     final project  fall     

iii  data
a  data source
to perform any meaningful analysis on recidivism prediction  it was critical that we find a longitudinal study on prisoners who were tracked following their release from the prison
system  while there were interesting questions to ask in the
unsupervised setting  we had our minds set on a classification
model that would make use of the binary outcome vector
that stated  did he she reoffend  feature  the united states
department of justices bureau of justice statistics  bjs  was
the only source that had large scale studies of the type we were
searching for  we could gain access to a study conducted from
          through the national archive for criminal justice
data     
this data set  recidivism of felons on probation
is composed of        cases of felons released on probation
in the year       each case has     associated features
consisting of information ascertained from sentencing records 
probation files  and criminal history files  notably  the study
includes whether the felon re entered the prison system during
the four years this study was conducted  it is worth noting that
all of our analysis will be in terms of recidivism in this fouryear period only 
b  processing
let us now define a risk assessment instrument  as the
pennsylvania legislature did  to mean simply an empiricallybased model which uses known information about an individual that are relevant in predicting recidivism to do just that  in
order for us to construct such a model  we first had to process
our data such that our features were discrete and ordered  as
it did not make sense to run regression analysis on categorical
data   secondly  in order to draw conclusions as to predicting
recidivism at the time of sentencing  we needed to ensure that
all of the features we included in our models were known at
the time of sentencing and not afterward 
to this end  we identified the number of previous convictions  type of crime committed  whether there was a presentence investigation  number of address changes  drug abuse
history  education  and a host of demographic information
as suitable features  one of the challenges we faced early
on was that many features we would have liked to include 
such as employment and income  were pre processed  assigned
weights  or coded in a way that made them unavailable to
us  additionally  several features had to be discretized and
the categorical features needed to be transformed into binary
indicator variables  type of crime committed  for example  was
divided into nineteen new binary features that corresponded to
indicator functions of whether or not the crime was a certain
crime 
perhaps the greatest challenge we had with our data set was
in dealing with missing data  when the outcome  if the subject
reoffended or not  was not known we threw those examples
away  unfortunately  though not unexpectedly  many of the
input data points were listed simply as unknown  moreover 
the distribution of unknowns was largely uniform so it was
unfeasible to throw away certain features or examples  in the

 

end  we decided it best to deal with this using inter feature
correlations 
 i 
given that a feature vector xj had an example xj that was
missing  we determined the feature vector xk most correlated
 i 
with xj   we then found the value of xk for that training
 n 
 n 
example  then  for every example xk in xk where xk  
 i 
 n 
xk   we found the corresponding value xj in xj   we then
found the average  mean for ordered data  mode for categorical
 n 
 i 
data  of all the associated values xj to assign to xj  
finally  this left us with an        example by    feature
data set that was ready for analysis 
iv  m odels
a  supervised learning
for this study  we looked at   supervised learning algorithms  naive bayes  logistic regression  support vector
machines  and random forest classifiers 
naive bayes   the naive bayes algorithm looks at the
conditional probabilities of an input given a certain output as
well as the marginal probabilities of both that input and output 
it then calculates the conditional probability of an output given
that input with 
q
p xi  y p y 
p y x    i q
i
i p x  
and predicts the output with the highest condition probability 
logistic regression   the logistic regression algorithm
assumes that conditional probabilities of outputs given input
features follow a logistic function as follows 
 
    e      x 
  by looking at a joint likelihood function 
y
l    
p y i  xi    
p y     x   

i

and setting the partial derivative with respect to  equal to
   the algorithm solves for the maximal parameters   for
each new test example  the features x are put into the logistic
function  and if the value is greater than      output   is
predicted 
support vector machine   a support vector machine
 svm  uses the lagrangian dual form of the optimal margin
classifier to construct a separating hyperplane between positive
and negative examples  the general form of the dual optimization problem for a linear svm is 
max w     

x

i 

i

 x i j
y y i j   xi   xj  
  i j

such that 
i     i
x
i y i    
i

for nonlinear classification  we use kernels to define feature
mappings to represent non linear data as feature vectors in
terms of only inner products and create a hyperplane in the

fics     final project  fall     

 

transformed space  we tried   different kernels in our tests 
polynomial  sigmoid  and gaussian  defined  respectively  as 
k x  z     xt z   c d
k x  z    tanh axt z   r 
  x  z   
   
random forest classifier   a random forest classifier is an
ensemble learning algorithm that captures the average or the
mode of many regressions  the data is split into b different
branches of n samples each  called xb   yb   each branch then
uses logistic regression to get a training rule fb and calculates
the mean of these rules as 
k x  z    

b
  x
fb  x 
fb  
b
b  

the idea behind this is that the mode of our regressions
would capture a stronger prediction than would any single
regression  moreover  as regressions may overfit to the training
data  this ensemble algorithm corrects for such overfitting 
b  unsupervised learning
k means classifier   the k means algorithm classifies each
training example into one of a few clusters based off of
minimum distance to the cluster center
c  i    argminj   xi  j    
 
the cluster centers  j   start as random but are then updated
according to the samples placed in them with 
p
  ci   j xi
j   pi
i  ci   j 
c  feature inference
mutual information   in order to look at the similarity
between features and the outcomes  we used the mutual
information statistic  in particular  we looked at the similarities
between the values our features xi took on and the positive
 reoffending  outcome y      as follows 
m i xi   y       

xx
xi  y  


p xi   y     log

p xi   y     
p xi  p y     



v  r esults and d iscussion
we first began by establishing naive assumptions as predictors and figuring out baseline test errors associated with
using these predictors  the five we looked at were to assume that everyone recommits         error   assume no one
recommits         error   assume that all people under the
age of    will recommit         error   assume all violent
criminals recommit         error   and assume that all with
previous records recommit         error   our subsequent
results would be compared to these baselines  however  it is

immediately apparent that there is quite a low error associated
with predicting no one recommits given how imbalanced our
training set is  as a result  our initial estimated test errors could
barely exceed the predictive power of the null hypothesis
that no one recommits a crime  given this  we split our
subsequent analyses into two main sections  one in which we
trained on the full  imbalanced data set  and another in which
we randomly sampled both positive and negative training
examples in order to create a set with even numbers of the
two 
full dataset
baseline
all features
  features
after clustering

  error
       assume no one recommits 
       polynomial svm 
       logistic regression 
        clusters  polynomial svm 

table i
f ull dataset r esults   t he best test errors when training and
testing on the full dataset  c learly  the test error remained
more or less constant until the hybrid model of k   means
clustering and supervised learning was employed  

a  full data set analysis
our first step in deriving a model was to train the four
supervised learning algorithms  from our set of        examples  we performed k fold cross validation with    folds
and calculated the average test error  using all of the features
in our set  we found that the errors we could get barely
exceeded the predictive power of the null hypothesis that
no one recommits a crime  there were slight variations for
the various models  but the best that each could do was
       error for logistic regression         for the svms
 best with polynomial kernel   and        for the random
forest classifier 
while this showed some improvement over many of our
baseline tests  it was clear that the prediction was very skewed
towards giving a negative result and in regards to that baseline 
we saw no improvement 
b  balanced subset analysis
seeing how skewed our model was towards the negative
prediction and realizing that close to     of the data used
for both training and testing were negative examples  we
attempted to create a more even subset of the data  we took
a random sampling of negative examples until the number of
negative examples matched the number of positive examples
in our data  we then concatenated all of these examples into
a matrix that we will refer to from now on as our balanced
set 
we found baselines and employed learning algorithms to
this balanced set in the exact same way that we had in the
full  unbalanced set  for this set  if you assume that everyone
recommits or no one recommits the test error is      if you
assume that all criminals under    will recommit it is        
if you assume all violent criminals recommit it is        and
if you assume all with a previous record recommit it is        
our results for the test error of the four unsupervised
algorithms in the balanced set was significantly improved

fics     final project  fall     

from these baselines  when using all of the features in our
data  logistic regression achieved        error  the svm with
guassian kernel gave        error  and the random forest
classifier resulted in        error 
balanced dataset
baseline
all features
  features
after clustering

 

we could not  however  drastically outperform the model that
used all of the features 

  error
       assume only young criminals recommit 
       random forest classifier 
       random forest classifier 
         clusters  polynomial svm 

table ii
f ull dataset r esults   t he best test errors when training and
testing on the balanced dataset  w hile the overall accuracy
of these models are not as good as with the full training set 
we have improved performance over the baselines   t he  
feature model has as much   and even slightly more   accuracy
as the model using all of our features  

c  feature selection
next  we found that our training error was significantly
lower than the test error  as low as    in some cases   our
models were most likely suffering from high variance  since
we had a limited number of examples  we could not really
increase the size of our training data  we looked at using more
folds for cross validation  but had no significant improvement 
we then tried to use fewer features 
we performed both forwards and backwards search to find
the features with that showed the largest change in test error
when added or removed  for both models  backwards search
yielded few results  as the model was already so skewed in
one direction and removing one feature at a time did not give
a significant change in the error 
using forward search on the full data set  the algorithm
effectively identified the lowest occurring features  when
averaging across lr  nb  rf  and svm  the lowest expected
test error  using cv  resulted from a   feature model which
included the four lowest occurring features as well as the crime
committed being a weapons offense  naturally  the features
that were most predictive were simply the lowest occurring
features  as they were likely to result in the model predicting
a negative result  and this procedure clearly revealed nothing
surprising  more importantly  however  at no point were we
able to ascertain a group of features that gave us a test error
below the       error that we had seen before that seems to
predict that almost no one would recommit 
for the balanced set  on the other hand  we could identify  
features that could get the training error to within a statistical
margin of the results when using all of our features  see
figure     these features   whether or not the person was
hispanic  whether or not there was an investigation prior to
the trial  and whether or not the person was a female   were
not necessarily the ones that we expected  when training and
testing  using cross validation  on this balanced set using only
these three features  the test error converged to near the error
when considering all features  and in some cases was better 
the best test error came from the random forest classifier
with an error of         slightly lower than the best with the
full set and again significantly improved from our baselines 

fig     forward feature selection  the training error from training and testing
 with cv  on the balanced data set  the test error for every algorithm  except
naive bayes  comes down significantly and converges after the first three
features added 

finally  we looked at the mutual information statistic to see
which features shared the most information with the results
vector in hopes that this could shed light on which features
to use  the results produced a few dichotomies that follow
stereotypes that one may predict  men are positively correlated with recidivism  women are negatively  single people
positively  married people negatively  black people positively 
white people negatively  perhaps surprisingly  the other feature
that shared the most information with the results was the
binary value that indicated a crime was burglary  see figure
   
while these dichotomies give some interesting insight  the
features are almost definitely correlated with many other
factors  some of which we did not have access to  for example 
being black is very highly correlated in our data with having
a prior criminal record  being single  and being employed
less of the time before the crime was committed  therefore 
it is important to acknowledge the inter dependence of so
many features  especially demographic features  rather than
jumping to conclusions  indeed in the fivethirtyeight and
marshall project collaboration  they found that even when
they tried to explicitly eliminate certain biases they couldnt 
in one analysis  they found that they could remove features
on race and income without losing much information as other
features  such as being single and unemployed or being male
and without a high school diploma  simply served as proxies
for your removed features 
we tested models based on just these features with strong
mutual information  but again our models failed to significantly outperform the null hypothesis for the full set and did
not perform better than our model from feature selection for
the balanced set 
d  hybrid model
it was not until we combined the unsupervised learning with
the supervised learning tool of k means clustering that we saw
improvement in the test error  we ran the k means algorithm

fics     final project  fall     

fig     graph of mutual information  mutual information between each
feature and the outcome vector  the features with the strongest mi are circled 
the blue circle represents the crime burglary  the green circle  from left to
right  is being a man  being single  and identifying as black  and the red
circle  from l to r  has the counterparts  being a woman  being married  and
identifying as white 

with random initial cluster centers and then once the data was
classified  trained and tested using    fold cross validation on
the samples in each individual cluster  this was repeated   
times to get an average error  the best test error that we found
in all of our tests was       when the data was sorted into
  clusters and we classified the data in each cluster using an
svm with a polynomial kernel 
we ran a similar protocol for the balanced set  but found
actually slightly worse results after clustering  there are a
number of possible explanations for this  but perhaps this
clustering can not do as good a job of separating the data into
very distinct groups due to the even distribution of positive
and negative examples that you start with 
it is important to note that every time that the tests were
performed  the training and testing was done on within the
same cluster  therefore  it is important that the cluster centers
that are chosen for training are held fixed when clustering
a new example  only then can this clustering improve your
accuracy of prediction 
vi  c onclusion
the questions and controversies surrounding riskassessment as a tool in criminal sentencing cannot be
overstated  statistics  after all  allow us to infer generalizations
about groups of people  to use the actions of previous
collections of what we deemed similar people in deciding
the fate of an individual carries implicit moral philosophical
assumptions and value judgements which we do not
necessarily agree with  still  independent of this  the concept
of predicting recidivism poses an interesting academic
challenge 
given the data available to us and the time scale of this
project  we could not develop a model that predicted recidivism with accuracy significantly different than making a
naive prediction that no one recommits  based off of this  and
combined with the gravity of a false positive result  someone
serving longer jail sentences   our model shows no added value
of using statistics to look for individuals who will recommit

 

crimes  when we had a contrived set with even numbers
of positive and negative examples  we could outperform this
naive prediction by almost two fold  however  attaining this
even distribution may never be feasible and a     error is
still far too high in our minds to base sentencing based off of
this model 
in the future  if a similar predictive model is to be built
and used  a far more in depth and larger data set is needed 
not only would we want to look at more examples  but we
see merit in both tracking criminals over a longer period of
time and having more complete information about someones
background in order to control for correlated features  we
believe that recidivism prediction is possible  but given the
resources we currently have  we are not able to make an
adequately performing model 
vii  acknowledgments
many thanks to professor andrew ng and the entire cs   
teaching staff for all of their support on this project 
r eferences
    http   www legis state pa us  
    the marshall project  the new science of sentencing  fivethirtyeight 
oct       
    patrick a  langan  phd  david j  levin  phd  recidivism of prisoners
released in       federal sentencing reporter  vol      no     recent
state reforms ii  the impact of new fiscal and political realities
 october        pp        
    alexia d  cooper  ph d   matthew r  durose  howard n  snyder  ph d 
multistate criminal history patterns of prisoners released in    states 
september          ncj        
    alexia d  cooper  ph d   matthew r  durose  howard n  snyder  ph d 
recidivism of prisoners released in    states in       patterns from
     to       april          ncj        
    allen j  beck  ph d   bernard shipley  recidivism of prisoners released
in       april         ncj        
    recidivism of felons on probation             united states department
of justice  office of justice programs  bureau of justice statistics 
    risk assessment  pennsylvania commission of sentencing  pcs  pennsylvania state university 

fi