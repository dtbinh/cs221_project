cs    machine learning  autumn     

learning instrument identification
lewis guignard and greg kehoe
stanford university
lewisg stanford edu gpkehoe stanford edu
abstract
in this project  we utilize machine learning techniques to construct a classifier for automatic instrument
recognition given a mono track audio recording  we focus on the classification of eight instruments
commonly used in rock music bands  by examining the spectral content of each instrument  we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set 

  

introduction

the primary goal of this project is to classify
single instrument recordings for eight instruments  acoustic guitar  electric guitar  electric
bass guitar  tenor drum  bass drum  snare
drum  cymbals  and hi hat  the motivation
for instrument identification is to provide insights and results that we hope to use in our
continued research of multiple source separation from a single mono track recording 
the input to our model consists of individual audio recordings for each instrument
in mp  format  we perform feature extraction
from the discrete fourier transform  dft  of the
normalized signal and label each example with
a unique integer corresponding to the instruments class  we then use this data for analysis
and train multiple svms to make predictions
on the test set 

  

related work

recent work on instrument classification for
music information retrieval  mir  focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording  popular approaches include
manual feature set construction and evaluation using classic machine learning algorithms
          the use of neural networks to learn
the feature set      and combinations of manual design techniques with neural networks for
learning      in each of these studies a common
goal is to create or learn a set of features that
will improve classifier accuracy  additionally  a

small set of features are typically derived from
the dft such that computational complexity is
reduced 
this project contributes a novel feature set
design that allows us to obtain       classification accuracy using an svm  similar to other
work  we manually designed the feature set
from the dft  however  we do not use features
extracted from the time domain signal 

  

dataset

our data set consists of       samples  obtained
from both online and live recordings  the table below shows the distribution of samples 
we originaly sought to use samples from entirely online sources for rapid data collection 
however  individual samples for guitar proved
difficult to find  in order to generate samples
for each guitar  we used an audiobox usb and
audacity to record individual notes along the
fretboard of each guitar and generate mp  files
with a duration of approximately     seconds
per note  samples for the percussion instruments were extracted from youtube videos
using     
instrument
acoustic guitar
electric guitar
electric bass guitar
tenor drum
bass drum
snare drum
cymbals
hihat

n samples
   
   
   
   
   
   
   
   

  of data
    
    
    
    
   
    
   
   

given that our feature selection depends
 

fics    machine learning  autumn     

on the spectral content of the training examples  we were concerned about how dissimilarites in quality between wav audio  easily
produced by our live recordings  and mp  audio as output by     would bias the data set  a
brief analysis of the dft for the same recording encoded as both    bit wav at      khz
and mp  at    kbps revealed that no significant
information loss occurred that incumbured our
feature extraction  therefore  we decided to
use mp  as the common input format 

  

features

features for each sample are obtained by using the dft and identifying the    most predominant frequencies with the greatest percentage contribution to the total power of the
signal  given that our samples are not uniform
in length  we first normalize each sample to
have unit energy  this also removes any effects
due to variance in volume levels among the
samples  after normalizing  we find the    frequencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution  using a
log range accounts for a larger octave band    
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve tone equal temperament      thus  for
each of the    selected frequencies  we extract
a   tuple of frequency  power  and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contribution  an example of feature extraction is
shown in figure   for the same note played on
the bass guitar and acoustic guitar  we observe
that each sample gives a unique signature of
each instrument in the frequency domain 

figure    feature selection  bass and electric guitar

alternatively  we considered using the entire fft as the feature vector for each sample 
however  in order to avoid high variance  we
chose to select a small number of discrete features relative to the size of our training set 
this stemmed primarily from the difficulty of
acquiring a large number of training examples  furthermore  by selecting a small set
of features shown to be most relevant by our
preliminary analysis  we were able to obtain reduced computational complexity and achieve
run times of under a minute 

  

methods

several models were applied to the dataset 
first in an exploratory manner  k means  principal component analysis  pca    and then
for instrument classification  a support vector
machine implementing a variety of kernels 

    

k means

k means lends itself as a quick model to attempt on the data  and as its a relatively simple
model to implement  gave us good intuition as
to how separable the classes are  at least in a
linear regime 
in the k means classification algorithm  k
centroids are assigned randomly in the space
of the data  centroids being the euclidean centers of mass for each cluster  in our case  k  
  for all the instruments  but we also clustered
on lower values of k to look at subsets of instruments  once the centroids are randomly
assigned  the following algorithm is iterated
until centroid locations stop changing 
   assign each example to its closest centroid
   update centroids to be the center of mass
for the examples assigned to it 
after the centroid locations stop updating 
the algorithm has completed  converged   and
k classes have been assigned to each datapoint 
this unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is separated in the feature space chosen 

 

fics    machine learning  autumn     

    

principal component analysis

pca is a method of dimensionality reduction
that can be used for visual inspection of the
data  and can also be used to map large dimensional data to smaller dimensions  feature
sizes   while capturing as much of the variance
 information  of the data as possible  for our
purposes  pca was used to visually interpret
any structure in the data  and again gain an
intuition of both how separable each class is 
and what models might perform better than
others in separating them 
pca uses a singular value decomposition
to find the principal eigenvectors of the data 
one can then visualize the data in this new
space  using a subset of the first n principal
components  ordered by amount of variance
of the data explained  n is usually picked
as   or   for visualization purposes  finding
the direction of the first principal component
is equivalent to finding u such that   u       
and 
 
m

m

   x  i   t u   

i   

is maximized  where m is the number of
training examples  and x  i  is a specific training
example  the second principal component can
then be found by finding the direction v that
maximizes the same equation  but is orthogonal to u  and so on for higher order principal
components  in this way  one can be sure to
describe the maximum amount of variance of
the data in the fewest directions 
the mean and variance of each feature are
usually normalized  to   and    respectively 
when performing pca  so as not to give disproportionate importance to specific features 

    

support vector machine

support vector machines  svms  are one of
the most popular out of the box machine
learning algorithms for classification  for their
ease of use and wide success  the svm works
by finding an optimal hyperplane separating
the data into two classes  that gives the largest
geometric margin  i e  has all data maximally
distant from the hyperplane  when data is not
linearly separable  one can include lagrange

multipliers that introduce error terms that allow some examples to be inside the geometric
margin  and even possibly on the wrong side
of the hyperplane  the maximization problem
is 
m

max

m

 

d
e
y  i   y   j   i  j x  i     x   j  

   

s t    i  c  i           m

   

 i    

i   

i j  

m

 i y  i      

   

i   

these i and inner products can then be
used to solve for the hyperplane 

t x   b  

m

  i y  i  

d

e
x  i     x   b

i   

with this theoretical motivation  one can
use the sequential minimal optimization algorithm to solve for the hyperplane of a given
dataset 
one of the great benefits of svm is the inner
product of datapoints in the optimization  this
can be replaced by a kernel  which can map
the features into a higher dimensional space 
including mixing terms  etc 
when multiple classes are present  svm can
still be used  there are two ways to tackle the
problem  namely one versus one or one versus
all  in one versus one  there are k  k      
models trained  one for each pair of classes
 where k is the number of classes   the final
classification of a test point is given to the class
for which it gets the most votes from the above
models 

     regularization and model selection
for regularization  we used    fold cross validation  cv  to compare different models used 
   fold cross validation trains on      of the
data and tests on the remaining       and repeats for all    ways to divide the dataset in
this way  the test error from each hold out set
is then averaged to find the estimated test error
of the model overall  in this way  one can still
train on the whole dataset given  and achieve
a pseudo test error 
 

fics    machine learning  autumn     

in choosing which features to use in the
model  we implemented both forward search
and backward search and compared the results 
in forward search  one trains a model on each
feature alone and chooses the model with the
lowest cv error  then  one iteratively checks
which of the remaining unused features will
lower the cv error when included in the model 
the algorithm finishes when adding any unused feature will only increase the cv error 
in backwards search  one starts with a model
using all features  then iteratively chooses to remove one feature that lowers the cv error the
most  the algorithm terminates when removing any more single features will only increase
the cv error 

  
    

results   discussion

k means

in implementing k means on all eight instruments  we found no distinguishable mapping
from the classes generated by the clustering
to the classes of instruments in our dataset 
we assume this is because  at least in a linear
space  there are large overlaps of features from
each class  we then asked ourselves how well
k means would work on instruments whose
fourier transforms seemed far apart  we measured success of the algorithm by ratio of number of correctly classed datapoints to total number of datapoints 
in comparing the data of electric guitar and
cymbals  k means with   clusters achieved
       accuracy  and when comparing electric
guitar to tenor drum        accuracy was obtained  with no good clustering on all eight
instruments  or on separating the electric  bass 
and acoustic guitar  we moved to other algorithms 

    

figure    pca of all data

figure   shows the complete dataset in the
first three principal components  explaining
    of the variance of the data  with proportion of variance explained for each component
being 
pc  
  var expnd

 
    

 
    

 
   

principal component analysis

the goal of running pca on the dataset was
exploratory in nature  to look for any structure
in the dataset and gain insight into the separability of the classes of instruments  we found
when looking at all instruments in the first
three components  a tetrahedral shape  where
the guitars were spread along one face  and
 

cymbals along another  all data converges to
one point in this space  and three boundaries of
the tetrahedron are sharp  in that data does not
move beyond them  we would be interested in
finding other instruments or sounds   effects
that might cross these boundaries 

figure    initial feature selection

figure   illuminates the three guitars alone
in the first three principal components  the
instruments are not linearly separable in this

fics    machine learning  autumn     

subspace  but their respective centroids are distinct  we find the mediocre results of k means
surprising  considering how distinct the data
is here  although we note that regularization
or model selection was not performed on the
feature set for k means  the three principal
components explain       of the variance  with
the proportion in each component being 
pc  
  var expnd

    

 
    

 
    

 
   

support vector machine

for the classification algorithm  we chose an
svm algorithm to attempt to classify all   instruments  and to classify the subset of electric 
acoustic and bass guitars  models of linear 
gaussian  and polynomial kernels were trained
on a random      subset of the data  and tested
on the remaining     for a range of applicable
parameters  we found the polynomial kernel
to have the best performance of the three kernels  varying cost  c   gamma     and degree
 d  per the below kernel  for cost  see equation
     
k   x  i    x   j       x  i t x   j   d

   

we attempted a model for every combination of the following values of parameters 
c                  
            
d           
the model with lowest test error when classifying all instruments used  c             d  
   and when classifying the three guitar instruments  used  c              d     

using these optimal parameters  we ran   fold cross validation with each model on its
respective dataset  then applied both forward
and backward selection to attempt a better error rate using a subset of the features 
cv error
forward
backward

all instruments
     
     
     

guitars
     
    
    

using the backwards subset of the features
    of the    features   looking only at guitars 
we train on     of the data and test on the
remaining     to produce the following confusion matrix 
truth
predict
acou
bass
elec

  

acou
  
 
 

bass
 
  
 

elec
 
 
  

conclusions   future work

when separating musical instruments  choosing an indicative subset of features to predict
on is extremely important  using frequency 
amplitude and power of the top ten peaks of
the spectrum of the samples proved a good subset of features  as shown in pca visually and
svm analytically  k means did not perform
well  and we find this intriguing considering
how well svm performed  we assume this is
because of the freedom that the svm method
has in mapping to higher order spaces  and
including mixing terms 
given more time and resources  we would
move onto source separation  possibly augmenting an independent component analysis
method  or exploring neural networks 

 

fics    machine learning  autumn     

references
    y takahashi and k kondo  comparison of two classification methods for musical instrument
identification  in consumer electronics  gcce        ieee  rd global conference on  pages      
ieee       
    elzbieta kubera  alicja a wieczorkowska  and magdalena skrzypiec  influence of feature sets
on precision  recall  and accuracy of identification of musical instruments in audio recordings 
in ismis  pages         springer       
    peter li  jiyuan qian  and tian wang  automatic instrument recognition in polyphonic music
using convolutional neural networks  arxiv preprint arxiv                  
    dg bhalke  cb rama rao  and ds bormane  automatic musical instrument classification using
fractional fourier transform based mfcc features and counter propagation neural network 
journal of intelligent information systems  pages           
    youtube to mp  generator  www youtube mp  org 
    octave band  https   en wikipedia org wiki octave band 
    equal temperament  https   en wikipedia org wiki equal temperament 

 

fi