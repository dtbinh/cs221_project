college football bowl game predictor
evan cheshire

 

austin childs

thomas leung

in trod u cti on

college football is one of many great traditions found in american universities  the football
team provides a source of entertainment and a sense of community for both students and
alumni  and many fans spend three hours every saturday in the fall rooting for their
team  the season is quick  and broken into two parts  a regular season and a playoff  the
regular season is composed on average of about    games per team  most within the same
conference  at the end of the season  many teams with winning records are invited to a bowl
game  the bowl game is the final game of the season for every team and is the best part of
the season for many college football fans  as teams who appear to be evenly matched play
each other in many exciting games 
for many fans  bowl season leads to gambling  specifically  out of the list of many bowl
games  fans pick winners of each separate bowl  they also assign a confidence that their
pick is correct  ranging from    of bowl games  the number that is assigned can only be
used once  if the fan picks the winner of the game  then he gets the total points assigned for
that bowl game  at the end of bowl season  the person with the maximum score wins  a
combination of picking winners in bowl games and placing high confidence on bowl games
in necessary for winning the game 
the goal of our project is to maximize the potential score of this described game  based on
the regular season statistics of both teams within the matchup of the bowl game  we calculate
a winner and a potential difference in score  the greater the score difference between
individual teams  the more likely the win  thus from the predicted score difference  we
would have a winner and a confidence measurement for each bowl game 

 

rel ated wo rk

previous stanford projects have focused on predicting the winners of bowl games based on
regular season statistics  predicting upsets in college football  and predicting the winners of
nfl games  in general  the past projects were classification problems and used methods
including logistic regression  naive bayes  and support vector machines               for
comparison  we looked at the results of nfl game prediction models such as microsofts
cortana model  accurate to      and nate silvers elo model  accurate to             on
the other hand  the goal of our project is to also assign a confidence to the predicted outcome
of each game  so our goal is regression of either the scores or the differential 

 

data

for our training and testing data  we did not need to scrape the data as we found the data on
a blog      the data from           was reported for all variables that we wished to
test  we created a corresponding mysql database  and were able to use python and
matlab to query this data  we averaged the regular season game statistics for each team 
which we used as features for our models  the data contained approximately     bowl
games  and    features per team  the features were averages or variances of the offense and
defense over their regular season performance 

fi 

mod el s p eci f i cati on s

we took two approaches during our initial modeling stage  both utilizing linear regression 
the first was to predict what a team would score given their average offensive statistics over
the regular season and their opponents defensive statistics over the same time period  as an
alternative  we built another model to predict a margin of victory given all of the information
about both teams in the bowl game 
   

s c o re r e g re s s i o n a n d s v m s

the first began by running linear regression in order to get the projected scores and from
that information calculate a spread between the two teams  the higher the spread  the more
confidence we have in the pick  as a baseline model  we included all of the features we had
available for the regression  for the test set  we parsed out a single season data and used the
remaining ten as training data  this was iterated over every season  thus giving us     fold
cross validation  when predicting scores  we had an average mse  across all    years  of
        to follow this up we used the predicted scores to calculate which team would win
the game and had an average mse of         this provides very little lift over simply using
a coin to randomly pick winners 
in an attempt to improve our results and better the model  we implemented best subset 
forward stepwise selection  and backward stepwise selection for feature selection  these
resulted in two subsets of variables to use for predicting score and winning team respectively
as seen in appendix  table a  regarding the score predictions  our average mse was       
and forecasting the victor had an average mse of         thus we had marginally better
results  but still not acceptable
following feature selection  we attempted to utilize an svm to both regress the score and
classify the winner  using all features left us with respective average mses of        and
       respectively  without tuning  this method performed essentially the same as the linear
regression with specific parameters 
finally  we used a logistic model to predict the probability of each team winning and the n
selected the team with the highest odds  we ran a baseline model with every parameter and
then used the same feature selection methods as before  best subset and stepwise  to narrow
them down  this resulted in an mse of        for baseline and        for the reduced
model  clearly  the logistic regression was most successful but still not satisfactory  the
winner selection accuracy across all seasons can be seen in figure   

figure    prediction accuracy for score models

fi   

d i f f e re n t i a l r e g re s s i o n

the second model was a linear regression on score differential  instead of predicting score
based on one teams offense and the opposing teams defense  a score difference was
projected from both offensive and defensive factors from both teams  like the original score
model  we used a cross validation approach on a season by season projection to assess the
model  again  one season would be the test set  and all other season would be considered
training data  the initial model used every single variable from both teams offense and
defense  this was to establish an initial baseline upon which to improve and compare  the
results of this model were less than perfect  the average came to be around a coin flip 
which is pretty inadequate for selecting a winner  this is the baseline plot in figure   
on top of this initial model  we ran a feature selection to find out which features truly
contributed to the overall model  essentially  each feature had a p value  a measurement of
how much the feature was contributing to the overall data set  from               different
regressions could be had  with one season left out each time  the p values would be
different for the parameters with separate years left out  the initial rule was established that
for all    models  a feature would remain if the min p  values        and the max p values   
    
with this initial rule  a jump in performance was noticed immediately  however  certain
statistics would appear for one team  but not for the other team  i e  team   offense average
points would be in these statistics  while team   offense average points would not be in the
set  thus we werent necessarily confident that these features could describe our model
correctly 

figure    prediction accuracy for differential models

fiwe were able to hone in on variables with randomization  unlike the score linear regression 
where the data is set  we could randomize the matchup so that the teams in the team   and
team   column would be different for every regression  running this over    
randomizations  we saved the features in which the p  values hit the rules  now referred to as
  in figure    the p value feats is the average pick percentage where the statistics
change for every single iteration of the model based on the p  value rule 
to select finalized features for the differential regression model  we selected the values from
 that appeared in at least     of the iterations  these values also had to both appear for
team   and team    these particular features are in the appendix table as the differential
model  the pick probability on a year by year basis is displayed in figure   as the diffreg
feats  the last plot is using the features found in the score regression called score
feats  all of these models average around a     correct pick probability 

 

con cl u si on

overall  we found the best results using the differential model and running variable
selection  the accuracy at     is only slightly below professional models  which attain
around      and provides a significant lift over the baseline attempts  predicting individual
scores provided little benefit and the methodologies applied to this outcome may be better
served in the differential modeling context 
further improvements could be made given more resources  namely scraping data for
different features  running pca to reduce dimensionality  or developing an anomaly model
to try to predict upsets  the last of those suggestions would be the hardest  but also has the
highest potential to make significant improvement  most of the variation between accuracy
across seasons is due to the amount of upsets from year to year  currently  our models are
naturally designed to assign predicted victory to the favorite the large majority of the time  a
more robust model would incorporate a method to account for upsets 

 

ref eren c es

    b  hamadani  predicting the outcome of nfl games using machine learning   online  
available  http   cs    stanford edu proj     babakhamadani  predictingnflgames pdf 
    as padron and j  sinsay  upset prediction in college football    online   available 
http   cs    stanford edu proj     padronsinsay upsetpredictionincollegefootball pdf
    b  liu and p  lai  beating the ncaa football point spread   online   available 
http   cs    stanford edu proj     liulai beatingthencaafootballpointspread pdf
    j  hamann  what it takes to win  a machine learning analysis of the college football
box score   online   available  http   cs    stanford edu proj     hamann whatittakestowin pdf
    cortanas predictions  dec            online   available 
http   www cortanapredictions com  
         nfl predictions  dec            online   available 
http   projects fivethirtyeight com      nfl predictions 
    perspectives on college football  dec            online   available 
http   thenationalchampionshipissue blogspot com         

fi 

ap p en d i x
table a  features for reduced models
feature

linear model
score

linear
model win

logistic
model

differential score
features

variance in defensive
touchdowns allowed

x

x

x

x

defensive turnovers

x

defensive punt return yards
allowed

x

defensive field goal
percentage allowed

x

defensive penalty yards

x

defensive points allowed

x

x

offensive first downs

x

x

offensive kickoff return
yards

x

x

x

differential
model

x

x

x

x

x
x

x

offensive completion
percentage

x

x

defensive completion
percentage allowed

x

x

defensive kickoff return
yards allowed

x

defensive punt return yards
allowed

x

x

x

x

offensive penalty yards

x

offensive field goal
percentage

x

offensive punt return yards

x

defensive first downs
allowed

x

fi