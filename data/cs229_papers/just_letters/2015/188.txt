predicting answer quality on community q a websites
tin yun ho  ye xu
december      

 

introduction

more than ever before  internet users obtain needed information from community based q a websites such as quora  stack
exchange  and other forums  a critical component of making these sites work is ensuring that the most high quality answers
are pushed to information seekers first  and also ensuring that writers are encouraged informed on how to provide high
quality information  in the longer run  identifying what makes a good answer can also eventually be used to help create
effective ai based q a systems by allowing them to better rank and weight potential answer choices 
in light of this  we decided to create a system that can take any given question posts answer raw text and metadata and
push the highest quality answers to the top of the page in ranking  we formulated the problem in two ways  as a pointwise
and a pairwise ranking problem using binary response variables and evaluated using mean average precision  map   for our
classification algorithms we used regularized logistic regression and regularized linear gaussian kernel support vector machine
classification  for feature selection we used regularization parameters as well as forward search 

 

related work

the most relevant work we found came from the semeval competition of      task    nak      their challenge was to
effectively classify posts in community based q a sites  this time a qatar lifestyle discussion forum  into whether they
answer the question  potentially provide useful information  or are flat out unhelpful  each competitor posted their papers
online  and we reviewed the winning entries as well as some followup papers posted after the competition  a strength that
the top papers shared  tra     hou     nic     was that they went far beyond just using relevant word counts inclusion
as features  including significantly more metadata such as word length number  sentence length number  xml content tags 
etc  they also incorporated a variety of different measures of textual similarity between the question and answer such as
tfidf cosine similarity  word vec cosine similarity  measures of whether the question and answer come from the same topic
distribution  and more  all strengths which we adapted to our problem 
we also found several weaknesses with the winning papers  the first and most obvious one was that they by and large
treated the text itself as a bag of words  and at best a bag of phrases   as opposed to actually teasing out semantic meaning
from statements and determining whether the meaning of those statements actually truly answered the posed questions from
a semantic perspective  the second was that there was no mention of the use of interaction terms  even though one would
expect that the appearance of certain words would only be helpful when answering certain topics or types of questions  the
latter problem we solved through the addition of interaction terms  but the former problem we couldnt come up with a good
way to solve  finally  one major difference between our project and the winning papers was that they were provided with
authoritatively labeled data  while for us  score on stackexchange is at best a noisy indicator of quality  hence motivating
us to create our own response variable  explained below  
later papers we reviewed published after the competition branched out in a variety of ways  one was with the usage
of different types of neural networks  zho      another was to try identifying patterns of dialogues between answers in
response to a given question  bar      both approaches improved performance  but as the former was not a major focus of
cs     and the latter was not as applicable to stackexchanges data  since most questions only had     answers and little
dialogue   we did not apply these approaches 

 
   

experimental setup
collecting data

for data  stackexchange periodically provides anonymized dumps of all user contributed content on their entire network 
this includes  for each of their domains  every single question and answer posted as well as their major attributes such as
id  parentid  to identify the parent question of an answer post   creation date  score  upvotes minus downvotes   view count
 only for questions   title and body full text  tags  favorite count  owners user id  tags  answer count  for questions   accepted

 

fianswer id  comment count  and more  we used the data dump released on august           we are currently starting just
with the ask different dump of posts related to apple products which has        answer posts   inc   

   

creating an appropriate response variable

for the response variable  we sought an objective metric of answer quality  we initially tried using pure score as our metric 
but found that it was highly correlated to a variety of factors irrelevant to quality  for example  the natural log of score is
correlated to age of the post  r          a natural log estimate of the number of views of the answer post ln parentviews  
postage parentage   r          age of the parent question post  r          natural log of the ratio between post age and
parent age ln postage parentage   r          etc  this makes intuitive sensean answer post which has existed for a longer
time and has had more people view its original question post is likely to have a higher score regardless of quality 
thus  we used ordinary least squares  ols  to create a prediction of the response variable  y  based on these nonquality related features for each answer post  then by subtracting the predicted response variable from the real response
variable  ynew   y  y   we get a measure of the part of the response variable that is currently unexplained by the above
non quality related features and thus more likely to purely measure quality 
here  we use the unregularized version of ols implemented in sklearn  which seeks to solve the optimization problem
minw   xw  y       where x is an m  n matrix of real numbers  with each row representing a data point and each column
representing the features  that we wish to control for   w is a n    matrix of real numbers with each row representing the
weight for its respective feature  to be estimated in the optimization problem   and y is a m    matrix with each row entry
equivalent to the raw score for that respective data point  finally  the norm here used is l   combined with the squared
term  it equates to z t z where z is the term inside          bui    
for the original y variable  we set y   log score      if score is zero or positive  and y   score if score is negative  where
 is a positive constant that varies depending on the range of scores for a given domain  the natural log smoothing helps
us deal with the long tail of positive scores and also enables our aforementioned features to have much more explanatory
power correlation to the dependent variable 
for specific features  we chose post age  parent age  parent view counts  order of answer post  as well as each of their
respective natural log transformations  next  by running the ordinary least squares algorithm  we obtained adjusted scores
 ynew   y  y  that were almost completely uncorrelated with these non quality related features  r        for each   normally
distributed around a mean of    and still leave a lot to be explained  r         between y and y   finally  we translate this
value into a very simple and evenly split classification for each sample where helpful posts with score greater than mean are
class    and unhelpful posts with score less than mean are class   

   

pre processing and feature selection

we began by extracting all the question answer posts and metadata from the data dump  converting from xml to python
dictionary format  next  we created the following metadata and text based feature matrices 
 topic tags  log total and binary existence by type for parents  questions  of each answer post
 xml tags  log total  raw count and log count by type  tfidf score  binary existence by type for each answer body text
 question metadata  log   of characters  words  sentences  sum of question words  binary existence of question words
 answer metadata  answer log number of characters  words  sentences
 word vec representations  word vec representation of each question and answer posts body text
 raw text data  lower case unigram and bigram binary existence  raw and logged counts  tfidf scores for answer body
 q a similarity  word vec  tfidf cosine similarity between question and answer body text vectors
 interaction terms   nd degree interaction terms between a dimension reduced matrix of word tfidf frequency features
and a dimension reduced matrix of topic metadata features
for word vec representations  we used the pre trained implementation provided through the spacy toolkit  co     which
is a dependency based implementation of word vec based on  lg     each word in the vocabulary w  w is associated with
a vector vw  rd where d is the embedding dimensionality  the entries in the vectors are latent  and treated as parameters
 
 
to be learned  the probability of a given pair of words existing in the data is measured as p  d     wi   wj       exp v
w vw  
i

j

and the probability they dont exist in the data is p  d     wi   wj        p  d     wi   wj    given a large corpus of pairs of
words observed in the data  wi   wj    d  as well as a list of randomly generated pairs of words  assumed not in the data 
 wi   wj    d    we then maximize log likelihood  using stochastic gradient descent  
 x

x
arg max
log p  d     wi   wj    
log p  d     wi   wj  
vw

 wi  wj  d  

 wi  wj  d

 

fiimplementations differ based on how they decide whether a given pair of words counts as a positive example  traditional
implementations use a context window of k words  if two words appear within k words of each other in the raw text  then
the pair is a positive example  the dependency implementation considers a pair of words a positive example if one word is
syntactically either the immediate head of or modifier of the other word  appearing either immediately above or below each
other in the grammatical parse tree derived from the raw text   the advantage of this is that it goes beyond just capturing
topical similarities between words to also removing spurious correlations between words that just happen to be close to each
other in the text  as well as capturing functional similarities  finally  to represent a given question answer post  we take
the average of the word vec vectors of its individual words  effectively giving us a fixed  compact      variable  numerical
representation for every post in our corpus 
for tfidf score  we used the sklearn implementation  bui      tfidf stands for term frequency times inverse documentfrequency  where term frequency is one plus the log of the natural count of occurrences of a word in any given post and inverse
document frequency stands for   over the number of answer posts a word has appeared in within the entire corpus plus  
to normalize prevent zero division  the advantage over raw counts is that this formulation discounts words that appear
frequently in all posts  and thus likely carry less information  such as the or a   finally  each of these scores is normalized
by dividing by the l  norm calculated over all the values for that feature 
for both word vec and tfidf based similarity  since for both models each post is represented as a fixed length vector  the
v 
representing how
natural way to calculate similarity between different posts is with the cosine similarity metric    vv      v
    
close in pointed direction two posts are to each other in vector space 
next  for reducing feature matrix dimensionality to some target k before creating interaction terms  we use the sklearn
implementations of pca and truncated singular value decomposition  svd   bui      pca involves selecting the top k
pm  i   i t
 
 corresponding to the top k eigenvalues by value   where the x i  values from
eigenvectors of the matrix    m
i   x x
the original x matrix have already been scaled  had mean subtracted and then divided by standard deviation   these top k
eigenvectors correspond to the k vectors for whom when each scaled x is projected onto them  the mean squared projection
distance to origin is the greatest  and thus these eigenvectors maximize the capture of variance from the original dataset in
a lower dimension   then to compute any x in this new reduced dimension space  we just need to take u x  where the ith
row of u corresponds to the transpose of the ith greatest eigenvector derived above 
truncated svd is similar to pca and can be shown to be equivalent to pca if we were to actually subtract feature wise
means from each feature value beforehand  bui      the practical impact of this is that it can be applied to very large
dimensional sparse matrices without densifying them  since it leaves the zeroes as they are   and is thus more appropriate
for our large tfidf feature vectors  which otherwise wouldnt fit in memory if densified   more specifically  this algorithm
creates an approximation of a given feature matrix x of the form x  u v t where x  rmn as before  u  rmk    is
a diagonal matrix in rkk and v t  rkn   u t then becomes our new reduced representation of the x matrix 
finally  we created interaction terms because we hypothesized that specific unigrams bigrams are likely only predictive
depending on the post topic  however  if we exhaustively covered all of the potential interaction terms we would have ended
up with a feature matrix far beyond what our memory could handle  thus  we used dimension reduction as a way to improve
computational tractability  furthermore  we didnt create every interaction term  but only the second degree interaction
terms between matrices of the form xi yj where xi would come from the first matrix and yj would come from the second  in
total  we created   question topic features  using pca  and     word features  using truncated svd   taking interaction
terms left us with      interaction terms 
together these features still number in the millions so wittling them down will be important to preventing overfit  to
improve computational tractability  we ran forward search on groupings of features  for example one grouping would be the
xml tag counts  another would be the topic binary tag features   specific steps are to first initialize f as the empty set  next
run a for loop where for each feature grouping in the list of groupings  let fi equal f  grouping i  then obtain average of
test error from   fold cross validation using regularized logistic regression with c        explained below  on fi   then set f
to be the fi with lowest error obtained from this process  finally  output top feature subset obtained from all cycles 
the following features were included in the optimal subset  in this order   the word tfidf matrix dimension reduced to    
features  we used this to represent the larger tfidf matrix since calculations were taking too long   the xml tag tfidf matrix 
the word vec representation of the answer post  question body text metadata  word vec representation of the question post 
and the log sum of xml tags  to our surprise  interaction terms did not make the cut 

 
   

methods  algorithms and evaluation
pointwise approach

we first implemented a pointwise approach to our ranking problem  where we treat each individual question answer pair as a
training example  then try to learn which class  good answer or bad one  the question answer pair belongs to  we used two
primary algorithms  regularized logistic regression and regularized support vector machine classification with a linear kernel
 linear svc  and gaussian kernel  gaussian svc  as implemented in sklearn   bui       primarily because unlike bernoulli
naive bayes  multinomial naive bayes  or gaussian discriminant analysis  they do not make as significant requirements on the

 

fidistribution structure of their inputs and have been known to perform well on text classification tasks  especially regularized
logistic regression  as andrew ng mentioned in lecture    
pm
  t
 i 
t  i 
regularized logistic regression seeks to solve the minimization problem minw b   w w  c i   log   exp y  w x  
b     where decreasing the size of the c term means that the relative penalty for having larger coefficients versus reducing
error on classifying the data becomes higher  this corresponds to the original logistic regression formulation where we seek
to maximize the log joint likelihood of the data where the data is distributed according to the logistic function  with the
addition that here we also assume that the w values are themselves distributed normally with a mean of    so maximizing
log p  w   and thus some function of     wt w  the same as minimizing    wt w  becomes part of our objective  similarly  note
 
that maximizing the log of the logistic function log   exp z 
is equivalent to minimizing log     exp z   if we expand the
log term  note that in this formulation an l  norm is used as the penalty  but it can also be replaced by anp
l  norm 
m
support vector machine  svm  seeks to solve the primal minimization problem minw b     wt w   c i     i    where
decreasing the size of the c term also implies that the relative penalty for larger coefficients versus misclassified samples
becomes higher  it requires the constraints  y  i   wt  x i      b        i  and   i      i  this corresponds also to the
original formulation of svc where we seek to identify the hyperplane that maximizes the margin between all data points
and the hyperplane  with the adjustment that instead of requiring complete linear separability  we impose a penalty of c  i 
if point i is classified with a margin of less than    or       i  where   i      in this formulation an l  norm is used as the
penalty  next  using lagrangians and kkt conditions we can derive the dual formulation as follows 
max w     

m
x
i  

i 

m
  x  i   j 
y y i j hx i    x j  i
  i j  

p
with the constraints    i  c holding i and i i y  i       and finally  the term in the angular brackets is the kernel 
which we use both a linear formulation for  of form xt z  as well as a gaussian formulation for  of form exp   x  z      
allowing us to implicitly map to an infinite dimensional feature mapping of x 

   

pairwise approach

we also implemented a pairwise approach to our ranking problem  where we train a model to recognize  given a pair of
answers to a given question  which answer is better  this requires transforming the data set so that the new answer pair
feature vectors are just the difference between the two individual answer feature vectors with different class labels from the
same question  the new class label for the answer pair          then indicates which feature vector should be ranked higher 
concretely  assuming there are three answers to a question  the corresponding training set of the question can be denoted
as   y    x      y    x      y    x      where yi  i            is the class label of the question answer pair defined in     and xi  i     
      is the corresponding feature vector  the new answer pair based set of feature vectors then becomes   x   x      x  
x      x   x      finally  the label for  x   x    is    if y    y       if y    y    if y    y    which means these two vectors are
in the same class  we dont include them in the training set   hgo   
finally  we use rankingsvm with linear gaussian kernel and pairwise logistic regression with l  l  norms  both use the
same optimization functions described in      but with the newly transformed features and labels described above 

   

evaluation method

to evaluate our ranking models performance  we treated it as an information retrieval  ir  system that seeks to return
results with the good answers  class    being ranked  and thus appearing  before the bad ones  class     reflecting a real life
situation where we would expect a community q a website to place the best results first on the page  accuracy is then based
on comparing the ranking our ir system returns to the ground truth ranking  with a focus on where the class   answers are
located in the ranking  especially for binary response variables  research literature commonly uses mean average precision
 map  for this purpose  so we do so as well 
specifically  for each question and for every position of a good answer in the ranked sequence of answers sorted by model
score  we calculate an average precision  ap   next  we average ap across all test set questions  mathematically 
 q 
mi
  x   x
m ap  q   
p recision aik  
 q  i   mi
k  

where  q  denotes how many questions in the test set  mi denotes how many answers in the ith question aik denotes a set
of ranked answers from the top answer until k th answer
in order to carry out this evaluation  we needed a dataset organized by questions  where each question had at least two
answers and these answers werent all good or all bad  thus  we removed questions with only one answer and also deleted
questions whose answers are all good  or bad   this left us with        questions and        child answers  next  for each
individual experimental run  we randomly selected     of the questions from the whole dataset as a test set  and applied
  fold cross validation  cv  to the remaining     of the data  we fit algorithms on the training set  used the validation
 

fiset to choose hyper parameters based on map performance  then averaged the optimal hyper parameters obtained on each
fold of cross validation  and applied them to the test set to get an estimated map  we repeated the experimental strategy
multiple times then aggregated the estimated map for each algorithm across all experiments 
finally  in each experiment  we applied both the pointwise approach and pairwise approach  both approaches include
logistic regression with l  l   svm with linear gaussian kernel  to the same training set and estimate hyperparameters
from the same validation data  for logistic regression and svm with linear kernel  the hyperparameter is c  please see
section       for svm with gaussian kernel  the hyperparameters are c and   we used grid search to find the optimal
hyperparameters  our c candidates are from        to      with equal steps in logarithm   candidates are                 
                        

 

results and discussion

we found that the optimal c are quite different between pointwise approach and pairwise approach  figure   shows map vs 
different c for applying different models to training and test set  for example  optimal c of logistic regression  l   is     
from pairwise approach  but      from pointwise approach  see figure    a    furthermore  the optimal  of svm gaussian
kernel is        in most cases  to simplify the visualization of hyperparamter tuning  we fixed           and visualized
trends of map across different c in figure   

 a  logistic regression  map vs  c

 b  svc  map vs  c

figure    outputs versus hyperparameter choice
as the image shows  all the methods we implemented do not have obvious overfitting issue when we chose appropriate
hyperparameters  overall  the pairwise approach did better than pointwise approach in terms of map  between the two
pairwise  or pointwise   logistic regression with l  norm outperformed l  norm  and rankingsvm with gaussian kernel
outperformed linear kernel  for logistic regression  this may be because l  tends to delete insignificant features while l 
may shrink some coefficients close to zero but wont delete them altogether  for svm  the gaussian kernel takes interactions
among features into account which we hypothesized earlier should be important  however  if we consider training time 
logistic regression only took   to   minutes  pointwise svm took   to   hours  rankingsvm took even longer     hours 
since the training size of the pairwise approach is much larger 

 

conclusions and next steps

as variance was not our main challenge  our next step will be to reduce bias  the most obvious way to fix bias is with better
features  currently our features are largely bag of words  not taking into account syntactic ordering   dont try to uncover
semantic meaning  and are just based on the raw text itself  not using any third party authoritative source to determine
whether posts accurately answer questions or not  another direction to consider is  especially as we scale this approach to
larger data sets with millions of posts  stack overflow   trying to use different neural network architectures that can more
effectively capture the nonlinear interactions between features  we began to do this with the gaussian kernel but can likely
do more  seeing as the boost from linear to gaussian kernel on svc was only around     finally  doing any of this will
require moving to a much faster and more parallelized architecture  as time was a major bottleneck for us even with the
current feature set models 
 

fireferences
 hgo   

ralf herbrich  thore graepel  and klaus obermayer  large margin rank boundaries for ordinal regression 
in  mit press  jan        chap     pp          url  http   research microsoft com apps pubs default 
aspx id       

 bui    

lars buitinck et al  api design for machine learning software  experiences from the scikit learn project  in 
ecml pkdd workshop  languages for data mining and machine learning        pp         

 lg   

omer levy and yoav goldberg  dependency based word embeddings  in  proceedings of the   nd annual
meeting of the association for computational linguistics  volume    short papers   baltimore  maryland  association for computational linguistics  june       pp          url  http   www aclweb org anthology p       

 bar    

alberto barron cedeno et al  thread level information for comment classification in community question
answering  in  proceedings of the   rd annual meeting of the association for computational linguistics and the
 th international joint conference on natural language processing  volume    short papers   beijing  china 
association for computational linguistics  july       pp          url  http   www aclweb org anthology 
p        

 co   

syllogism co  spacy        url  https   spacy io 

 hou    

yongshuai hou et al  hitsz icrc  exploiting classification approach for answer selection in community
question answering  in  proceedings of the  th international workshop on semantic evaluation  semeval       
denver  colorado  association for computational linguistics  june       pp          url  http   www aclweb 
org anthology s        

 inc   

stack exchange inc  stack exchange data dump  aug        url  https   archive org details stackexchange 

 nak    

preslav nakov et al   eds  proceedings of the  th international workshop on semantic evaluation  semeval
       denver  colorado  association for computational linguistics  june       url  http   www aclweb org 
anthology s     

 nic    

massimo nicosia et al  qcri  answer selection for community question answering   experiments for arabic
and english  in  proceedings of the  th international workshop on semantic evaluation  semeval        denver 
colorado  association for computational linguistics  june       pp          url  http   www aclweb org 
anthology s        

 tra    

quan hung tran et al  jaist  combining multiple features for answer selection in community question
answering  in  proceedings of the  th international workshop on semantic evaluation  semeval        denver 
colorado  association for computational linguistics  june       pp          url  http   www aclweb org 
anthology s        

 zho    

xiaoqiang zhou et al  answer sequence learning with neural networks for answer selection in community
question answering  in  proceedings of the   rd annual meeting of the association for computational linguistics
and the  th international joint conference on natural language processing  volume    short papers   beijing 
china  association for computational linguistics  july       pp          url  http       www   aclweb   org  
anthology p        

 

fi