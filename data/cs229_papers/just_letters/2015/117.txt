handwritten digit recognition via unsupervised
learning
xinyi jiang  huy pham  qingyang xu
december         

abstract

principal component analysis and sparse autoencoder 
as well as better heuristics such as input discretization
and image centralization 
then we improve the initialization process of kmeans  either by the probabilistic k means   initialization or by choosing the centers which minimize certain cost function we construct 
finally  we run the clustering process with certain
modifications such as k medians and k medoids algorithms and gauge their effects 
with the improved feature set and initialization 
our k means algorithm achieves     classification accuracy on average      higher than the original 
spiking neural network is the best performing unsupervised method for handwritten digit recognition
up to date  we reproduced the     accuracy of diehl
and cook        with the brian simulator of snn 
for reference of accuracy  we also run supervised
algorithms such as svm and neural network  svm
yields     high accuracy  and it also reveals some inherent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsupervised approaches  neural network has a surprisingly
low accuracy around      and we find that the fitting
parameters tend to oscillate without converging 

we present the results of several unsupervised algorithms tested on the mnist database as well as techniques we used to improve the classification accuracy 
we find that spiking neural network outperforms kmeans clustering and reaches the same level as the
supervised svm  we then discuss several inherent
issues of unsupervised methods for the handwritten
digit classfication problem and propose several methods to further improve the accuracy 

i  introduction
handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition  currently the supervised learning methods yield much higher classification accuracy than
most unsupervised ones 
despite its high accuracy  supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually labeled  on the other hand  unsupervised approaches
have the potential of being much more efficient to
train and can be generalized to more complicated classification problems such as handwritten alphabet or
chinese characters with low additional cost 
in this project  we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy  we use the mnist database     which consists of        images in training set and        in test
set  each image is a        array of pixels in gray
scale ranging from   to     and labeled with the digit
it represents 
our first approach is the k means clustering algorithm  divided into three stages 
we first preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more effciently  we use feature selection techniques  such as

ii  preprocessing data
    general methods
we first reshape each image from a        array of
pixels into a vector of the same size  the default
metric we used to cluster and classify the images is
the euclidean distance between them  since k means
only assigns the inputs to clusters to minimize the total distance from each training example to its corresponding cluster centroid  there is no way to directly
calculate the accuracy  instead  in any given cluster
s  we choose the digit d which appears the most frequently in s and calculate the ratio of all d present
 

fiin s and take this as the classification accuracy of most relevant structure of each digit  each neuron in
s  the total accuracy is obtained by averaging over the input and output layers corresponds to a pixel in
accuracies of the ten clusters 
the training images  and we test to find the optimal
number of hidden neurons that leads to lowest training errors 
    feature selection
we use the sigmoid function as the activation function for the autoencoder  hence we need to normalthe high input dimensions  if weighted equally  not ize the input coordinates to between   and   so the
only causes the program to run extremely slowly but the autoencoder can effectively reconstruct the input 
also obscures the essential underlying structure of once the training is complete  we step through the
each digit  which is the most crucial in the classifi  training set again and feed the outputs of the hidden
cation process and exactly what our program tries to neuron on each training example as the new input to
learn  hence we first need to reduce the dimension of the k means clustering algorithm 
the input and replace it with a much smaller subset
we find that the optimal hidden dimension for
of most relevant features 
training with      examples is      with accuracy as
towards this end  we implement two algorithms  high as pca  however  the accuracy has a large flucprincipal component analysis and sparse autoencoder  tuation with the hidden dimension  fig      also  we
to compare their accuracies  we train both models find that sparse autoencoder cannot reduce dimenunder the same initial condition of      inputs and sion as effectively as pca since the accuracy drops
with the same number of reduced dimensions 
sharply when the hidden dimension gets below     

      principal component analysis
principal component analysis selects  via the covariance matrix  the kdimensional linear subspace
spanned by the        dimensional inputs with the
largest correlation to each other  in practice  for     
inputs  the accuracy does not vary much when we
lower the input dimension to      and we find that the
optimal number of reduced dimensions is     fig    

figure    accuracy fluctuates with varying hidden
dimensions for sparse autoencoder

    heuristics
      input discretization
the intensity of the pixels  which ranges from   to
     may be redundant and create extra noise in clasfigure    reducing dimension with pca increases sification since the structure of the digits should be
the classification accuracy
largely independent of the intensity  hence  we need
to empirically determine the optimal cutoff in pixel
intensity  discretize the pixel values to   or   and use
      sparse autoencoder
these binary values as the new input for k means 
the sparse autoencoder is a variation of neural netthe optimal cutoff value we find is     for     
work that tries to reconstruct the input through a training examples  with this threshold  the effect of
small number of hidden neurons  which encode the input discretization varies among differently initial 

fiiii  k means
clustering algorithm

ized clustering algorithms  fig    c f  section      
the accuracy of the probablisitc k mean   slightly
improves and the effect is consistent with larger training sets  however  for deterministically initialized
k means with pca  the accuracy is lower with discretization since we have eliminated too much infor      improved initialization
mation from the input so that the new covariance
matrix is less effective in revealing the essential corthe k means clustering algorithm is sensitive to the
relation between the input components 
intialization of the centroids which  if randomly chosen  often converges to local optimum that does not
necessarily yield the best classification 
to improve centroid initialization  we implement
the k means   heuristic     which first randomly
chooses a training sample as the center  computes the
distance d x  of every other inputs to its closest center  and then randomly chooses another center from
the other inputs with probability weighted by d x    
it repeats this process to generate all ten centroids 
we also implement a deterministic heuristic which
computes the cost function for each image i

c i   

n
x

d i  ii  
k   d ii   ik  

pn
i  

figure    the effect of input discretization varies with where d i  ii   represents the euclidean distance bedeterminisitic  left two  and probablistic  right  ini  tween our image and the ith training example  we
then take the ten images with lowest cost function as
tialization 
the initial centroids 
both initialization methods show overall improve      image centralization
ment on accuracy  fig      with k means   slightly
outperforming minimization of the cost function  for
the digit is independent of whether it appears slightly comparison  we also try to initialize the centroids with
to the left or right of the image  we use the transla  the first ten images of the training set 
tional invariance to fix the center of all images at the
same point and reduce uncorrelated variance 
for each image i  we compute its center by the
weighted average of the pixel values

 p
 

 m n    imn

p

m

 m n    imn

   p
 

 m n    imn

p

n

 m n    imn

 
 

where  x  denotes the integer closest to x  we round
to the closest integer instead of rounding down since
the latter can cause the off by   shift which is detrimental to the classification 
we find that the images in the mnist dataset are
already very well centered  computing the weighted
average with respect to the original pixel values and
the discretized binary values both give the same cen  figure    k means   overall outperforms cost function minimization with      trianing samples
ter          for every image in the data set 
 

fi    execution

and communicate with each other by sending spikes
through synapses  we train the model by updating
the weight associated with each synapse depending
on how frequently spikes travel on it  so far snn has
the best performance on handwritten digit recognition and  aided by the brian simulation package and
their code available online  we are able to reproduce
the     accuracy by diehl and cook    
one issue with the current implementation of snn
is that after training  we need to label each neuron
with the digit it represents by showing the snn images of all digits and see how each neuron responds 
in this sense  snn still requires labels of the images
and therefore is not completely unsupervised 

in order to get a good estimate of average case accuarcy and minimize the effect of anomalies in random initialization  we need to run the k means clustering for multiple trials on the same training set to
gauge the overall effect 
each round of clustering terminates when the cost
function  defined as the sum of euclidean distances
squared of all images to their assigned centroids 
converges within       after each convergence  we
choose at random a training example from each cluster as the new centroids and repeat the clustering to
see if it further minimizes the cost function 
the program terminates when choosing new centroids either has not improved the cost function for
kf ix consecutive trials or when it has improved for
kimp trials in total  for the results below  fig      we
set kimp   kf ix     and report the averaged accuracy 

v  supervised
learning algorithms
    support vector machine
we use the liblinear library    to run svm and find
that it yields high accuracy which increases with
training set size  but certain digits such as   and  
have consistently higher error rate  which means that
their structures are inherently harder to classify 

figure    accuracy vs  training set size for regular
k means and k means  
rerunning the clustering in the above process in
general yields lower values of the cost function  however  in practice this does not guarantee a higher clas  figure    error rate of each digit showing certain
sification accuracy  therefore  even with the reduced digits consistently worse than others
input from feature selection  the default metric of euclidean distance still has limited effect in capturing
    neural network
the structural similarity between digits 
our neural network has input and output layers of
the same dimension as the training images  we use
sigmoid function as activation  train the model by
varying the number of hidden neurons and find that it
has overall low accuracy around      close investigation shows that the fitting parameters often oscillate
without converging during each backward propagation update  in a few cases when they do converge 

iv  spiking neural
network
the spiking neural network  snn  is a biologically
inspired model in which neurons evolve in time
 

fihowever  these parameters yield very high error  the tially quantify the structural similarity between improblem persists when we instead use softmax func  ages which may correctly suggest whether they reption to compute activation 
resent the same digit  the current euclidean metric
does not take into account the position of the pixels 
and we tried another metric which scales the pixel values according to the distance from the center  but it
yields even lower accuracy  still  we should try to construct better cost functions whose output effectively
quantifies similarity 
finally  as mentioned in part iv  the current imthe overall performance of each algorithm we imple  plementation of snn is not strictly unsupervised  so
ment is summarized in fig    below 
it still has the disadvantage of requiring large  manually labeled traing sets  based on the work by diehl
and cook  we would like to correctly classify the excitatory neurons in snn without labels  this would
make snn much less expensive and much easier to
generalize into other situations 

vi  discussion and
future work

vii  reference
figure    overall accuracy of each algorithm

   mnist database of handwritten
http   yann lecun com exdb mnist 

we improve k means clustering with pca and
sparse autoencoder to reduce the dimension of the
input  as well as k mean   and cost function to optimize initialization  overall  the improved versions
of k means achieve     accuracy when we train with
     inputs  k means   and pca run fairly quickly
while sparse autoencoder takes longer to train 
one inherent issue with handwritten digit recognition is that when we look at the distribution of digits
inside each cluster after running k means  we usually
find that digits            tend to be much better classified into their respective clusters than       and   
the error rate of each digit in svm  fig     confirms
that the structures of       and   are inherently harder
to classify even in supervised setting 
we believe the underlying cause of this problem
is that the various optimizations we implement to kmeans mainly improve the clustering efficiency  however  as discussed in section      higher clustering efficiency does not guarantee higher classification accuracy if the algorithm cannot correctly tell one digit
from another  therefore  future work should focus on
improving the effectiveness of our algorithm to recognize the structural distinction of each digit
one possible solution  inspired by the generative
learning models  is to first construct prior models to
encode the most essential structure of each digit and
then perform clustering with these prior models as either centroids or reference to how we select centroids 
another remedy to the same problem is to improve
the cost function which is minimized by each iteration of clusteirng  the cost function should essen 

digits 

   andrew ng  lecture notes on neural network
and sparse autoencoder for cs   a 
   peter diehl  matthew cook 
unsupervised
learning of digit recognition using spiketiming dependent plasticity  ieee transactions in neural networks and
learning systems 
   hae sang park  chi hyuck jun  a simple and
fast algorithm for k medoids clustering  expert
systems with applications           
   andrew ng  cs    lecture notes 
   stanford unsupervised feature learning and
deep learning tutorial 
   david arthur and sergei vassilvitskii 
kmeans    the advantages of careful seeding 
soda    proceedings of the eighteenth annual
acm siam symposium on discrete algorithms
pages           
   chih chung chang and chih jen lin  libsvm
  a library for support vector machines  acm
transactions on intelligent systems and technology                     software available at
http   www csie ntu edu tw cjlin libsvm 

 

fi