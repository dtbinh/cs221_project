electric guitar pickups recognition

warren jonhow lee
warrenjo stanford edu

yi chun chen
yichunc stanford edu

abstract
electric guitar pickups convert vibration of strings to eletric signals and thus direcly affect the quality of sounds  in this project  two machine learning methods 
supporting vector machines  svm  and bayesian networks  are applied to classify pickups from sixteen audio features  the result shows that svm with linear
kernel and low penalty term is a good classifier  which has     of both training
and testing accuracy  in addition  bayesian networks  which has slightly weaker
performance on classification  can easily incorporate more variables and lead to
price prediction model of guitars 

 

introduction

pickup devices are electric transducers that captures vibrations of guitar strings and converts them
to electric signals  there are two commonly used pickups  single coil and humbuckers  and they are
shown in figure    ideally  the classification of pickups can be achieved by selecting features from
audio records and learning  since pickups directly affect the sound of guitars  on the other hand 
guitar pedals  such as overdrive effect  would distort the sound and thus decrease the classification
accuracy  therefore  the guitar sound used in this project should be clean and recorded directly from
amplifier or line in 

figure    two guitar pickups  single coil  left  and humbuckers  right 

 

data extraction

in this project  the data extraction consists of two stages  preprocessing and feature extraction  at
first stage  silence and noise are removed from original audio records  since they have no contribution to later machine learning process  this removing process is achieved by audio segmentation
algorithm      which is demonstrated in figure    the top plot shows the original audio record  the
bottom plot demonstrates the audio segmentation algorithm adapts svm to distinguish high energy
 

fiand low energy short term frames  the high energy frames correspond to the desired learning samples  the low energy frames are considered noise or silence and therefore discarded 

figure    demonstration of audio segmentation algorithm  the low energy frames  such as the
rightest one in the figure  are classified as noise silence and thus discarded  the high energy frames
are remained for later learning processes 
after preprocessing  sixteen features are extracted from audio signals  thirteen mel frequency cepstral coefficients  mfccs   spectral spread  spectral centroid and spectral flatness  mfccs are
commonly used in speech recognition systems as short term power spectrum of sounds  spectral
spread is associated with the brightness of sound  spectral spread measures the bandwidth of the
spectrum  spectal flatness represents noisiness of the power spectrum  mfccs and the other three
spectral features in a sound are shown in figure   and figure   

figure    variation of thirteen mel frequency cepstral coefficients with respect to time frames 

 

figure    variation of three spectral features
with respect to time frames

supporting vector machines

after obtaining features  svm is applied to classify two pickups  note that the training data is
arranged chronologically  since the temporal property of music can not be ignored  in our tests  such
arrangement can improve the learning curves 
svm is applied with several kernels and various amount of penalty  the following four plots show
the learning curves of svm with linear kernel  in each plot  the green curve is training score  accuracy  versus size of training data  the blue curve is cross validation score versus size of training
 

fidata  which can be considered as test accuracy  the desired result is that the green curve and the
blue curve converge to the same value  as shown in figures  low penalty c         svm with
linear kernel achieves such convergence 

figure    svm with linear kernel and
penalty c    

figure    svm with linear kernel and
penalty c      

figure    svm with linear kernel and
penalty c       

figure    svm with linear kernel and
penalty c        

svm with polynomial kernel  which is     x  x    x     is also tested  the result is shown in following two plots  it illustrates that penalty does not affect the learning curves under polynomial
kernel  in addition  the learning curves indicate svm with polynomial kernel is over fitting  since
the difference between training accuracy and test accuracy is big 

figure    svm with polynomial kernel and
penality c    

figure     svm with polynomial kernel and
penality c        
 

fitable    applying the learned svm to audio files that come from different players on different
guitars 
testing file name
accuracy sample size
singlecoil   single note 
singlecoil   mixture 
singlecoil   mixture 
humbucker   mixture 
humbucker   mixture 

      
      
      
      
     

   
   
   
   
   

after learning the desired svm  linear kernel and       penalty   the next step is to test on new
audio files      which consist of different pitches  different playing techniques  and different tones 
table   shows the accuarcy of the learned svm on five test audio files  singlecoil  is composed of
only one note and the other four are mixtures of chords and notes  table   indicates that the svm
performed bad on the single note audio file  this matches our expectation  since the svm is learned
from audio files with several chords and notes  in addition  the learned svm has high accuarcy on
the other four audio files  it demonstrates that svm is a good classifier for guitar pickups  even if
the recording data come from different players on different guitars 

 

bayesian networks

bayesian network is a probabilistic graphical model that represents random variables and their conditional dependencies via a directed acyclic graph  it has been widely applied to artificial intelligence  medical diagnosis  etc  however  in this pickup classification problem  there are two challenging points  first  the network structure of bayesian network is not known in advance 
second  data of features are continuous valued 
to solve the problems  the recent research of one team member at the stanford intelligent system
lab has been used  the research applies bayesian statistics with the proposed priors to find the most
probable discretization policy on each continuous variable according to the data of variables in its
markov blanket  in addition  the discretization procedure is incorporated with k  structure learning
algorithm to learn a discrete bayesian network  for more detail  please refer to     
once the discrete bayesian network is learned from the continuous data  the prediction on testing
data is done as follows  assume xn is the categorical variable and  x    x            xn   is the testing
data  then the prediction is made by calculating
p  xn   x    x            xn     p  xn   x    x            xn    
and choosing the value of xn with higher probability  notice that the joint probability on the rhs
can be factorized as
yn
p  x    x            xn    
p  xi   parentxi   
i  

figure    is the learned discrete bayesian network  in order to reduce the runtime  only seven
important features  mfcc  to mfcc   spectral spread  spectral flatness  are used in the learning
process and the upper bound of parents for each node is limited by two  this network has    
accuarcy on training data and     accuracy over all testing data in table   except singlecoil   the
performace is slightly weeker than svm 
although bayesian networks performed worse than svm in the classification problemsm  they have
an advantage which svm can not easily achieve  incorporating with other features and variables
in the network  for example  in the future work  guitar brands and wood materials obtained from
image processing of videos might be introduced to determine price of guitars along with pickup
information  then figure    shows a possible network  where price is assumed be directly affected
by pickups  wood materials  and brands 
 

fim 

m 

m 

sf

ss

m 

m 

pu

figure     the learned bayesian network from seven selected features and the pickup  m stands for
mfcc  ss stands for spectral spread  sf stands for spectral flatness  and pu stands for pickups 

pu

wd
bd
image
process

au
audio process

pz
video file

figure     a possible bayesian network to predict price of guitars from videos  au stands for audio
features  pu stands for pickups  wd stands for wood materials  bd stands for brands  and pz stands
for price  the audio process box corresponds to the network in fig     wood materials and brands
can be learned by image process  pickups  wood material  and brands can be used to predict prices
of guitars 

 

conclusion

in this project  svm with linear kernel is shown to be a good classifier for electric guitar pickups 
for audio files with clean sound and recorded directly from amplifier or line in  svm has    
accuracy  bayesian network  which has weaker performance than svm  has     accuracy and
provides more variety of models  these results are promising  since audio data come from different
players on different guitars with different brands  however  for more general applications  such as
learning from random audio files  the method proposed in the project is not feasible  mixture of
guitar sounds and other audio sources would significantly affect the predict accuracy  therefore  in
the future  a method to distinguish guitar sounds from other sources might be introduced before the
pickup classification problem 
references
    theodoros giannakopoulos and aggelos pikrakis  introduction to audio analysis  a matlab approach 
academic press       
    ted drozdowski  http   www gibson com news lifestyle features en us tone hunting           aspx 
    yi chun chen  tim wheeler  and mykel kochenderfer  learn discrete bayesian network from continuous
data  http   arxiv org abs             submitted to machine learning 

 

fi