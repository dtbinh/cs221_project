pulse type classification for the large underground xenon dark matter
search
kelly stifter
i  introduction
the large underground xenon  lux  detector     is a
particle physics experiment that is searching for dark matter 
the main component of the detector is a large vessel filled
with liquid xenon that acts a target for dark matter particles 
an incoming particle interacts with an atom of xenon and
creates an initial burst of photons and liberates electrons  the
electrons then drift upwards through the liquid xenon and
release a second burst of photons as they cross the liquidgas interface through a process called electroluminescence 
the first and second bursts of photons are called s  and
s   these signals are detected by grids of photomultiplier
tubes above and below the liquid xenon  based on differences
between s  and s  waveforms  it can be predicted whether
the incoming particle interacted with the electron cloud or
nucleus of a xenon atom  the key to detecting a dark matter
signal is accurately separating these types of events 
therefore  it is imperative that s  signals  s  signals  and
other pulse types are labeled correctly with high precision 
the importance of this task is increased because of the vanishingly small proposed rate of dark matter interaction and
the relatively high amount of background events  while the
current classification performed by the lux data processing
software is     accurate  it has several limitations 
for these reasons  i developed a new pulse type classifier
which takes various quantities relating to a pulse waveform 
and predicts a class for the pulse  my algorithm is able
to match the current lux classification in accuracy  while
providing new insights into the physics of the processes
being studied and a confidence level for each classification 
ii  previous work
the current algorithm used by the lux data processing
software utilizes user defined cuts to separate pulse populations  the pulses are plotted using various reduced pulse
quantities  rqs   such as pulse area and pulse width  and
population boundaries are drawn in various parameter spaces
based on user defined cuts defined by simulation of physics
processes  this algorithm works well and is thought to have
an accuracy approaching      its strength is separating
populations that are distinct from one another in parameter
space  but it is known to have issues in boundary regions 
other low background experiments perform event classification in similar ways  the cryogenic dark matter search
 cdms  defines a number of cuts in parameter space that are
based on calibration data  approximately half the calibration
data is used to define the cuts  and the remaining half is
used to calculate the cut efficiencies and estimate expected

backgrounds  it has been shown that the efficiency of these
cuts ranges between             though these cuts allow
the cdms collaboration to make very sensitive predictions
on the existence of dark matter  it is clear that the efficiency
of the cuts could be improved 
to this end  cdms has investigated event classification
through machine learning  brown graduate michael attisha
included his work on the application of neural networks
to event analysis in his ph d  thesis      his goal was to
discriminate between two different types of events using a
multilayer perceptron network  after accounting for overfitting through an empirical bayes procedure  he obtained
significant improvement       over the primary analysis
of the same data  while this is a notable improvement  the
technique is still limited by the fact that the labels for the
training data were deduced through cuts  a process similar to
the one defined above  ideally  the labels would be produced
in some other way  more independent of the physics 
the thought to improve the pulse classification process
using machine learning has also recently come up within
the lux collaboration  uc davis graduate student james
morad has started a working group for machine learning
using lux data  and pulse type classification is the first goal 
he has already implemented a nearest neighbors classifier
using the hand scan data described in sec  iii  unfortunately 
nearest neighbors will share shortcomings with the current
lux algorithms   the main populations will be well defined 
but the contested regions will suffer 
iii  data set
there are five main classes of pulses in this problem 
the first two are s  and s   as described earlier  which are
the signals that lux is looking for  therefore  it is very
important to be able to separate these two pulse types from
each other  and from the different types of noise  s  signals
are typically narrow pulses of medium intensity  while s 
signals are longer and of much higher amplitude 
the next two classes are single photons and single electrons  which are the most abundant classes  these two classes
are produced by the same physics processes that produce s 
and s  signals  they are just deemed too small to include in
the final sample  this can make it difficult for classifiers to
tell the difference  so single photons are often confused with
s  signals and single electrons with s  signals 
the final main pulse type is noise  this class is a catchall for many things  including electronic noise and other
physics processes that are not of interest  examples of all
pulse classes can be seen in fig        and   

fiapproximately      pulses have been classified  the pulses
classified as i dont know were removed from the training
data  so a total of      training samples were used 

 a  s  signal

 b  s  signal

fig     left  s  signals are shorter and less intense than s 
signals  right  s  signals are longer and more intense than
s  signals 

 a  single photon  sphe 

 b  single electron  se 

fig     left  single photons are small  single peaks produced
through the same physics process as s  signals  right 
single electrons are long collections of small peaks produced
through the same physics process as s  signals 

 a  pmt noise

fig     principle component analysis performed on the luxclassified data for the purposes of visualization  the various
colors represent the   possible classifications for pulses  red
is s   orange is s   green is sphe  light blue is se  and
dark blue is noise 

 b  electron burp

fig     two examples of noise  left  electronics noise from
a pmt  right  a physics process called an electron burp 

from each event in lux data  there is a list of rqs that
have been calculated by the lux data processing software 
fifty of these rqs are specific to individual pulses  they
include things like pulse width  pulse area  etc  of the   
pulse rqs     were viable for use as features  meaning they
differed between samples and were never infinity  this subset
of features was considered for use in the final classifier 
before use in training  all the features were normalized to
have a mean of zero and a standard deviation of one 
there are two sources of existing labels for this data  the
first is from the lux data processing software  the second
is a hand scan of pulses that was performed by various lux
scientists  principle component analysis was performed on
the data for visualization purposes  and can be seen in fig   
use the lux labels  and   using the hand scan labels 
james morad created a website     for the purpose of
hand classification by lux scientists  users are shown a
pulse and asked to select one of eleven classifications  one
of the options being i dont know  the remaining ten
classes consist of the five main classes described above  as
well as five sub classes within the main classes  to date 

fig     principle component analysis performed on the handscan data for the purposes of visualization  the various colors
represent the    possible classifications for pulses 

iv  methods
a number of classifiers were tested on this data set  three
multi class classifiers were used  naive bayes  nb   support
vector machines  svms   and random forests  rfs   i also
defined two classifiers myself  which i denote combined
with base  cwb  and combined without base  cwob  
the first multi class algorithm used was naive bayes 
which estimates the probability of each class using bayes
rule 
p  x y p  y 
p  y x   
p  x 
in the above equation  y is the label for the training sample
and x is the feature vector  the naive assumption that all

fifeatures are independent leads to 
qn
n
y
p  y  i   p  xi  y 
p  y x   
 p  y 
p  xi  y 
p  x 
i  
finally  the label that maximizes this probability is chosen 
y   argmaxy p  y 

n
y

p  xi  y 

i  

the specific version of naive bayes that is implemented
in the scikit learn python package     assumed that the
likelihood of each feature was gaussian  which is quite close
to the truth  with this  values for p  y   and p  xi  y  can be
estimated by maximizing the joint likelihood of the data 
l y j   x k l    

m
y

p  x i    y  i     

i  

m
y

p  x i   y  i   p  y  i   

i  

the second multi class algorithm used was an svm 
svms define an optimal margin between two classes by
solving a dual problem to maximize the distance between
training samples and the classification boundary 
max w     


m
x

i 

i  

m
  x  i   j 
y y i j hx i    x j  i
  i j  

s t     i  c  i             n
and

m
x

i y  i     

i  

for the case of this problem  a gaussian kernel was used  so
the inner product in the equation above is replaced with 
hx i    x j  i   exp  x i   x j      
this algorithm provides a classification between two classes 
in order to support a full multi class classifier  a one vs one
approach is taken  a separate classifier is built for each pair
of classes  and the results from all classifiers are averaged 
the final multi class algorithm used was a random forest      the backbone of a random forest is a collection of
decision trees  a tree is created by splitting the training set
into subsets based on some criterion on the features that
defines the best splitting for that set  the process is then
continued  and each subset is split into further subsets using
the same criteria  here  the criterion used to split sets was
minimizing the gini impurity  this is a measure of how likely
a random sample from the subset is to be mislabeled  if it
were randomly labeled based on the existing distribution of
labels in the subset  using the relative frequency of class j
at node t  p  j t   the gini impurity can be calculated as 
x
g t      
p  j t  
j

the idea of a random forest is that each tree in the
collection is trained not only on a randomly selected subset
of training samples  but also on a randomly selected subset
of features when splitting a node  the scikit learn implementation combines the results from all trees by considering the
probabilistic prediction for each class 

the cwb classifier is the first of two algorithms that
i defined  this classifier takes five individual classifiers 
one base multi class classifier which classifies all data into
all classes being used  and four binary classifiers  denoted
experts  which perform a one vs rest classification by separating classes one through four from the rest of the data 
the result from the base classifier is taken as the
foundation of the labels  in most cases  only one of the
experts will select a training example for its class  and the
base classification is overwritten by the expert  in the case
that two or more experts claim it  rules are introduced to
break ties  if a sample is classified as both class one and
class three  it is assigned to class one  similarly  if a sample
is assigned to class two and class four  it is assigned to class
two  this decision is informed by the fact that it is worse to
miss an s  or s  signal than it is to analyze an extra noise
signal  and also by the fact that it was found in a series of
small experiments that the correct label in these cases was
always class one or two  and never class three of four  in all
other ties  whether they are two   three   or four way   the
classifier defaults to the value given by the base classifier 
the cwob classifier is very similar to the cwb classifier 
except that it has no base classification  it takes only four
binary classifiers to define the first four classes  any sample
that is not claimed by one of the four experts  or any tie that
is not broken by the rules described by the cwb classifier 
is instead simply classified as a fifth  noise class 
v  results
one way to quantify the success of a classifier is accuracy 
the percentage of training samples that are labeled correctly 
it would unfair to train an algorithm on a set of data  and
then test against that same set  for this reason  an extra step
called cross validation is taken when measuring accuracy 
cross validation involves splitting the data set into k
subsets  for this problem  the data was split into four folds
using a method called stratified cross validation  in which the
percentage of samples from each class is preserved in each
subset  this was done because some of the classes have far
fewer samples than others  and a subset that lacks a class is
not ideal  the classifiers are then trained on three of the four
folds of data  and tested on the remaining fold  this is done
four times  holding out a different fold each time for testing 
using the accuracy of the training and testing sets  a
learning curve can be created  a learning curve plots the
training error and the testing error as a function of the
number of training samples 
a confusion matrix is a table that makes it clear if two
classes are being confused with one another  the  i  j 
element of the matrix represents the number of samples that
are predicted to have label j  and are actually labeled i 
precision and recall are two other common measures of
success of a classifier  precision is defined as the number of
correctly predicted positives divided by the total number of
predicted positives  and recall is defined as the number of
correctly predicted positive divided by the total number of
actual positives 

fia  results based on hand scan data
the hand scan data was first classified using several offthe shelf classifiers  implemented in the scikit learn package 
additionally  one of the classifiers that i defined  the cwb
algorithm  was also tested  both variations of this algorithm
used rf experts  but one used an rf base while the other
used an svm with an rbf kernel as the base  these were
implemented with default values for the hyper parameters 
the accuracy of each classifier  as computed by   fold
stratified cross validation  is shown in table i 
naive bayes
svm
rf
cwb  rf base  rf experts
cwb  svm base  rf experts

no surprise that all samples from class   are mis classified
as class   
more problems are evident when the learning curve 
shown in fig     is plotted 

        
        
        
       
        

table i  accuracy of algorithms in classifying hand scan
data 
the algorithm with the highest accuracy was the cwb
classifier with an svm base and rf experts  so this one
was selected for further investigation  a python package
called optunity     was used to perform hyper parameter
optimization on the rf experts to select values for the
number of trees to create in each forest and the maximum
number of features to consider at each node  additionally 
feature selection on the svm base was performed  and it was
found that while there are only four features that significantly
contribute to the classification  the optimal number to use
is seven  even after these optimizations were implemented 
the accuracy did not increase by a statistically significant
amount  a sample confusion matrix is shown below 

  
 

  

 

 

 

 

 

 
 

    
  
 
     
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
   
  
   
  
   
       
 
    
 
   
 
   
 
   
 
   
 
   

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 


 
 

 

 

 

 

 

 

 
 

fig     confusion matrix for the cwb classifier with an svm
base and rf experts  created by training the classifier on
    of the training set  and testing on the remaining     
as can be seen  classes   and   are often confused with one
another  as are classes   and    the experts were supposed
to address this issue  but even the experts can not distinguish
a single class from the rest of the data  there are other
problem areas  as can be expected with unbalanced classes 
it is especially problematic since some of the smaller classes
are technically sub classes of one of the four main classes 
for example  class   is a sub class of class    therefore it is

fig     learning curve for the cwb classifier with an svm
base and rf experts  note that the training error increases
as the number of training samples increases  the two curves
seem to be converging to a value that is far from   
this learning curve indicates that the algorithm simply
isnt sufficient to classify this data  this can be inferred
because the testing error actually increases as the number of
training sample increases  and the training and testing error
seem to be converging to a value that is far from    this
was rather surprising  since the lux classification achieves
close to     accuracy 
after considering the pca performed on the data  and the
fact that there is a stark difference between the hand scan
and the lux classification labels  it can be inferred that
the labels for the hand scan data are flawed  while this is
good feedback for the lux collaboration  it meant that the
classifier would have to be trained on the lux classification
data  which is the classification that is being tested against 
b  results based on the lux classification
the same three off the shelf classifiers we re trained using
lux classification labels  the cwb classifier with an rf
base and rf experts  as well as the cwob classifier  with rf
experts were also trained  the accuracy of each classifier  as
computed by   fold stratified cross validation  is shown in
table ii 
naive bayes
svm
rf
cwb  rf base  rf experts
cwob  rf experts

        
        
          
          
          

table ii  accuracy of various algorithms in classifying the
lux classification data 
even with no optimization  the classifiers perform much
better on this data  this was somewhat expected because
it is easier to split data into only five classes instead of

fiten  but it also indicates that the population of each class is
more separated in parameter space than it was in the handscan data  the cwob classifier with rf experts was selected
for further investigation because it performed with a high
accuracy  allowed great freedom over hyper parameters  and
left room for further algorithm enhancement 
the hyper parameters of each expert classifier were optimized using the optunity package  feature selection was
not performed on the expert classifiers since the rf method
is relatively insensitive to non predictive features  after optimization  the classifier achieved an accuracy of            
a sample confusion matrix is shown below 


  
 
 
 
 
      
 
 
 


 
 
   
 
 


 
 
 
      
 
 
 
 
  
fig     confusion matrix for the cwob classifier with rf
experts  created by training the classifier on     of the
training set  and testing on the remaining     
now  the experts are taking care of all major problem
areas that were present before  still  an occasional signal gets
mistaken for noise  it is entirely possible that these pulses
were actually mis classified by the lux classification  and
that this classifier is labeling them correctly  this requires
further investigation by the lux collaboration 
another metric of classifier success is the precision and
recall for each class  which can been seen in table iii 
class
 
 
 
 
 

precision
    
    
    
    
    

recall
    
    
    
    
    

table iii  precision and recall values for each class  using
the cwob classifier with rf experts 
the precision and recall values only serve as further proof
that the classifier is performing well  the weakest point is the
precision of class    this means that pulses that arent noise
are being classified as noise  or that the experts are missing
things  if it is found that these pulses are not mis labeled  this
could be addressed by penalizing the incorrect classification
of signal as noise more heavily than the opposite 
the learning curve  shown in fig     shows an increasing
trend in the cross validation prediction rate as the number of
training samples increases  this indicates that the classifier
could benefit from more training samples  the low rate of
generalization error also indicates that the classifier is likely
not over fitting the data  this is not surprising  since the rf
method is not prone to over fitting 
though the cwob classifier does not exceed the lux
classification in accuracy  it does provide some useful information that the current classification process does not  first 

fig     learning curve for the cwob classifier with rf
experts  the small difference between the training and cv
error shows that the classifier is not over fitting the data 
additionally  the increasing trend of the cv prediction rate
indicates that the classifier would benefit from additional
data 
it gives insight into which features are actually important
in classification by calculating feature importances  this can
aid in understanding what differentiates the physics processes
that create these signals  additionally  the classifier can also
give probabilities for a pulse belonging to each class  this
would prove useful in the overall data processing software 
especially in the event building step 
vi  conclusion
one notable insight provide by this study is that humans
are bad at doing this classification by hand  if hand scan data
is going to used to train a classifier or expand the current
classifications  it should be labeled only by highly trained
scientists and its correctness would need to be verified 
the cwob classifier with rf experts performs very well 
with an accuracy of             and could very likely
be improved with the addition of more training samples 
this matches the     level of accuracy that the current
lux classification achieves  but also gives new  additional
information  including the most important features in each
classifier and a prediction probability for each class and each
pulse  which can be used in the larger analysis framework 
there are may directions that future work could take 
the current classifier could be improved or expanded by
considering additional ensemble methods  such as extremely
randomized trees  adaboost  or gradient tree boosting 
the rules for the combined classifier could be adjusted
to account for the various probabilities assigned by each
classifier  or the individual decision trees could be changed
to consider a different criterion in splitting nodes  taking a
completely new route  the performance of a neural network
could be investigated  or learning could be performed on the
time series of the pulse waveform itself 
acknowledgments
many thanks to james morad for providing the hand scan
data set  and to mitchell mcintire for his helpful insights 

fir eferences
    d  s  akerib  et al   the large underground xenon  lux  experiment  nuclear inst  and methods in physics research  a     pp     
            
    m  woods  a comprehensive study of the large underground xenon
detector  ph d  dissertation  university of california   davis       
    m  attisha  cryogenic dark matter search  cdms ii    application
of neural networks and wavelets to event analysis  ph d  dissertation  brown university       
    j  morad  plux  http   plux physics ucdavis edu 
    pedregosa  et al   scikit learn  machine learning in python  jmlr 
    pp                  
    l  breiman  random forests  machine learning                    
    m  claesen  et al   http   optunity readthedocs org

fi