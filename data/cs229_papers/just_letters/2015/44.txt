cs     final report

 

object classification for autonomous vehicle
navigation of stanford campus
heather blundell and sarah m  thornton
email   hrblun smthorn  stanford edu

abstractwith automated vehicle technologies rapidly advancing  fully automated vehicles may not be far behind  for
an autonomous vehicle to properly navigate the environment 
it will need information about objects it is likely to encounter 
an affordable and popular source for this information is taken
from camera images  we apply and compare different supervised
learning techniques to images for object classification  particularly  we investigate softmax regression  support vector machines
and convolutional neural networks  we generate baseline results
using the cifar database  and then move to a larger dataset
from imagenet to improve model accuracy  the models trained
and validated on imagenet are then tested against our own
test data generated from gopro footage of driving around the
stanford campus 

i  i ntroduction
s automotive manufacturers incorporate more and more
automation into passenger vehicles  it seems inevitable
that fully automated vehicles will soon be upon us  many
autonomous vehicle platforms     are outfitted with cameras 
which are low cost devices that capture the surrounding
environment similarly to human eyes  camera images capture
rich information about the environment an autonomous vehicle
traverses 
we propose an object classification problem using camera
images of objects an autonomous vehicle is likely to encounter 
in particular  we bound the driving scenario to navigating
stanford campus around other vehicles and students  to accomplish this task  we will compare different supervised learning techniques with varying features for image recognition 
one supervised learning technique we investigate is a support
vector machine  svm   which allows for high dimensional
feature kernels in case data is not linearly separable  another
supervised learning technique we propose to use is softmax
regression because it is a generalized logistic regression algorithm and the output is a probability of the label  in addition
to comparing these two learning techniques  we will discuss
the impact of varying features to the learning algorithms 
in particular  we will look at comparing the features of
rgb values and gray scale values  lastly  we will discuss an
alternative supervised learning approach using a convolutional
neural network  which does not require manual specification
of features 
in order to focus the problem on the goal of assisting
autonomous vehicle navigation on campus  the project will be
limited to classification of items most likely to be found around
the stanford campus  we include baseline results from training
and validating on the cifar dataset for nine classes  bus  car 

a

bicyclist  motorcycle  pickup truck  construction truck  caltrain  pedestrian and tree  we also form results from training
and validating on an imagenet dataset for five superclasses 
bicycle  people  sign  tree and vehicle  the imagenet models
are tested against images we gathered from gopro cameras
while driving around the stanford campus 
ii  r elated w ork
while the work we present in this paper looks at object
classification for a broad range of objects  there are those
that focused on an even smaller subset of computer vision
algorithms for autonomous vehicles  for example  traffic signs
are part of the rich information captured by a camera mounted
in a vehicle  autonomous vehicles can take advantage of the
current infrastructure if they are able to properly identify and
read these signs  through the use of a genetic algorithm and
neural network  de la escalera et al      demonstrate the ability
to not only recognize and classify traffic signs but also the
condition of the sign at various times of day 
camera images also capture other vehicles as well as
vulnerable road users such as pedestrians and bicyclists  when
cameras are mounted to the infrastructure  messelodi et al     
leverage minimal changes to the background and road plane in
order to create a real time vision system capable of detecting
and tracking vehicles on the road and estimate the respective
speed  they classify seven categories of bicycle  motorcycle 
car  van  truck  urban bus and extra urban bus  and use a
combination of model based and feature based techniques to
help distinguish between similar models such as motorcycles
and bicycles  since autonomous vehicles may require traveling
through non urban intersections  gavrila     uses a vehicle
mounted camera to detect pedestrians by implementing a realtime shape based object detection system which extends the
chamfer system and filters false positives using a radial basis
function based verification method 
more recently  standards for benchmarking object classification have surfaced through machine learning competitions
such as kaggle     and the pascal visual object classes
competition      although these competitions require a larger
number of classes to identify  they showcase advances in
machine learning to this greater variability in the dataset 
for imagenet  both litayem et al      and maji et al     
use support vector machines  krizhevsky et al      show
convolutional neural networks perform well on imagenet  in
the following sections  we show results of both support vector
machines and convolutional neural networks on imagenet 

fics     final report

 

iv  s oftmax r egression and svm

iii  datasets
a  cifar

a  features

for our baseline object classification dataset  we use select
classes from cifar    and cifar           cifar    includes      color images of    classes with      images per
class  while cifar     includes     classes but with only    
images per class  since we are using both datasets  we chose
to select the minimum size of images per class to prevent a
skewed dataset  thus  we only sample     images per class
out of the available      from cifar    
since we are focusing on the application to autonomous
vehicles  we only use classes representative of likely objects
the vehicle may encounter on the road  from cifar     we
use classes automobile and truck  from cifar      we use
nine classes  people  trees  bicycle  bus  motorcycle 
pickup truck and train  for each of these classes  we
partition our     available images for that class into    
images for training and     images for validation testing 

in this report  we only look at using rgb values and grayscale values as features for softmax regression and an svm 
   rgb  the rgb values are the default feature provided
by cifar and imagenet  and are provided as three   bit
values for each pixel  for the rest of the paper  we focus on
images gathered from imagenet and will refer to cifar for
comparison with our milestone report  since the images are
       rgb images  each feature x i  in the training set
 r        
   gray scale  to obtain the gray scale information  we
transform the rgb values using a weighted sum of the r  g
and b components      

b  imagenet
for a larger number of examples per class  we gather images
from imagenet       in our project milestone  we found the
number of classes and number of examples per class spread
the data too thin for proper object classification  thus  for
the imagenet data  we collapse the number of classes to
five superclasses  bicycle  people  sign  tree and vehicle  even
though imagenet provides us with many more examples  they
are not consistently processed like cifar  we address this by
scaling the images to     pixels and then cropping them into
       images  for our training set  we gather       images
per class  the validation set contains       images per class 

x i          r i          g i          b  i 
where r i    g i  and b  i  are the  r       vectors corresponding to red  green and blue values of the pixels for feature
x i    thus  each gray scale feature x i  in the training set is
now  r        
b  softmax regression
object classification of five superclasses is a multi class
classification problem  so we turn to an inherently multi class
algorithm known as softmax regression  softmax regression is
logistic regression for multinomial data  we use the python
library scikit learn      to train a softmax regression model
and use it to predict on our validation set  since we are
using logisticregression   from scikit learn with the
multinomial setting  it uses solvers that only support l 
regularization with primal formulation  our softmax regression
problem has the following form  with weight decay  

c  gopro footage
both the cifar and imagenet datasets are used for training
and validation  we created our own dataset for testing  to do
this  we mounted two gopros to the interior of a vehicle  we
drove the vehicle around the stanford campus with the cameras
recording at    frames second for about one hour  providing
us with a total of approximately two hours of footage  in
processing of the footage  we grabbed one out of every   
frames for analysis  we selected images that captured objects
in the five superclasses used from imagenet  bicycle  people 
sign  tree and vehicle  each of these images were then passed
through a segmentation algorithm      to further parse them
into the desired classes  the segmentation process outputs the
images in various resolutsions and sizes  so similarly to the
imagenet dataset  we scaled and cropped them to       
pixel images  we successfully assembled     images per
superclass  the number of examples from each dataset are
summarized in table i 
table i  dataset sizes per class
dataset
cifar
imagenet
gopro

train
   
     
 

validate
   
     
 

test
   

 
j     
m

  m k
x

t  i 
n
o
ej x
  y  i    j log pk
lt x i 
i j  
l   e

 
 

k n
 x  
ij  
  i j  

   rgb  using       training examples from each class 
we trained a softmax regression model and obtained an accuracy of         when the model is tested on the      
example validation set  we see a degradation in performance
by almost    with an accuracy of         on the cifar
dataset  we had a degradation of about     between training
and validation 
   gray scale  in comparison  training a softmax regression model on gray scale features obtained an accuracy of
        when tested on the validation set  the accuracy again
decreased to         overall  using gray scale features seems
to lose information about the image compared to using rgb
features 
c  svm
another approach to multi class classification is one vsrest  where one class is positive and all other classes are
negative  thus  creating k number of models  one for each
class  using scikit learn  multi class functionality is built in
to their several svm implementations  for the baseline  we

fics     final report

 

fig     both training and validation errors start to converge to
a large error for both cifar and imagenet  gray scale valued
features always perform worse than rgb valued features 

 a  rgb feature on validate set 

 b  rgb feature on test set 

 c  grayscale feature on validate set   d  grayscale feature on test set 

fig     validation errors for both cifar and imagenet remain
high despite increase in training set size  similarly to softmax 
gray scale features perform worse than rgb features 

 a  rgb feature on validate set 

 b  rgb feature on test set 

 c  grayscale feature on validate set   d  grayscale feature on test set 

fig     normalized confusion matrices for softmax 

fig     normalized confusion matrices for svm 

use linearsvc    which defaults to using the linear kernel 
the other default settings use l  regularization with dual
formulation and c      so we have the following problem
formulation 
m
m
x
  x  i   j 
max
i 
y y i j hx i    x j  i

 
i  
i j  

the image features by training a convolutional neural network  cnn   given that naturalistic stimuli in images are
multiparametric  cnn models can learn many parameters
and dynamics not captured by simpler models  moreover 
overlapping receptive fields in the convolutional layer allow
for sensitivity to small sub regions of the i  note that in this
image classification task  our oracle would be for the selfdriving cars visual classification system to match the precision
of a humans manual image classification 
we implemented an eight layer cnn architecture to classify our image dataset using the keras theano based deep
learning library       which achieves nearly      accuracy
on our larger imagenet training set and also generalizes well
on our held out validation set data  a detailed description of
our model architecture  layer by layer  is given as follows 

s t 

   i  c  i              m
m
x
i y  i      
i  

   rgb  we train this linear kernel svm using the      
training examples from each class and achieve an accuracy of
        unfortunately  the validation set produces an accuracy
of         which indicates our svm is over fitting when using
rgb features 
   gray scale  comparing with gray scale features  we
achieve an accuracy of        using the training examples 
the validation set obtained an accuracy of         the results
of gray scale features to rgb features  again  shows better
performance with using rgb values 
v  c onvolutional n eural n etworks
besides support vector machines implicitly representing
high dimensional features  we can also automatically learn

a  architecture
many competing factors influenced our model decisions 
of which the most important consideration was avoiding
overfitting to the training data  it was necessary to use a model
with complexity on par with the complexity of the training
data  hence  as we increased the size of our dataset with
more imagenet images and higher resolution             we
found our models to be less prone to overfitting than with our
small cifar dataset  we based our model on a vgg style

fics     final report

 

cnn  but ran many experiments to tune the parameters to fit
our   class imagenet dataset  the vgg style architecture of
simonyan and zisserman      found that the depth of a cnn is
important to its accuracy in the large scale image recognition
setting  and they also used very small        convolution
filters  therefore  our architecture has four  d convolutional
layers with small        filters  the first such layer consists
of           convolutional filters  next  our model applies a
relu nonlinearity  defined by relu x    max    x   which
helps to ensure sparse activations  this layer is immediately
followed by another  d convolutional layer and a relu
nonlinearity  followed by a max pooling layer which downsamples the input by   along both the width and height
dimensions  the function of the max pooling layer is to reduce
the spatial size of our input  which reduces the number of
parameters  and hence controls overfitting  we next use a  d
convolutional layer but with more filters          filters 
followed by a relu  followed by another  d convolutional
layer of this size and a relu  followed by a max pooling layer 
finally  we flatten the output of the last max pooling layer into
a vector and pass it through two fully connected layers  the
first of which has     neurons and the last fully connected
layer has   neurons  the number of classes  in order to output
the cnns predictions y  in total  our cnn architecture is
relatively deep  consisting of   layers  as described above  our
objective function was categorical cross entropy  defined for
the truth y  rn and the cnns prediction y  rn   where n
is the number of classes  as
cce y  y    

n
x

yj log yj   

j  

which is a good loss function for multi class classification
problems and softmax output units 
b  weight initialization and optimizer
as these are both critical for the cnns performance  especially considering that the objective function is non convex  
we experimented with several choices of weight initialization
and optimizer  initially for our choice of weight initialization 
we used glorot uniform       a commonly used initialization 
if we let nin and nout be the number of neurons feeding into
and feeding out of the weights of a layer  respectively  then
glorot uniform initializes these weights randomly according to
a uniform distribution scaled by    nin   nout    however  we
found that what worked best was a he uniform initialization 
recently introduced by he et al        which initializes the
 
 
weights according to a uniform distribution scaled by
nin
the authors report that this type of initialization works better
for relu nonlinearities  which our model uses   whereas
glorot uniform is more appropriate for sigmoid and tanh
nonlinearities  without this uniformly random initialization
of weights in the first convolutional layer  we found that
our model would classify nearly all training  and validation 
images into one or two of the five classes  which we believe
was a problem with a local optimum  also  to further address
this problem  we used sgd with momentum as our optimizer 

the results of sutskever et  al      suggest that first order
nesterov momentum methods are key to avoiding sgd getting
stuck in poor local optima  momentum serves as an additional
term added to the sgd gradient updates to help speed up
convergence and make sgd more stable by avoiding the
problem of vanishing gradients  with momentum   sgd
updates the parameters  of an objective function j   as
follows 
vt     vt  j t  
t     t   vt  
we found that the nesterov momentum parameter  that gave
our model the best generalization error was      when our
sgd learning rate was      with decay       and using a
mini batch size of      we initially experimented with minibatch sizes of    and      but found this intermediate minibatch size to work best since it is small enough to help avoid
poor local optima  but also large enough to avoid difficulty
converging to a good optima due to gradient noise 

c  decisions to control overfitting and biases
to prevent overfitting  we used dropout  recently introduced
by       after two of our  d convolutional layers during
training  more importantly  we used dropout after our large
    neuron fully connected layer  as this layer is responsible
for most of the parameters in our cnn  here  we set     of
input units randomly to   at each update during training time 
which helped to balance the complexity of our parameters with
the nature of our imagenet dataset  we also applied dropout
after two of our  d convolutional layers  but at a probability
of only      instead of     because the parameters introduced
by these layers are fewer in number  however  the cnn still
overfit slightly to the training data  which we hypothesize was
due to its nevertheless large number of parameters 
moreover  we used an l  weight regularization penalty of
     on both of our dense layers  as mentioned in the previous
section  our cnn was more biased to sparse representations
in its image classification  from our observations  adding
l  regularization discouraged this imbalanced classification 
giving us confusion matrices with few images in our datasets
predicted off of the diagonal  see fig     

fig     example cnn misclassifications  on the left  a validation example of a sign misclassified as a tree  on the right 
a test example of people misclassified as a tree 

fics     final report

 

table ii  accuracy for different models on imagenet
model
softmax rgb
softmax gray
svm rgb
svm gray
cnn

fig     cnn training loss decreases over     iterations 
especially during the first    iterations  the cnns validation
loss also decreases in the first    iterations  and then increases
only slightly due to overfitting 

 a  training set 

 b  validation set 

 c  test set 

fig     normalized confusion matrices for cnn 

vi  d iscussion
for softmax regression and svm  we notice a trend when
comparing using rgb values and gray scale values for features in that rgb values overall outperform using gray scale
values as seen in figs    and    when looking at training vs 
validation error for softmax regression compared to svm  we
see softmax regression has less of a discrepancy between the
two errors  this suggests our softmax regression model suffers
from high bias  we can see the svm has high variance because
the training error is significantly lower than the test error  even
though we increased the set size from cifar to imagenet 
there is no improvement in the validation error  this suggests
the need to explore other feature representations of the data 
most likely one independent of scaling or rotation of objects
in the image patch such as sift      or surf      
as for our cnn  although it started to overfit slightly
around    iterations  the decreasing training loss of our

training
accuracy    
    
    
    
    
    

validation
accuracy    
    
    
    
    
    

test
accuracy    
    
    
    
    
    

model demonstrates that it can most effectively learn from
our training set given its hyperparameters  however  when
tested on our stanford gopro footage  the cnn had very
low accuracy on people and signs  see fig   c   predicting
most people as bicycles or trees and predicting most signs
as trees  this behavior is likely because  given the nature of
stanfords campus  many of our test images featured people
riding bicycles or walking by trees and most signs are by trees 
so  this lack of mutual exclusivity of our classes with respect
to our test set made choosing a single class difficult 
a comparison of the cnn and all the models applied in
the paper is summarized in table ii  we see cnn performed
the best with an almost perfect training accuracy and almost
double the validation accuracy of the other models  since
the test accuracy of the cnn and the other models was not
particularly high  this suggests more post processing of the
gopro footage and our datasets would improve the scores 
with more computational resources  our future work would
consist of performing zca whitening on the datasets which
is shown to reduce high correlation between adjacent pixels
      as can be seen in fig     even our cnn had difficulty
classifying images that contained multiple classes in one
image  even though we labeled the right image in fig    as
people  we could have instead labeled it as tree  the fact that
the cnn labeled it as tree gives insight into how its predictions
depend on what are the most prominent features of the image
 here  the tree is largest   for future work  we could help to
resolve this issue by further cropping the images or using
bounding boxes 

vii  c onclusion
we produced results for object classification of five superclasses of objects an autonomous vehicle may encounter while
driving along stanford campus by training and validating on
imagenet and testing on our set gathered from gopro footage 
we analyzed three supervised learning techniques  softmax
regression  support vector machines and convolutional neural
networks  our   layer cnn exhibited the best classification
accuracy of our three methods  in comparison to rgb and
grayscale features for softmax and svm  for our desired
application  it is critical that an autonomous vehicle is able
to classify unseen examples with high accuracy  and our cnn
model is able to to do that on the validation set  to put this
onboard an actual vehicle  we would need to further improve
processing of real time images to pass to the cnn for proper
classification  to be safe on the road  an autonomous vehicle
should be able to recognize multiple objects in a single image 

fics     final report

 

r eferences
    j  m  gitlin  face to face with fords self driving
fusion hybrid research vehicles  august           online  
available  http   arstechnica com cars         face to face with fordsself driving fusion hybrid research vehicles 
    a  de la escalera  j  m  armingol  and m  mata  traffic sign recognition and analysis for intelligent vehicles  image and vision computing 
vol      no     pp               
    s  messelodi  c  m  modena  and m  zanin  a computer vision
system for the detection and classification of vehicles at urban road
intersections  pattern analysis and applications  vol     no       pp 
           
    d  m  gavrila  pedestrian detection from a moving vehicle  in computer visioneccv       springer        pp       
    a  goldbloom  data prediction competitionsfar more than just a bit
of fun  in data mining workshops  icdmw        ieee international
conference on  ieee        pp           
    m  everingham  l  van gool  c  k  williams  j  winn  and a  zisserman  the pascal visual object classes  voc  challenge  international
journal of computer vision  vol      no     pp               
    s  litayem  a  joly  and n  boujemaa  hash based support vector
machines approximation for large scale prediction  in bmvc       
pp      
    s  maji  a  c  berg  and j  malik  efficient classification for additive
kernel svms  pattern analysis and machine intelligence  ieee transactions on  vol      no     pp             
    a  krizhevsky  i  sutskever  and g  e  hinton  imagenet classification
with deep convolutional neural networks  in advances in neural information processing systems        pp           
     a  krizhevsky and g  hinton  learning multiple layers of features from
tiny images       
     j  deng  w  dong  r  socher  l  j  li  k  li  and l  fei fei  imagenet 
a large scale hierarchical image database  in computer vision and
pattern recognition        cvpr       ieee conference on  ieee 
      pp         
     k  e  van de sande  j  r  uijlings  t  gevers  and a  w  smeulders 
segmentation as selective search for object recognition  in computer
vision  iccv        ieee international conference on  ieee       
pp           
     g  bradski  dr  dobbs journal of software tools 
     f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikit learn  machine learning in python  journal of machine
learning research  vol      pp                 
     f  chollet  keras  theano based deep learning library  code 
https   github  com fchollet  documentation  http   keras  io 
     k  simonyan and a  zisserman  very deep convolutional networks for
large scale image recognition  arxiv preprint arxiv                 
     x  glorot and y  bengio  understanding the difficulty of training deep
feedforward neural networks  in international conference on artificial
intelligence and statistics        pp         
     k  he  x  zhang  s  ren  and j  sun  delving deep into rectifiers 
surpassing human level performance on imagenet classification  arxiv
preprint arxiv                  
     i  sutskever  j  martens  g  e  dahl    and g  e  hinton  on the importance of initialization and momentum in deep learning  in proceedings
of the   th international conference on machine learning  vol     
icml        pp           
     n  srivastava  g  hinton  a  krizhevsky  i  sutskever  and r  salakhutdinov  dropout  a simple way to prevent neural networks from overfitting  the journal of machine learning research  vol      no     pp 
               
     y  ke and r  sukthankar  pca sift  a more distinctive representation for
local image descriptors  in computer vision and pattern recognition 
      cvpr       proceedings of the      ieee computer society
conference on  vol     ieee        pp  ii    
     h  bay  a  ess  t  tuytelaars  and l  van gool  speeded up robust
features  surf   computer vision and image understanding  vol      
no     pp               
     a  coates and a  y  ng  selecting receptive fields in deep networks 
in advances in neural information processing systems        pp      
     

fi