recognizing network traces of various web services
alexandros hollender  alexander schaub
i 

i ntroduction

security on the internet has become a major topic
of interest during the past few years  some major companies  for example  try to promote the use of https
everywhere  google announced they might improve the
pagerank of websites using this technology   https
also enables users to protect their privacy over open
wifi hotspots  however  there are means of retrieving
private information using so called side channel attacks 
in short  one can for example analyze the sizes of the
packets sent and received by some user  and gain access
to a lot of information on what they are doing  this idea
has been applied to bank websites  where the investment
portfolio could be retrieved using only the packet size 
or on a health website  where the disease a person was
looking for could also be retrieved in a similar matter 
although both websites are protected using https     
we also investigated this kind of threat in previous work
    that was published in the crisis   conference  we
focused on the auto completion functionality of search
engines  which enabled us to retrieve the search term the
user was looking for in various situations 
in this project  we will expand our work by building a
classifier that is able to detect whenever a user visits the
homepage of a search engine and uses its autocompletion
feature  since our previous work enabled us to relate the
corresponding network traces to the user input  we would
therefore be able to scan large sets of network traces
and retrieve the search terms looked for by some user 
our project comprises two main parts   first  detecting
whenever a users loads the homepage of a search engine
 among a defined set of search engines   second  detect
the exact network trace corresponding to the packets
exchanged while the autocompletion feature is being
used  using the first part  we would be able to detect
which search engine was being used  while the second
part allows us to target this specific search engine and
retrieve the relevant packets 
ii 

r elated work

the first part of our problem is quite extensively
treated in     where a multinomial naive bayes classifier

is used  several optimizations presented in that publication could be used to further improve our classifier 
overall  this proves the feasibility of the first part  our
application being quite specific  we did not find any
previous work treating the second part of our problem 
however  there is some work concerning the more general problem of analyzing network traces using machine
learning techniques 
a big part of the literature on this subject focuses on
traffic classification  given a network trace  the objective
is to extract the different flows contained in it and to identify which protocols and applications they correspond to 
possible approaches include the naive bayes classifier
      the em clustering algorithm     or unsupervised
bayesian classifiers      
another problem that is also quite extensively treated
is network anomaly detection  the goal is to be able to
recognize unusual activity in a network that is caused
by attacks or intrusions  neural networks       nearest
neighbor     and svms          are some techniques that
have proven very useful for this problem 
however  the majority of the network related literature using machine learning assumes that there is
more information available than in our setting  most
approaches use information located in the packet headers 
which we suppose to be unavailable because of encryption  only few use a similar setting to ours  where only
timing and packet size are available  some examples are
    where this setting is used for traffic classification and
    as stated earlier 
iii 

data sets and f eatures

since no data sets for this quite particular application
are available  we generated our own data sets through an
automated process  specifically  we used selenium   in
order to control the web browser  load pages and trigger
the auto completion feature  and the jnetpcap library   in
order to intercept the packets and thus store the packet
sizes  because we must wait until the pages are fully
 
 

http   www seleniumhq org 
http   jnetpcap com 

filoaded  it takes around   s to build one training example 
however  we can build as many training examples as
needed 

of the feature vectors as in the first part is used in the
second part of the project 
iv 

m ethods and r esults

a  identifying loaded webpage

a  identifying loaded webpage

for this part of the project  each training example
consists of the web trace captured when loading a certain
webpage  the number of packets captured usually ranges
from     to      packets  depending on the website
loaded  but also slightly fluctuating each time we load the
same website  to obtain feature vectors of constant size 
we define the j  th feature to be the number of packets
of size j that appear in the captured web trace  i e 

for this part we mainly used a naive bayes classifier
with a multinomial event model and laplace smoothing 
specifically  we used the training set to estimate

xj     packet  t   size packet    j 

where t is a raw captured web trace  because of limitations in the protocols used to transfer data on the internet 
the size of a packet cannot exceed      bytes  thus  each
training example is a      dimensional vector  counting
the number of occurrences of each packet size  it is
important to note that by using those features  we lose
some information that was contained in the raw captured
web trace  e g  the order of the packets  however  as
we will see later  the features we use retain enough
information for our application  while also being much
easier to work with than the raw web traces  using
our automated tool we were able to build     training
examples for each of the websites we want to identify 
b  auto completion trace detection
for this part  a training example corresponds to the
   packets exchanged between the user of an autocompletion feature and the server of the search engine  each
training example corresponds to the network activity
triggered by a user entering one more letter of its
request  and causing google to update the suggestions
being displayed  we built around     of those training
examples  in order to get web traces corresponding to the
average browsing behaviour  without auto completion
usage   we recorded the network activity corresponding
to a surf session comprised of popular websites  such as
facebook  wikipedia  news websites  etc    the test set
corresponds to a browsing session including one or more
uses of the auto completion feature  during the browsing
sessions  we recorded about        packets  finally  we
simulated a mixed session  popular websites and search
engine use  that would be used to test our algorithms  and
captured about         packets  with a total of    uses
of the auto completion feature  the same representation

p size j webpage i 

i e  the probability that a given packet in the web trace
of webpage i has size j   this yields a multinomial
distribution over the packet sizes for a given webpage
i  when estimating those probabilities we use laplace
smoothing to avoid any of them being equal to    very
important as a lot of packet sizes never occur in the
training set   we also use the training set to estimate
p webpage i  
using the naive bayes assumption we can then estimate the probability that a given web trace corresponds
to webpage i as
    
y

p size j webpage i xj p webpage i 

j  

divided by the probability that the web trace occurs 
which does not depend on i  and we can thus leave out
when maximizing with respect to i   in this expression xj
is the number of packets of size j occurring in the given
trace  the most probable webpage for the given web trace
is the webpage i that maximizes this expression 
we implemented this algorithm and first tried it on
a binary classification problem  identifying whether a
web trace corresponds to loading google  google com 
or wikipedia  en wikipedia org   for both of these webpages  a web trace corresponds to     packets  so
we thought it would be interesting to see if we can
differentiate them  using   fold cross validation on our
dataset containing     google traces and     wikipedia
traces we obtained an average test error of     we were
quite surprised  but after taking a closer look at the
dataset  we understood why the classifier worked that
well  we plotted the average number of packets of each
size for both cases on fig   and fig   
we also estimated how indicative of each website
each packet size is  a packet size j is indicative of
website google if
log p size j google   log p size j wikipedia 

fifig    

fig    

google  average number of packets of size          

wikipedia  average number of packets of size          

dataset with our naive bayes algorithm we obtained an
average test error of        this was quite surprising as
we did not expect naive bayes to work as well with  
classes as it did with   
however  as it turned out  we had been quite lucky 
continuing on our track  we added   more webpages
to the dataset  twitter  twitter com   ebay  ebay com 
and linkedin  linkedin com   running naive bayes on
this    class problem yielded an average test error of
     taking a closer look at the results  we noticed that
most of the google  youtube and ebay test examples
where classified as linkedin  and most of the twitter
test examples were classified as yahoo  by adding those
  additional webpages  we had added some packet size
distributions that were too close to the other ones and
naive bayes was no longer able to differentiate them
properly 
as a result  we decided to try another standard algorithm on our dataset  logistic regression  our problem
being a multi class classification problem  we trained
one classifier for each class  the positive examples
were the examples corresponding to the class  and the
negative examples were those corresponding to all other
classes  we then classified each test example to the class
whose classifier gave the highest value  using   fold
cross validation yielded an average test error of about
   for both the   webpage and the    webpage case 
thus  logistic regression seems to have a comparable
performance on the   webpage case  but  unlike naive
bayes  it is able to keep that performance on the   webpage case 

is large  where we use the probabilities estimated by
our naive bayes algorithm  we thus found out that the
most indicative packet size for google is       and
the most indicative packet size for wikipedia is      
those values make sense if we look at the graphs  fig
  and fig     even though packet size      appears a
lot for wikipedia       times on average   it appears
relatively often for google too      times on average  
thus  naive bayes finds the other two packet sizes more
indicative of each class 
the next step was multi class classification  we added
  other webpages to the dataset  amazon  amazon com  
bing  bing com   facebook  facebook com   yahoo  yahoo com  and youtube  youtube com   thus  our dataset
now contained     web traces corresponding to   different webpages  using   fold cross validation on this

however  logistic regression has a huge disadvantage
with respect to naive bayes  it runs much slower  so 
if the webpages we want to classify have packet size
distributions that are different enough  using naive bayes
is certainly the best idea  this is especially true in a
setting where the classifier has to be re trained quite
often  because most webpages are not static objects  but
change daily  however  if some packet size distributions
are too close  naive bayes will not work  and we will
have to use logistic regression 
b  auto completion trace detection
we used the     traces corresponding to the autocompletion feature for the search engines google and
bing  as well as the average browsing session  to
train the naive bayes classifier  for each of the search
engines  we then tried to find the packets corresponding

fito the autocompletion feature in the test web trace  since
the number of packets corresponding to one use of the
autocompletion feature is fairly constant  around    for
google     for bing   we used a sliding window in
order to determine the positions of those traces  for
example  for google  we first considered the    first
packets  packets   to     of the test trace  used the naive
bayes classifier in order to label them google or nongoogle  then moved to the packets at offset   to    
classified again  etc 
since a lot of packets are overlapping between two
successive windows  we expect our algorithm to find
much more probable autocompletion traces than the
actual number of times we used this feature  again  the
feature space has      dimensions  the elements of those
vectors are the number of packets of a given size in a
sequence 
the naive bayes classifier tends to find most of
the auto completion traces  but at the cost of a large
number of false positives  many of those false positives
are to be expected  but the rate of false positives
remains high  therefore  we have to use an other
algorithm in order to determine more reliably the traces
corresponding to the auto completion feature  or use the
naive bayes as the first step of a multi layered algorithm 
we decided to implement k means clustering as the
second layer  the idea is to remove packet sequences
that are misclassified by the naive bayes classifier  for
example a sequence containing a large number of packets
that have the same size and that happens to be the size of
one of the packets appearing often in our positive training
data  the number of clusters has been determined by
trial and error  we settled with    clusters  once the   
clusters have been determined by k means clustering 
we select the one  in the case of bing  or two  for
google  clusters whose centroids are the closest from the
centroid of our positive training data  since the clustering
is stochastic  we run the clustering several times and then
select the results for which the closest centroid had the
smallest distance to the centroid of the positive training
data among all the runs  k means was applied using the
scikit python library     
lastly  we also modified the way we computed the
precision and recall  only counting as positive matches
the offsets which exactly match the offsets we recorded
seemed too restrictive  therefore  we counted as positive
match any offset that was close enough to the recorded

offsets  but for every recorded offset we would not
count more than one positive match  for example  if we
recorded that the packets at offsets    to    correspond
to the use of the autocompletion feature  and after the
k means clustering we get as results the offsets        
   and     then we would have a recall of    the usage
of auto completion was correctly detected by offsets  
and     which are close enough to     and a precision
of     the offsets    and    were misclassified  and one
usage of auto completion was correctly detected  thus 
 
        figures   and  
the precision would be    
show the precision recall curves for google and bing
respectively  with and without using k means clustering
as a second layer  different precision   recall values have
been obtained by varying p y   the probability for a given
sequence of packets being one corresponding to the usage
of auto completion 
fig    

precision vs recall for google traces

as the results show  applying k means clustering
after naive bayes effectively removes unrelated packets
sequences while keeping the correctly classified ones 
especially in the case of bing  filtering the result from
the first stage using k means clustering removes a lot
of misclassified packet sequences  the shapes of the
precision recall curves may seem strange  the noise in
the data might explain this  as well as the large amount of
randomness  it is not even sure that the positive training
data and the part of test data that should correspond to the
auto completion are drawn from the same distribution  as
those distributions of packet sizes vary in time  

fifig    

precision vs recall for bing traces

we also tried svm and one class svm to classify
the sequences  it did not seem to work   one class svm
did not recognize a single trace  and svm had a too low
precision 
v 

c onclusion

for the first part  we were able to identify a loading webpage amongst   of the most popular webpages
with very high accuracy using a naive bayes classifier 
however  extending the set of possible webpages hugely
increases the classification error  and we had to use
logistic regression to obtain good results in that case 
whether we can use naive bayes or not depends on
the exact nature of the packet size distributions of the
webpages  future work on this topic could include testing
on even more webpages to see if logistic regression
continues to perform well  as well as examining other
classification algorithms that run faster than logistic regression  another possibility would also be to use feature
selection to decrease the dimension of the feature space
and make logistic regression run faster 
for the auto completion detection  naive bayes
with additional k means clustering seems to work quite
well  while there are still a non negligible number of
false positives  we managed to reduce the sequences
of packets which might contain auto completion usage
from about         to less than     with most settings 
one particular aspect we did not consider yet is the
ordering of the packets  although the exact ordering of

the packets with the same purpose varies sometimes  it
is not completely random  this is not taken into account
by our approach  which is therefore more robust  we
miss less sequences  but less precise  we still have
false positives   a third layer of filtering might be the
solution  but it would be more complex to design than
naive bayes or k means clustering 
overall  it seems that classical machine learning
methods can be adapted for network analysis purposes 
although the way the data is processed and represented
requires extra care  by combining both of our findings  it
seems possible to scan through large web traces in order
to determine which search engine might have been used
during a browsing session and when the auto completion
feature had been used  these traces can  for example 
be easily obtained by a standard laptop with off theshelf hardware running the adequate  free  software and
installed near a busy public or even private wifi hotspot 
or by intercepting the data at router level  using our prior
work      it would be possible to determine which search
terms have been looked for by the user  only looking
at the sizes of the exchanged packets  although it is
not possible to do so anymore on google   the size of
the packet containing the relevant data we exploited in
our earlier paper has been completely randomized now 
rending side channel attacks on this particular feature
probably unfeasible   there are a lot of web services that
have not yet been protected against these attacks 

fir eferences
   

   

   

   

   

   

   

   

   

    

    

    

shuo chen  rui wang  xiaofeng wang  and kehuan zhang 
side channel leaks in web applications  a reality today  a
challenge tomorrow  in security and privacy  sp        ieee
symposium on  pages         ieee       
levent ertoz  eric eilertson  aleksandar lazarevic  pang ning
tan  vipin kumar  jaideep srivastava  and paul dokas  mindsminnesota intrusion detection system  next generation data
mining  pages              
eleazar eskin  andrew arnold  michael prerau  leonid portnoy  and sal stolfo  a geometric framework for unsupervised
anomaly detection  in applications of data mining in computer
security  pages        springer       
dominik herrmann  rolf wendolsky  and hannes federrath 
website fingerprinting  attacking popular privacy enhancing
technologies with the multinomial nave bayes classifier  in
proceedings of the      acm workshop on cloud computing
security  pages       acm       
anthony mcgregor  mark hall  perry lorier  and james brunskill  flow clustering using machine learning techniques  in
passive and active network measurement  pages        
springer       
fabian pedregosa  gael varoquaux  alexandre gramfort  vincent michel  bertrand thirion  olivier grisel  mathieu blondel 
peter prettenhofer  ron weiss  vincent dubourg  et al  scikitlearn  machine learning in python  the journal of machine
learning research                    
alexander schaub  emmanuel schneider  alexandros hollender  vinicius calasans  laurent jolie  robin touillon  annelie
heuser  sylvain guilley  and olivier rioul  attacking suggest boxes in web applications over https using side channel
stochastic algorithms  in risks and security of internet and
systems  pages         springer       
taeshik shon  yongdue kim  cheolwon lee  and jongsub
moon  a machine learning framework for network anomaly
detection using svm and ga  in information assurance workshop        iaw    proceedings from the sixth annual ieee
smc  pages         ieee       
charles wright  fabian monrose  and gerald m masson  hmm
profiles for network traffic classification  in proceedings of
the      acm workshop on visualization and data mining for
computer security  pages      acm       
sebastian zander  thuy nguyen  and grenville armitage 
automated traffic classification and application identification
using machine learning  in local computer networks       
  th anniversary  the ieee conference on  pages        
ieee       
zheng zhang  jun li  cn manikopoulos  jay jorgenson  and
jose ucles  hide  a hierarchical network intrusion detection
system using statistical preprocessing and neural network classification  in proc  ieee workshop on information assurance
and security  pages            
denis zuev and andrew w moore  traffic classification
using a statistical approach  in passive and active network
measurement  pages         springer       

fi