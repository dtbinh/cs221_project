 

predicting and identifying hypertext in
wikipedia articles
neel guha  annie hu  and cindy wang
 nguha  anniehu  ciwang  stanford edu

i  introduction

ii  related work

wikipedia now has millions of articles online and
tens of millions of views each hour  in order to
maintain a high standard of organization across this
sheer volume of content  it is essential to ensure accurate and helpful intra article linking  the wikipedia
manual of style states that links represent  major
connections with the subject of another article that
will help readers to understand the current article
more fully   
in this project  we optimize article linking towards
this goal by applying machine learning to predict
hypertext in wikipedia articles  see figure   for
an example of hypertext   as inputs  our algorithm
processes tokens  words and n grams  from articles 
we then use naive bayes  logistic regression  and a
svm to output a predicted classification  plain text
 no link  or hypertext  link   in this paper we describe
the process for our data collection  feature selection 
and the relative performance of various algorithms 
note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks 
while there has been considerable work on predicting
hyperlinks  especially using context resolution based
techniques   we felt there had been significantly less
work on predicting hypertext 
our model enables large scale automated editing of
wikipedia linking in a low cost but effective way and
can also be used to make link suggestions to individual
contributors  outside of wikipedia  our system can
be generalized to any broad database of documents
or texts  for example  it could be applied to predict
hypertext in news corpora or academic literature 

there is considerable prior work involving using
existing wikipedia links for entity disambiguation
     rather than general applications  the goal of our
paper is to identify semantically important hypertext
within the articles themselves 
prior approaches to the problem of hypertext detection rely heavily on keyword identification algorithms 
generally  these approaches score potential keywords
by evaluating     word occurrence statistics or    
linking patterns on similar pages  the traditional
statistical approach  which depends solely on word
occurence counts  involves methods such as tf idf   
independence test  and  keyphraseness   proportion
of occurrences that are hypertext  across the training
set      these methods have moderate success  with
upper precision recall f  scores in the         range 
the second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness      and     outline specific formulas to
evaluate these measures      introduces a novel algorithm  ltrank  to identify similar pages  then
identify candidate links from those similar pages that
might be missing on the given page  the state of theart algorithm for link detection has a precision and
recall of approximately          
we aim to develop a better model by incorporating
not only statistical and contextual features  but considering the absolute importance of each word itself 
i e  proper nouns are more likely to be relevant links 
    suggests a computational linguistics approach to
keyword extraction involving part of speech  pos 
tagging in additional to statistical features  we explain our feature selection in more detail below 
iii  data set and features

fig     in this sentence  the terms american and academician are hypertext 

 

http   en wikipedia org wiki wikipedia manual of style

for clarity  we define  hypertext  as a word in
an article that appears  in the article content  as a
clickable link to another article  our algorithm works
as follows 
   model training  for each article ai in a training
set of articles a  we we extract each unique token
 word  ti from ai and represent ai as the set of unique

fi 

tokens it contains  for each token we then construct a
feature vector xi based on the attributes of ti relative
to ai  its parent article  and a  all the of articles   if
in ai   ti is a word with a link to another wikipedia
article  we label ti s output vector yi as hypertext
 y     otherwise we label it as plaintext  y       the
set of all xi   yi s across all ai in a forms our training
set 
   model testing  when testing this model on an
article at   we follow a similar initial procedure  we
extract each unique token from at and construct its
corresponding feature vector  we then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in at   if a feature vector
xi is classified as hypertext  then its corresponding
token is predicted to be linked on in the article at  
because wikipedia only links on the first occurrence of a word  as opposed to every occurrence of
the word   note that we represent each article by the
list of unique tokens it contains 
a  features
to determine whether a word should be linked  we
thus considered features pertaining to the word itself 
as well as the word in the context of the article and
data set as a whole  the four features included in our
model are 
   whether or not a word is proper noun    or   
proper nouns typically denote significant people  places  or things  which usually have their
own wikipedia pages  if these words appear in
an article  they should be linked 
   length of the word  an integer value  longer
words are more likely to be less common  names 
or technical terms  in any of these cases  the
word should be linked 
   the tf idf score of the word  a float value
between   and    relative to the article in which
the word appears in  tf idf  or term frequencyinverse document frequency  is a numerical
statistic that denotes how important a word is
to a document  specifically  for a term t in a
document d  d  tfidf t  d    tf  t  d   idf  t  d 
where tf  t  d  is the normalized frequency of
term t in document d and idf  t  d  is the log
of the proportion of the number of documents
in d containing the term t 
   the proportion of  the number of articles in
which the word is linked  to  the number of
articles in which the word is mentioned but not
linked   a float value  this gave us a measure of
how often a word was linked if it was mentioned 

b  data set and data collection
the words that constitute links to other pages
differ significantly from page to page and are highly
dependent on the context of the page  entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing  for example 
in the wikipedia article on paul ryan  the entity
 marathon  is mentioned but does not link to its
corresponding wikipedia page  however in the page
for  running    marathon  is mentioned and links to
its corresponding wikipedia page  because of this 
attempting to predict hypertext across a set of articles covering diverse topics can be tricky  with such
diverse data  it can be hard to determine meaningful
conclusions on which terms should and shouldnt
constitute links 
we hypothesize that accurately predicting hypertext must leverage an implicit understanding of the
articles context and topical focus  we tested this by
running our model on two different data sets 
   intuitively  articles from the same wikipedia
category should have similar distributions of
hypertext  limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic context of the articles  in this project  we
selected random samples of     training articles
and    test articles from the wikipedia  forms
of government  category  we called this our
government dataset 
   in order to test our model in a case where
the articles in the dataset do not originate from the same context  we ran our
model on a     training articles and    
test articles uniformly drawn from a mixture
of   wikipedia categories   judaism   ancient
greece   environmental science   technology   
we called this our multicategory dataset 
we scraped and parsed each of these articles using
beautifulsoup  for each article we extracted a list of
unigrams bigrams as well as a list of terms that are
linked on 
we evaluated a variety of possible features to
model  we treated each unique word in a given article
as a separate training example 
creating the feature vectors for the training set
required significant pre processing of our data  we
utilized the nltk and scikit libraries to perform part
of speech classification and tf idf scoring  respectively 
we applied nltk part of speech tagging to the
tokenized plaintext of each article  and filtered the

fi 

words tagged as proper nouns  we then used scikit
tf idf term weighting to vectorize and score each word
in the context of the article in which it appeared  we
wrote custom scripts to determine the other features 

iv  methods
in our algorithm  we experimented with the following models 

a  dummy model
in order to establish a baseline for our algorithms
performance we used a dummy stratified classifier
from the sklearn library      this classifier establishes
a baseline performance by randomly predicting a
class for each training example  because our data
set contains a fairly significant class imbalance  the
number of terms that arent links far exceeds the
number that are   the probability of picking each class
was weighted by the class distribution in the training
set 

b  gaussian naive bayes
naive bayes models are useful for quick and initial
evaluations of a dataset to explore the feasibility
of building some classifier  though they make the
 naive  assumption of conditional independence between features  theyve proved to be surprisingly reliable  for a yi with with an associated set of features
x       xn  
p y 
p  yi  x         xn    

n
y

d  support vector machine
support vector machines attempt to classify data
by constructing a multidimensional hyperplane to
separate data points of differing classes  in effect
identifying a decision boundary   the ideal decision
boundary is given by a hyperplane which is at the
maximum distance from the closest points of each
class  referred to as support vectors   in the case
where the data is not linearly separable  svms utilize
kernel functions to project the data into a higher
dimension where a linear separator can be found 
specifically  svms operate by solving the optimization problem 
max w     

m
x
i  

i 

m
  x  i   j 
y y i j hx i    x j  i   
 
i j  

s t  i     i              m
m
x
i y  i     

   
   

i  

v  results and discussion
a  data
the government category dataset yielded         training feature vectors and        test feature vectors  for
both training and testing  around        of the vectors
corresponded to a linked word  the multicategory data
yielded         training feature vectors and         test
feature vectors  for both training and testing  around       of vectors corresponded to a linked word 

p  xi  y 

i  

p  x         xn  

   

we first implemented a gaussian naive bayes model
where the feature likelihood is assumed to follow a
gaussian distribution 

c  logistic regression
we used a logistic regression model with l  regularization      in this type of classifier the hypothesis
function takes the form of a logistic curve 
 
h  x   
   
    exp x 
where h  x  can be viewed as the probability y    
given an x  we used an l  penalty with logistic
regression 

fig     hypertext in original article  top  vs  predicted hypertext from model  bottom  

our confusion matrix after running logistic regression
on the government category dataset was 
true no
true yes

predict no
      
      

predict yes
       
      

although the model very accurately predicted true nonlinked words  over     of predictions were false negatives 
or words predicted as non links that should actually have
been links  a large majority of these false negatives were
common words  such as  music   that were linked in the
article as part of a multi word phrase  such as  vietnamese
music   we hypothesized that updating our model to

fi 

consider bigrams  and eventually general n grams  should
eliminate much of these false negatives 
our confusion matrix after running logistic regression
on the multicategory dataset was 
true no
true yes

predict no
          
          

predict yes
          
          

even though we expanded the size of our dataset and
diversified the categories we considered  the proportions
of true positives and true negatives were very similar
between the two datasets  again  we hypothesized that
considering single word tokens instead of n grams caused
much of this similarity  the imbalance in our linked and
non linked classes may also have contributed to the high
false negative rate 

b  metrics
given the significant class imbalance in our data set 
we evaluated our algorithms performance by measuring
their accuracy  precision  recall  f    and auc score  precision is calculated as the number of correctly identified
hypertext tokens divided by the total number of hypertext
tokens proposed by the system  recall is defined as the
number of correctly identified hypertext tokens divided
by the total number of hypertext tokens in the original
document  and f  score is the harmonic mean of the
precision and recall  auc is defined as the area under
the roc curve  which is created by plotting the recall
against the false positive rate  the number of incorrectly
identified hypertext tokens divided by the total number
of non hypertext tokens  at various threshold settings 

within a single category  logistic regression and svm
consistently outperformed naive bayes across all metrics 
naive bayes likely performs poorly because of the independence assumptions it makes on the features  whether
a word is a proper noun is likely strongly correlated
with the proportion of times it is linked when mentioned  particularly within a set of related articles  for
instance  the proper noun  vietnam  was linked in most
of the articles in which it appears  given our feature
set  logistic regression and svm seem to have reached an
upper threshold in terms of performance  using articles
across several categories  accuracy and auc values for all
models were roughly the same  however  svm performed
considerably worse  while logistic regression performed
considerably better across most metrics  svm most likely
suffered in the multi category case since the classes became less separable  consider important technical words
that are mentioned few times in one article but many
times in articles from other categories  resulting in high
word length and low tf idf score  now consider short
names of important historical figures  which have low
word length but high tf idf score  since both types of
terms are good candidates for hypertext  this illustrates
how the classes are difficult to separate with a  n   dimensional hyperplane  since these instances are more
likely to occur in the multi category case  svm has a
much lower f  score and recall  thus  logistic regression
clearly performs best when the data is not restricted to
semantically related articles 

fig     a comparison of the performance of various models for
the government data set
fig     a comparison of the performance of various models for
the mixed categories data set

d  feature analysis
c  comparative model performance
the results of the various models along each of the
metrics mentioned above for the government data set and
the multi category data set can be seen in figure   and
figure   respectively  as expected  the dummy model
performed very poorly and was surpassed by every other
model 

to analyze which of our features were most relevant
to our predictions  we performed a simple leave oneout feature analysis on each of our main models  for
each feature  we retrained and retested the model with
that feature excluded  and compared the resulting metrics
across features 
for the government dataset  word length was actually
our most relevant feature  with tf idf values and hypertext

fi 

e  experimenting with bigrams
for the government data set we also experimented with
running our algorithm on bigrams  for each article   in
addition to creating feature vectors for single word tokens
  we create feature vectors for bigrams  pairs of adjacent
words   bigrams are frequently used in many natural
language applications and have been shown to be effective
in a wide range of problems  though all of our models
outperformed the dummy model when run on bigrams 
there was significantly less variance between the models
 see     in fact  all of the models performed significantly
poorer on the bigram dataset then on the single word
dataset 
fig     loo feature metrics for government dataset

proportions much less relevant  this seemed to suggest
that hypertext depended only loosely on article context 
directly contradicting our earlier hypothesis  since we
limited input vectors for this dataset to one category of
articles  the articles may have been too similar  limiting
the usefulness of these two features in differentiating
context  the similarity of article context combined with
our relatively small sample size may have also caused
overfitting to our training set  and an inflated accuracy
of prediction  to further explore these two observations 
we then ran leave one out feature analysis on our multicategory dataset 

fig     a comparison of the performance of various models for
the government data set using bigrams

however  our resulting confusion matrix from running
logistic regression did significantly reduce our number of
false negatives  as predicted 

true no
true yes

predict no
      
      

predict yes
      
      

vi  future work

fig     loo feature metrics for multicategory dataset

surprisingly  our results did not change much even
after considering multiple categories  in fact  word length
became even more relevant to our results  though prior
approaches to predicting hypertext tended to focus on
statistical measures of word importance  such as tf idf 
our results when mixing these measures with linguistic
features such as word length seem to suggest that linguistic features correlate much more strongly to predictions 

in this project we described an algorithm for predicting
and identifying hypertext in wikipedia articles  the logistic regression and svm algorithms perform best in the
case of a single category data set  however  across multiple categories  logistic regression definitively outperforms
all other algorithms  reaching     accuracy and     auc 
possible next steps could include expanding the data
set and identifying more features to help decrease falsenegatives  we had computational difficulties  limited
ram  in implementing n grams in this project  and were
only able to achieve mediocre results with bigrams on a
small data set  with greater resources we would hopefully
be able to complete a more thorough implementation of
n grams  this too could improve our results 

fi 

vii  references
    ratinov  lev  et al   local and global algorithms for disambiguation to wikipedia   proceedings of the   th annual meeting of the association
for computational linguistics  human language
technologies volume    association for computational linguistics       
    mihalcea  rada  and andras csomai   wikify   linking documents to encyclopedic knowledge   proceedings of the sixteenth acm conference on conference on information and knowledge management 
acm       
    gabrilovich  evgeniy  and shaul markovitch   computing semantic relatedness using wikipediabased explicit semantic analysis   ijcai  vol    
     
    milne  david  and ian h  witten   learning to link
with wikipedia   proceedings of the   th acm conference on information and knowledge management 
acm       
    adafre  sisay fissaha  and maarten de rijke   discovering missing links in wikipedia   proceedings of
the  rd international workshop on link discovery 
acm       
    hulth  anette   improved automatic keyword extraction given more linguistic knowledge   proceedings of the      conference on empirical methods in
natural language processing  association for computational linguistics       

viii  tools
    ntlk library  bird  steven  edward loper and
ewan klein  natural language processing with
python  oreilly media inc       
    scikit learn  pedregosa et al  scikit learn  machine
learning in python  jmlr     pp                  

fi