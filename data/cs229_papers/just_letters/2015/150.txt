cs    project report
offline music recommendation
emilien dupont  isabelle rao  william zhang

 edupont  isarao 
wzhang   stanford edu
abstract the goal of this project was to recommend
songs to users based solely on their listening histories 
with no information about the music  we applied various
collaborative filtering methods to achieve this  user user
neighborhood models  item item neighborhood models
and latent factor models  we achieved the best results
with item item cosine similarity  the code for this project
can be found here 

i  introduction
companies like spotify and pandora recommend songs to their users based on user listening
histories and on meta data about the music  in
this project we attempt to make recommendations
based solely on user history  using the million
song dataset      this dataset is comprised of a
large number of user listening histories used for
training  separate from this is a test set comprised
of users whose listening histories did not appear
in the training set  it is split into two parts  a
visible one  consisting of half of each unseen users
history  and a hidden one  consisting of the true 
other half of the same users listening histories 
using various collaborative filtering methods  we
attempt to predict the hidden half  given the visible
half 
ii  related work
the methods and procedures in our recommendation system are used widely  not only in music 
but in various other areas such as movies  news and
e commerce  companies like facebook  twitter
and linkedin also use such methods to recommend
friends followers connections      as such  there is
a large amount of literature relating to this subject 
perhaps most famously  netflix     challenged
competitors to come up with an algorithm to

recommend movies to their users based on their
viewing history  the winning entry used a combination of various methods  but one of the most
succesful algorithms was based on latent factor
models  which we will explore in this project 
as another famous example  amazon uses both
item item and user user correlations to recommend
products to customers      we will attempt to
emulate this approach using cosine similarity 
as mentioned by hu et al       one of the most
common approaches to collaborative filtering is
that of neighborhood models  see below for more
explanation   the underlying assumption is that
users with similar ratings on some items will
have similar ratings on the others  an analogous
assumption is made for items that share similar
ratings for many users   another set of methods
that has shown promise recently relies on lowrank matrix factorization  which seeks to uncover
the most important factors governing song choices 
these two approaches are the ones we will be
focusing on in the rest of this paper 
iii  dataset   features
the data is in the form of  user  song  play
count  triplets  for example 
  isabelle  hey jude    
  isabelle  shake it off     
  william  whole lotta love     
  emilien  shake it off    
the training set contains    million such triplets 
corresponding to     million users and    k
songs  the test set consists of    k users and
   k songs  there are a few interesting statistics
we can note about the data 
 there are     songs in the test set which do
not appear in the training  so we will never
be able to predict those

fithe average number of play counts for a user
is    
 the average number of unique songs for each
user is   
if we let u denote the number of users and i the
number of items  or songs   then we can store the
data as a u  i matrix of play counts with entries


cu i   number of times u has listened to song i
a  count to rating
one of the main challenges for this project was
the fact that we wanted to apply methods for
explicit feedback  e g  a imdb users rating for
a movie is explicit   whereas our data was implicit
in the form of song counts  most collaborative
filtering methods rely on each entry of the useritem matrix being a rating  e g      stars   we
have therefore experimented with several different
ways of defining a rating r u  i  of user u on
item i from play counts 
 r u  i    c u 
  i   counts 
  if c u  i     
 r u  i   
 binary counts 
  otherwise
c u i 
r u  i    max
 max normalized 
i c u i 
we have experimented with using these ratings
as features and our results are shown in the
sections below 


seen  in order to make accurate predictions we
need to have encountered a significant amount of
the songs in the training set  which means we need
to train on a large number of triplets 
iv  methods
we used two different types of collaborative
filtering methods  neighborhood models and latent factor models 
a  baseline model
as a baseline  we recommended the     most
popular songs to every user 
b  neighborhood model
we implemented two types of neighborhood
models  the assumption behind neighborhood
models is that if two users u and v are similar
 in a sense that will be made precise   then user
u will like songs in user vs listening history  we
use cosine similarity  as seen for instance in     
to define the similarity between   users 
similarity u  v   

where u is the row corresponding to user u in
the rating matrix  item item similarity is defined
in a similar way
similarity i  j   

fig     number of distinct songs listened to by all the users  as a
function of number of users in the dataset 

finally  we should note that when the data is
initially loaded in  a lot of new songs  as shown
in figure     once a few users have been loaded in 
you will start to see more songs you have already

ut v
kukkvk

it j
kikk jk

where i is now a column of the rating matrix  intuitively  two users are similar if they have listened
to a lot of the same songs  the dot product will be
over a lot of the same song indices   similarly  two
songs are similar if they have been listened to by
many of the same users  now given our similarity
matrix  how do we make recommendations  to do
this  we define a score function which represents
our guess of how much a user u will enjoy a song
i  let u i  be the set of users who have listened
to song i  then we define    
score u  i   



f  similarity u  v  

vu i 

where f  x  is some scoring function  a similar
definition can be given in terms of item item

fisimilarities  to make a recommendation for a user
u  we then simply pick the k songs with the highest
score and recommend those 
c  latent factor model
latent factor models are another approach we
considered during the course of this project 
specifically  we considered an approach inspired
by the singular value decomposition  the idea is
to approximate the ratings matrix r as the product
of two rank k matrices  r  x t y   where x is a
k  u matrix and y is a k  i matrix  the hope
is to be able to respectively summarize each user
and item by the k dimensional vectors xu and yi  
where the k components capture the salient latent
factors behind the ratings matrix  intuitively  we
would like the product x t y to be as close to r
as possible  minimize for instance the frobenius
norm of their difference   however  this is likely
to overfit the data on the observed ratings  one
way to avoid this is to include regularization terms 
and the optimization problem can thus be formally
stated as below 
min
x y



v  results   discussion
a  evaluation metric  mean average precision
before discussing results we need to define our
evaluation metric  the predictions are evaluated by
means of the mean average precision  as described
in the admire   paper      define the feedback
matrix m        ui   where mu i     if song i
appears in user us unseen listening history    otherwise  and yu a prediction for user u          u  
where yu   j    i indicates that song i is ranked at
position j for user u  its assumed that yu omits
items already known to be played by u  three steps
are necessary to compute the map 
 for any k  define the precision at k pk as the
proportion of correct recommendations within
the top k of the predicted ranking 
 
pk  u  y   
k


 ru i  xut yi      x kxu k    y kyi k 

gradient descent can be used to optimize the
objective function  however  it is non convex because of the dot product xut yi   and it turns out that
gradient descend is often slow  requiring several
iterations  another optimization routine is alternating least squares  which  as its name suggests 
alternatingly treats x and y as constants and
optimizes for the other variable  see algorithm    



  
 pk  u  y   mu yu k 
nu k  

where  is a threshold that represents how
many of the top predictions in yu to include 
and nu is the minimum of the number of
hidden songs in a test users history  and  
the final step is to average the previous
quantity over all m users 
map  

algorithm   alternating least squares
initialize x y  using svd  for example 
repeat until convergence 
 for u       u do
  
xu 



yi yti   x ik

ru i ru




ru i yi



ru i xu

ru i ru

  



ru i ri

xu xut   y ik

ru i ri

 
ap u  yu  
m
u

b  results and analysis

for i       i do
yi 

 mu y  j 

j  

the next step is to take the average of the
previously computed precisions 
ap u  y   

ru i observed

k

in all of the announced results that follow 
we determined the parameters using k fold crossvalidation  where k      on a subset of the data
that spanned   k users in the training set and  k
users in the test set  chosen randomly   in figure
  we show the map for our various algorithms
on a set of   k training users and  k test users
 corresponding to    k songs and     million
triplets    we used  mostly  binary ratings for the
song counts    if song has been listened to and  
otherwise   as can be seen  the item item cosine
similarity model performs the best  while user user

fifig     performance of various methods on a   k user training 
 k test training  best map is       

fig     performance vs number of factors on   k training users 
 k test users

similarity gives similar results  while outperforming the baseline  the latent factor model did not
perform as well as the other two models 

the optimal values of the regularization parameters depended on the size of the datasets  for
  k training user  the regularization parameter was
chosen to be     which is also the value used in
figure     as can be seen  increasing the number
of factors improves the performance of the latent
factor model  this is to be expected as we will
capture more information about each user and
item with larger vectors  however  when using a
very large number of factors     on the plot  the
performance starts to dip  a likely indication of
overfitting 

c  tuning the models
   cosine similarity  for the similarity models 
we experimented with tuning a parameter  in the
range       on the similarity measure
similarity u  v   

ut v
kuk  kvk    

results are shown in figure   

d  different types of ratings

fig     performance of user user collaborative filtering on a   k
user training   k test training for various values of 

as can be seen  the best choice for  is approximately      the standard choice for       
gives very similar results  but we do obtain slight
improvements  order of      in map  
   latent factor model  for the latent factor
model  there were several parameters we could
tune  the number of factors k and the regularization
parameters x and y  

fig     performance of various rating types on   k training users 
 k test users for user user cosine similarity 

we also experimented with using different count
to rating functions as described earlier  surprisingly  the best results were obtained when we used
binary ratings  i e  r u  i      if user u has listened
to song i and   otherwise  this means we throw
away all information about how many times the

fiuser has listened to the song  while this could be
useful information  for our model it creates a large
unwanted bias towards highly played songs  we
experimented with using logs of play counts as
well to reduce this  but binary ratings still had the
best performance 

more than   k users without the computations
becoming prohibitively slow  we believe one of
the reasons for not obtaining higher map scores
is related to this  and it would have been very
interesting to experiment with our models on larger
matrices 

e  number of predictions

vi  conclusion   future work
in this project  we studied and compared the
performance of two types of collaborative filtering
models using implicit feedback  both the neighborhood model and the latent factor model significantly outperformed the baseline of recommending
the most popular songs to every user 
the procedure which performed the best was itemitem cosine similarity for which we obtained a
map of         we used this in combination with
binary ratings for which we also obtained the best
results 
the main challenge in getting these procedures
to yield a satisfactory map is that collaborative
filtering methods have been shown to work well in
the context of recommendation  however  our task
was somewhat different  we are trying to predict
what other songs a user already has listened to  in
future work  we would like to explore variants of
the latent factor model  some of which also try to
include an additional weights matrix representing
the confidence in observing a certain song for
a particular user  furthermore  we believe that
running our models on larger matrices would have
yielded a significant improvement in performance 

fig    

map    vs number of recommendations 

unsurprisingly  increasing the number of predictions made  increases the map  however  there is
a tradeoff between recommending a large amount
of songs and having a high map  firstly  it does
not make sense to recommend        songs to a
user  secondly  after some threshold the increase in
the map becomes so small it is almost negligible 
as can be seen in figure        recommendations
is a good threshold  which is also the one chosen
by the initial paper     
f  computational limitations
as mentioned previously  in order to make accurate predictions  we need to load in many user
histories  indeed  initially we will see a lot of new
unseen songs and as the number of users increases
we will see fewer new songs  as can be seen in
figure    the number of new songs added to the
count matrix stops increasing significantly after we
load in about    k users  which corresponds to
about    k songs  this means our count matrix
would be    k by    k  which is very large
 even though we  of course  use sparse matrices  
both cosine similarity and latent factor models
are very slow on matrices of this size  as we
were limited to using a single machine to run
our algorithms  we could not run our models on

r eferences
    james bennett and stan lanning  the netflix prize  in
proceedings of kdd cup and workshop  volume       page    
     
    michael d ekstrand  john t riedl  and joseph a konstan 
collaborative filtering recommender systems  foundations and
trends in human computer interaction                   
    yifan hu  yehuda koren  and chris volinsky  collaborative
filtering for implicit feedback datasets  in in ieee international
conference on data mining  icdm       pages        
     
    paul b kantor  lior rokach  francesco ricci  and bracha
shapira  recommender systems handbook  springer       
    brian mcfee  thierry bertin mahieux  daniel pw ellis  and
gert rg lanckriet  the million song dataset challenge  in
proceedings of the   st international conference companion
on world wide web  pages         acm       
    j ben schafer  joseph konstan  and john riedl  recommender
systems in e commerce  in proceedings of the  st acm
conference on electronic commerce  pages         acm 
     

fi