cs    project  doctor bayes
brandon beckhardt
 beb    

leonid keselman
 leonidk 

anthony perez
 aperez  

abstract
in order to alleviate the impact of healthcare spending on the the last mile of
medicine  we built a system to predict a users illness simply based on a description of their symptoms  electronic medical records and clinician notes are difficult
to access and so we explore online medical data bases as data sources  from the
data on the freebase  mayo clinic  and wikipeida data bases  we trained a naive
bayes  logistic regression  random trees  and cosine similarity model many of
the techniques we employed were aimed at eliminating over fitting and improving
generalization 

 

introduction

according to recent oecd estimates  healthcare expenditures for developed countries account for
    to     of their national economies  in order to alleviate the impact of healthcare spending
on the the last mile of medicine  we built a system to predict a users illness simply based on a
description of their symptoms  originally wed hoped to develop a classification system on top of
electronic medical records  clinician notes  and the icd    disease classification system  we were
unable to find publicly available emrs so instead we created doctor bayes  an illness classification
system whose education comes only from reading about medicine on the web 
there are many interactive disease classification applications available but we wanted to see how
well we could classify diseases solely based on plain text collected from the web and generated by
a user  the input to our algorithm is raw  unprocessed  plain text  like the text that you are reading
now  this text would be generated by a user  the output is the name of a disease  in text form 
selected from a list of diseases known by and supplied to the algorithm  we implemented many
algorithms to transform the input to an output  of which a naive bayes model was most effective 
this is a supervised learning task because we know the disease each document is describing  below
are some example inputs and outputs  the model outputs the top five most likely disease with their
probabilities 
q    redness  itchiness and difficulty opening eyes in the morning 
a       conjunctivitis  pink eye      e     glaucoma     e     dermatitis     e    blepharitis      e    sleepwalking 

 

related work

in the work most related to ours  researchers from  m built an automated tagging scheme  which
takes clinician notes and predicts a standardized disease code       for their testing and training  they
were able to use electronic health records  ehrs  and found that regularized logistic regression
could give them a     accuracy  while we originally wanted to pursue a similar task with labeled
ehrs  we were unable to find any publicly available datasets of medical records or clinician notes
 the  m researchers used data from  ms proprietary database   thus  we had to find and create our
own datasets  our work on this is specified in our section on data  an interesting differentiation is
that our mismatch of datasets puts a much larger emphasis on our systems ability to generalize 
there is a long history of machine learning being applied to medical classification  most major
reviews for medical classification provide a wide overview of different learning methods            
these reviews suggest that these methods to diagnostics across all of medicine  however  almost
all their examples only to binary classification of a single disease  that is  oncologists build cancer
detectors  cardiologists build heart disease detectors      and so on  our system is different in that
it is a general diagnostics system  relevant to first pass classification of any illness  as long as its
 

fiour automated catalog   similar sentiment can be found in modern overviews of learning methods
in medicine 
as our classification methods focus on using plain text analysis to predict diseases  we also reviewed
modern methods in text classification  of specific interest was work on baysian pragmatic reasoning  where a speaker model is used to help shape the probabilities of hearing certain utterances     
the key concept of that work is that  if three things can be described as cold but two of them are
cold and wet  then a speaker saying cold is more likely to be referring to the not wet one  if
we treat our document counts as a model of speech  then cosine similarity  which performs normalization across every feature vector in the training set  can be seen as performing the same desired
effect  this method performed very well on our dataset 
one of our primary and most successful methods for classification came from using multinomial
naive bayes  which have a long history of success when used with other methods like tf idf     
other papers in the field have suggested the use of priors to help over fitting       we implemented
a set of priors for all our classes and found it was actually detrimental to our results 

 

data

our data consists of free text we collected from mayo clinic  freebase and wikipedia  with the
known structure of the mayo clinic and freebase databases  we were able to extract both a list
of symptoms and a description for each disease from the two sources  we unified our data by
exploiting freebases list of available aliases for each disease  after unifying our sources  there
are     different possible disease classes  for each class we have   examples  two from freebase
 descriptions and symptoms   two from mayo clinic  descriptions and symptoms   and one from
wikipedia bring us to a total of      examples  we use the wikipedia examples as a test set and
the other examples as a training set  the reason we do this instead of cross validation is because
we cannot split any of the data sets  because each example set contains only one example of each
disease  therefore  any testing scheme is going to pit one example set against the others in terms of
testing and training  we chose wikipedia as the testing set due to its more free form nature which
would better reflect informal human writing  in order to avoid giving away the correct label to our
learning algorithm  we remove all instances of the disease name from the wikipedia articles  we
also created a set of    handwritten testing examples which model what a human might input into
the system  these handwritten examples consistent our best example of real user input 
   

priors

we could not find a cumulative list of occurrences for all diseases so we decided a way to gauge
the likelihood of a disease would be the number of results in a google search  ex  alzheimers
of results for diseases
medical   each prior was calculated as number
  generating priors in this manner
total number of results
turned out to be a failure and actually decreased our results in both the top   prediction and top  
prediction 

 

feature extraction

we extract features from plain text data sources by building a document term matrix  each row
represents a document  a disease article   each column represents a term  a word   and each element
represents the number of times the term  column  appears inside the document disease article  row  
user queries are vectorized the same way we generate a row for the document term matrix  terms
in user queries which were not present in any of the documents are discarded  all of our feature
engineering is aimed at reducing the feature space because we identified this problem as having
high variance  prone to overfitting  for most of our learning algorithms due to the training error
being much higher than test error  we implemented five techniques  tf idf weighting  stemming 
stop words removal  document frequency based removal  and non alphabetical removal 
tf idf is a weighting scheme for word counts in the document term matrix  term frequencies are
weighed according to 
 
tfw  

    log countw   if countw    
  otherwise

where tfw is the new term frequency for element w and countw is the observed term frequency
from each document for element w  note this is element wise   the inverse document frequency
 

fiweighting is give by 
idfw   log n      nw   

where idfw is the idf weight of element w  n is the number of documents  and nw is the number
of documents containing the term corresponding to the column of element w 
for each document  the raw count of each feature term is passed through the term frequency weighting function  then multiplied by the inverse document frequency weighting  tf idf weighting has
two big effects  words which are common to many documents have a very low weight and are essentially eliminated if they are too common because the weighting is so low  reducing our feature
space   and a disease with a single word appearing many times in its articles is not tied so closely to
that word 
stop words are a human made list of word that carry no information  which are removed from the
documents  document frequency based removal refers to removing words that are in more than    
of documents  non alphabetical removal refers to removing words that do not contain alphabetical
characters  the idea behind these techniques is that the words they remove are not predictive of
which disease a user might have because they are words that do not mean anything they only contribute to statistical noise  with these techniques we reduced our number of features from       to
      
stemming is a process that reduces a word to a more basic form  for example  the words reduces 
reduced  reducing  and reduce map to reduc after stemming  stemming tries to capture the
meaning of a word but discards its grammatical form  by applying stemming we reduce our number
of terms from       to       with all other techniques applied  

 

methods

we implemented several learning algorithms for this problem  multinomial naive bayes  logistic
regression  cosine similarity  and randomtrees  the implementations of these algorithms are
described below  we also implemented an svm  bayesian network  gradient boosted trees  and
artificial neural network  but these algorithms were implemented for the cs     portion of this
project and are not described in this paper 

   

naive bayes

naive bayes algorithms are a set of supervised learning algorithms based on applying bayes theorem with the naive assumption of independence between every pair of features  bayes theorem gives
us 
p y x   

p y p x y 
p x 

where y is the class label we predict and x is a feature vector we want to classify  since we want to
predict the class label with the highest probability given our feature vector  we have 
y   arg max p y x    arg max
y

p y p x y 

y

p x 

  arg max p y p x y 
y

the naive bayes assumption that the probability of a word appearing in a document is conditionally
independent of any other words appearing in the document gives us 
y   arg max p y p x y    arg max p y 
y

y

n
y

p xi  y 

i  

where xi is the value of the ith feature in x  which in our case is the occurrence of a given word in
the document  we compute the likelihood of each disease and rank them based on their probability 
we implemented the multinomial version of naive bayes which models p x y  as a multinomial
distribution  the distribution is parametrized by the vectors y             n    where n is the number
of words in our dictionary and y represents a disease  yi represents the probability of word i
occurring in a description for a given disease  by maximum likelihood  we estimate our parameters
n  
as yi   nyyi n   where nyi is the number of times feature i appears in descriptions of disease y in
the training set and ny is the total number of all words that occur in the descriptions of diseases y  
is a smoothing parameter and was empirically chosen to be one  the priors  p y   were not learned
from the data set in our case  but rather were supplied as described in the priors section  or were
assumed to be uniform 
 

fi   

logistic regression

to use logistic regression in a multiclass classification problem we used a one versus rest scheme 
let c be the number of classes in the classification problem  under this scheme  we train c logistic
regression models  one for each class  each of which provides a probability that the input vector
belongs to its class  we can order classes by probability from most likely to least likely and choose
the most likely for prediction 
 
each logistic regression model is trained with l  regularization where      
   was chosen
empirically  the mathematics behind each of the models is derived below 
each model learns a parameter vector  with the same length as the input vector  the probability
each model predicts is given by 
 
h  x    g t x   
    exp t x 
where x is the input feature vector  to find the value of  we maximize likelihood of the data 
     

m
y

p y

 i 

 x

 i 

     

i  

m
y

y  i 

 h  x  

 y  i 

    h  x  

 

i  

m
x

y

 i 

log h  x         y

 i 

 log    h  x  

i  

applying regularization 
      

m
x
 t
 i 
 i 
  
y log h  x         y  log    h  x  
 
i  

now we derive the stochastic gradient ascent rule to find the maximum likelihood  
d
dj

       y

   y

 
g  t x 
 
g  t x 

     y 
     y 

 
d
t
 
g  x   j
   g  t x  dj
 

t

   g  t x 

t

t

 g  x     g  x  

d
dj

t

 x  j

t

   y    g  x        y g  x  xj  j    y  h  x  xj  j

this gives us the stochastic gradient ascent rule 
j    j    y

   

 i 

 h  x

 i 

 i 

  xj

 

cosine similarity

cosine similarity can be thought of as a nearest neighbors method  cosine similarity stores the
document term matrix  then  when the user enters a query  that query is also transformed into
a vector of word counts  to predict a disease from a query  for each document  documents and
diseases are interchangable in this model  the algorithm calculates a score for the document query
pair  given by 
d    q
scored  
       q     
   d  
where d  is the vector representation of document d   q is the vector representation of the query 
this score represents the cosine of the angle between the query and the document vector  the
algorithm sorts the documents by this score and returns this list as the result  with higher score
meaning the document is a closer match to the query  the top scoring document is given by 
d    q
d   arg max
       q     
d    d  
in order to avoid having multiple documents map to the same disease in the result  documents
that map to the same diseases are combined in the document term matrix before normalizing the
document vectors  this is done by adding the document vectors element wise 
   

random trees

we used the scikit learn implementation of random trees  we did some hyper parameter tuning and
found results stopped improving after about     trees  we also found that having no fewer than four
examples in a leaf node was a good way to reduce tree depth and improve generalization  in addition 
we tried a variant of gradient boosted trees that has recently been popular in kaggle competitions
     we found that the results were generally no better than the random forests we were already
using  and the long training time  on the order of hours  discouraged us from trying to parameter
tune an improved variant 
 

fi a  error on training set  training on both freebase
and mayo clinic data and testing on the freebase descriptions  these are our top five accuracy results 
 b  data set error analysis  using all of our feature
manipulations with the given training data  these are the
top five accuracy results  to provide a sense of significance of these results  random guessing would have
a        top one accuracy while physicians average
a       top one accuracy      the best naive bayes
model achieves a     top one accuracy on the manual
test data and     top one accuracy for wikipedia test
data 

 

results

as seen in figure  b  the algorithm that yielded the overall the best results for the manual test set
was naive bayes  tf idf was the feature manipulation that dramatically increased the results for
almost all algorithms  we consider the manual test sets the best metric for success because they best
represent an input that a user of this application would input 

figure    feature manipulations error analysis  using all of our data sources with the given feature
manipulations  these are the top five accuracy results 

 

cs    project

in general  wed used this concept of disease classification across both our cs    project and cs   
project  here we provide a terse summary of the methods explored in our cs    project  in general  our cs    project focused on exploring relationships across our dataset and designing novel
classification methods  we implemented lsa      and lda     to perform automated document
similarity  to automatically match articles across our datasets  similarly  we used word vec      to
train our own word embeddings on our corpus and compared those to the results from glove     
for symptom alias detection  additionally  we implemented a bayes net for classification and a
 

fifigure    using naive bayes with all of our feature extraction techniques  we train the model on a
subset of the diseases and test only on diseases in that subset to produce the plot below  classes are
equivalent to diseases  
novel method for applying convolution neural networks to text classification   by using our word
embeddings to order our feature vectors  unlike previously published work on text cnns            

 

conclusion thoughts for industry

there are many web applications that predict diseases  all of them using hand crafted metrics and
experts in the field to construct the model  using only free text from the web and no field knowledge 
we were able to get results that were comparable to the       success rate seen by physicians  this
was done in two months without medical records or heavy clinical testing and we still had problems
we wanted to tackle  such as finding priors and reducing medical jargon 
although doctors are irreplaceable  it would be beneficial to the healthcare system to have a way
to reliably get an assessment of health without human interaction  we see this application going
beyond a plain text description of symptoms  to a fully interactive system that can measure full
biometrics and gauge health in ways that a human simply cant  we are far from the day where we
can reliably get a diagnoses and prognosis for medical conditions  but artificial intelligence will play
a key role in the improvement of medical paradigms in the years to come 

references
   
   
   
   
   
   
   
   
   
    

david m blei  andrew y ng  and michael i jordan  latent dirichlet allocation  in  the
journal of machine learning research           pp          
tianqi chen  higgs boson discovery with boosted trees  in  may              pp       
scott deerwester et al  indexing by latent semantic analysis  in  journal of the american
society for information science            pp          issn             arxiv  arxiv 
         v  
kenneth r foster  robert koprowski  and joseph d skufca  machine learning  medical diagnosis  and biomedical engineering research   commentary  in  biomedical engineering
online              p      issn          x 
m  c  frank and n  d  goodman  predicting pragmatic reasoning in language games  in 
science             pp          issn             doi          science         
rie johnson  effective use of word order for text categorization with convolutional neural networks  in          pp          arxiv  arxiv          v  
am kibriya et al  multinomial naive bayes for text categorization revisited  in  in ai      
advances in artificial intelligence         pp          issn           
igor kononenko  machine learning for medical diagnosis  history  state of the art and perspective  in  artificial intelligence in medicine              pp         issn            
ashley n  d  meyer et al  physicians diagnostic accuracy  confidence  and resource requests  in  jama internal medicine                p        issn            
tomas mikolov et al  distributed representations of words and phrases and their compositionality  in  advances in neural information processing systems        pp           

 

fi    

    
    
    
    
    

jeffrey pennington  richard socher  and christopher d  manning  glove  global vectors
for word representation  in  proceedings of the      conference on empirical methods in
natural language processing  emnlp              pp            url  http   www 
aclweb org anthology d        
paul sajda  machine learning for detection and diagnosis of disease  in  annual review
of biomedical engineering             pp          issn            
karl michael schneider  techniques for improving the performance of naive bayes for
text classification  in  computational linguistics and intelligent text processing i        
pp         
jyoti soni et al  predictive data mining for medical diagnosis  an overview of heart disease prediction  in  international journal of computer applications              pp       
issn             doi                     
michael subotin and anthony r davis  a system for predicting icd    pcs codes from
electronic health records  in  workshop on bionlp bionlp         pp       
xiang zhang and yann lecun  text understanding from scratch  in  corr
abs                    url  http   arxiv org abs            

 

fi