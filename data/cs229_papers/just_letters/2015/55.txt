swing copters ai
monisha white and nolan walsh
mewhite stanford edu   njwalsh stanford edu
fall       cs     stanford university

   introduction
for our project we created an autonomous player for
the game swing copters 
    swing copters
the goal of swing copters is to keep the avatar alive
as long as possible  the avatar dies when it collides
with a platform  a hammer  or either edge of the
screen  which we will call a wall   a player has one
way to control the avatars trajectory  tapping the
screen to reverse the avatars acceleration 
despite being a simple game  it is surprisingly
difficult to play  beginning players can rarely even
make it past the first platform 

left 
a screenshot of
swing copters
gameplay

play well  without many hours of practice  people
have difficulty navigating through even a few
consecutive platforms 
    setup
the input to our algorithm is the current game state 
the positions of the avatar  the swinging hammers 
and the platforms  the velocity of the avatar and
angular velocity of the hammers  and the acceleration
of the avatar  we used mcts and q learning to
create players who would then output the players
chosen action  click or dont click  corresponding
to negating the avatars acceleration or not  
we implemented our own version of the game to use
for training and testing our algorithms  we also
implemented a modified version of k means
clustering to extract information about the game state
from screenshots of the original version of the game 
this serves as a proof of concept that our player
could be used to play the game on a mobile device or
emulator  the input for this problem was a screenshot
of the game  and the output was a location  x  y
coordinates  of the avatars location in that
screenshot 

right 
    motivation
video games provide a great testing ground for
machine learning algorithms and techniques 
consequences of failure are low  and many games
pose unique and unanticipated difficulties that would
not occur in the real world 
swing copters is a good candidate for a machine
learning project since it is very difficult for humans to

our version of
swing
copters

fi   related work
computer games are a popular area for machine
learning algorithms  in       engineers at google
released a paper detailing a system called
deepmind  this system used a combination of qlearning and a neural network to play old atari
games with great success  mnih  kavukcuoglu 
silver  graves    antonoglou         this paper
motivated much of our work on swing copters even
though we did not end up using a version of their
deep q network algorithm 
there are several instances of people detailing
methods creating autonomous players for flappy
bird  a similar mobile game  gatt       vaish       
all implementations we found use some form of qlearning  this is a reasonable approach since qlearning lends itself to work in video games in
general  in addition to playing games created for
humans  q learning is also being used as a way to
control the behavior of non player characters in new
games  patel       
we only found one other implementation of a
learning algorithm for the game swing copters  that
project also used q learning  but performed their
research on a simpler version of the game without the
swinging hammers  miller  beiser         our work
improves upon their results by using a full model of
the game 
   dataset and features
     q learning player

 avatars x distance to the platform its facing
 avatars x distance to the nearest wall
 avatars x distance to the wall its facing
 product of avatars distance from center  velocity 
and acceleration
 avatars minimum stopping distance from wall
 avatars minimum stopping distance from
platform
 indicator on whether or not the avatar had enough
time to make it to the gap between platforms
 product of avatars signed x distance to the gap
between platforms and acceleration
each of the continuous features above was discretized
into indicator features on ranges of their values before
being given to the q learning algorithm 
the biggest challenge for feature selection in our
implementation  was that to be successful  a set of
features needed to be able to differentiate between
two very similar states  at any point  a swing copters
player can choose between two states  one where the
avatar is accelerating to the right  and one where the
avatar is accelerating to the left  the first features that
improved our results significantly were indicators on
the avatars stopping distance from a wall  how
close to a wall the avatar would be if the player started
accelerating in the opposite direction immediately 
we computed this stopping distance from the wall
that the avatar was moving towards  these features
allowed our player to learn how to avoid crashing into
the walls  sides of the playing area   computing
similar features on the avatars relation to platforms
and hammers were also helpful 

as input  our algorithm takes a single complete game
state  all values needed to describe the game state are
discrete  on the order of pixels   in order to
completely describe a single state of swing copters 
we give our algorithm the positions of all game
objects  hammers  platforms  and the avatar  along
with their respective velocities and the avatars
acceleration  we then compute features on the game
state 

    k means avatar detection

the biggest challenge to our approach was coming up
with useful features  we considered many different
ways to combine aspects of the game state 

   methods

 product of avatars velocity and acceleration
 product of avatars distance to gap between
platforms and acce eration  with differing
velocities 

for the avatar detection portion of our project we took
    screenshots of gameplay on the original version
of swing copters  we then compressed each
screenshot down to a     by     pixel image in order
to lower the runtime of the algorithm  next  we
annotated the avatars location  x  y coordinates  for
each screenshot  this formed our test set 

    monte carlo tree search
before we built our q learning player  we
implemented a player that uses monte carlo tree
search  mcts  

fione difficult aspect of swing copters is that the
player can only control the direction of the avatars
acceleration  all actions produce magnified
differences in the avatars future position  we wanted
to factor in these delayed changes in position by
seeing how the game would play out given a
particular action  mcts is well suited for this
because it estimates the reward of a particular action
by performing a series of simulated games 
each iteration of mcts has four phases  selection 
expansion  simulation  and backpropagation 
   in the selection phase  we select an
unexplored state of the games state tree  s 
   during expansion we expand the state tree to
include all states that could result from
actions taken from s 
   during simulation  we play a random game
starting from s 
   finally  in backpropagation we update the
predicted values of each state along the path
from s to the root of the state tree 
another aspect of swing copters that makes mcts a
worthwhile approach is that consistent short term
survival leads to success  a player using mcts
computes the expected reward by averaging the
randomly played games  in swing copters  although
random play will not lead to long term success  it can
give a good idea of the likelihood of short term
survival 
    q learning
q learning is a reinforcement learning technique that
updates a model using state  action  reward  next state
tuples in order to try and learn the best action value
function  for our project we implemented q learning
using a linear function approximation model as
follows 
  define features  s  a  and weights w
  define qopt s  a  w    w   s  a 
on each  s  a  r  s  
w   w   qopt s  a  w  
 r    vopt s    s  a 
here qopt is the current best estimate of the value of
a state  action pair  linear function approximation
means that we estimate qopt using the inner product
between the current weights w and the feature vector
 s  a   as in linear regression  from each  s  a  r  s 

we attempt to update w so that future predictions
better match the true value of an action  using our
learned qopt from each state we choose the action
that has the highest estimated value 
    k means avatar detection
k means is an unsupervised learning algorithm that
finds clusters in data 
   randomly initialize a set of cluster centroids
      k
   repeat until convergence 
for each i set 
for each j set 
k means alternates between assigning every data
point to the nearest centroid  and updating each
centroid to the average of all points assigned to it  the
algorithm converges when all points remain in the
same clusters
our goal with k means was to take advantage of the
fact that different objects in swing copters are
colored differently 
k means avatar detection 
input  screenshot of swing copters
   assign     average color of avatar
   assign    k   color of random pixel in the
image
   run k means to convergence keeping  
constant
output  the average location of pixels assigned to   
the same algorithm can be extended to find the
location of the hammer and platform objects  first we
initialize   to be the average color of the object we
want to detect  then  instead of returning the average
location of pixels in that cluster  we perform k means
on the location values of those pixels  this gives a set
of clusters  with each one centered around one
instance of the object being detected  at any point in
swing copters there are either four or six platforms
and hammers  thus  we can run this algorithm with
k     and k     and pick the result with the tightest
clusters 

fi   results
in order to measure the success of a player  we performed a series of trials and averaged the number of frames
of the game that the player survived 
    mcts
the mcts algorithm produced a mediocre swing copters player  the player regularly navigated through a
few platforms  already better than a beginning human player  however  it was not consistently successful  and
rarely scored better than a practiced human player  scores improved slightly with more simulated games 
however  the computation time required increased drastically  making it impractical as a way to play the game
in real time 

    q learning player
the quality of the q learning player depended heavily on the set of features it used to learn  as we discovered
more useful features  the player was able to avoid more objects and survive longer  using our full set of useful
features  our player can survive indefinitely 
features

survival time  frames 
 n    

stopping distance from wall

      

stopping distance from wall
  platform

       

stopping distance from wall
  platform   multiple distances  
  vertical space

indefinite

human player
beginner

      

practiced

       

fifor our q learning player we tested different exploration probabilities  and discovered that a value of zero
worked best  there is enough randomness inherent in the game that many different states were still explored 
at the same time  the inputs required to be successful in swing copters are precise  so choosing a random action
will often lead to dying shortly after 
    k means avatar detection
we performed our k means detection algorithm on the set of annotated screenshots using different values for
k  we calculated our error as the average number of pixels our prediction differed from the annotation 
as k increased we saw a decrease in
our error  we realized that this was
because with higher values of k  it was
more likely that one of the centroids
was initialized to be the color of the
platforms  the closest color to the color
of the avatar  

by initializing one of the centroids to
the average color of a swing copters
platform  we were able to bring the error
down to less than two pixels 

   conclusion
we are very impressed by the results from our q learning player  using the top performing set of features  the
player learns to survive indefinitely  the q learning player far out performed the mcts player and practiced
human players  while we saw that mcts continued to improve its play as we increased the number of
simulated games  doing this also decreased its speed to below acceptable performance  the q learning player
only needs to perform two vector multiplications between the weights and the features of the two possible state
action pairs in order to decide which action to take  this leads to lightning fast performance  we were also
pleased by the success of the k means algorithm for detecting the avatars location  an error of less than two
pixels can create an accurate enough game state to successfully play the mobile version of the game  with
more time  we would like to extend the algorithm to detect the locations of hammers and platforms  then we
would be able to complete our original goal of creating a player that plays the original version of the game on
a mobile device 

fireferences
gatt  brian   reinforcement learning   flappy
bird   goldsmiths university of london           
 http   www briangatt com portfolio flappy documentation pdf  
miller  jared  and chris beiser   creating
autonomous swing copters using rl
techniques      dec        web     dec 
     
 https   github com cadtel qcopters blob mast
er qcoptersreport pdf  
mnih  v   kavukcuoglu  k   silver  d   graves 
a     antonoglou  i          playing atari
with deep reinforcement learning  nips
patel  purvag   improving computer game bots 
behavior using q learning   southern illinois
university carbondale    dec       
vaish  sarvagya   flappy bird rl        
 http   sarvagyavaish github io flappybirdrl 
  

fi