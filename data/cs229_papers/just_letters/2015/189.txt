automatic hilghter of lengthy legal documents
yanshu hong
computer science department
email  yanshuh stanford edu

   introduction
legal documents are known for being lengthy  to our knowledge  some categories of legal documents contain duplicated information that do not require our attention  however  manually
extracting non duplicate information from documents requires
considerable amount of effort  thus  we want to use machine
learning algorithms to pick up unordinary sentences for us 
in this paper  we propose a set of algorithms that filters out
duplicate information and returns useful information to the user 
we are able to train a learner that can mark unordinary parts
of a legal document for manual scrutinization 
our learning process contains two phases  at the first phase 
we pick some legal documents that contain common patterns 
e g  software user agreements  to form a knowledge base for the
trainer  we then run lda     model on these documents  the
lda model will return us with a set of common topics across
the knowledge base 
at the second phase  we take a new piece of legal document
as the test sample  we first remove common topic words from
the test document to increase differences between sentences 
we then use word vec          to convert sentences into vectors  after generating the feature space  we run agglomerative
clustering and local outlier factor lof      algorithms on
the feature vectors to detect special sentences in the given
document  last  we use pca and t sne to visualize our result 

tian zhao
electrical engineering department
email  tianzhao stanford edu

extract ones closer to the center 
based on the work of qazvinian and radev  cai       
    presented a salient sentence extraction strategy by adding
a ranking policy for sentences within each cluster  however 
cai did not discuss on how to select the initial condition of
centroids  in fact  it seems to be the case that the centroids are
randomly selected in cais proposed algorithm  since the final
clustering would vary a lot as the intial condition changes  we
choose to not adopt cais path 
blei  david m   andrew y  ng  and michael i  jordan       
presented the lda model to extract common topic words from
a set of documents  we find this model very robust  and in fact
lda is used widely accross contextual anormaly detection  for
example  mahapatra and amogh      presented a text clustering
strategy based on lda  their model works well  however their
feature extraction part can be improved by using algorithms that
translate the relation between words into vectors  having this
in mind  we use word vec as our major text feature extraction
framework 

   dataset and features
we generate our samples by parsing html pages of legal
documents  here is a sample text from itunes user agreement 

   related work
though text mining related works are often seen  few of
them address the issue of sentence level anormaly detection 
however  we are able to draw inspirations from previous works
on unsupervised auto generation of document summary 
qazvinian and radev            presented two sentences
selection models based on clusters generated by hierarchical
clustering algorithms  c rr finds the largest cluster and rank
the importance of sentences by their order of appearance in the
cluster  c lexrank selects the most centered sentences within
the cluster to be the salient ones  while their clustering strategy
performs well on segmenting an article  their strategy on finding
the salient sentences does not apply to our case  based on
their design  a sentence with the most common pattern would
be chosen as a salient one  however  it is also the case that
such sentence would contain the most duplicate information 
therefore it should not require much attention from the reader 
similarly  nanba and okumura            discussed ways of
citation categorization  which essentially cluster sentences and

figure    a sample text
our training dataset contains    legal documents  which
include         words  we use two documents for testing  one
is the itunes user agreement  which contains       words  we
manually label some special sentences in this document  we
then run our pipeline on this document  and use the number of
labeled sentences highlighted by our pipeline as a measure of
how successful the pipeline is 
the other document is the atlassians user agreement  which
contains       words  due to the size of this document  we

ficannot label all the sentences  instead  we use this document
for generalization test  we run our pipeline on atalassians
user agreement  read the highlighted sentences  and subjectively
check if these sentences are unordinary  since the problem we
defined is unsupervised  we can only evaluate the final results by
subjective means  to increase the credibility of our evaluation 
we also use quantitative methods for evaluating the robustness
of our pipeline  we will discuss more about these methods in
section   
we remove prepositions  numbers  punctuations because they
add noise to our samples  for example  i have iphone and i
have an iphone are treated as the same sentence in our pipeline 
we also realize that some words with different suffixes may
have the same meaning  for example  mature and maturity
should be understood as the same word  in preprocessing step 
we run an algorithm called porter stemming      to convert
words with the same stem and similar suffixes into one word 
we have two different sets of feature extraction and selection
strategies  first  we use lda to construct common topic words
across the training dataset  the feature extraction within lda
is done by taking all documents as a single corpus and then
by applying the variational inference method discussed in the
original paper 
after running lda on all the documents  we get a list of
topic words  a topic is a list of words  e g  trademark  services 
may  use  application  agreement  content  we observe that these
topic words do not contribute to the specialty  of a sentence 
therefore  we remove the topic words from the test document 
second  for the test document  we use word vec to translate
a word into a vector  word vec is a two layer neural network 
it takes text corpus as input and its output is a set of feature
vectors for words in that corpus  for each word  we generate
    features  these word vectors allow for numeric operations
on words  for example  the vector of king added by the vector
of woman would return the vector of queen  after running
word vec  we establish a mapping from words to vectors 

figure    part of word vector of apple
we assume that the feature vector of a sentence is the sum
of feature vectors of all the words in this sentence  we then
run clustering algorithms on the vector representations of all
the sentences 

   methods
we use em algorithm to learn the hidden parameters of the
lda model  we then use agglomerative clustering and lof to
learn the distributions of unordinary sentences in the document 
we will discuss these methods in the following subsections 

     em for estimating lda parameters
the lda model uses a word w as a basic unit  a document is
a sequence of n words denoted by w    w         wn    a corpus contains m documents is denoted by d    w         wm   
lda takes the following generative steps for each document
w in a corpus d 

   choose n  poisson   
   choose   dir   
   for each of the n words wn  
i choose a topic zn  multinomial   
ii choose a word wn from p wn  zn      where p is a
multinomial probability conditioned on zn and  
the probability of seeing an n word document  p  w     
is given as 
r
p
p  w        n
n  
z p  wn  z   p  z   p    d 
to estimate the two hidden parameters        from a corpus
d  one needs to maximize
pm the log likelihood 
log p  d       d   log p  w d       
which is not tractable  to resolve this issue  the lda paper
recommends to use variational inference method  the key of
this method is to apply jensens inequality to obtain a lower
bound of log p  w      let q z    be the joint pdf of z   
then 
rp
p  w z    
d
log p  w     
z q z    log
q z  
  l     
note that  r p
p  w z    
l      
d
z q z    log
q z  
rp
p  z  w   
  log p  w     d
 
z q z    log
q z  
rp
q z  
 
q z    log p  z  w    d
rp z
 
q z 
  log p  w    d
z
  kl q z     p  z   w         log p  w    
 log p  w       l       kl q z     p  z   w       
since we want to find a l     that is as much close to
log p  w     as possible  we want to find a parameterized
q z    that has as small kl distance to q z    as possible 
let z   be parameterized by    respectively 
 n 
i e   q z      q   n
  
n   q zn  
let l         denotes the lower bound of log p  w     
then 
pm
n  d
log p  d      d   l w d       d     d     d       
  l          
where      n d           d    
the idea of em is to start from initial     and iteratively
improve the estimate 
e step  given    
        arg max    l         
m step  given    
        arg max    l         
these two steps are repeated until l converges  we wont
show the complete derivation here  but we use the following
updating equations for getting      
in e step  the update equations for  and  are 
p  d 
 n d 
 d 
i
  i w d  exp z i    z  j j   
n
where z is the digamma function 
pnd  n d 
 d 
i    i   n   i
 
in m step  the update equation for  is 
pm pnd  n d 
ij   d   n  
i
  wnd   j  
there is no analytical solution to   but it can be obtained by
maximizing
p
pk
pk
l   d  log   i   i    i   log  i  
pk
pk
 d 
 d 
  i    i     z i    z  j   j    
using newtons method  where  is the gamma function 

fi     agglomerative clustering

   experiments and results

for a new test document  we first get the feature vector
of each sentence  we then use agglomerative clustering to
segment the document  we first specify that we want to have
n clusters  we then put each sentence into its own cluster 
and iteratively merge the closest clusters until only n clusters
remain  we then look at these clusters  if there exists a cluster
that contains only one sentence  then we know that this sentence
is a special one  as it is never merged into other clusters 
we define the cosine similarity between two sentences as the
dot product of their normalized vector representations  we then
calculate cosine distance from cosine similarity and use this
distance as our clustering metric 
the distance between any two clusters a and b is taken to
be the mean distance between elements of each cluster      
in section    we will discuss on how to choose the right
number of clusters to achieve the best clustering structure 
it is worth mentioning that we find k means and k means  
performing much more unstable than agglomerative clustering
in our case  the final outcome of k means and k means  
depends heavily on the initial assignments of the centroids 
however  since both methods choose initial centroids in a semirandom fashion  it is not guaranteed that the final clustering
would remain the same from run to run  in contrast  agglomerative clustering does not depend on initial assignments  as
a result  it creates more stable outcomes 
before running lof  we recommend to run agglomerative
clustering first  and then remove the discovered anormaly
sentences from the test document  the reason is that we want
to decrease the local density around unordinary sentences so
that these sentences can be more easily picked up by lof 

in this section  we first discuss how we choose the hyperparameters for lda and for agglomerative clustering  we will
then talk about the performance achieved by our pipeline on
the test cases 

     tuning hyperparameters
in this subsection  we will use the itunes user agreement as
an example to show how our hyperparameter choosing strategy
works  there are    sentences and      words in this document 
       choosing parameter for lda model  for lda model 
number of topics is a critical parameter  we enumerate over
possible number of topics and study how the perplexity changes 
we then use the number of topics that yields the best perplexity 
documents in our training dataset are unlabeled  therefore 
we wish to achieve high likelihood on a held out testset  in
fact  we use the perplexity of a held out test set to evaluate the
model 
intuitively  a lower perplexity score indicates better generalization performance  the perplexity is equivalent to the inverse
of the geometric mean per word
pmlikelihood 
log p wd  
pm
  
perplexity dtest     exp  d  
d   nd
using the same notation as in section     
in our experiment  we use the corpus generated by parsing
html pages of legal documents from atlassian  github  and
apple  we held out     of the data and trained on the
remaining     

     local outlier factor  lof 
in our feature space  each sentence is represented as a point 
therefore  if we find a point with low local density  then this
point is an outlier  and its corresponding sentence is unordinary 
to help explain how lof works  we introduce two terms 
nk  a   set of k nearest neighbors to a 
k d a   distance from a to its k th nearest neighbor 
rdk  a  b    max k d a  b   distance a  b    where rd
denotes reachability distance  we use the maximum of two
distances to get a stable result 
the local reachability
density of a is computed as 
p

figure    perplexity given number of topics
we found that the perplexity is the lowest when there are
around      topics  we found that for the itunes user
agreement  using   topics works the best for improving the
differences between sentences 

rdk  a b 

  
lrd a        bnk n a 
k  a  
it is then compared
p with its neighbors using 
lrd b 

k  a 
lofk  a    bn
 lrd a  
 nk  a  
which is the average local density of neighbors divided by the
objects own local density  if this value is greater than    it
tells that the local density around this object is lower than its
neighbors  and thus is an outlier 
this method studies the local distribution of points  and
helps us improve the performance on top of agglomerative
clustering  basically  it is hard to have insight of how points are
distributed within an agglomerative cluster  using lof helps us
understand the distribution of points within a cluster 

       choosing number of clusters  we use silhouettes     
score to evaluate the robustness of clusters  we enumerate over
possible numbers of clusters  and pick the one that yields the
best silhouettes score 
intuitively  larger silhouettes score indicates that the distance
between each cluster is large  and the maximum distance from
the farthest point in a cluster to its centroid is small  the formal
definition is given as follows 
for each point i  let a i  be the average dissimilarity of i
with all other points in the same cluster  let b i  be the lowest
average dissimilarity of i to any other cluster  of which i does
not belong to  the cluster with the lowest average dissimilarity

fiis defined to be the neighboring cluster of i  this cluster is also
the next best fit for i  silhouettes score is defined as 
b i a i 
  where    s i     
s i    max a i  b i  
if s i  is close to    it means that the average dissimilarity
between i and its neighboring cluster is much greater than the
dissimialrity between i and its own cluster  in this case  we say
that i is properly clustered  in contrast  if s i  is close to     it
means that i would be more properly labeled by its neighboring
cluster  we use the average of s i  over all sentence vectors of
the test document as a measure of how tight each cluster is and
how far away each cluster is from its neighbors  the curve is
shown in figure   

figure    first   dimensions of pca result
in figure    result of pca also shows that unordinary sentences are picked up at locations with lower local densities 
however  in figure    points are denser than in figure   
this indicates that t sne does a better job on signifying the
differences between points than pca does 

     tests

figure    silhouette score given number of centroids
from figure    the average silhouettes score is high when
more than    clusters are used  the reason is that when having
more than    clusters  most of these clusters contain only
one sentence from the itunes user agreement  as a result  the
average dissimilarity within each of these clusters is    and the
corresponding silhouettes score goes up to   
in practice  we do not use more than    clusters  the major
concern is that if most of the clusters contain only one sentence 
then we cannot tell if this sentence is an anormaly or not 
therefore  we only look at the silhouettes scores when we have
few clusters  and we find that having   clusters works the best 

     visualization
in this subsection  we look at how the unordinary sentences
picked up by our pipeline distribute in  d space  we use tsne     and pca to extract three dimensions from features of
our results  in both figures  points in dim yellow or blue are
farther from the view point  the blue triangles mark positions
of unordinary sentences 

figure    first   dimensions of t sne result
in figure    as we can see  the unordinary sentences locate at
places with lower local density 

since we are trying to solve an unsupervised learning problem  it is hard to quantify test accuracy  to mitigate that  we
create test cases by using three groups of sentences 
   we insert irrelevant sentences into the test document  for
example  we added we are such stuff as dreams are made
on our little life is rounded with sleep  which is a quote
from shakespeare 
   we modify original text  for example  we change apple
reserves the right     to apple reserves     to be scrutinized
by great fire wall of china 
   we subjectively choose some special sentences  for example  we highlight you further agree to not    harassing 
threatening  defamatory     as a special sentence 
for itunes eula  small test case   we inserted   totally irrelevant sentences  modified   original sentences  and subjectively
chose    special sentences 
for atlassians eula  large test case   we inserted   totally
irrelevant sentences  modified    orignal sentences  and subjectively chose    special sentences 
we choose to not insert or modify a lot of sentences within
the test documents  the reason is that as the number of modified
or inserted sentences increases  the chance of randomly picking
one sentence and observing that the sentence is in group   or
  also increases  thus  inserting and modifying few sentences
in the large test document help to show that our pipeline does
not succeed by choosing sentences randomly 
we run our pipeline on these test cases   and we check how
many of sentences in each group are picked up by our pipeline 
the result is shown in the following table 
  of words
  of sentences
  of group  
  of group  
  of group  

itunes eula
    
  
     
      
    

atlassians eula
    
   
     
    
    

from the table  it is shown that our pipeline is very accurate
on picking up irrelevant sentences  however  it does poorly
on picking up partially modified sentences  we find out that

fiour assumption on generating the sentence vector causes this
problem  and we will discuss possible improvements in section
   it seems that our pipeline does not perform well on picking up
group   sentences  the reason is that when manually creating
test cases  it is very hard for us to pick up all the unordinary
sentences  moreover  we sometimes pick up sentences that are
actually quite common in legal documents due to our limited
knowledge of how user agreements are written 
therefore  to better evaluate the performance on group  
sentences  we read the output from our pipeline  and manually
construct a confusion matrix for it  for the large test case 
true positives    
false negatives  n a

false positives   
true negatives  n a

we cannot give an account on false negatives and true
negatives  because for a lot of sentences we cannot confidently
tell if it is too ordinary 

references
    blei  david m   andrew y  ng  and michael i  jordan  latent
dirichlet allocation  the journal of machine learning research  
                 
    mikolov  tomas  et al  efficient estimation of word representations in vector space  arxiv preprint arxiv                  
    mikolov  tomas  et al  distributed representations of words and
phrases and their compositionality  advances in neural information processing systems       
    breunig  markus m   et al  lof  identifying density based local
outliers  acm sigmod record  vol      no     acm       
    van der maaten  laurens  and geoffrey hinton  visualizing data
using t sne  journal of machine learning research            
           
    radim rehurek  optimizing word vec in gensim  nov         

   future work
in this report  we propose a pipeline that finds unordinary
sentences in a given legal document  we construct the pipeline
by first building a training dataset of many legal documents
sharing similar topics  we then use lda model with em
algorithm to find the common topic words across the dataset 
when coming to a new test document  we first remove common
topic words from it to increase differences between sentences 
we then use word vec to extract sentence features  cluster the
sentences and use lof to find the outliers  finally  we run tsne and pca to visualize the first   dimensions of the result 
we evaluate the performance of our pipeline using two types
of method  first  we check perplexity of the lda model and
silhouettes score of the clusters to measure the robustness of
our model  second  we manually create test cases by adding
three groups of sentences  and we measure the performance of
our pipeline on these test cases 
regarding finding unordinary sentences  the majority of the
job is done by lof  this is because most of the sentences use
similar words and are clustered closely  as a result  clustering
cannot filter out a lot of unordinary sentences  however  lof
only considers the local density of an area regardless of which
cluster it belongs to  as a result  lof is able to pick up more
differences between sentences 
future work includes improving our sentence model  meanwhile  we make a naive assumption that the vector of a sentence
is the sum of vectors of words  however  this assumption
ignores the semantics of phrases in a sentence  therefore  the
next step of improvement would be to consider more of relation
between words within a sentence  for example  we can use a
bigram or trigram model to address the correlation between
words in a sentence 

acknowledgments
we would like to thank cs    staff for their help through
the quarter 
we would like to thank albert haque  our project mentor 
for his great ideas that help us improve the project 
we would also like to thank prof  andrew ng for the great
class 

    qazvinian  vahed  and dragomir r  radev  scientific paper
summarization using citation summary networks  proceedings of
the   nd international conference on computational linguisticsvolume    association for computational linguistics       
    cai  xiaoyan  et al  simultaneous ranking and clustering of
sentences  a reinforcement approach to multi document summarization  proceedings of the   rd international conference on
computational linguistics  association for computational linguistics       
    nanba  hidetsugu  and manabu okumura  towards multi paper
summarization using reference information  ijcai  vol           
     mahapatra  amogh  nisheeth srivastava  and jaideep srivastava  contextual anomaly detection in text data  algorithms    
                
     porter  martin  the porter stemming algorithm        see
http   www tartarus org martin porterstemmer 
     sokal  robert r  a statistical method for evaluating systematic
relationships  univ kans sci bull                      
     rousseeuw  peter j  silhouettes  a graphical aid to the interpretation and validation of cluster analysis  journal of computational
and applied mathematics                  

fi