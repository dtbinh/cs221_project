stanford university

predicting volatility in equity markets using
macroeconomic news

cs     final project
authors 
carolyn campbell
rujia jiang
william wright

supervisors 
professor andrew ng
 ta  youssef ahres

dec         

fi 

   introduction
on june           months of debt negotiation between the greek government  headed by prime minister
alexis tsipras  and its creditors  including the imf and fellow eurozone countries  broke off abruptly  tsipras
announced a snap referendum regarding the terms of the pending bailout  by the following morning  s p   
was down significantly  as political and economic uncertainty grew in europe  investors across the world
were moving their funds away from risky assets that could be negatively affected by a greek default  and
towards safer assets  days later  when the greek situation had resolved  the process reversed as equities
rallied 
these market movements are not uncommon  just weeks later  crisis erupted in the chinese equity markets  and asset values were responding to the uncertainty this provoked  in this paper  we investigate how
macroeconomic sentiment immediately impacts the volatility in liquid markets  by measuring market volatility through the vix  volatility index of the s p       using data pulled from twitter  we are researching
how breaking economic news affects the markets  and subsequently how to predict which news stories can
increase volatility  we will consider a tweet significant  in that the news presented in the tweet contributes
to volatility in the market  if within    minutes of the tweet being tweeted  the volatility of the asset increases
by one fifth of a standard deviation 
by employing three machine learning techniques for classification  we aim to predict the volatility inducing
power of a single tweet and or news story  using a training set  we will identify key words and their associated
probability of increasing volatility in the markets using naive bayes  svm and logistic regression  the input
of our algorithm will be the word count of each bucket of tweets in a dictionary we created  we then use the
three prediction methods to output a predicted increase in the vix  which will be a binary variable  does
it increase by a fifth of a standard deviation or not   in order to create a viable trading strategy  we aim to
predict moves with above     accuracy 
   past work
there is a lot of research on predicting market price movements by conducting technical analysis using
historical and time series data      a large portion of the studies fall into developing techniques to analyze markets trends from texts such as financial market news     or social media posts      some tactics
include tokenizing each article  and matching them with stop word lists      using a subset of article terms
as features      and using tagging of named entities to group nouns into predefined categories      the latter
two techniques are more frequently used in question answering  qa  systems  where themes have been
predetermined      however  to analyze the sentiments within markets without knowing the objectives  the
lexicon based approach is a more robust technique  although it is subject to the effect of words around a
single word  and trends can sometimes be hard to detected by using single word weighting     
once texts have been dissected into data sets  there are several machine learning approaches to make predictions in price movements in various financial products  one is support vector regression  which performs
linear regression in the high dimension feature space to predict stock prices      logistic regression and
neural networks are also plausible in analyzing stocks trends      however  all these methods are sensitive
to bias and noise  and the study performances tend to improve as more noises are removed from original
data sets  in addition  the majority of past research focus on price prediction of specific stocks  and very
few of them focus on the volatility of the overall markets  we have taken inspiration from various article
and the techniques used  such as svm and logistic regression  but have elected to view this is a classification
problem since we are not focusing on single name stocks but rather market volatility 
   dataset and features
we have pulled macroeconomic
the accompanying news article 
condensed format  in addition 
short views on the market  we

news from twitter  news sources will tweet a headline and the link to
which allows for us to access the key topics frequently and in a rather
hedge funds  banks  and analysts will frequently tweet either articles or
have selected    accounts that we deemed relevant  and tweeted with a

fi 

significant frequency  the twitter api allows for the most recent      tweets per account to be exported
from the website  which provides at least a month of tweets  and hence  news  per account  because we are
investigating the immediate market reaction  and therefore using intraday data  this is more than sufficient 
this amounted to more than         tweets  bloomberg retains intraday data  at    minute intervals  for
  months  which we have also pulled  we bucketed tweets into thirty minute intervals  so that instead of
regarding an individual tweet as a single data point  we are looking at clusters of tweets  the aim of this
strategy is to reduce the noise in the model and reduce the amounts of false classification  for instance  if
cnn breaking news is tweeting about kanye west having a baby at the same time that oil prices begin to
plummet  both would be associated with moves in volatility  the kanye west tweet has no impact on moves
in the s p      but if treated as its own data point  the model might believe that kanye was a key word 
we consider only tweets between   am and  pm to align with the market intra day data 
we also constructed our own dictionary  based on our universe of tweets  rather than using a pre implemented
dictionary  because of the hashtag feature on twitter  we suspect that there are key words that will affect
market movements that are not real words  such as  grexit  and are commonly used to unify a particular
topic  similarly  analysts often will use the stock ticker in a tweet when discussing an update about a singlename stock  such as  aapl   by constructing our own dictionary  we ensure that these trending topics are
included  which can perhaps be even more significant than actual english words  we cleaned the tweets by
eliminating all characters that were not letters  and eliminated urls  the dictionary that we used contains
just over        words  which serves as our features for the algorithms 
after the processing of data  we reduced our         tweets to     data points  each data point is a vector
the length of the dictionary  corresponding to the frequency of each token  feature  in that block of tweets 
because we are using time series data  we cannot use cross validation techniques  so we use     of the data
     points  for training and     points for testing 
figure   shows a plot of the vix since       daily data   while we used intra day data going back   months 
this illustrates how events  such as the greek financial crisis  the lehman brothers default  the devaluation
of the yuan  and other macro events effect the volatility 

figure   

   methods
     naive bayes  the first classification model we consider is the multi factor naive bayes  the key
assumption of this model is the conditional independence of the features  this amounts to the following
assumption  for any features xi and xj   given that y      i e  that the vix has gone up substantially  then
the knowledge of xi has no effect on our beliefs about xj   mathematically  we say 
p xi   xj  y        p xi  y      xj  
this is a strong assumption to make about our twitter data set  the naive bayes is used as a preliminary
classification  but we explore two other methods that relax this assumption 

fi 

we want to fit the following parameters  k y     p xk  y       k y     p xk  y      and y   p y   for all
k features  in order to fit this model  we maximize the log likelihood function to find the estimates 
m
y
l y   k y     k y      
p x i    y  i   
i  

we additionally apply laplace smoothing to avoid issues when we arrive at a word that is unknown in our
dictionary  we have the following estimates 
pm
pm
pm
  y     nk i    
  y     
  y     nk i    
i  
i  
  k y     pm
  y   i  
k y     pm
m
i     y     ni   n
i     y     ni   n
where m is the number if training points  nk i is the frequency of the k th token in the ith training point  ni
is the total number of tokens in the ith training set  and n is the number of tokens 
having fit these parameters  we make a prediction on a new example with given features x by calculating
the ratio
p y     x 
p y     x 
     support vector machines  svm   subsequently  we employed the support vector machine method 
which performs linear
 regression in the high
  dimension feature to maximize the functional margin  given
a training set s    x    y            xn   y n     for a linear regression model with the linear functionf  x   
hw  xi   b we want to find the weight vector w that solves the following problem
 
minimize
  w  
w
 
subject to y  i   hw  x i  i   b      i           m
where m is the number of training sample  we can solve the dual problem of the above optimization as
m
m
x
x
maximize w     
i 
y  i  y  j  i j k x i    x j   


i  

i j  

subject to i     i           n
m
x

i yi      i           n

i  

where k x i    x j    is a kernel function  which we have chosen to be gaussian  and m is the number of
support vector 
for our project  we use the liblinear package to perform the svm  to use the built in functions in the
liblinear  we constructed our training sets and test sets into sparse matrices of size  a  b   where a  
  of tweets and b     of tokens  each row of the matrix represents a tweet  the j th column of the ith row
represents the number of times the j th token appeared in tweet i  this is a sparse matrix  and therefore we
only record non zero values along with their index numbers  vix going up is indicated as class     and vix
not trending as class    
     pca and logistic regression  finally  we performed logistic regression  logistic regression makes
the least amount of assumptions on the dataset  and since we cannot be sure that the assumption of conditional independence holds true in the naive bayes algorithm  logistic regression makes sense  logistic
regression is a model that measures the relationship between a categorical binary dependent variable  which
we have taken to be whether or not the vix has increased by a certain amount  and the independent variables
 the features of our dictionary  from the tweets   it estimates the probabilities of the categorical variable
using the logistic function 
 
our hypothesis has the form h  x      e
t x   the sigmoid function  we assume that p y     x      h  x  
letting n be the number of features in our dictionary    rnx  is the vector that we are trying to fit in from
out training sample  and x  rnx    such that x      and xi is the count of the ith word in the data point 

fi 

using log likelihood methods  we should be able to fit the parameters   however  we have        features
and only     data points  which makes this impossible  therefore  we need to employ dimension reduction
techniques to make this a feasible strategy 
principal component analysis  pca  is a procedure that reduces the dimension of a set of observations
 tweets  for us  by using orthogonal transformations to convert correlated covariates  the features  into a set
of linearly uncorrelated variables  know as principal components  the first principal component accounts
for as much variance in the data set as possible  and as we increase the number of principal components  we
increase the amount of variance accounted for  each principal component is an eigenvector of the covariance
matrix of the data set  guaranteeing the orthogonality of the components  effectively  we can use a number of
principal components significantly less than the number of features  we began with        features  perform
principal component analysis  and selected the first     principal components to use  reducing our dimension
by a factor of     
by multiplying the first     loadings  a        by     matrix  by our matrix of covariates  we reduce create
a new design matrix of size     x      using this new design matrix  we perform a logistic regression using
the binary vix as our response  the function glmfit in matlab inputs the data and outputs the model
with fitted parameter   the predict function in matlab  inputs the model created from the training set
and tests the model using the     testing data by making predictions of the probability of a vix increase 

   results
we want to compare the three classification models to determine which one best fits our data  therefore 
we use hold out cross validation with     of our data for training  and the remaining     for testing  and
compare the estimated generalization error accuracy of each model  since the impact of certain words on
the vix changes in time  it does not make sense to train with data that occurs after the testing data  for
this reason  we do not use k fold cross validation  as we need our testing data to be the most recent     of
the tweets 
the primary metrics for each model that we are interested in are accuracy  precision  and recall  accuracy
is the proportion of correctly classified data points to total number of data points  precision or positive
predictive value is the number of true positives over the total number of positives predicted  or the probability
that a positively predicted data point is actually positive  recall is the proportion of positive data points
that are correctly classified  in other words  we have the following formulas 
acc    t p   t n   m 

p rec   t p  t p   f p   

recall   t p  t p   f n   

where t p stands for true positive  t n true negative  f p false positive  f n false negative  and m number
of data points 
we first look at the confusion matrix to get an idea of how each model is performing 

figure   

we see from figure   that both naive bayes and logistic regression do a good job predicting negative
data points  but do not predict very accurately the positive data points  the svm algorithm predicts more
positive data points than naive bayes and logistic regression  but also has a lot more fale positives  lowering
its precision  these observations are made more precise in figure    which compares each of the models main
metrics to each other 

fi 

figure   
from figure    we see that logistic regression has the best accuracy and precision at     and     respectively  with naive bayes trailing with     and     for accuracy and precision  although svm does
much better than both naive bayes and logistic regression in recall  we still consider svm to be the worst
performing model for our purposes  this is because our trading strategy will only enter into a position in
the market if we predict a positive result from the data  thus  the most important metric for us is precision 
since we can see it as the probability that the position we enter into will be profitable or not  therefore 
since logistic regression has the highes accuracy and precision  we come to the conclusion that it is the best
model for our problem 
the svm algorithm performed much worse than the other two algorithms  this was caused by the noise
in our data  and small number of data points  most of the twitter accounts we used had many tweets that
had no impact on the markets at all  but the algorithm still included them in that analysis  this along with
the small number of data points led to the svm algorithm choosing a decision boundary that was not very
accurate  the reason that logistic regression outperformed the naive bayes algorithm is that the naive
bayes assumption of the conditional independence of the features definitely does not hold in our case  also 
due to the size of the dictionary  it is possible that both svm and naive bayes were overfit  to mitigate this
effect  we created our dictionary using a subset of the tweets  before performing our analysis  due to the
fact that pca was performed before doing the logistic regression  it depended on fewer features  lowering
the chance that we overfit the model 
   conclusion
using three supervised learning techniques  we have developed a methodology for predicting volatility movements  with an accuracy between         the logistic regression model outperformed both naive bayes
and svm due to less assumptions being made  and a lower chance that the model was overfit  we believe
that with more concentrated tweets  and eliminating tweets that are not macroeconomic headlines  or headlines unrelated to the s p    movements  the accuracy and precision of the models would greatly increase 
also  by modifying and shorteningthe dictionary to include exclusively marketrelated words and tokens  we
would lower our chance of overfitting  and see an increase in accuracy  with more time  we would look into
different ways to obtain our data to make it cleaner  such as pulling headlines from bloomberg  we would
also look into tweaks of our models  to address some of the problems they were having  such as using a
different kernel for svm  or adding regularization to account for outliers 

fi 

references
    t  ding  v  fang  and d  zuo  stock market prediction based on time series data and market sentiment       
    t  l  im  p  w  san  c  k  on  r  alfred  and p  anthony  impact of financial news headline and content to market
sentiment  international journal of machine learning and computing            p      
    p  meesad and r  i  rasel  predicting stock market price using support vector regression  in informatics  electronics  
vision  iciev        international conference on  ieee        pp     
    d  moldovan  m  pasca  s  harabagiu  and m  surdeanu  performance issues and error analysis in an open domain
question answering system  acm transactions on information systems  tois              pp         
    r  p  schumaker and h  chen  textual analysis of stock market prediction using breaking financial news  the azfin text
system  acm transactions on information systems  tois              p     
    s  sekine and c  nobata  definition  dictionaries and tagger for extended named entity hierarchy   in lrec       
pp           
    m  s  a  wolfram  modelling the stock market using twitter  school of informatics          p     

fi