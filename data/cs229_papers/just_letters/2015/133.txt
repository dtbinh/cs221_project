predicting musical eras of songs
yandi li  jiaqi xue  yueyao zhu

abstract
the goal of the project is to predict the year in which a certain piece of music was created  we
used a subset of million song dataset written for standardized tests to train different models including
naive bayes classifier  generalized linear model  random forest  and gradient boosting machine 
comparisons are made based on the results mean square error and feature importance  from various
training sizes and different models 

 

introduction

there are many ways to categorize music  such as genre  artists  target listeners and cultural context  in
addition  different musical eras can also be regarded as an important standard to summarize a music feature 
certain music has very obvious mark of their time  and people have special taste for certain types of music
as a result 
the goal of the project is to predict the year in which a certain piece of music was created  there are
songs that do not have the mark of the time they really belong to  what we try to do  is satisfying the need
of people  who have particular favor for a type of music from a certain musical era  for example  music in
late   s  we can give out the list of music that have the common characteristics belonging to that period
no matter it was created at that time or not 

 

dataset

the dataset we are planing to utilize in our project  to design  implement  tune and test our machine learning
model is the million song dataset  which is a dataset containing audio features and metadata for a million
popular music tracks  http   labrosa ee columbia edu millionsong    collected by columbia university and
the echo nest  a subset of the dataset exists which has been split into train and test sets that ensures no song
from the same artist exist in both the train and test set to avoid the producer effect  the mentioned subset
dataset contains    attributes  with a most important value year ranging from      to       features
that were planning to explore and utilize to predict the feeling of the year includes the timbre average
and timbre covariance values  in addition  from the original million song dataset  more metadata about an
entry  music  can be extracted such as the genre tags  the artist name  and also evaluations includes the
songs danceability  pitches  they features contain discrete data as well as continuous data  that can be used
to describe the feeling of the song  of a particular year 

 

fi 

methods

   

baseline model  naive bayes classifier

naive bayes was introduced under a different name into the text retrieval community in the early     s     it
remains a popular baseline method for text categorization with word frequencies as the features  considering
we are trying to solve a similar problem  categorizing the music into eras based on their musical features  it
is reasonable and feasible to choose naive bayes classifier as the baseline model for the project 
the assumptions on distributions of features are called the event model of the naive bayes classifier  for
discrete features like the ones encountered in document classification  including spam filtering   multinomial
and bernoulli distributions are most popular  considering the particular characteristics of musical tracks 
the team chose multinomial naive bayes model to train the dataset collected from million song dataset 

   

generalized linear model

generalized linear model  glm  is a flexible generalization of ordinary linear regression that allows for
response variables that have error distribution models other than a normal distribution  the glm generalizes
linear regression by allowing the linear model to be related to the response variable via a link function and
by allowing the magnitude of the variance of each measurement to be a function of its predicted value  in our
project  glm is a logical choice to try for several reasons  firstly  we have a large datasets  the scalability
of our model is an important factor and glm is apparently very cheap to compute  secondly  there are   
attributes in our data  choosing the right feature is a difficult task and glm could solve this by giving
corresponding parameter    thirdly  linear regression is reasonable by intuition as features of a song should
not have more sophisticated relationship with its era  such as square  log and so on 

   

random forest

random forests uses an ensemble of decision trees to make regression and classification models  the trees
are built from a training dataset and can be used to make predictions on a test dataset  random forests
grows many classification trees  to classify a new object from an input vector  put the input vector down
each of the trees in the forest  each tree gives a classification  the forest chooses the classification having
the most votes  over all the trees in the forest   because decision tree learning is invariant under scaling and
various other transformations of feature values  it is robust to inclusion of irrelevant features  and produces
inspectable models  the group decided to model with random forests learning considering that we have a
rather large bases with a handful of input variables and random forests is able to handle thousands of input
variables without variable deletion and gives estimates of what variables are important in the classification 

   

gradient boost model

gradient boosting is tried on our dataset as a way of experimenting advanced and more complicated model
to see if this can help achieving a better machine learning result  the gradient boosting technique works
as other boosting methods that combines sub optimized models to form a hopefully better model  as in the
scope of machine learning  the methods boost the performance by combining weak learns into one strong
learner  at each stage  the model generates an imperfect model  i e   a weak model that can be efficiently
achieved  then the gradient boost model updates the loss as the difference between the prediction results

 

fiand the learning goal  by learning with gradient descent algorithm and generating and combing these weak
models  the gradient boosting method may be able to generate a model with a good performance 

 

results

   

baseline model  naive bayes classifier

after tuning around the hyper parameters in the multinomial naive bayes model  the following configuration
was chosen for the naive bayes model used to get the baseline results  laplace smoothing was not applied
in the models training and predicting process as it turned out that enabling laplace smoothing didnt yield
much improvement of the model  the minimum standard deviation was capped to be at least       for
observations with not enough data  and the minimum probability for observations with not enough data was
capped to be at least       as well 
with the above configuration of the multinomial bayes model  a first run with      training entries and
    testing entries generated a mse        and logloss         on the training set and mse        and
logloss         on the testing set  another run on a larger training set        data entries  and the same
testing set generated mse      and logloss       on the training set and mse      and logloss       on
the testing set  therefore  we have already observed that an increased training set can lead to more accurate
result even with the naive bayes model 

   

generalized linear model

in the model we built  we included intercept term  considering both speed and performance  we set objective
epsilon to be          meaning the model converges if it changes less that this value  and the max iterations
to be     for testing of our model  we decided to use k folds validation and set k to be    
for the year we tried to predict  there are two data type options for us  enum or numeric  at first
thought  numeric data seems to be more logical  as time is a continuous variable  however  there are several
problems in using this data type 
   in our training data  each song only has its year of its releasing  so the data is not very accurate if we
consider it to be continuous variable 
   the evolving of music is not linear to time  in some year  there might be big changes in the entire
music work while other years seem to be very calm  therefore even though time is continuous  it does
not help to make our model more accurate 
   the prediction result may not make sense if we use numeric data type  for instance  if there is not
song in year       it doesnt make any sense when we predict any song to be released that time       
in conclusion  even the variable we try to predict is continuous  it still should be considered a classification
problem  by training our data  we now there are different types of music  and then we try to categorize
songs into these types in terms of the year they seem to be released in 

 

fiafter deciding the parameters  we used different amount of data to see the performance of our model 
we used data from       training samples to       training samples and get result as below 

figure    mse vs  sample size

 a    k samples

 b    k samples

 c    k samples

 d    k samples

 e    k samples

figure    features for different sizes of samples

   

random forest

for the purpose of the project  we choose the number of trees ntrees  to     the mtries is set to be the
square root of the number of predictors as recommended  row sample rate is set to       after some testing
and tuning 
below is the result get from       to       training sets 

 

fifigure    mse vs  sample size

 a    k samples

 b    k samples

 c    k samples

 d    k samples

 e    k samples

figure    features for different sizes of samples

   

gradient boost model

as for some major parameters for the gradient boost model used in this project  we choose the number of
trees to be    with the maximum depth to be    the learning rate is set to be      the r  stopping threshold
is set to be        which stops making trees when the r  metric exceeds         the relative tolerance for
metric based is set to be       with mse as the metric 

 

fifigure    mse vs  sample size

 a    k samples

 b    k samples

 c    k samples

 d    k samples

 e    k samples

figure    features for different sizes of samples

 
   

conclusion
mse range

as in the projects  we first observed the mse from each model to evaluate and compare between the results
of different models  without a huge dataset  all models  glm  rf and gbm  can generate a mse that we
believe is acceptable for the purpose of our application  as to grab the feeling of years of a music  the task
itself is a vague task as human beings can find it extremely hard to tell  and most likely not care which year
exactly a song is composed  on the contrary  a user of this application may care much more about whether
a group of songs that our model generates share the same feelings  i e   share the same features 

   

feature importance

the feature weights generated from each model tends to assign importance to a similar selection of features 
this means that each model in our case agrees on which feature leads to the decision of which year a song
is created and thus aligns with our expectation of the users enjoying the services from this application 

 

fi 

reference

    russell  stuart  norvig  peter                artificial intelligence  a modern approach   nd ed   
prentice hall isbn                

 

fi