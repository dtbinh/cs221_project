cs    
project final report
fast optimization of functions of inner products via
newton stein method
murat a  erdogdu
erdogdu stanford edu

department of statistics  stanford university
december         

 

introduction

in this paper  we consider the problem of minimizing a sum of n functions
n

minimize f      


 x
fi   
n

   

i  

through projected iterations onto the compact and bounded parameter set c  rp   where fi   r  r
are   times almost everywhere differentiable convex functions    c is the parameter  we assume
that the functions fi s can be represented as inner products of the parameter  and feature vectors
xi  rp   that is 
fi       hxi   i  

   

for some function  
this problem is commonly encountered in machine learning literature where  is chosen to be a
suitable loss function quantifying the misfit  such as  p  loss or hinge p loss  in this paper  we focus on
the regime where the number of observations samples n  is much larger than the dimension of the data
p  i e   n  p     in the target regime  stochastic algorithms are quite popular as their per iteration
cost is o p  which is independent of the number of observations n  such methods use a gradient  or
sub gradient  of a single  randomly selected observation to update the current iterate  bottou        
though they have negligible per iteration cost  the convergence rate of these methods might be
extremely slow  there are several extensions of the classical stochastic descent algorithms  providing
significant improvement and or stability  bottou        duchi et al         schmidt et al         
batch algorithms  on the other hand  enjoy faster convergence rates  though their per iteration
cost may be prohibitive  in particular  second order methods attain quadratic convergence rate  but
constructing the exact hessian matrix generally requires excessive amount of computation  many
algorithms aim at forming an approximate  cost efficient scaling matrix  in particular  this idea lies
at the core of quasi newton methods  bishop        
an alternative approach to construct an approximate hessian matrix makes use of sub sampling
techniques  martens        byrd et al         vinyals and povey        erdogdu and montanari        
many contemporary learning methods rely on sub sampling as it is simple and it provides significant
boost over the first order methods  further improvements through conjugate gradient methods and
krylov sub spaces are available 
 

fiin this paper  we focus on the iterations of the form  projection is omitted here for simplicity  
t    t  qt  f  t    

   

where qt is a suitable scaling matrix updated at each iteration   f    is the gradient of the function
f and  is the step size  our focus is to construct a suitable and cost efficient sequence of scaling
matrices  qt  t   that provides sufficient curvature information as in quasi newton methods 
in a recent paper  erdogdu         authors proposed a method called newton stein method
for training generalized linear models  glms   the method is based on a stein type lemma  which
provides a fast update per each iteration  the simple idea is to remove the dependence of the matrices
involved in the computation of qt to the iterates  this ways  such matrices can be computed only
once and used throughout the iterations without requiring update  even though the experiments are
convincing  the novelty is limited to glms such as linear logistic regression 
in this paper  we generalize newton stein method to functions of inner products which broadens
the range of applicability of the suggested method  the method will again rely on the same argument
based on steins lemma  and following  erdogdu and montanari         we will introduce further
improvements through sub sampling and eigenvalue thresholding  this way  the algorithm will be
applicable to a wide variety of problems such as support vector machines with hinge p loss  linear
programing with log barriers or even non convex problems such as neural networks etc 

 

proposed algorithm  generalized newton stein method

the task of constructing an approximate hessian can be viewed as an estimation problem  assume
for simplicity that fi s denote a loss function between the linear prediction and the observation  i e 
fi       yi   hxi   i   for simplicity  we will mostly drop the dependence on the observations and
simply write fi       hxi   i  
assuming that the features xi s are i i d  random vectors with bounded support  the hessian of
the aforementioned minimization problem has the following form
n
 t  
 x
q
 
xi xti       hxi   i   e xxt       hx  i    
n

   

i  

  
is just a sum of
where      denotes the second derivative of the function f   we observe that qt
i i d  matrices  hence  the true hessian is nothing but a sample mean estimator to its expectation 
another natural estimator would be the sub sampled hessian method suggested by  martens       
byrd et al         erdogdu and montanari         similarly  our goal is to propose an appropriate
estimator that is also computationally efficient 
following the idea in  erdogdu         we use the following stein type lemma to derive an efficient
estimator to the expectation of hessian 
lemma      stein type lemma  erdogdu          assume that x  np       and   rp is a
constant vector  then for any function    r  r that is twice weakly differentiable  we have
e xxt  hx  i     e  hx  i     e       hx  i  t   

 

   

fithe proof of lemma     is straightforward and can be found in  erdogdu         in eqs    and
   we realize that the true hessian is also an estimator to the quantity on provided by lemma     
therefore  we can instead come up with an estimator for the right hand side of eq     we notice
that the right hand side of eq     becomes a rank   update to the first term  hence  its inverse can
be computed with o p    cost  quantities that change at each iteration are the ones that depend on
  i e  
       e       hx  i   

and        e       hx  i   

     and 
p     are scalar quantities and can
p be estimated by their corresponding sample means
       n  ni         hxi   i  and        n  ni         hxi   i   with only o np  computation  we
emphasize that sub sampling can be also used at this stage  but since we consider batch updates 
cost of computing the gradient is o np   therefore  using sub sampling at this stage of the algorithm
will not provide huge impact in terms of order of magnitude 
to complete the estimation task suggested by eq       we need an estimator for the covariance
matrix   a natural estimator is the sample mean where  we only use a sub sample s   n  so
that the cost is reduced
to o  s p    from o np     sub sampling based sample mean estimator is
p
t
bs  
denoted by 
is xi xi   s   which is widely used in large scale problems  vershynin         we
highlight the fact that lemma     replaces newton methods  nm  o np    per iteration cost with
a one time cost of o np     we further use sub sampling to reduce this one time cost to o  s p    
in general  important curvature information is contained in the largest few spectral features 
following  erdogdu and montanari         we take the largest r eigenvalues of the sub sampled covariance estimator  setting rest of them to  r      th eigenvalue  this operation helps denoising and
would require o rp    computation  this eigenvalue thresholding operation is denoted by r below 
notice that the updates in eq      are based on rank   matrix additions  hence  we can simply
use a matrix inversion formula to derive an explicit equation  this formulation would impose another inverse operation on the covariance estimator  since the covariance estimator is also based on
rank r approximation  one can utilize the low rank inversion formula again  we emphasize that this
operation is performed once  therefore  instead of nms per iteration cost of o p    due to inversion  the proposed algorithm requires o p    per iteration and a one time cost of o rp     assuming
that our algorithm and nm converge in t  and
 t  iterations respectively  the overall complexity of
our algorithm iso npt    p  t      s    r p   o npt    p  t     s p  whereas that of nm is
o np  t    p  t   
even though proposition     assumes that the covariates are multivariate gaussian random
vectors  the only assumption we will make on the covariates is that they have bounded support 
which covers a wide class of random variables  we note that bounded support assumption can
be further relaxed to a more general case called sub gaussian random vectors  but this will not
be pursued here  the main reason that newton stein method works for general distributions is a
consequence of the fact that the proposed estimator in eq      relies on the distribution of x only
through inner products of the form hx  vi  which in turn results in approximate normal distribution
due to the central limit theorem when p is sufficiently large  we will discuss this phenomenon in
detail in the final report  combining the aforementioned arguments  we may write the following for
an hessian estimator 
b s        t  r  
b s  t  t  t r  
b s   
 qt         t  r  

 

ficonvergence rate

log   error 

 

subsample size
newst   s       
newst   s        

 

 

 
 

  

  

  

iterations

  

  

figure    composite convergence over two different sub sampling sizes 

as mentioned before  the second term on the right hand side is just a rank   update to the first term 
therefore  we can invoke a fast inversion formula and obtain the the scaling matrix
 
 
t   t  t
 

b s    
qt  
 
   
r  
b s  t   t i
   t  
   t      t     hr  

 

current theoretical results and future work

the main contribution of this work is the generalization of the bound given only for glms by
 erdogdu        to functions of inner products  the following theorem is our main result 
theorem      assume that the function  has bounded and lipschitz continuous second and fourth

derivatives  let the features x    x         xn be i i d  random vectors supported on a ball of radius k
with


e xi      
and
e xi xti    
where  follows the r spiked model 
if n   s  and p are sufficiently large  then there exist constants c  c    c    c  depending on the properties of the covariates and  such that with probability at least    c p    we have
 t  




 

       t         t      
   
where the coefficients   and   are deterministic constants defined as
r
p
 
    c  d x  z    c 
min   s   n  log n  
 

    c   

fiand d x  z  is a probability metric defined in lemma  b   
the above theorem provides a per step bound which can be further used to obtain bounds
on number of iterations  however  the main importance of the above result is that it explains
the dependence of convergence behavior to the data dimensions quite well  in particular  we first
notice that as the squared term would dominate  the convergence will start as quadratic at start 
therefore  the initial convergence will be determined by the convergence coefficient     as the
iterations proceed to the correct minimizer  the squared term will become small and therefore  the
linear term will dominate  therefore  the slope of the linear term will be determined by     this
behavior can be observed in figure   where we notice that sub sampling size also has a major effect
in the convergence rate  that is  larger sub sample size gives a longer quadratic rate of convergence 
however  this imposes a trade off between the faster convergence rate and the per iteration cost 
we notice that the coefficient of the quadratic term is just a constant  however  the coefficient
of the linear term is composed of three components  the first term in   quantifies how accurate the
gaussian approximation was  we expect this term to be small as the dependence of the parameter
and the feature vectors are linear  which will invoke a central limit theorem given that the  is
well spread 

 

future work towards a complete paper

in this work  we generalized the newton stein method to more general minimization problems which
was previously proposed for glms  we state the following probable future work to get a complete
paper 
 an upper bound on the number of iterations can be easily obtained using the properties of
the composite convergence  the upper bound will have two terms     one from quadratically
converging term     one from the linearly converging term 
 the distributional assumptions on the covariates  i e  bounded support  can be relaxed to
more general class of distributions such as sub gaussian  even though this requires significant
amount of manipulations  the proof idea is quite similar 
 we can demonstrate the value and applicability of the generalized newton stein method on
examples such as linear programing with logarithmic bariers  linear support vector machines
with hinge p loss etc  we also note that when applied to a glm problem  generalized newtonstein method reduces to the classical newton stein 
 extensive numerical studies are needed to compare the proposed algorithm to well known
optimization methods  we expect to see significant improvement over quasi newton methods
such as bfgs  l bfgs as they only rely on gradients and iterates to construct an approximate
scaling matrix 

 

fireferences
 bishop        bishop  c  m          neural networks for pattern recognition  oxford university
press  inc   ny  usa 
 bottou        bottou  l          large scale machine learning with stochastic gradient descent  in
compstat  pages        
 byrd et al         byrd  r  h   chin  g  m   neveitt  w   and nocedal  j          on the use of
stochastic hessian information in optimization methods for machine learning  siam journal on
optimization               
 duchi et al         duchi  j   hazan  e   and singer  y          adaptive subgradient methods for
online learning and stochastic optimization  j  mach  learn  res               
 erdogdu        erdogdu  m  a          newton stein method  a second order method for glms
via steins lemma  in nips 
 erdogdu and montanari        erdogdu  m  a  and montanari  a          convergence rates of
sub sampled newton methods  in nips 
 martens        martens  j          deep learning via hessian free optimization  in icml  pages
       
 schmidt et al         schmidt  m   roux  n  l   and bach  f          minimizing finite sums with
the stochastic average gradient  arxiv preprint arxiv           
 vershynin        vershynin  r          introduction to the non asymptotic analysis of random
matrices  arxiv           
 vinyals and povey        vinyals  o  and povey  d          krylov subspace descent for deep
learning  in aistats 

 

fia

proof of the theorem

we will provide the proofs of the theorem      matrix concentration results in this section are
mostly based on the covering net argument provided in  vershynin         similar results for matrix
forms can also be obtained through different techniques such as chaining as well  on the set e  we
write 
z  
t
t
t
t
t
  f      t     d t     
    q  f           q
 


z  
 
t
t
 f           d  t      
  i  q
 

since the projection pc can only decrease the    distance  we obtain


z  


t
t
 
t  
t

 f           d 
k
  k   i  q
 k   k   
 

   

 

the governing term  with       that determines the convergence rate  the first term on the
right hand side  can be bounded as




z  
z  
 t  



t
 
t
 
t

i  qt

 f           d 
 f           d    q   
 kq k   

 

 

 

 

we define the following 
h
i
h
i
e     e       hx  i     e       hx  i  t 
note that for a function f   e f  hx  i     h   is a function of   with a slight abuse of notation 
we write e f  hx  i     h   as a random variable  we have
z  




 t  

 t  
 
t
t 
 f           d    q    e   
 q   
 
 
 




   e xxt       hx  t i    e t  
 
z  


z  


 
t
t
   
t
            d  e xx
  hx          i d 
 


 
 
 



z  


t    
t
t
      hx      t    i d 
 
e xx   hx   i    e xx
  
 

 

we find an upper bound for the terms on the right hand side  in order to accomplish this  we
state a lemma for each term in section b 
using lemmas b    b    b   and b    the right hand side can be bounded above by
z  


 t  

  f      t     d 
 q   
 
 
r
r
p
p
 c 
  d x  z    c 
log  n    c  kt   k 
min   s   n  log n  
n
 

filastly  using lemma b    we obtain
kqt k    
combining the above results we get
z  



 t  
 
t

f
 
 
 



  d
 q
 






 
 
r
 r

p
p
t
 c 
  d x  z    c 
log  n    c  k   k   
min   s   n  log n  
n

   
    

finally  we combine the above inequalities and obtain our main result 


z  


 
t
t
t  
t

k
 f           d 
  k   i  q
 k   k 
 
 
r

 r
p
p
t
 c 
  d x  z    c 
log  n    c  k   k  kt   k   
min   s   n  log n  
n

b

main lemmas

lemma b     erdogdu          there exist constants c  c such that  with probability at least  c p   
r


 t  

 q    e t    c
 

p
 
min   s   n  log n  

where the constants depend on k  b and the radius r 
based on the second and fourth derivatives of cumulant generating function  we define the following function classes 
n
o
n
o
h    h x          hx  i      c  
h    h x          hx  i      c  
n
o
h    h x    hv  xi        hx  i      c  kvk       
lemma b    the bias term is can be upper bounded by the probability metric d x  z  for a gaussian
random vector z  i e  



t    
t
t 
e xx   hx   i    e     dh   x  z    kk  dh   x  z    kk   r  dh   x  z  
 

lemma b     erdogdu          there exist constants c    c    c  depending on k  b  l and r such
that  with probability at least    c  ec  p
 n


z  
z  
  x



xi xti
      hxi       t    i d  e xxt
      hx      t    i d 


n
 
 
i  
 
r
p
 c 
log  n  
n
 

filemma b    there exists a constant c depending on k and l such that 



z  


   
t
t
t
   
t
t
e xx   hx   i    e xx
  hx          i d 
  ck   k   

 

 

proof  first  we apply the fubinis theorem  and obtain



z  


   
t
e xxt       hx  t i    e xxt
  hx          i d 
  

 
 
z   h
n
oi 


e xxt       hx  t i         hx      t    i  d 
 
  

 

 

we take the integration out  and obtain
z   h
n
oi


e xxt       hx  t i         hx      t    i   d 
 
 
z   h
i



e xxt l hx        t    i   d 
 
 
h
i z  
 e kxk   kt   k  l
     d 
 

 

lk    
 

kt   k   

lemma b     erdogdu          if n   s  are sufficiently large relative to p  we have

 
 

  
t   t  t
 t



 
b
q    

 

 

   
r
s
 
t
t
t
t
b
    t  
              hr  s      i  
for some constant  depending on the properties of the functions fi  

c

hoeffding type inequalities

lemma
c    let xi  rp   for i              n  be i i d  random vectors supported on a ball of radius

k  with mean    and covariance matrix   also let f   r  r be a uniformly bounded function
such that for some b      we have kf k   b and f is lipschitz continuous with constant l  then 
there exist constants c    c    c  such that
fi
fi
 
r
n
fi  x
fi
p log n 
fi
fi
p
sup fi
f  hxi   i   e f  hx  i  fi   c 
 c  ec  p  
fi
n
bn  r  fi n
i  

where the constants depend only on the bound b and radii r and



k 

we skip the proof of above lemma as it is similar to the that of below lemma 

 

filemma
c    let xi  rp   for i           n  be i i d  random vectors supported on a ball of radius

k  with mean    covariance matrix   also let f   r  r be a uniformly bounded function such
that for some b      we have kf k   b and f is lipschitz continuous with constant l  then  for
v  s p    there exist constants c    c    c  such that
fi
fi
 
r
n
fi  x
fi
p log  n 
fi
 
  fi
p
sup fi
 c  ec  p  
f  hxi   i hxi   vi  e f  hx  i hx  vi  fi   c 
fi
fi
n
n
bp  r 
i  

where the constants depend only on the bound b and radii r and



k 

proof of lemma c    as in the proof of lemma c    we start by using the lipschitz property of the
function f   i e        bp  r  
kf  hx  i hx  vi   f  hx    i hx  vi  k  lkxk   k    k   
lk     k    k   
for anet t     bp  r      t such that right hand side of the above inequality is smaller
than l k  then  we can write
fi n
fi fi n
fi
fi  x
fi fi  x
fi
fi
fi
 
  fi
 
 
 
  fi
f  hxi   i hxi   vi  e f  hx  i hx  vi  fi  fi
f  hxi    i hxi   vi  e f  hx   i hx  vi  fi
fi
fin
fi fin
fi
i  

i  

   lk      

    

this time  we choose


 
 lk    
and take the supremum over the corresponding feasible  sets on both sides 
fi
fi
n
fi  x
fi
fi
fi
sup fi
f  hxi   i hxi   vi   e f  hx  i hx  vi   fi
fi
bn  r  fi n i  
fi
fi
n
fi 
fi  x
fi
fi
f  hxi   i hxi   vi   e f  hx  i hx  vi   fi    
 max fi
fi  
t fi n
 

i  

now  since we have kf k  b and for fixed  and v  i              n  f  hxi   i hxi   vi  are i i d 
random variables  by the hoeffdings concentration inequality  we have
fi
fi
 


n
fi  x
fi
n 
fi
fi
 
 
f  hxi   i hxi   vi  e f  hx  i hx  vi  fi         exp       
p fi
fin
fi
 b k
i  

using eq       and the above result combined with the union bound  we easily obtain
fi
fi
 
n
fi  x
fi
fi
fi
p
sup fi
f  hxi   i hxi   vi   e f  hx  i hx  vi   fi   
fi
fi
n
bn  r 
fi i  
fi
 
n
fi  x
fi
fi
 
  fi
 p max fi
f  hxi   i hxi   vi  e f  hx  i hx  vi  fi     
fi
t fi n
i  


n 
   t   exp       
 b k
  

fiwhere      lk       by standard  net arguments over a ball of radius r in p dimensions we have
  p 
p

r p
r p
 t   
 
 

  lk    
as before  we require that the right hand side of above inequality gets a decay with rate o p  
by straightforward algebraic manipulations  we obtain that  should be
s
 
r


b k  p
p log n 
  l  r  k   n
 
 o
 
log
n
b 
n
which completes the proof 

  

fi