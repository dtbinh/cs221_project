user review sentiment classification and aggregation
steven garcia
garcias stanford edu
abstract
user reviews provide a wealth of information but are often
overwhelming in volume  in this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself  we then investigate clustering techniques to identify key positive and negative sentiment aspects to provide a user friendly summary for users 

  

introduction

user reviews on product websites such as yelp or amazon provide a wealth of information about the subject of the
review  in many cases when selecting a service  i e  restaurant  or when choosing a product  i e  book  dishwasher
etc   the amount of information available in user reviews
can be of greater volume and more trustworthy than the
official product descriptions provided by the vendor 
the number of reviews for a single item can at times be
overwhelming  for instance  the amazon kindle keyboard
  e reader has over        user reviews as of this writing     
the review reader can gain an understanding of overall satisfaction by looking at summary statistics such as the average
rating or score  but the details about what make each product great or not are hidden inside the body of the reviews
themselves  with such a large amount of data to process  it
would be beneficial to automate the process and generate a
summary of the reviews 
in this project we propose an approach to review summarization that leverages trained models for classification and
clustering to derive sentiment for the key aspects of a reviewed item  given a set of item reviews  our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about 
this paper is structured as follows  in section   we discuss
related works in the ares of sentiment detection and analysis 
in section   we provide an overview of the amazon and yelp
datasets used in our experiments  in section   and section  
we outline our novel approach to review summarization and
present results  finally  in section   we conclude our findings and outline future directions of exploration 

  

background

liu and zhang     outline five tasks that are core elements
of sentiment analysis  entity extraction and grouping  aspect
extraction and grouping  opinion holder and time extraction  aspect sentiment classification  and opinion quintuple
generation  in this work we will focus on the tasks of aspect
extraction and grouping  and aspect sentiment classification 
pang et al       successfully use bag of words features to
predict positive and negative labels for movie reviews  they
find little difference between naive bayes and svm classifiers  based on this work our experiments begin with bagof words features and naive bayes classifiers 

ping yin
pingyin stanford edu
das et al      classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a significant impact on accuracy  similarly  we experiment with several models for our classifier
and choose our best model using precision and recall metrics 
cataldi et al      propose a system that leverages nlp to
identify feature level sentiment  there is significant cost in
parsing sentence structure and typically such works is limited to small data sets  although we utilize large data sets
in this work  we explore the the impact of pos tagging in
a later component of our system where the cost to generate
this data is reduced 

  

dataset

in this work we experiment with two datasets  the yelp
challenge dataset containing    m reviews spanning   k
businesses and    k users       we also utilize the amazon product data set which provides nearly two decades of
reviews for a variety of product segments      from the amazon dataset we experiment with the video games  tool and
home improvement  and musical instruments subsets  for
all datasets  each record contains a review text and an overall review score  this work is limited to consuming those
two fields but could be extended to include the data provided other fields 
for each review we breakdown the reviews into sentence
snippets and then into tokens to use as features  additionally some experiments utilize stemming and part of speech
tagging  all text manipulation  stemming and tagging was
performed using the natural language toolkit      figure  
shows the distribution of sentences per review and the breakdown of review scores per dataset 

  

methods

our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects  to
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review 
a key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set  given the large number of reviews in our data sets
we instead label each sentence with the overall review score 
the overall review score therefore acts as a surrogate score
for each sentence in that review  although noisy  we can
justify this decision for two reasons  first  due to the overwhelmingly large amount of data we expect that the learning algorithm will successfully learn to classify sentences to
a reasonable degree  as shown in figure    all but one of our
data sets has at least         reviews at every rating  second  our extraction of representative aspect sentences does
not require every sentence to be correctly classified  indeed 
even with a large number of unclassified sentences  we can
still produce a meaningful summary for the user  in section   we measure the precision of this approach 

fifigure    dataset review and length breakdown
having established a classification of positive and negative
sentences within a review  we can then apply a clustering
algorithm to find groupings which are representative of key
elements 
an overview of our system is depicted in figure    the
sentence parser splits reviews into sentence fragments  the
sentence classifer then assigns a positive  negative  or unsure
label to each sentence  the clustering component groups
together common sentences of each labeled class  finally 
aspect selection filters the clusters and outputs a representative sentence for each selected grouping 

   

classification

to classify the sentences as positive or negative we consider three candidate algorithms  naive bayes classifiers are
well explored in the literature for use on text classification
problems  they are a natural fit for text classification as
they are a generative class of models that estimate the likelihood of observing the data  specifically  the classifier uses
bayes rule to determine the likelihood of a class given the
 y 
  and using an independence
data with p  y x    p  x y p
p  x 
assumption this reduces to

q
  n
i   p  xi  y j  p  y j 
pk q
  n
i   p  xi  y k  p  y k 
k

for a

set of features n  aiming to predict the likelihood of observing class j in the set of classes k 
mccallum et al      compare baysian models and find that
the multinomial naive bayes classifier is almost uniformly
better than the bernoulli classifier  however in this work we
deal with very relatively short documents  individual sentences   the multinomial model specifically accounts for
feature frequency in determining the likelihood of a document  this makes sense for longer documents  but for short
sentences  like the ones we aim to classify  it could be argued that the frequency of the features is less important
than the actual occurrence of those features  as such we
consider both the bernoulli naive bayes classifier and the

figure    overall system architecture

multinomial naive bayes classifier 
one limitation of the naive bayes classifiers is that they
make an independence assumption  a tree classifier can implicitly capture dependency between features  to explore
this we also experiment with a gradient boosted tree classifier      this classifier is built as the aggregation of a collection of trees  to classify an instance  the set of trees are
evaluated and the average score of all trees is taken as the
classification result 
to train the model  on each training iteration a new tree
is constructed to maximize correct classification of the training set  to build each tree  nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes  the process is repeated for the output
of each node up to a maximum depth which is specified as
a parameter  one optimization to reduce training time is
to randomly sample features for each node partition  as opposed to trying every possible feature   a related optimization is to sample from the training data for each training
iteration  bag fraction   both of these optimization are controlled through parameters  to avoid over fitting you can
specify a minimum number of samples in a leaf node  other
parameters used by the model include the learning rate  and
the maximum number of trees to train 
an interesting property of this algorithm is that as each
tree is added  the learning algorithm will evaluate the accuracy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously classified incorrectly  in this way  each new tree is more targeted
towards those subsets of the training data that are hard to
classify 
as noted in section    our labels are approximated for
each input sentence using the overall review score  to reduce
ambiguity for our model we train on only very negative  one
star  and very positive  five star  reviews  in section   we
show that this results in a classifier that is suitable for the
task 
our features are tokens  or terms  extracted from the review sentences  for each token we count the number of
occurrences of that token in the training sample  we choose
to stem our tokens to allow the model to better leverage the
features  stemming is applied using the nltk library  to
reduce the feature space we stop terms that occur in less
than    of our training data with the expectation that such
rare terms will not impact the final clustering result 
the final step for deploying our classifier is to ensure high
accuracy  as not every sentence can be classified successfully as positive or negative  we apply two thresholds to our

ficlassifier  a high threshold is used to limit which instances
are classified as positive  and a low threshold is used to limit
the negative classifications  anything in between is considered unsure  using this approach we can control for any
target level of precision on the classification  we select a
target of     precision in this work  although this can be
tuned as desired with a trade off on recall 

   

clustering and aspect selection

with a trained classifier  we now consider the task of clustering and aspect selection  we use a k means     classifier
to create clusters of similar sentences among the positive and
negatively classified results  k means works by establishing
k centroids in an n dimensional space  and then for every
data point assigning that point to the nearest k centroid 
once complete the centroids are updated to be the actual
center of all items assigned to its cluster  the algorithm
then iterates the above steps until the centroids have converged 
is typically the euclidian distance as given
p
p distance
by ki   xsi   x  i       where s is the set of data points
to cluster 
one challenge is that the number of clusters within a positive or negative sentence set is unknown before applying the
clustering algorithm  to address this issue we propose a criteria for valid clusters and iterate the clustering algorithm
over a range of k values  once complete we select the value
of k that resulted in the largest number of valid clusters  we
define a valid cluster to be one where the most frequent  
features terms can be found in more than     of the cluster  and where the cluster consists of    or more different
review sentences  the first criteria is a heuristic which ensures that the cluster contains a subset of terms that will
be identifiable for the aspect  the second criteria ensures
that our cluster is representative of multiple reviews  the
parameters are configurable and explored in section   
finally  having clustered a set of classified sentences  we
select a representative example from each valid cluster  by
selecting the sentence that is closest to the centroid using
euclidiean distance 

  

experiments

for all experiments below we sample data as specified in
the experiment description  the sampled data is then split
into training and test sets at a ratio of     train and    
test  results presented are for test data unless otherwise
specified 
our first experiment compares naive bayes classifiers with
the gradient boosted tree classifier for the yelp data set 
not every sentence will be strictly positive or negative  indeed some may be both   as such we aim to classify only
the strongly positive and negative sentences for our final results  to achieve high precision we apply two thresholds 
one on the upper end for positive sentiment  and one on the
lower end for negative sentiment  figure   shows the recall precision curves on the test data as we vary thresholds for
both positive and negative classifiers  models were trained
using         positive and negative training samples  the
results show that the gradient boosted tree provides superior accuracy on almost all points of the curve for classifying
sentences back to the original label  based on this result we
select the gradient boosted tree classifier for further experiments 
as discussed in section      the gradient boosted tree
model requires various parameters  to get the best performance out of the model we run a sweep across the parame 

figure    recall precision curves of positive and
negative classifiers for candidate algorithms
ter space selecting     possible parameter combinations and
choose the best performing model  figure   shows the how
tuning the parameters can improve the performance of the
model by showing the reduction in l  error  each time we
trained our model  we ran the parameter sweep  we found
our best model  including the improvements discussed in the
remainder of section    using       trees  a learning rate of
      a bag fraction parameter of       feature sampling of
          minimum labels per leaf  and a maximum tree size
of      finally we select our thresholds to obtain     precision for both the positive and negative classifiers 
one variable that can impact the performance of a model
is the number of training samples to use  to explore this
we trained our model using a range of training set sizes 
figure   shows the impact on recall for the positive and
negative classifiers when targeting     precision  note that
after splitting the yelp data set reviews into sentences we

figure    l  error on test set for gradient boosted
tree over model parameter space for yelp data set

fifigure    effect of training set size on recall at    
precision

clear trend of predicting a larger fraction of positive cases for
reviews that are labeled positive    and   stars  when compared to classifying negative cases for the negatively labeled
reviews    and   stars  
a critical assumption in this work is that the overall review label can act as a surrogate for the sentence label  to
verify that this assumption we randomly sampled     classified sentences from   entities in the yelp data and manually assigned labels of positive negative neither to those
sentences  table   shows that the positive predictor achieves
our expected precision of      classifying only    neither
labeled examples as positive  the negative classifier is less
accurate than expected with only     true negatives in the
negative prediction set and     positives classified as negative  we discuss potential improvements for the negative
classifier in section   
with classified sets of positive and negative sentences for
each review subject we now apply clustering and select representative sentences from clusters  we run k means clustering with a range of values for k from    to     and observe
three general patterns in the results  first  a cluster may
be too narrow with a handful of sentences  second a cluster
may be too broad  this often seemed to be the case for a
table    accuracy of predictions
judgment
positive
neither
negative

figure    distribution of predictions across labels
for all data sets after training model  yelp  y  
amazon  tools and home  at   amazon  video
games  av   and amazon  musical instruments
 am 
have on the order of    million sentences overall and just
over   million   star sentences  the figure shows that once
we are unable to select a balanced amount of positive and
negative training samples  our classifier begins to bias towards a positive prediction  it is worth noting that before
we reach the point of imbalance we see that recall continues to increase as we add more training data  this suggests
that obtaining more data could help  however the rate of
improvement is diminishing so it is unclear how much more
recall we could obtain for the cost  we chose to train our
final model using   million positive and negative samples 
our model is trained using sentence words as features  a
problem with this approach is that terms like rose and roses
are considered completely independent features although they
represent the same aspect  by applying stemming to our
feature set we can combine these features and allow the
model to better leverage the signal provided by these features  at our selected precision level of      recall for the
yelp positive classifier increases from     to      and for
the negative classifier it increases from     to      before
stemming our model uses       features  and after stemming
it uses     features 
having trained and tested our model on the one and five
star data  we now assess the performance of the model for
all candidate sentences  figure   shows the distribution of
predicted labels when compared to the overall review label
for our data sets  as expected a large fraction of the sentences are classified as unsure  for all data sets there is a

positive
   
  
 

prediction
negative
  
  
   

table    cluster filtering accuracy
k
  
  
  
  
  
  
  
  
  
   

narrow
 
  
  
  
  
  
  
  
  
  

broad
 
 
 
 
 
 
 
 
 
 

useful
 
 
 
 
 
 
 
 
 
 

table    cluster refinement with noun tokens
all features
topic
attentive staff
crab cake
chocolate strawberries
filet mignon
food dressings
reservation
romantic setting
rose flowers
salad cart
seafood
sommeli
table side food
fantastic
four queen  location 
incredible
red
share
treat

useful
y
y
y
y
y
y
y
y
y
y
y
y
n
n
n
n
n
n

noun features
topic
useful
beef wellington
y
broad menu
y
chocolate stawber  y
ries
classy
y
sommelier
y
sorbet
y
steak
y
table side salad
y
bottle
n
course
n
experience
n
house
n
old
n
restaurant
n
thing
n

fitable    examples from final results
data set
yelp

item business

  

aspect representation

hugos cellar

 
 

wonderful caesar salad prepared table side to our specifications 
they also have an extensive wine list  and a knowledgeable sommelier 
tucked underneath the four queens hotel in downtown las
vegas  this place   


 
amazon  tools and home

g  power minden
led recessed bulb

 

 

amazon  video games

god of war  ascension

 

 

amazon  instruments

brokeback mountain

 


single cluster which acted as a catch all for the sentences
that the algorithm was unable to classify more specifically 
finally there were clusters for groups that tended to represent a small group of features with a moderate number of
sentences  in an effort to select sentences that are of value
to the user we propose a filtering criteria for selecting clusters which removes a cluster that consists of less than   
review sentences  this restriction allows us to select only
aspects that are representative of a wide number of reviewers  secondly we require that a large portion of the cluster
 we select      be represented by at most   features terms 
that is  a subset of   features must be present in     of the
sentences belonging to the cluster  this allows us to ensure
that a cluster has a small focus aspect as represented by the
feature set 
to examine our filtering criteria we selected a sample review set  applied our clustering algorithm and filtering criteria  we then labeled the filtered clusters for each value of
k  table   shows for each value of k the number for removed
clusters in each identified class  the results show that no
clusters are removed that would be useful for the user 
with confidence that our cluster filtering criteria can remove clusters of little value to the user we now examine
the clusters that successfully pass through the filter  after
clustering and filtering we observed some clusters are still
generic in nature  for example a cluster formed primarily
around the feature incredible can contain many aspects  to
address this issue we applied pos tagging and attempted to
cluster only features that were also classified as nouns  table   shows how restricting clusters to noun features changes
the types of clusters formed  for each cluster we provide a
manual label summary to describe it  and a label indicating
if it would provide useful information to the user  however
there are still clusters that are centered on concepts of little
value to the user such as house which can relate to house
wine or on the house and so on  further  applying this restriction led to a loss of aspects that were useful such as
attentive staff and reservations  we proceed without pos
tagging and note that we need to further refine the clustering
selection in future work 

got these to replace cfl bulbs in my great room and kitchen and
all i can say is hot damn i love these things 
no more dimmer problems and saves me quite a bit of money on
my electric bill 
im going to just eat the rest of them as the burn out  and replace
them with a known brand 
i particularly found the blades of chaos system to be the best it
has ever been in the series 
the puzzles in god of war are great  sometimes exceedingly challenging  especially in the second game    
ascension just feels like a basic bare bones god of war without all
the memorable moments   
both michelle williams and anne hathaway give commendable
performances as    
simply put  this is the best movie ive seen in a while 
i thought this was supposed to be a love story  

finally to select results for the user we choose the sentence that closest to the center of the cluster  we present
a sample of the final results for our system over the various
data sets in table    we can see that the negative clusters
lack precision in some cases due to the lack of accuracy at
the classification phase  as observed in the yelp and video
game examples   similarly there are cases where the positive cluster has an aspect which is of questionable value  for
example the simply cluster for the instrument data   however  overall the system is able to identify clusters pivoted
on aspects that users should find of interest 

  

conclusions and future work

we have developed a system to process a large number
of reviews and report sentences from the reviews that are
representative of the best and worst aspects  we found that
with only review level labels  we could adequately train a
classifier to predict positive sentences  for classification we
recommend a gradient boosted tree over naive bayes classifiers  we developed a heuristic to filter the number of
clusters to present to the user for each classified review set 
finally we show that that our system correctly identifies key
aspects with end to end examples across multiple data sets 
our approach of utilizing the review score for sentence
level labels worked well for positive sentences  but was less
effective for negative sentences  labeled data sets at the
sentence level may provide richer data with which to train
our model  use of pos tagging was unable to improve the
performance of our clustering algorithm  however the literature suggests this is a candidate avenue of exploration
for the classifier  our clustering at times formed clusters
around common concepts  advanced nlp techniques may
be of value to help improve the clustering component of our
system  finally  our experiments made some decisions with
respect to parameterization  feature space and training data
that although pragmatic are open to scrutiny  we intend to
explore these decisions along with an exploration of other
mechanisms for selecting the optimal number of clusters as
described in the literature 

fi  

references

    amazon  kindle keyboard  wi fi    e ink display 
http   www amazon com 
kindle wireless reader wifi graphite 
product reviews b   y  p m   accessed november
         
    s  bird  nltk  the natural language toolkit  in
proceedings of the coling acl on interactive
presentation sessions  pages       association for
computational linguistics       
    c  j  burges  from ranknet to lambdarank to
lambdamart  an overview  technical report
msr tr          june      
    m  cataldi  a  ballatore  i  tiddi  and m  a  aufaure 
good location  terrible food  detecting feature
sentiment in user generated reviews  social network
analysis and mining                      
    s  r  das and m  y  chen  yahoo  for amazon 
sentiment extraction from small talk on the web 
management science                       
    j  a  hartigan and m  a  wong  algorithm as      a
k means clustering algorithm  applied statistics  pages
             
    b  liu and l  zhang  a survey of opinion mining and
sentiment analysis  in c  c  aggarwal and c  zhai 
editors  mining text data  pages         springer
us       
    j  j  mcauley  c  targett  q  shi  and a  van den
hengel  image based recommendations on styles and
substitutes  in r  a  baeza yates  m  lalmas 
a  moffat  and b  a  ribeiro neto  editors  sigir 
pages       acm       
    a  mccallum  k  nigam  et al  a comparison of event
models for naive bayes text classification  in aaai   
workshop on learning for text categorization  volume
     pages            
     b  pang  l  lee  and s  vaithyanathan  thumbs up  
sentiment classification using machine learning
techniques  in proceedings of the acl    conference
on empirical methods in natural language
processing volume     pages       association for
computational linguistics       
     yelp  yelp challenge dataset 
https   www yelp com dataset challenge dataset 
accessed october          

fi