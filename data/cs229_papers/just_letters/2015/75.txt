biomarker identification for early stage
diabetes diagnosis in mice liver cells
andrea agazzi      vincent deo   
    theoretical physics department  universit de genve  switzerland
    electrical engineering department  stanford university  ca  u s a 

agazzian  vdeo stanford edu

december   th      

a bstract
this projects investigates automated diagnosis possibilities from metabolite micro array measurements in liver
cells  at an incipient stage of the disease  l    regularized logistic regression  fisher discriminant analysis and random forests classifiers are studied  yielding promising results and hindsight on biomarker significance 

   i ntroduction
diabetes mellitus is one of todays most common chronic diseases  affecting approximately    m of people worldwide in           the disease  resulting in serious cardiovascular complications  is divided into two categories 
type   diabetes  accounting for approximately     of the cases  is a gradually developing form of the disease  and
can be prevented if diagnosed at an early stage  motivated by the inherently metabolic character of the disease  the
maechler lab at the university hospital of geneva is investigating a novel form of early stage diagnosis  searching
for quantitative biological markers in a patients liver cells  the liver is responsible for carbohydrate metabolism
and is therefore expected to be affected first by incipient diabetes 
this project aims to analyze the data conveyed from the maechler lab and tackle the issues of early stage diagnosis and biomarker identification  specifically  the algorithm input is the result of micro array experiments
of metabolite concentrations in both sick and healthy mice liver cells  we apply different combinations of preprocessing and supervised classification algorithms  for the purpose of automated diagnosis of new data points 
furthermore  we aim to extract the most significant features indicating the presence of the disease through inspection of the best performing classifiers 

   r elated w ork
micro array and metabolite profiling experiments have been an object of interest in the machine learning literature ever since they appeared in cell biology  the data is high dimensional  and therefore heavily regularized
approaches often become the methods of choice for these problems      for feature extraction and preprocessing of micro array data  the most recurring algorithms in the literature are pca      information gain  correlation
analysis  and false discovery rate thresholding     
for the classification  widely applied methods include support vector machines  svm  with different kernels 
fisher discriminant analysis  fda   logistic regression  logreg   k nearest neighbors classifiers  decision trees
and random forests      in most cases  these can be complemented with a regularization procedure      encouraging model sparsity and providing helpful preprocessing to high dimensional and correlated data 
given the wide range of high dimensional problems in the computational biology and high throughput data
analysis framework  there is no unique state of the art pipeline for feature selection and data classification clearly
outperforming others      therefore  the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis 

 

fi   d ataset description
the database this project uses was jointly created by the maechler lab at unige and the zamboni lab at ethz 
and was transmitted to the authors under a confidentiality agreement  a colony of    mice was divided into two
subgroups of    and    elements  the first group mice were genetically modified  knockout  ko  to induce diabetes  and the second group mice were kept as control  ctrl   out of each group  samples of   to   mice were
taken  without replacement  at weeks         and    of life  table     for each selected mice    samples of liver cells
are processed through mass spectrometry     and the concentrations of metabolites are estimated  not uniquely
but up to mass equivalence   for a total of     features per sample 
type

week  

week  

week  

ctrl
ko

  
  

  
  

  
  

week   
 post symptoms 
  
  

total
  
  

table    dataset distribution  week of sampling and sick  ko  or healthy  ctrl  labels 
our training set is the array of spectrometric measurements  x  i     i      r      with labels giving the week
of sampling w  i                  and the knockout state of the mice y  i           for each of the p       sampled liver
samples  we note x the design matrix and y the binary label vector 
since the number of features is larger than the number of samples  data preprocessing is an important step to
avoid critical overfits  dimensionality reduction algorithms were applied prior to data classification 
pca we compute the most significant n eigenvectors of the empirical covariance matrix c   x t x   where n is
selected by cross validation  figure    left   and project the data in the span of these principal component vectors
e             e n   x pc   x   e         e n   
c orrelation t hresholding we estimate the pearson correlation coefficient  y   x   j    

d   x   j  
cov y

  the n
y  x   j  
features with highest absolute value of the correlation coefficient are selected  with n chosen by cross validation 
this method has been preferred over mutual information thresholding  for the predictors being real valued 
l   regularized logistic regression this classifier is applied  and the induced model parameter sparsity     is
used as feature selector for a further classification by another algorithm 

   m ethods
m ethodology the dataset was used to train different supervised classification algorithms  whose quality was
quantified through their test scores in either leave    out or      bootstrap cross validations  the best performing
classification algorithms were used thereafter for quantitative and qualitative biomarker significance assessment 
f isher d iscriminant a nalysis a fisher linear discriminant  fda      was implemented hands on as a starting
point to assess the potential of the dataset  and then compared to the built in matlab lda and svm with default
configurations  the data is preprocessed using pca  section     after which the first fisher vector  normal the
maximally separating hyperplane  is computed  the offset of the decision boundary between the two classes may
be selected a posteriori  in order to adjust the false positive false negative ratio to meet requirements 
s upport v ector m achines classifiers were attempted with a variety of kernels over pca preprocessed data 
the linear kernel yields preliminary results of the order of the fda  although less stable over extensive test runs 
polynomial  gaussian and sigmoid kernels did not seem adapted to the dataset  leading to less satisfactory results 

 

fir egularized l ogistic r egression logistic regression  the generalized linear model with conditional probability p y x  bernoulli distributed  is a natural choice for a   class linear classifier  the high dimensionality of the
data at hand suggests the application of regularization procedures  this is equivalent to setting a bayesian prior
p     on the parameters of the model  and results in a modified log likelihood function 
l       

x

log p y  i    x  i          log p    

i

where the regularization parameter  is chosen by cross validation  most popular choices of prior distribution
are exp   and n       i    resulting respectively in so called l   and l   regularizations  l   regularization is attractive in this framework as it induces sparsity in the model parameters      a weighted choice of l   and l     called
elastic net      has also been implemented  this learning algorithm was implemented using the python library
scikitlearn      
r andom f orests random forest  rf  classifier      is an ensemble learning approach based on decision trees 
the latter sequentially divide the training data into subsets on single decision rules that maximize class separation 
as for tree bagging  rf classifiers bootstrap the training set and construct a decision tree for every bootstrapped
sample  reducing the high variance of the decision tree classifier approach  rf differentiates form tree bagging by
p
performing each split over a randomly chosen subset of features  of typical size  features  this randomized bootstrapping prevents correlated trees in the forest and improves accuracy  interestingly  rf are invariant to rescaling
of data  but tend to extract only a small subset of discriminative variables when features are highly correlated  this
learning algorithm was implemented using the python library scikitlearn      

   r esults and discussion
parameter selection for feature extraction and regularized classifications  some hyperparameters must be
picked to optimize the expected score of the algorithm  this is done by maximizing the cv score over the full
range of possible parameter combinations on the training set with a repeated leave    out procedure  which has
been chosen because of the relatively small dataset size  this includes the regularization parameter for l    logreg
and the dimension of the pca subspace for fda  the resulting validation curves are displayed in figure   
g eneral r esults the scores for some of the preprocessor classifier algorithms that have been applied in this
analysis are reported in table    out of the considered methods  those with best performance were studied in
more detail  with a preference for methods allowing a direct ranking of discriminative features in order to facilitate
biomarker extraction 
method
lda
linear svm
quadratic svm
l    penalized logistic reg 
l    penalized logistic reg 
elnet penalized linear reg 
random forests

preproc 
pca
pca
pca
corrthresh
corrthresh
corrthresh
l    logreg

cv score
     
     
     
     
     
     
     

sparsity
  
  
  
     
     
     
      

table    leave    out cross validation scores for preprocessor classifier pairs tried on the dataset 

m odel evaluation the learning procedures listed in section   were evaluated with different cross validation
procedures        bootstrap     fold  leave one out and leave    out cross validations  detailed results are shown

 

ficross validation of dimensionality in pca preprocessing
   

 

   

score

   

   

   

   
testing score
training score
fontsize

   
  

  

  

  

  

  

  

  

  

   

   

   

principal component dimension n

figure    left  cross validation of the number of pc dimensions in the fda algorithm  the training score
ceils at    whereas the testing score drops to     when dimensionality is higher than  number of points
     for separability reasons  red vertical line   right  cross validation of the regularization parameter
c in the l    logreg preprocesser randomforest classifier algorithm  colored areas correspond to the
standard deviation on the errors  the black solid line represents the sparsity of model parameters as
a function of the regularization parameter  both cross validations are realized in a repeated     times
leave    out fashion 

for pca preprocessing with fda classifier in table    and for the two other algorithms in table    cross validations
have been implemented manually in python and matlab 
total tests
    
predicted
ctrl
predicted
ko
accuracy
      

actual
ctrl

actual
ko

    

  

  

    

true positive
rate        
false negative
rate       

false positive
rate       
true negative
rate        

prevalence
      
positive predictive
false discovery
value        
rate       
false omission
negative predictive
rate       
value        
positive likelihood
diagnostic odds
ratio       
ratio       
negative likelihood
ratio       

table    detailed confusion matrix for the linear discriminant classifier

algorithm
l    reg logistic regression  
random forest
correlation thresholding  
l    reg logistic regression

true positive

false positive

true negative

false negative

accuracy

    

   

  

    

      

    

  

  

    

      

table    reduced confusion matrix for the l    logreg and random forest classifiers

 

fif eature extraction and biomarker identification the selected algorithms fda  l    logreg and random
forests provided us with the ability to extract the importance of each feature in the discrimination between the two
classes  we validate the extraction of these biomarkers by measuring the stability of their prevalence coefficients
across algorithms  figure   and the associated table show the stability of biomarker prevalence  the comparison
between l    logreg and fda is most natural because the weight of the features are quantified by the corresponding
component of the vector normal to the maximally separating hyperplane 
it is also a key point to verify our best found biomarkers against the medical knowledge as reflected by the
current scientific literature  preliminary research on the matter shows a consistent correlation between biomarker
rankings and the amount of available scientific literature about diabetes mentioning the biomarker 
prevalence coefficents of biomarkers
   

biomarker weights   l   logreg

    

   

compound   rank
ajugalactone
udp glucosamine
cholic acid
udp glucose
uridine

    

   

    

   

l    logreg
 
 
 
 
 

rf
  
 
  
  
  

lda
 
 
  
  
 

    

 
 

   

   

   

   

   

   

   

biomarker weights   fda

   

   

 
      

figure    left  prevalence coefficients of the    most significant biomarkers in the l    logreg algorithm
versus their fda prevalence  the dashed black line is provided as a guide to the eye  right  rankings by
prevalence of a selected subset of biomarkers across all three classifiers 

   c onclusion and f uture w ork
this work has addressed the problem of supervised classification and biomarker identification in metabolic profiling data of mice liver cells depending on the presence of type ii diabetes  the high dimensionality of data
requires testing different preprocessors and learning algorithms for supervised classification  the preprocessor classifier combinations that lead to the highest cross validation scores are pca fda  l    regularized logistic
regression random forest classifier and correlation thresholding l    regularized logistic regression  for each
of these three most successful methods  the features with the highest influence on the classifier have been ranked
and compare favorably 
possible directions for future work include the application of methods integrating prior biological knowledge
 such as the known topology of the metabolic graph  for example through overlapping sparse group lasso or preconditioned pca techniques 
the authors hope that this work will drive interest for further analytics in the biomedical scientific community 
in validation  continuance  and in building denser datasets that will help shed light on the statistical significance
of this work and future ones 

 

fir eferences
    y  shi and f  b  hu  the global implications of diabetes and cancer  the lancet  vol       no        pp       
           
    t  hastie  r  tibshirani  and j  friedman  the elements of statistical learning  springer series in statistics 
new york  ny  usa  springer new york inc        
    k  y  yeung and w  l  ruzzo  principal component analysis for clustering gene expression data  bioinformatics  vol      no     pp               
    j  quackenbush  computational analysis of microarray data  nature reviews genetics  vol     no     pp     
          
    t  li  c  zhang  and m  ogihara  a comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression  vol      no      pp                 
    t  fuhrer  d  heer  b  begemann  and n  zamboni  high throughput  accurate mass metabolome profiling
of cellular extracts by flow injectiontime of flight mass spectrometry  analytical chemistry  vol      no     
pp                  pmid           
    r  tibshirani  regression shrinkage and selection via the lasso  journal of the royal statistical society  series
b  methodological   pp               
    r  a  fisher  the use of multiple measurements in taxonomic problems  annals of eugenics  vol     no    
pp               
    h  zou and t  hastie  regularization and variable selection via the elastic net  journal of the royal statistical
society  series b  statistical methodology   vol      no     pp               
     f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion  o  grisel  m  blondel  p  prettenhofer  r  weiss 
v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikit learn 
machine learning in python  journal of machine learning research  vol      pp                 
     t  k  ho  the random subspace method for constructing decision forests  pattern analysis and machine
intelligence  ieee transactions on  vol      no     pp               

 

fi