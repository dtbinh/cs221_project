prediction algorithm for crime recidivism
julia andre  luis ceferino and thomas trinelle
machine learning project   cs      stanford university

abstract
this work presents several predictive models for crime recidivism using supervised machine learning techniques  our
initiative was focused on providing insights which would help
judges to make more informed decisions based on the analysis of individuals proneness to recidivism  different approaches were tried and their generalized error were computed and compared using cross validation methods  models are trained on two large data set collected from the
inter university consortium for political and social research
 icpsr  

problem formulation
this study provides element of answers to the following
questions 
    can we create an accurate predictive model to detect individual likely to commit recidivism  if yes  what
would be its accuracy 
    if a judge  was to have a very limited access to data 
what would be the most important features he would want
to collect on an individual to make a reliable judgement 

model development
introduction
today  the united states have one of the highest recidivism
rate in the world  with     billion people in jail  almost
    of the prisoner will be re arrested after their release 
this poses a serious problem of safety  and proves that we
dont make the decision that really make us safer  judges 
even though they have good intentions  make decision
subjectively  studies show that high risk individuals are
being released     of the time while low risk individual
are being released less often than they should be  milgram
         ideal would be to detain an offender for precisely
the right amount of time so that he is not re arrested after
his release  but in the mean time does not spend excessive
time in prison  with machine learning tools we can produce
accurate predictive models based on various factors such as
age  gender  ethnicity  employment  detecting patterns in
recidivism would provide supporting arguments for judges
to determine the appropriate sentence  which will decrease
safety risks while trying to avoid over punishment 
the purpose of this project to use data and analytics
to transform the way we do criminal justice  using supervised learning we can design a predictive model for
recidivism trained on historical data collected in the us 
this decision making tool will help the judges determine
whether a new offender is dangerous or not  by giving him
a recidivism score 

our analysis consisted of the following steps  
 a  data acquisition  crime and felonies are sensitive information which added latency for our team to
collect  the data required for machine learning applications
needed to comply with two main characteristics      to be
large enough such that the machine learning techniques can
converge to stable parameters      contain relevant features
to the problem we want to evaluate  bearing in mind these
requirements  our team searched online information from
different penitentiary institutions and research centers in the
us  additionally  our team contacted himabindu lakkaraju 
a phd student in the cs department who is working on
artificial intelligence applied to human behaviours related
to criminology for feedback  after extensive exploration 
our team found a relevant database in the inter university
consortium for political and social research  icpsr 
website  this data set was collected by smith and witte
        and has information of two cohorts of inmates that
were released in      and      from the prison of north
carolina  note that publicly available data sets are ancient 
due to prescriptions  which means they are often numerical
re transcription of manually stored data 
 b  data pre processing  the format of the data required pre processing to transcript them from there original
format  sas or spss  to more simple  csv file format 

fi c  feature extraction  the most important feature
collected for both cohorts was whether or not individuals
committed recidivism after release  in total  there were
   features per individual  race  alcoholism problems 
drug use  after relase supervision  i e  parole   marital
condition  gender  conviction reason  i e  crime or felony  
participation in work release programs  whether or not
the conviction was against property  whether or not the
conviction was against other individuals  prior convictions
if any  number of years of school  age  time of incarceration 
time between the release day and the record search  whether
they committed recidivism in the previous time span  the
time span from the release month to the recidivism date 
and a flag indicating if the individuals file was part of the
training set of the study in      
 d  preliminary analysis  we have run direct analyses using existent libraries in python  numpy  scipy and
scikit learn  and matlab  liblinear and libsvm   diagnostics
were run taking into account the first    features of the
data set  excluding time to first recidive and file category  
the output was whether or not a released convict would
commit recidivism 

analysis and results
our initial results running simple diagnostics on both
matlab libraries and python libraries show an excellent
agreement  preliminary diagnostics using simple logistic
regression and linear svm showed a testing error rate
hardly below     which remains fairly high given the size
of our data set  about       training points  for about     
testing points  
in light of the previous observations we decided to explore the following different next steps 
  run a principal component analysis to study the contribution of each features to the principal vectors 
  explore different algorithms  some outside the ones
covered in class  to identify the most performing ones 
  draw learning curves using different algorithm to provide
insight on potential error mitigation strategies 
  study the feature distribution among the data set as well as
engineer features to understand importance and correlations 

principal component analysis
our team acknowledged the need for understanding how the
features relate to each other  moreover  our team realized in
the preliminary analysis that the svm algorithm was very
expensive in terms of computational time  consequently  we
team explored a principal component analysis in the data
set using the complete set of features to reduce the problem
dimensionality  and to understand which set of features

carried the largest variance of the problem  the features
were normalized to have mean   and standard deviation   
figure   shows that the feature age dominates the first
component  whereas the feature time served dominates
the second one  also  results show that the first component
explains nearly     of the total variance  whereas the
second component explains    of the total variance 

figure    contributions of features on pca  blue   st
comp  red   nd comp 
furthermore  our team did a transformation of the feature
space into the first component subspace  a preliminary
analysis with linear kernel svm revealed that the test error
was     using a   fold cross validation  similarly  using
the first two components  the test error was      and for
the first three components  the test error was      these
results indicated that svms did not perform better than
the simple logistic regression  and that the computational
time involved in svms was hundreds of times larger than
logistic regression  we consensually decided to stop
using svms with different kernels and focus on exploring
different algorithms 

algorithm exploration
direct runs
our team used a set of machine learning algorithms to
verify which would perform the best in terms of accuracy
of the prediction  our team chose the algorithms based
on the material covered in the class cs    as well as
common ones such as random forest gradient boosting
recommended by lakkaraju  these algorithms are usually
good predictors in cases on which the data set has several
classification features  table   shows the algorithms that
were used in this part of the project  and the associated
test and training errors in a   fold cross validation  these
results were calculated using the default parameters of the
algorithms in the sklearn library of python  i e  the random
forest and the gradient boosting algorithm were run using
   trees and until there was only one element on each leaf 
perceptron and logistic regression algorithms did not have
a relevant parameter user definition 

fialgorithm
perceptron
logistic regression
random forest
gradient boosting

training error
     
     
    
    

test error
     
     
    
    

table    algorithms in the direct run
the results in table   indicate that gradient boosting is
the algorithm with the least test error  nevertheless  to
be conclusive about the supremacy of gradient boosting
over the other algorithms  our team decided to evaluate the
sensitivity of the results to the parameter definition of the
algorithms 
parameter estimations
the previous table proved the efficiency of algorithms
using trees  we therefore pursued that effort tried to find
the optimal parameter settings for our problem  figure  
shows how the number of estimators  i e  number of trees 
affects the test error in the random forest and gradient
boosting algorithms  it can be observed that with a greater
number of estimator  the error decreases  yet  there is
a threshold because using an inconsiderate number of
estimators increases significantly the computational time 
we therefore decided to use    estimators as a good balance
between a reasonable test error and running time 

figure    sensitivity of test error with respect to the trees
maximum depth

optimum maximum depth for random forest is     whereas
for gradient boosting is    using these parameters  the test
error in random forest was        and the lowest test error
in gradient boosting was       
note that the major difference between the   types of
algorithm is that during the training process  random
forests are trained with random samples of the data exploiting the fact that randomization have better generalization
performance  on the other spectrum  the gradient boosting
algorithm tries to add new trees to complement the ones
already built  it also tries to find the optimal linear combination of trees  assume final model is the weighted sum of
predictions of individual trees  in relation to a given train
data  this extra tuning might be deemed as the difference 
note that  there are many variations of those algorithms as
well  within the scope of the project we have used the most
common version of the algorithms as described above 

learning curves

figure    sensitivity of test error with respect to number
of estimators
considering    estimators for both algorithms  we then
plotted the variation of the test error for the maximum depth
of the trees  i e  of sub divisions   we also set the number
of elements per leaf at    elements as a limit to subdividing
the selected sub set of data  figure   points out that the

considering that the previous analyses indicated that the
best algorithms to predict recidivism are random forest
and gradient boosting  our team looked for improving the
performance of these algorithms  the parameters found in
the parameter estimation subsetcion were used  the performance was measured by the test error reported in a   fold
cross validation  to diagnose these algorithms  i e  to verify
whether or not the test error could be reduced and to find
possible ways of reducing it  our team constructed learning
curves  these curves compare how the training error and the
test error vary as a function of the size of the training sample 
figure   shows the learning curves for random forest
and gradient boosting algorithms  additionally  it shows
how the simple logistic regression algorithm compares to

fiboth the random forest and gradient boosting algorithms 
this figure indicates that the logistic regression algorithm
has its test error very similar in value to its training error 
this may explain that in order to improve our prediction 
there is a need for reducing the bias of the problem 
therefore  looking for additional features could improve
our predictions  conversely  the random forest algorithm
shows that its training and test error are very dissimilar 
this fact might be associated to a model with high variance 
nevertheless  after reducing the variance of the model by
modifying the maximum depth and the minimum number
of leaves in the model  no better test errors were found  the
gradient boosting method situates between both previous
methods  its training and test error are not as similar as in
the logistic regression  but not as dissimilar in the random
forest  interestingly  this method achieves the lowest test
error        
remarkably  all the methods have a flat test error curve
when using      data points  nearly a third of the data set 
or more  this lead us to think that the machine learning
algorithms reached converged values  and therefore a larger
data set would not improve our predictions 

figure    learning curves

feature engineering
statistical approach
one of the core of the study was exploring the impact of
the different features in predicting if an individual is likely
to go back to jail after his release  a first exercise that we
did was looking at the distribution of our features within
the data set used  note that there is only   non binary

figure    distribution     of the features per age groups

features  age  time served  number of school years  number
rules violated in prison and number of priors  we have
run different simple statistical visualization methods such
as histograms  distribution of the binary features given
segments of population  per age  time served and school
years  and finally mapping the distribution of a binary
feature at the intersection of   segments 
as the plot above shows  we can easily catch obvious trends
such as the fact that gender is unlikely to be a good predictor given the proportion of male in our population  also
immediate patterns are visible marriage and age  youngsters
and elderly have lower companionship rates  mainly  this
analysis led us to think that we did not necessarily need to
take into account all the features to make a good prediction 

figure    mapping of recidivist per groups of age and time
served in prison
figure   and   show matrices with age bins along the y axis
and respectively  school years and time served in the
x axis  the whiter the rectangle is  the more frequent that
person goes back to jail  these graph indicates a strong
correlation between age and years spend in prison when
looking at the recidivist population  this initial exploration
lead us to manually  using linear logic combinations  

fiquestion is to know weather to include the feature race or
not  in an optic to make our model as fair as possible it
would be interesting to try and remove the feature gottfredson        

online learning
peoples behavior and trends are always evolving in a society  therefore if such an algorithm is used for judicial decision  it would be important to constantly keep updating it as
we get new data points  consequently it is proposed to use
online learning algorithms 
figure    mapping of recidivist per groups of age and time
served
feature selection
we have used both forward backward  figure    feature
selection to measure the importance of the features w r t to
one another  without doubts the most crucial features are
 in order  
    ethnicity
     time served
     school years
     rule violations
part of the difficulty was understanding the which features were the most indicative of individuals likely to
recidive  manually engineering features  linear logic
combinations  as been explored un fruitfully  we believe
this approach should be pursued in feature work 

ethic
the problem we are trying to solve raises a lot of ethical
questions  how good must predictive efforts be to justify
using them to take restrictive actions that implicates the
liberties of others  this is a very ethical concern that needs
to be thought through in the case where the algorithm is
used for real decision making 

conclusion
our work is an attempt to recidivism modelling  we use features that are easily accessible by the judge and have a significant impact on the probability of recidivism  it was determined that the best predictive model is the gradient boosting
algorithm using    features  follow  felony property  with
an error of        in further work  this error rate could be
significantly decreased by using a bigger data set  huge data
set with millions of points have already been collected in the
us  yet  we could not access it for this project since an irb
protocol is required for sensitive data on human subjects 
this exploration is not be confused with a willingness to
substitute judge by machines  on the contrary  it helps them
make better decision to improve the american criminal justice system  to make it more just  objective and fair 

references
gottfredson  s          race  gender  and guidelines based decision making  journal of research in crime and delinquency 
           
milgram         why smart statistics are the key to fighting crime 
ted talk 

figure    backward selection using lr  rf and gb

limits and further work
do race matter 
extreme racial disproportionalities exist in american jail
population  therefore  it induces a bias in our model  the

fi