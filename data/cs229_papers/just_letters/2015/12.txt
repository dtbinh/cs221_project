using machine learning for medium frequency
derivative portfolio trading
abhijit sharang

chetan rao

department of computer science
stanford university
email  abhisg stanford edu

stanford university  netflix inc 
email  chetanr stanford edu

abstractwe use machine learning for designing a medium
frequency trading strategy for a portfolio of   year and    year
us treasury note futures  we formulate this as a classification
problem where we predict the weekly direction of movement of
the portfolio using features extracted from a deep belief network
trained on technical indicators of the portfolio constituents  the
experimentation shows that the resulting pipeline is effective in
making a profitable trade 

i  i ntroduction and r elated work
machine learning application in finance is a challenging
problem owing to low signal to noise ratio  moreover  domain
expertise is essential for engineering features which assist in
solving an appropriate classification or regression problem 
most prior work in this area concentrates on using popular ml
techniques like svm                and neural networks          
     coupled with rigorously designed features  and the general
area of focus is financial time series forecasting  with deep
learning techniques  we can learn the latent representation of
the raw features and use this representation for further analysis
     in this paper  we construct a minimal risk portfolio of  
year and    year t note futures and use a machine learning
pipeline to predict weekly direction of movement of the
portfolio using features derived from a deep belief network 
the prediction from the pipeline is then used in a day trading
strategy  using derivatives instead of the underlying entities
themselves leads to a more feasible problem  since derivatives
are less volatile and hence have clearer patterns 
the rest of the paper is divided into   sections  in section
   we describe the dataset and the raw features  section  
discusses the methodology in rigour  in section    we highlight
the findings of the experiment  finally  we conclude with the
possible improvements in this pipeline 
ii  dataset and raw features
we use data for   year and    year us treasury note futures
 shortcodes zf and zn respectively  obtained from quandl   
we have daily data from       where each data row consists
of the opening price    the closing price  the high price  the
low price  the open interest rate and the trading volume  note
that all these prices are mid prices  hence the bid ask spread
  https   www quandl com data cme
  https   www cmegroup com education files understanding treasuryfutures pdf

is ignored  the prices on which we perform the rest of the
analysis are obtained as the mid price of the opening and the
closing price each for zf and zn  this precludes us from
trading at a higher frequency than placing a single trade in the
day  but simplifies classification  figure   shows the historical
price evolution of zf and zn  as expected  in the long term
the bond market goes up  but the relative spread has minimal
drift 

fig     daily instrument price series
after removing rows containing missing information and
some high leverage points during the      financial crisis  we
are left with around      data points in total  we use    
of the data for training  the next     data as validation set
and the remaining     data for testing purpose  we do not
perform a k fold validation as we are only interested in how a
historical model performs on new data  the number of positive
and negative examples  as defined below  are      and     
for the training set      and     for the validation set  and
    and     for the test set 
our raw features consist of daily trend  current price moving average  computed over the past week with weekly
and biweekly moving averages each for zf and zn  hence 
we have              raw features  we perform a minmax normalization of these features for later analysis  with

fieach data point  we associate a label    if the portfolio price
 defined below  goes up   days from now and    otherwise 
iii  m ethodology
the flowchart below shows the complete ml pipeline of
the strategy  a component wise description follows 
zfraw
znraw
 

zfnorm
filter

 

znnorm
 

 

features

portfolio

ypred  k   zf  k   zn  

f  zf  zn  
 

f    zf  zn  
stacked rbms

 
classifier

fig     ml pipeline of the strategy
a  portfolio construction
for obtaining the daily price series of the portfolio  we
use principal component analysis  pca  on the price series
of zf and zn  pca transforms data in a higher dimensional
space to a space with equal or lower dimensionality with the
basis vectors ordered by the percentage of variance explained
by projection of the data along them  since treasury note
derivatives depend singularly on the interest rate  around    
of the variance in the data is explained by the first principal
component  hence  in practice  the loadings of the second
principal component remain fairly constant when computed
over a long duration of time  moreover  one observes stationarity in the resultant portfolio price series  implying that the
portfolio is minimal risk  hence  pport   p c  yr  p yr  
p c   yr  p  yr   where p c  yr and p c   yr are expected
to hold opposite signs  subsequently  whenever we perform a
single unit transaction on the portfolio  we go long on one
of the futures and short on the other in proportion to the
magnitude of their loadings in the second component  this
ensures that our positions in the two legs of the portfolio are
hedged  the daily portfolio price series for the whole data is
displayed in figure    observe that the drift component of the
price series is almost negligible 

fig     portfolio daily price series

layer of visible units and a layer of hidden units  and can
be used to learn a probability distribution over the input 
both layers of an rbm share weights  which are learned
using contrastive divergence
algorithm      a dbn models
q
  
 
 
k k  
p  x  h           h    
  p  h     h     where
k   p  h  h
x   h    p  hk   hk   is a conditional distribution for the
visible units conditioned on the hidden units of the rbm at
level k  and p  h     h    is the visible hidden joint distribution
in the top level rbm  see figure       in our pipeline  the
belief network consists of   rbms where the first rbm is
gaussian bernoulli and the second rbm is bernoulli  for
best performance  both rbms should be continuous  gaussian
is a special case   but training these rbms is extremely
difficult      in practice  using continuous input in only the first
visible unit seems sufficient for our application  we use the
standard algorithm proposed in      which consists of greedily
doing contrastive divergence on each rbm for learning the
reconstruction weight matrix and passing the transformed
input to the next rbm  the output of dbn is a binary latent
representation of the original input features derived from the
corresponding bernoulli distribution 

b  feature generation
to extract latent representation of the raw features defined
from the instruments  we use a deep belief network  dbn  
which consists of stacked restricted boltzmann machines 
a boltzmann machine is an energy based model where the
energy is a linear function of the free parameters    a restricted
boltzmann machine  rbm  consists of a bipartite graph of a
  http   deeplearning net tutorial rbm html

fig     representative dbn
  adopted from
http   www iro umontreal ca  lisa twiki bin view cgi public deepbeliefnetworks

fic  fine tuning and classification
the latent representation of the features is then used for
constructing a classifier for the prediction goal defined above 
there are three classifiers which we experiment with 






logistic regression   in logistic regression  we model
  y
 y
p  yi  xi      xi          xi      and  xi    
 
and maximise the log likelihood of the
  exp  t xi  
input  to prevent over fitting  typically a negative ridge
penalty is also associated with the objective function 
support vector machine   an svm discriminates between
data by constructing a hyperplane wt  x    b     by
 
p
i subject to yi  wt  xi     b  
minimising   w  
   c
   i   i     i  where  xi   is either xi or a higher
dimensional representation of xi   in either case  it does
not need to be computed explicitly as  wt  x    b  can
be obtained via kernel trick for classification since w
is a linear combination of a few  xi   which are called
support vectors 
neural network   a neural network consists of an input
layer  a few hidden layer of nodes  typically   or   
and an output layer with number of nodes equal to the
m
x
cardinality of categories  it minimises
 yi  yi     

   dbn training  in our algorithm  the purpose of the
belief network is to learn a hidden representation of the raw
features used  using recommendation from      we create a
stack of   rbms  with the first rbm containing    nodes in
the hidden layer and the second rbm containing    nodes
in the hidden layer  each rbm is trained for     iterations
with a mini batch size of     in the contrastive divergence
algorithm implemented in python  since there is a tendency
for the adjacent data points in the price series to be correlated 
we shuffle the training data at the beginning of every macroiteration  all hyper parameters  including the node sizes in
the hidden layers  are selected by monitoring the average
reconstruction error on the validation set  the reconstruction
error evolution for the best set of hyper parameters on training
and validation sets is shown in figure   

i  

where yi   h g xi     g is some linear combination of
node values in the hidden layers of the network and h is
an activation function  typically sigmoid or hyperbolic
tangent function   training a neural network involves
minimising the mean square error  mse  defined above
using gradient descent and back propagating the gradient
across multiple hidden layers for updating the weights
between nodes  here  instead of randomly initialising the
neural network  we use the reconstruction weights learned
in the deep belief network to form a neural network with
  hidden layers 
iv  e xperimentation and results
a  portfolio weights
we perform pca on the training data by standardising the
raw price series of zn and zn  we use the mean and standard
deviation of the training to standardise the raw price series of
validation and test for computing pca on them separately  the
second component loadings come out to be      for zf and
      for zn in the training       and       for the validation
set  and      and      for the test  the validation set loading
confirms the hypothesis that the portfolio is minimal risk  we
use the training loading in training  validation and test sets for
generating the labels for our supervised learning algorithms 
b  training the model
there are two stages of training  in the first stage the dbn
parameters are learned in an unsupervised manner and in
the second stage  fine tuning of the classifier parameters is
performed using supervised learning 

fig     reconstruction mse vs epochs for two stacked rbms
in the belief network
   supervised learning  we optimise the parameters for
neural network by gradient descent and for logistic regression
by gradient ascent  the optimal number of epochs used for
the algorithm is decided by the performance on the validation
set  for svm  we use the validation set for optimising the cost
parameter c  we use scikit      implementation of svm with
a gaussian kernel  own implementation of logistic regression
and implementation of      for neural network  an example
evolution of mse with epochs for neural network is shown in
figure   
c  predictions from the classifier
the precision and recall rates for the three classifiers in
shown in table   and table   respectively for the test data and
in table   and table   respectively for the training data  the
corresponding roc curves for test  true positive vs false positive rates  are shown in figure      and    for our application 
we lay more emphasis on recall than precision since we do
not want to lose any profitable trading opportunity that exists 
the average recall hovers around     for all three versions

fialgorithm
svm
logistic regression
neural network

actual 
      
      
      

actual 
      
      
      

table iv  training precision rate for each direction

fig     mse vs epochs for neural network

of classification in both training and test  this points at a nontrivial bias existing in the models  however  similar accuracies
in training and test imply that the models can be generalisable
enough  during training  we also observed high variance in
the neural network parameters  refer figure   above  which
we fix by introducing a momentum value of      this shrinks
the existing values of the coefficients by half during every
update of mini batch gradient descent  although the accuracy
is not very high  in financial applications accuracies which are
      higher than random can guarantee a profitable trade 
algorithm
svm
logistic regression
neural network

actual 
   
      
      

fig     roc curve for svm

actual 
      
      
      

table i  test recall rate for each direction

algorithm
svm
logistic regression
neural network

actual 
      
     
      

actual 
      
      
      

table ii  test precision rate for each direction
fig     roc curve for neural network
algorithm
svm
logistic regression
neural network

actual 
      
      
      

actual 
      
      
      

table iii  training recall rate for each direction

d  trading strategy
using the prediction of the classifier  we build a day trading
strategy where at the beginning of every day we go long  buy 
on the portfolio if the price is going to go down after   days
and short  sell  otherwise  we do not need to be long before

we sell the portfolio since short selling is allowed on most
commodity exchanges including cme  since the portfolio is
an artificial spread of the instruments  for going long we take
a long position in zf and a hedge adjusted short position in
zn and vice versa for going short  the trade size that we use
is    units of zf and   units of zn  which is the approximate
integer ratio of the p c  loadings defined earlier  finally  at the
end of   days  we square off the open position and accrue the
resulting dollar tick difference in our pnl  profit   loss  
a qualitative comparison of this strategy against a strategy

fiwhile a simplistic strategy can generate a moderately positive pnl  sophisticated algorithmic trading involves making
more complex decisions for which one would want to quantify
the magnitude of movement  for which building a regression
based strategy can be helpful  for learning the quantified
amount of portfolio price change  recurrent neural networks
come in handy  moreover  state of the art belief networks
work very well for continuous inputs  and it is worthwhile
to generate continuous features from rbms for solving an
appropriate classification or regression problem  the current
approach for trading seems promising  and it would be interesting to use it for trading multiple spreads and butterflies of
derivatives traded on multiple exchanges  moreover  it would
also be worthwhile to build a higher frequency trading strategy
if one has access to tick by tick data 
r eferences
fig     roc curve for logistic regression
which places trades by obtaining predictions from a random
classifier is shown in figure     observe that the drawdown of
the strategy is quite low  and excluding exchange transaction
costs and taxes  the profitability for a trade size of    units
for the portfolio is around       dollars over a span of two
and a half years 

fig      portfolio pnl vs number of days traded
v  c onclusion and future work
we build a day trading strategy for a portfolio of derivatives using features learned through a deep belief network
from technical indicators of the constituents of the portfolio
and using them to generate a predictor of weekly direction
of movement of the portfolio  we observe that the feature
reconstruction algorithm converges smoothly  we obtain a      higher accuracy than a random predictor with the three
classification techniques that we use and a much higher pnl 

    yoshua bengio  practical recommendations for gradient based training
of deep architectures  in neural networks  tricks of the trade  pages
        springer       
    dumitru erhan  yoshua bengio  aaron courville  pierre antoine manzagol  pascal vincent  and samy bengio  why does unsupervised pretraining help deep learning  the journal of machine learning research 
                
    edward gately  neural networks for financial forecasting  john wiley
  sons  inc        
    geoffrey hinton  a practical guide to training restricted boltzmann
machines  momentum                 
    geoffrey e hinton  training products of experts by minimizing
contrastive divergence  neural computation                       
    geoffrey e hinton  simon osindero  and yee whye teh  a fast learning
algorithm for deep belief nets  neural computation                 
     
    wei huang  yoshiteru nakamori  and shou yang wang  forecasting
stock market movement direction with support vector machine  computers   operations research                        
    iebeling kaastra and milton boyd  designing a neural network for forecasting financial and economic time series  neurocomputing           
          
    kyoung jae kim  financial time series forecasting using support vector
machines  neurocomputing                     
     r  b  palm  prediction as a candidate for learning deep hierarchical
models of data  masters thesis       
     f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikit learn  machine learning in python  journal of machine
learning research                    
     francis eh tay and lijuan cao  application of support vector machines
in financial time series forecasting  omega                     
     robert r trippi and efraim turban  neural networks in finance and investing  using artificial intelligence to improve real world performance 
mcgraw hill  inc        

fi