drug  store  sales  prediction  
chenghao  wang   yang  li  

abstract       in   this   paper   we   tried   to   apply   machine   learning   algorithm   into   a   real   world   problem     drug   store   sales  
forecasting   given  store  information   and  sales  record  we  applied  linear  regression   support  vector  regression svr   with  
gaussian   and   polynomial   kernels   and   random   forest   algorithm    and   tried   to   predict   sales   for         weeks    root   mean  
square  percentage  error   rmspe   is  used  to  measure  the  accuracy   as  it  turned  out   random  forest  outshined  all  other  
models  and  reached  rmspe  of          which  is  a  reliable  forecast  that  enables  store  managers  allocate  staff  and  stock  up  
effectively      

    introduction  
this  problem  is  one  of  several  machine  learning  problems  on  kaggle       the  aim  of  this  problem  is  to  forecast  future  
sales  of         rossman  drug  stores  located  across  germany  based  on  their  historical  sales  data   the  practical  meaning  of  
solving  this  problem  lies  in  that  reliable  sales  forecasts  enables  store  managers  to  create  effective  staff  schedules  that  
increase  productivity  and  motivation   whats  more   for  the  purpose  of  practicing  what  we  learnt  from  the  machine  
learning  class   this  problem  saves  us  the  trouble  of  collecting  data   and  in  the  meanwhile  provides  a  perfect  real  case  to  
apply  supervised  learning  algorithms   

    related  work     
  

as  a  matter  of  fact   substantial  effort  has  been  put  into  sales  prediction  problems   due  to  promising  performance   

artificial  neural  networks   anns   have  been  applied  for  sales  forecasting  in  many  scenarios   thiesing   f m   implemented  a  
neural  network  forecasting  system  as  a  prototype  to  determine  the  expected  sale  figures      whats  more   r j   kuo  utilized  
a  fuzzy  neural  network  with  initial  weights  generated  by  genetic  algorithm   gfnn   and  further  integrated  gfnn  with  ann  
forecast  using  the  time  series  data  and  promotion  length      this  is  closely  related  to  our  problem  because  promotion  has  
proved  to  be  one  of  the  most  important  features  in  our  dataset   there  are  some  interesting  attempts  too   for  example   
xiaohui  yu  tried  to  predict  sales  of  products  based  on  online  reviews      and  michael  giering  tried  to  correlate  sales  with  
customer  demographics      as  for  beginners  to  get  started  with  sales  prediction  problem   smola  described  a  regression  
technique  similar  to  svm  called  support  vector  regression   svr       breiman  posed  random  forest  algorithm     which  is  
based  on  decision  trees   but  randomness  is  added   it  performs  very  well  compared  to  many  other  algorithms   including  
neural  networks   discriminant  analysis  etc   and  is  robust  against  overfitting   svr  and  random  forest  are  both  
implemented  in  out  project   

    dataset  and  features  
the  dataset  of  this  problem  can  be  found  online    the  data  comes  in  two  sets  

                                                                                                                
    the  link  to  this  problem   https   www kaggle com c rossmann store sales  
    the  link  to  dataset   https   www kaggle com c rossmann store sales data     

  

fi    sales  dataset     historical  sales  data  for         rossman  stores  from            to              features  
include  store  number   date   day  of  week   whether  theres  a  promotion   whether  its  a  school  or  state  holiday  and  sales  on  
that  day   
    store  dataset     stores  individual  characteristics   features  include  store  type   assortment  level   nearest  
competitors  distance  and  when  the  competitor  was  opened   and  whether  theres  a  consecutive  promotion   
throughout  our  trial   weve  tried  to  take  advantage  of  different  subset  of  features   however   reducing  number  of  
features  didnt  increase  accuracy  for  this  problem   so  all  features  are  used  for  building  models   
         and  k fold  cross  validations  are  used  in  this  problem  for  training  and  testing   root  mean  square  percentage  
error   rmspe   is  used  to  measure  accuracy   which  is  defined  as       

 
 

        
 
           
 

    methods  
there  are  two  methods  to  train  the  data   one  is  to  train  each  store  separately   which  means  forecasting  sales  of  a  
single  store  based  on  its  own  sales  record   regardless  of  store  attributes   the  other  one  is  to  train  all  stores  together   
considering  store  attributes  as  parameters   
to  train  each  store  separately   one  straightforward  idea  is  to  apply  linear  regression   according  to  the  normal  
equation                        we  can  easily  predict  sales  by                 
further  more   we  figured  that  this  problem  can  actually  be  kernelized   here  consider  that  case  of  applying  map  
estimate  for      to  avoid  overfitting   which  results  in  the  following  primal  problem  
                           
  if  we  calculate      as                         and  define       

 
     

             we  can  see  that  this  problem  

can  actually  be  kernalized   thus  we  can  apply  the  kernel  trick   we  tried  gaussian  kernel  and  polynomial  kernel  in  this  case   
which  is  illustrated  as  following   
 a   gaussian  kernel           exp 

        
    

  

 b   polynomial  kernel                          polynomials  of  degree  up  to  d   
our   next   model   for   this   project   is   random   forest   regression    we   tried   this   model   because   its   fast   and   can  
accommodate  categorical  data   rf  first  picked  a  certain  amount  of  data  from  the  dataset  randomly   ie   bootstrap   and  then  
picked   a   certain   amount   of   features   out   of   the   total   features   randomly   to   build   decision   trees    the   final   result   for   each   test  
data  is  average  of  results  obtained  by  all  these  decision  trees   decision  trees  usually  overfit  the  data   however  randomness  
will  average  out  the  high  variance  

    experiments  and  results  
linear  regression  
linear  regression  is  used  as  our  baseline  model            cross  validation  is  used  here  to  divide  the  data  set  into  
training  set  and  test  set   as  it  turns  out   linear  regression  gives  us  a  rmspe  of          

support  vector  regression  
  

fi  one  thing  special  on  the  implementation  of  svr  is  that   it  need  to  build  an  m m  matrix   where  m  indicates  the  number  
of  training  samples   since  the  size  of  our  training  set  is               its  unrealistic  to  operate  on  the  whole  dataset   to  take  
use  of  the  abundant  dataset  practically   we  build  a  svr  model  for  each  store   and  compute  the  mean  of  each  stores  rmspe  
as  our  final  error  rate      
firstly   we  applied  polynomial  kernel  and  gaussian  kernel  for  a  single  store   store      by  trying  different  pairs  of  
    for  gaussian  kernel   and  different  pairs  of       for  polynomial  kernel   we  found  that  when                 
     gaussian  kernel  gives  the  best  rmspe  of          when                       polynomial  kernel  gives  the  best  rmspe  of  
        two  kernels  are  comparable  in  this  scenario  
secondly   using  the  method  of  finding  optimal  pairs  of  parameters  discussed  above   we  applied  gaussian  kernel  and  
polynomial  kernel  to  all  stores   as  we  dig  deeper  into  the  dataset   we  found  that  accuracies  vary  on  different  time  period  
of  prediction   below  are  figures  of  how  rmspe  varies  with  different  time  period  for  prediction      

  
 a   gaussian  kernel                                                                                  b   polynomial  kernel  
figure      how  rmspe  varies  with  different  time  period  to  predict  
as  shown  above   rmspes  for  both  kernels  first  increase  and  then  decrease  as  time  period  of  prediction  gets  larger   
for  gaussian  kernel   rmspe  reaches  minimum  when  predicting  for  just  one  week   however   for  polynomial  kernel   rmspe  
reaches  minimum  when  predicting  for     weeks   given  this  result   we  draw  figures  of  rmspe  for  all  stores  using  gaussian  
kernel  and  polynomial  kernel   predicting  for     week  and     weeks  respectively      

  
  

fi a   gaussian  kernel predict  for     week                                         b   polynomial  kernel predict  for     weeks   
figure      rmspe  for  each  store  
in  terms  of  average  rmspe   polynomial  kernel         beats  gaussian  kernel           significantly   in  the  meantime  
polynomial  kernel  is  also  more  robust  than  gaussian  kernel   given  that  there  are  fewer  outliers  and  no  extreme  
outliers rmspe     in  the  figure  of  polynomial  kernel   so  overall   polynomial  kernel  suits  the  dataset  better  and  provides  
more  reliable  results   
  

random  forest  
we  applied  random  forest  after  merging  all  data  including  all  the  categorical  data   we  used  scikit learn  package  of  
python  for  implementing  the  algorithm      
the  two  main  parameters  we  tuned  for  rf  is  the  number  of  trees  and  the  size  of  the  random  subsets  of  features  to  
consider  when  splitting  a  node   we  used     fold  cross  validation  to  get  rmspe  while  varying  these  parameters   two  plots  
are  shown  below   from  these  plots   we  could  see  that  rmspe  doesnt  change  too  much  after  tree  number  reaches      and  
after  feature  number  reaches          

  
fig      how  rmspe  changes  with  feature  number    

     

               fig      how  rmspe  changes  with  tree  number  

  
after  tuning  and  fixing  the  optimal  parameters   we  tried  to  change  the  size  of  the  training  data  to  fit  the  test  data  
better   we  used  the  last  two  weeks                          as  our  test  data  to  get  our  final  prediction  rmspe  result   we  
got  a  plot  rmspe  vs   number  of  month  before  the  test  period  shown  below   we  could  see  rmspe  almost  doesnt  change  
after  month  number  reaches  around       our  best  result  for  rf  is          figure     is  the  importance  ranking  bar  plot  for  the  
most  important      features  shown  below   competitors  and  promotions  prove  to  have  the  biggest  impact  on  sales   whereas  
features   we  also  plotted  how  rmspe  changes  as  the  duration  for  test  data  increases  as  shown  below   we  can  see  our  rf  
model  is  still  relatively  accurate  even  for  a  long  duration   up  to     months   

  

fi  
      fig      how  rmspe  changes  with  number  of  month        

      fig      feature  importance  ranking       most  important   

     

  

fig      how  rmspe  changes  with  number  of  month  for  test  duration  

    conclusion  and  future  work  
the  following  table  shows  the  results  of  our  models   

  

model  

rmspe  

remarks  

size  of  test  set  

linear  regression  

       

for  any     

      days  

svr  with  polynomial  kernel  for  store     

       

                 

      days  

svr  with  gaussian  kernel  for  store     

       

                  

      days  

svr  with  gaussian  kernel  for  all  stores  

avg  of         

each  store  chooses  its  own  optimal         

   week  

svr  with  polynomial  kernel  for  all  stores  

avg  of         

each  store  chooses  its  own  optimal         

   weeks  

random  forest  

       

                     max  features       trees  

   weeks  

as  is  shown  in  the  result   among  all  models   random  forest  works  the  best   and  provides  a  reliable  prediction  of  the  
sales   linear  regression   svr  with  gaussian polynomial  kernels  and  rf  all  have  their  own  strengths  and  limitations   by  
implementing  these  algorithms   weve  studies  the  properties  of  the  dataset  and  made  reasonable  predictions   in  the  future   
we  wish  to  use  the  fact  that  sales  records  are  consecutive  in  time   and  see  how  time  series  affect  prediction  result   also  
there  are  still  many  effective  machine  learning  algorithms  worth  trying   so  we  would  like  to  try  more  algorithms  in  the  
future   such  as  gradient  boosting  and  k nearest  neighbors  algorithm   

  references  
     thiesing   frank  m    and  oliver  vornberger    sales  forecasting  using  neural  networks    neural  networks           
  

fiinternational  conference  on   vol       ieee          
     kuo   r   j    a  sales  forecasting  system  based  on  fuzzy  neural  network  with  initial  weights  generated  by  genetic  
algorithm    european  journal  of  operational  research                            
     yu   xiaohui   et  al    a  quality aware  model  for  sales  prediction  using  reviews    proceedings  of  the    th  international  
conference  on  world  wide  web   acm          
     giering   michael    retail  sales  prediction  and  item  recommendations  using  customer  demographics  at  store  level    acm  
sigkdd  explorations  newsletter                         
     smola   alex  j    and  bernhard  schlkopf    a  tutorial  on  support  vector  regression    statistics  and  computing                 
          
     breiman   l            random  forests   machine  learning                  
     scikit learn   machine  learning  in  python   pedregosa  et  al    jmlr       pp                      

  

fi