diagnosing type ii diabetes based on medical records
madeleine gill

katherine holsteen

haju kim

stanford university

stanford university

stanford university

mmgill stanford edu

kholsteen stanford edu

hajuk stanford edu

i  introduction
type ii diabetes plagues an estimated    of the u s 
population and gives rise to a variety of challenging medical
complications  an estimated     of these individuals are
living with the disease undiagnosed  the primary aim of this
project was to use patient medical history to develop a
machine learning algorithm to classify individuals with or
without a diagnosis of type ii diabetes  an accurate
classification algorithm could serve as a clinically useful tool
to advance the prevention and control of the disease by
empowering doctors to prescribe preventative measures to atrisk patients and take earlier action to order diagnostic tests 
our data source for this project was a      kaggle
competition to solve the same classification problem  the
available data included three years of medical records for
      patients paired with a patient level indicator for
presence or absence of type ii diabetes  the inputs to our
algorithm were numeric features that we developed based on
the medical record data  we used boosted trees  random
forest  and support vector machines to predict the binary
outcome of a positive or negative diagnosis of type ii
diabetes 
ii  related work
a substantial body of literature has been published on
predicting incidence of type ii diabetes within five to ten
years based on disease history and clinical measurements
completed a systematic survey and external validation of   
of these models                      twelve relied on basic
predictors  including bmi  blood pressure  waist and hip
circumference  food frequency  and disease history   and
thirteen extended models also included biomarkers  glucose
and or hba c   the highest auc for   year risk of diabetes
was      in the basic models and      in the extended models 
the majority of these studies used logistic regression  which
allows for easier interpretation of effect sizes but may fail to
capture more complex nonlinear relationships and interactions
among features  our project adds a wider variety of machine
learning methods including non parametric tree based
methods and support vector machines with the goal of
building a robust model with lower bias  our project also
contributes new utility in its cross sectional nature and
immediate applicability for current rather than future
diagnosis  to the extent possible with our available data  we
created features based on the characteristics identified as
important in the literature  including older age          higher
bmi          and high blood pressure  hypertension       we
did not have access to some of the more specific metrics such

as waist and hip circumference  lifestyle variables or diabetesspecific biomarkers 
in addition to scholarly studies  prior work on this same
problem includes the      kaggle competition  through
interviews published on kaggle  the top two teams reported
investing substantial time into researching and developing
features  including categorizing diagnoses based on publicly
available groupings of icd   codes and identifying
medications with relevance to diabetes      each of the top
three teams implemented gradient boosted models as part of
their winning submission      we tried to pattern our
methodology in a similar way 
iii  dataset and features
our dataset is composed of medical records for the years
     through      for       subjects  including physician
visit transcripts with vital signs and diagnoses  lab test results 
and medications  the median number of transcripts was   per
subject  in preparation for the kaggle competition  they were
edited to remove diabetes specific diagnoses  lab results  and
prescriptions and assign each individual a binary indicator for
presence or absence of diabetes  of our sample        of
subjects have prevalent type ii diabetes  twice the estimated
prevalence for the u s  population  we divided this dataset
into a     training sample  n         and     validation
sample  n          for the initial analyses  we used a
balanced subset of the training sample including all of the
diabetic training patients  n        and an equal number of
randomly selected non diabetic patients  n        total  
we spent significant time examining the available data
and developing features with potential correlation to type ii
diabetes  the full set of    features is shown in table d  
these include medians of bmi  weight  height  and blood
pressure across transcripts  and counts of lab tests and
abnormal lab results over the three year window  we created
counts of diagnoses within clinically relevant categories 
using the clinical classifications software groupings of icd  codes specifically designed for healthcare data analysis     
the counts of diagnoses within each of the    most common
categories were included in our feature set  we also created
counts of medications with potential relevance to diabetes  in
particular  we counted specific hypertension prescriptions
such as lisinopril and hydrochlorothiazide because of the
established correlation between type ii diabetes and
hypertension  missing data did not present a major obstacle 
in the rare case of no data available to calculate a median  we
imputed the median value for the training sample  the count
variables did not require imputation because the absence of

fispecific diagnoses or medications corresponded to a count of
zero 
iv  methods
for the primary analysis  the full set of    features on the
balanced training set  n         was used to train each of
three supervised learning algorithms for classification     
gradient boosted trees      random forest  and     support
vector machines 
a  boosted trees
boosted trees is an algorithm that sequentially combines
weak learners and ensembles them through a weighted
majority vote to predict the class  boosted trees begins with a
small tree and builds additive tree models  if an observation is
misclassified  the observation receives more weight in the
next iteration  the final classifier is a weighted sum of the
decisions made by trees  the majority vote   for which the
weights are assigned based on their prediction performance 
thus  boosting tries to reduce the bias of a large number of
small trees that have low variance by finding the optimal
linear combination of trees in relation to the training data  the
gradient boosting algorithm is described below     

b  random forest
random forest is an algorithm based on bagging
 bootstrap aggregation  that averages predictions from a series
of decorrelated bootstrapped classification trees       thus 
random forest tries to achieve low variance by reducing the
correlation between the individual trees through bagging and
random feature selection  when used for classification 
random forest aggregates a class vote from individual trees
and then predicts the outcome with the majority vote  one
advantage of random forest is that random forest tends to
work well with very little tuning required  the only
parameter to optimize is the number of variables randomly
sampled as candidates at each split  varied according to
                             random forest is implemented
through the randomforest and caret packages in r 
c  support vector machines  svms 
support vector machines constructs a hyperplane decision
boundary in the feature space that maximizes the functional
margin  since we do not expect our data to be linearly
separable  we implement the soft margin svms and
formulated svm optimization problem with slack variables as
below 

gradient boosted trees
soft margin svms
  
  

initialize                        
for      to m 
 a  set
   
                          
    
 b  for      to  
i  compute                        
ii  fit a classification tree to the targets      
           giving terminal regions   
             
iii  compute
 
 
  
 
               
          

iv  update                 
   
   

  

output                         

boosted trees are known to be robust to outliers  missing
data  heterogeneous predictors  and can automatically select
variables       for this project  boosted trees algorithms are
implemented using the gbm and caret packages in r  when
tuning the parameters  we evaluate different combinations of
the interaction depth                  and the number of trees
to grow                             with    fold cross
validation with   replications 

primal
 w b



 
      
 
  

s t 

                               
              

dual





       
  

s t 



 
                     
 
   

               


         
  

thus  our optimization problem minimizes the training
error traded off against the margin  the parameter c is a
regularization term that allows us to control the trade off
between the slack variable penalty and the size of the margin 
the higher the value of c  the higher the variance of the
function  we tried both the linear and polynomial kernels  the
latter increases the flexibility of the decision boundary by
introducing polynomials with an intercept  the polynomial
kernel can be formulated as 
                 
when tuning the models  we limited the possible polynomial
degrees to       because of a lack of prior theory suggesting
higher degree relationships among the features  we evaluate
different combinations of the regularization term   
                                                            

fi      and the degree of polynomials          svm is
implemented with the kernlab and caret packages in r 
based on strong cross validation performance  secondary
analyses used the boosted tree algorithm to estimate three
additional models      using only the top four most important
features for the balanced training set      adding in the full
feature set for the full training sample  n       with
observations weighted inversely to outcome class size  and    
the top four features for the full weighted training sample 
as a follow up analysis  we estimated and evaluated the
boosted trees model separately for younger patients       
years old  and older patients       years old   these agestratified models included the full feature set and the full
weighted training sample 
we chose to optimize area under the roc curve  auc 
as our metric of predictive utility  the auc quantifies the
trade off between sensitivity and specificity at all cut off
thresholds  for each of the models estimated     fold crossvalidation was used to select parameters that maximize the
mean auc on the held out fold  finally  each model was
evaluated on the held out test set 
v  experiments results discussion
a  initial classification models
the following   models were run on the balanced subset
of the training sample with the full feature set      boosted
trees with     trees with an interaction depth of     a
shrinkage rate of      and minimum    observations per
node      random forest with   randomly selected predictors
considered at each split      svm with a linear kernel with
cost of       and     svm with a degree   polynomial kernel 
the cross validation results suggested an optimal cost of      
the results for these four models in terms of cross validation
auc and test auc are shown in table    the test roc
curves are compared in figure    roc auc on the test set is
largest for the boosted trees          and smallest for the
linear svm          but the different algorithms showed
relatively similar performance overall 
the area under the pr curves  not shown  ranges
between       and       for the four models  indicating
generally low precision in identifying prevalent diabetes cases
in the test dataset  for the development of this classification
tool  low precision is less of a concern than sensitivity and
specificity in practice 
table   cross validation auc and test auc for the four models using
the balanced training set  n         and full feature set  p      

boosted
trees

random
forest

svm
linear

svm
polynomial

cv auc
 se 

     
       

     
       

     
       

     
       

test auc

     

     

     

     

figure   roc curves for predictions on the held out test set with each of
the four initial models

figure   shows the variable importance scores for the top
   most important variables from each model  these charts
highlight the consistency in the key predictor variables across
algorithms  particularly in the standout four predictors
ct ccs       count of hypertension diagnoses   yearofbirth 
bmi med  median bmi over all transcripts   and ct ccs   
 count of lipid metabolism disorder diagnoses   indeed  these
predictors reflect known patterns in diabetes epidemiology 
older age and higher bmi independently increase ones risk
for type ii diabetes  while hypertension and hyperlipidemia
are common comorbidities with overlapping risk factors    
         

figure   variable importance for boosted trees  random forest  and svm
models applied to the balanced training dataset  variable importance for
linear and polynomial kernels were identical 

b  variations on the boosted trees model
based on the strong cross validation performance of
boosted trees in the initial model  we estimated three
variations on the boosted tree model to explore the predictive
power with a reduced predictor space and with the full
training set  first  we used a variable importance threshold of
   to select the top   variables from the initial boosted trees
model as the reduced predictor set  we estimated a reduced
model with the balanced training set using only the top  

fifeatures using     trees with an interaction depth of    next 
we added the      additional non diabetic training subjects
back into the training dataset  total n         and estimated
two boosted trees models including observation weights
inversely proportional to class sizes  these models utilized
    full feature set with     trees and interaction depth of    
and     top   features with     trees and an interaction depth
of    the auc of these three models for cross validation and
for the test set are shown in table   
with only four features  the boosted trees model
estimated on the balanced training set provided an auc of
      on the test dataset  this result achieved from a very
limited subset of meaningful predictor variables shows the
potential for a useful classification model requiring only a few
pieces of a patients medical history  the addition of negative
training examples did not significantly change the auc for
the full feature set or the reduced feature set  suggesting that
our initial models were not overfitting to the smaller balanced
training data  see table     the breakdowns of
misclassification rates by age and bmi are shown in table   
false positive rates are higher among those patients
displaying the key risk factors of older age and higher bmi 
while false negatives are more common among patients
without those characteristics 

c  age stratified model
since age is such an important predictor  we stratified the
training sample by age at study onset  year       and
estimated separate classification models on each strata to
compare predictive performance and key variables between
age groups  we estimated two separate boosted trees models
using the full feature set  except yearofbirth  on the patients
from the full training set      patient with age      n  
           trees with interaction depth of    and     patients
with age       n          with     trees and an interaction
depth of     figure   shows the discrepancy in predictive
performance across the two age groups  for patients    
years old  the model predicts well with a test auc of       
but for older patients  the predictive performance deteriorates
 test auc           the variable importance plots shown in
figure   reveal that bmi is by far the most important
predictor among the older patients  followed by ct ccs     
 hypertension   for younger patients  ct ccs     disorders of
lipid metabolism  rose to the top followed by bmi and
ct ccs       four out of the five top predictors are identical
across the age strata  the similarity of the predictors and
difference in accuracy suggests common risk factors among
the groups  but more noise in the older population due to an
accumulation of different diseases and stresses over ones
lifetime 

table   cross validation auc and test auc for boosted tree models on
balanced and weighted training dataset with full and reduced feature
sets 

full feature set

cv
auc
 se 
test
auc

top   features

balanced
         

full
         

balanced
         

full
         

     
       

     
       

     
       

     
       

     

     

     

     
figure   roc for boosted tree models by age group

table   classification rates by age and bmi

d 

n test
obs
    

     

     

d

   

    

     

d 

   

     

     

d

   

     

     

figure   variable importance for boosted trees by age group

normal weight
 bmi     

d 

  

     

     

d

   

    

     

table   cross validation auc and test auc for boosted tree models on
full feature set except for year of birth by age group

overweight
       bmi      

d 

   

     

     

d

   

     

     

d 

   

     

     

d

   

     

     

diabetes
age      
age     

obese  bmi       

pred 

pred  

full feature set except year of birth  p       
weighted training set
age    
age     
cv auc
 se 

     
       

     
       

test auc

     

     

fifigure   classification error vs  sample size for the boosted trees model 
run with the full feature set on progressively larger subsets of the full
training dataset

as an investigation into the performance of our boosted
trees models  we took a series of progressively larger samples
of our full training dataset and conducted parameter selection
and model estimation separately on each one  finally we
evaluated each resulting model on the held out test set  the
training and test misclassification error rates for each sample
size are shown in figure    the training error increases
between      and      observations and then stabilizes
around     as sample size continues to increase  the test
error remains relatively constant around     across all
sample sizes  the similar high rates of test and training error
rates suggest that our model suffers from high bias rather than
high variance  this is to be expected given that our feature
space was missing some key diabetes diagnostics like glucose
and insulin related lab results  which in many cases are
unexplained by the records we have available  throughout
the project we were aware of the limitations of our feature
space and repeatedly created and tested new features
including a wider range of diagnosis categories and ratios of
diagnosis counts to the number of physician visits  in the end 
these additions did not improve predictive performance over
the core set of    features used in these analyses 
vi  conclusion
we used three different classification algorithms to
predict the presence or absence of a type ii diabetes diagnosis
based on features created from patient medical records 
among the three learning algorithms used  the boosted trees
model performed the best  followed by random forest and last
support vector machines  svm may have fit the data less
well because of the high level of overlap among the classes 
even in a higher dimensional feature space
we demonstrated that the boosted tree model represents a
preliminary version of a potentially useful clinical tool  the
roc curve for the boosted tree model with the full feature set
indicates that an optimal cutoff threshold could provide
sensitivity of     and specificity of      given just a few
pieces of information from ones medical record  age  bmi 

number of hypertension diagnoses  and number of lipid
metabolism disorder diagnoses   the boosted trees model
yielded an auc of        a doctor could use either the full
or reduced model to obtain a relatively accurate prediction of
a patients diabetes status and help guide a choice to order
diagnostic blood tests 
analysis of the boosted trees model for different sample
sizes revealed no improvement in test error with additional
training examples  though the boosted trees model shows
promise  both test and training error remain higher than
desired  indicating high bias in the model  in future steps to
improve these models  we could incorporate clinical expertise
to expand and refine the feature space to help reduce bias 
this could be particularly helpful with a focus on risk factors
specific to the older adult population  where the boosted trees
model showed the greatest room for improvement 
references
    s  mani et al   type   diabetes risk forecasting from emr data using
machine learning   in amia annual symposium proceedings  vol       
p            
    j  t  jaana lindstrm   the diabetes risk score  a practical tool to
predict type   diabetes risk   diabetes care  vol      no     pp          
     
    s g  wannamethee et al   the potential for a twostage diabetes risk
algorithm combining nonlaboratorybased scores with subsequent
routine nonfasting blood tests  results from prospective studies in older
men and women   diabetic medicine  vol      no     pp              
    j  hippisley cox et al   predicting risk of type   diabetes in england and
wales  prospective derivation and validation of qdscore   bmj  p 
    b          mar    
    a  abbasi et al   prediction models for risk of developing type  
diabetes  systematic literature search and independent external
validation study   bmj  vol       p  e    e           
    w  rathmann et al   prediction models for incident type   diabetes
mellitus in the older population  kora s  f  cohort study   diabetic
medicine   a journal of the british diabetic association  vol      no     
p               
    m  zwemer   practice fusion diabetes classification   interviews with
winners               online    accessed             
    agency for healthcare research and quality  rockville  md   
 healthcare cost and utilization project  hcup    june        online  
 accessed   december       
    t  hastie et al  the elements of statistical learning  data mining 
inference  and prediction  springer series in statistics       
     j  friedman et al   additive logistic regression  a statistical view of
boosting  with discussion and a rejoinder by the authors    the annals of
statistics  pp                
     m  w  andy liaw   classification and regression by randomforest   r
news      pp              
     p  j  lin   clinical multiple chronic conditions in type   diabetes
mellitus  prevalence and consequences   vol      no          
     g  j bosman and m  m  b  kay   alterations of band   transport protein
by cellular aging and disease  erythrocyte band   and glucose transporter
share a functional relationship   biochemistry and cell biology  vol     
no      pp                  

fi