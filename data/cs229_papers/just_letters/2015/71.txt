analyzing donations to      presidential candidates

raphael palefsky smith and christina wadsworth
stanford university
rpalefsk stanford edu  cwads stanford edu

 

introduction

with an election cycle coming up  theres a huge amount of discussion and media focus on which
groups   and what kinds of individuals   will vote democrat and which will vote republican  we
wanted to apply a machine learning approach   given details about a person  can we determine
whether theyre a democrat or republican  thankfully for society but unfortunately for our project 
theres no public dataset on how individuals vote  so for our approach  we used the next best
thing  donations to presidential candidates  whenever anyone gives to a candidate  the candidate
is required by law to publicly disclose the donors  self reported  name  occupation  workplace 
donation amount  date of donation  and full address  these records are available as huge csvs
from the federal election commission  and so our data collection consisted of a simple download 
the input to our algorithm was one of these donation records  minus the candidate  we then
used logistic regression to output a predicted political party for the donor  democrat or republican  this is a simple binary classification problem  so most of the work was in the pre processing
and feature optimization stages 
our baseline for accuracy was a humans performance on the same task  we pulled    random
samples from the dataset  and attempted to classify them ourselves using every democrat republican stereotype in the book  to our surprise  we only got half of them right   our intuition was no
better than guessing  so  we were particularly excited to buck the computers can only approximate human ability trend and potentially create a classification model that was better than a human 
this was a joint project with cs     but our work for that class was only related in that it
uses the same dataset   our goal in cs    was to use constrained k means to find optimal locations
for candidate fundraisers  christina wadsworth is not in      so her work was limited to the
k means efforts  so  all of the work presented here  beyond basic data acquisition  was solely for
cs    and performed by raphael palefsky smith 

 

related work

the first paper we found was political campaigns and big data     by david w  nickerson and
todd rogers  which gave a general overview of data processing techniques employed by real world
political campaigns  while it didnt delve into our specific problem  it noted that campaigns
had used supervised learning  including logistic regression  to analyze propensity to donate 
despite its generality  we felt that the paper validated that our model was appropriate for the task at
hand  after all  if real campaigns used these techniques  theyre probably good for our purposes  too 
then  we looked at concrete approaches to predicting donations and political sentiment using
other data sources  namely blog posts and tweets  in mining sentiment classification from
political web logs      kathleen t  durant and michael d  smith predict a blog posts political
affiliation with     accuracy using naive bayes  similarly  colin conrad  in his paper predicting
political donations using data driven lifestyle profiles generated from character n gram
 

fianalysis of heterogeneous online sources      used common n gram techniques to predict the
donations of twitter users with over     accuracy  he used the exact same donations dataset
we used  similarly employing the candidate as a label  but using tweet data as his algorithms
input as opposed to our use of the donors personal information  the approaches of durant smith
and conrad are fascinating  but they require the donor to have written blog posts or tweets  our
algorithm has the advantage of operating solely on the donors personal information  without the
donor having written a single word 
we then examined an approach that  like us  used demographic data to draw conclusions  in
an analysis of the      presidential elections using logistic regression      jairo nicolau used
surveys of voters in a      brazilian election  respondants were asked for their age  gender 
skin color  schooling  religion  party sympathies  and who they voted for  instead of using ml
techniques as we did and building a classifier  nicolau took a pure statistics approach and used
logistic regression to determine the most important factors in an individuals vote  his methodology
had distinct advantages over ours   his data contained more demographic attributes  and he had
actual data on voting behavior rather than our analogue of donations  however  our approach
doesnt require expensive polling  instead relying on freely available public information 
finally  we found a non academic project that approached our exact problem with the same
data and same algorithms that we used  jasmine wilkersons political party affiliation predictor
    used the same fec dataset to predict republican or democrat using logistic regression  her
model used slightly different features  incorporating the donation amount feature and including
records from senate and house candidates  since she didnt report her models accuracy  we could
not compare it to our own  but it was reassuring to see that others have attacked the problem with
almost identical methods 

figure    actual stanford professors in dataset

 

dataset and features

our dataset was comprised of donations to the four candidates with the most money raised at the
time  hillary clinton  bernie sanders  jeb bush  and ben carson  this was downloaded from
the sunlight foundations influence explorer      we also augmented our data with income byzip code statistics from the irs      all together  our data consisted of almost         individual
donations and        zip codes 
most of our pre processing was filtration of the donation set  the raw data contained donations from pacs and businesses  which we removed in order to only analyze individuals  we then
removed duplicate donors  ones who had donated multiple times and therefore showed up more than
once  to arrive at a little more than         unique individuals  we then removed donation amount
and data information  so our model would generalize beyond an individual donation event  after
building a model with over     accuracy on a test set  we tried it out on our friends  asking them
to provide their information and see how our classifier faired  to our surprise  despite its incredible
accuracy on the test set  it was wildly inaccurate in the real world  correctly classifying only   out
 

fiof   of our friends  after manually digging throug our data  it turned out that the donors title  mr   
ms   was only reported by republican candidates  this was a great reminder that accuracy can be
misleading  and one must always fully understand their data  so  the title feature was removed as
well  then  we augmented each individual with the average income in their zip code  and the irs
tax bracket corresponding to that average in order to discretize and contextualize the raw number 
sample data is shown in figure    finally  we use scikit learns     dictvectorizer to transform
our data into templated features  for instance  a donor with zip code       will have zip      
    and zip           

 

methods

to build our model  we settled on the stochastic gradient descent learning algorithm because of
its balance of speed and accuracy  after trying out several objectives  hinge  logistic regression 
ordinary least squares   we settled on logistic regression  as it provided the best accuracy   svm
had      higher test error  and least squares added over     error   the stochastic gradient descent
rule is as follows  where h is the objective function  
loop until convergence  
for i     to m  
for j     to n  
 i 
j    j    y  i   h  x i    xj
 
 
 
the algorithm is pretty simple  for each training example  it loops through each feature  for each
of these features  it makes a prediction using the current best guess objective function h   when
this is subtracted from the label y  we get a measure of how wrong the guess was  this error
factor is multiplied by a learning rate  and the value of the feature itself to form a delta  which is
then used to update the best guess parameter vector  
in more practical terms  stochastic gradient descent works by evaluating one feature at a time 
computing the gradient for the objective at that point  and nudging the parameters in the opposite
direction  bit by bit  these nudges push the parameters to a local minimum  since the loss
functions used have only one minimum  this local minimum ends up being the global minimum as
well  as to our specific objective function  logistic regression looks like this 
h  x    g t x   

 
  et x

where g z  is called the sigmoid function  the advantage of using g is that it smoothly
squishes the output of t x to lie between   and    this means it works wonderfully for binary
classifiction problems like ours  since y will only ever take on the values   or   as well  therefore 
the decision boundary is just h  x         simple  and it performs well 

 

experiments results discussion

we used scikit learn for all aspects of our application  specifically  we employed its built in
stochastic gradient descent implementation  this means we got hyperparameters for free  we
still played around with them ourselves  setting the learning rate to a several constant values  our
best       had       error   but none of them outperformed scikits built in defaults based on years
 
of research  for instance  it defines the learning rate as   t t
where           and t  is
    
determined by a heuristic proposed by leon bottou  we opted not to use mini batches  since we
had no need to formulate the problem as online learning 
our primary metric was simple accuracy on a test set   while we eventually evaluated precision  recall  and f   the harmonic mean of precision and recall   our how well are we doing
metric was accuracy  the number of examples correctly classified divided by the total number of
examples  since we had relatively plentiful data  we were content to use the much faster and simpler
 

fihold out cross validation instead of k folds  we held out     of our samples  chosen randomly  as
a test set and used the rest for training 
to pick the optimal feature set  we used a rudimentary but incredibly effective method  brute force
evaluation  that is  we simply computed every possible subset of our features and evaluated the
test error on each candidate  we chose our final features as the subset with the highest accuracy 
why did we use this approach instead of more sophisticated methods like backwards search or
filter selection  we had only    features  so there were only               possible subsets 
and since evaluating error was embarassingly parallel  we took advantage of the stanford corn
systems    core machines and distributed the workload  all in all  it took less than ten minutes
to find our optimal feature set  and it was great knowing we had the guaranteed global maximum
for accuracy  figure   plots test and training error as we evaluated each feature subset  re arranged
from maximum to minimum  it demonstrates the huge difference feature selection can make 

figure    feature subset vs error
our worst feature set  area income  city  tax bracket  had over     error  our best set  first middle last name  employer  occupation  state  zip code  tax bracket had under      we were
surprised that average income in the donors zip code actually hurt our accuracy  furthermore 
while that figure discretized into tax bracket improved our accuracy  it contributed less than      
so  our stereotypes about republicans being accross the board richer ended up not helping much 
once our features were chosen  we plotted the classic test training desired error curves  figure     with an admittedly overly optimistic desired error of     we got two main takeaways from

figure    training set size vs error
this graph  firstly  our test error seemed to be pretty much flat as we added more training examples 
because of this plateau  we felt confident that we didnt need to acquire more data  secondly  the
 

firelatively large gap between training and test error pointed to a variance problem 
the rest of our experimentation focused on reducing this variance  we recieved three suggestions for tackling the problem  using a simpler algorithm  using fewer features  and adding
in regularization  its hard to get simpler than logistic regression  but just for kicks  we tried
least squares regression and got over     error  so that was a non starter  using fewer features
was also a no go  since our brute force feature selection showed that we couldnt get a lower error
with any other set  finally  we tried out multiple forms of regularization  including l  and l  
admittedly  we did not exhaustively explore the hyperparameters for these regularizations 
rather  we tried a couple different constant factors for each  and were disappointed with the results 
the best we could get with l  was a      increase in error  and l  was even worse  it brough our
error up to      so  regularization didnt bear much fruit  however  we were still pretty pleased
with our results  here are our key metrics and confusion matrix 

 a  key metrics

 b  confusion matrix  rows actual 
columns predicted 

figure    evaluating results
from the confusion matrix  we can determine that our false democrat rate is        whereas our
false republican rate is        this means we are much  much more likely to falsely categorize
someone as a democrat  which makes sense given that our dataset contains nearly twice as many
democrats as republicans  one such false democrat was a realtor from wilsonville  oregon 
while oregon is an incredibly democratic state  which our model learned   there are always
exceptions 
why did logistic regression perform best  in our trials  it outperformed svm by less than
a percent  so its not as if logistic regression blew everyone else out of the water  we attribute this
slight performance boost to its absolute simplicity   the simpler the algorithm  the less chance of
overfitting  nevertheless  due to the variance problem evident from our graph  we believe we still
had a slight case of overfitting  while we attempted to solve this issue through regularization  we
didnt succeed  and removing features only increased our error  so  our  not too terrible  overfitting
remained unmitigated 

 

conclusion future work

despite our variance issue  we were still surprised by our accuracy  given just someones name 
job  and address  we can predict their political affiliation with over     accuracy  and thanks to
the relatively small number of features  we could compute the guaranteed optimal feature set via
brute force optimization  our accuracy is especially impressive considering the abysmal       performance of humans  evidently  our model picks up on trends that a human will miss 
our best performing algorithm  logistic regression  is also one of the simplest  we believe this
simplicity  coupled with our relatively few features  reduced overfitting and kept our variance much
lower than it could have been  sometimes  simpler is better 
finally  if we had more time and  wo manpower  we would definitely attack the variance problem 
we believe it would possible  with the right sort of regularization or more nuanced learning algorithms  to close the training test error gap by a couple more percent at least  that is to say  given a
year of work by a team of five  our hunch is that we could get above     accuracy  but given our
constraints and rudimentary methods      isnt half bad 

 

fi 

references

    nickerson  david w   and todd rogers  political campaigns and big data  the journal of economic
perspectives                    web   
    durant  kathleen t   and michael d  smith  mining sentiment classification from political web logs 
proceedings of workshop on web mining and web usage analysis of the   th acm sigkdd international
conference on knowledge discovery and data mining  webkdd        philadelphia  pa       
    conrad  colin  predicting political donations using data driven lifestyle profiles generated from character n gram analysis of heterogeneous online sources         
    nicolau  jairo  an analysis of the      presidential elections using logistic regression  brazilian political
science review                     
    wilkerson  jasmine  political party affiliation predictor  datasciencedojo  datasciencedojo     june
      web     dec       
 http   demos datasciencedojo com demo political party   
    real time federal campaign finance  influence explorer  sunlight foundation     dec        web    
dec       
 http   realtime influenceexplorer com newest filings   ordering  filing 
number min raised   committee class p time range      cycle report type 
monthly  
    soi tax stats   individual income tax statistics   zip code data  soi   irs  irs     aug        web 
   dec       
 https   www irs gov uac soi tax stats individual income tax statistics
 zip code data  soi   
    scikit learn  machine learning in python  pedregosa et al   jmlr     pp                  

 

fi