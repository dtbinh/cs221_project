dance type classification in irish and scandinavian folk music
elliot kermit canfield and iran roman
center for computer research in music and acoustics 
stanford university  stanford  ca       usa
 kermit iran  ccrma stanford edu

   methods

abstract
     song corpus

we used      folk songs stored in abc  format primarily downloaded from john chamberss collection      these tunes were
divided into several categories  sorted by dance typereel     
songs   jig      songs   hambo     songs   pols     songs  and
hornpipe     songs   after data standardization  for each dance
type we randomized the song order and truncated our data sets 
this was done so we had the same number of training and testing
examples across dance types and thus  decreased the possibility of
biasing larger data sets 

for centuries  western cultures have written folk songs down  in
the   st century  this has resulted in large databases of music from
all around the world  we have built  trained  and tested classifiers on irish and scandinavian dance music using songs encoded
in symbolic representation  abc format   downloaded from john
chambers online folk song database  these tunes were sorted by
dance type  reel  jig  hambo  pols  and hornpipe  raw data was
preprocessed to be in the same key  have no ornaments  and no
abbreviations  features extracted were standardized to have zero
mean and unit variance  extra trees classification and variance
thresholding allowed us to reduce feature dimensionality from   
to     we classified our data using support vector machines  svm  
logistic regression  nave bayes  gradient descent  and k means
clustering  we evaluated these classification algorithms using leaveone out cross validation  trained on    percent of the data  and
tested on    percent  our svm classification methods turned out
to be the most accurate with less than      training and testing
error 

     data standardization and normalization
we found obvious notational variability  as the original corpus contains songs that were input by different authors  in order to compare songs both within and across dance type  we standardized our
data by removing duplicate entries  transposing all songs into the
same key  removing grace notes  removing polyphony  expanding
repeated sections that were notated in an abbreviated manner  and
normalizing rhythmic subdivisions  figure   outlines the steps we
followed to clean the data 
after feature extraction  we performed additional processing
to normalize our data so that the various machine learning algorithms would not be biased towards extreme data values  we encoded categorical data as individual features  then  we tested normalizing the data to be between zero and one  as well as having
zero mean and unit variance  the second normalization proved to
be more successful across algorithms 

   introduction

as early as       alan lomax envisioned using algorithmic methods for the classification of folk music      this automatic method
would allow musicologists to associate a newly discovered musical score with other folk songs similar in style and genre  since
then  teams of musicologists have employed a variety of machine
learning techniques to classify musical genre and style           
within the past decade  mckay and fujinaga have worked toward
developing feature extraction algorithms that classify song genres
using high level musical features      in this paper  we were interested in classifying folk music of irish and scandinavian origin 
using a subset of john chambers collection of more than       
codified and organized tunes from europe  we have built  trained 
and tested a classifier on irish and scandinavian dance music  in
order to extract our features  we used a subset of mckays and
fujinagas feature extraction algorithms      more specifically  we
wanted to see how several standard machine learning algorithms
including support vector machines using linear  rbf  and polynomial kernels  k means clustering  logistic regression  nave
bayes  and gradient descentperform at classifying dance type
in a one vs one classification task for each pair of dance types in
our corpus 

     feature selection
while metric features might play a larger role classifying dance
types correctly  melodic features can also help identify a songs
genre  melodic feature extraction consists of building histograms
of melodic interval and pitch classes throughout a song  additionally  we calculated metrics that summarize melodic direction arc 
cadences  and repeated notes  rhythmic features included time
signature  features describing note duration  time between consecutive notes  and variability of time between consecutive notes  we
identified    total features     melodic and    rhythmic  that could
potentially be useful for classifying song type  we used tools from
music   to facilitate feature extraction     
after extracting features  we employed variance thresholding
and extra trees classification to evaluate the usefulness of our fea  abc notation is a plain text format designed to be both human and
machine readable  the abc format includes all melodic and rhythmic
features of these folk songs  it is primarily used for notating folk songs
and uses a standard character set to represent music in symbolic notation 

 

fidata preparation
western music notation

abc notation
t 
o 
r 
z 
s 
m 
l 
k 
  

another supervised learning algorithm that we tried out was
logistic regression  which makes a binary decision about our data
based on the sigmoid function seen in equation   
transcription from music
notation to abc format

g z   

v  astg  ota polska
sweden
hambo polska
     john chambers  jc trillian mit edu 
handwritten ms by jc from the     s
   
   
d
a b ag f a   d f a  f    f gf e f  ge c e  d e f  d    

z   t x

 
 

j    j  

translation from abc to
western music notation

j    


j  
j

m
 x
 h  xi    y i   
  i  

   

   

this algorithm repeatedly takes steps toward the direction that
lowers j   with the steepest slope 
the final supervised learning algorithm was nave bayes  we
expect the distribution in several of the melodic and rhythmic features that we extracted to be gaussian  with our data clean and normalized across features  we implemented a gaussian nave bayes
algorithm for classification  as shown in equation    where the likelihood of the features describing x is assumed to be gaussian 

figure    steps followed to standardize each song in the corpus 

tures  given the authors music theory backgrounds  the final feature selection was assessed through a combination of musical intuition  and the results from these feature selection algorithms  in the
end  we reduced the number of features from    to     describing rhythmic landmarks in our data  and    describing melodic
qualities 



 xi  y   
 
p  xi   y    p
exp 
 y 
 y 

     classification algorithms

   

k means clustering was the unsupervised learning classification method that we also used to classify the data  as seen in equation   

we used a one vs one classification approach where we compared
every dance type to each of the other song types  we randomly
selected    songs of each dance type to serve as the training set
and    to be the final evaluation set  for each classification task 
we randomized the order of songs in the training sets and evaluation sets  although the distinction between training and testing was
never changed 
for our classification  we used five supervised learning algorithms and one unsupervised learning algorithm  our supervised
learning algorithms included a support vector machine  svm 
with a linear kernel  we decided to use a linear kernel  as the
total number of features      is small in comparison to the amount
of training data     songs in each case   we also implemented an
svm with a radial basis function  rbf  kernel  gaussian kernel  
which is shown in equation   
  x  z   
 
   

   

logistic regression basis its output on a finite features space describing the data  given our data set and our one vs one classification methodology  this algorithm is a simpler model compared to
the svm algorithm 
the next supervised learning algorithms we implemented was
gradient descent  for which the formula can be found in equation
   where  is called the learning rate and j   is the cost function
as seen in equation   

   transpose to c
k  c
   g a gf e g   c e g  e    e
d e  fd b d  c d e  c    

k x  z    exp 

   

where 

   remove grace notes
k  d
   a b ag f a   d f a  f    f
e f  ge c e  d e f  d    

   expand repeats
k  c
  g a gf e g   c e g  e    e
d e  fd b d  c d e  c 
  g a gf e g   c e g  e    e
d e  fd b d  c d e  c 

 
    ez

argmin
s

k x
x

kx  i k 

   

i   xsi

in our specific case  the number of clusters  s  only had two
dimensions  thus k      and our task is to find the mean  i   of
each s 
we evaluated the performance of the supervised learning algorithms using leave one out cross validation on the training data 
in this process  the model is training on all the data except a single training set  which is left aside for validation  this procedure
is repeated exhaustively until all data points have been set aside 
the results of this algorithm evaluation method are summarized in
table   
the metric to test performance of k means on the training data
was the f  score  which relates the number of correct positive results to the actual number of examples in each of the classes on
which the algorithm was trained  these results are also summarized in table   

   

the rbf kernel maps to an infinite feature space where x and z
are two vectors containing the input feature space for two different
classes and  is a free parameter of the gaussian distribution 

 

fi     visual representation of data

with close to chance performance across algorithms  the linear
and rbf svm algorithms showed the highest average test accuracy  while k means clustering presented the lowest one 
figure   shows the output of various classification algorithms
of the first two principal components of a pca reduced version of
the training data for the hornpipe vs jig classification task  it is
interesting to compare the performances of the algorithms back to
back  visually  it is clear that the data separates fairly well  the
svm with linear kernel  gradient descent  nave bayes  and logistic regression all draw a similar decision boundary  while the
svm with rbf kernel has a similar error to the other algorithms 
its large number of support vectors is evidence to extreme overfitting  k means clustering does not know the labels of dance
types  and its decision boundary is not as accurate as the other algorithms  it finds two gaussian distributions among the data that
do not reflect the true categorical labels 

in order to plot the output of our classification algorithms using
a  d scatter plot  we carried out principal component analysis
 pca  to find the first two principal components of the feature
space  the equation for pca that finds the first principal component is shown to maximize
 
m
m
  x  i t  
  x  i   i t
t
u 
   
 x
u    u
x x
m i  
m i  
where u is the unit vector representing the direction on which the
data points x i  can be projected  the first principal component
found by pca depicts the axis on which the variance of the data is
retained and maximized  we can find k principal components by
maximizing the first k us to project the data into k dimensional
subspace 
   results and discussion
     algorithm accuracy after feature selection
the training rate of our linear svm algorithm before feature selection is shown in figure  a  each line represents classification
between a pair of song types  before reducing our feature dimensionality  the number of training examples needed for classification
accuracy of most song type pairs to converge oscillated around    
we used variance thresholding and extra trees classification to reduce the number of features to     the training rate after feature
selection for the same classification algorithm is shown in figure
 b  reducing feature dimensionality lowered the number of training examples needed for classification accuracy to converge across
song type pairs 

 a  before feature selection

 a  svm linear kernel

 b  svm rbf kernel

 c  gradient descent

 d  nave bayes

 e  logistic regression

 f  k means clustering

 b  after feature selection

figure    learning rate for svm with linear kernel before and
after feature selection  each line represents classification between
a pair of song types 

     evaluation of machine learning algorithms
the training accuracies of our classification algorithms  as quantified by leave one out cross validation  f  score was the validation
metric of k means clustering   is summarized in table    classification algorithms were able to separate most song type pairs  except hambos vs pols and reels vs hornpipes  the algorithm with
the lowest average training accuracy was k means clustering 
with the test data  our classification algorithms separated most
song type pairs with test accuracies around    percent  however 
classification of hambos vs pols and reels vs hornpipes remained

figure    comparison of classification algorithms for the hornpipe
vs jig dance pair  the graphs plot the first two principle components of a pca reduced version of the data  training data is displayed with gray circles and test data is printed with white circles 
gold circles ring the support vectors and xs show the centroids of
clusters  when applicable   data points that match the color of the
background show correctly classified songs while ones that are the
opposite color are misclassified 

 

fifigure   shows cases where the classification goes horribly
wrong  in one plot  we see the result of an svm with a  rd order polynomial kernel  there are several mislabeled points in
the training set that the algorithm tries to overfit  unsuccessfully  
in the second plot  the dance types are completely inseparable 
clearly the algorithm stands no chance to draw a reasonable decision boundary for this pair of dance types 

    michael cuthbert  christopher ariza  and lisa friedland 
feature extraction and machine learning on symbolic music
using the music   toolkit  ismir       
    shyamala doraisamy  shahram golzari  noris mohd  norowi 
md  sulaiman  and nur izura udzir  a study on feature selection and classification techniques for automatic genre classification of traditional malay music  ismir       
    cory mckay and ichiro fujinaga  automatic genre classification using large high level musical feature sets  ismir 
     
    cory mckay and ichiro fujinaga  jsymbolic  a feature extractor for midi files  icmc       
    jcs abc music collection  http   trillian mit 
edu  jc music  
    music    a toolkit for computer aided musicology  http 
  web mit edu music    

 a  svm  rd order polynomial kernel

 b  nave bayes

figure    two cases where the algorithms failed to classify the data
satisfactorily  in  a  we see a polynomial kernel with severe overfitting  in  b   the data is inseparable  and thus the classification is
no better than chance 

   conclusions
clearly  our use of machine learning has a long way to come before it will rival a human ear  dance types that are more dissimilar 
such as hornpipe vs jig  are easy to classify  however  classification of dances that are similar in meter and style  such as reel
vs hornpipe  turned out to be no better than chance  our results
suggest that we can write new feature extraction methods that take
advantage of mesoscale qualities in our data  these qualities could
include phrase and sub phrase quantifiers  form decoders  and ngram identification of rhythmic and melodic motives  in all cases 
we suspect a more sophisticated treatment of events at multiple
temporal scales would greatly increase the accuracy of our classification algorithms 
finally  we can expand our classification methods to include
unsupervised learning algorithms and neural networks  most of
the algorithms we explored are supervised algorithms that preassume we know the dance type of the training data  we think
it would be interesting to see if we could trace musical similarity through dance forms in an unsupervised manner  in order to
gain cultural and historical insights about how folk music spread
throughout northern europe 
   references
    alan lomax  folk song style  notes on a systematic approach to the study of folk song  international folk music
journal       
    wei chai and barry vercoe  folk music classification using
hidden markov models  international conference on artificial intelligence       

 

fitable    training accuracy for algorithms on a normalized    sample training data set  data was normalized to have zero mean and
standard deviation of one  cross validation is the validation method for all algorithms except k means clustering  for which f  score is the
validation metric 

hambo vs hornpipe
hambo vs jig
hambo vs pols
hambo vs reel
hornpipe vs jig
hornpipe vs pols
hornpipe vs reel
jig vs pols
jig vs reel
pols vs reel
average accuracy

svm  linear 
               
               
               
               
               
               
               
               
               
               
    

svm  rbf 
               
               
               
               
               
               
               
               
               
               
    

k means
               
               
               
               
               
               
               
               
               
               
    

logistic regression
               
               
               
               
               
               
               
               
               
               
    

nave bayes
               
               
               
               
               
               
               
               
               
               
    

gradient descent
               
               
               
               
               
               
               
               
               
               
    

table    test error of algorithms on a normalized    sample test set  data was normalized to have zero mean and standard deviation of
one 

hambo vs hornpipe
hambo vs jig
hambo vs pols
hambo vs reel
hornpipe vs jig
hornpipe vs pols
hornpipe vs reel
jig vs pols
jig vs reel
pols vs reel
average accuracy

svm  linear 
    
    
    
    
    
    
    
    
    
    
    

svm  rbf 
    
    
    
    
    
    
    
    
    
    
    

k means
    
    
    
    
    
    
    
    
    
    
    

 

logistic regression
    
    
    
    
    
    
    
    
    
    
    

nave bayes
    
    
    
    
    
    
    
    
    
    
    

gradient descent
    
    
    
    
    
    
    
    
    
    
    

fi