acoustic identification of cardiomyocytes
alex lemon
 

introduction

pluripotent stem cells can be made to differentiate into cardiomyocytes  heart muscle cells   however 
there are three different types of cardiomyocytes  atrial  ventricular  and nodal  and it is difficult to force
cells to differentiate into a particular type  in therapeutic applications it is extremely important to use the
correct type of cardiomyocytes  implanting the wrong type of cell will not restore function as desired 
and may lead to cancer  thus  it is extremely important to be able to accurately identify different types
of cardiomyocytes  moreover  the identification process must be minimally invasive if the cells are to be
subsequently used for therapy  in this paper we use noninvasive acoustic measurements of heart muscle
cells to classify the cells as atrial  ventricular  or nodal  we apply several classification methods to the data 
including k nearest neighbors  multinomial regression  discriminant analysis  support vector machines 
and tree based classifiers 

 

related work

junyi et al showed that the different types of cardiomyocytes can be identified using the electrical signals associated with action potentials      these measurements are obtained using an invasive technique
called patch clamping  which renders the cells unfit for subsequent therapeutic use  more recently a team
of researchers at stanford has been using a photonic crystal hydrophone to take acoustic measurements
of murine cardiomyocytes   this work is currently unpublished  although technical details of the measurement device are available       catherine jan  a doctoral student in the department of electrical engineering  stanford university  and sally kim  a postdoctoral researcher in the department of psychiatry 
stanford university  provided their acoustic measurements for our analysis 

 

data and preprocessing

the data set consists of thirty four time traces ranging in length from thirty seconds to two minutes 
the original data was sampled at    khz  while the features of interest occur in a frequency range that is an
order of magnitude slower  i used   downsampling with averaging to reduce the size of the data  while
preserving the features of interest 
after downsampling the data  i developed a method for extracting the individual pulses  this method
was based on three assumptions about the nature of the data 
    each pulse can be approximated as a triangle wave of width w      ms  we let t denote the triangle
wave that starts at time t  see figure    
    the pulses corresponding to different cells are added to give the observed signal   in particular 
different pulses cannot cancel each other out  
    pulses begin at relatively few of the time sample points 

 

t

t w

figure    a triangle wave
based on these assumptions  we propose extracting the individual pulses by solving the following optimization problem 
p
 
minimize   kx     k    kk 


subject to       

where      is a regularization parameter  intuitively  we approximate the time trace as a weighted sum
of triangle waves  the first term in the objective is the approximation error  which we want to be small 
 

fithe penalty term kk  is used to promote sparsity  corresponding to the assumption that pulses start at
relatively few of the time sample points  the constraint     models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells  and there is no cancellation  thus 
the optimization problem above is a form of sparse  nonnegative regression  an example of the results are
shown in figure    where we see that the method is effective at extracting the pulses 
    
   

signal  x t 

    
   
    
 
    
   

 

   

   

   

   

 

time  t

figure    approximating the measured signal using triangle waves
having extracted the pulses from the measurements  we manually labeled      observations  representative examples of the pulses corresponding to the three different types of cells are given in figure    we
labeled all of the observations twice in order to assess the quality of our labels  the two labels differed for
approximately one quarter of the examples  indicating that the problem is difficult  and suggesting that an
error rate of about      should be considered a success 

 a  atrial

 b  nodal

 c  ventricular

figure    pulses corresponding to the three different types of cardiomyocytes

 

classification methods

the following classification methods were applied to the data  more detailed explanations of these
methods are given in james et al     and hastie et al     
 a  k nearest neighbors  in this method we classify an observation with predictor vector x  by identifying
the k training examples whose predictor vectors are closest to x    and using a majority vote among
these neighboring training examples   we used the euclidean distance to measure closeness  but it is
also possible to use other metrics   the parameter k must be chosen to balance bias  which increases
with k  and variance  which decreases with k  
 b  multinomial logistic regression  an extension of the usual two class logistic regression  k class multinomial logistic regression assumes that the conditional class probabilities are
exp  t x 
 
pk 
    k   exp kt x 
 
p y   k   x   
 
pk 
    k   exp kt x 
p y   k   x   

 

k            k    

fiwhere             k  are parameters  which we can fit using the method of maximum likelihood 
 c  support vector machine  svm   a support vector machine partitions the predictor space in order to
maximize the distance of the observations from the decision boundaries  while keeping the classification errors small  more concretely  we fit an svm by solving the following optimization problem 
maximize   m
j    i   m

subject to  

pp
     
pp j   j
yi      j   j xj    m     i  
pn i   
i   i  c 

important considerations for svms are the kernel  that is  the measure of similarity between predictors  and the cost parameter c  in addition  because our classification problem has more than two
classes  we need to decide whether to use one versus one comparisons or one versus all comparisons   in a one versus one comparison  we fit an svm for all pairs of classes  and predict the class
that appeared most in the pairwise classifications  in a one versus all comparison  we fit an svm for
each class against all of the other classes  and predict the class with the highest confidence   we used
the standard radial kernel and one versus alls comparisons in this project 
 d  discriminant analysis  in discriminant analysis we assume that the predictors from each class come
from a multivariate normal distribution  with possibly different parameters for each class  we estimate the distribution parameters using their sample analogs  that is  the sample mean and sample
variance   and we classify a new observation to the class with the highest probability density at the
observed value of the predictor vector 
 i  linear discriminant analysis  lda   if we assume that the variance matrix is the same for all classes 
then we obtain linear decision boundaries 
 ii  quadratic discriminant analysis  qda   if we assume that the variance matrix is different for different classes  then we obtain quadratic decision boundaries  we tend to obtain better results
with qda than lda if the variance matrix differs substantially across classes  and worse results
otherwise 
 e  tree based methods 
 i  classification trees  in a classification tree  we recursively partition the training set using binary
decisions based on the predictors  in order to classify a new observation  we apply the same
sequence of binary decisions to the predictors of the new observation  and label the observation
using majority vote when we reach a terminal node  trees are very prone to overfitting  which
can be ameliorated by tuning the number of terminal nodes 
 ii  random forests  a random forest is an ensemble of classification trees  the trees in the ensemble
are decorrelated by bootstrapping the data used to generate the trees  and randomly selecting
the predictors that can be used for decisions at each node  classification is performed using a
majority vote of the trees in the ensemble  important parameters are the number of trees in the
forest  and the number of predictors selected at each node 
 iii  boosting  like random forests  boosting also uses an ensemble of trees  however  instead of
producing a decorrelated ensemble  boosting instead uses subsequent trees to fit the errors in
previous trees  the key parameters are the number of trees and the learning rate 

 

results

all algorithm parameters were chosen using ten fold cross validation repeated ten times  that is  with
ten different random groupings for cross validation  repeating cross validation gives better estimates of the
error  and decreases the standard error of these estimates   in particular  we chose the number of neighbors
for k nearest neighbors  the number of terminal nodes in the classification tree  the number of predictors

 

fichosen for each node in the random forest  and the number of trees in the boosting ensemble  the crossvalidation curves are shown in figure    the vertical red lines in these plots mark the minimum error  while
the horizontal red lines are one standard error above the minimum  the final fitted models were chosen
using the one standard error rule  that is  we selected the simplest model that was within one standard
error of the minimum cross validation error 
performance metrics for the different classification algorithms are given in table    many of the methods exhibit substantial overfitting  that is  a training error much lower than the cross validation error   i
attempted to reduce overfitting by using the one standard error rule to select less complex models  and this
seemed particularly effective for k nearest neighbors  where we see a u shaped cross validation curve 
nevertheless  substantial overfitting remains  particularly for the random forest  i think much of this overfitting can be attributed to the noisy labels in the data  as mentioned above  i labeled all of the data twice 
and the labels were inconsistent for about one quarter of the observations  with such a relatively low
signal to noise ratio in the training data  overfitting is very difficult to avoid 
several algorithms came close to the      error rate that is a lower bound on the accuracy due to the
noisy labels  in particular  the random forest achieved an error rate of       the multinomial logistic
regression did not perform well  indicating that linear decision boundaries are probably not appropriate 
both forms of discriminant analysis performed especially poorly  which is likely a result of the fact that
the assumptions of these generative models are not even approximately correct   recall that these models
assume that the predictor vector for each class has a multivariate normal distribution  there is no reason to
think our data satisfy this assumption  
knn

tree















    

crossvalidation misclassification error









    

    
    



    

    



    

crossvalidation misclassification error

    






























    





 

  





  

  

  

 

 

 

 

k





    



  

  

  

tree size

 a  knn

 b  tree










     











     







































     

     





     



     

     

     

crossvalidation misclassification error



     

     



     

crossvalidation misclassification error

     

random forest







 

  

  



  

  

   

m

   

   

n trees

 c  random forest

 d  boosted tree

figure    cv curves for choosing algorithm parameters

 

conclusion

we used acoustic measurements to classify cardiomyocytes as atrial  nodal  or ventricular  such a noninvasive classification procedure is essential for practical cardiac stem cell therapy  the classification problem
is very difficult  a human only managed to achieve an error rate of about       we obtained an error rate
 

fitraining
algorithm

error

confusion matrix
a
a

knn

      

n
v



   
    
  
a

a

multinomial

      

n
v



   
    
  
a

a

svm

      

n
v



   
    
  
a

a

lda

      

n
v



   
    
  
a

a

qda

      

n
v



   
   
  
a

a

tree

      

n
v



   
    
  
a

a

random forest

      

n
v



   
  
 
a

a

boosted tree

      

cross validation

n
v



   
    
  

n

  
   
  
n

  
   
  
n

  
   
  
n

  
   
  
n

   
   
   
n

  
  
  
n

 
   
 
n

  
   
  

error

v

  
   
   

confusion matrix
a

      



v

     
      
    

a



a
n

v

a


  
   
   

      

v

     
      
    

a



n

v

a


  
   
   

      

v

     
      
    

a



n

v

a



  
   
   

      

      

v

a
n
v

                
                 
                

a



n

v

     
      
    

a



n

a

      

v

     
      
    

a



n

a

v


  
   
   

      

v



v


 
  
   

n

a

a

      

n
v

         
           
          

v

n

v


  
   
   

n
v

         
           
          


               
                  
               

v


  
   
   

n
v

          
           
          

n
v

n

    
     
    
n

    
     
    
n

          
            
         

v

    
     
     
v

    
     
     
v

    
     
     

table    performance of classification algorithms
of      using a random forest  and also had success with k nearest neighbors  support vector machines 
classification trees  and boosted trees  multinomial regression and discriminant analysis performed poorly 
the largest source of error for most methods was classifying nodal cells as atrial 
the most important step for future work is obtaining more accurate training data  the labels used
in this project were not assigned by a biologist  if a domain expert were to assign the labels  then we
could obtain more accurate training data  which should reduce the overfitting problems that we observed 
many of the methods can probably be improved by further tuning  for example  we did not use crossvalidation to adjust the cost parameter in the svm  or the number of trees in the boosting algorithm  it may
also be worth investigating specific methods for differentiating between these two types of cells  perhaps
adding special features that can identify this difference  finally  another interesting direction of research
is using multiple sensors simultaneously  if we have one sensor for each clump of cells  then we can use
independent components analysis  ica  to separate the signals corresponding to the different clumps 

 

fireferences
    trevor hastie  robert tibshirani  and jerome friedman  the elements of statistical learning  springer   
edition       
    gareth james  daniela witten  trevor hastie  and robert tibshirani  an introduction to statistical learning with applications in r  springer       
    catherine jan  wonuk jo  michel
 j  f  digonnet  and olav solgaard  photonic crystal based fiber hydrophone with sub     pa  hz pressure resolution  ieee photonics technology letters            
          
    junyi ma  liang guo  steve j  fiene  blake d  anson  james a  thomson  timothy j  kamp  kyle l 
kolaja  bradley j  swanson  and craig t  january  high purity human induced pluripotent stem cellderived cardiomyocytes  electrophysiological properties of action potentials and ionic currents  american journal of physiology  hearty and circulatory physiology         h      h           

 

fi