cs     fall     
final project report

stanford university

 

relevance analyses and automatic
categorization of wikipedia articles
george pakapol supaniratisai  pakapark bhumiwat  chayakorn pongsiri
december         

  
  
abstract
we used a unigram model for an article with each word represented by a vector in high dimensional space that captures its
linguistic context  the word to vector model was learned through a neural network using a skip gram model  k means
algorithm was applied on word vectors of each article to get clusters of word  which were then used to find similarity index
based on the locations and weights of clusters  given a set of article pairs  we found a linear correlation between similarity index
outputted from the algorithm and humans ratings on similarity  we then applied hierarchical clustering on a group of articles
based on their similarity indices and construct a categorization binary tree  we evaluated the tree by asking humans to play
odd one out games  given an instance of a triplet of articles  choosing one article that is the most different  and we found that
the tree correctly classified approximately     of the odd one out instance compared to the data from humans 
keywords  word vector  word vec  skipgram model  softmax regression  k means clustering  hierarchical clustering
natural language processing  dichotomic analysis  unigram model  similarity analysis  relevance analysis

introduction and background
the main challenge of semantic level analyses of
articles ranges from the sparsity problem  the fact that
appearance of each english words are not frequent
enough  to the knowledge representation of semantics  the
fact the computer cannot directly detect the similarity
between words other than identical string tokens  we
tackle the problem by representing each word as a highdimensional vector  where we can directly calculate
semantic similarity between words  not only this approach
allow us to realize the similarity of two articles with few
overlapping words  but it also mitigates the sparsity
problem arose in the extremely short prompt  such as
sentences or questions  
another challenge concerns how to represent an
article  one way to accomplish this is to look how frequent
each word appears in the articles  yet  some words are less
meaningful than others and do not contribute to the
relevance of the article to the others  in this project  we
implemented an algorithm to filter out these words and
perform a sampling on the rest and incorporate a word tovector model to represent each article with centroids and
weights based on word vectors 
data set
we primarily use wikipedia articles as a source of
data  for each article  we represent each article by a sparse
vector of a unigram model   that is  a mapping of each
english word appearing in the article to the frequency

count   we use the python package wikipedia to retrieve
the articles in a small scale on line with the analysis  then
we filter the frequent functional words  approximately top
      words in english language  for hierarchical
clustering  our data is retrieved from the resource  that
google releases for creating the vector representation of
words on the latest research tool tensorflowtm 
algorithms
i  word vec
we apply the gensim library  which internally applies
the deep learning algorithm  especially hierarchical
softmax skip gram algorithm   this library uses the similar
algorithm to tensorflowtm   however  gensim library has
the better interface for training and storing data   we use
this algorithm because the hierarchical softmax algorithm
can deal with the great number of inputs with the small
computational complexity per training instances  says 
o log v  where v is the number of the word output
vectors  since gensim library requires having an input as a
matrix of words where each row represents a sentence and
each column represents a word in the sentences  we apply
the simplified model by separating our training data into a
sentence of exactly    words so that we have     million
training sets  after passing the training set to the gensim
function  we receive an output as the     th dimensional

                                                                                                                
  http   mattmahoney net dc text  zip  

fics     fall     
final project report

stanford university

 

vector representation of each word and subsequently save
such data in the separate file 
ii  frequent function words filtering
we utilized two methods to filter frequent words  the
first method is simply to delete the frequent function
words  e g  the  is  not        deleting top    function words
reduces the noises from frequent words into a workable
level  another algorithm is that we can gather several
articles  i e        and pick a word that appear more than
n times in r  of articles  typically  we choose n     and r  
   
iii  finding hotspots in word vector space by kmeans clustering
after filtering function words from the article  we
apply k means algorithm to group words that have similar
meanings into clusters  we set up to have exactly eight
clusters because the smaller number of clusters is likely to
pool irrelevant words together and yield a poor result 
while the larger number of clusters will make the program
run slower without yielding any significantly better result
 due to the more empty cluster   every loop will update the
centroid of each cluster to the mean of the vector
representation of all words in the cluster  in addition to
clustering words  we assign a weight for each cluster by
calculating the mean of the word count for each word in
the cluster  in other words  since we can ensure that the
same word will definitely be clustered in the same group 
the weight of the cluster is the sum of the square of the
word counts for each distinct word in the cluster and
divided by the total word count as shown in the figure 

figure    displaying modification of k means weighted with
number of appearances of each word
iv  similarity index
for simplicity  we define the following variables 
     a ith     dimensional centroid for cluster a
     a ith     dimensional centroid for cluster b
     a ith weight for cluster a
     a ith weight for cluster b
     a heuristic computing the similarity from article a
toward article b

 

     

   max          
   

where                           
in order to evaluate the degree of similarity between
two articles a and b  we apply a similarity index heuristic
computing an average of h a  b  and h b  a   where h a 
b  represents the summation of the weight for each cluster
multiplied by the dot product between the centroid of such
cluster in article a to the closest centroid from article b 
i e  
similarity  index  
    

j     kj    

 
       max         

l

         
 

 
       max         

    

hierarchical clustering
after obtaining similarity indices  we applied
hierarchical clustering to categorize articles  hierarchical
clustering starts with assigning each article to a cluster  at
first  each article therefore would belong to its own cluster 
at any step in time  we seek to combine any two clusters
with highest similarity  the similarity between two clusters
is calculated from the average similarity index of all pair of
articles across clusters  merging recursively  we will finally
get one list  and we will be able to trace back to construct a
binary tree  as we call a categorization binary tree  to
demonstrate merging steps 
odd one out for categorizing 
odd one out  o   for categorizing has two primary
subroutines  the first subroutine is to perform an analysis
to pick the oddest article among any three articles  the
second subroutine is to construct an optimal  or near
optimal  dichotomous tree from o  output of the first
subroutine 
i a  oddity
from each triplet of articles  we seek to choose the
oddest one  first  let     l   o be the sparse vector of the
three articles  we compute the dot product between each
pair of articles  out of the three pairs  we reason that the
largest value means the two articles are similar  and the
other one must be the odd one  however  since the dot
product of the three articles can be close to one another 
we cannot be      confident that we can actually odd out
one articles from the three  as such  we weight our
decision by oddity defined as
 l
oddity   log
 o  lo
if similarity between article   and article   is the maximum
similarity  and vice versa for   other cases  we conclude
our routine with the pseudocode below 

                                                                                                                
   some parts of this category is used jointly in cs    
specifically greedy divide  

fics     fall     
final project report

stanford university

oddoneout n   n   n   
similarity      n    n 
similarity      n    n 
similarity      n    n 
oddarticle    the one not in max dot product
oddity    log         as defined
return  oddarticle  oddity 
for n   n   n  in sparsevectorsofarticles 
dataset add   n  n  n  oddoneout n  n  n     
  

 note  this part can be replaced by other method of
similarity analysis  for example  we may use similarity
obtained from word vectors  or the heuristics obtained
from k means clustering in hierarchical clustering  see
algorithm ii  as well  
i b  recursive dichotomic division
from the dataset  we want to divide the articles into
the dichotomous tree tuv so that the reward is
maximized  where our reward is defined by the sum of the
oddity of each constraint that are satisfied  a constraint is
satisfied if the odded out article gets divided before  i e  at
the higher hierarchy than  the other two articles  
    

oddity    data    
  v   v

tuv   arg max   
 

to do so  we can perform a recursive division of the
articles into two sets  i e  assign number   or   to each
articles  until we reach one or two articles  if we reach the
case of two articles  it is easy to divide the set of two articles
to two sets of one article each  that is  we only need to
construct a systematic way to divide a set of articles that
best satisfy the constraints 
suppose we have a set of articles  we can proceed with
a greedy assumption  which we know is suboptimal at
times  that we divide it so that the reward is maximized in
each iteration and division  the reward for each tree node

results
similarity index between two wikipedia articles
we randomly selected pairs of wikipedia articles from the
pool and using the algorithm described in the previous section
to evaluate the similarity index  we found that over    pairs of
articles  the index ranges from   to     we also asked   people
to rate similarity of these pairs of articles on the scale of   to   
the plot between humans ratings and the similarity index
 computers ratings  is shown in figure   below  we found a
linear relationship between the two ratings with a correlation 
of     
figure     comparison of human raing of relevance and
computer rating of relevance

 

on a set of constraints  data set  is defined by the sum of
oddity of each token that is correctly divided  that is 
     

oddity         l    t    
  v   v

where    denotes the     assignment of article    
to achieve the maximum reward for each tree node 
we run the following iteration  starting  from random
assignment  we modify  at most  one assignment for each
token with chance     to comply with those constraints 
only if the modification will yield a better reward 
greedydivide listofarticles  dataset  
randomly assign     to each article
iterate until convergence 
for each token  n  n  n  odd oddity  in dataset 
if assignment agrees or random is below     
continue
else 
modify only one assignment to comply
check if it yields more reward  continue 
else  revert

note that in all cases  we can modify at most  
assignment to comply with one constraint 
the idea is that the process of the iteration will
monotonically increase the reward  we add the element of
probability to prevent the large error in case the algorithm
greedily modifies the assignment as it iterates through the
data  and block any further changes  we also define the
convergence as when the assignment is not modified for     times of whole iteration  also note that although this
iteration method generally yields a high percentage of
reward compared to the limit  i e  summation of oddities
in the dataset   it does not guarantee the maximum
attainable reward 
we perform the division for each node of tree until we
reach one or two articles  which can easily resolve to a leaf
node

fics     fall     
final project report

stanford university

 

hierarchical clustering
we generated   sets of articles  each containing    different articles  for each set  given similarity indices of all article pairs 
we applied hierarchical clustering to construct categorization binary trees as shown in figure   below 

figure     dichotomous tree constructed using hierarchical clustering with word vectors
for a comparison purpose  given the same lists of articles  trees that are constructed by odd one out algorithm are shown in
figure   below 

figure     dichotomous tree constructed using odd one out instances
to evaluate the performance of the algorithms to construct categorization binary trees  we collected an odd one out
test set from   people  an odd one out instance was presented to a person  and a person  with his or her best judgment  would

fics     fall     
final project report

stanford university

 

choose an article that is the most different from the rest  we say a tree correctly handle an odd one out instance correctly when
the article that was chosen to be odded out by a human is the same as the article that is the farthest from the rest two in the
tree  note that the odd one out instance we use to evaluate the performance here are randomly selected and different and nonoverlapping with the instances we used to train the model for the odd one out algorithm 
finally  we found that the trees from hierarchical clustering and odd one out algorithm correctly classified      
and        of the instances respectively 
by weigh of the modified k mean algorithm and similarity
discussion
index  which is calculated from the weighted dot product
we found that similarity index obtained from our
of the centroid of each cluster from different articles  this
algorithm does a fine job capturing the relevance between
algorithm performs a high efficiency  however  it has a big
two wikipedia articles  the use of word embedding allows
runtime 
us to realize the similarity between two articles that have
the odd one out algorithm constructs a dichotomous
few overlapping word  which otherwise could not be
tree by weigh on the reward for each node  which we did
accomplished by a model that simply uses a unigram or
not utilize in this project  but has a potential to be useful
bigram word counts as a feature  for example   soy sauce 
with tagging of each sub tree  since it is possible optimize
sesame oil  yields high similarity index because they both
the dichotomous tree and greedy dividing separately  one
are ingredients for a similar type of food  but if we simply
may obtain a larger data set of human odd one out
use a unigram word count feature vector and find a cosine
instance response  and use it to pick the best variation of
similarity  we will get a low similarity 
the model   e g   we can penalize any division that leads to
the categorization trees from hierarchical clustering
incorrectness  to allow the article triplets that have less
correctly classify approximately     of the odd one out
clear     division to be handled in the lower nodes 
instances  slightly more accurate than the odd one out
to sum up  our algorithms added a possibility to
algorithm  hierarchical clustering  however  is slow
efficiently analyze the texts without the need to account for
because we need to find similarity indices between all pairs
the complex language features  e g  syntactic and
semantic structures   with our algorithm  we can compare
of articles in the interested pool  which runs in   l  
the similarity between any pairs of articles with any length
time  one observation is that hierarchical clustering tends
and construct the dichotomous tree model for any group of
to construct trees that are less balanced than those from
wikipedia articles  which is a basis for further auto tagging
odd one out algorithm
each article by determining the most relevant words from
the word bag for each node 
conclusion and future work
we perform two algorithms to analyze the similarity
between two articles and construct a dichotomous tree
model  in which they have tradeoffs in different aspects 
the hierarchical clustering constructs a dichotomous tree
works cited
sojka  petr   software framework for topic modelling with large corpora   proceedings of the lrec     
workshop on new challenges for nlp frameworks  by radim eh ek  valletta  malta  elra               print 
 assessing relevance christopher d  manning  prabhakar raghavan and hinrich schtze  introduction to
information retrieval  cambridge university press      
tomas mikolov  kai chen  greg corrado  and jeffrey dean  efficient estimation of word representations in
vector space  in proceedings of workshop at iclr       
tomas mikolov  ilya sutskever  kai chen  greg corrado  and jeffrey dean  distributed representations of words
and phrases and their compositionality  in proceedings of nips       
vector representations of words  tensorflow org google inc      

fi