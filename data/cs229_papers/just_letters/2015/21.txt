cs     final report
a study of ensemble methods in machine learning
kwhangho kim  jeha yang
abstract
the idea of ensemble methodology is to build a predictive model by integrating multiple models  it is
well known that ensemble methods can be used for improving prediction performance  in this project we
provide novel methods of combining multiple learners in classification tasks  we also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods 

 

introduction

an ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way  typically
by weighted or unweighted voting  to classify new examples with the goal of improving accuracy and reliability 
as combining diverse  independent opinions in human decision making to pursue a more protective mechanism 
the ensemble model provides an efficient way to improve accuracy and robustness over single model methods 
ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms 
in this project  we focus on classifier ensembles and propose novel algorithms of combining method  our methods
provide very simple yet effective way to determine optimal weight of each classifier in a sense that it seeks not
only empirical risk minimization but diversification of classifiers 
for application  we analyze napa valley wine quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method  our methods also show superior performance than
simple voting method 

 

related work

in the recent years  experimental studies conducted by the machine learning community show that combining the
outputs of multiple classifiers reduces the generalization error  quinlan        opitz and maclin        kuncheva
et al         rokach         ensemble methods are very effective  mainly due to the phenomenon that various
types of classifiers have different inductive biases  geman et al         mitchell         indeed  ensemble methods
can effectively make use of such diversity to reduce the variance error  tumer and ghosh        ali and pazzani 
      without increasing the bias error  in certain situations  an ensemble can also reduce bias error  as shown
by the theory of large margin classifiers  bartlett and shawe taylor        
given the potential usefulness of ensemble methods  it is not surprising that a vast number of methods is now
available to researchers and practitioners  there are several factors that differentiate between the various ensembles methods  rokach        summarizes four factors as below 
  
inter classifiers relationship   how does each classifier affect the other classifiers
  
combining method   the strategy of combining the classifiers
  
diversity generator   how should we produce some sort of diversity between the classifiers
  
ensemble size   the number of classifiers in the ensemble
in this project  we particularly focus on the combining method  ali and pazzani        have compared several
combination methods  more theoretical analysis has been developed for estimating the classification improvement
by tumer and ghosh        
 

fics     final report

 

kwhangho kim  jeha yang

application   napa valley wine quality score data

to test whether our proposed ensemble algorithms really work and to see how much improvement can be made 
we conduct an empirical study  for this empirical study we use napa valley wine quality score data    

   

purpose

a wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critics evaluation of that wine  this wine quality score is assigned after the wines have been released to the
market  since it directly affects the price and demand of the wine  for the wine companies predicting the wine
quality score is very important 
we want to predict the wine quality score with given soil conditions of the year in which the wine was produced 

   

data description





min 
median
mean
max 

we have       complete  data points 
wine quality score is integer valued  and ranges from   to   
there are    predictors   x  labeling information   color  fixed acidity  volatile acidity  citric acid 
residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density  ph  sulphates  alcohol
the training data is imbalanced   that is  there are major         labels that appear most of the time
and minor           labels 
x

color

 
    
    
    

red 
    
while 
    

fixed
acidity
   
   
   
    

volatile
acidity
    
    
    
    

citric
acid
 
    
    
    

 a  histogram of wine quality score

residual
sugar
   
   
   
    

cl
    
    
    
    

free
so 
 
  
  
   

 b  correlation matrix

total
so 
 
   
   
   

density

ph

so   

alcohol

     
     
     
     

    
    
    
    

    
    
    
    

   
    
    
    

 c  correlation matrix

figure    basic data description

 

ensemble methodology

   

single model methods and basic analysis

we fit the following prediction models to our training data set  
   ordinary least square  ols  regression
   robust linear regression   m estimation rlm   least trimmed squares  lts 
   weighted least square  wls  regression   weights   reciprocal of the square root of the size of each score
   l  regression   backfitting   wls  l r 
  this

dataset is given by prof  t  hastie and originally used in stats    a class  in this simulation  to keep confidentiality of
the original data set  we only use a part of training set of the original dataset 

 

fics     final report

kwhangho kim  jeha yang

   variable selection   forward selection  fwd   backward selection  bwd   all subset selection  all 
   penalized least squares   ridge  lasso  elastic net
   principle component regression  pcr 
   partial least squares  pls  regression
   basis expansions and regularization   natural cubic splines with lasso  ncs 
    local polynomial regression  lpr  with selected variables from  
    support vector machine  svm    linear kernel  polynomial kernel  radial kernel
    linear discriminant analysis  lda 
    logistic regression  log 
    k nearest neighbor  k nn 
note the following details of these methods  

regression models  from   to     give real valued scores  which are then rounded off to be integers 

in the wls regression model  weights are chosen to avoid the phenomenon that a model mostly
predicts major scores due to not predictors but its criterion 

svm with k labels is the one against one approach  in which k k      binary classifiers are trained
by svm with   labels and majority vote determines the label of a data point 
we conducted   fold cvs for different learning methods to estimate the generalization errors  using      data
points randomly chosen from the training data set the remaining     data points is set to be the test set  
table    rmses for different learning methods

cv
test

l r
     
     

elanet
     
     

pcr
     
     

ncs
     
     

lpr
     
     

svm
     
     

lda
     
     

log
     
     

note that   methods were chosen to be representatives of redundant methods and independent of each other 
also  svm denotes svm with the radial kernel from now on 
from table    we can see that all methods show mediocre performance  although classification models are slightly
better than regressions  if we can pick up models which are good to predict the major labels and the others which
is good to predict the minor labels and ensemble them  it may contribute to the overall performance improvement 

   

ensemble algorithms

first we construct a simple heuristic ensemble classifier using the idea described above  lets look at the per
class score  misclassification rates for typical methodologies selected above 
table    per class misclassification rates
score
 
 
 
 
 
 
 

l r
   
    
    
    
    
   
   

elanet
   
    
    
    
    
   
   

pcr
   
    
    
    
    
   
   

ncs
    
   
    
    
    
   
   

lpr
   
    
    
    
    
   
   

svm
   
    
    
    
    
    
   

lda
    
    
    
    
    
    
   

log
   
    
    
    
    
    
   

from this table  we can observe that

svm outperforms the others for major classes  and it is the only classifier which successfully distinguishes some of the class   data points 

lda classifier performs much better job for class   and   than regressions and svms did 
based on these observations  we decided to combine three classifiers svm  lda  and elastic net  which is included
because it may have predictive power related to regression settings  although being dominated by svm  in the
 

fics     final report

kwhangho kim  jeha yang

way that strengthens the strengths and makes up for the weaknesses   that is  put the weights decreasing in the
order of svm  lda  and elastic net  when choosing weights under this scheme  we did several experiments with
different combinations of the weights and empirically chose the best model among them without using any formal
optimization over the training data  the prediction rule obtained from our heuristics is as follows  
algorithm     ensemble method based on simple heuristics  heuristic 
prediction       svm        lda        elanet
next  we discuss a reasonable process to find the optimal weight vector more systematically  suppose that we
have a finite set of classes          c  and learning methods l         lm for classifying these classes  using the
given predictors and data set   and use k fold cv  let n denote the number of observations so with the k fold
pk
cv it follows that n   k   nk   where in our data nk       for all k           k  for each  k  m   let y  k  and
y  k m  denote the realized value of classes for k th cv fold and the predicted value of classes for k th cv fold
based on the learning method lm trained on the rest of the data  i e  trained on the data points taken out for
the k th cv fold  respectively  finally let the matrix x  k  denote y  k    y  k       y  k m     then we obtain a
nk    vector y  k    and nk  m matrix x  k  for our analysis  now consider the following algorithm 
algorithm     ensemble method based on l  minimization of cv errors  l cv 
with given constructions  x  k   k and  y  k   k   we find the m    optimal ensemble weight vector   as
k
x

argmin
rm

  y  k   x  k         argmin  t
rm

k  

k
x

t

x  k  x  k     

k  

k
x

t

y  k  x  k   s t   t          

k  

it can easily be shown that algorithm   has lower cv rmse  without rounding predictions  which is necessary
for the comparison and expected to have a little effect on cv rmse  than any of single learning methods  by
letting  be columns of the m  m identity matrix  from this algorithm  we have the following ensemble 
prediction         ns         lpr         svm         lda
now recall that we used misclassification rates per class to choose learning methods and give them weights 
similarly  we could instead use squared errors per class ssepc   to be consistent with rmse  here we propose
  automatic ensemble methods based only on ssepc  the first one is a modification of algorithm   
algorithm     simple ssepc
qpensemble  ssepc 
 k m 
 k 
 k 
 k 
 y
 yi    and s  k      scm   cc  mm  rcm   simple
for each  k  m  c   let scm   
 k 
i y
 c i
pm i 
ssepc ensemble is defined by m   m mm   where  is the solution of the quadratic programming problem
min t

rm

k
x

s  k t s  k   s t        t     

k  

this optimizes an upper bound of the cv rmse without rounding   and has the same property as algorithm    
the cv rmse of the simple ssepc ensemble is smaller than those of l         lm  
to see these  lets consider the k th cv fold  and omit superscripts k and k from
pmys for simplicity  we can then
obtain an upper bound of sse  from the k th cv fold  of an ensemble method m   m lm as follows  
ssek   

m
m
x x
x x
 m 
 m 
 
 
m yi  yi     
 
m
 yi  yi       
i

 

m  

m  

c x
m
x
x  m 
 
 
m
 yi  yi       
c   m  



i

i yi  c

c x
m
x
   k   
scm    
 
m
c   m  

 l 

x

l m

 k 

l m scl s k 
cm    

x

 

 m 

 yi  yi   yi

 yi   

i yi  c
c x
m
x
 
 k 
 
m s k 
k  
cm     ks
c   m  

 l mm

 l 

 m 

l m  yi  yi   yi

 l mm

 l mm

x

x

 yi   

fics     final report

kwhangho kim  jeha yang

where the inequality is due to the cauchy schwarz inequality  which makes
the upper bound
of
pk
pk a function
 k 
 
s  k    summing up these upper bounds for k           k gives sse   
sse

ks
k
 
k
 
k  
k  
pk
pk
pk pc  k   
t
 k t  k 
 k 
 

s          furthurmore  note that
k   
k   s
k   ks
k  
c   scm is the sse of lm
pk
when m      l     l    m  i e   t k   s  k t s  k     sses of l         lm           combining     and
    results in the claim above  applying algorithm   to our data resulted in the following sparse ensemble
prediction         lpr         svm
finally  we revisited the first heuristic idea and came up with the following algorithm 
algorithm     forward stepwise ssepc ensemble  fwd ssepc 
let r be the c  m matrix of cv sse per class  hence its rows are classes  
    define the best single model as the current model  and update r by removing the corresponding column 
    for each column of r  divide each element by the corresponding cv sse per class of the current model  to
obtain the matrix of those ratios 
    if all of those ratios are larger than    stop   otherwise  find the column with the smallest sum of ratios 
include the corresponding single model to the current set of models  and update the model by using algorithm  
of the new set of models and r by removing the corresponding column 
    repeat   and   until convergence  in terms of the supremum of the change in weight over all single models 
lets discuss the ideas of step   and    first of all  the smaller the ratio of the sse of a class between a single
model and the current model is  the better the single model works in the corresponding class  therefore  summing
up those ratios over all classes could be considered as a measure of how much a single model complements the
current ensemble model  treating all classes equally  furthermore  if those ratios are greater than   for all classes 
than we dont need to include the corresponding single model to the ensemble 
implementing this algorithm gave the ensemble
prediction         svm         lda         l r

 

results and discussion

below are the experiment results of   algorithms we discussed so far 
table    rmses for ensemble methods

cv set
test set

heuristic
     
     

l cv
     
     

ssepc
     
     

fwd ssepc
     
     

by comparing table   with table    it is found that
  
all four ensemble methods perform better than any of single classifiers in terms of rmse 
  
ensemble methods show their strengths particularly when they combine the complementary predictive
powers of multiple models 
  
in particular  we can observe that the background ideas of fwd ssepc worked in the sense that
lda was picked following svm as done in the heuristic ensemble  although the next step actually
increased the rmse from       to       

 

future work




from the results  l cv and fwd ssepc showed similar performances  therefore  we can further
compare them via experiments or theoretical analysis  also  it can be examined if simple ssepc
always guarantees sparse ensemble models  as shown in our experiment 
to improve fwd ssepc  we can try to develop a measure for the third step as a function of the
ratios of sses  other than the one we used before 
 

fics     final report

 

kwhangho kim  jeha yang

reference

   ali k  m   pazzani m  j   error reduction through learning multiple descriptions  machine learning        
              
   bartlett p  and shawe taylor j   generalization performance of support vector machines 
and other pattern classifiers  in advances in kernel methods  support vector learning  mit press  cambridge 
usa       
   opitz  d  and maclin  r   popular ensemble methods  an empirical study  journal of artificial research     
              
   mitchell  t   machine learning  mcgraw hill       
   dietterich t   ensemble methods in machine learning  in j  kittler and f  roll  editors  first internationalworkshop on multiple classifier systems  lecture notes in computer science  pages       springer verlag      
   dietterich  t g   machine learning research  four current directions  ai magazine                    
   rokach  l  and maimon  o   clustering methods  data mining and knowledge discovery handbook  pp 
              springer 
   rokach  l   decomposition methodology for classification tasks  a meta decomposer framework  pattern analysis and applications                 
   rokach  l   ensemble methods in supervised learning  springer        
    tumer  k  and ghosh  j   error correlation and error reduction in ensemble classifiers  connection science
                        
    bennett  k  p   demiriz  a   maclin  r   exploiting unlabeled data in ensemble methods  kdd    proceedings of the eighth acm sigkdd international conference on knowledge discovery and data mining          
    rokach l   genetic algorithm based feature set partitioning for classification problems  pattern recognition 
                     
    geman s   bienenstock  e   and doursat  r   neural networks and the bias variance dilemma  neural computation               
    tan a  c   gilbert d   deville y   multi class protein fold classification using a new ensemble machine
learning approach  genome informatics                  
    quinlan  j  r   c     programs for machine learning  morgan kaufmann  los altos       
    quinlan  j  r   bagging  boosting  and c     in proceedings of the thirteenth national conference on
artificial intelligence  pages               
    sohn s  y   choi  h   ensemble based on data envelopment analysis  ecml meta learning workshop  sep 
        

 

fi