reinforcement learning for adaptive traffic signal control
final project  cs      machine learning   stanford university
jeffrey glick  jdglick stanford edu   m s  candidate  management science   engineering 
   december     
introduction
by       two thirds of the worlds     billion people will live in urban areas      in many cities  opportunities to expand
urban road networks are limited  so existing roads will need to more efficiently accommodate higher volumes of traffic 
consequently  there is a pressing need for technologically viable  low cost solutions that can work with existing infrastructure
to help alleviate increasing traffic congestion 
improving the efficiency of traffic lights is key to reducing traffic congestion 
most static phase timings are chosen to work acceptably well in all conditions
but are rarely optimal  some modern traffic light control systems  often involving
induction loops installed on road surfaces  utilize actuated logic  such systems
allow basic if then rules to be pre programmed by a trained traffic engineer with an
understanding of local traffic dynamics  however  the installation  programming
and maintenance of such systems is costly and time consuming  furthermore 
the inflexible logic in these systems cannot adapt to changing environments and
optimize control policies  the motivation of this study is to design  simulate
and analyze an adaptive traffic signal control system that can out perform the
legacy traffic light control systems described above  this paper details efforts
to to dynamically optimize a standard four way intersection stop light utilizing
reinforcement learning  specifically  the q learning algorithm  
the decision process is outlined in figure    every time the traffic light is
about to enter a new phase  the control agent must first detect the state of the
environment  eg  queue lengths   with this perception of the state  the agent
must select some action  specifically  the agent decides how many seconds the
upcoming phase will run for  at the next decision point  depending on the quality
of new state  where quality is defined quantitative by an objective function   the
figure    rl for adaptive traffic signal
control agent is either punished or rewarded  the control agent initially explores
control
a wide range of states and actions  eventually the agent learns the best action
for any given state and picks the best action with a high likelihood 
related work
there is growing body of work in which employ model less temporal difference reinforcement learning methods like qlearning to a range of traffic optimization problems  two relatively early studies by weiring          provided a framework
for simulating traffic systems using micro simulation software and comparing performance of different reinforcement learning
approaches  mersetivcs      simulation study showed that q learning outperforms static and actuauted logic control systems
      however  instead of using realistic  temporally dynamic traffic conditions  his control agent was trained in relatively
static environments in which the traffic was either saturated and unsaturated  steingrover explores the feasibility of
deploying rl based traffic light control policies in a sub network of    intersections in which decision making is optimized
locally  but a penalty term is utilized for queue spill over that affects an adjacent intersection       q learning performs
well when each local control agent is provided with full  perfect information about the state of the entire sub network and
neighboring decision making processes  several other recent studies                     all create simulation test beds to
investigate whether q learning can be applied to traffic control problems  all of them yield promsing academic results  but
the prospects of deploying these system designs into the real world is uncertain 
a recent literature in this field is provided by mannion       he considers the feasibility of deploying some rl techniques
outside of simulation test beds and into the real world  his discussion raises the following points  first  simulated arrival
rates and traffic dynamics into simulation test beds often lack the volatility and chaos of normal traffic flows  second  he
questions the robustness of the learning pipelines and processes researched  many of the proposed systems would require
significant tuning and supervision to deal with unique nuances of specific intersections 
finally  he notes that many researchers make the implicit assumption that the learning agent has perfect  real time
information about the world around it  providing the learning agent access to a wide feature set is easily done in a simulation environment and certainly useful for comparing performance of different algorithms  in the context of the rl cycle
 figure     efficient learning agents with access to full information can precisely associate each state action state move some

 

fiperceived value  however  when considering the feasibility of deploying an intelligent and adaptive traffic control system  providing the agent access to perfect information would require extensive vehicle to control agent communication in conjuction
with some combination of deployed hardware systems including cameras  radar  induction loops  counters or other physical
instrumentation  these systems are expensive to install  calibrate  operate and maintain 
garcia et  al  provides an thorough overview of implementation challenges associated
with temporal difference methods like q learning      with a focus on how to map an infinite
continuous state spaces to something more manageable  the learning agent needs a framework to generalize its experiences so that it can tractably approximate value functions  the
study suggest that discretizing the state space with vector quantization  i e  k means clustering algorithm  provides a reliable approach for mapping similar continuous states into
discrete buckets  we borrows elements of that discretization approach herein 
simulation setup  data generation   state space definition
this study utilizes a multi agent micro simulation tool to simulate a traffic system involving cars  roads  intersections and traffic lights  open source software sumo  simulation of
figure    sumo
urban mobility  was selected for the study  the software is highly flexible  well documented
and supports control of traffic light signals using the api calls from an external python
script using the traffic control interface  traci  package  it has extensive command land
functionality and a graphic user interface  several other researchers have used sumo for
similar studies including recent work out of google research         
the single intersection of palm and arboretum on stanford campus was selected   
a map of the intersection was first exported from openstreetmap org  next  the
map osm file was edited and cleaned in the java open street map editor 
among other tasks  entry points for vehicle spawning were identified  speed limits
were set  road edges were properly connected at valid junctions and the traffic control
signal was configured    the intersection was purposefully modified from its true form 
using two lanes on all approaches into the intersection and using a common eightphase stop light cycle  the map file was then converted into an  xml file which can
be read by the sumo configuration file 
figure    fixed phase times
next  we had to generate arrivals data for the simulation  based on some observations during both rush hour and off peak travel times  synthetic data was generated
to approximate non homogeneous arrival rates into the system  additionally  the decision of a car to     turn left     go
straight or     turn right for each of the four approaches was approximated  considering observed behavior of the intersection 
polynomial functions were fit to this synthetic data in order estimate time dependent arrival rates of cars arriving at one of
the four spawn points and exiting out of one of the three other edges  figure    
with the assumption that vehicles arrive according to a poisson process
with the mean arrival rate determined by the fitted functions  and some added
randomness   a schedule of arrivals was randomly generated in a routes xml
file for every    hour run of the simulation  the file provides a start location 
start time and end location for vehicles over the entire    hour simulation 
the vehicle will automatically choose the proper lane depending on if it is
turning left  right and will pick either lane if it is going straight  vehicles may
turn left during cycles   and    figure     but do not have the right away 
right turns on red are allowed and all basic rules and courtesies are followed 
realistic acceleration  deceleration lane maneuvering and other behaviors are
also programmed into the simulation 
motivated by a desire to design a system which requires limited  or no 
figure    variable arrival and turn rates
investment in locally deployed physical sensors  this research seeks to design
an end to end learning pipeline and control system which uses limited input
information  we consider the use of data which might realistically be available from smart phone geo location data  being
emitted by passengers in cars traversing the intersection  
so  where i denotes each of the four intersection approach lanes lane  i   n orth  south  east  w est    the state of the
system s  r   is defined by the following elements   k  phase which is about to start  h  hour of the day    qi   the queue
  the

author bikes through this intersection daily and is often frustrated by its sub optimal fixed phase timings 
baseline fixed phase light timing shown in  figure     was selected because on the fact that it minimized the average objective function
compared to other reasonable fixed timing policies
  by including the h as an element in our state s  we make the inherent assumption that traffic dynamics have cyclic temporal dynamics 
 a

 

fisize of each of the four approaches     wi   current cumulative waiting time of all of the stopped cars     nvi   total number of
vehicles in each lane within a fixed radius around the intersection  
methods   experimentation
q learning is an off policy  temporal difference  td  control algorithm  originally developed by watkins in            by
exploring states  actions and rewards  the intent is to learn quality values q s  a  for each state and action which serve
to estimate the true optimal value function  the algorithm continuously updates these q s  a  values in an asynchronous
manner  a mechanism for delayed reinforcement        the conditions for eventual convergence to an optimal q  s  a  is that
each pair  s  a  be visited an infinte number of times and the learning rate  be updated in the manner shown in equation
    below       the one step update of the q learning algorithm is 
h
i
q s  a          s  a  q s  a     s  a  r   maxa  a q s    a   
   
as the first step in our q learning pipeline  we need to map our r   state space to a set of discrete states  we employ
vector quantizer v qh k  i e  k means clustering algorithm  to achieve this      for each hour of the day h and each of the
four light phases where we can take an action k  our quantizer maps the states as follows 
v qh k  s    s  ch k  h  k
where ch k is our full set of centroids for each unique h and k  we next consider the question of how to decide card ch k   
which is the budget of clusters allocatedpfor each h  k pair  following significant experimentation  we developed insight that
  
card ch k   should be proportional to i   var si    why  if the continuous state space for a given h  k pair is highly
variable  i e  rush hours when traffic volume can spike suddenly  then a relatively larger number of clusters should be
assigned in order to ensure that the v qh k can perform a sufficiently precise mapping of similar states  on the other hand 
if the range states is relatively compact in terms of euclidean distance  i e    am   then a smaller number of clusters can
be assigned and perform their intended task with the same level of precision  map similar states to the same discrete state  
so  in order to choose card ch k   and generate a v qh k  s  for each h and k  we executed the following procedure 
for each h  k 
    passively observe the system for   days  record all available state vectors  r   and concatenate the n observations
into a matrix s where each column j represents a dimension of the state 
    let stdev s j be p
the standard deviation for each column j of s 
    let card ch k     j stdev s j   
    run the k means clustering algorithm on s using the selected number of centroids  store the resulting v qh k  s 
function for later use to map state vectors s to some ch k
    given the existence in some  h  k  a future state s will be mapped to a discrete state according to v q s   
argminyc  dist s  y   where dist is the euclidean distance 
next  the learning rate  s  a  used in equation     above is defined as 
 s  a   

     
 
              
n s  a 
     

   

where n s  a  is the number of times that action a has been taken from state s   s  a  controls how we weigh the most
recent experience and the past experiencing when updating a quality value q s  a   early in the learning process when
 s  a       the initialized value of q s  a  is completely wiped out and replaced by the rhs of the update  ref equation
      as n s  a  increases   shrinks and we place relatively less weight on a single new experience in the q s  a    update
process 
the discount factor  is some value          in the context of a vehicle control problem  we want to set  close to one in
order to prevent myopic decision making  a value of        was found to work well  
a wide range of punishment  a k a reward  functions have been used in the literature  with the sum of squared queue
sizes being a popular option  we compute an objective function that incorporates both queue sizes and cumulative waiting
time of stopped cars in the queue  while these two performance indicators tend to be highly  positively  correlated  we do
  queue sizes is the count of stopped cars waiting at the intersection  this is the most important metric from a traffic optimization standpoint
and features prominently in the reward function of related research studies
  waiting time is the most important metric from the passengers point of view
  nv provides the control agent some awareness of number of cars about to arrive to the intersection 
  off policy algorithms can update the estiamted value functions using hypothetical actions where an assumption is made that the best action
will be taken  on policy algorithms only make updates based strictly on experiences    
  card c
h k   is the number of clusters and corresponds to the the nclusters argument in the sklearn cluster kmeans   function in
python 

 

fiwant to have some trade off between the two in order to avoid situations where a single car might be waiting in a   car queue
for an infinite amount of time  our reward function is 
r 

 
x

q  qi  q   w  wi  w

   

i  

selections of of both the leading coefficients and power terms in this function have significant impact on the the eventual
policies arrived at  based on experimentation and observation  we utilized q     and w     in order to bring qi and wi
onto approximately the same scale  using a quadratic term on q is a common in other studies  which encourages a balancing
of queue lengths      both q and w were set to     
we next define the discrete set of actions ak  ak where k denotes the light phase where some decision on an action
needs to be made  k             since the odd numbered k values are the yellow transition phases which are fixed at   seconds
 i 
each  we choose card ak        k and assigned each ak as a discrete integer value between reasonably designated lower
and upper constraint on the time that each phase k can last  for k        we define ak                              seconds  and
for k       we define ak                                     seconds  
 is a control parameter which manages the explore vs  exploit objectives of the q learining process   is typically fixed
at some value  usually between     and    for early stages of the learning process  an   sof t policy is utilized where
the best action  with the highest q s  a  value is chosen with probability
    and the remaining actions are chosen with
p
uniform probability  after some reasonable threshold of experience   n s  a   is gained  a control policy will then be shifted
to   greedy  under this control policy  the action with the highest q s  a  is chosen with probability   the remaining
probability mass is spread over the other options uniformly  consequently  they are only visited occasionally 
as an alternative to the tradition  greedy or  soft control policies  this study utilizes softmax approach by which  s  is
a function of the state s 
fip
fi
fi
fi
 
aa q s  a fi
card a  fi

   
 s    
p
 
n s 
a 
aa
card a 
due to our highly varied values of q s  a   values as low as       at times   this approach was initially utilized to prevent
numerical underflow but there are other advantages  it also dynamically updates as n s  a  grows  codifying a gradual shift
towards a greedier control policy   can be adjusted as a tuning parameter to gradually control the transition from an
exploratory to exploitative policy  this study utilized      
this definition of  s  then serves as the key input to assign probability mass across the set of actions for a given state 
exp q s  a   s  
 aa
aa exp q s  a   s  

ps  a    p

   

p
note that this function guarantees that a ps  a      and actions with a higher
q s  a  will be assigned a relatively higher probability mass  when it comes time to
select an action  random number generation is used to select an action based on this
discrete probability distribution 
results   analysis
our learning pipeline begins with an observation period of a   days  our agent uses
k means clustering in order to discretize the state within each  h  k  pair  the number
of centroids chosen is shown in figure    since variance of the state vector values
are higher during rush hours  a relatively higher proportion of centroid clusters were
utilized   exactly as expected  in total  the procedure produced n        unique
states  we found that this produced a set v q   mapping functions with roughly
equal precision  furthermore  compared to initial naive attempts to discretize the
figure    results of unsupervised
state space discretization via k means
state space uniformly  the distribution of n s  a  occurrences across the      states
was much less sparse  providing some indication that we are effectively utilizing the
allocated discrete states  by the end of day    the average number of visits to each
state was      with a standard deviation of       only    hadnt been visited a single
time   we experimented with different values n  n   some hypothetical centroid budget  and the implications for the
q learning process were not surprising  the greater the number of discrete states  the the longer exploratory period for the
q learning algorithm  but the better the eventual performance  under regime of a very large n   far more  s  a  pairs need to
be visited to build reasonably accurate q s  a  scores  with too small of an n   we improved our mean objective score faster
but performance plateaued at a lower level 

 

fiour primary metrics for assessing the performance of the algorithm on a day over day basis include the mean  median
and min of the objective function  a    day simulation run shows the evolution of these summary statistics in figure    the
baseline values of each summary statistic  computed from the   day observation period of the static policy  are plotted as
horizontal black lines for reference  we can see that the daily mean objective function at first oscillates around the baseline
objective value and then after the second week gradually improves  at the end of day     the mean reaches      which a    
improvement from the baseline performance  the daily median objective value also tracks upward and is about     higher
after day     we also monitor the minimum objective function because we care about queue blow ups and other exceptionally
bad situations over the course of a day  the min daily value initially drops but starts to recover towards the baseline 
these results show that the q learning algorithm does indeed improving average system performance in terms of the objective function  a composite measure
of queue length and waiting time   but at an snails pace  we dont see any
convergence of performance even after    days when each state has been visited
an average of     times  we examine our discrete probability distributions for
actions  at this point in the learning process  for some state s  wed like to be
well into a  s greedy control policy  picking argmaxaa q s  a  a vast majority
of the time  however  at day     an average of     probability weight  median
of     probability weight  was applied to the argmaxaa q s  a   an average
of     probability mass is assigned to the top three options  consequently  we
recognize that we should have utilized a more aggressive value for  s  and could
have increased the value of  to some value between   and   to attempt to speed
up convergence to help get a greedier control policy at this late stage 
by initializing all of the q s  a  pairs at be zero  we ensured that each would
figure    mean  median and min of
be visited at least once since the rewards are all negative  from a bayesian
objective function after    days
standpoint  this was a a flat prior and it likely would have been helpful to initialize
with something other than a flat prior to encourage most of the early exploration within a subset of actions that we believed
would be best for some particular state 
as the algorithm progressed  a weighted moving average of the objective function
was visualized to provide insight about temporal and day over day performance could
be analyzed for any discernible patterns  figure   provides a view about the relatively
noisy nature of the algorithm during early days when the control policy encourages
exploration  later  we would see these moving average curves incrementally shift
upward but with a good deal of stochasticity 
conclusion   future work
this study was carried out to confirm the value of applying reinforcement learning
to improve traffic intersection system performance  we carefully set up a complex
multi agent model with realistic traffic flows  we used an unsupervised method to
reasonably partition an infinite state space and then tasked our learning agent to
figure    moving average of objective learn optimal policies in a in a temporally dynamic environment  this study profunction  first    days 
vides another confirmation that q learning shows promise for solving traffic light
control problems and improving overall system performance compared to fixed policies  however  in spite of the relatively straightforward nature of the algorithm  the
lack of convergence and difficulties with selecting model parameters also reveals the
non trivial challenges of deploying a q learning learning pipeline 
with the theoretical benefits of q learning well established  more work needs to move research out of simulation test beds
and into the field  much of the the industry activity attempting to exactly this are developing business models which call for
the deployment of extensive physical instrumentation at intersections  including sensors  radar  cameras  induction loops and
other equipment   such systems show promise and are attractive from a revenue standpoint  but they are very expensive for
the customer  i e  a city government  to purchase and certainly not affordable for much of the developing world 
we suggest that researchers focus on the desining a system that uses limited information  we might imagine that an
agent only has access to geo location data for some sub set of vehicles which is only near real time  i e  a   or   second
time lag   in such a regime  an agent would first need to learn how to generate estimates about what state the system is in
and develop novel mechanisms for verifying these estimates based on subsequent signals  the output of that first learning
problem would then serve as noisy inputs to the q learning traffic light control process studied above  developing learning
pipelines and designing systems which are low cost  reasonably effective  deployable and require minimal hands on support
from experts should be the focus 

 

fiacknowledgments
michael bennon  managing director at the stanford global projects center helped with problem formulation and has
provided ongoing feedback  cs     students allen huang and jesiska tandy contributed to the early brainstorming and
literature review phases of this project 

references
 pyt  python packages numpy  polyfit   scikitlearn  cluster kmeans  
            world popluation prospects  technical report  un department of economic and social affairs population
division 
    abdelgawad  h   rezaee  k   el tantawy  s   abdulhai  b   and abdulazim  t          assessment of adaptive traffic
signal control using hardware in the loop simulation  in intelligent transportation systems  itsc        ieee   th
international conference on  pages           ieee 
    abdoos  m   mozayani  n   and bazzan  a  l          holonic multi agent system for traffic signals control  engineering
applications of artificial intelligence                 
    abdulhai  b   pringle  r   and karakoulas  g  j          reinforcement learning for true adaptive traffic signal control 
journal of transportation engineering                
    alizadeh  f  and papp  d          estimating arrival rate of nonhomogeneous poisson processes with semidefinite programming  annals of operations research                
    araghi  s   khosravi  a   and creighton  d          a review on computational intelligence methods for controlling traffic
signal timing  expert systems with applications                 
    asmuth  j  t          model based bayesian reinforcement learning with generalized priors 
university graduate school new brunswick 

phd thesis  rutgers

    baluja  s   covell  m   and sukthankar  r          approximating the effects of installed traffic lights  a behaviorist
approach based on travel tracks  in ieee intelligent transportation systems conference 
     brys  t   pham  t  t   and taylor  m  e          distributed learning and multi objectivity in traffic light control 
connection science             
     covell  m   baluja  s   and sukthankar  r          micro auction based traffic light control  responsive  local decision
making  in ieee intelligent transportation systems conference  itsc       
     de oliveira  d   bazzan  a  l   da silva  b  c   basso  e  w   nunes  l   rossetti  r   de oliveira  e   da silva  r   and
lamb  l          reinforcement learning based control of traffic lights in non stationary environments  a case study in a
microscopic simulator  in eumas 
     el tantawy  s  and abdulhai  b          multi agent reinforcement learning for integrated network of adaptive traffic
signal controllers  marlin atsc   in intelligent transportation systems  itsc          th international ieee conference
on  pages         ieee 
     el tantawy  s   abdulhai  b   and abdelgawad  h          design of reinforcement learning parameters for seamless
application of adaptive traffic signal control  journal of intelligent transportation systems               
     fleck  j  l   cassandras  c  g   and geng  y          adaptive quasi dynamic traffic light control 
     garca  j   lopez bueno  i   fernandez  f   and borrajo  d          a comparative study of discretization approaches
for state space generalization in the keepaway soccer task  reinforcement learning  algorithms  implementations and
aplications  nova science publishers 
     guez  a   silver  d   and dayan  p          efficient bayes adaptive reinforcement learning using sample based search 
in advances in neural information processing systems  pages          
     humphrys  m          action selection methods using reinforcement learning  from animals to animats           
     isa  j   kooij  j   koppejan  r   and kuijer  l          reinforcement learning of traffic light controllers adapting to
accident  design and organisation of autonomous systems  pages     

 

fi     jadhao  m  n  s  and kulkarni  m  p  a          reinforcement learning based for traffic signal monitoring and
management  in international journal of engineering research and technology  volume    esrsa publications 
     jin  j  and ma  x          adaptive group based signal control by reinforcement learning  transportation research
procedia            
     kalganova  t   russell  g   and cumming  a          multiple traffic signal control using a genetic algorithm  in
artificial neural nets and genetic algorithms  pages         springer 
     khamis  m  a  and gomaa  w          adaptive multi objective reinforcement learning with hybrid exploration for
traffic signal control based on cooperative multi agent framework  engineering applications of artificial intelligence 
          
     koltovska  d  and bombol  k          intelligent agent based traffic signal control on isolated intersections  tem
journal              
     lin  w  h  and wang  c          an enhanced     mixed integer lp formulation for traffic signal control  intelligent
transportation systems  ieee transactions on              
     mannion  p   duggan  j   and howley  e       a   an experimental review of reinforcement learning algorithms for
adaptive traffic signal control  autonomic road transport support systems  autonomic systems  birkhauser springer 
     mannion  p   duggan  j   and howley  e       b   learning traffic signal control with advice  in proceedings of the
adaptive and learning agents workshop  at aamas       
     marsetic  r   semrov  d   and zura  m          road artery traffic light optimization with use of the reinforcement
learning  promet traffic transportation               
     massey  w  a   parker  g  a   and whitt  w          estimating the parameters of a nonhomogeneous poisson process
with linear rate  telecommunication systems              
     medina  j  and benekohal  r          sensitivity of reinforcement learning agents to aggregated sensor data in congested
traffic networks  in t di congress       splanes  trains  and automobiles  pages         asce 
     medina  j  c  and benekohal  r  f          agent based traffic management and reinforcement learning in congested
intersection network  nextrans project      iy    
     moghaddam  m  j   hosseini  m   and safabakhsh  r          traffic light control based on fuzzy q leaming  in artificial
intelligence and signal processing  aisp        international symposium on  pages         ieee 
     ozan  c   baskan  o   haldenbilen  s   and ceylan  h          a modified reinforcement learning algorithm for solving
coordinated signalized networks  transportation research part c  emerging technologies          
     rezzai  m   dachry  w   moutaouakkil  f   and medromi  h          designing an intelligent system for traffic management  journal of communication and computer            
     robinson  a  and christopher  m          learning traffic light control policies 
     sanchez  j  j   galan  m   and rubio  e          genetic algorithms and cellular automata  a new architecture for
traffic light cycles optimization  in evolutionary computation        cec      congress on  volume    pages          
ieee 
     srinivasan  d   choy  m  c   and cheu  r  l          neural networks for real time traffic signal control  intelligent
transportation systems  ieee transactions on              
     steingrover  m   schouten  r   peelen  s   nijhuis  e   and bakker  b          reinforcement learning of traffic light
controllers adapting to traffic congestion  in bnaic  pages         citeseer 
     suryavanshi  s  and kotaru  m          predicting gas usage as a function of driving behavior 
     sutton  r  s  and barton  a  g          reinforcement learning  an introduction  the mit press 
     wang  y   won  k  s   hsu  d   and lee  w  s          monte carlo bayesian reinforcement learning  arxiv preprint
arxiv           
     watkins  c  j  and dayan  p          q learning  machine learning                
     watkins  c  j  c  h          learning from delayed rewards  phd thesis  university of cambridge england 
 

fi     weinberg  j   brown  l  d   and stroud  j  r          bayesian forecasting of an inhomogeneous poisson process with
applications to call center data  journal of the american statistical association                    
     wiering  m  et al          multi agent reinforcement learning for traffic light control  in icml  pages          
     wiering  m   van veenen  j   vreeken  j   and koopman  a       a   intelligent traffic light control  institute of
information and computing sciences  utrecht university 
     wiering  m   vreeken  j   van veenen  j   and koopman  a       b   simulation and optimization of traffic in a city 
in intelligent vehicles symposium       ieee  pages         ieee 

 

fi