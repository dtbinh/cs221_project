 

predicting running times from race history
matt millett and tj melanson

abstractin this project  we explored two different
methods to predict runners times by using their race
histories  baseline prediction used peter riegels method
of distance ratios  and the oracle was the stanford running club coach  each which achieved      error on
randomly chosen races from a preliminary dataset of   
runners  on the real data set of     runners  the riegel
predictor had      overall error  we implemented locally
weighted linear regression  which achieved down to   
average error with leave one out cross validation on    
different runners  however  the weighted regression failed
to predict any value at all for approximately     of
the data  we also built a hidden markov model  hmm 
modeling fitness states  which achieves      overall error 
and also offers more insight into a runners actual state
of fitness 
keywordshidden markov model  riegel  athletics  running  times  fitness  loocv  locally weighted linear
regression  stochastic gradient descent  em algorithm 
baum welch  expectation maximization 

i  i ntroduction
many runners like to think about what race times they
could have run or could run on a particular date  our
models attempt to predict runners race times and fitness
based on their race histories  right now  there are only
a few reliable methods to predict race ability  but they
are generalized for large groups of  often elite  runners 
this is partly because none of them take advantage of a
runners entire race history to individualize predictions
by only focusing on a single performance  for example 
vo  max testing  one method to detect fitness  involves
strapping a respirator to ones face and running on
a treadmill  obviously impractical  coaches can also
hazard a guess at what an athlete might run  but they
can be prohibitively expensive for the average runner  in
this project  we attempted to find a way to predict race
times based on race history 
in our method  we use data from a runners race
history that could give them a much better idea of how
they will perform  our first method is locally weighted
linear regression on individual runners to directly predict
their race times for given distances  our second method
uses a hidden markov model  hmm  to predict an
athletes fitness  his predicted fitness can then be used
to predict his race time for a given distance with a
particular probability  the input to our algorithm is a
matt millett  sunetid millett  is in cs     and cs      the work
for the locally weighted linear regression was done particularly for
cs      the work done on the hmm was mainly for cs     but
since its so difficult to implement it was also partly done for cs     
discussion of error and comparison  however  is so interlinked its
hard to separate them  so really the discussion applies to both classes 
tj melanson  sunetid melanson  is in cs     

runners race history and or a series of other runner race
histories  we use our locally weighted linear regression
and our hmm to output predicted race times  future or
hypothetical  
a  related work
there is limited research on analytically predicting
a runners future race time using full race histories 
most models rely on a single performance to guess at
ones racing abilities  the vo max test is a test where
the subject breathes through a respirator and runs at
maximum effort on a treadmill  although accurate  the
equipment needed to perform the test is expensive and
cumbersome  the famous coach jack daniels attempts
to measure vo max fitness from a single race  but it
parameterizes fitness with a single value and we didnt
use it as a baseline for this project      peter riegel 
an engineer  wrote in a      paper how to predict a
new race time based on a single recent result      but
no analytical model takes in a full history of past race
times to predict future times 
b  evaluating results
we evaluate our results in two ways  the first way
is with leave one out cross validation  loocv  for the
locally weighted linear regression  leaving out one race
and using the rest of a runners races to predict the race
excluded from training data  for the hmm  there is no
objective measure of fitness states  so we only evaluate
the times the model outputs  if the regression is unable
to make a prediction at all  due to matrix underflow
errors   we count the number of failed predictions as
well  this corresponds to the question  what could i
have run for this distance at this time 
for the hmm  we evaluate error by doing forward
prediction  predicting ones next race based on all the
previous race history  we cannot use loocv directly
because it is too slow and impractical for this method 
instead  we leave    athletes out of our training set to
use as our test set  the hmm prediction corresponds
to the question  what will i run in my next race  to
normalize across distances  we actually predict the error
in m s instead of seconds  we calculate our relative error
as 
predicted speed  true speed
true speed
unfortunately  our oracle was limited on time  so she
was only able to predict one randomly selected race per
athletes  for    athletes   but the rest of our models have
full error analyses 

fi 

c  gathering data
flotrack com provides a large database of race histories for many athletes  however  it provides the data
in human readable rather than machine readable form 
so we had to utilize a web scraping tool to extract
the needed data  we gathered this data from random
athletes on tfrrs org      each file of data is one athletes
race history downloaded from the website  we have
  datasets  one where we found athletes who raced
in cross country meets  and one where we found the
athletes who raced in track meets  people who raced in
the cross country meets tended to race the     m to the
     m  many cross country runners also run longer
events in track  so we ended up with lots of      and
     data from them as well  we also found athletes
from track meets who ran everything from the   m to
the      on the track  to scrape this data  we used the
beautifulsoup library for web scraping in python     
we ended up with data for     athletes  each athlete
has approximately       races in their history  where a
race consists of the race date  the race time  the race
distance  and the name of the meet where the race was 
note that the races stored in this website only store
history of college races  part of the requisite for data
selection was that the athlete had at least    races 
because otherwise it would be difficult to have enough
data to work with  particularly for our locally weighted
linear regression 
p reliminary a pproaches   baseline and
o racle
to get a good feel for the problem  we used a small
data set for our initial predictions  we scraped race data
for    top ncaa athletes from flotrack org      we
used this data to find initial estimates for error  both
the baseline and oracle predicted for the same races 
note that while these errors may seem large      
error corresponds to more than a   minute difference
in  k time  or    seconds in the        there are a
couple reasons for this  firstly  courses can vary vastly
in difficulty and we had no way to featurize this 
and secondly our initial scraping failed to differentiate
hurdle races and normal races  which have large time
differences 

a  baseline
peter riegel  an engineer  developed a widely used
time predictor based on a single race      take the race
you want to run of distance dnew and the most recent
time you have run told for a race of distance dold   the
time you run in the  tnew can be calculated as 

    
dnew
tnew   told 
dold
the baseline we use bases the new race off of the most
recent race run  indirectly setting a to whatever would
make the most recent distance be proportional at the
most recent time  while this is a fast method  it fails to
incorporate entire race history  on our preliminary data
set  riegels method averaged        error in predicting
times  where it took the athletes most recent race and
plugged it into the equation to predict the new time   in
this case  loocv and forward prediction are the same  
b  oracle
pattisue plumer is the coach of the stanford running
club and a former olympian  for our oracle  we asked
her to predict the times of    ncaa athletes of random
races in their career  she was allowed to look at the
entire rest of the race history  unfortunately  this doesnt
perfectly model a coach who personally knows the
athlete and can make more precise predictions  she
achieved       error on her predictions 

ii 

fig     this is what the data on the website looks like
before scraping  it has the date  meet  event  and time
listed  ultimately  we only used the date  event  and time 

iii  a pproaches
once we gathered our final dataset  the riegel predictor predicted race times with an average of     
error  this is a machine learning task  where given
the inputs of race history we output an expected race
time  there is some error that even the best predictor
will never overcome  which has to do with human
variability across races regardless of fitness  there is
also variability of difficulty of cross country courses and
things like the weather on the day of the meet that
throw a lot of variance into the data  in our dataset
we were only able to scrape meet names  and this
made it difficult to actually find that sort of data that
introduces this variability  we used a locally weighted
linear regression first to model this problem  and then a
hidden markov model  which we thought could better
model these uncertainties of athletes state of mind  the
weather  and a whole host of other factors that affect
race performance 
a  locally weighted linear regression
the first thing we did was to use locally weighted
linear regression to predict race times  the features of
the data we used were the race distance and the race
date  race date was featurized as number of days since
the athletes first recorded race  we used the normal
equation to find the value of  for the regression 
    x t w x   w y

fi 

where w is a weight vector corresponding to the weight
of each date     
we tried two different kernels to predict runners
times  the first was simple  if the distance of what we
were predicting matched a test data point  wed give that
data point a weight of    otherwise itd be of weight   
this gave      error on the track and cross country
datasets  with failures to predict          races  and an
average root mean squared error of      m s for crosscountry data and      m s for track data  for a few
examples  our model averaged about   second off for the
    and    seconds off the  k  our model fails to predict
race times when the date or the distance is too far off
from anything the model has seen before  the kernel sets
the weights of everything to   or tiny numbers that make
taking the inverse of the matrix for the normal equation
impossible or so small it causes underflow  we left these
out of our percentage error calculation and treated them
as their own statistic  because any human looking at the
outputs of the predictor would understand  for example 
its ludicrous to run   kilometers in       seconds  and
yes  this was one of our outputs    without removing
these failures  the error shoots up to      on average  
we also tested a gaussian kernel to predict weights 
which ultimately ended up giving us the best error 
we chose a gaussian kernel because its commonly
used for locally weighted linear regression      and
our intuition tells us as runners that as distance and
date drop off  the importance of each weight could
be approximately normal  the gaussian kernel sets the
weight w of an observation r    rdistance   rdate  t
while predicting at point p    pdistance   pdate  t as 
w e

 

q
 
 c 

where

q    rp 

distancew eight
 
 
datew eight




 rp 

and c as the bandwidth parameter 
because of the lack of prior literature on this topic 
we plugged in different values for the respective weights
and bandwidth parameters to see what would work  in
our modeling assumptions  we decided that the distance
was a more important factor than the date  but to get the
best results we would need to use both  distance gave
specificity to the race and date gave us an idea of how
the runner improved over time 
one of the main issues with locally weighted linear
regression is that it fails to predict races that are too far
out from an event horizon unless the bandwidth value is
large enough to turn it into simple linear regression  this
makes it difficult to extrapolate to predict in the future or
distances outside the data set originally provided while
retaining the useful locality this technique provides 
using these assumptions  we plugged in different
weight and bandwidth values to see what gave the
lowest error values  setting datew eight        and
distancew eight     and bandwidth      gave      
error on the track seeded data set with          failures 
but actually didnt do that much better on the crosscountry data set than simply looking at the distance 
giving       error with          failures to predict
times  interestingly  this was about as good of a value
as we could get  even after adjusting all the parameters 
the fact that locally weighted linear regression performs better on track data shouldnt be surprising  on
the track  there is much less uncertainty about if the
course is measured properly  or if there will be any
hills  because there wont be   most tracks are virtually
identical  it is intriguing that introducing a weight for
the date drives down the error so much  but that suggests
that overall in our dataset  people tend to vary more in
their race times from week to week  and incorporating
the date allows us to better capture this variability 
although we can somewhat model this variability
with locally weighted linear regression  we decided to
use a different model that allows us to model this
uncertainty more directly and has more explanatory
power  the hidden markov model 

b  hidden markov model

fig     this is a graph of the data for one of our runners 
on the x axis is distance  in m   on the y axis is speed
 in m
s    and on the z axis is the number of days since
the athletes first race 

the second approach we took was to build a hidden
markov model as shown below 
runners have hidden states of fitness  and from those
states they emit race times with certain probabilities 
there is no literature on how to model any of
these probabilities  so we chose what distributions
made the most sense to us  states of fitness are
parameterized by   values  s and e  which roughly
correspond to raw speed and endurance ability  recall
that riegels formula tells us that ones race speed
drops off as a power function of distance  roughly
based off of riegels work  we decided to model this as 
tavg   sde

fi 

where tavg is the expected time that one will run given
a certain value of s and e  respectively  s and e are
proxies for ones sprint and endurance abilities  smaller
values for both s and e correspond to faster times  we
decided that race times would be normally distributed
around tavg   and we used standard deviation as a fixed
hyperparameter value  with respect to distance  see
hyperparameters for a more detailed discussion   thus 
we can say 
t  d  s  e   n  sde   d 
where d is the race distance  is the standard deviation
in ms   s is the speed value  and e is the endurance
value  we also initialized all our transition probabilities
between fitness states to be uniform  while this is
certainly not true in practice  it worked well enough
for initialization  we discretized our continuous state
space by initializing our s and e values for each state
as linearly spaced values 
we trained our hmm
with a version of expectation maximization known as
the baum welch algorithm  the expectation step is a
forward backward algorithm used to find the probability
of the sequence landing in any of the particular states 
the forward part determines the probability that a given
sequence will land on a given state  and the backward
determines the probability that the remaining values
will occur after that state  the distribution of time
given a distance and fitness  parameterized by s and
e  is normally distributed about the mean of sde   as
described above 
the maximization step must find the values of 
that maximize the probability of a given sequence 
because s and e are dependent on each other  with
respect to distance and time   determining the maximum
likelihood that a given s and e would give a sequence
is impossible  as multiple values of s and e can result in
the same datapoint  because of this  rather than using a
deterministic algorithm to find the most likely pairs of

fig     our hmm  shown if it had   states  
hidden states are parameterized by s and e values
and emissions are races  which consist of a time and
distance  transition probabilities were calculated using
the baum welch algorithm and emission probabilities
were assumed to be normally distributed about the mean
value of sde   where d is the input race distance  we did
stochastic gradient descent to maximize the expected
probability of the s and e values for a training input
sequence 

s and e  we used stochastic gradient descent  sgd  to
adjust the parameters to the best value 
we used the python library yahmm  yet another
hidden markov model library      to build our hmm 
the library allowed us to run the baum welch em
algorithm for hmms to maximize the probability of
particular parameters for us by training on the dateordered race sequences from        of our athletes  on
the remaining    athletes  we ran a forward inference
for their final races  using their entire race histories up to
that point  we then found the most likely state of fitness
for the runner to be in and then calculated sde as our
estimated race time  loocv  training on     athletes
and test the    rd s final race time based on his history 
would be infeasible  because creating and training the
hmm each time would take prohibitively long  instead 
   runners from the original dataset became the test set 
while the other    were used to train the data  note
that the error function here is slightly different than that
used for the locally weighted linear regression  lwlr  
the lwlr interpolates over all of a single athletes
races  but the hmm extrapolates from an athletes race
history up to a point to predict his her future time   for
the locally weighted linear regression  it doesnt make
much sense to try to do future prediction because the
locally weighted regression will tend to do much better
on interpolating data  
c  hyperparameters
we set   the standard deviation of predicted time
for a particular distance  equal to       ms   we chose
this value because on the surface  it makes sense  on
a bad day  for example  one could run up to        ms 
   m      seconds slower in the      or    seconds
slower in the  k  because of runtime limitations  we also
only had    different states of fitness to choose from 
additionally  there are hyperparameters for the em
algorithm  the  of the sgd  the edge inertia  and
transition pseudocount  edge inertia is the inverses of
step size  for the model training  after some manual
tuning  we found an  of      to be the optimal value 
any smaller value didnt change the s or e values 
while a much larger caused the gradient descent to
diverge  when testing data  we found that the number
of edges between fitness states far exceeded the number
of transitions in the sequences  so we had to add a
transition pseudocount to smooth out the transition
probabilities  similar to laplace smoothing   as pseudocount smoothing did not always provide stable answers 
we also set the edge inertia to be     so only small
changes in transition probabilities occurred 
d  initialization
neither the baum welch nor the sgd maximization
guarantee a global minimum  because s and e depend
on each other  both algorithms could converge to a
nonoptimal solution  an extreme example is a value
of e     and s      fit the data point of      for an

fi 

 a  narrow initialization parameters

 b  wide initialization parameters

fig     training the hmm with different initialization
states with values of s and e  the size of each bubble
indicates prior probability  red indicates the initialization and blue indicates the final parameter values 

     a reasonable value   it fails to scale for longer
distances  it assumes that the runner never tires  which
is highly unlikely   this means the hmm is sensitive to
how its initialized  so we had to feed it sensible original
parameters  e probably varies from person to person  but
riegels formula told us e        however  we played
with values of s and e to create pairs of    m and  k
times that made sense to us based on our own running
experience  e g  a         m corresponding to about
      or       for a  k  depending on ones endurance  
we played with the initialization states to see how we
the error changed  as expected  training on a narrower
range of states brought prediction error from    to    
whereas training on a narrower range brought overall
error from      to       a much smaller improvement 

iv 

d iscussion

one advantage of hmms is that they tend to be more
descriptive about the data they analyze than regression 
by parameterizing these states as states of fitness and
assigning each race to a single fitness state  we are able
to actually find out some numbers that give one an idea
about his her sprint ability and endurance  accounting
for ones entire race history up to that particular point of
inference  hmms may also be able to extrapolate better
with less data  as a generative  not discriminative model 
the hmm builds a probability model for extrapolation 
which we hoped would work well with fewer races per
athlete  and this turned out to be true  for our models 
on average the hmm performed slightly better relative
to baseline 

a  accuracy
the most important output for each of these models
is the raw accuracy  but its important to note the error
values we calculate for the hmm and for the locally
weighted linear regression have a subtle difference 
the hmm error is for extrapolation to future races 
whereas the locally weighted linear regression error is
for interpolation  despite these differences  we can
compare each techniques error to our baseline error 
since riegels baseline predictor only takes one race
as input instead of many  it technically performs both
predictions 
the errors are summed up in the table below  the
locally weighted linear regression  lwlr  error in
the table is the interpolation error on loocv  listed
along with the number of failures to predict it had  the
hmm error in the table is the error on predicting an
athletes final race given his her total history up to that
point 
baseline
lwlr
hmm

xc error
    
   
           
fails 
    

track error
    
   
           
fails 
    

overall
    
    
    

again lwlr to hmm error measure slightly different things  so we have to be careful when comparing
their error values  respectively  they predict the times an
athlete could have run vs  times an athlete will run 
however  we can compare both the riegel prediction
error to both of them  since the riegel prediction only
looks at a single race 
v  c onclusion
our predicted models actually didnt perform as well
as we expected them to compared to our baseline  the
main exception to that was locally weighted linear regression on track data  these models seemed promising 
especially the hmm  because the hmm is more of a
descriptive model of whole states of fitness  with more
rich and complete race histories  our models might be
able to perform even better  but because each runner
only had about       races  it was much harder to
adapt our models to the data  particularly with the
locally weighted regression  which failed to produce a
reasonable prediction on     of attempts  that said  we
also were able to optimize to slightly beat the baseline 
which is exciting 

fi 

r eferences
   

   

   
   
   

   

   

daniels  jack  vdot running calculator  jack daniels vdot
running calculator  the run s m a r t  project  n d  web    
dec        https   runsmartproject com calculator  
riegel  peter s  athletic records and human endurance  a
time vs  distance equation describing world record performances
may be used to compare the relative endurance capabilities of
various groups of people  american scientist                 
tfrrs  track field results reporting system  web     dec 
      https   tfrrs org  
beautiful soup  last updated    sept        python library    
dec        http   www crummy com software beautifulsoup  
top rankings  men  xc        ncaa division   
flotrack  n p      oct        web     dec       
http   www flotrack org rankings subjective      
ng  andrew  cs    problem set    semi supervised
learning     oct        web     dec       
http   cs    stanford edu materials ps  pdf 
yahmm  yet another hidden markov model  jmschrei 
last updated   dec        python library     dec       
https   github com jmschrei yahmm 

acknowledgment
the authors would like to thank pattisue plumer for
all her expertise as an oracle  wed also like to thank
the cs    and cs    tas who helped us out on
this project  especially bryan mccann  who promptly
responded to all our emails 

fi