cs    machine learning final project

 

automatic product categorization for
anonymous marketplaces
michael graczyk  kevin kinningham
abstractin this paper  we present a machine learning algorithm to classify product listings posted to anonymous marketplaces  we
classify these listings according to the type of product being sold  the categories are derived from the    product categories on a
popular anonymous marketplace  agora  our algorithm is a combination of tf idf for feature extraction  pca for feature selection  and
svm for classification  we compare our algorithm to simpler models  including multinomial event naive bayes and a baseline algorithm
that uses simple string pattern matching  we achieve an accuracy of     on a withheld test set compared to an accuracy of     our
baseline model 

f

 

i ntroduction

anonymous marketplaces are a rapidly growing segment
of online illegal drug sales  figures   and   show the
mainstream user interface and product listing growth of
one such site  however  due to their clandestine nature  it
can be difficult to extract information about product listings
without manual intervention  law enforcement officials and
interested researchers must have an expert manually tag
each listing or rely on error prone ad hoc methods of catagorizing product listings 
to improve this process  we developed a machine learning algorithm that can automatically categorize listings with
high accuracy  the input to our algorithm is the listing text 
including its title and description  we then use tf idf to
extract features followed by pca to select features from
the text  finally  we use a svm to classifiy the features and
output a product category 
all data processing and machine learning was executed
using tools and algorithms in scikit learn      plots were
generated using matplotlib     

fig     agora marketplace on
january     

 

fig     new product listings
over time

r elated w ork

there have been several attempts to analyze anonymous
marketplaces          in      the author crawled the silk
road for eight months  analyzing product listings and the
overall distribution of product listings  however  they relied

on vendor supplied categorizations  which do not exist
for all marketplaces  additionally  some vendors purposely
misclassify their product to appear higher in marketplace
search results  correcting for these problems requires manual labeling of at least a large fraction of the data  as
has been used in analysis on other marketplaces  such as
ramp  
most of the machine learning techniques we used are
well documented in the literature as effective building
blocks for document classification systems  we extract word
features from each listing using term frequency inverse
document frequency  tf idf   which has proven effective
in document categorization      p        using svd for dimensionality reduction is a common technique for reducing
the size of the feature space for document classification     

 

dataset and f eatures

we used an anonymous marketplace product listing dataset
created by the researcher known as gwern      this dataset
was created by crawling several marketplaces daily  from
june  th       to july  th        for each product listing  we
extracted the posting text  including name and description 
the resulting dataset was then cleaned to remove parse
errors and duplicate listings  we then tokenized the cleaned
listings by extracting all words and numbers seperated
by spaces or symbols  the final dataset had about       
unique product listings and was used as input to feature
extraction 
crawl date
category
title
price
description

          
mdma
   grams of interways crystal clear molly
           btc
this is    grams of interways crystal clear molly  it will
be both rocky and sandy as thats how i received it  i am
pre packaging these up accordingly and only cr   
table  
example product listing on agora

to convert the text to features  we used term frequecyinverse document frequence  tf idf   tf idf converts

fics    machine learning final project

 

each token in the listing to a token weight based on how
important that token is to the listing  normalized by the
number of times the token appears in the whole corpus 
this normalization helps to reduce the impact of common
token in the corpus  finally  each listing is then converted to
a vector of token weights  similar to word vec 

tf t d   number of times token t appears in document d
idf t   log

total number of documents
number of documents the token t appears in

wt d        log tf t d          idf t  
additionally  we manually labeled about     product
listings to train our classifier as well as to use for crossvalidation  we classified each product listing into one of
twelve categories  these product categories were derived
from the product categories on both agora      one of
the largest anonymous markets  and  r darknetmarkets  a
popular forum for advertising and discussing drugs  

we use this transformation to select input features
for our supervised classification algorithm by computing
xlabeled   xlabeled wr   where xlabeled is a matrix containing
our labeled training data 
   

supervised product categorization

we categorize product listings using a soft margin support
vector machine  svm  for each category  we discriminate
categories using a one versus rest decision rule 
each support vector machine learns a decision boundary
by maximizing the margin between the decision boundary
and each training points 
more formally  we will consider a single category c
and explain how the svm determines the decision boundary hc   for simplicity  we assume each label has value
yi     if product i is in category c and   otherwise  let
d     xi   yi    i           n   be the training data set with
features xi  rr and labels yi         
n
x
 
i
  w      c
 
i  

argmin
w  b

 

m ethods

s  t 

our learning algorithm combines unsupervised feature selection with a supervised classifier  our data set includes
a large number of uncategorized documents and a small
number of manually categorized documents  in both cases 
each documents length is only a few dozen words  because
of the the relatively short length of the documents and
the small size of the labeled training set  a supervised
learning algorithm alone would have insufficient discriminative information to perform robust classification  instead
of using supervised learning in isolation  we extract more
discriminative features from our unlabeled data set using
a simple dimensionality reduction  these high quality features are then used to train a supervised classifier which
is much more likely to generalize well to new examples
because the underlying structure of the feature space is more
representative of the semantic structure of our entire corpus 
   

unsupervised feature selection

we perform feature selection using principal component
analysis  pca   in this case  the principal components are
uncorrelated inferred meanings of tokens based on their
usage patterns within the training data  this technique is
known as latent semantic indexing  lsa  because each
document is indexed  described as vector  by a set of inferred statistical  latent  parameters which are chosen using
their semantic relationships  this technique has been found
to be an effective way to extract structure from unlabeled
documents     
let xunlabeled  rn m be the matrix of n documents
and m tf idf features for which there are no category
labels  we compute the truncated singular value decomposition with rank r to produce the transformation matrix
wr  rm r   that is  we find compute

argmin
wr rm r

fi

fi
fixunlabeled  xunlabeled w r
fi
 

fi 
  fifi
 
  fif ro

   

yi  w  xi  b      i
i   

   

the optimization problem can be rewritten into its socalled dual form  the dual form reveals a simpler optimization problem without explicit slack variables     and
where many of the optimization parameters will be zero 

argmax
rn

s  t 

n
x
i  

n

i 

 x
i j yi yj x i xj
  i j

   i  c  i              n
n
x
i yi    

   

i  

 
   

e xperiments and r esults
algorithm tuning procedure

our algorithm includes four hyperparameters that are not
automatically chosen as part of the training process  these
hyperparameters are maxdf   r  c   and the svm regularization penalty and loss function 
      maxdf
maxdf is the maximum document frequency allowed for a
token to be used a feature  any token which appears in the
data set with frequency greater than maxdf is considered a
stop token and is ignored by the classification algorithm 
a larger value of maxdf gives more information to the
learning algorithm  but increases computational cost and
adds potentially useless features to the feature selection
process 
in our dataset  some tokens apply to several categories 
and thus appear in a lot of documents  even thought they
might be useful in discriminating categories  for example 
the token mg occurs in many listings  but does not usually
occur in listings categorized as marijuana or other  likewies 
tokens like india or china can usually give us some

fics    machine learning final project

 

hyperparameter
maxdf
r
c

idea of the class of drug  but are sill very common in the
overall dataset  for this reason  we expect the optimal value
of maxdf to be high  since we do not want to throw away
words soley becuase they are common 
     

svm regularization function

for the svm regularization penalty and loss functions  we
restricted our choices to l  or l  regularization penalties
and linear or quadratic loss functions  the choice to limit
the possible loss functions was made to simplify our implementation 
     

   

experimental procedure

we analyzed our algorithm by fitting our complete processing pipeline using all of our training data set  then testing
the accuracy of the model using a previously untouched test
set  the structure of our processing pipeline can be seen in
figure     

svm regularization weight  c  

c is the svm regularization weight  this real number determines the relative importance of regularization as compared
to maximizing the margin  we use the same regularization
for each category for simplicity  large values of c imply
more complicated decision boundaries in which some input
features may be vastly more important than others  small
values lead to simple decision boundaries which consider
each feature dimension similar in importance  since our use
of feature preselection serves to make the data somewhat
compact  we expect the optimal c to be somewhat small 
     

table  
hyperparameters used

pca output dimensionality  r 

r is the dimensionality of the feature space selected by
pca  the svm operates on training examples which have
been transformed into this feature space  larger values of r
increase computation time but make it easier for the svm
to find high margin  simple  separating surfaces between
each class and non class training examples  svm hypothesis
found for larger r may also generalize better because the
svm was able to use more information to decide on its
hypothesis  smaller values of r simplify computation and
make it more difficult for the svm to find high margin
separating surfaces 
however  smaller values of r may improve discriminative quality in the input features  the unsupervised feature
selection process uses vastly more data than the svm training process  so feature selection may reveal structure in the
data that cannot be learned by the svm  smaller r enables
feature selection to make stronger quantitative statements
about the semantic structure of the data 
     

value
    
   
    

hyperparameter selection

to choose values for our hyperparameters maxdf   r  and
c   we used course grained grid search with   fold cross
validation  grid search operates by exhaustively enumerating every possible combination of hyperparameters and
selecting the combination that performs the best on the
validation test set 
we searched     possible values for c logarithmically
spaced from     to   and    values of r linearly spaced
from     to      we also searched     possible values
of maxdf logarithmically spaced from     to    we also
experimented with both l  and l  for svm regularization 
and decided to use l  since it performed better on our
dataset 

fig     data pipeline

we had    of our data at the beginning of our algorithm
design process and had not used it for training or cross
validation  we used our processing pipeline to compute
category predictions for each point the in test set 
   

results

the most important metric for our classification algorithm
is categorization accuracy  the accuracy for a test set is
the proportion of labels that were correctly assigned  for
comparison  we compared the accuracy of our method with
three other algorithms 






a baseline model that used simple substring search 
with substrings chosen by an online market expert 
for example  if a product listing contained the text
xanax  then the listing was classified under benzos 
the   test provides a simple way to remove features
that are not correlated with any labeling  and is
much faster than svd  however  it is also much less
accurate and has a much higher output dimension
than svd 
multinomial naive bayes  using the same   features 

fics    machine learning final project

 

the accuracy for each model is shown in figure    the figure
shows that our model outperformed several more simple
models 

fig     normalized confusion matrix

fig     comparison of model accuracies
benzo
dissociative
ecstasy
misc
opiate
steroid
psychedelic
research chemical
prescription
stimulant
marijuana
other

bensedin mot unmarked som bars  mg
diazepam xanax clonazepam zepose
purity reputation mxe     g methoxetamine chopping lab requested
    pressed pills mephedrone dutch red
ecstasy express crystals   
camel zolpidem lunesta eszopiclone caffeine phenargan zolab tranax
 mg   mg heroine methadone opium
codeine fentanyl naloxone tramadol
  ml boldoject dianabol ml propionate
sibutramine sibutramin test testosterone
buddha stand sheet babies mushrooms
psilocybe hearts blotter nbome lsd
fluoroamphetamine       al   g lad chiral
dichloropane mdai fa apdb
mobic pseudoephedrine tadalafil generic
dexamphetamine medications
coke amphetamine check modafinil
modalert adderall methamphetamine
open grown dream crash kush weed hash
wax taste sativa
size windows custom ways facebook account kinesiology book dpz guide

table  
top tokens for each category

the precision recall curve compares shows the tradeoff between precision  the number of true positives over
the total number of positives  vs recall  the number of true
positives over the number of true positives plus the false
negatives  as we vary our algorithms parameters  figure  
shows the precision recall curve for our model 
classification quality was not equal between each category  figure   shows the confusion matrix of our algorithm
on our test set  the grid position in row i and column
j shows the number of times a listing which is truly in
category i was classified by our algorithm as category j  
the values in the grid cells show the absolute number of
test points  while the colors show that number normalized
by the true number of test points in each category 
the confusion matrix shows that our classifier was particularly bad at classifying the other category  we believe

fig     normalized confusion matrix

this is because of the large variety of other products available and the somewhat arbitrary definition of the category 
for example  we one category of goods we included in
other was illegal digital goods  like movie downloads 
because of this  drugs that had similar descriptions to unrelated products got misclassified as other  to highlight this
effect  our algorithm misclassified a strain of weed called
the big lebowski as other because we had included a
label for the movie download of the big lebowski  see
table   for the complete example 
true category
predicted category
title
description

marijuana
other
    ounce the big lebowski
the big lebowksi     week cure indica
sativa blend  over the years pot has gotten
a lot stronger especially with the advent of
indoor hydroponic growing under metal
halide and high pressure s   

table  
text of typical misprediction

fics    machine learning final project

 

f uture w ork

in this project  our training and testing used data from only
a single market  the algorithm could be made more robust
by including data from more sources  test data drawn from
a broader source would also provide a better generalization
estimate 
additionally  our algorithm could have considered features other than the product listing text when classifing
products  for example  we could have used product price
as a feature  however  as the amount of product is not listed
in a consistant format  normalizing the prices would require
extracting the amount of product being sold 

 

c onclusion

in this project  we developed a machine learning algorithm
that can classify product listings according to the type of
product being sold  our algorithm consisted of a pipeline
using tf idf for feature extraction  svd for feature selection  and svm for final classification  we then used crossvalidation to select hyperparameters and tune our algorithm 
we also evaluated our algorithm in comparison to several other models 




a baseline model that used simple string search
svm with a chi squared test for feature selection
multinomial naive bayes

our algorithm had an accuracy of     compared to the
baseline accuracy of      our algorithm also outperformed
the alternate svm and multinomial models 
our algorithm was able to outperform the alternate
models because it was able to take advantage of structure
in the large unlabeled dataset  in this project  we had a
small amount of labeled data  and a very large amount of
unlabeled data  the svd we use is performed on the unlabeled data  which helps to expose structure not captured by
the labeled data  we then project our labeled data onto the
vector space chosen by the svd and train our svm on the
result  this allows to take advantage of both our unlabeled
and labeled data in selecting our decision boundry 

fig     new listings by category on agora

we also used our classifier to measure the number of
new products by category over time  we found that by far

 

the most common product listed on agora is stimulants 
interestingly  this was significantly different than an earlier
anonymous market  the silk road  where marijuana was
the most common listed product  with stimulants a distant
fourth     
this also matches reports from user and vendor forums 
many of whom have complained that the legalization of
marijuana has reduced the profitability of selling online 
conversely  amphetamine demand has dramatically risen 
while production costs have dropped  this has resulted
in an apparent increase in product listings  as far as we
are aware  we are the first to rigorously document this
switch  this shows that our algorithm is extremely useful in
practice  particularly for law enforcement and researchers 

fics    machine learning final project

r eferences
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and
e  duchesnay  scikit learn  machine learning in python  journal
of machine learning research  vol      pp                 
    j  d  hunter  matplotlib  a  d graphics environment  computing
in science   engineering  vol     no     pp             
    g  branwen  silk road  theory and practice  http   www 
gwern net silk   road  augest       accessed             
    n  christin  traveling the silk road  a measurement analysis of
a large anonymous online marketplace  in proceedings of the   nd
international conference on world wide web  international world
wide web conferences steering committee        pp         
    p  r  christopher d  manning and h  schtze  introduction to information retrieval  cambridge university press       
    j  t  sun  z  chen  h  j  zeng  y  c  lu  c  y  shi  and w  y  ma  supervised latent semantic indexing for document categorization  in
data mining        icdm    fourth ieee international conference on 
ieee        pp         
    g  branwen  dark net market archives             http   www 
gwern net black market   archives  july       accessed            
    a  greenberg  drug market agora replaces the silk road
as king of the dark net  http   www wired com         
agora bigger than silk road  sept        accessed             

 

fi