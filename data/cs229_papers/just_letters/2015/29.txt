named  entity  recognition  and  question  answering    
using  word  vectors  and  clustering  
  
zia  ahmed  
rajkiran  veluri    
zahmed stanford edu  
rveluri stanford edu    
cs      machine  learning  project  
computer  science  department   stanford  university  

  

problem  statement  
in   this   paper   we   investigate   the   word vec   model   as   proposed   by   tomas   mikilov   for   determining   word  
relationships   and   use   the   word   vectors   to   implement   a   named   entity   recognition    ner    system    ner  
plays  a  key  role  in  many  natural  processing  tasks  like  question  answering   qa    this  is  due  to  the  fact  
that   answers   to   many   questions   are   named   entities   that   depend   on   the   semantic   category   of   the  
expected   answer    in   this   context    we   examine   the   effectiveness   of   our   ner   algorithm   to   identify   the  
entity   category   of   the   expected   answer    in   particular   we   study   the   methodology   of   boosting   the  
performance  of  the  ner  system  by  training  on  pre annotated  natural  language  questions  combined  with  
the  annotated  free  text  data   we  hypothesize  that  the  ner  system  can  benefit  from  the  inclusion  of  the  
pre labelled   questions    further   we   explore   clustering   mechanism   to   classify   word   vectors   into   entity  
classes  and  discuss  how  clustering  can  be  used  to  improve  the  performance  of  the  ner  system     

introduction  
the   word vec   model    proposed   by   tomas   mikolov   at  google    used   skip   gram  and   continuous   bag   of  
words    cbow    model   to   create   word   embeddings          an   extension   of   word vec   is   the   glove   model  
proposed  by  penninglon   which  is  easier  to  parallelize        these  models  have  enabled  natural  language  
processing  tasks  like  ner   pos  tagging  to  avoid  manual  designing  of  features   by  using  word  vectors  that  
capture   the   syntactic   and   semantic   information   through   latent   dimensional   features          still    the   finer  
nuances   of   word   embeddings   in   vectors   space   have   not   been   understood   fully    for   named   entity  
recognition ner     we   decided   to   use   word   vectors   but   a   comparable   performance   has   been   reported  
using   brown   clusters   which   is   a   hierarchical   agglomerative   clustering   algorithm   relying   on   maximizing  
the  mutual  information  of  bigrams         an  advantage  of  brown  clusters  is  that  it  works  better  on  rare  
words    another   scalable   algorithm   using   deep   learning   for   ner   was   proposed   by   collobert   and  
weston       ratinov  and  roth  discuss  issues  in  designing  ner  systems  in           in  this  paper  we  use  the  
vector   representations   of   words   to   implement   a   neural   network   based   ner   system    which   is   then  
utilized  to  aid  in  question  answering  by  reducing  the  answer  candidates   

word  vectors  
the   method   used   to   generate   word   vectors   is   the   continuous   bag of word   model    cbow    by   mikolov   et  
al            cbow  is  a  neural  network  model  which  tends  to  predict  the  target  word  based  on  the  input  
window   of   context   words   surrounding   the   target   word    the   training   process   creates   low dimensional  
word   vectors    each   word   is         dimensional    for   each   word   in   the   training   corpus    the   word   vectors  
which   are   contextually    syntactically   and   semantically   similar   tend   to   lie   near   each   other   in   this   low  
dimensional   space    as   shown   in   the   pca   analysis   of   the   few   handpicked   words   from   the   vocabulary   
refer   figure       for    d   pca   representation   of   word   vectors    we   use   these   word   representations   as  
features  to  build  the  ner  system  which  is  described  below   we  implemented  cbow  model  and  trained  

fiusing   googles   dataset          the   algorithm   performed   well   on   smaller   subsets   of   our   data   but   when  
training  on  a  large  vocabulary       m    the  training  time  became  excessively  large   so  we  used  pre trained  
word  vectors        

named  entity  recognition  
ner   is   a   classification   problem    where   each   input   word   is   classified   as   being   a   location    person   
organization    miscellaneous   and   other    not   any   named   entity     the   algorithm   uses   tokenized   text   to  
train   a   neural   network   model   for   named   entity   recognition   with   multiple   classes    the   detailed  
annotation   structure   for   the   dataset   is   given   at           the   training   and   the   testing   data   for   the   ner  
algorithm   is   taken   from   conll     corpus    the   data   consists   of   sentences   with   one   token   per   line   and  
each   token   is   associated   with       possible   labels     o    loc    misc    org    per    representing   the   classes  
defined   above    the   word   vectors   learned   using   the   cbow   model   were   used   to   construct   context  
windows  that  serve  as  input  features  to  the  neural  network   
the   model   is   implemented   as   single   layer   neural   network   with   word   embedding   as   the   input   layer  
feeding  to  feedforward  algorithm   the  predicted  class  vector  is  then  compared  to  the  actual  class  and  
the   delta   error   propagates   back   updating   the   model    as   the   algorithm   iterates   through   the   dataset   it  
learns  both  the  classifier  and  the  word  representations     
the  feedforward  operation  is  given  by  the  following  set  of  equations  
   
               
   
       
       

 

   

where                     is  the  context  window   w  and  u  are  the  model  parameters  and  g  is  the  softmax  
function   the  cost  function  to  minimize  is  given  by  the  following  equation  
 

 
 

                

     
       


  
 

 

  

 

    

where   m   is   the   number   of   data   samples    k   is   the   number   of   entity   classes   and      is   the   regularization  
parameter    the   parameters   are   learned   using   stochastic   gradient   descent   algorithm   and   gradient  
checking  is  used  for  bug free  implementation   
the   evaluation   of   the   implemented   algorithm   was   done   using   the   conll     conlleval   perl   script    the  
script  evaluates  the  ner  systems  capability  of  identifying  named  entities   it  gives  a  clear  presentation  of  
the   performance   of   the   system   on   various   entity   categories    person    location    organization   
miscellaneous  and  other   based  on  the  precision   recall  and  f   measures   

results  
tuning  parameters  
the  parameters  of  the  system  that  were  tuned  for  higher  accuracy  were   

fi




the  regularization  constant      
the  learning  rate      
the  context  window  size   c   
the  number  of  iterations   epochs   

we  used  context  window  size  of     and     for  the  model   better  results  were  achieved  with  size      it  was  
observed   that   increasing   number   of   iterations   does   not   necessarily   increase   the   performance    the  
improvement   stagnated   after        iterations    for   regularization   parameter                gave   the   best  
performance    the   performance   was   decreasing   for   higher   values   of       the   learning   rate        was  
optimized  at          lower  learning  rate  gives  inferior  results       the  optimal  values  found  for  the  tuning  
parameters  are  given  in  table      
  

parameters  
epoch  
learning  rate      
regularization      
context  window  size   c   

  
loc  
misc  
org  
per  
system  

optimal  value  
    
       
      
   

table      optimal  parameter  values  for  ner  

recall  
        
        
        
        
        

precision  
        
        
        
        
        

f   
        
        
        
        
        

table      ner  evaluation  results  

  
         
         
        

recall  

        

precision  

        

f   

        
       

      

loc  

misc  

org  

per  

system  

      

    figure      ner  evaluation  results  

question  answering  
named   entity   recognition   systems   are   used   in   a   lot   of   nlp   tasks    in   particular    they   play   a   prominent  
role   in   question answering    named   entity   recognition   systems   are   typically   used   in   question   answering  
systems  like  afner  to  narrow  down  the  candidate  answers  which  match  the  semantic  category  of  the  
selected  answer     for  example   the  answer  to  the  question  which  is  the  capital  of  france   the  system  
identifies   the   category   of   the   expected   answer   to   be   a   location    loc     thus    the   system   will   only  

ficonsider   the   named   entities   with   category   loc   as   answers   thereby   affecting   both   the   precision   and  
performance  of  the  overall  system   
in   this   paper   we   utilize   our   neural   network   based   ner   model   to   identify   and   classify   nes   in   natural  
language   questions    we   hypothesize   that   the   ner   system   can   benefit   from   the   inclusion   of   the   pre 
labelled   questions   in   the   training   corpus    the   training   and   testing   corpus   for   the   experiment   was  
downloaded  from         the  training  data  consists  of        annotated  questions  with  categories  per   loc  
and  org   similarly  the  test  data  consists  of       pre annotated  questions   
the   performance   is   baselined   using   the   system   trained   on   the   conll     corpus   and   tested   on   the        
test  questions   the  baseline  results  are  given  in  the  table      although  the  f measure  for  the  free  text  
test  corpus  was           the  systems  performance  drops  to          when  tested  on  the       annotated  
questions  test  corpus   next   we  trained  the  ner  system  on  the  training  corpus  of        pre annotated  
questions  and  tested  the  resulting  model  both  on  the  conll    test  data  and  the       test  questions   the  
results   of   the   step   are   given   in   table        the   system   performed   well   on   the   annotated   questions   test  
corpus  but  failed  miserably  for  the  conll    free  text  test  corpus   finally   the  ner  system  was  trained  on  
using  both  the  conll    corpus  and        pre annotated  questions  corpus   the  performance  on  both  the  
test  datasets  are  given  in   table       we  achieved  an  f measure  of         on  the  question  test  data  when  
the  training  data  contained  both  the  free  text  and  pre annotated  data   
the   results   obtained   in   this   work   suggests   that   the   ner   system   used   in   aiding   question   answering  
system  benefits  from  including  questions  in  the  training  corpus   to  build  a  ner  model  which  provides  an  
f measure          we  should  build  a  training  corpus  which  is  a  suitable  mix  of  free  text  and  annotated  
questions   as  shown  in  table      the  inclusion  of  free  text  in  the  training  data  is  not  relevant  if  we  have  
sufficient   questions   to   train   the   ner   system   and   the   system   is   used   only   for   question   answering    but   to  
build  a  general purpose  model  the  system  will  benefit  from  combination  of  training  data   
  
train  
type  
free  text  

eval  

sentences  
       

tokens  
        

free  text  
questions  

recall  
     
       
       

precision  
     
       
       

f measure  
     
       
       

recall  
     
       
       

precision  
     
       
       

f measure  
     
       
       

recall  
     
       
       

precision  
     
       
       

table      baseline  ner  results  
train  
type  
questions  

eval  

sentences  
      

tokens  
       

free  text  
questions  

table      trained  on        pre annotated  questions  only  
train  
type  
free  text     questions  

sentences  
       

eval  
tokens  
        

free  text  
questions  

table      trained  on  both  free  text  and  pre annotated  questions  

f measure  
     
       
       

  

ficlustering  word  vectors  for  training  ner  
the   clustering   was   done   using   k means   algorithm   on   the         dimensional   word   vectors   generated   by  
word vec   model    we   constructed   clusters   of   multiple   granularities    through   hierarchical   clustering   the  
primary   intuition   being   that   clustering   would   give   us   a   unsupervised   automated   way   to   increase   our  
training   data    similar   to   construction   a   ner   gazetteer    we   found   that   unigram   clusters   capture   broad  
categories  of  entities   countries  and  states  in  a  single  cluster   names   and  would  be  useful  in  ner  if  there  
are   more   number   of   labels    also   increasing   the   number   of   clusters   gave   us   better   separation   of   entities   
the   clusters   generated   were   used   to   train   the   ner   model    the   results   are   given   in   table           an  
important   realization   was   that    the   single   word   clusters   do   not   capture   context    so   training   n gram  
clusters   setting  n  to  be  the  size  of  the  context  window   could  be  more  useful  approach  in  clustering          
training  corpus  
clusters  
conll       clusters  
clusters  
conll       clusters  

cluster  granularity  
     
     
      
        

f measure       
       
       
       
       

table      testing  cluster  granularity  

further  study  
combining  neural  networks  with  word  vector  models  for  named  entity  recognition  is  an  active  field  of  
study   named  entity  recognition  using  recurrent  neural  networks   rnn   and  long  short  term  memory  
 lstm   is  also  a  promising  future  direction  and  better  results  have  been  achieved  by  it   future  directions  
of  study  for  question  answering  would  focus  attention  on  dynamic  memory  networks  that  make  the  use  
of  word  vectors  by  combining  a  knowledge  base   or  facts   to  achieve  state  of  the  art results        

acknowledgments   
we  would  like  to  thank  prof   andrew  ng  and  our  project  mentor   youssef  ahres   for  their  guidance  and  
support  during  the  project     

  
figure      pca  representation  of  word  vectors  

  

fireferences   
  
      https   code google com p word vec      
      miller   s    guinness   j       zamanian   a           may    name  tagging  with  word  clusters  and  discriminative  
training   in  hlt naacl   vol       pp                
      siencnik   s   k           may    adapting  word vec  to  named  entity  recognition   in  nordic  conference  of  
computational  linguistics  nodalida         p          
      mendes   a   c    coheur   l       lobo   p   v           may    named  entity  recognition  in  questions   towards  a  
golden  collection   in  lrec   
        lin   d       wu   x           august    phrase  clustering  for  discriminative  learning   in  proceedings  of  the  joint  
conference  of  the    th  annual  meeting  of  the  acl  and  the   th  international  joint  conference  on  natural  
language  processing  of  the  afnlp   volume    volume      pp                association  for  computational  
linguistics   
        kumar   a    irsoy   o    su   j    bradbury   j    english   r    pierce   b            socher   r            ask  me  anything   
dynamic  memory  networks  for  natural  language  processing   arxiv  preprint  arxiv              
      mikolov  t   sutskever  i   chen  k   corrado  g  s     dean  j          distributed representations of
words and phrases and their compositionality  in advances in neural information processing systems  pp 
           
        mikolov  t   chen  k   corrado  g     dean  j          efficient estimation of word representations in
vector space  arxiv preprint arxiv           
        pennington  j   socher  r     manning  c  d          glove  global vectors for word representation 
proceedings of the empiricial methods in natural language processing  emnlp                      
         ratinov  l     roth  d         june   design challenges and misconceptions in named entity recognition 
in proceedings of the thirteenth conference on computational natural language learning  pp           association for computational linguistics 
        brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram
models of natural language  computational linguistics                 
        collobert  r     weston  j         july   a unified architecture for natural language processing  deep
neural networks with multitask learning  in proceedings of the   th international conference on machine
learning  pp            acm 

data  
       http   www cnts ua ac be conll     ner annotation txt    
                         https   qa l f inesc id pt wiki index php resources  
       http   mattmahoney net dc text  zip  

  

fi