speech to song classification
emily graber
center for computer research in music and acoustics  department of music  stanford university

abstract
the speech to song illusion is a perceptual phenomenon where listeners perceive the transformation of certain
speech clips into song after approximately ten consecutive repetitions of the clips  both perceptual and acoustic
features of the audio clips have been studied in previous experiments  though the perceptual effects are clear  the
features driving the illusion are only known to relate to isolated acoustic features  in this paper  speech clips are
examined from a music theoretical viewpoint  typical music theoretic rules are used to derive context dependent
features  the performance of classification trees is then used to assess the utility of the music theoretically derived
features by comparing them to spectral features and linguistic features  contour features are found to differentiate
the speech clips into transforming and non transforming variants suggesting that music theoretic schema may be
responsible for driving the perceptual classification 

introduction

within syllable pitch and perfect fifth jumps made the
sts illusion more likely 
it seems that musical features and the ability to access those features drives the sts illusion  in the present
study  i explored the naturalistic stimulus set used by
tierney et al   i differentiated the stimuli based on musictheoretic features  i e  context dependent features rather
than linguistic  semantic  rhythmic  or pitch features
alone  seven feature categories  linguistic  rhythmic  harmonic  contour  pitch  spectral  and general  each with
several features were evaluated in terms of their loocv
test error in classification trees that predicted the perceptual class of the test stimulus  i e  transforming or
non transforming  contour features were found to be the
best predictors of stimulus type  this supports the notion
that context helps drive the sts illusion 

the speech to song  sts  illusions is a perceptual phenomenon where listeners perceive the transformation of
a given speech clip into song after approximately ten
consecutive repetitions of the clip  deutsch et al         
listeners do not perceive this transformation for all
speech clips  thus several perceptual and neuro imaging
studies have aimed to figure out what the perceptual difference is between the clips that transform and
the clips that do not transform  deutsch et al         
 tierney et al           hymers et al          these studies were able to find significant differences in behavioral
responses and brain responses to transforming and nontransforming  or not yet transformed  stimuli 
given the neural and behavioral difference between
transforming and non transforming stimuli  it is also
of interest to know what about the stimuli drives
the sts illusion  tierney et al  used statistically
matched stimuli for each group  transforming and nontransforming  such that average syllable length  average syllable rate  and average fundamental frequency
differences between the groups were not perceptually
significant  tierney et al          within syllable frequency change and inter accent intervals were however
found to be different between the transforming and nontransforming stimuli though they were not purposely
manipulated in the experiment  margulis et al  explored the relevance of repetition onset timing and semantic syntactic content for the strength of the sts illusion
 margulis et al          as semantics became less and less
relevant  the strength of the illusion increased  falk et al 
also found that certain pitch and rhythmic properties facilitated the sts illusion in their careful manipulations
of just two clips  falk et al          most notably  stable

related work in machine learning
differentiating speech from music is a common machine
learning task  usually  spectral features like mfccs 
centroid  flux  and tilt  extracted from time domain signals are useful for discriminating between speech and music  scheirer and slaney         this works because most
music contains instrumental contributions which have
very different spectral characteristics from the speaking
voice  indeed  spectral features are useful for classifying
different musical genres without voice as well  mandel et
al  were even able to classify individual artists by retaining detailed information about full audio clips  i e modeling unaveraged mfccs for each clip as a mixture of gaussians  mandel and ellis         nam et al  took an unsupervised learning approach to find useful features for music tagging annotation classification  nam et al         
 

fi 

in doing so  they were able to use a simple linear classifier to distinguish genres  this method is compelling
because the features were not hand crafted as mfccs
and most other spectral features are 
it is challenging to find features that are useful for discriminating between the speaking voice and the singing
voice because spectral information is no longer highly
informative  thompson developed a successful method
to classify speaking and singing based on pitch stability and pitch probability distributions  thompson        
however  in the present application  all audio signals are
recorded speech  therefore a different method for feature
extraction must be used 
pitch tracking and onset detection algorithms  used
in music information retrieval tasks  are useful for parsing time domain audio into note like units  lee and ellis
developed a robust pitch tracking algorithm for speech
that uses a multi layer perceptron classifier to eliminate
octave errors and noise errors that typically plague autocorrelation pitch trackers  lee and ellis         lee and
ellis algorithm also finds the probability that the speech
in each time frames is voiced or unvoiced  the start of
voiced segments is often analogous to note onset times 
the findings of falk et al  support this idea as they found
that intervocalic interval stability was more important
than intersyllabic interval stability  falk et al         

were obtained by averaging the    dimensional mfccs
made from    ms hann windows with     overlap calculated by the auditory toolbox  slaney         figure  
shows an example of estimated pitches  estimated onsets 
and a full transcription for a transforming clip 

figure    output of sacc  from top to bottom  spectrogram 
pitch estimates  p voiced   the probability that the phoneme being spoken is voiced  i e   vowel like  full transcription of the clip 
speaker said linen of this sort in public 

all feature categories and features are summarized in
table   below 
table    feature descriptions

dataset and features

feature category

stimuli     suitable sts clips with mean duration       
seconds  sd           were excerpted from audiobook
recordings  these clips were previously evaluated in a
behavioral and functional imaging study  thus correct
labelings were known  tierney et al          differences
between average duration  syllable rate  syllable length 
fundamental frequency  phonetic content  and semantic
structure were considered and found effectively insignificant between the transforming and non transforming
clips  all clips were mono recordings with       hz sampling rate 
processing  all audio processing was done in matlab  lee and ellis subband autocorrelation classification  sacc  was used for initial pitch and onset detection
estimates  lee and ellis         full transcriptions were
made by hand to correct any errors in sacc  and all features were derived from those transcriptions with the exception of the spectral features  the mean mfcc vectors

linguistic

rhythmic

harmonic

contour

pitch
spectral
general

features
number of syllables
number of stressed syllables
longest word
total number of onsets
number of strong beats
pickups
syncopations
hemiolas
implied meter
implied tonic
implied dominant
implied other
mode
non diatonic pitches
resolution level
resolution strength
number of melodic leaps
number of melodic steps
largest leap size in semitones
number of consecutive leaps
histogram of scale degrees
range in semitones
melisma
mean mfccs
key
number of notes
number of unique notes
total duration

table    error statistics
linguistic
rhythmic
harmonic
contour
pitch
spectral
general
all

loocv error
      
   
      
     
      
       
      
       

hit rate
    
      
      
     
      
            
      
     

miss rate
    
      
      
     
      
            
      
     

false alarm rate
      
      
      
     
      
     
      
      

correct rejection rate
      
      
      
     
      
     
      
      

precision
      
      
      
   
      
      
   
     

recall
      
   
      
     
      
   
      
      

fi 

classification methods

results

cart    classification and regression trees work by segmenting the feature space of a dataset into discrete bins 
a prediction can made according to which discrete bin
a test samples features fall into  in classification trees
 as opposed to regression trees  bin boundaries are determined by recursive binary splitting  a greedy procedure
where splits are chosen to maximize node purity at the
time of the split  james et al          for example  given
data xirmxn   if the cutpoint s were chosen for predictor xj   there would be two resulting regions  one region
containing all samples where xj   s and one region containing all other samples where xj  s  the goal of the
classification tree is to choose s and j such that the resulting regions contain samples from only one class  n b 
this is an ideal case   now that the class labels for those
regions are known  any sample that falls into them can
be assigned the appropriate label 
with just one split however  it is likely that the resulting regions will not contain single class labels  in this
case  the class that is most common in a region becomes
the class label for that region  the classification proportions for a region r can then be calculated for each
possible class k 

rk  

number samples with class k in region r
number samples in region r

the gini index g measures the node or region impurity over all classes 

gr  

k
x

rk     rk  

k  

finally  the classification tree aims to create regions by
choosing j and s that minimize the gini index  if all the
samples in a node or region are from the same class  what
we want    gr      the tree continues to make spits until
the nodes are pure  or some threshold has been passed 
therefore the number of splits can serve as an indication figure    top to bottom  classification tree based on contour
features  classification tree based on pitch features  classification
of how complicated the classification process was  addi  tree based on harmonic features
tionally  splits closer to the root of the tree can be said to
be more important that splits near the leaves of the tree 
in order to asses which feature category was most relclassification trees are easy to interpret  i e  it is clear evant for differentiating the sts stimuli  i created sepwhich feature was chosen for every split  and what the arate classification tress for each category  because i
value of the particular feature was to make the best split  had a limited set of training data  i choose to evalui chose to use classification trees precisely for those rea  ate the performance of the trees by leave one out crosssons 
validation  loocv   table   shows the loocv error 
  this

description is based on an introduction to statistical learning by gareth james et al 

fireferences

 

confusion matrix values  and precision and recall metrics 
hits were counted when the test sample turned into song
and the prediction was correct  miss were counted when
the test sample turned into song and prediction was incorrect  false alarm were counted when the test sample
was not song yet song was predicted  i e  transforming  
correct rejection was counted when the test sample was
not song and the prediction was also not song  i e  nontransforming  
the three trees with the lowest error and simplest
structure are shown in figure    the contour features  number of melodic leaps  number of melodic steps 
largest leap size in semitones  and number of consecutive
leaps  appear to be the most relevant for differentiating
the sts stimuli  the root node divides the stimuli according to the number of jumps that take place in the
melody  the second split is based on the largest jump
size in the melody  a jump greater than     semitones  a
perfect fifth plus a quarter tone  predicts that the melody
will not be perceived as song 

conclusion

discussion

references

the features selected by the trees in figure   support the
idea that musical context is playing an important role in
the sts illusion  previous work has shown that pitch stability and jumps of perfect fifths help to improve the sts
illusion  falk et al          these features however do not
relate to the melody of an sts clip as a whole  a melody
is made out of certain pitches with certain rhythms  but
the shape of the melody and the tension and release of
the melody help to make it sound good or bad  right or
wrong  the particular pitches and their placement create the melodic shape and the tension yet they are not
identical to shape and tension 
in order to capture the shape of the melody  i created
features like number of jumps and biggest jump  to
encode the level of tension  i created harmonic features
that indicated if the melody contained an implied tonic
harmony  dominant harmony  or other harmony because
those harmonies index the level of tension and resolution
within the melody 
the tree based on contour features shows that the
number of jumps within a melody matters  given that
the melodies were under     seconds  one can imagine that
it would be difficult to sing one if it had many large jumps 
as margulis et al  found  it is likely that listeners perceive the illusion more strongly when they can sing along
with the melody  margulis et al          the tree based
on harmonic features shows that the presence of destabilizing pitches  non diatonic pitches  is also important
in differentiating the transforming and non transforming
clips  these pitches make the underlying key less clear 
more work should be done  but these findings suggest
that context is important to the perception of the sts
illusion 

 deutsch et al         deutsch  d   henthorn  t   and lapidis  r 
        illusory transformation from speech to song  j  acoust 
soc  am                   

though many audio machine learning algorithms make
use of spectral features  or distributions of spectral features to classify audio  this application introduces a
unique dataset for classification where both classes of audio would  under normal circumstances  be called clean
speech 
based on the results  the feature set which best classifies the sts stimuli is the melodic contour feature set 
this suggests that our perceptual categorization of the
sts clips is closely tied to inherent tonal aspects of the
clips  in general good melodies tend to have smooth
contours  see root of the contour tree   melodies that
are easy to produce also tend to have smaller ranges
 see root of pitch tree   therefore oft repeated musictheoretic schema my help listeners perceive the sts illusion for those stimuli that are music theoretically wellformed  the role of speaking now needs to be disentangled from the role of context and rule following in this
perceptual phenomenon 

 falk et al         falk  s   rathcke  t   and bella  s  d         
when speech sounds like music  journal of experimental psychology  human perception and performance                 
 hymers et al         hymers  m   prendergast  g   liu  c  
schulze  a   young  m  l   wastling  s  j   barker  g  j   and
millman  r  e          neural mechanisms underlying song and
speech perception can be differentiated using an illusory percept 
neuroimage             
 james et al         james  g   witten  d   hastie  t   and tibshirani  r          an introduction to statistical learning with
applications in r  springer  new york   st edition 
 lee and ellis        lee  b  s  and ellis  d  p  w          noise
robust pitch tracking by subband autocorrelation classification  based on dissertation  columbia university 
 mandel and ellis        mandel  m  i  and ellis  d  p  w         
song level features and support vector machines for music
classification  in reiss  j  d  and wiggins  g  a   editors  international society for music information retrieval conference 
pages        
 margulis et al         margulis  e  h   simchy gross  r   and
black  j  l          pronunciation difficulty   temporal regularity   and the speech to song illusion  frontiers in psychology 
auditory cognitive neuroscience    article        
 nam et al         nam  j   herrera  j   slaney  m   and smith 
j          learning sparse feature representations for music
annotation and retrieval  in international society for music
information retrieval  number ismir  pages        
 scheirer and slaney        scheirer  e  and slaney  m         
construction and evaluation of a robust multifeature
speech music discriminator  in proceedings of the international
conference on acoustics  speech  and signal processing  ieee 
 slaney        slaney  m          auditory toolbox  version   
technical report  interval research corporation 
 thompson        thompson  b          discrimination between
singing and speech in real world audio  mit lincoln laboratory 
pages        

fireferences

 tierney et al         tierney  a   dick  f   deutsch  d   and
sereno  m          speech versus song   multiple pitch sensitive

 

areas revealed by a naturally occurring musical illusion  cerebral cortex            

fi