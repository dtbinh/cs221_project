how can machine learning help stock
investment 

dict stock performance  they found that even simple
neural learning procedures showed better prediction
accuracy than classic statistical techniques  e g   multiple linear regression  they also claimed that with
careful network design  model performance can be
further improved 

xin guo
email  guoxin stanford edu

 

introduction

the million dollar question for stock investors is
if the price of a stock will rise or not  the fluctuation of stock market is violent and there are many
complicated financial indicators  only people with
extensive experience and knowledge can understand
the meaning of the indicators  use them to make good
prediction to get fortune  most of other people can
only rely on lucky to earn money from stock trading  machine learning is an opportunity for ordinary
people to gain steady fortune from stock market and
also can help experts to dig out the most informative
indicators and make better prediction 

levin     designed a multilayer feedforward neural
networks to select stocks  he showed that his model
can make good prediction even if data is contaminated by large ratio of noise 

ghosn and bengio     also investigated artificial
neural networks to predict future returns of stocks 
with a serials of experiments  they concluded that
artificial neural networks have the best performance 
when the neural networks for different stocks do not
share any parameter or only share some parameters 
in another word  to get the best prediction  one always needs to train model specifically for each stock
the purpose of the present project is to investigate and there is no universal model for all the stocks with
the modeling of stock price movement trend and build the best performance 
up models to predict if the close price of a stock rises
tsai et al       examined the performance of clasor falls on the next trading day  the problem belongs
sifier ensembles on the prediction of stock return and
to a classification problem 
made comparison with single classifiers  i e   neuthe input to my algorithm includes     mov  ral networks  decision trees  and logistic regression 
ing average of historical close prices     trading vol  they studied the impact of different types of classifier
ume  and open  highest  lowest  and close prices of ensembles and majority voting and bagging  they
the present trading day     financial indicators  e g   concluded that in general  classifier ensembles perdecisionpoint price momentum oscillator  pmo   form better than single classifier 
money flow index  mfi   percentage price oscillaleung et al      compared the forecasting perfortor  ppo   and et al       and    self developed price
mance of classification models to predict the direction
movement trend based on local taylor expansion and
of index return and level estimation models to predict
spline fitting  totally  there are     features in the
the value of the return  they concluded that classifiinitial feature bag  i then trained logistical regrescation models always perform better than level estision  support vector machine  svm   and random
mation models  they also showed that the forecastforest models to predict the close price rises or falls
ing from classification model can be used to develop
on the next trading day  e g     means stock price
trading strategies for more trading profits 
will rise on the next trading day    means stock
  dataset and features
price will fall 
the data for the present project was downloaded
in this project  i used python  e g   scipy      and
from
quandl com through api  the initial data inmongodb     for calculation and data storage 
cludes daily stock open  highest  lowest  and close
  related work
prices  volume  dividend  and split ratio  i chose
stock prediction is a complicated and challenging to focus on   stocks with the longest data history 
problem  most researchers focus on stock selection i e   american airline  aa   general electric  ge  
problem and the prediction of stock return 
hewlett packard company  hpq   and international
refenes et al      applied neural networks to pre  business machines corporation  ibm   each of the
 

fistocks has       day data from january         to
october           for cross validation purpose  i
used the first     samples as training data set  and
the last     samples as validation data set 

    movement trend     the second method is to
use k means to classify the price difference into
two groups and label the group with large price
difference as   and the other group as   

the following steps were performed for data clean  here  the second method with k means model is
ing and normalization 
equivalent to find a new price difference boundary to
   i first cleaned the data to remove the entry with label the data  whereas the price difference boundincomplete or invalid information  for example  ary in the first method is    table   lists the pricesome entries have   volume meaning that on that difference boundary of the second method 

correlation between price difference and features

day  there is no trading at all  since the entries
with incomplete invalid information are few  the
cleaning should not have impact on the modeling  the data was saved into a mongodb  i e  
each stock as a collection and the trading information on one single day as a document 

    
    
    
    
    
    

   i developed a python code to calculate a se    
rial of technical indicators  e g   accumula    
tion distribution line  aroon  average direc    
     
tional index  bandwidth   b indicator  and et
  
   
   
   
   
feature id
al      the moving average of historical prices figure    correlation between features and price differand some self developed indicators are also cal  ence of aa 
culated  in total  there are     features in the
feature bag 
to understand the data  i checked the correlation
   since the magnitudes of features range from between the features and price difference  figure  
o      to o        to avoid the model is dominant shows this correlation of aa as an example  in genby large magnitude features  all the features are eral  there is no obvious strong correlation between
normalized with their mean and standard devi  features and price difference  for the result shown
ation as the following formula 
here  the correlation ranges from      to       most
of the correlation is negative and their magnitude is
f  mean value of f
f  
 
    about       the correlation between the features and
standard deviation of f
price difference of the rest stocks showed the similar
variation and range 
aa
ge
hpq
ibm
  model
quality
assessment
                           

method

table    price difference boundary from k means classification 

for two class classification problem  based on confusion matrix  i e   the number of true positive  tp  
true negative  tn   false positive  fp   and false negative  fn   many metrics are defined to assess the
performance of a model  however  most of metrics
are not reliable  for example  accuracy will yield misleading results if the number of samples in different
classes are quite different  f  scores only considers
tp but no tn 
in the present project  i used mattews correlation
coefficient  mcc  to assess the performance of mod 

since my objective is to predict price rises or falls 
to label data samples  i first calculated price difference by subtracting the present close price from the
next trading day close price  i labeled my data with
two methods 
    movement trend     the first method is to label
a sample as   if price difference is positive  i e  
the price raises on the next trading day and  
if price difference is negative  i e   the price falls 
 

fi a  linear    x  x   

els      mcc is defined as
m cc   p

tp  tn  fp  fn
 t p   f p   t p   f n   t n   f p   t n   f n  
   

as shown  mcc takes into account all the components in confusion matrix and is a general balance
measurement regardless the sample number variation
of different classes  mcc can be considered as a
correlation between predicted value and true value 
i e  

 

 c  rbf  exp  x  x    
 d  sigmoid  tanh    x  x     r 
   random forest classification model is an ensemble learning method for classification  random
forest model is to train several decision trees
with a random subset of features  feature bagging  and a random sample with replacement of
training sets      the prediction of random forest models takes the average of all the decision
tree prediction or the majority vote of all the decision trees in the model  because random forest
model takes the average result  it decreases the
variance in decision trees prediction 

   m cc     means all predictions are right 
   m cc     means model prediction is no better
than random prediction 
   m cc     means no prediction is right 
therefore  the objective of the project is to find the
models with the highest mcc value 

 

 b  polynomial      x  x     r d

 

feature selection

models

as introduced in sec     there are     features
in
the initial feature bag  i wanted to know if all
in this project  i applied logistical regression  svm 
and random forest models  due to the space limita  the features are important and if i can use only subset of features without losing prediction accuracy  in
tion  i briefly introduce them below 
the present project  i applied two feature selection
   logistical regression is a linear model for classitechniques  i e   random forest feature selection and
fication  the hypothese is written as
forward search 
 
h  x   
 
        random forest feature selection
    et x
random forest model can provide ranking scores
to improve the performance of the model  two for features  the larger the ranking score is  the more
cost functions are considered  i e   l  penalized important the feature is  to obtain the ranking sore
cost function 
of a feature  one needs to  first  obtain the average
value of out of bag error  second  permute the feature
 
min t    cni   log exp yi t xi             among the training data and obtain a second out of c  
bag error  and the ranking score is proportional to the
and l  penalized cost function 
difference between the two out of bag errors 
min kk    cni   log exp yi t xi        
 c

   
    

   for svm model  since the sample set is not linearly separable  the primal problem is 
validation mcc

 
min wt w   cni   i  
 

w b 

   

subject to yi  wt  xi     b     i   i     i  
        n  its dual is
 
max i i  i j yi yj k xi   xj   

 
yt 

aa
ge
hpq
ibm

    
    
    
    

    
     

  

   

feature no 

   

   

   

    figure    variation of validation mcc values when fea 

tures are added gradually in the order of feature ranking
        i  c  i           n  from random forest model 

subject to
here  k xi   xj      xi  t  xj    i considered the
to check if all the features are important  i gradfollowing kernel functions 
ually added features into feature bags according to
 

fitheir ranking scores  important feature is added
first   trained the model  and obtained the mcc
value of the validation sample set  figure   plots
the variation of validation mcc value when less important features are gradually added to the feature
bag  it shows that in general  the validation mcc
value first increases as the feature number increases 
with some magic feature combination  the validation
mcc value reaches its maximum  and then the validation mcc value decreases as the feature number
increases  comparison among the results of all the
stocks shows that when the validation mcc value
reaches its maximum  the feature combination is different for different stocks  for example  ge and hpq
only needs less than    features to reach the maximum validation mcc value  whereas aa and ibm
needs much more than that  based on this result  i
chose the feature combination with the best validation mcc value to further train and finetune models 
the feature number of the best candidate of each
stock is listed in table   and table   for movement
trend   and movement trend    respectively 

mcc value of different stocks reaches its maximum
with different feature combinations  for each stock 
i chose the feature combination with the best validation mcc value to further train and finetune models 
the feature number of the best candidate of each
stock is listed in table   and table   for movement
trend   and movement trend    respectively 

 

model training and result discussion

in this section  i explained my procedure to train
model and discussed the prediction result 

   

grid search and model training

when i trained models  i applied grid search by
searching a parameter space to find the model with
the best performance  for example  for logistical regression  as introduced in sec     there are two types
of cost functions and a parameter c  i trained model
candidates on two grids  one is with l  penalized cost
function and c values in                               
and the second one is with l  penalized cost function and c values in                                 af    forward search
ter the best model candidate is found  i kept the cost
forward search is a feature selection algorithm to function unchanged and search c in a smaller range
reduce the number of features  the search procedure around the c value of the best candidate  and reof the forward search is introduced in lecture note peated this process until the validation mcc value
and is not repeated here 
does not change much in the neighborhood of the
current best c value  similar search procedure is
also applied to svm and random forest 
    
more specifically  for each stock  i first used all the
features and trained logistical regression model with
l  penalized cost function and c      this model is
considered as a baseline model  then i applied grid
search technique to find the model with the best performance  the best model candidates by using all the
features  features selected by random forest  and features selected by forward search are obtained 

    

validation mcc

    
    
    
    
    
    
     

aa
ge
hpq
ibm
  

   

   

feature no 

   

   

   

figure    history of the best validation mcc values in
forward search with logistical regression 

result and discussion

table   and table   list the result of the baseline model and the best model for the sample set labeled by movement trend   and    defined in sec     
respectively  the best models trained with all the
features  feature selected by random forest  and feature selected by logistical regression forward search
are listed 

in the present project  i applied forward search
with logistical regression  figure   plots the history
of the best validation mcc values in the forward
search  in general  as the feature number increases 
the best validation mcc value increases dramatically
first  varies slowly and reaches a maximum value in
comparing to the baseline model  model perforthe middle  and decreases rapidly when the feature
mance
has been improved significantly with the help
bag is almost full  similar to the random forest feature selection result shown in sec       the validation of grid search and feature selection techniques  for
 

fiaa
ge
hpq
ibm

baseline
val  mcc
       
       
      
     

all features
model
best val  mcc
rf
     
svm  lin  
     
lr
     
lr
    

model
svm  rbf 
svm  rbf 
svm  lin  
svm  rbf 

rf feature selection
feature no 
best val  mcc
   
     
  
     
  
     
   
    

forward search  lr 
feature no 
best val  mcc
   
     
  
     
  
     
   
     

table    baseline and best models for the movement trend   with feature numbers and best validation mcc 
aa
ge
hpq
ibm

baseline
val  mcc
      
     
    
     

model
lr
lr
lr
lr

all features
best val  mcc
     
    
    
    

model
svm  sig  
lr
lr
lr

rf feature selection
feature no 
best val  mcc
  
     
  
    
   
    
  
    

forward search  lr 
feature no 
best val  mcc
   
    
  
    
  
    
  
    

table    baseline and best models for the movement trend   with feature numbers and best validation mcc 

example  for ge in table    the validation mcc value
increases from        baseline  to        note that
negative mcc value means the model prediction is
worst than random prediction  

   

train
validation

   
   

  mcc

   

comparing the baseline model with the best model
using all the features of hpq and ibm in table   and
all the stocks in table    we can see the grid search
technique helps to find better model and largely improves model performance  i also found that the best
model candidate is much different among the four
stocks  in table    even if all the best models using
all the features are logistical regression  their actual
model forms  i e   cost function and c value  are quite
different  the information on detailed model form is
omitted here due to space limitation   this result
is consistent with previous research that to have the
best prediction  one needs to train models for each
stock instead of train one model using the data of all
stocks     

   

   
   
    

    

    

    

train data no 

    

figure    learning curve of the best logistical regression
model for ibm 

those metrics are omitted here  due to space limitation   grid search and feature selection techniques
improve not only mcc value but also other metrics
derived from confusion matrix 

in figure    i showed the learning curve of the best
logistical regression model for ibm with samples labeled by movement trend    the training and valiin general  the best models with features selected dation error converges to a similar value  if we want
by random forest and forward search have better per  to further improve the model performance  we need
formance than those with all the features  that more informative features 
means each stocks may be only sensitive to certain
feature combination  the models in table   have bet    summary and future work
in summary 
ter performance than those in table    hence  a good
data label algorithm is important to the prediction
   models with best validation mcc are built up
accuracy 
based on current feature set with the help of grid
search  random forest feature selection  and formetric
baseline model
best model
ward search techniques  mcc and other metrics
f 
     
     
are significantly improved 
recall
     
     
precision
accuracy

     
     

     
     

   stock prediction is quite feature and stock dependent  different feature subsets and different
models are best for different stocks 

table    validation f  score  recall  precision  and accuracy of the baseline and best model of aa in table   

   a good classification model to label sample may
help to increase prediction accuracy 

to further show model performance improvement 
in table    i listed the validation f  score  recall  precision  and accuracy of the baseline and best models
of aa in table   as an example  the definitions of

   for more accurate prediction  more features are
needed to provide more useful information 
 

fireferences
    mongodb  https   www mongodb com 
    scikit learn  http   scikit learn org  
    technical indicators and overlays  http   stockcharts com school doku php id chart school 
technical indicators 
    breiman  l  random foests  machine learning                
    ghosn  j   and bengio  y  multi task learning for stock selection  in nips       
    leung  m  t   daouk  h   and chen  a  s  forecasting stock indices  a comparison of classification
and level estimation model  international journal of forecasting                   
    levin  a  u  stock selection via nonlinear multi factor models  in nips       
    matthews  b  w  comparison of the predicted and observed secondary structure of t  phage lsozyme 
biochimica et biophysica acta                    
    refenes  a  n   zapranis  a   and francis  g  stock performance modeling using neural networks 
a comparative study with regression models  neural networks                  
     tsai  c  f   lin  y  c   yen  d  c   and chen  y  m  predicting stock returns by classifier
ensembles  applied soft computing                     

 

fi