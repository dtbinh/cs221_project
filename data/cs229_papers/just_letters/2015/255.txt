handwritten english alphabet recognition using bigram cost
chengshu  eric  li
chengshu stanford edu
fall       cs     stanford university
abstract  this paper describes a new approach to handwritten english alphabet recognition 
namely using bigram cost between english characters to improve performance       images     
for each of the    uppercase and lowercase english letters  are obtained from nist database    and
are preprocessed to be fed into models like softmax classification  nave bayes  support vector
machine and feed forward neural network  by using bigram cost  the performance of svm and feed
forward neural network is improved by about three percent 
keywords  handwritten character recognition  image processing  feature extraction  feed forward
neural networks  support vector machine  nave bayes  bigram cost  search problem

 

introduction

optical character recognition  ocr  is one of
the most fascinating and successful
application of automatic pattern recognition 
in the past a few decades  ocr has been an
active field for research in artificial
intelligence ocr also has a wide range of
real world application such as business card
information extraction  book scanning 
assistive technology for blind and the
automatic processing of invoices  receipts and
legal documents
my motivation for this project is based on the
intuition that when we as humans try to
recognize a character in a word  we also use
the context information to help us do that  in
fig     for example  we can see the letter r
looks more like an i rather than an r 
however  we as humans will know almost for
sure it is a r based on the letters around it 
therefore  i try to experiment combining
normal image processing algorithm and
bigram cost between english characters

figure    an example of using context
information to recognize a character

to see whether this synergy could the test
performance 
the input for my algorithm is     by    
pixels image that contains one english
uppercase or lowercase letter  i then used
softmax classification  svm  nave bayes 
neural network to output a predicted letter that
this image represents 
the baseline that i used to evaluate my result
is to run svm on raw pixels for the     of
my images and test on the other     which
gave an error rate of         only a little bit
better than a random guess  the oracle i am
aiming for is human recognition  my friends
and i manually attempt to recognize      
characters and got an error rate of      
i share this project with cs     most of my
work could be shared between both cs   
and c    such as data gathering and preprocessing  feature extractions 
experimentation with different models and
algorithms test results and error analysis 
however  i got my inspiration for this project
from the cs    assignment reconstruct
where we used bigram cost to solve vowel
insertion problem  i modified and adopted
some of the code from that assignment and

fiand run my test based on a state based search
model 

 

related work

in the past decades a large number of volumes
have been dedicated to character recognition 
rachana herekar and prof  s r  dhotre 
proposed a novel way of feature extraction
using zoning  which i have tried myself  see
details below  and achieved an error rate
about            j pradeep  e srinivasan and
s himavathi   on the other hand  used
vertical  horizontal  and diagonal method for
their feature extraction  moreover  yusuf 
perwej and ashish chaturvedi  extracted their
features by putting a    segment grid on top
of the image that produced a feature factor of
dimension     which i have also tried
myself   all three papers use different feature
extraction methodology but all use feed
forward neural network to train their models 
which presumably gave them the best result
during their experimentation  
on the other hand  v k  govidan and a p 
shivaprasads paper  offers a more qualitative
and general review of character recognition 
such as some of the recurring challenges in
the field and the previous attempts to
overcome them  lastly  charles c  tappert
and sung hyuk cha  touched upon online
handwriting recognition  which is not
necessarily my research topic but they raised
several interesting insights in how people put
down strokes all of the papers above has
been extremely helpful for me to understand
previous attempts and come up with
reasonable feature extraction methods 

 

data preprocessing

my dataset comes from nist database    
and consists of       samples  that is        
by     pixels images for each of the   
uppercase and lowercase english alphabet 

i preprocessed the image by first cropping out
the central part which actually contains the
character and then resizing it back to     by
    pixels fig    below illustrates the
preprocessing process for a sample h 

figure    preprocessing a sample h

 

feature extraction

five different feature extraction methods have
been experimented 
i  raw pixels  use raw pixels from the
preprocessed image      by     pixels   each
pixel is either black           or white      
           the feature vector is of dim       
ii  grid threshold method   put a grid of   
by    onto the image  for each region  loop
through all the pixels and check whether more
than threshold percentage of them are
black pixels  if so  put a   onto that region 
otherwise  put a zero  please see figure   for
an illustration of a grid of   by    the feature
vector is of dim      for a grid of    by    

figure    grid threshold method
iii  grid percentage method  similar to ii  
instead of setting a threshold directly use the
percentage of black pixels as a field in the
feature vector  the feature vector is of dim 
   
iv  zoning   put a grid of   by   onto the
image  and classify each zone to one of the

fisix different types 

feature vector of dimension        raw
pixels       loop detection          
during training  i tried to minimize the log
likelihood function shown below by taking
the gradient with respect to eachi and using
stochastic gradient assent to update eachi 

figure    six types for each zone
v  loop finding  use breadth first search to
figure out whether there is a loop in the
image  for example  if the image represents
b  this feature should be    if the image
represents m  this feature should be    the
algorithm itself has an error rate of about    

figure    log likelihood function for softmax
the algorithm ran very slowly and gave an
error rate of over      so i eventually gave it
up 

after testing all these features using svm
with       training examples and       test
examples  a combination of i  and v  are
chosen to be my final feature choices as they
perform the best 

i downloaded svc package from scikit learn
library and used it for multi class
classification  svm is originally designed to
do binary classification  i e  drawing a line to
separate the data into two classes  scikit learn
implement svc in the one against one
approach  if n is the number of classes  then
n  n        classifiers are constructed and each
one of them categorizes training data into two
classes   the result is then aggregated 

  methods and algorithms
four different learning algorithms have been
experimented
a  self implemented softmax classification
i implemented from scratch a softmax
classification algorithm based on the last
section of lecture note   

figure    mathematical notation for softmax
y can anything from   to     which represents
a  b  c   z  a  b   z  x is the

b  support vector machine

c  nave bayes
i downloaded nave bayes package from
scikit learn library and used multinomialnb
for multi class classification  for each epoch
the algorithm is updating
where
represents the probability
of feature i appearing in a sample belonging
to class y   by default it also uses laplace
smoothing of alpha    
d  feed forward neural networks
i downloaded pybrains library and tried one
hidden layer of                 neurons

fi  advanced method  for cs    
inspired by the cs    assignment
reconstruct  in which we used bigram cost to
solve vowel insertion problem based on a
search problem model  i decided to test the
results of my character recognition in the
context of a english word 
first of all  i collected      most common
english words  and then randomly sampled
letters from my hand printed data to form
those words  these become my test sets 
secondly  i generated a probability table for
english characters by using
english bigram   txt found on github   
each line is the frequency of that two letter
combination apparently  th is the most
frequent combination thanks to the word the
and he is the second thanks to the word he
and she  based on these frequencies a
probability table is generated  where
table a  b  is the probability that b comes
after a as opposed to other letters that come
after a

  
figure    english bigram   txt

have curchar coming after prevchar  the
second term measures how likely the image
represents currentchar solely based on its
physical form by changing  and weighing
these two terms differently  the purpose is to
investigate whether adding bigram cost will
improve the performance of character
recognition in a more realistic context where
the model will classify the test image not only
based on how it looks  but also the previous
character and the bigram cost between them 

 

experiments and results

svm and nave bayes are trained on all
      examples whereas due to the
computational limit  i could only train my
self implemented softmax classification
algorithm on      examples and feed forward
neural network on      examples within
reasonable amount of time  so when reading
this paper please bear in mind that the result
may favor svm and nave bayes to the other
two 
the x axis is  and the y axis is the error rate 
     means the model is purely using the
images physical form to classify image 
whereas        means the model is weighing
bigram cost from the previous character
higher than how the image actually looks like 
lastly  when calculating error rate  the
uppercase and lowercase letters of the
following are treated as the same category 
  c    i    j    k    o    p    s    u    v    w    x    y    z  

the last part is putting everything together
into a state based search problem each state
encapsulates the current feature vectors and error  rate  in   
    
the previous character chosen  in each state 
predict the current character using already
    
trained svm nave bayes neural network
and pick the top five predictions to be the next
    
possible actions  for each of the actions  the
    
cost is      times the following
bigramcost prevchar  curchar   
    
     p y   curchar   x 
   
the first term measures how fluent it is to
   

svm  

nave  
bayes  


                       

neural  
netwo
rk  

finave bayes is hardly affected by the bigram
lastly  for feed forward neural network i
cost at all  svm performs the best when   
found that the larger the number of neurons
     with error rate         when      
there is in the hidden layer  the better
meaning bigram cost is not used  the error rate
performance it can achieve 
is         similar trend happens for neural
neural  network
network as well  therefore bigram cost did
error  rate  in   
help drive down the error rate
     
since the confusion table is too large to fit
into this page to be legible  i will include a
table in which the rows represent all the
letters and the columns represent the top five
letters that this letter has been classified to 

    
    

neural  
network  

    
    
   
   

 

                

   of  neurons  in  
the  hidden  layer

conclusion and future work

some of the key observations are the
following  first of all  the bigram cost method
did improve the performance of the model  by
around     although the improvement is
expected to be larger  other observation is
that some letters are much more difficult to
classify than others  the bottleneck letters are
highlighted in the table on the left 
lastly neural network does not perform as
well as expected  maybe because the number
of neuron being used is still very small 
some potential work includes finding better
feature extractions method to further improve
svm  nave bayes and neural networks
performance  reducing error rates for those
bottleneck letters  furthering experimenting
with neural network by increasing its neuron
numbers and hidden layers 

fi 

reference

    herekar  rachana r   and prof  s  r dhotre   handwritten character recognition based
on zoning using euler number for english alphabets and numerals    i iosrjce iosr
journal of computer engineering  i                      web 
    pradeep  j   e  srinivasan  and s  himavathi   diagonal based feature extraction for
handwritten alphabets recognition system using neural network   international journal
of computer science and information technology ijcsit                    web 
    perwej  yusuf  and ashish chaturvedi   neural networks for handwritten english
alphabet recognition   international journal of computer applications ijca             
     web 
    mantas  j   an overview of character recognition methodologies   pattern recognition     
                
    tappert  charles c   and sung hyuk cha   english language handwriting recognition
interfaces    i text entry systems  i                  web 
     nist special database       i nist special database     i   n p   n d  web     dec 
     
          support vector machines        support vector machines  scikit learn     
documentation  n p   n d  web     dec       
     welcome to pybrains documentation   welcome to pybrain s documentation  
pybrain v    documentation  n p   n d  web     dec       
    deekayen         most common us english words   gist  deekayen  n d  web     dec 
     
    padraigmaciain   padraigmaciain textual analysis suite   github  padraigmaciain  n d 
web     dec       

fi