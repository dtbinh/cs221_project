 

predict employees computer access needs in
company
xin zhou   wenqi xiang
email  xzhou   wenqi stanford edu
  department of computer science   department of electrical engineering

abstractwhen an employee is hired by a company 
given his her job role  model can be built to predict
an employees access needs  this auto access model tries
to minimize the human labor and time for granting or
revoking employee access  the input to our algorithm
is information about the employees role at the time of
approval  we then use logistic regression  naive bayes
and support vector machine  svm  models to output
a predicted action for a specific resource  while  due
to the unique characters of our data set  with only   
samples labelled as    simply applying models like logistic
regression  naive bayes and svm can not get a satisfactory prediction on this unbalanced dataset  alternatively 
we use the one hot encoder to preprocess the data before
applying different models and achieve a much higher
accuracy  around      in labelling computer access needs 
keywordsunbalanced prediction  logistic regression 
svm  naive bayes  one hot encoder  cross validation 
greedy feature selection 

i 

i ntroduction

when an employee starts work at a company  he or
she first needs to get access to computer resources  like
applications and web portals  to fulfill their role  during
their daily work  employees may encounter roadblocks
when trying to get access they need but not granted 
 e g  not able to log into a reporting portal   in order to overcome access grant obstacles  it takes time
to manually grant the needed access  especially when
employees move throughout a company  regranting their
access needs wastes a nontrivial amount of time and
money  to solve this problem  its important to build a
model on historical data to automatically determine an
employees access needs  which minimizing the human
involvement required to grant or revoke employee access 
our project uses different machine learning models to
achieve this goal and compare the performance among
these models 
ii 

related work

the unbalance problem can cause suboptimal classification performance      which is pervasive and prevalent

in many applications including risk management  text
classification  and medical diagnosis monitoring  in recent years  lots of researchers have made efforts to solve
this problem both at the data and algorithmic levels 
at the data level  different forms of resampling are
applied  batista et  al     compared different sampling
strategies and their combinations  they present that it is
applicable to combine focused over and under sampling 
on the other hand  using recognition based instead of
discrimination based learning  adjusting the costs of the
various classes and changing the probabilistic estimation
at the tree leaf in the decision tree are all good ways
related to algorithmic level  b  zadrozny and c  elkan   
gave a more flexible and direct method to the treatment
of different parts of the decision space by adjusting the
probabilistic estimation within cost sensitive learning 
additionally  more accurate measures such as receiver
operating characteristic curves  cost curves                 
and g means metric    are applied  whats more  existing
feature selection methods are not very applicable for
unbalanced data sets  instead  zheng and srihari     proposed a new feature selection approach  which separately
selects features for positive and negative classes and then
combines them 
iii 

dataset and features

a  original dataset
the
data
from
kaggle
website
 https   www kaggle com  consists of real historical
data collected from              there are around
      samples in our data set  we use     of them
for training  and the rest for testing  employees are
manually allowed or denied access to resources over
time  that is to say  action  y  is binary  to be   if the
resource was approved or   if the resource was not 
for each employee  there are   non negative numeric
features  which are 
    role rollup  
     resource
     manager id
     role rollup        department name     role title
     role family description      role family     role code

fi 

for instance  employee a has the following
information  action 
resource 
manager id 
role rollup   
role rollup   
department name 
role family description 
role family 
role title 
role code  is                                  
                                           means
that the employee can use resource        other
numbers stand for information about the employee 
there are many copies for each feature  which is corresponding to the real world  for example  two employees
may have different role title  but both are in the same
department  then they have the same department name 
b  unbalanced dataset
unbalanced dataset means that  in a classification
problem  instances of some classes are extremely more
than others  in our dataset  there are     of samples
labelled to be    and only    of them labelled to be
   the unbalanced datasets make classifiers generally
perform poorly on prediction because they are based on
training examples and output the simplest hypothesis that
best fits the data      that is to say  the simplest hypothesis
is often the one that classifies almost all instances as
positive  whats more  for our project  the negative
instances can be treated as noise and then be ignored
completely by the classifier  a popular approach towards
solving these problems is to bias the classifier  which
pays more attention to the negative instances  this can be
done  for instance  by increasing the penalty associated
with misclassifying the negative class relative to the
positive class  another approach is to preprocess the data
by oversampling the majority class or undersampling the
minority class and then to create a balanced dataset  for
our project  these data preprocessing methods can not
provide a satisfactory performance  so we apply another
data preprocessing approach which significantly improve
the train and test accuracy 

equal to the value of the largest number  namely     we
get its sparse matrix as follows 


















 
 
 
 
 
 
 
 
        
    

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 



















   

many machine learning algorithms learn a single
weight per feature  assume a dataset only contains a single categorical feature department  with values marketing  services and inventory  like our dataset 
without loss of generality  they are encoded as      and   
it then has a weight w for this feature in a linear classifier 
which will make a prediction based on w  x   b      or
equivalently w  x   b 
the problem now is that the weight w cannot encode a
three way choice  the three possible values of w  x are
   w and  w  either these three all lead to the same
prediction or services and marketing lead to the
same prediction  or marketing and inventory have
the same prediction  its no possible for the model to
learn that services and inventory should be given
the same label  with marketing the odd one out 
one hot encoder effectively blows up the feature
space to three features in this case  and each of them
will get their own weights  so the decision function is
now w m arketing   x m arketing    w services  
x services    w inventory   x inventory    b  where
all the xs are booleans  in this space  such a linear
function can express any sum disjunction of the possibilities  e g   marketing or services   which might be
a predictor for someone having access    
after one hot encoder  we apply logistic regression 
naive bayes and svm to make the prediction 

c  one hot encoder
due to pretty low percentage of samples labelled as   
building model on the original dataset will get very high
and constant test accuracy  around       but its useless
because it never makes a prediction to be labelled as   
to tackle this problem  one hot encoder is applied to do
the data preprocessing before using any models 
one hot encoder generates a sparse matrix for each
feature  for example  all values for a certain feature may
be                              and the largest number is    
our feature vector is                      t   and
the number of columns for its sparse matrix would be

iv 

methods

a  feature section  greedy feature selection
it is reasonable to suppose that there is only a small
number of features that are relevant  to avoid the potential overfitting problem for our prediction  we can use
forward search combined with a certain cross validation
to do the feature selection  while for our learning task 
we decide to use another approach of feature selection 
the dataset includes   features  and each of them
corresponds to different resources  departments  and etc
to an employee  in reality  different features have inner

fi 

relationship to each other  the next step is to find the
optimal feature combination to build the model  we use a
greedy feature selection algorithm to achieve this goal  in
the greedy feature selection algorithm  for each iteration
of prediction  we apply the cross validation algorithm
to the training data  where     of data examples are for
training and the rest are for testing  to evaluate the current
feature sets  firstly  we choose each of these   features
to do prediction and estimate the test accuracy over ten
times  then  we compare the   test accuracies and choose
the feature whose test accuracy is the best one  this
chosen feature is combined with the other eight features
one by one to do the prediction again  these   pairs
of features are paralleled to   prediction accuracies and
again we choose the pair corresponding to the highest
accuracy  similarly  we use this chosen pair to combine
with the rest   features to get the combination of three
features  eventually  we can get the best combination of
features 

here    l    is the the vector of partial derivatives
of l      h is hessian matrix  an n by n matrix  if we
include the intercept term  h will be an  n      by  n  
   matrix  where 
hij  

   l  
i j

   

d  svm
we use svm to make the prediction 
this
is
implemented
by
skilearn svm svc 
a
library
offered
online
at
http   scikitlearn org stable modules generated sklearn svm svc html 
this implements an svm using a linear kernel  the
optimization is as follows 

min wb
b  cross validation

 
 

kwk    c

pm

i   i



we use k fold cross validation during each iteration
of the greedy feature selection algorithm  where k      
the process is as follows  firstly  we randomly split the
dataset into k disjoint subsets of m k training examples
each  lets name these subsets as s            sk   secondly 
for each model mi   we evaluate it as follows  for j  
s
s
           k  train the model mi on s 
sj
sj  
s
sk  i e   train on all the data except sj   to get some
hypothesis hij   then  test the hypothesis hij on sj to get
sj  hij    the estimated generalization error of model
mi is then calculated as the average of the sj  hij  s
 averaged over j   finally  pick the model with the lowest
estimated generalization error  and retrain that model on
the entire training set s  the resulting hypothesis is then
output as our final answer 
c  logistic regression

s t y i wt x i    b      i   i         m
i    i         m

e  naive bayes
in our prediction task  inputs are discrete valued and
the prediction is a classification problem  so  we can
apply naive bayes  naive bayes methods are a set
of supervised learning algorithms based on applying
bayes theorem with naive bayes  nb  assumption of
independence between every pair of features  given a
class variable y and a dependent feature vector xi through
xn naive bayes theorem can be stated as follows 
p  y x    x       xn    

this project is a classification problem  and we use
logistic regression to build the model  we choose
sigmoid function as the hypothesis 


t



y   h  x    g  x     xr

 

   

given the logistic regression model  we use newtons
method to fix   the updated rule for  is 

   

p  y  p  x    x       xn  y 
p  x    x       xn  

   

using the naive independence assumption that 
p  xi  y  x       xi    xi        xn     p  xi  y 

   

for all i  this relationship is simplified to 
p  y  ni   p  xi  y 
p  x         xn  
q

p  y x         xn    

   

since p  x         xn   is constant given the input  we can
use the following classification rule 
      h     l   

   
p  y x    x       xn    p  y 

n
y
i  

p  xi  y 

   

fi 

so  we can get that 
y   argmaxy p  y 

n
y

p  xi  y 

    

i  

and we can use maximum a posteriori  map  estimation
to estimate p  y  and p  xi  y   the former is then the
relative frequency of class y in the training set  the
different naive bayes classifiers differ mainly by the
assumptions they make regarding the distribution of
p  xi  y 
in spite of their apparently over simplified assumptions  naive bayes classifiers have worked quite well in
many real world situations  famously document classification and spam filtering  they require a small amount
of training data to estimate the necessary parameters 
bernoulli nb implements the naive bayes training
and classification algorithms for data that is distributed
according to multivariate bernoulli distributions  and
it requires samples to be represented as binary valued
feature vectors  the decision rule for bernoulli naive
bayes is based on 
p  xi  y    p  i y  xi       p  i y       xi  

v 

    

e xperiments  r esults  d iscussion

using accuracy as a metric to evaluate different classifiers on highly unbalanced dataset is virtually useless 
for example  an unbalanced dataset with    samples to
labelled    y    and   samples to be labelled    y      
a classifier that classifies everything to be   will be    
accurate  but it will be completely useless as a classifier
since it never really make a prediction  to use a more
reasonable evaluation on the performance of various
models  two metrics are introduced  the sensitivity and
the specificity  for our prediction task  sensitivity can be
defined as the accuracy on the negative instances  y   
 true negatives    true negatives   false positives    while
specificity can be defined as the accuracy on the positive
instances  y     true positives    true positives   false
negatives    kubat et al     suggested the g means metric
defined as 

    
g   acc   acc    
here  acc  is sensitivity and acc is specificity  this
metric has been used by several researchers for evaluating classifiers on unbalanced datasets        we will also
use this metric to evaluate our classifiers and compare
the performances of logistic regression  naive bayes
and svm 
after using one hot encoder to preprocess our unbalanced dataset  logistic regression model is applied
to test the specificity  sensitivity and g means  the table

fig    

table results for logistic regression

fig     table results for logistic regression with combined good
features

results for logistic regression model in figure   show
seven metrics for both the training data and the testing
data  true negative  tn   true positive  tp   false negative
 fn   false positive  fp   the specificity  the sensitivity 
and the g means 
as we see  the specificity is pretty high  above      
for both the training dataset and the testing dataset 
but the sensitivity is much lower  around      for the
testing dataset  since the number of false positive is much
larger than the number of true negative based on our
unbalanced dataset  even though in this case  accuracy
for the true negative is improved to      by the model 
the sensitivity can only achieve to      for the testing
dataset 
the dataset contains   features and each feature
corresponds to the employees relative information in
the company  so the mutual relationship among these
features can influence the computer access  to optimize
forward feature selection algorithm  we use a greedy
feature search algorithm which searches a single good
feature  then a combination of two good features  and
so on  furthermore  for each iteration  we use the cross
validation algorithm to evaluate the current feature sets 
the table results using logistic regression with the good
features combination for the same seven metrics is shown
in figure    it indicates after using greedy feature search
algorithm  it slightly improves the performance 
we also apply naive bayes model to our preprocessed
dataset  and the table results for the same seven metrics
are shown in figure   and figure    the g means
is decreased when only applying naive bayes model
compared to logistic regression model  but increased
when using naive bayes model with combined good
features 
to further evaluate the performance of our models 
we also plot the accuracy for both the large sample set
 y     small sample set  y     and the overall accuracy 
these three accuracy metrics for both logistic and naive

fi 

fig    

table results for naive bayes

fig     table results for naive bayes with combined good features

fig    

performance for svm

accuracy metrics are show in figure    the x axis shows
different c regularization parameters we choose for the
model  it shows the g means is increased up to     
which is higher then the previous two models  but the
test accuracy for the small sample set is decreased to
        and the test accuracy for the large sample set
is almost achieved to       the kernel for this figure
is radial basis function  rbf   and we also test other
kernels which generate similar results  not shown here  
vi 

fig    

accuracy results for logistic regression and naive bayes

bayes models are shown in figure    as we see  when
using logistic regression model  the training accuracy
for the large or small sample set is pretty high  above
      and the test accuracy for the large sample set is
around      but only around     for the small sample
set  the overall test accuracy is around     
compared to test accuracy results in logistic regression model  test accuracy for the small sample set
 around      is much lower when using naive bayes
model even though the test accuracy for the large sample
set is slightly increased to     
lastly  we apply the svm model to our preprocessed
dataset  the performance including g means and the three

c onclusion  f uture w ork

simply applying models like logistic regression 
naive bayes and svm can only achieve low test accuracies  in our project  they only predict   or randomly
predict   or   that is shown in the milestone report  in
an unbalanced set and that the use of on hot encoder
to preprocess dataset can greatly improve their performance  from the above results  we can conclude that the
best overall test accuracy we can achieve is around    
when using the svm model  g means      is also the
highest in this case   but the best test accuracy for the
small sample set is only around      to balance the
test accuracy between the large and small sample sets 
the logistic regression model with combination of good
features give us the best prediction  the test accuracy
for the small and large sample set is around     and
    respectively  the overall test accuracy is achieved
to      to further improve the test accuracy  the future
work we can do is combining two three more features
together and hash them to be a single new feature  in
this way  it considers some features together to make a
prediction which indicates in reality  some information
together about the employee is better to predict the
computer access 
r eferences
   

akbani r  kwek s  japkowicz n  applying support vector
machines to imbalanced datasets m   machine learning  ecml
      springer berlin heidelberg              

fi 

   

   

   

   

   

   

   
   

    

    

    

aha  d          tolerating noisy  irrelevant and novel attributes
in instance based learning algorithms  international journal
man machine studies              
kubat  m  matwin  s          addressing the curse of imbalanced training sets  one  sided selection  proceedings of the
  th international conference on machine learning 
kubat  m   holte  r  matwin  s          learning when
negative examples abound  in proceedings of ecml      th
european conference on machine learning 
chawla n v  japkowicz n  kotcz a  editorial  special issue
on learning from imbalanced data sets j   acm sigkdd explorations newsletter                  
batista g e  prati r c  monard m c  a study of the behavior of
several methods for balancing machine learning training data j  
acm sigkdd explorations newsletter                    
zadrozny b  elkan c  learning and making decisions when
costs and probabilities are both unknown c   proceedings of the
seventh acm sigkdd international conference on knowledge
discovery and data mining  acm                
provost f  fawcett t  robust classification for imprecise environments j   machine learning                       
t  fawcett  roc graphs  notes and practical considerations for researchers  http   www hpl hp com personal tom
fawcett papers  index html       
c  drummond and r  holte  explicitly representing ex  pected
cost  an alternative to roc representation  in proceedings of the
sixth acm sigkdd international conference on knowledge
discovery and data mining  pages              
j  furnkranz and p  flach  an analysis of rule evalua  tion metrics  in proceedings of the twentieth interna  tional conference
on machine learning  pages              
z  zheng  x  wu  and r  srihari  feature selection for text
categorization on imbalanced data  sigkdd ex  plorations 
                

fi