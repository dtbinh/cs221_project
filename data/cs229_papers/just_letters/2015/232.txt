neighborhood and price prediction for san francisco airbnb
listings
emily tang
departments of computer science  psychology
stanford university
emjtang stanford edu

 

kunal sangani
department of economics
stanford university
ksangani stanford edu

introduction

since its founding in       airbnb has become a symbol of the sharing economy and has changed the way people travel  as
of       the site advertises over     million listings in        cities around the world  in this study  we focus on listings in
san francisco  where the company first started 
airbnb listings not only give us a window into how participants in the new sharing economy market their offerings  but also
give us a unique insight into the characteristics of a city and its neighborhoods  san francisco in particular is known for its
diverse neighborhoods and cultural enclaves  and the citys airbnb listings give us an opportunity to catalogue similarities
and differences between neighborhoods 
using the inside airbnb projects complete set of listings available in san francisco  we develop classifiers able to predict
a listings neighborhood and its price  we choose these two outputs for their potential applications  predicting a listings
neighborhood gives us insight into the cultural elements visible through text  image  and amenities that might link neighborhoods together  and could potentially be applied for a recommendation system  e g  if you enjoyed your stay in the haight
ashbury  we recommend trying alamo square    and predicting the appropriate pricing of a listing  especially taking into
account features unique to the sharing economy of how hosts describe their offering  may be helpful in internal pricing tools
that airbnb offers to its hosts 
the inside airbnb project provides data on the complete set of listings available in san francisco  we input text features 
image features  and other information associated with each listing to an svm classifier to predict the     the neighborhood a
listing is in  and     the listings price  separated into discrete ranges   in the following sections  we explain the context for
this work  describe our dataset and methods for feature extraction and classification  and present an analysis of our results 

 

related work

as far as we are aware from our literature search  there are no published studies that apply machine learning techniques to
data from the inside airbnb project   the projects data has largely been used for visualizations of listing types in each city  
of a small number of papers that analyze airbnb data  a relevant paper is by lee et al       in which the authors find that
social factors like number of references  host response rate  and number of reviews  are even more important determinants
of room booking than conventional factors like price and amenities offered  the importance of such social factors suggests
that the ways in which hosts describe their listings and themselves may well be of large importance when determining price
visitors are willing to pay 
we also draw from the literatures on price prediction and neighborhood detection  one notable example of machine learning
used for neighborhood detection comes from a paper by zhang et al      that uses foursquare user check ins and semantic
information about places to detect neighborhoods in cities  the study finds for three urban areasnew york  san francisco 
and londonthat measuring homogeneity between places and the users frequenting them allows the authors to detect neighborhoods as clusters  and that the neighborhoods identified match qualitative descriptions of the cities neighborhoods  the
objective of the neighborhood classification part of our project is similar  in that we use data from several listings that fall
within neighborhoods to understand characteristics of each neighborhood and the diversity between them 
the literature on algorithmic pricing is more extensive  hill     explains airbnbs ai based dynamic pricing tools  discussing
how the original regression based tool released in       which used amenities of a listing and information about neighboring
properties to predict the appropriate pricing  was refashioned into the companys most recent  reinforcement learning based
 

fitool  aerosolve  the tool uses microneighborhoods to inform pricing and uses user booking as a feedback tool to improve
future pricing tips  our work is based on a freeze frame of data and so does not include any dynamic component  but the text
and image features we explore may add to the accuracy of airbnbs price tools as well as provide the basis for additional tools
on suggested listing marketing to property owners 
finally  we draw from the literature on machine learning techniques  these references are covered in the methodology section 

 

dataset

our
dataset
of
airbnb
listings 
available
    
includes
     
listings
posted
in
san
each listing contains text informationa name  general description  a host bio  and a description of the space offered 
neighborhood  and local transitas well as an image thumbnail     pixels per inch  usually    x     and several other
fields describing the amenities offered  number of bed  and
bathrooms  type of property  type of bed  square footage of
the space  etc  the per night price of each listing was also
included  as well as the neighborhood of the listing  of   
possible sf neighborhoods  and a cumulative review score 

through
francisco

the
as

inside
airbnb
of
november
  

project
     

figure    an example of an airbnb listing

in order to reduce the burden on our multiclass classifier for neighborhood prediction  we first pre process the data to exclude
listings that belonged to neighborhoods containing fewer than    listings     of the dataset   this reduces the number
of listings to      and the number of neighborhoods to     the remaining listings are split into train  dev  and test sets
           

 

feature extraction   methods

for each listing  we extract five sets of features      listing information features      multinomial bag of words features     
multinomial word class features      text sentiment features  and     visual features 
listing information features include the property type  apartment  condo  bed   breakfast  dorm  house  etc    the bed type
 futon  airbed  real bed  etc    the hosts cancellation policy  and the number of beds  bedrooms  bathrooms  and guests
accommodated 
to create our bag of words features  we aggregate each listings name  summary  space  description  experiences offered 
notes  and host bio fields  we then use the nltk packages porterstemmer to stem words to their root and choose the      
stems that occur across the most entries in the test set  after filtering out all stop words and all neighborhood words  to
prevent our neighborhood classifier from simply using mentions of neighborhoods for prediction   we arrive at a list of    
stems  the multinomial feature vectors were then calculated as the frequencies of each stem in each listings text  with the
sum of the features normalized to sum to one 
a closer look at the dataset reveals that words like comfortable and cozy might be used interchangeably between listings  in
order to address this  we handpick   word classes  people  nightlife  activities  style  accessibility  culture  nature  amenities 
and comfort  for instance  the style word class includes the stems of the words modern  brand  victorian  style  decor 
gorgeous  marble  elegant  boutique  and trendy  multinomial word class features are counts of the number of words belonging
to each of the nine word classes in each listing  with the sum of the features normalized to sum to one  for sentiment features 
we use the textblob package  which calculates the polarity of a segment of text by averaging the polarity of each word in the
text included in the packages lexicon 
finally  we download all listing images and extract visual features using the standard bag of words model in computer vision 
we randomly sample     images to create a dictionary of      words  to create the dictionary  we extract speeded up
robust features  surf  descriptors from the     images using opencv  surf is a performant scale  and rotation invariant
interest point detector and descriptor used to find similarities between images      we use k means clustering to form     
clusters  or visual words for the dictionary  using this visual word dictionary  we create feature vectors for each listing by
extracting surf descriptors from the listings image  for each descriptor  we select the closest cluster in the dictionary  in
brief  the visual feature vector contains the frequency of each visual word in the listing image  and is normalized 
we input the features listed above into a support vector machine  svm  with a linear kernel  implemented using sklearns
svm package  we discretize our listing prices into two buckets separated at the median          and develop two classifiers
to     predict the range in which a listings price falls and     predict the listings neighborhood   since the number of listings
per neighborhood is not identical across neighborhoods  we use sklearn svms balanced class weight setting   using l 
 

firegularization  our svm optimizes the objective    
m

x
 
min   w      c
i
 w b  
i  

s t  y  i   wt x i    b   
i    i           m

i   i           m

note that the parameter c adjusts the sensitivity of the classifier to outliers  in order to tune our parameter for c  we use
gridsearch and vary c from       to           the value of c for which performance on the dev set is highest is then used
for the final classifier  which is trained on the train set and tested on our held out test set 
we plot learning curves for each classifier  and note that our model has high variance  indicating an overfitting problem 
to account for this problem  we use feature selection  specifically sklearns recursive feature elimination  rfe   rfe is
an algorithm that recursively considers smaller subsets of all features  by removing features with the smallest weights after
training on a set of features for each iteration  since rfe requires us to choose the number of features we want to ultimately
use  we conduct an experiment varying the number of features to select the best k  which is shown in our results below 
to better understand our classifier  we run ablative analysis  by removing one component at a time  building our classifier
using the train set  and seeing how our classifier accuracy changes on the dev set  in addition  we run feature selection on
only the bag of words text features for both neighborhood and price predictions  because we are interested in comparing and
analyzing the words between neighborhoods and prices 

 
   

results and discussion
svm performance for price and neighborhood prediction

after tuning the c parameters  example for price prediction model shown in table    on the dev set  we test our models on a
held out test set  the train and test accuracies  precision  and recall for each set of features  as well as for the entire system  are
presented in tables   and    for price prediction and neighborhood prediction  respectively   we note that the price prediction
model achieves a test accuracy of       using all features  which is significantly higher than the chance accuracy of     
for the neighborhood prediction model  we find that  interestingly  the highest accuracy achieved is       using only the bag
of words text features  in neighborhood prediction  the chance accuracy baseline is     which can be achieved by simply
predicting the neighborhood with the greatest number of examples 
the receiver operating characteristic  roc  curve for price prediction is included in fig     the auc reported is       
which suggests that the classifier performs quite well in differentiating between true and false positives   we also binarize
our labels for neighborhood prediction and generate    roc curvesone for each neighborhoodbut exclude this figure
for sake of space  in neighborhood prediction  the aucs average      across neighborhoods   we also inspect confusion
matrices for both the price and neighborhood classifiers  in particular  use our confusion matrix from the neighborhood
classifier to generate a heat map of prediction accuracy by neighborhood  as seen in fig     note that prediction accuracies
tend to be highest for the most central neighborhoods  including the mission  south of market  and castro upper market  
this makes sense  since those neighborhoods are best known for their unique characteristics  and those characteristics might
be best detected in the features we extract 
   

learning curves

since the train accuracies are incredibly high for neighborhood predictions  we plot the learning curves for both models  to
determine whether our model is overfitting  in fig     we see that gap between the training scores and the cross validation
scores for price prediction is quite large  indicating a problem of high variance and thus overfitting  in fig     we note that
training scores for predicting neighborhoods has maxed out at nearly   and are extremely far from the cross validation scores 
which also points to an issue with overfitting 
   

feature selection

there are two solutions to high variance  finding more training examples or reducing the number of features  since our dataset
was limited to listings in san francisco  finding more training examples was not possible  so  to help mediate overfitting 
we use feature selection  specifically sklearns rfe algorithm  to reduce the number of features  because recursive feature
elimination  rfe  requires us to select the end k number of features  we conduct an experiment to determine the best k  we
vary the number of features  run rfe on our models for each number of features  and test the models on our dev set  the
resulting train and dev accuracies from this experiment are plotted in fig    and    for price and neighborhood prediction 
respectively  
 

fifrom fig     we see that selecting     features for rfe gives us the best dev accuracy for prices  and that selecting    
features does so for neighborhoods  we build our models  run rfe using the best number of features  and plot the learning
curves again to determine whether the overfitting problems for neighborhoods and prices have been reduced  for our price
prediction model  when comparing the two learning curves in fig    and    we see that the train accuracy has decreased  and
approaches the dev accuracy curve  in addition  we see that the dev accuracy after feature selection remains very close to
     putting these together  we conclude that feature selection slightly alleviate overfitting of our price prediction model 
comparing the two learning curves for our neighborhood prediction model  we find similar improvements  after feature
selection on     components  the training accuracy is no longer maxed out at    and has decreased to around       we exclude
our learning curve after running rfe for sake of space   in addition  we find that feature selection actually improves our
accuracy slightly from       to        finally  we notice that the cross validation score increases as number of training
samples increases  this upward trend suggests that  if more training examples were available  they may have helped improve
our classifier 
   

ablative analysis

to determine which component of our five features accounts for the most improvement in our svm models  we run ablative
analysis  and remove one component at a time to see how our classifier accuracy on the dev set changes  for price prediction 
we see from table   that the listing information features are the most significant factor  this makes sense because housing
prices are very much determined by the property type and size  the number of bedrooms and bathrooms  and so on  for
neighborhood prediction  we see from table   that the bag of words text features are the most important component  this
suggests that neighborhoods are quite diverse and are hard to capture from the listing information  sentiment  or images  but
that the richness of the word choice used to describe the surroundings of the neighborhoods  such as vibrant  bars  nightlife 
museums  is best at differentiating between neighborhoods 
   

comparing prices and neighborhoods

interested in seeing how word importance differed across price and neighborhood models  we run feature selection on just
the bag of words features  we discover that the word features that remain after rfe for price prediction are actually a subset
of the word features that remain for neighborhood prediction  most of the words important in both prices and neighborhoods
describe the house and its interior  such as patio  remodel  luxurious  clean  modern  and cozy 
we can intuitively see that these words  though they describe an individual listing home  could also be applied to neighborhoods that include housing units of similar style and age  thus  the words that describe a listing can also distinguish one
neighborhood from another  on the other hand  the words found to be important in only neighborhood prediction have a
larger scope and are related to culture and larger surroundings  these words include vibrant  cafe  church  peace  bustling 
culture  museum  and history 

 

conclusion and future work

our analysis of the inside airbnb projects listings for san francisco shows that we can successfully predict neighborhood
and price range using an array of features extracted from listings  while our classifiers initially suffer from high variance  our
feature selection at least partially remedies this overfitting  both classifiers we develop perform significantly above baseline 
and suggest that listing information  text features  and more can be harnessed effectively for predicting neighborhood and
price 
with more time and computational resources  we hope to develop our model for price prediction  the results weve presented
here split prices into two discrete ranges   in our work on this project  we also developed a classifier to predict price across
four buckets  split at the datas quartiles  and found encouraging accuracy rates   to provide meaningful price tools to airbnb
users  we would need to discretize our price range into smaller buckets to offer targeted and narrow price range suggestions 
our final goal in neighborhood prediction is to provide a recommendation system to airbnb users as mentioned in this papers
introduction  our work in this paper established that listing text is rich enough to allow us to predict in which neighborhood
a listing belongs  in future work  we hope to take advantage of this diversity of word choice to cluster neighborhoods with
similar cultures or characteristics together  we may even be able to do this across cities  as the inside airbnb project provides
data for several cities around the world  and catalogue both cultural differences across cities and neighborhoods from city to
city that are somewhat similar 
finally  we notice in our analysis that there is significant overlap between the features that predict neighborhood and price  in
future work  it would be interesting to understand the correlations between neighborhoods and prices more deeply  upscale
neighborhoods naturally have higher priced listings  and tracing out the qualities of neighborhoods that encourage consumers
to pay higher prices may be an important application for real estate   urban planning 

 

fiprice prediction
neighborhood prediction

 

table    tuning c parameter on
svm model for price prediction
c
train acc  dev acc 
     
      
      
    
      
      
  
      
      
    
      
      
      
      
      

 

figure    roc curve for price
prediction  auc         
figure    results from rfe for
price prediction
figure    learning curve for
price prediction
figure    learning curve for
price prediction after running
rfe using     features

sentiment features
      

      

      

      

      

      

      

      

train acc  test acc 

      

      

      

      

      

precision

      

      

      

      

      

bag of words text feats 

listing information

word classes

sentiment

overall system

      

      

      

      

      

      

      

      

      

      

table    ablative analysis for price prediction

table    train and test accuracies  precision  and recall of svm for price prediction

word classes
      

      

recall

listing information
      

      

features used

bag of words text feats 
      

      

      
      

      

      

      

      

      

      

bag of words text feats 

listing information

word classes

sentiment

overall system

      

      

      

      

      

      

      

      

      

      

train acc  test acc 

visual features
      

      
      

      

      

      

figure    heat map of san francisco neighborhoods by prediction accuracy

component

all features

table    ablative analysis for price prediction

      

      

recall

sentiment features
      
      

      

train acc  test acc  precision

word classes
      
      

      

features used

table    train and test accuracies  precision  and recall of svm for neighborhood prediction

listing information
      

train acc  test acc 

bag of words text feats 
      

component

visual features

feature selection for neighborhood prediction  svm linear kernel 

figure    results from rfe for neighborhood prediction

all features

figure    learning curve for neighborhood prediction

 

 

fi 

references

    zhang  a  x   noulas  a   scellato  s     mascolo  c         september   hoodsquare  modeling and recommending
neighborhoods in location based social networks  in social computing  socialcom        international conference on  pp 
        ieee  http   arxiv org pdf           pdf
    hill  d          how much is your spare room worth   spectrum  ieee               
    lee  d   hyun  w   ryu  j   lee  w  j   rhee  w     suh  b         february   an analysis of social features associated
with room sales of airbnb  in proceedings of the   th acm conference companion on computer supported cooperative
work   social computing  pp            acm 
    dataset available here  http   insideairbnb com get the data html
    bay  h   tuytelaars  t     van gool  l          surf  speeded up robust features  in computer visioneccv       pp 
          springer berlin heidelberg 
    ng  andrew              lecture notes  support vector machines  notes      pg      

 

fi