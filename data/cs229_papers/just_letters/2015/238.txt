cs    final project report

 

predicting a happier place
lisa wang karen wang grant means
 lisa     kwang   ghmeans    stanford edu

i  i ntroduction
in communities with higher well being  people live longer
and more productive lives  and local economies flourish  in
an effort to improve understanding of what leads to vibrant
communities and fulfilling lifestyles  our project explored
community level factors and their significance for determining well being in americas largest metropolitan areas  we
hypothesized that the frequency of points of interests  pois 
in a community as well as basic macro level factors were
sufficiently accurate indicators of well being  thus  the input
to our algorithm was poi frequencies and economic social
factors for each metro area  we then used logistic regression 
extra trees classifier  and support vector machines to output
a prediction of whether a metro area is of high or low wellbeing 
ii  p rior w ork
in       schwartz and eichstaedt et  al  from the university
of pennsylvania and michigan state university predicted the
well being of       us counties using sentiment analysis on
twitter data  they used the heuristic of life satisfaction  ls  as
measured by random phone surveys in response to questions
such as how satisfied are you with your life   with choices
ranging from very dissatisfied to very satisfied  we think
that it would be interesting to build upon the implications
raised by this paper that location is a fundamental underlying
factor in peoples overall happiness or satisfaction with life     

the majority of prior work was done by hand using traditional
survey methods or aggregating previous research  the only
existing well being applications we could find for machine
learning involved twitter data  so our features appear to add a
unique spin on the issue of community factors in well being 
iii  datasets
openstreetmap  osm  is a collaborative  open source
alternative to google maps      the data tags points of
interest  pois   marked by their longitude and latitude
coordinates  with their names and functions 
the gallup healthways index is a collaborative project that
provides data on the community  financial  physical  purpose 
and social well being scores  as well as overall city satisfaction
of the     largest metropolitan communities across america
     each community is assigned an index  or score  for each
well being category based on phone interviews with random
samples of people from each community  gallup defines
scoring high in each category to mean 





in       quercia and ellis et  al  studied the relationship
between sentiment expressed in tweets and community
socio economic well being for twitter users throughout
london  they found that tweet sentiment for individuals
in a community and the socio economic well being of the
community were highly correlated  the usefulness of social
media features in determining well being could help inform
future feature construction for our wellbeing predictions     
in       joachims explored the use of svms for learning
text classifiers from examples  he found that svms achieve
substantial improvements over other methods of classification
and behave robustly over a variety of different learning tasks 
this indicates that svms are a state of the art method and
validates our decision to use svms for classification  given
these results  it makes sense that an svm was our most
accurate method for classification     
in summary  using twitter data to gain insights into communities was a clever approach to determining well being  based
on the available literature regarding well being  it appears that



community  liking where you live  feeling safe and
having pride in your community 
financial  managing your economic life to reduce stress
and increase security 
physical  having good health and enough energy to get
things done daily
purpose  liking what you do each day and being motivated to achieve your goals 
social  having supportive relationships and love in your
life 

well being scores took on a floating point value between
    and      overall city satisfaction took on a percentage
value indicating what proportion of respondents interviewed
felt generally satisfied with the city area they live in  for
our project  we defined high well being as having a high
score in one or more of these indexes  although schwartz
and eichstaedt et  al  used the ls heuristic for well being
as discussed above  this would have involved a non trivial
challenge on our part of acquiring the coordinates for more
than one thousand us counties to extract osm data  we
decided that the gallup data  which covers a smaller but
more well defined set of large metropolitan areas  was more
suitable for our purposes 
community statistics were culled from various governmental  commercial and ngo sources  including the tax
foundation           u s  department of commerce       u s 
census bureau      and the new york times     

fics    final project report

 

iv  p roblem a pproach   f eatures
initially  we sought to predict the community well being
score  as defined in the gallup data  for each area by
training a set of regression models  the features included
frequencies of pois which we believed to be correlated with
well being  we extracted the    most frequent tags from
osm  which included leisure and amenity resources such
as fitness centers  beach resorts  pubs  arcades  hot springs 
taxi stands  etc  in order to tease out meaning from values
between metro areas with populations that span well over
an order of magnitude  we included frequency per capita by
normalizing poi counts based on the communitys population
size  for each community  we computed a scaling factor of
its population size divided by the minimum population size
over all communities  and multiplied each poi frequency by
said scaling factor 
however  after evaluating our initial results  we decided to
pivot from regression models to classification models instead 
moreover  the gallup data had a relatively small range  ex  social well being scores only ranged from     to      discretized
into intervals of       which made it difficult to perform
regression on  we also expanded our feature set to attempt to
capture more macro level phenomena of a community  osm
classifies pois into larger categories of sustenance  education 
transportation  financial  health care  culture  and sport  we
postulated that the counts of poi categories may be just as
important as the counts of individual pois in predicting a
communitys well being  and thus included these in our feature
set  we also included additional features representing macroqualities of cities such as regional classifications  coastal
proximity  and geography for a total of     features per
community  a sample of our data is given in table i 
table i
e xcerpt from dataset after feature normalization
community

reno  nv
youngstown warrenboardman 
oh pa

social
well being
score
   
   

taxi
stand

common
area

    
    

     
    

city
tax
rate
    
    

are very similar  his method provides a clearer distinction
between data points of high and low well being compared to
simply splitting the data in half 
vi  r egression m odels
a  linear regression
we started with linear regression since it is the most
straight forward regression model  it minimizes the squared
error by performing the ordinary least squares algorithm 
the minimization problem can be described as 
min   x  y    
where x is the mxn design matrix containing the features of
the m training samples   is the nx  vector of weights and y
is the mx  vector of targets 
b  epsilon support vector regression
svr uses support vector machines  svms  to perform
regression by minimizing the norm of the weights subject
to approximating the pairs  xi   yi   with  precision  the
advantage of using svrs included the possibility of using
kernels  we used linear and rbf 
c  other models
we also tried ridge regression  lasso regression  polynomial
regression  and epsilon support vector regression with polynomial kernel  but found these to perform worse than the models
above 
vii  r egression e valuation m ethods
a  train validation test split
for the regression approach  we set aside     of our data
to split into a       train and validation set  which we used
to improve our models  the remaining     was reserved as
test data which we did not touch until we tuned our models
based on our train and validation sets 
b  regression metrics

the gallup data provided us well being heuristics for the
    most populous metropolitan communities in the us 
where communities are defined as groups of one or more
cities  while extracting data from osm  we ran into errors
with the extraction api  and were only able to extract poi
counts for     different communities for our regression
models 

since we were working within a regression model  we used
mean squared error  mse  as well as the explained variance
score  evs  to compare the performance of our models on the
training as well as the validation set 
   mean squared error  let m be the number of samples 
let yi be the true target of the i th sample  and let yi be the
predicted value of the i th sample  then  the mean squared
error is defined as 
m
  x
 yi  yi   
m se y  y   
m  

for our classification approach  we decided to split our
targets into the top      middle      and bottom    based on each
individual well being score  we then discarded the middle  and
used the top and bottom for a total of    training examples to
perform binary classification  since data points in the middle

   explained variance score  this score measures the
proportion of variance created by the regression model to the
true variance of the data set  a score of     is perfect and
lower scores are worse 
v ar y  y 
ev s y  y      
v ar y 

v  data c ollection   p reprocessing

fics    final project report

 

viii  r egression f eature s election
we used three different methods to perform feature selection
for our regression models  we hoped that feature selection
would help us determine the most significant features to
improve performance and find the amenities in a community
most correlated with well being 
a  single feature regression
we ran all our regression models on the training data
separately for each feature to determine the most important
independent features  however  this approach does not capture
e g  the covariance between multiple features 
b  recursive feature selection  rfe 
this method selects features by starting with the full feature
set and recursively reducing the feature set until the desired
number of features is reached  it repeatedly trains the given
estimator on the data  eliminating the features with the lowest
weights  hence  it is very similar to backward search 

 
 
min w b kwk
 
s t  y  i   wt x i    b      i           m
however  in order to solve the problem efficiently  svms
convert the primal objective into its dual form and leverages kernels which map lower dimensional data into higherdimensional spaces in which data can be more easily separated  for our study we used linear and  gaussian  radial basis
function  or rbf  kernels 
c  adaboost
adaboost is short for adaptive boosting  and is an ensemble
method that uses many other types of learning algorithms
 weak learners  and computes a weighted sum representing
the final output of the boosted classifier  a boost classifier is
of the form 
ft  x   

after performing feature selection using single feature regression  we reduced the number of features to     the   
features include among others the tags doctors  fire station
and park  we computed the mse and evs for both training
and validation sets  our results for regression are shown in
table ii 
table ii
e valuation of r egression m odels
mse train
     
     
     

evs train
     
     
     

mse valid
    
    
    

ft  x 

t  

ix  r egression r esults

model
linear regression
sv regr  linear
sv regr  rbf

t
x

evs valid
    
    
    

x  c lassification m odels
a  logistic regression
logistic regression attempts to find parameters  that best
fit the model 
y     if t x    
y     otherwise
the parameters are found using maximum likelihood estimation with the probability distribution given by the logistic
function 
 
g t x   
    et x
b  support vector machines
svms attempt to find a separating hyperplane between two
classes of data by fitting the model 
y     if wt x   b    
y     otherwise
it attempts to give the biggest buffer against error by optimizing the primal objective defined as 

where each ft is a weak learner that takes x and returns
a real value indicating its class  as long as the performance
of each weak learner is better than random guessing  the final
classifier can be shown to be a strong learner 
d  extratrees
decision trees are used in supervised classification problems
to create a model that predicts target values using decision
rules  extratrees is short for extremely randomized trees 
and is an ensemble method that relies on randomly splitting decision tree nodes  the extratrees algorithm is more
computationally efficient than random forests and produces
a smoother decision boundary due to the randomization 
xi  c lassification e valuation m ethods
since we only had    training examples  we decided
to use k fold cross validation rather than make a standard
train validation test split  we chose k      in order to leave
out less data each time  moreover  since our evaluation
heuristics of f  scores  precision  recall  and accuracy tend
to have large variances  a higher k produces more trials to
average over and thus higher confidence in our evaluation
results 
we standardized the data through mean removal and variance scaling such that each feature has mean   and variance
   the svm methods expect the mean for each feature to be
  and the same variance for all features makes sure that each
feature has equal importance  we performed five classification
algorithms on our data set  using three different kernels for
the support vector machines  running the algorithms without
feature selection produced results that were not much better
than random classification  however  that was not a surprise
since we had     features  hence  we performed feature
selection by scoring each feature with the statistical anova

fics    final project report

 

table iii
f eature s election r ankings for each index

fig     evaluation of number of features

index
rank  

financial
bar

rank  
rank  
rank  

playground
college grads
coastal

rank  
index

gdp per
capita
social

rank  
rank  
rank  

taxi stand
video arcade
club

rank  
rank  

common area
city tax

physical
community
center
waste basket
dentist
fitness
center
pop  growth
community
college grads
city tax rate
gdp per
capita
gdp growth
pop  growth

purpose
pharmacy
water park
stadium
city tax
pop  growth
overall city
satisfaction
college grads
city tax rate
gdp per
capita
gdp growth
pop  growth

f score and chose the best k features  we tried out different
k values in the range between   and    and found   to
be the best  after feature selection  we ran all classification
algorithms on each data set of the six data sets 
to evaluate our approaches  we computed the accuracy  precision  recall and the resulting f  scores for each experiment 
accuracy in the binary classification context is just the percentage of predictions that were correct  so it is computed by
tp   fn
tp   fp   tn   fn
xii  c lassification f eature s election
a  choosing the number of features for feature selection
to find the best number of features for feature selection  we
did a search using svm with rbf kernel over feature numbers
in the range between   and     the best accuracy and f  score
was achieved when we selected the   best features according
to their anova scores  fig      decreasing or increasing the
number of target features impacted the accuracy and f  scores
significantly 
b  feature selection rankings
we performed feature selection on each dataset separately 
table iii shows the five best features for each dataset  interestingly  the most indicative features were exactly the same
for the community well being and the overall city satisfaction
datasets  which is a sign that community well being might be
the most important indicator of overall satisfaction  in addition 
the selected features for these two datasets consisted mostly
of non physical features such as college graduation rates or
gdp growth rather than amenities  in contrast  social wellbeing is mostly correlated with amenities and physical features
such as taxi stands  video arcades  clubs and common public
areas  even though we cannot make any conclusions about
causation  this information shows that the most indicative
features are very different between the datasets  while physical
features are good indicators for physical and social well being 
they are less significant indicators for community well being
and overall city satisfaction  hence  just by looking at the
materialistic features of a city  we might not get a good idea
of how happy the people living there actually are 

fig     evaluation of learning algorithms

xiii  f inal r esults
a  comparing algorithms
we started with logistic regression as our baseline and tried
support vector classification with three different kernels as well
as ensemble methods  including adaboost and extratrees 
logistic regression already performed well with an average f 
score and accuracy above      fig      only two other models
we tried were able to improve the performance of logistic
regression  with support vector classification using an rbf
kernel at the top  reaching an accuracy of      and an f 
score of       since svc with an rbf kernel worked better
than svc with a linear kernel  this could be a sign that there is
more complexity in our data and higher dimensional functions
are needed to model it  the ensemble classifier extratrees had
higher f  than logistic regression  but lower accuracy  since
extratrees is a more complex classifier and did not outperform
logistic regression  it is usually better to choose the simpler
one 
b  comparing datasets
we also computed the scores of svc with rbf for each
dataset  fig      as the chart shows  the scores varied signifi 

fics    final project report

fig     evaluation of datasets

fig     evaluation of training set size

cantly between the datasets  the best results were achieved for
the community dataset  with the highest f  score and balanced
precision and recall  for our application  we desire precision
and recall to be balanced  especially since both classes are
equally important  interestingly  the datasets social and purpose had very high recall  but lower precision compared to
the other datasets  this could indicate that the positive class
of higher well being was easier to predict than the the negative
class for that particular dataset 
c  increasing training set size
for evaluating sample set sizes we used k fold cross validation  since we began with    training examples on which we
decided that k      was the maximum number of folds that
made sense  the trend of our evaluation scores with training
set size suggested that our results grew more reliable with
a bigger training set  fig      note especially that recall got
closer precision  which is desirable since both are important 
for a training set size of     the accuracy was much higher
than precision or recall  this is because accuracy takes class
size into account  and k fold does not necessarily give the
same distribution of happy unhappy classes 
xiv  c onclusion
our work showed that predicting well being of communities
in the u s  using a combination of openstreetmap and municipal data as features is a promising approach  which could

 

potentially be further improved by increasing the training set
size  by discretizing the problem into a binary classification
problem  we simplified the task  but were able to make more
concrete progress as measured by accuracy and f  scores 
feature selection turned out to be very useful for improving
our results and also gave us interesting insights about our data 
out of the five well being categories  community well being
seemed to be most correlated with overall city satisfaction  and
they both shared the same set of most significant statistical
features  however  pois were helpful to identify social and
physical well being  we were able to achieve good results
for classifying high vs  low well being scores  with the best
results for community well being  overall  our work is useful
for municipal leaders to get insights into the different aspects
of well beings in their communities and can help indicate wellbeing levels when polling data does not exist  openstreetmap
and municipal data are available for many countries around
the world  and the data is being expanded everyday  hence 
by combining these globally available datasets  we have experimented with a new approach to predict and analyze wellbeing using machine learning techniques  that could be further
generalized to study well being across the world 
xv  f uture w ork
some of the limiting factors for our exploration of
happiness were our intuition for which features are most
highly correlated with wellbeing and how much data collection
and preprocessing was required to construct a sufficiently
robust set of features  a more in depth exploration of
happiness would go beyond these limitations to employ all
osm tags for each metro area and expand statistical features
to include a much larger corpus of macro level community
information  during data preprocessing and feature definition
we decided to normalize our features linearly based on
population size  however  the actual data might not scale
linearly  and we would need to do more research on whether
this was an accurate assumption  for our classification
problem  we could have tried not discarding the middle
one third of training examples  we also could have tried more
hyperparameter tuning in our learning algorithm evaluation 
especially for svm such as using grid search 
in the process of our study we were able to create a novel
dataset that combined municipal with osm data  which we
can publish to future researchers can use it  finally  we could
expand our study to include communities outside of the us 
since an implication of our study was being able to predict
well being reliably in communities even when there is no
polling data  openstreetmap data exists over the entire world
and is continuously updated  which would allow us to assess
the happiness of communities that are not covered by galluphealthways  since individual surveys are time consuming and
expensive  this could allow us to extend our knowledge of the
worlds overall well being at a large scale 
r eferences
    main page  retrieved november           from wiki openstreetmap org 
wiki main page

fics    final project report

    gallup healthways well being index  retrieved november          
from www gallup com poll        galluphealthways wellbeing index 
aspx
    schwartz  andrew h   eichstaedt  johannes c  et  al          characterizing geographic variation in well being using tweets
    quercia  daniele  et al  tracking gross community happiness from
tweets  the      acm conference on computer supported cooperative work  association for computing machinery  seattle       
http   dl acm org citation cfm id         
    joachims  thorsten  text categorization with support vector machines 
learning with many relevant features  springer  new york       
http   link springer com chapter         bfb        
    walczak  jared  state individual income tax rates and
brackets  the tax foundation  washington  d c       
http   taxfoundation org article state individual income tax rates andbrackets      
    henchman  joseph  sapia  jason  local income taxes  city  and countylevel income and wage taxes continue to wane  the tax foundation 
washington  d c        http   taxfoundation org article local incometaxes city and county level income and wage taxes continue wane 
    cities with the most college education residents  the new york times 
new york       
    table    annual estimates of the population of combined statistical
areas  april         to july               population estimates 
united states census bureau  population division  march      
https   www census gov popest data metro totals      tables cbsaest        csv 
     news release  united states department of commerce  bureau of
economic analysis  september      
     pedregosa  fabian  et al  scikit learn  machine learning in python 
journal of machine learning research  october      

 

fi