cs    final project

 

spoken character recognition
yuki inoue  yinoue     allan jiang  jiangts   and jason liu  liujas   

abstractwe investigated the problem of spoken character
recognition on the alphabets  numbers  and special characters 
we used the mel frequency cepstral coefficients  mfcc  as
the feature points to characterize the spoken characters  and then
reduced the dimensionality of the feature vector by applying the
principal component analysis  four different machine learning
algorithms were trained using the dimension reduced feature
vectors  and we compared their performance  for the alphabet
set  we realized that many of the letters with similar sound
structures were confused for each other  so we instead took a twolayer approach  first determine which character set an input
is in  and then classify the sample within the set  we used the
k means algorithm to determine the character set  for the best
performance  we achieved        accuracy for the alphabets 
       accuracy for the numbers  and        accuracy for the
special letters 

i  i ntroduction
online security is more important than ever  with the
explosion of the amount of private information online  it has
become a commonplace for websites to require single and
sometimes even double security features to protect the users
private information  this includes requiring the user to type in
a one time code  which is usually a randomly generated string
of characters containing the alphabets  numbers  and special
characters  typing in these characters can be an annoyance to
many  especially in mobile settings  where data entry is far
more constrained than in a laptop or desktop setting due to
lack of dual screen capabilities for most smartphones 
also  consider the situation in which one has to type in
unfamiliar words such as foreign names  since the character
order is seemingly random to the user  he she has the same
problem of having to switch constantly between the screens 
both of these situations can be remedied if we have a
software that can interpret the voice input of the user and
type out the result  in this project  we investigate the spoken
character recognition for the alphabets  the numbers  and the
special characters  to aid those people who have to type in
sequences of characters  to solidify the problem  we will
define the input to our algorithm as a one second wav file 
in which a single character  a        etc   is spoken  and using
a supervised learning algorithm  one of logistic regression 
naive bayes  support vector machine  or neural network  to
output the prediction on which character was spoken 
ii  r elated w ork
almost all prior work in using machine learning to tackle
spoken character recognition leveraged a cepstrum based preprocessing pipeline to generate a set of features to be fed into a
back propogation neural network  bpnn   the main variation
in prior works were in how they performed their pre processing
steps and how they created their neural network architecture 

a  hand chosen features
existing classifiers that achieved the highest performance
are neural networks that were fed hand picked features  roger
cole and mark fantys english alphabet recognition  ear 
system achieves about     accuracy by using hand picked
signal processing features that were specific to improving
performance on the english alphabet           they did not
perform classification on other characters  such as numbers  
on a high level  cole and fantys method was a four step
process  first they filtered and digitized their audio samples 
second they used a neural network to track when a pitch began
and ended  third they measured features over time and split
them into different linguistic segments  and fourth they used
another neural network to classify the segments they identified 
although they report the highest accuracy  they were only able
to do so due to their explicit phonetic segmentation and the
use of speech knowledge to select the best features to capture
the linguistic information  as such the features chosen in their
work are good for english  but are not generalizable to other
languages 
in an earlier work by burr  specially chosen filters and
delays were used to extract features from letter data      in
particular  the processing pipeline for letters was hand tuned
to be different from the data processing pipeline used for
spoken digit recognition in this paper  the pre processing in
this paper used fewer linguistic features specific to english
letters and reported about     accuracy on letters and about
    accuracy on digits 
b  neural network architecture
other papers used different network architectures with different transfer functions  for example  in a paper by adam
and salam in       the audio pre processing step used was
mfcc  which is highly standard   and the neural network
architecture used was     input layer nodes      hidden layer
nodes  and    output layer nodes for the alphabet      they
used the hyperbolic tangent transfer function in the hidden
layer and a linear transfer function in the output layer  they
report     accuracy with this architecture  finally  in a paper
by reynolds and tarassenko in       a radial bases function
neural network classifier was used on general characters in
multiple languages  they did not report accuracy on the
english alphabet specifically  but their overall accuracy was
about     
iii  dataset and f eatures
the sample collection was done by ourselves  we recorded
three samples from each person  each corresponding to the
three character groups  i e  the alphabets  numbers  and special
characters   for each of the recordings  we asked the speaker to

fics    final project

 

pronounce one character every time we tapped their shoulders 
we made sure that the shoulder taps are separated by at least
  second  so that each samples are spaced out enough and
thus do not interfere with each other  after we collected the
samples  we subdivided them into one second sound bites of
the sound samples using a matlab script  for example  an
alphabet recording was subdivided into    one second sound
bites  each capturing an alphabet letter  the script first takes
the square of the signal  i e  calculates the power of the signal  
then applies a low pass filter by taking a moving average  the
resulting signal waveform looks as the bottom right  finally 
the script subdivides the signal by looking at the peaks of the
filtered signal  because the peak of the filtered power signal
occurs at the same part for all of   syllable characters and for
most of the   syllables characters  the script is able to line up
the samples well 

a filter to the carrier tone  and it determines which word
is being spoken  since the objective of this project is voice
recognition and not speaker recognition  the feature points
should be chosen as such to characterize the filter applied to the
carrier tone  not the carrier tone itself  however  this is easier
said than done  for we can only collect post filtered soundbites 
one of the ways to extract the filtering characteristic is to use
the mel frequency cepstrum coefficient transform 
b  mel frequency cepstral coefficient transform  mfcc 
the mfcc allows for the extraction of the filtering characteristic of the audio files  though we will avoid the indepth explanation of how mfcc can achieve such a result
as it is not the main focus of the project  mfcc essentially
takes the fourier cosine transform of the log magnitude of
the fourier transformed audio sample  allowing it to analyze
the frequency response of the filter applied onto the carrier
tone  since the main focus of the project is to apply machine
learning algorithms onto the extracted feature points  and
cared less about how we extract those feature points  we did
not implement the mfcc  instead  we used mfcc m in the
auditory toolbox by malcolm slaney     to apply the mfcc
transform to the sampled audio  an example mfcc output is
as follows 

fig     raw sound file  left   sound wave squared  right 

overall  we collected             and     sound samples for
the alphabets  numbers  and special character sets  respectively 
with one second sound samples  we still felt that the data
was not in a digestible format  we applied the mel frequency
cepstral analysis to each sample and performed principle
component analysis on the resulting feature vectors to find
the most significant feature points  both mfcc and pca are
explained in detail in the next section 
after the data collection  we tested our learning algorithm
as follows 
   randomly choose   
   test on the remaining   
   repeat steps   and   one hundred times
where step   is added to reduce the variation from the choice
of the training set 
iv 

m ethodology

a  voice synthesis basics
voice synthesis can be modeled with two steps  the first step
is the carrier tone synthesis using vocal cords  since the carrier
tone is just a raw tone created by simply vibrating ones vocal
cord  it cannot be deciphered as a recognizable word  the
carrier tone depends on the physical composition of the speaker
such as the length of the vocal cord  making this step unique to
each individual and also invariant on the word being spoken 
the second step involves shaping this carrier tone using the
contour of the mouth  this can roughly be modeled as putting

fig     raw audio snippet of spoken letter

fig     example mfcc feature output heatmap
as can be seen from the figure above  the mfcc outputs
   feature points per sampled time period  and as we ultimately chose to subdivide the   second sound bite into    
subsections  the mfcc roughly outputs      feature points
per sound bite 
we tried to add more features other than the direct mfcc
output to enhance the performance  such as adding the  st  nd
derivatives of the mfcc and duration of the signal input  but
nothing seemed to improve the result significantly  therefore 
we decided to just use the raw outputs of mfcc as the feature
vector 
c  pca
with the parameter settings of the mfcc feature extraction
that produced the best performance  the size of the feature

fics    final project

 

vector became roughly       though it is hard to exactly
say how large the dimension of a feature vector should be
to a given problem       dimensional space was just too
large  as most of the ml algorithms run very slow for such a
large feature vector  we therefore decided to use the principal
component analysis  pca  to reduce the feature dimension 
pca reduces the dimension of a dataset to dimension n by
taking the n right singular vectors that correspond to the
largest singular values as the basis  with this change in basis 
the dimension of the feature points is lower  but the resulting
covariant matrix should still approximately be the same  after
pca  the      dimensional feature vector was turned into a
   dimensional feature vector 
d  k means algorithm
the k means algorithm is an algorithm used in unsupervised learning problems  the main idea of the algorithm
involves minimizing the following objective function over the
set of clusters si  

arg min
s

k x
x

kx  i k 

   

i   xsi

the algorithm estimates the solution to the objective function by calculating the cluster centroids  which are the estimates of the   the algorithm has two steps  first  each data
in the sample set is put into a cluster that is closest to a cluster
centroid  this is the assignment step  then  the location of
each cluster centroid is recalculated by taking the mean of the
sample points that it contains  this is the update step  in our
project  the k means algorithm was used to group letters that
have a similar mfcc feature points  we expected that since the
mfcc extracts features that are strongly correlated with the
spoken characters  by we can automate the grouping of similar
sounding characters by applying the k means algorithm to the
samples we collected 
e  naive bayes
the first algorithm we used to test our data was the
naive bayes model  we started with this  because of its ease
of implementation and general robustness  the assumptions
behind naive bayes are the each sample is independent from
one another  although we had speakers repeat through the
letters three times  we still posit that the independence holds
true  the rationale behind this is the fact that we specified
subjects to put pauses in between each character spoken  thus 
when a subject says a followed by b  the way b is
spoken is independent of the pronunciation for a  we used
the gaussian naive bayes model  since it seemed like a safe
assumption that data would be somewhat normally distributed 
f  support vector machine
next  we moved on to using a support vector machine 
also known for its reliability  much of the magic behind the
support vector machine lies in its kernel method  which is

a mathematical trick used to perform calculations on feature
vectors of  potentially  infinite dimension  we experimented
with three types of kernels  gaussian  poly  and linear  during testing  the gaussian kernel consistently performed much
worse than both the polynomial and linear kernels 
table i  labelling errors of different svm kernels on
classification of special characters
kernel
error

linear
     

poly
     

gaussian
     

the rationale behind this is that the equation for the gaussian kernel is 


kx  x  k 
k x  x      exp 
   

   

as an exponential equation  this effectively maps our feature
vectors to infinite dimensions  this creates a model that is
more complex than our dataset actually is  thus yielding high
error rates  on the other hand  the linear kernel is merely
represented as a dot product  with the polynomial kernel
being a finite linear combination of these dot products  with
these less complex models  we had much better results that
consistently outperformed our naive bayes model  perhaps 
with more data  we would be able to use more feature points
from pca and possibly yield better results with the gaussian
kernel  in addition  when selecting a penalty  the l  penalty
yielded better results than the l  penalty  suggesting that
outliers should be treated with extreme caution 
g  logistic regression
logistic regression performs surprisingly well as compared
to other classification algorithms  during our trials  we saw
that it performed almost as well as the svm with a linear
kernel 
for support vector machines and logistic regression  we
used the one versus rest scheme  the strategy consists in fitting
one classifier per class  for each classifier  the class is fitted
against all the other classes  in addition to its computational
efficiency  only n classes classifiers are needed   one advantage of this approach is its interpretability  since each class
is represented by one and one classifier only  it is possible to
gain knowledge about the class by inspecting its corresponding classifier   http   scikit learn org stable modules multiclass 
html one vs the rest 
h  back propagation neural network
the last classifier we used to fit our data was a back propagation neural network  the backpropagation neural network
estimates the gradients of a cost function over a network architecture and updates the weights of the transfer functions from
layer to layer accordingly  the strength of neural networks is
that network architectures with many hidden nodes allow the
classifier to create increasingly complex hypotheses 

fics    final project

 

we trained a network with a single hidden layer with   
nodes for alphabets     nodes for digits  and    nodes for
special characters  we used the maxout transfer function for
the hidden layer and the softmax transfer function in the output
layer 
v  e xperiments  r esults  d iscussion
a  alphabets
the classification of alphabets was the hardest out of the
three character sets considered  obviously with alphabets having the most number of categories to label     compared to   
for special characters or    for numbers   it is expected to be
the hardest  however  that was not the only complexity related
to the alphabet classification  unlike the numbers or the special
characters  there are some alphabets that sound very similar 
this has been pointed out by different papers         and
are typically called sets  more specifically  the sets noted
by other researchers are the e set  containing the family of
alphabets that have an e ending  i e  b c d e g p t v z   and
the m set  containing m and n 
we first ran the ml algorithms without taking a special note
of the sets  figure   shows the result for the naive bayes 
and we see that certain letters vastly underperform others  for
exapmle  letters such as h and w perform well  but other
letters  especially the letters in e set  are grossly mislabeled 
at this point  we suspected that this is due to the e m sets 
and surely enough  the confusion matrix in figure   confirms
that the letters in the e m sets are being mislabeled within the
sets  but we did not stop there  looking at the confusion matrix
more closely  we saw more than the m e sets  for example 
q u are mislabeled as each other  and so are a j k  this
makes sense  as those letters sound similar to human ears 
therefore it is logical that when mfcc extracted the features
for those letters  the resulting feature points are similar  this
result motivated for some way to make an educated guess on
the sets for the rest of the    letters 
since we do not have the prior knowledge of how different
letters are clustered with each other  set determination is an
unsupervised problem  to tackle the problem  we ran the kmeans square algorithm on the samples 
each plot represents a cluster  and the bars represent how
many letters are part of that cluster  as expected  letters that
sound similar such as q u and a j k mentioned before  are in
the same cluster  one thing to note is that l o  which do not
necessarily sound similar  are in the same cluster  k means
algorithm allowed us to discover sets that we would have
otherwise not have been able to guess  after fine tuning  we
ended up with   different sets 
empowered by the knowledge of sets  we once again ran
the test  the learning algorithm now has two layers  the first
step is to determine which of the   sets the sample belongs
in  and the second step actually determines which letter it is 
the results are summarized in table   
b  numbers and special characters
numbers and special characters are much easier to classify 
as the characters sound very differently from each other 

fig     naive bayes error without set differentiation

fig     naive bayes confusion matrix

therefore  we decided that there is no need to look for sets to
improve on the accuracy  in general  there was no difference
in dealing with numbers and special characters  the results are
summarized in table   

fics    final project

 

fig     k means clustering set differentiation

table ii  results of different algorithms on spoken digit and
character classification

also  many of the sound samples we collected were from
those who do not speak english as their native language  unlike
the american english speakers cole and fanty tested on 
therefore  our sound samples have larger noises added to them
in the form of accents  this was intentional  as the main goal
of the project is to create a software anyone can use 
finally  the feature extraction method we used is independent of the target language  unlike that of the other researchers 
who used pronunciation patterns very specific to english  such
as the notion of sonorant and fricative sounds  therefore  there
is a higher chance for our algorithm to perform better for
different languages 
we have a couple of topics in our minds for the future 
first  since the main objective of the research is to create
an algorithm to classify characters regardless of the type  i e 
alphabets  numbers  special characters   instead of creating ml
algorithms for each of the three types  we want to create an
ml algorithm that can take any kind of inputs  second  as
mentioned in the discussion section  our algorithm was able
to ignore the fact that the   was pronounced differently 
since this kind of variation in how the special characters are
pronounced is to be expected  it would be nice if we can
extend our project to account for that  more specifically  use
the idea of sets  developed when we were studying alphabets 
to account for such a variation  for example  both hash and
pound will be in the same set of    such a system would
be a very useful and practical method of data entry for mobile
systems 
r eferences
   

success rate
alphabets
numbers
specials

nb
     
     
     

svm
     
     
     

logreg
     
     
     

nn
     
     
     

the results are better as expected  one thing to note here
is that unbeknownst to us when we were collecting  the  
character was pronounced as hyphen and dash  though
this may not have worked if every single character labels were
pronounced multiple ways  it is nice to know that the ml
algorithms were robust to such a variation 
vi  c onclusion and f uture w ork
unfortunately  just by looking at the numbers  our algorithm
vastly underperformed when compared to works done by other
researchers  however  there are few details that explain why
this does not necessarily mean that our work is useless  first 
our results indicate that we do not have enough sample size to
fully utilize the ml algorithms  we observe an overfit for all of
the algorithms except for the naive bayes  from the fact that
all three produce excellent result       success rate  when
we set the training and the testing samples to be the same  in
short  the complexity of the ml algorithms are too high for
our sample sizes  therefore  with the inclusion of more data 
our algorithm is predicted to do even better 

   

   
   

   
   

t  adam  m  salam  et al  spoken english alphabet recognition with mel
frequency cepstral coefficients and back propagation neural networks 
international journal of computer applications             volume 
     
d  j  burr  experiments on neural net recognition of spoken and written
text  acoustics  speech and signal processing  ieee transactions on 
                     
r  cole and m  fanty  spoken letter recognition  in proc  third darpa
speech and natural language workshop  pages               
r  cole  m  fanty  y  muthusamy  and m  gopalakrishnan  speakerindependent recognition of spoken english letters  in neural networks 
            ijcnn international joint conference on  pages       
ieee       
j  reynolds and l  tarassenko  spoken letter recognition with neural
networks  international journal of neural systems                      
m  slaney  auditory toolbox  interval research corporation  tech  rep 
              

fi