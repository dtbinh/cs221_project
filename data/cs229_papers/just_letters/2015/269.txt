autoranking amazon reviews
ivan gozali  ilker karakasoglu
stanford university
 igozali  ilker  stanford edu

abstractwe consider the problem of predicting real valued
scores for reviews based on various categories of features of the
review text  and other metadata associated with the review  with
the purpose of generating a rank for a given list of reviews  for this
task  we explore various machine learning models and evaluate the
effectiveness of them through a well known measure for goodness
of fit  we also explored regularization methods to reduce variance
in the model  random forests was the most effective regressor in
the end  outperforming all the other models that we have tried 

i  i ntroduction
in the recent decade or so  online shopping has in many ways
replaced the need to go to physical stores  while this is more
convenient and efficient  and provides consumers with a wider
range of products compared to shopping at a department store 
there are drawbacks as well  one of the main ones being the
inability to physically hold and assess the quality of the product
before purchase 
as such  people are increasingly reliant on product reviews 
as it allows them to assess the product in various aspects to see if
it matches their needs  however  for popular products reaching
hundreds or thousands of reviews  it is difficult for potential
buyers to efficiently sort through the more helpful reviews 
shopping websites  such as amazon  have implemented a
helpfulness vote indicator so that consumers can rate other
buyers reviews to improve the experience  however  most of
the reviews that exist in amazon have either very low or zero
votes  especially for popular products that have a high number
of reviews  its difficult for newly written reviews that are
helpful to be read by people  since only the old and helpful
reviews show up on the top of the review list for those products 
in our project  we pose the problem of designing a supplemental way to rank reviews by looking at their certain features 
such that these reviews which could potentially be helpful can
be surfaced to the top of the list  we attempt to use machine
learning algorithms that are appropriate for string data and
ranking  by analyzing the portion of reviews that have enough
visibility and a high number of votes  we aim to build a model
that could predict the helpfulness of reviews with zero or few
votes 
ii  r elated w ork
there has been a fair amount of related work being done
on amazon product reviews  a study done by minna et al
performed very similar work in using binary classification to
determine whether a review is helpful or not      by extracting several features from the review text and evaluating
performances of different classifiers  including naive bayes and
support vector machines with various kernels  the authors were
able to achieve an accuracy of around     
bolter used distinct classification models for each product
category in amazon  noting that different words might work

better as predictors depending on the category of the product     
words that are frequently used in the electronics category might
not work well when it is used to classify clothing products 
yet another study done by kim et al performed very similar
work in using supervised learning methods to generating scores
to rank reviews      in this study  helpfulness scores for reviews
were derived from how many upvotes or downvotes a review
received from human readers  which in turn were used to
train the regressor  the study concluded that svm regression
showed very promising results  with rank correlations up to
      while the most significant features were shown to be
unigrams  product rating  and the length of the reviews 
having taken a look at some of the studies  we note a couple
of areas that we can further investigate  most of the studies
performed above attempted to fit a binary classifier to determine
whether the reviews are either helpful or not  however  it is
not possible to generate an ordering of reviews by helpfulness
simply by assigning binary labels to reviews  because there is
no quantity to compare whether some reviews are more helpful
than others  therefore  the user will still have to read through
several reviews before being able to make a decision to buy
the product  in this sense  a regressor would be more suited to
perform the task of generating a rank 
in addition  in the study done by minna et al  the authors did
not attempt to tune the hyperparameters to optimize some of the
classifiers that they were using  we note that by considering
effects of regularization  we can reduce the variance of our
regressors such that they may perform better 
we acknowledge that our work is very similar to that of kim
et al  due to time constraints  we limited the scope of our project
to training regressors using a subset of features used by kim
et al  and evaluating the performance by using the standard
coefficient of determination metric for evaluating goodness of
fit in regression problems  our contributions are the evaluation
of several other regressors than those that were considered in
the aforementioned study  and the exploration of regularization
methods to reduce variance and bias in the regression models 
iii  dataset and f eatures
a  dataset
we utilized the set of amazon reviews written within the
date range of may        july       in particular for the toys
and games product category  these reviews were originally
procured and curated for a study done by mcauley et al
     each review within the set contains information about the
reviewer id  product id  review upvotes and downvotes  the
review text itself  the rating the reviewer gave the product  the
title of the review  and the time the review was written  all
formatted in json  for the implementation  we use python
utilizing the libraries of pandas and scikit learn     

fithe original dataset contained      million reviews  in order
to able to extract enough number of trustworthy features and
reflect the objective of this study better  we filtered the review
data with the following criteria      each review needs to have
more than    votes  thereby having enough visibility   and    
the review should exist in popular products  with more than   
reviews   by using these criteria  we reduced our data set from
     million down to about        reviews  we then randomly
selected     of the data as the training set  and the other    
as the test set 





word count
the number of words in the review text 
unique word count
the number of unique words in the review text 
sentence count
the number of sentences in the review text 
automated readability index    
the automated readability index score  which is a measure
of text readability  is computed as follows 
ari       

b  outcome variable
borrowing the idea from the study done by kim et al      we
define the following quantity to provide a measure of the sense
of helpfulness a review provides given the i th review 
y  i   



upvotes i 
upvotes i    downvotes i 

originally  we thought of defining the outcome variable to be
the difference between upvotes and downvotes of a particular
review  this definition  however  doesnt differentiate the case
between a review that has     upvotes and     downvotes
versus a review that has   upvotes and   downvotes  although
one would think that the first review could have been less helpful
due to the review having downvoted by people 
we visualized the data to get a better understanding of it 
the histogram of the review counts grouped by the outcome
variable  as shown in figure    indicates skewness in the data 
approximately     of the reviews have helpfulness scores
between     to   

     
     

nword
ncharacter
     
nword
nsentence

metadata features
we used the number of stars that a reviewer has given a
product in his her review as part of our feature set 
bag of words
we also used the bag of words model to generate additional
features for each review  the bag of words model uses a largedimensional  sparse vector to count word occurrences in a
review text with respect to some vocabulary of words  more
precisely  let x i   rn be the sparse vector containing word
occurrences in the i th review  where n is the size of our
vocabulary  if review i contains c occurrences of the j th word
 i 
in our vocabulary  then xj   c 
iv  r egression m odels
we modeled the problem as a regression  to evaluate the
performance of our regression models  we used the coefficient
of determination  denoted by r     which is a standard measure
of goodness of fit for our model  given that x i  is the data
point to be evaluated  y  i  is the actual value of the outcome
variable  y  i  is the predicted value of the outcome variable  and
y is the mean of all values of the outcome variable in the data
set  the coefficient of determination is calculated as follows 

     
p  i 
 y  y  i    
r       pi  i 
 y  
i  y

     
    
    
    
    
 
   
fig    

   

   

   

upvotes total votes

   

   

histogram of helpfulness  the outcome variable  the ratio of upvotes
in total votes 

c  features
we used the following features extracted from each review
text  classified into broad categories 
textual features
 text length
the count of characters in the review text  including
punctuation and spaces 
 character count
the number of alphabetical characters in the review text

lesser values of r  mean that the model doesnt fit the data
 i e  the model cannot explain the variance of the data   while
it typically ranges from   to    for very low performance cases
r  can be negative 
next  we explain the four models we studied  linear regression  ridge regression  support vector machines for regression
and random forests 
a  linear regression
linear regression poses the problem of solving for the
parameters  that minimizes the following cost function 
j       x  y    
where  x is the design matrix of size m  n created
by
m training  sample as row vectors  and y t  
    having
   
y
y
      y  m  is the vector containing the target
variables for each sample  also note that   z       z t z is the
l   norm of a vector z  rn  
given   we make a prediction given a new data point x  as
follows 
y   t x 

fib  ridge regression
similar to linear regression  ridge regression attempts to
minimize the least squares cost function with the additional term
         in particular  we have 
j       x  y             
the additional term gives preferences to a parameter vector 
with a smaller norm  which makes the regressor less susceptible
to outliers in data  to understand this intuitively  note that if the
parameters are unconstrained  each parameter can be arbitrarily
large  and this happens when the model tries to fit outliers 
by constraining the parameters  we discourage the model to
try its best to accommodate outliers  hence reducing variance 
the addition of this hyperparameter   also called the penalty
constant  allows us to control model complexity and achieve
the optimal balance of bias and variance 
c  support vector machines for regression
while typically used for classification problems  support
vectors machines  svms  can be used for regression as well 
like svms for classification  support vectors machines for
regression  svr  contain all the main features that characterize
maximum margin algorithm 
we can see svrs mathematical foundations and the intuition
of it by illustrating on a linear model  suppose we have a
training set  x  rmn and y  rmk where x i s are the
features and yi  s are the observations  then the linear regression
model is given by
y   f  x    wt x   b
where w  rnk is the parameter vector that we wish to
estimate  and b  rk is the intercept term  to estimate w  we
can use the  insensitive measure  where the goal is to find
a function f  x  that has at most  deviation from the actually
obtained target y  i  s for all the training data  meanwhile  in
order to keep the variance low  we want f  x  as flat as possible 
to achieve this  errors  small residuals  of size less than 
are ignored  we seek to minimize kwk   while using the insensitive measure  formally  we can model this problem as
a convex optimization problem 
 
min kwk  
 
 
s t   

y  i   wt x i    b  
wt x i    b  y  i   

this convex problem is only feasible if there actually is a
function f that approximates all pairs  xi   yi   with  precision 
this may not be the case  to handle infeasible problems  we
soften the loss function by introducing slack variables i   i  
then  the  insensitive loss function    is described by
 
 
if     
    
     otherwise 
finally  we can write the svr formulation as
m

x
 
 i   i  
min kwk     c
 
i  


 i 
t  i 

y  w x  b     i
t
 i 
s t    w x   b  y  i      i


i   i
   
c     controls the tradeoff between the model complexity and
the amount up to which deviations greater than  are tolerated  
and c are two fundamental parameters which in different ways
determine the model flatness  the training errors  the number of
support vectors and the bias variance tradeoff of svr 
the use of kernels  which we shall not discuss in this paper 
is a powerful method used to transform input data into a higher
dimensional space  and can be used with svr as well 
d  random forests
regression with random forests is a powerful nonparametric
technique      it is classified as an ensemble method since it
aggregates many separately grown trees and generates a single
unified model 
regression trees are used as building blocks  a top down 
greedy approach is used to partition the feature space into
a specified number of distinct non overlapping regions  the
number of partitioned regions affects the bias variance tradeoff 
trees typically suffer from high variance  bagging is used to
fight against high variance  for bagging  many deep trees are
grown from bootstrapped samples  each tree has high variance 
but low bias  averaging all these trees reduces the variance  the
number of trees is not a critical parameter  using a very large
number will not cause overfitting  a sufficiently large number
can be chosen after the error levels off 
random forests are also a bagging method but with a
difference  each grown tree only uses a random sample of p
predictors out of all n predictors  the motivation of this is
to decorrelate trees  when all predictors are used  all grown
trees look like each other  therefore  bagged trees are very
correlated  which negatively affects the reduction in variance by
averaging  random forests solves this problem by considering
only  np  n portion of predictors at each tree split  for p   n 
random forests turn into simple bagging 
the parameters to be tuned for random forests are     the
number of random predictors p      the maximum number of
leaf nodes  which determines how deep a tree is grown  and
    the number of trees to be grown and averaged 
to estimate the test error of a random forest  cross validation
is not necessary  out of bag  oob  error is used  it can be
shown that on average a tree uses two thirds of the data  the
remaining one third is out of bag  for a data point  we can
therefore predict the response of it using the trees in which
that data point was oob  it is a valid test error estimate since
trees that were not fit using that point are used 
v  r esults and d iscussion
the scores obtained by all considered methods are shown in
table i 
a  linear regression
we first experimented with the simplest model  linear regression  since linear regression has the intrinsic assumption that
the considered data is linear  it did not perform well  however 
due to the high number of features  the performance was still

firegressor
linear
ridge         
svr  kernel linear 
svr  kernel rbf 
random forest
table i 

 
rtrain
    
    
    
    
    

we attempted to reduce variance for our linear regression
model by using ridge regression  by retraining the model for
various values of the penalty constant  we determined that the
optimal value for the penalty constant is         which
gave the highest score in the test set and cross validation  the
learning curve for ridge regression is shown in figure    as 
increased  the penalty on the model increased and the flexibility
decreased  it resulted in reduced variance  this can be seen
 
 
 
increased  
and rcv
on rtrain
which decreased while rtest
forced insignificant features towards zero in regression  further
increasing  would have penalized too much and decreased
 
 
scores 
and rcv
rtest
    

 
rtrain
 
rtest
 
rcv

    
    
    

c

b  ridge regression

    
    
    
    
    
     
     
      

 
 
 
 
                                           



fig     grid search cross validation for linear svr hyperparameters  red
colors show high cross validation r  scores  up to     while blue colors show
lower cross validation scores 

very low mean and a high standard deviation  it indicates that
it suffers from both high bias and high variance 
secondly  we considered the rbf  radial basis function  kernel 
 
compared to the linear kernel  it performed better on rtrain
 
 
and rcv
  however  its rtest
was not improved  the big gap
 
 
between rtrain
and rtest
shows that there is a serious problem
of high variance  since rbf is nonlinear  it is more prone to
overfitting  again  cross validation was used to pick  and c
 see figure     the pair        and c      scored the highest 

    
 
fig    

   

    



    

    

    

learning curve for ridge regression

ridge regularization resulted in a relatively large improvement in the performance  at the expense of slightly higher bias 
 
we were able to improve the rcv
by      to      
c  support vector regressors
svr has   main factors to tune  kernel   and c  our strategy
was to choose two kernels and optimize  and c for each one 
we first experimented with a linear kernel  and we found that
svr did not perform well  we performed an exhaustive grid
search using cross validation for the hyperparameters  and c 
and the results are shown in figure    observing the results 
we saw while both  and c affected the r  scores   was the
dominant hyperparameter 
in the end  even when we chose the hyperparameters that
gave us the best r  scores  c                     linear
svr still scored a relatively low r  on the training set  the
 
test set and during cross validation  in particular  rcv
had a

score

   
   
   
   

 
rcv

    

score

score

 
rcv
    
    
    
    
    

various regression models r  scores for the training set  the test set  and cross validation or oob  

relatively decent  it is a very rigid method  therefore  it suffered
most from high bias 

    
   

 
rtest
    
    
    
    
    

    
    

   

   



   

   

 

  

c

  

  

fig     cross validation scores of svr with a rbf kernel  each plot is shown
for a varying hyperparameter while the other is kept constant at its optimum
value 

compared to the svr with a linear kernel  svr with a rbf
kernel performed a lot slower  due to its lengthy development
time  we only tested for  and c around a limited range 
d  random forests
unlike the previous methods  random forests are nonparametric  which means that we did not need to make assumptions
about the form of the data when using them 
we first chose a large enough number of trees to grow       
how the scores settled down with the increasing number of
trees can be seen in figure   
we then optimized for the maximum number of leaf nodes
and the number of features  p  considered when looking for a

fi 
rtrain
 
rtest
 
roob

score

   
   
   
       

    
number of trees

    

fig     the scores of random forests given for different the number of
grown trees  after a sufficiently large number of many trees  scores level off
ax expected 

   

   

   

   
score

score

 
split in a tree again using oob scoring  see figure     roob
increased with the number of leaves but then decreased due to
overfitting  same happened for increasing p  as p approached
n  grown trees became more correlated with each other so there
was less reduction in variance  we chose the maximum number
of leaf nodes to be     and p        as pointed out in the
literature      for regression purposes p is best chosen around
n    our results also confirm this 

   
   

   

 
rtrain
 
rtest
 
roob

   

    
                   
         
maximum number of features  p  maximum number of leaf nodes
fig    

learning curves of random forests 

random forests scored much higher than other methods  we
attribute this to their nonparametric character  data is unlikely to
have an easily parameterized structure  therefore  previous parametric techniques performed worse due their wrong assumptions
 
 
about the data  the big difference between rtrain
and rtest
signals overfitting 
finally  using this most successful model  we ran a significance test on the model to see the most important features
on the outcome using the gini importance criteria  as seen in
figure    textual features dominate the bag words features  the
words relevant to the subject are at the top among bag of words 
star rating given to the product by the reviewer is the most
significant feature  investigating data  we see that most helpful
reviews are the ones which rated the products highest  here we
emphasize that textual features based on text length are highly
correlated with each other  therefore  it is hard to argue which
ones are more significant than others  however  their combined
significance is clear 
vi  c onclusion and f uture w ork
we have successfully shown that there is a possibility to automatically rank reviews  we modeled the problem as regression
and extracted features that represented the data  we applied four

star rating
text length
character count
readability
unique word count
word   game 
word count
punctuation
sentence count
word   xbox 
 
fig    

relative feature importance

  

  

  

  

   

top ten most significant features according to the random forest
regressor

different regression methods and compared their properties  we
observed that random forests performed the best 
due to the complexity of the problem at hand  we believe
there is a lot of room for improvement in the future work by addressing three main challenges  creating a more complex feature
set  reducing high dimensionality and overcoming skewness 
the textual features that we used were very basic ones that
was only concerning the text length properties and readability 
more advanced natural language features such as sentiment
analysis can be added  moreover  bag of words can be improved
by using term frequency   inverse document frequency  tfidf   or the word vec     library developed by google that
implements the continuous bag of words or the continuous ngram models  another feature can be the background of a
reviewer  a reviewer with previous helpful reviews may have a
higher probability of making more helpful reviews in the future 
the feature space is relatively high dimensional which makes
it necessary to have a large data set in order to get decent results 
we are also aware that some of the features are highly intercorrelated  to address these problems  feature selection methods
or principal component analysis shall be applied 
lastly  skewness has a big negative impact on the performance of models  especially  support vector machines are
known to be susceptible to skewness of data  as studies have
indicated       in order to deal with this problem  simple
techniques such as upsampling  downsampling  or stratified
sampling of the data can be employed  while the study mentioned also provided another entire methodology to mitigate
skewness 
r eferences
    j  rodak  m  xiao  and s  longoria  predicting helpfulness ratings of
amazon product reviews  http   cs    stanford edu proj     jordan 
  rodak    minna   xiao    steven   longoria    predicting 
  helpfulness   ratings   of   amazon   product   reviews 
pdf        accessed             
    s  bolter  predicting product review helpfulness using machine
learning and specialized classification models         online   available 
http   scholarworks sjsu edu etd projects    
    s  m  kim  p  pantel  t  chklovski  and m  pennacchiotti  automatically
assessing review helpfulness  in proceedings of the      conference
on empirical methods in natural language processing  association for
computational linguistics        pp         
    j  mcauley  r  pandey  and j  leskovec  inferring networks of
substitutable and complementary products  in proceedings of the   th
acm sigkdd international conference on knowledge discovery and
data mining  ser  kdd     new york  ny  usa  acm        pp     
      online   available  http   doi acm org                        

fi    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion  o  grisel 
m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas 
a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay 
scikit learn  machine learning in python  journal of machine learning
research  vol      pp                 
    j  p  kincaid  r  p  fishburne  r  l  rogers  and b  s  chissom 
derivation of new readability formulas  automated readability index 
fog count and flesch reading ease formula  for navy enlisted personnel 
tech  rep   feb         online   available  http   www eric ed gov 
ericwebportal detail accno ed      
    l  breiman  random forests  machine learning  vol      no    
pp              online   available  http   dx doi org         a 
 a             
    t  hastie  r  tibshirani  and j  friedman  the elements of statistical
learning  ser  springer series in statistics  new york  ny  usa  springer
new york inc        
    x  rong  word vec parameter learning explained  arxiv preprint
arxiv                 
     g  wu and e  y  chang  class boundary alignment for imbalanced
dataset learning  in in icml      workshop on learning from imbalanced data sets       

fi