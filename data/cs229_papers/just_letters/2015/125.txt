cs     final project  autumn     

 

predicting bill passage
kyle gulshen  noah makow  pablo hernandez
stanford university   cs    

abstractin the legislative process  vast amounts of time and
effort are spent on working to understand how various congresspeople will vote on a bill  our research has sought to build
a system that will model how each congressperson will react
to a newly introduced bill  such a model might help congresspeople to get a sense of the reaction a bill they are seeking
to propose would provoke  and provide the general public with
a sense of how likely a given bill in congress is to pass and
become a law  our algorithm uses a training set where each
previous vote from a congressperson becomes a training example 
properties of the bill and voting circumstances are features  and
the outcome is how the congressperson actually voted on the bill 
our research has explored the effectiveness of different machine
learning algorithms  glms and svms  on modelling this data 
next steps will include building models for each congressperson 
and aggregating our predictions over the entire congress to
determine whether or not a bill will ultimately pass 
keywordsbills  congress  machine learning  logistic regression  svm  naive bayes  dw   gaussian  kernel

i  i ntroduction
in the legislative process  vast amounts of time and effort
are spent on working to understand how various congresspeople will vote on a bill  our group sought to explore what
ultimately goes into having an individual congress person
vote in favor of or against a certain bill  the thought process
behind taking this approach revolved around the idea of
lobbying individual representatives  equipped with a model of
how representatives might vote  lobbyists would know whom
to target and how to allocate resources in order to gain support
for a particular bill  in addition to lobbying  legislators can
get a sense of which representatives will be in favor of a bill
they are crafting  and tailor their efforts to reflect these results 
our approach to this problem involved training a model
for each sitting representative  merging vote history data from
govtrack and bill specific features from congressional bills 
we were able to train our models based on a representatives
voting history  given a new bill as input  the model would
output whether or not the given representative would vote in
favor of or against the bill  for the purposes of this paper 
we have trained models for three different representatives  in
practice  a model would be trained for every single sitting
representative  and it would be possible to aggregate results
across the entire congress to predict whether or not a bill would
pass and with what margin 
ii  r elated w ork
our approach to this problem has not been extensively
researched in the past  most research in this space has been

focused on leveraging bill text and making a single prediction
as to whether or not a bill will pass or fail  yano  smith  and
wilkerson        demonstrate this approach  augmenting bill
features related to bill sponsorship  committee membership 
state  and time of introduction with the text of the bill  having
trained on bills in the        th congresses  their model
was able to attain a     error rate on bills in the    th
congress  smith        generalizes the challenge addressed
in his      paper  given a body of text t pertinent to a
social phenomenon  make a concrete prediction about a
measurement m of that phenomenon  obtainable only in
the future  that rival the best known methods for forecasting
m   this approach is starkly different from the approach we
took  which focuses on making predictions for individual
representatives  we believe that our approach to the problem
provides finer granularity and  as mentioned above  gives
valuable insight for lobbyists and others interested in swaying
or learning about the reactions of individual representatives 
smith  baek  et al         take another novel approach to
the problem  utilizing campaign finance data as input features
for their learning  data was collected from publicly available
sources on donations from corporations and individuals to
politicians  the stated opinions of corporations and other
organizations on legislative actions  and the records of how
members of congress voted on these measures  smith  baek 
et al         take a similar approach of predicting votes for
each congressperson  the accuracy of their models varies
significantly based on the approach  in the end  they conclude
that there is no strong evidence that politicians vote based
solely on the financial contributions they receive from certain
industries  however  there does exist a strong correlation
between money flow and political party that is reflected in
the voting process where an individaul politican is very likely
to vote along his or her party line 
poole  rosenthal              provide a description of their
d nominate score  which is a critical feature in our training
set  the d nominate score is a multidimensional classification that attempts to ideologically cluster representatives based
on their voting history and other features  the first dimension
provides a rough approximation of how liberal or conservative
a representative is  in this paper  this score is referred to as
dw   where dw          and   corresponds to liberal 
whereas   corresponds to conservative 
iii  dataset and f eatures
our dataset comes from two different sources 
congressional bills had the data that we used for the
features on our bills starting at the   rd congress  govtrack

fics     final project  autumn     

 

helped us to collect information on how individual congresspeople voted while they were active in legislation  the
three congresspeople we collected data for were  roy blunt
  a republican senator from missouri  nancy pelosi   a
democratic representative from california  and former
speaker of the house   and paul ryan   current speaker of
the house for the republican party  for each individual  we
have around       examples that served as our dataset 
significant preprocessing had to be done  using sql  the
two data sets had to be merged  according to the respective
bill id  congress number  and house it was proposed in 
data examples missing key features were discarded  the
information needed for the merge did not match correctly 
so the congress people information had to be processed in a
manner such that it could be compared for a join  the final
set of features we examined were  congress  bill number 
chamber  commemoratory bill indicator  major  minor 
private bill indicator  age  delegate  district  dw   dw  
frstconh  frstcons  gender  leadcham  majority  mref 
party  and state  ultimately  our feature set was narrowed
down to the bill authors dw  nominate score  along
with each bills major topic and minor topic  the dw nominate score is a measure of how liberal or conservative
a particular author is   on a score ranging from          our
dataset was all included in tables  for example 
congress bill id hr s

dw 

major minor

   

 

 

     

  

    

   

    

 

      

  

    

  

   

 

     

 

   

  

    

 

     

 

   

  

    

 

      

  

    

   

    

 

     

 

   

iv  m ethods
we investigated fitting our data using   different approaches 
 a  logistic regression   b  support vector machines with a
gaussian kernel  and  c  naive bayes  all the aforementioned
algorithms are suited to classification problems  but there are
formulations and assumptions specific to each  methods are
described below 
a  logistic regression
we first explored using logistic regression to model our
data  here  we aim to minimize the following cost function 
m
 x
j  
 h  x i     y  i    
  i  

   

where h   t x  and the i s are the weights parametrizing
the space of linear function from x to y  in order to learn
the parameters   we minimize the cost function j   using

stochastic gradient descent  in this algorithm  we repeatedly
run through our training set  and update our parameters 
according to the gradient of the error with respect to the current
training point under observation  i e 
 i 

j    j    y  i   h  x i    xj

   

b  svm with a gaussian kernel
our data was also learned on using a support vector
machine with a gaussian kernal as our model  here the
objective is to minimize 
m

x
 
min w b kwk    c
i
 
i  

   

y  i   wt x i    b      i   i           m

   

i     i           m

   

such that 

our kernel selected in this algorithm is in the form of 
kx  zk 
 
   
   
providing us with a value close to   when x and z approximately equal  and   as their difference grows  the goal of
the svm algorithm is to find the maximum margin separating
hyperplane between the data  and by examining solving the
dual of this optimization problem  we are then able to learn
efficiently in high dimensional spaces 
k x  z    exp 

c  naive bayes
for our naive bayes approach  we seek to now use a generative model in order to be able to accurately fit our data  here 
the goal is to model the class prior and the conditional feature
prior   p y  and p xx  y   respectively   using a multinomial 
multivariate distribution  this is done since it is for predicting a
model whos observations are categorical  thus the probability
of a congressperson voting yes on a bill becomes 
p y 

m
y

p xi   y 

   

i  

where xi        xm are the features of the dataset corresponding
to the voting decision tethered to class y  and where our model
is parametrized by 
i y   p xi   y 

   

y   p y 

   

here we also institute laplace smoothing to shift some of
the probabilities towards potential test sets that were not seen
in the training set  to allow for this  the estimates for our
parameter  for predicting   as the output  now becomes 
pm
 i 
y  i 
        
i     xj    
pm
    
j y    
 i 
    
i    y

fics     final project  autumn     

v 

 

e xperiments  r esults  d iscussion

we ran into interesting challenges along the way when
trying to model this problem  the following sections address
these issues 

c  results
the following are the confusion matrices reported when
running our three methods on nancy pelosis dataset  training
on      testing on remaining      
logistic regression
p n

a  establishing a baseline

p       

to properly measure our performance it was necessary to
establish an appropriate baseline to measure against  one
approach  given our knowledge of the partisan nature of
congress  is to measure against a party line baseline in which
the predicted vote is yes if and only if the congressperson
proposing the bill is in the same party as the congressperson
who is voting  this approach  however  misclassified around
    of votes  in fact  a more accurate baseline model is
simply to predict yes for all votes  resulting in around       misclassification  depending on the congressperson  this
model is superior likely due to the structure of the voting
process in congressmost bills that make it onto the floor are
favored by the majority  and there are a number of bills that
are likely to receive a near unanimous vote 

b  unbalanced classes
our data suffered from unbalanced classes  the following
table demonstrates the disparity between the number of yeas
and nays 
blunt pelosi ryan
y

    

    

    

n

   

   

   

 n

    

    

    

undersampling our datathat is  excluding some yes
votes from the data until there was a roughly equal number
of yes and no votesfailed to improve anything  the model
still almost always predicted yes for the test set  meanwhile
maintaining near    training error for the svm model   we
then tried to change the cost associated with false positives 
by weighting false positive cost at     times or more of the
cost of false negatives  the model would switch from voting
all yes to all no on the test set  in fact  manipulation of this
parameter only resulted in an oscillation between these two
extremes   note  this approach was also tried on the original
unbalanced data  but no weighting appeared able to change
the voting behavior from all yes to all no  
we also went the route of oversampling  since our research
resulted in information that oversampling could increase the
minority class recognition with sacrificing less of the majority
class recognition rate  adding new examples would increase
the time to learn over the training set  but this proved to be
negligible in practice  attempts to oversample also failed to
improve the results of our models 

n       
svm
p n
p      
n      
naive bayes
p n
p       
n

  

  

below are precision   accuracy results across all algorithms 
when testing on     of each dataset 
logistic regression
person precision accuracy f  score
blunt

   

     

      

pelosi

      

      

      

ryan

      

      

      

svm
person precision accuracy f  score
blunt

   

      

      

pelosi

      

      

      

ryan

   

      

      

naive bayes
person precision accuracy f  score
blunt

      

      

      

pelosi

      

      

      

ryan

      

      

      

for logistic regression  the average errors were produced
when running k  fold cross validation with    k      when
done across all congresspeople 
person
blunt

train

test

             

pelosi              
ryan

             

fics     final project  autumn     

below are the plots comparing test vs  train error across
the congresspeople  when running k fold cross validation with
   k     

 

the training and test error on a       train test breakdown
of the data  we determined that the training error was
unacceptably high and so sought out a better model to reduce
the high bias we observed 
having implemented a general glm  we tried a variety
of different link functions  but none outperformed logistic
regression  we then switched to a svm model  a linear
kernel also failed to outperform logistic regression  an
svm with gaussian kernel was finally able to produce
less training error then logistic regression and in fact
produced training errors close to     with our test error still
high  we concluded that the svm was subject to high variance 
while this was encouraging  the model nearly always
predicted a yes vote on the test data  realizing this was likely
due to unbalanced classes  we tried a variety of methods
to improve our results  as a result  we experimented with
both oversampling and undersampling  duplicating negative
examples to balance  oversampling  the classes did not
improve our models  similarly  undersampling increased
training and test error  we also experimented with different
threshold values  in other words  our decision boundary
hyperplane became t x    where we were able to vary
         
while altering the threshold gave us a better balance of
misclassified examples  it increased the overall error and we
decided to not include this in our final models  we believe
that altering the threshold is helpful  but in doing so exposed
other issues with our model  such as that our feature set may
not actually be very good  lastly  we also tried different cost
functions with the svm to weight the cost of false positives
versus false negatives  as discussed above  for very small
changes in these costs our model oscillated between always
outputting   or always outputting   

vi 

c onclusion  f uture w ork

similar approaches to tackling a problem like this revolved
around analyzing bill text  our approach focused on features
pulled outside of the bill itself  and produced comparable
results  looking at the graphs across all our models  it seems
that both the logistic and the naive bayes model are suffering
from high bias  a problem ultimately tracing back to feature
selection  the svm algorithm suffers from high test variance 
naive bayes classification was our best performing algorithm 

d  discussion
our first approach was to quickly run logistic regression on
each individual congresspersons dataset  we then measured

future work would focus mainly on manipulating the
dataset  most of the issues that our team ran into revolved
around not having sufficient data  on the no votes  to be
able to learn on   with more time  we would analyze the
feature selection process in finer detail  and potentially explore
different sources of data to tackle the same problem  we would
also seek to integrate existing research into our approach  such
as including bill text analysis and financial data 

fics     final project  autumn     

r eferences
   

   

   
   
   

   
   

   

yano  t   smith  n  a   wilkerson  j  d         june   textual predictors
of bill survival in congressional committees  in proceedings of the
     conference of the north american chapter of the association
for computational linguistics  human language technologies  pp           association for computational linguistics 
smith  s   baek  j  y   kang  z   song  d   el ghaoui  l   frank  m 
       december   predicting congressional votes based on campaign
finance data  in machine learning and applications  icmla       
  th international conference on  vol     pp            ieee 
smith  n  a          text driven forecasting 
poole  k  t   rosenthal  h          congress  a political economic
history of roll call voting  oxford university press  chicago
poole  k  t   rosenthal  h          d nominate after    years  a
comparative update to congress  a political economic history of roll call
voting  legislative studies quarterly       
govtrack api documentation  govtrack us  web    dec       
https   www govtrack us developers api  
download
congressional
bills
project
data 
congressional
bills
project 
download 
web 
 
dec 
     
http   www congressionalbills org download html  
efficient resampling methods for training support vector
machines with imbalanced datasets  web    dec       
http   www cs ox ac uk people vasile palade papers resampling methods svm pdf 

 

fi