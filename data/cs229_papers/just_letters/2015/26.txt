speakertagger  a speaker tracking system
jordan cazamias
jaycaz stanford edu

  

naoki eto
naokieto stanford edu

abstract

our project is a speaker tagging system  which can distinguish between speakers in a recording of a conversation 
we applied various supervised and unsupervised machine
learning algorithms such as support vector machines  neural nets  gaussian mixture models  and kmeans clustering 
to assign each segment of audio data to the correct speaker 
we also tried various combinations of different acoustic features like spectral flatness  loudness  and mfcc  overall 
neural nets worked best on average  with roughly     accuracy  svms had the best maximum performance  with one
file in particular classified at nearly     accuracy  however 
for all approaches  there was a very wide variance in performance  most approaches swung widely between roughly
    and     accuracy for different sound files  our next
step to further develop our system would be to find the root
cause as to why certain files resulted in such poor performance  after that  additional optimizations could be made
to create a system that can work in real time rather than on
a pre recorded sound file 

  

ye yuan
yy     stanford edu

value using a parameter search  additionally  for a supervised approach  part of the speech file is hand annotated
with speaker times so the system can attempt to fill in the
rest 

input  raw audio recording of a conversation

   feature extraction
break up raw data into chunks  apply mfcc

   speaker activity detection
eliminate chunks where no one is speaking

introduction

recording a conversation such as a business meeting  a
news broadcast  or an interview can help preserve important conversational moments  but working with audio data
is a frustrating task  when you actually have to go back
through an archive of recordings  looking for one particular
moment  its quite the chore since audio is not easily searchable  however  if you could transcribe the speech in your
audio files  suddenly navigating through them becomes a
breeze  this is why so much research work is being put into
systems that can automatically extract the text from audio 
beyond that  however  there are other useful pieces of metadata that would make transcripts much more rich and useful 
one important example  for a recording of a conversation
with multiple speakers  its not just useful to know whats
being said  but whos saying it  this is the focus of our
project 
we have created a system to perform speaker diarization  the process of distinguishing between the speakers in a
recording of a conversation  we will also informally refer to
it as speaker tagging  it is primarily approached as an unsupervised learning problem  although supervised approaches
are possible  the main input to the system is a raw audio
file of a conversation  and the output is a timeline showing
when person a is speaking  when person b is speaking  and
so on  other metadata can be provided in the input as well 
for instance  the number of speakers can be directly fed into
the system to save time  although its possible to guess this

   segmentation phase
separate different speakers  only looking locally
       

                             

   

   clustering phase
assign similar clusters to a global speaker id
a a a a

b b b b b b b b a a a a a a a

b b

final output 
a timeline showing when each speaker is speaking

a

b

a

b

figure    the general steps of a bottom up diarization algorithm  note that some of these steps may
be ignored or merged together 

   

related work

the primary influence on our diarization system was anguera
et  al s literary review of the field  speaker diarization  a
review of recent research      in it  the general diarization

fialgorithm is presented  as shown in figure    and various
approaches to each piece of the algorithm are documented 
the paper also documented which of the approaches was
most popular for each step of the algorithm 
from this paper  we were able to find concrete examples of
state of the art diarization systems and get an idea of what
approaches are most popular  for instance      is an example of a more typical diarization system  it uses bottom up
clustering  breaking an audio file into uniform segments and
clustering these segments using gmms  similar to our approach 
by contrast      takes a top down approach  creating an
evolutive hidden markov model that represents how the
speakers in a particular conversation transition from one
speaker to the next 
we also discovered papers on real time diarization systems  although we did not end up creating a real time diarization system  we feel that a real time system is a good
next step to take once we have a robust system that can
work on entire files 
friedland and vinyals      for instance  take a supervised
approach to live speaker identification  where each speaker
provides about    seconds of uninterrupted speech before
proceeding with the conversation  they achieved around
    accuracy with     seconds of latency  while their system is fairly accurate  however  the practical benefits of a
real time system are counteracted by the fact that there is
a training requirement 
on the other hand  kinnuen et  al      achieve an unsupervised system with roughly     accuracy that can identify a speaker in less than a second  it uses vector quantization rather than the popular gmm based approach  as
well as some smart computational speed up methods such as
confidence based speaker pruning  its an example of what
kind of results can be achieve when great amounts of work
are put into choosing the best algorithms for the job  for
our purposes  however  it is outside of what we could achieve
in the scope of time given  and so is more of a source of inspiration for future work 
figure   provides a general framework for a bottom up
diarization algorithm  variations  of course  can exist  and
these steps also dont take the specific implementation algorithms into account  as previously mentioned  the different
approaches can vary widely 

  

   
     

algorithm
data preprocessing

we divided our non overlapping speech data into intervals  with our interval size being      samples  or roughly
     seconds  after this preprocessing  each audio file had
roughly between       and       intervals of speech and
non overlapping speakers data  since the amount of data in
each audio file being composed of overlapping speakers varied widely  from     to     

     

features

the following main features were tested individually and
combined  loudness  energy  mfcc  and spectral flatness 
to get these features  we used a feature extraction tool called
yet another audio feature extractor  yaafe      
loudness
loudness is defined in our models as the energy in each
bark scale critical band  normalized by the overall sum 
bark scale critical bands are    frequency bands  ranging
from    hz to       hz  loudness is dependent on both
energy and frequency  loudness was selected because some
people talk with an energy at certain frequencies that are
different from others  of course  a persons loudness can
change and could depend on emotion  i e  when a person is
excited  his her loudness may change  
energy  mel frequency cepstral coefficients  mfcc  
spectral flatness
energy is defined in our models as the root mean square
of the samples in the audio frame  mel frequency cepstral coefficients  mfcc  are coefficients that are built from
mel scale filters being applied to frequencies  the mel scale
determines from the actual frequency the frequency that humans perceive  also known as pitch   in our feature extraction     mfcc coefficients are used  spectral flatness is
calculated in our models as the ratio between the geometric
and arithmetic mean of the power spectrum  power spectrum is defined as the square of the fourier transform of the
signal  energy  mfcc  and spectral flatness were selected
because they have been shown to perform well relative to
other features in past papers     

implementation

our implementation will be a bottom up approach that
uses some classification technique to determine which speech
segment belongs to which speaker  we will examine the use
of both supervised and unsupervised approaches 

   

parts with speech and no overlapping speakers 

datasets

our dataset of choice is the international computer science institute  icsi  meeting speech data corpus
 https   catalog ldc upenn edu ldc    s     a collection of
recordings of    meetings  these meetings totaled roughly
   hours and had a sample rate of       hz  the meeting
audio is recorded in a special  sph format  but was converted
to a  wav file before being used in our system  each meeting also provides a hand annotated transcript with timestamps specifying when each speaker is speaking  from this
transcript  we can identify non speech  speech with specific
speakers  and overlapping speech  we focused mainly on the

combined
we also looked at the feature where we stacked loudness 
energy  mfcc  and spectral flatness features in an array to
experiment with combining features 

     

number of speakers algorithm

number of speakers
determining the number of speakers was attempted using schwarzs bayesian information criterion  bic   bic is
used for model selection and has a penalty term to lower the
possibility of the model overfitting  bic is calculated as
bic      loglikelihood   d  log n  
where loglikelihood is the maximum log likelihood of the
model  d is the number of parameters  n is the number of
data points  and d  log n   is the penalty term  the smaller
the bic  the better the model     

fiunfortunately  model selection with bic still tended to
overfit  predicting over    clusters consistently for each of
the audio files  which only had at most    speakers 

     

unsupervised learning on speaker tagging nonoverlap audio

for unsupervised learning  both kmeans and gaussian
mixture models  gmm  were used  note that for unsupervised learning  labels given by our models did not always
necessarily match up with labels given in the data set  so
frequency mapping was used  basically  the cluster with the
most data points was matched with the data that had the
label that appeared most frequently  and so on 
kmeans clustering
kmeans clustering is a simple unsupervised learning algorithm that clusters data into k number of groups  qualitatively  in this algorithm  we first randomly choose k points
to be our initial centroids  then  we group each data point
in the data set with the closest centroid  the positions of
the each groups centroids are recalculated  and the process repeats until the positions of the centroids converge 
kmeans was chosen because it has been shown that this simple method has led to very similar performance as gmms
     we used pythons scikit learns kmeans module 
gaussian mixture models  gmms  gaussian mixture model is another unsupervised learning clustering algorithm  gmms assume that the data points are formed
from a mixture of gaussian distributions with unknown parameters  it utilizes the expectation maximization  em 
algorithm and groups data points by maximizing the posterior probability that the data point belongs in a cluster  the
em algorithm is used to estimate parameters when some of
the random variables are missing  qualitatively  the em algorithm is an iterative process over two steps  estimating
the missing data given current estimates of the parameters
and observed data via conditional expectation e step   and
maximizing the likelihood function with the new estimated
data to get new estimates of the parameters  m step   we
used pythons scikit learns gmm module 

to extend this to a multiclass classification system  we use
the one versus rest approach  where for a set of possible class
labels  y         yk    we train k classifiers f   x        fk  x  
where fj  x  distinguishes whether a data point is in class j
or is in any one of the other classes  we again used pythons
scikit learns svm module 
supervised learning with a neural network another
approach we experimented is to learn the speaker label via
neural network  the neural network we implemented is a
two layer fully connected neural network  on the hidden
layer  the sigmoid function was used to determine if a neuron
would fire  we use softmax for the scorer on the output
layer 
this supervised learning model takes the first third of the
audio file as training data and the second part as testing
data  the network can be represented as 

h   w  x   b  
z   sigmoid h 
   w  z   b  
ej
pj   p 
j
je
which p is a vector  representing the probability of a data
point x of having each of these labels  pj is the j th element
in p  the model will pick the highest probability and its
associated label will be the predicted  we used our own
implementation for neural networks 

  

results and discussion

the accuracy of our speaker tagging is shown in table  
for our features and algorithms  this accuracy is calculated
by the following formula 
number of intervals that are correctly classif ied to a speaker
total number of intervals

     

supervised learning on speaker tagging nonoverlap audio

for supervised learning  both support vector machines
 svm  and neural networks were used  we chose these two
algorithms because they have been used in past papers about
speaker diarization with some success           
supervised learning with svm svm was used to
learn speaker models given a subset of annotated data as
a training set  by default  to find the speakers in a file  we
fed in    minutes of non silent  non overlapping speech from
the file to train a multiclass speaker model 
under the hood  we are simply using svms with linear
kernels  i e  we are finding
 
arg min kwk 
w b  
subject to  for any i              n 
y  i   w  x i   b    

we can see that kmeans seems to work best on energy 
while gmm works best on spectral flatness  taking into account their standard deviations  kmeans and gmms performances overall are quite similar  as was expected 
in the svm linear classification model  we analyzed the
bias and variance for how well the data is fit into the model 
in general  a high variance value may fit the training data
set very well  but possibly overfit the training set and result in some noisy patterns  high bias value indicates low
risk of overfitting  but might also represent underfitting the
training data 

table    bias variance analysis of svm linear classification
   
   
   
   
   
   
c parameter
bias 
                                  
variance
                                   

fitable    results of the speaker
algorithm
features
loudness
mfcc
kmeans
energy
spectral flatness
combined
loudness
mfcc
gmm
energy
spectral flatness
combined
loudness
mfcc
neural net
energy
spectral flatness
combined
loudness
mfcc
svm
energy
spectral flatness
combined

hit rate of the models with icsi meetings
max
min median mean std  dev 
     
     
           
     
     
     
           
     
            
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
            
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     
     
     
           
     

in table    we can see that the training accuracy is slightly
lower  and yet still in the ballpark of the test accuracy 
the model for each feature is tuned separately  since the
size of each data point is different   loudness has    elements 
mfcc has     and spectral flatness and energy have only
one  the depth and size of the model is also different  the
size of the hidden layer is    for spectral magnitude and
loudness and yet    for mfcc  the learning rate is      
and the regularization is           for all the models  all
models are trained on the training set for     iterations 

  

figure    the plot of squared bias and variance
against c parameter
from figure   and table   we can see that the bias reaches
a peak when c is around     and the negative peak when c
is around      while variance remains about the same value
as c changes  recalling that mean square error is bias   
v ariance   irreducible error  we choose the point where
this sum is minimal  in this case  we can choose      the
reason behind the near constant value of these two values
may be due to a large  fairly homogeneous data set 
for the neural network  we can observe whether the training is over fitted or under fitted by comparing the training
accuracy with the testing  generally  when the training accuracy is too high and much higher than the testing accuracy  it is a sign of over fitting 

table    mean training testing accuracy of neural
network model
loudness mfcc energy spec mag combined

train
     
     
     
     
     
test
     
     
     
     
     

conclusion  outlook  and future
work

speaker tracking is a rather broad topic with many interesting and open ended problems  ranging from just determining the number of speakers to solving the problem of
speaker overlap  which is still an open problem in the field 
we tried to at least touch on some of these problems  with a
main focus on tagging the speakers in non overlapping conversations  for unsupervised learning  gmms tend to generally slightly outperform kmeans  gmm on spectral flatness
seems to perform the best for our models of unsupervised
learning  supervised learning on average performs better
than unsupervised learning  which is expected since the unsupervising models have no prior information except for the
number of speakers  on average  neural nets perform better
than svm  though in certain situations svm can perform
extremely well  nearly flawlessly classifying non overlapping
speech segments 
unfortunately  our methods are not as robust as we would
have hoped  it appears that they can perform quite well on
some audio files but also perform relatively poorly on other
audio files that are from the same location and obtained
using the same method  we would like to better understand
what causes our methods to perform so differently 
our implementation does not run in real time and runs on
recorded data  in the future  we can use vector quantization
to shorten processing time  vector quantization is a form of
approximation that maps an infinite set of vectors to a finite

fiset of vectors  which can aid in real time analysis  however 
a major problem with our system  as previously mentioned 
is when there are multiple speakers talking simultaneously 
this was significant in our data set  as overlapping speakers
composed of around     on average of each audio file  there
are at least three major parts to this problem  determining
when speaker overlap occurs  determining the number of
speakers in the overlap  and actually extracting the data for
each of the overlapping speakers  one approach would be to
apply a diarization module where the wave characteristics of
several speakers talking together is different than the wave
characteristics of one speaker     

  

references

    xavier anguera miro  robust  speaker diarization for
meetings  speech processing group  department of
signal theory and communications  universitat
politecnica de catalunya  barcelona  october     
    yaafe  an easy to use and efficient audio feature
extraction software  b mathieu  s essid  t fillon 
j prado  g richard  proceedings of the   th ismir
conference  utrecht  netherlands       
    m  ben  m  betser  f  bimbot  and g  gravier 
speaker diarization using bottom up clustering based
on a parameter derived distance between adapted
gmms  in proc  int  conference on spoken language
processing  icslp   jeju island  korea  october      
    rao  k  sreenivasa  and sourjya sarkar  robust
speaker verification  a review  springerbriefs in
electrical and computer engineering robust speaker
recognition in noisy environments                web 
    kinnunen  t   e  karpov  and p  franti  real time
speaker identification and verification  ieee
transactions on audio  speech and language
processing ieee trans  audio speech lang  process 
                     web 
    friedland  gerald  and oriol vinyals  live speaker
identification in conversations  proceeding of the   th
acm international conference on multimedia   mm
           web 
    vallet  felicien  slim essid  and jean carrive  a
multimodal approach to speaker diarization on tv
talk shows  ieee trans  multimedia ieee
transactions on multimedia                      web 
    anguera  xavier         robust speaker diarization for
meetings  ph d  thesis  universitat politecnica de
catalunya  
    schwarz  g          estimating the dimension of a
model  annals of statistics       pp          
     a  temko  d  macho  and c  nadeu  enhanced svm
training for robust speech activity detection  in
proc  icassp  hawai  usa       
     s  jothilakshmi  v  ramalingam  and s  palanivel 
speaker diarization using autoassociative neural
networks  engineering applications of artificial
intelligence  vol                

fi