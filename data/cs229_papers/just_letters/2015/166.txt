feature cost sensitive random forest
anna thomas
stanford university

thomasat   stanford   edu

abstract
in many applications  it is necessary to consider
not only the predictive power of a machine learning model  but also its computational cost at test
time  here we explore greedy methods for feature
cost sensitive random forest training  we also consider the case where different features share common
subroutines or other dependencies such that computing one reduces the computational cost of others  we
formulate the resulting problem as submodular minimization  therefore admitting a polynomial time solution  we demonstrate that for diverse applications 
test time cost can be reduced without significantly
compromising accuracy 

   introduction
in recent years  controlling test time computational cost has
emerged as a key challenge in the application of machine learning  if it is possible to achieve similar prediction accuracy
while  for example  significantly reducing the load time of a
web page or the cost of a medical test  then it will likely be
beneficial to incorporate knowledge of this tradeoff into model
training 
here we focus on the costs induced by the process of extracting
features at test time  as some features may be more expensive
to compute than others  we explore strategies for incorporating
this information into model training  with a tunable parameter
to allow user control over the tradeoff between prediction time
accuracy and feature cost 
in this project we focus on the random forest  breiman et al 
       which is a powerful and widely used model in machine
learning  it has been used successfully for body parts recognition  hand tracking  biological pathway analysis  ecological
classification  and many other applications  sharp et al       
shotton et al        cutler et al        diaz urte et al        
however  breimans random forest algorithm does not incorporate the cost of feature acquisition in training  this may result in situations where for two features which are near identical in information gain  the metric used to choose node splits 
but widely different in acquisition cost  the more expensive fea 

cs    final project      

ture is chosen  aggregated over many features and the various
trees of the forest  choices such as these may significantly increase the overall prediction time cost of the forest without corresponding improvements in performance 
the goal in this work will thus be to achieve classification accuracy as similar as possible to the standard random forest algorithm  while reducing the cost as much as possible  the
input is a labeled training set  feature costs  and a user determined tradeoff parameter between computational cost and
prediction accuracy  denoted  in following sections   and the
output is the trained forest 

   related work
early work in this area focused on building cascades of classifiers in order to reduce test time cost  for example  viola and
jones        develop a widely used object detection cascade
which allows background or other low probability regions of
an image to be discarded quickly 
xu et al         develop an extension to stagewise regression
in which a budget exists for the total cost  they solve a relaxation of their optimization problem via coordinate descent  and
show that it outperforms prior work  gao and koller       
introduced an ensemble method in which features are chosen
dynamically based on their expected classification gain in addition to its computational cost  thereby creating an instancespecific decision path for every test example  they show that
their method is able to meet the classification accuracy of traditional methods while significantly reducing the computatonal
cost 
karayev et al         do not focus on a specific classifier  but
rather develop a general method for feature selection based on
learning a markov decision process  they learn a feature selection policy via policy iteration  and exceed the performance
of xu et al         on the scene    dataset  one recent paper 
nan et al          explored a random forest model in which
a budget exists for the total feature cost  they grow trees using minimax cost weighted impurity splits and show that their
algorithm outperforms the then state of the art in tree based
cost sensitive classification 
in this paper we explore a different but related problem  in
which we do not have an explicit budget  but rather aim to maximize the weighted difference of a function which reflects the
quality of the features used in the forest and a function which
reflects the cost  section   of this work also differs from previ 

fious work in that we formulate cost sensitive feature selection 
in the case where we have access to a dependency graph between features which represents shared computational subroutines  as a submodular optimization problem 

   greedy forest construction
     problem formulation
similar to the formulation in nan et al        our goal
is to learn a classifier f from a family of functions f
that minimizes the sum of the expected errors and the
computational cost of the final feature set  assuming that test
instances are drawn from a probability distribution  x  y   d 

min exy  l y  f  x      ex  c f  x  

f f

   

where l y  y  is a loss function and c f  x  is the cost of evaluating the function of f on example x 
our formulation differs from that of nan et al         in that
we do not have a constraint on the feature costs  but rather
incorporate the cost minimization into the objective itself 
since in practice we are given a training set sampled from a
distribution  not the distribution itself  we will instead solve
the following problem 

min

f f

n
x

 fs  

l y  i    f  x i       

i  

x

cj

   

j  

     modified splitting criterion
we first test a simple randomized greedy algorithm  when
      this is equivalent to breimans original random forest algorithm  following the original algorithm  we use the
information gain  or reduction in entropy induced by splitting
on a feature  as a measure of the quality of a split  we modify
the split function used in tree construction to reflect the feature
cost in addition to the information gain  thus  if s denotes the
set of splits that has already been chosen  and c i s  denotes
the cost of computing feature i given that we have already chosen the splits in s  then the value of a split sij   i e  splitting at
value j for feature i  is given by 
f  sij  s    h n    h n  sij    c i s 

   

where h n   is the entropy of the current node 
here  the greedy objective used for computing splits is
weighted sum of the information gain and the feature cost 
once a feature has been used in any tree in the forest  its cost of
reuse is    as shown in algorithms   and    the vector c  rn
is the vector of feature costs  we apply this modified splitting
criterion in algorithm   

algorithm   cost sensitive random forest
input  x  rmxn   y  zm   c  rn   n  z    r
t   
for each tree i   n do
randomly sample training data to form x i and y i
t  c    g reedy t ree x i   y i   c   
c    c  
t t t
return t

algorithm   cost sensitive greedy tree
for each attribute i   m do
randomly sample splits sij and compute f  sij   as described in
   
s  argmins f  s 
ci     
create new node using feature i and split value j 
for each child node do
g reedy t ree   x i  s    y i  s   c   
return t  c

     complementary forest training
in order to achieve very low feature costs  it is necessary to
consider the
full set of features at each split  selecting a random set of n features  as is recommended in the original random forest paper  breiman et al         would prevent the
repeated reuse of a few features that is required to achieve very
low cost 
however  considering the full set of features at each split is
likely to increase the correlation of the trees in the forest  as
informative features may be used repeatedly  one recent study
of random forests found that  for many applications  low correlation among trees is a key factor in prediction accuracy of
the forest as a whole  bernard et al        
thus  in order to account for increased intra forest correlation
of the resulting model when computing splits based on the full
set of features  we incorporate a boosting like strategy  based
on the work of bernard et al         into algorithm   to reduce
this effect 
algorithm   complementary cost sensitive random forest
input  x  rmxn   y  zm   c  rn   n  z    r
t   
for each training example j   m do
 
wj   m
for each tree i   n do
randomly sample training data to form x i and y i
t  c    g reedy t ree x i   y i   c   
c    c  
for each training
p example j   m do
j    t    ti t i hi  xj     yj  
wj    j
t t t
return t

fifeature

cost

num  times pregnant
glucose tolerance
diastolic pb
triceps
insulin
mass index
pedigree
age

    
     
   
   
     
   
   
   

table    example of feature costs provided in the pima indian diabetes dataset 

     datasets and features
       p ima i ndian diabetes diagnosis
this binary classification task has   features  it is located
at https   archive ics uci edu ml datasets 
pima indians diabetes  the cost of each of the   medical tests or patient health records used to diagnose diabetes is
included and used as input to algorithms   and   

 a 

       s pam filtering
this dataset  also a binary classification task  is located
at https   archive ics uci edu ml datasets 
spambase  the features  which had already been extracted 
correspond to word frequencies  character frequencies  average length of uninterrupted sequences of capital letters  length
of longest uninterrupted sequence of capital letters  and the total number of capital letters in the email  feature costs were
not provided  so we set c s     s  

 b 

       d igit recognition
this is a multiclass classification task  the dataset is located
at https   archive ics uci edu ml datasets 
multiple features  the features  which had already
been extracted  correspond to pixel intensity averages in  
x   windows  feature costs were not provided  so we set
c s     s  
     results
we evaluate the greedy forest training method on three
datasets  for spam classification and digit recognition costs
were not provided  so we use c s     s   in general  it appears that for these three problems it is possible to reduce feature cost while not substantially reducing accuracy  for the
diabetes dataset  we are able to reduce cost  for example  by
an average of     while decreasing accuracy by an average of
      or     and      respectively  for the spam classification dataset  it is possible to reduce cost by     while reducing
accuracy by       for the digit recognition dataset  both methods which use the full feature set to compute splits produce a
significant accuracy reduction
as cost decreases  but computing

a random subset of size n  where n is the number of features 
allows for a cost reduction of       while accuracy decreases
by       we additionally note that complementary training

 c 
figure    cost versus accuracy tradeoff for three datasets  a  diabetes
diagnosis  b  spam classification  c  digit recognition  ffs   full feature set considered at each split  rfs   randomized feature set of size

n considered at each split  ffs   ct   full feature set and complementary training  algorithm     results are the average of    randomized trials  each point represents a different value of  

fic s  h   

x

x

 cf 

f s

w  t   

   

t e f t  t s   

     mutual information
in order to estimate the quality of each feature x we use the
mutual information between the feature and the class variable
y   defined as 
xx
p x  y 
   
m i x  y    
p x  y  log
p x  p y 
figure    cost versus accuracy tradeoff for the scene    dataset  using cost sensitive feature selection before applying breimans random
forest algortihm  results are the average of    randomized trials 

yy xx

in the case of continuous features  we discretize into bins before computing      we use    bins for all features 

appears to improve performance in two of the three datasets 
suggesting the importance of low correlation among trees  as
expected  the randomized subset selection algorithm is limited
in the extent to which cost can be reduced  see figure   for the
full tradeoff curves 

although it would have been better to take into account complementary relationships among features via the joint mutual
information  due to computational constraints we did not explore this  this would then become a problem of minimizing
the difference between submodular functions  for which good
approximation guarantees are not currently available 

   global optimization of feature set

     problem formulation

     submodular set functions

thus our final objective for cost sensitive feature selection is
to compute s   f   defined as 

in addition to greedy tree training  we may also want to explore
a global strategy for choosing the best subset of features to
use as input to the random forest algorithm  this section is
applicable to the more general problem of cost sensitive feature
selection  and not confined to the random forest model  here
we formulate this problem as a discrete optimization problem
in which we aim to identify the best possible subset of features
in order to balance feature cost and prediction accuracy  we
then use this subset of features to train the random forest  using
the original algorithm from breiman et al         
a submodular set function is one that satisfies the property of
diminishing marginal gains  for every x  y   with x  y
and every x   y  
f  x   x    f  x   f  y   x    f  y  

   

submodular functions have the appealing property that greedy
maximization of submodular functions is guaranteed to obtain near optimal results  and submodular minimization can be
solved exactly in polynomial time  nemhauser et al        
     cost dependency graph
we define a weighted hypergraph h    v  e  where each vertex v  v corresponds to a feature  for a subset of features
s  f   s  e if computing some feature f  s reduces
the cost of computing all other features s f by w  s   r 
thus  hyperedges correspond to groups of features which share
a common subroutine  we also have a vector c  rn of the
original feature costs 
thus  to assess the cost of a subset s of features  we compute 

s    argminsf c s  h   

x

m i f   y  

   

f s

where c s  h  and m i f   y   are defined in   and   respectively  and   r is a user determined parameter controlling
the tradeoff between computational cost and accuracy 
this objective is submodular  see   for details 
     optimization
several polynomial time algorithms for submodular minimization exist  orlin       schrijver       iwata        however  in practice it has been found that the minimum norm
point algorithm  proposed by wolfe        and extended to
submodular minimization by fujishige et al          is faster 
even though the worst case time complexity is currently unknown  thus we use the minimum norm point algorithm  as
implemented in the submodular function optimization toolbox  krause        wolfes minimum norm point algorithm
solves the following problem  given a finite set p of points in
rn   find the minimum norm point in the convex hull p of these
points  it is initialized with a random point  and then repeatedly
updates an affinely independent subset of points  a simplex  by
solving a linear optimization problem over p   this is repeated
until convergence  for a general polytope  the minimum norm
point algorithm can be exponential in the runtime  however 
for a submodular polytope  fujishige showed that the update
can be computed efficiently 
thus  in order to train the random forest  we solve the problem posed in     via the minimum norm point algorithm  and

fiworthwhile to explore in the future 

   conclusion and future work

figure    examples of images in the scene    dataset 

dataset

training

test

num  feat 

diabetes diagnosis
spam filtering
digit recognition
scene    classification

   
    
    
    

   
    
    
    

 
  
   
   

table    datasets used in this work  the first three were obtained
from the uci machine learning repository located at http   
archive ics uci edu ml   and the scene    dataset was obtained from matt kusner 

then apply the standard random forest algorithm  breiman et
al        on the resulting set of features 
     dataset and features
the scene    dataset  lazebnik et al        contains      images  each belonging to one of    scene classes  we used the
same features as xu et al        namely gist  spatial hog 
local binary patterns  self similarity  texton histogram  geometric textons  geometric color  and object bank  li et al        
the original dataset has       features total  due to computational constraints  the typical runtime for the minimum norm
algorithm we use to find the optimal feature subset was estimated at o n    by jegelka et al          we randomly sample
    features from each descriptor to arrive at     features total  the costs associated with these features are submodular
because once a feature associated with a descriptor is used 
there is no additional cost to using another feature from the
same descriptor 

in this work two general strategies for reducing test time cost
of the random forest algorithm have been explored  greedy tree
construction with the split selection modified to incorporate
the feature cost  and a cost sensitive feature selection method
which considers the dependencies among features  in the first
case  we show that a simple modification to the random forest algorithm allows for user control over the computational
cost of the trained classifier  in order to account for the increased intra forest correlation when more variables are considered at each node split  we also test a boosting like iterative
sample reweighting strategy  based on the work of bernard et
al           which generally improves performance  in the second case  we show that cost sensitive feature selection can be
formulated as a submodular minimization problem  for which
an exact solution can be found in polynomial time  this feature selection method can be applied to any model and is not
specific to the random forest 
overall  these results indicate that for several real world problems it is possible to significantly reduce test time cost with
minimal effects on accuracy  in the future  it would be interesting to test approximate methods for submodular minimization in order to improve training time  combine the two proposed methods  and consider model evaluation time in addition
to feature extraction time  it would also be interesting to use
the joint mutual information in assessing feature quality and
apply approximate methods for minimizing the difference of
submodular functions 

   acknowledgments
thank you to junjie qin for valuable feedback  and to matthew
kusner for providing the scene    dataset and feature costs 

   appendix
proof of submodularity  we need to show that for every
x  y   with x  y and every x   y  
f  x   x    f  x   f  y   x    f  y  

     results
c x   x   h   

we then evaluate the submodular cost sensitive feature selection method on the scene    dataset  using the same train test
split as xu et al          as shown in figure    we are able
to reduce feature cost by       while only reducing accuracy
by        however  beyond this initial reduction we unable
to further reduce cost  even though the set of features returned
by the feature selection algorithm grows progressively smaller
as  decreases  data omitted due to space   this suggests that
combining the cost sensitive feature selection algorithm with
cost sensitive splits  as in algorithms   and   may yield further improvements 
depending on the value of   the time to minimize   can be   
minutes or more  suggesting that approximate methods may be

x

m i f   y    c x  h 

f x x 

 

x

m i f   y    c y  x   h 

f x

x

m i f   y  c y   h  

f y  x 

x

m i f   y  

f y

c x x   h c x  h m i x  y    c y  x   h m i x  y  c y   h 
c x   x   h   c x  h   c y   x   h   c y   h 

since x  y and the marginal cost of a fixed feature can only
decrease as more features added  f is thus submodular 

   references
   bernard  simon  sbastien adam  and laurent heutte 
dynamic random forests  pattern recognition letters

fi                        
   bernard  simon  laurent heutte  and sbastien adam  a
study of strength and correlation in random forests  advanced intelligent computing theories and applications 
springer berlin heidelberg                
   blake  c l  and c  j  merz  uci repository of machine
learning databases        https   archive ics 
uci edu ml

    orlin  james b  a faster strongly polynomial time algorithm for submodular function minimization  mathematical programming                       
    ren  shaoqing  et al  global refinement of random forest  proceedings of the ieee conference on computer
vision and pattern recognition       

   breiman  leo  random forests  machine learning     
             

    schrijver  alexander  a combinatorial algorithm minimizing submodular functions in strongly polynomial
time  journal of combinatorial theory  series b     
                

   cutler  d  richard  et al  random forests for classification in ecology  ecology                         

    sharp  toby  et al  accurate  robust  and flexible realtime hand tracking  proc  chi  vol          

   daz uriarte  ramn  and sara alvarez de andres  gene
selection and classification of microarray data using random forest  bmc bioinformatics               

    shotton  jamie  et al  real time human pose recognition
in parts from single depth images  communications of
the acm                      

   fujishige  satoru  submodular functions and optimization  vol      elsevier       

    viola  p  and jones  m  robust real time object detection 
international journal of computer vision               
     

   gao  tianshi  and daphne koller  active classification
based on value of classifier  advances in neural information processing systems       
   iwata  satoru  a fully combinatorial algorithm for submodular function minimization  journal of combinatorial theory  series b                      
    jegelka  stefanie  hui lin  and jeff a  bilmes  on
fast approximate submodular minimization  advances in
neural information processing systems       
    karayev  sergey  mario j  fritz  and trevor darrell  dynamic feature selection for classification on a budget 
international conference on machine learning  icml  
workshop on prediction with sequential models       
    krause  andreas  sfo  a toolbox for submodular function optimization  the journal of machine learning research                      
    lazebnik  svetlana  cordelia schmid  and jean ponce 
beyond bags of features  spatial pyramid matching for
recognizing natural scene categories  computer vision
and pattern recognition       ieee computer society
conference on  vol     ieee       
    li  li jia  et al  object bank  a high level image representation for scene classification and semantic feature
sparsification  advances in neural information processing systems       
    nan  feng  joseph wang  and venkatesh saligrama 
feature budgeted random forest  arxiv preprint
arxiv                   
    nemhauser  george l   laurence a  wolsey  and marshall l  fisher  an analysis of approximations for maximizing submodular set functionsi  mathematical programming                      

    xu  zhixiang  et al  cost sensitive tree of classifiers 
proceedings of the   th international conference on machine learning       
    xu  zhixiang  kilian weinberger  and olivier chapelle 
the greedy miser  learning under test time budgets 
arxiv preprint arxiv                  
    wang  joseph  kirill trapeznikov  and venkatesh
saligrama  an lp for sequential learning under budgets  proceedings of the seventeenth international conference on artificial intelligence and statistics       
    wolfe  philip  finding the nearest point in a polytope 
mathematical programming                      

fi