classifying non manual markers in american sign language
pamela toman and alex kuefler

      december     

facial expressions and other non manual markers are the primary means by which
american sign language  asl  users signal sentence and clause type  negation  and a variety of
adverbs  work in the area of non manual marker recognition has the potential to make asl toenglish translation available in contexts where interpreters and human relay services are currently
rare  such as events with non signing family members  it also promises to generate scientific
insights into signed languages  an area of linguistics of which we know relatively little  despite the
potential for work in the area of signed language recognition  there is only limited research
investigating how to automatically identify non manual markers from video streams  we
contribute to the field by evaluating the performance of three feature extraction methods at the
binary classification problem of identifying the presence of a wh question in streaming rgb video 
unlike previous work that attempts to classify utterance types as units  we focus on feature
selection for identifying the presence absence of a non manual marker at different times in an
utterance  in order to lay a foundation for machine translation work  with theory based feature
tracking  we achieve approximately     frame level accuracy and     accuracy on sequences 
despite limited data  our attempt to use abstract features through neural networks resulted in close
to     frame level accuracy and     accuracy on sequences  a third approach using the scaleinvariant feature transform algorithm was unimpressive  with     accuracy 

related work
we have been able to identify only a few papers at the
intersection of facial feature recognition  machine learning  and
american sign language non manual marker processing  most
recently  benitez quiroz et al         learn temporal patterning
characteristics in a variety of non manual markers using detailed
hand annotated data  though they do not address the classification
task nor automated extraction of features 
the literature contains a handful of multi class classification
approaches  in particular  michael et al         use hmms with the
figure    the non manual
sift algorithm from computer vision that identifies features with
marker for the wh question
interesting gradients from images  liu et al         use a linguistically when  head tilted  eyebrows
informed two level crf  and vogler and goldenstein         michael
lowered  eyes narrowed  lips
drawn together  image is from
et al          and metaxas et al         address automatic
identification of non manual expressions in continuous signing despite ncslgr data and is overlaid
with intrafaces facial feature 
the occlusions of the face that are common in asl 
head pose  and eye gaze tracking
we were unable to identify any benchmark datasets in the
annotations 
literature  and although we would like to share our own  data use
restrictions complicate that endeavor  the closest are two papers that report multi class accuracies
on utterance classification  michael et al         report       accuracy on wh questions in a  class classification problem given    utterances  and metaxas et al         report       accuracy
on wh questions in a   class classification problem given     utterances  although somewhat
similar to ours  these papers seek to distinguish utterance types  whereas ours explores features
that help identify the particular period over which a specific marker is present in streaming video 
however  the existence of this work indicates that additional attention to feature selection has the
potential to be helpful to others working in automated non manual marker recognition 

data
we initially collected     utterance videos containing wh questions from the national
center for sign language and gesture resources  ncslgr  corpus  neidle and vogler         the
 

fivideos collected as data are compressed rgb form  and contain a total of        frames  including
      positive examples  and       negative examples  they span a variety of wh questions   and
examples come from five male speakers and three female speakers  positive examples are frames
or sequences of frames that contain a wh question marker  negative examples are frames or
sequences of frames from the same video that do not contain a wh question marker  wh words
used in rhetorical questions and in other statements are treated as negative examples 

feature extractors
we compare four different feature extraction methods  at the most theoretical  we use
features extracted by asl users with linguistics experience  this non automated approach
contextualizes performance on the task and offers a fully theory driven baseline  as an automated
but theory based approach  we work with specific identifiable points on the face using the carnegie
mellon robotics institutes intraface tool  xiong and de la torre         at the next higher layer of
abstraction  we find clusters of data driven high gradient change features through the scaleinvariant feature transform  sift  algorithm  and represent each image as a feature vector for the
presence of each cluster in each image  finally  at the highest level of abstraction  we use a
convolutional neural network  cnn  as a feature extractor  we discuss each in more detail below 
human baseline
using linguistics theorybased features painstakingly
encoded by humans  see neidle and
vogler         we identify a baseline
goal for performance that helps
contextualize machine
performance  sample binary framelevel features used at this highly
theoretical phase include eye
aperture     lowered lid   head
pos  tilt fr bk     slightly back  
figure    in general  the more training examples  the better the
and pos     noun  
performance  even when the number of features decreases  these
results suggest that this domain will benefit from additional good data 
because distinct utterances
were not annotated with a
consistent set of features  we experimented with preprocessing to balance number of features and
number of examples  as per figure    we found the best performance when including the most
samples  either by recoding missing target variables as non presence of the target feature or by
limiting to a very small set of features and using the few thousand associated examples  using the
smaller number of examples that shared a larger set of features did not perform as well  suggesting
that more training data  even when slightly incorrect  is very valuable for this question domain 
in the absence of a benchmark dataset  we treat the best human annotation performance of
              as a contextualizing performance goal  despite recognition that human annotations
and machine annotations are distinct  and that there may be correlational biases or missed
information in the human annotations that are not reflected in the machine learned annotations 
tracked facial markers
xiong and de la torre        develop an algorithm for supervised nonlinear optimization in
the realm of computer vision and apply it to facial feature detection  we utilize their executable
intraface to extract    facial landmarks  head pose  eye gaze  and iris features 
the configuration of non manual gestures combined in the wh question marker in asl is used for multiple question
words  e g   who  what  where  when  why  how  what for  how many   linguistically  the wh question marker indicates an
interrogative pro form 
 
 

fiwe augment the tracked features in
three ways  first  we extract the euclidean
distance deltas between each of the facial
landmarks to capture rotation invariant aspects
of the facial layout and directly measure
theoretically important variables like
eye mouth aperture and eyebrow height 
second  for each feature we add time sequence
deltas from the previous and next frames  and
we remove the neighboring frames from the
dataset to not contaminate the evaluation 
figure    by augmenting the tracking features  we were able
finally  after creating all the raw features we
to achieve        svm  and        logistic regression 
normalize each feature within each video
improvements in performance  with accuracy measured as
utterance  our results suggest that appropriate jaccard similarity   while simultaneously shrinking variance 
processing to make cross video features more
consistent and incorporate new features improves performance substantially  see figure    
we also experimented with trimming frames in which the facial orientation was an extreme
in at least one of pitch roll yaw to limit all training and testing examples to a frontal orientation 
however  we found that trimming as preprocessing offered only mild improvement  on the order of
              for svms   this finding suggests that given well preprocessed data and a model that
accounts of interaction effects  wh questions may be identifiable even with extremes of head pose 
and that any trimming is better used to eke out late stage improvements rather than to be a
primary source of concern 
flattened sift descriptors from intraface keypoints
the second feature extractor relies on scale invariant feature transform  sift   given
points of interest around the image  sift generates a set of orientation histograms  where each bin
contains the number of gradients facing a certain direction within a sub region of a  x  pixel area
surrounding the keypoint  lowe         the intent of this work was to identify informative features
of the face through gradients and textures  initial work learned the points of interest automatically
and then clustered them  creating a feature vector for each frame based on the clusters observed 
performance of this approach was low         frame level accuracy on true positives         on
true negatives   leveraging the high precision of the intraface facial landmarks  we then generated
   keypoints around signers faces  flattening the sift descriptor generated at each point into
distinct feature vectors for each frame  performance on sift given intraface selected points of
interest was also low         on true positives         on true negatives   leading to us focus
more of our efforts on the tracked facial features and neural network feature extractors 
deep convolutional feature extraction
by tuning connection weights across hidden layers  convolutional neural network  cnn 
models essentially learn a set of filters that automate the process of feature extraction  since cnns
can involve tens of millions of parameters  which require extensive time and large datasets that
were unavailable for this problem domain  we used a pre trained cnn with the same architecture
and learned parameters as alexnet  jia et al          alexnet consists of over    billion parameters
figure    cartoon of alexnet
architecture  convolutional and pooling
layers  red  apply a series of filters to
   x    rgb image input  before
activation vectors are generated in the
fully connected layers  blue  

 

fispread across eight layers  trained for a      class image recognition task on the imagenet dataset
 krishevsky  sutskever   hinton        russakovsky et al          we pre processed the data using
opencvs implementation of the viola jones algorithm  bradski         which extracted from each
frame a   x   pixel region around the subjects face  we fed the resized rois discovered during
pre processing into the input layer of the network and used as features the vector of activations
across one of the three fully connected  fc  layers that lie downstream of the networks learned
filters  this procedure is described by donahue et al         who reported best results on an image
recognition task using the      dimensional outputs from alexnets first fully connected layer
 fc    in testing  we also observed peak performance using fc  features  which occur earliest after
convolution and thus capture the most domain generalizable information of the three fc layers 

evaluation
we carried out two experiments to evaluate the approaches to feature extraction  the first
is a frame level binary classification task  the second is a time series sequence level binary
classification task  we standardized models across features as an svm with rbf kernel and
performed    fold cross validation in each experiment  due to the highly correlated appearance of
adjacent frames in a single utterance  we ensured that each utterance appeared in only a single fold 
and we avoided testing on frames and sequences that come from the same utterances as those in
the test set  see figure   for distinctions between utterances  frames  and sequences   our best
models achieve approximately     accuracy on the frame level task and     accuracy on the
sequence level task  the rest of this section provides details about each evaluation 
frame level task
our first evaluation of our three proposed feature extraction methods considers the setting
in which each of the        frames in the     video corpus are treated as individual examples 
each frame was assigned a binary label corresponding to whether or not it occurred between the
onset and offset of a wh question  positive examples account for approximately     of the dataset 
the error rates and confusion matrices of each svm extractor are given below 
tracked facial markers
accuracy            
pre   
pre   

actual   actual              
             

sift descriptors
accuracy             
pre   
pre   

actual   actual              
             

cnn  fc  only 
accuracy            
pre   
pre   

actual   actual              
             

figure    error rates and confusion matrices for the frame level task  accuracies calculated as jaccard similarity 

we find that tracked facial markers perform best for the frame level evaluation  followed by
the cnn as feature extractor  the sift descriptors are not competitive  so we focused our efforts
on improving the other two 
sequence level task
our second evaluation of our three proposed feature extraction methods divides each

figure    two utterances from the dataset  with details regarding frame and sequence evaluation  first line  english
translation  second line  frames with extent of wh question non manual marker shaded  third line  glosses and span of
manual signs  fourth line  sequences with positive examples shaded 
 

fiutterance into    frame sequences  positive examples contain a wh question in at least a third of
constituent frames  the sequence setting balances the desire for extracting detailed information
about timelines of feature presence with the desire for larger units that simplify the challenge
identifying a non manual marker from a single still frame  to balance the amount of positive and
negative data  we collected    additional samples from the ncslgr database that do not include
wh questions  this evaluation set consisted of        frames divided into       sequences across
    utterances  approximately     of these sequences were positive 
in this task  frames in a sequence vote as to whether they are part of a wh sequence  and a
hyper parameter learned threshold distinguishes predicted positive and negative examples  in
order to fit the threshold hyper parameter  we held out a single test set consisting of    utterances 
and used labeled    fold cross validation to split the remaining data into training and validation
sets  the threshold value maximizes the mean classification accuracy across the    validation sets 
tracked facial markers
accuracy     
 validation             

pre   
pre   

actual   actual              
             

cnn  fc  only 
accuracy     

 validation set             

pre   
pre   

actual   actual              
             

figure    error rates and confusion matrices for the sequence level task  accuracies are calculated as jaccard similarity
on the single held out dataset  the    fold hyperparameter setting results appear in parentheses 

we find that performance improves on the sequence level task compared to the frame level
task  and again that tracked facial features outperform the neural network approach  though both
perform well 

discussion and conclusions
the best performance is from the processing highly theory based facial feature tracking 
rather than with the highly abstracted cnns or hybrid sift method  we suspect this is driven in
large part by the limited amount of data available  such that data driven methods are at a
disadvantage  we also suspect the hybrid sift model might have performed better given
substantially more pre processing  such that the meaningful patterns were more apparent to it 
we found with the facial feature approach that appropriate pre processing improved
performance substantially  as such we would support a rigorous evaluation of preprocessing
options used in the literature  which range in complexity from the appropriate for  streamingtranslations methods used here to more complex suggested methods like   d deformable face
models  with regard to effectiveness  complexity  and runtime 
we believe the use neural networks in this application area is novel  given the          
performance on limited examples of a generically trained network  we believe that there is promise
in the further application of neural networks to this problem domain  until large annotated asl
databases are developed  we hope that work with pre trained neural nets may drive interest in
bringing big data tools to bear on this problem domain  thereby allowing learning cnns to become
a viable option for frame level classification and recurrent neural networks to become a viable
option for labeling at the level of sequences 
based on our experimental results  sequence classification through threshold voting
appears to be a good alternative that performs on par with hmms  michael et al          however 
we should maintain some skepticism  as the test set we evaluated on was relatively small    
utterances       frames   nevertheless  our results indicate that frames are indeed a useful unit of
analysis for this question  and that voting shows early promise as a means for leveraging singleframe information to make judgments about series and for identifying onset and offset of nonmanual markers in asl  as is needed to make progress toward machine translation 
 

fireferences
benitez quiroz  c f   gkgz  k   wilbur  r  b     martinez  a  m          discriminant features and
temporal structure of nonmanuals in american sign language  plos one       e      
bradski  g          the opencv library  dr  dobbs journal of software tools 
donahue  j   jia  y   vinyals  o   hoffman  j   zhang  n   tzeng  e     darrell  t          decafe  a deep
convolutional activation feature for generic visual recognition  icml       
krishevsky  a   sutskever  i     hinton  g  e         imagenet classification with deep convolutional
neural networks  advances in neural information processing systems    
liu  j   liu  b   zhang  s   yang  f   yang  p   metaxas  d  n     neidle  c          non manual
grammatical marker recognition based on multi scale  spatio temporal analysis of head pose and
facial expressions  image and vision computing                 
lowe  d          distinctive image features from scale invariant keypoints  international journal of
computer vision                
jia  y   shelhamer  e   donahue  j   karayev  s   long  j   girshick  r   gaudarrama  s     darrelll  t 
        convolutional architecture for fast feature embedding  in arxiv preprint arxiv           
metaxas  d   liu  b   yang  f   yang  p   michael  n     neidle  c          recognition of nonmanual
markers in american sign language  asl  using non parametric adaptive  d  d face tracking 
lrec   
michael  n   neidle  c    metaxas  d          computer based recognition of facial expressions in
asl  from face tracking to linguistic interpretation  proceedings of the  th workshop on the
representation and processing of sign languages  corpora and sign language technologies 
michael  n   yang  p   liu  q   metaxas  d     neidle  c          a framework for the recognition of
non manual markers in segmented sequences of american sign language  lrec   
neidle  c    vogler  c          a new web interface to facilitate access to corpora  development of
the asllrp data access interface  proceedings of the  th workshop on the representation and
processing of sign languages  interactions between corpus and lexicon  data available at
http   www bu edu asllrp  
russakovsky  o   deng  j   su  h   krause  j   satheesh  s   ma  s   huang  z   karpathy  a   khosla  a  
bernstein  m   berg  a c     fei fei  l          imagenet large scale visual recognition challenge  ijcv 
     
pedregosa  f   varoquauz  g   gramfort  a   michel  v   thirion  b   grisel  o   blondel  m   prettenhofer 
p   weiss  r  dubourg  v   vanderplas  j   passos  a   cornapeau  d   brucher  m   perrot  m    
duchesnay  e          scikit learn  machine learning in python  journal of machine learning
research                
vogler  c    goldenstein  s          toward computational understanding of sign language 
technology and disability             
xiong  x    de la torre  f          supervised descent method and its application to face
alignment  ieee conference on computer vision and pattern recognition  cvpr        
executable available at http   www humansensing cs cmu edu intraface 

 

fi