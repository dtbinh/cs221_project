predicting market volatility using semantic vectors
and google trends
anusha balakrishnan
anusha cs stanford edu

kalpit dixit
kalpit stanford edu

abstract
financial trading strategies rely on a knowledge of both the direction and range
of market movement  volatility   while existing methods have shown that trends
can be predicted using financial indicators or social media  most previous work
on predicting market volatility has focused solely on using financial indicators 
we hypothesize that social media can more accurately reflect public thought and
hence better predict future actions in the market  specifically  we posit that google
search volume  obtained using google trends  for financially relevant keywords
often foreshadows movements in the market  in this work  we describe a method
to find important financial keywords  as well as a linear regression model that uses
google trends data to predict market volatility on a per week basis  our results
show a significant improvement over baselines 

 

introduction

existing financial trading strategies often rely on using financial data from the past to make predictions about future market volatility  however  past data simply reflects past actions  but fails to
capture any information about public thought   a major predictor of the markets future  following
successful research that uses social data to predict other features of the market  we present an attempt to predict market volatility using google trends  which reflects real time popularity of search
terms  our approach contains two steps  first  we obtain a set of keywords related to the finance
domain using semantic vectors  then  we train a linear regression model to predict the volatility of
a future week using as input google trends data for those keywords in the previous week 

 

related work

past work on predicting market trends has shed some light on the usefulness of social data  some
recent papers attempt to show correlations between public mood  as expressed on twitter  and
daily changes in the closing values of the dow jones industrial average  bollen et al        and
other financial indicators  zhang et al         on the other hand  attempts to predict market volatility
have largely used past data to make predictions  fleming et al        corrado et al         and
these methods report varying levels of success  this paper aims to show that social data can have
powerful predictive power in modeling volatility  as it does for market movement  more recently 
preis et al         showed that an invented strategy based on google trends yields higher profits
than traditional trading strategies like buy and hold  we add to their work in two significant
ways  first  our final keyword set is obtained empirically rather than semi automatically  second 
we attempt to predict a measure of market volatility rather than directly determine profits using a
specific algorithm  this makes our method strategy agnostic and solidifies the relationship between
google trends and market movements 

 

dataset and features

we are working on the financial time series  defined by the value of the future of the s p    index
traded at the chicago mercantile exchange  cme   the data granularity was one week  where each
week is defined as the open of that weeks monday to the open on the immediate weekss monday 
for any mondays that were holidays  we looked at the first trading day after that monday 
 

fifigure    image representation of financially relevant words found through semantic vector clustering  larger words appear with higher frequency in the corpus of nytimes article snippets 

we use all the weeks starting from january      november       spanning     weeks of trading 
this time period is chosen since the market regime during this period is unchanging  contrast
this with including           in our dataset  where those years represent a time of global financial
crisis  in the financial world  we have close to zero expectation of one model working on both of
these periods  the  train  cross validation  cv   test  split was                 i e           
    weeks  because this is a time series  our cv is defined slightly differently than conventional  in
section     
   

target value

for each week we measure the high low percentage       
   



high low
open



 

keyword selection

we started with a set of    seed words  for e g  stock market  federal reserve  etc   related
to finance to query the new york times  nyt  article search api    which allows access to the
headline  lead paragraph  and abstract of all articles in the nyt from       we searched for articles
published after january  st         the start date of our financial dataset  containing one of our seed
words and listed under one or more financial news desks   this yielded a corpus of snippets from
       articles since       which we used to generate semantic vectors using google codes
word vec tool    we clustered these vectors into k      classes to obtain    sets of semantically
related words  and filtered out stopwords from these clusters using the standard english stopwords
list in pythons nltk package    we also tried other values of k  k               but found that
setting k      produced clusters that were as semantically general as possible while containing
minimal unrelated words 
since we posit that the search volume for financial terms specifically can predict market volatility 
we manually inspected the clusters obtained using word vec and identified clusters that contained
a majority of financial terms  we chose all the words from these clusters      words  for our
keyword set  see fig     
 

http   developer nytimes com docs read article search api v 
your money  business  financial  business day
 
https   code google com p word vec 
 
http   www nltk org book ch   html
 

 

fifigure    google trends from january        november      for the keyword labor

   

features

we obtained the weekly search volume index  svi  for each word in our keyword set by querying
a google trends api     the svi of a keyword represents the search volume of that term relative
to all other terms  one drawback of using google trends is that the svi for a keyword is based
on all searches containing that keyword  and so trend lines often contain spikes caused by searches
for unrelated phrases that contain the keyword  even trends filtered by category  finance  can
contain noise  as fig    shows  trends for labor within the finance category still show visible
spikes in popularity around september every year  coinciding exactly with labor day and potentially
obscuring trends that are indicative of market movements  we will rely on feature selection methods
to remove such keywords  our final feature set uses google trends data from the finance category
only 

 

approach

our basic prediction problem is  for a given week  predict the hi low  using google trends data
from the previous week for all keywords  formally  for each week w in our dataset  we define
x   rn to be the feature set for w containing the svi values from week w   for all keywords in
the keyword set k  k    n   we also define the hi low  y   r  and thus our training set consists
of m training examples   x i    y  i     i            m  
we train a standard linear regression model with hypothesis function h  x    t x  where    rn
is the parameter vector p
to find the optimal parameters   we minimize the standard least squares
m
cost function j        i    h  x i    y  i     using newtons method  to make a prediction on a
new week with feature vector x  we simply compute the predicted hi low  t x 
   

expanding window cross validation

we wanted our cross validation to reflect a real world scenario in which a model is trained using
data from all past weeks to make a prediction about a future week  with this in mind  we designed
expanding window cross validation  for any given week w  we train our model on all weeks from
t     to t   w    and make a prediction on week w  we then report the cross validation train
error and test error as the average error on the training and the test set  a single week  respectively
across all iterations of cross validation 
   

dimensionality reduction and feature selection

there are two motivations for feature selection  first  our original keyword set is much larger than
the size of our dataset  greatly increasing the risk of overfitting  second  as described in section
     several of our keywords  although relevant to the finance industry  have trends that are noisy
or unrelated to market volatility  for these reasons  we attempt to a  reduce the dimensionality of
our feature vectors  using principal component analysis  pca  and b  remove noisy features by
analyzing the correlation between the features and the hi low  
 

https   github com dreyco    pytrends

 

fifigure    train vs cv error while searching for the best value of number of principal components
to use from pca  as we can see  the cv error never does better than the baseline  instead  it stays
flat with   of components and then diverges 
     

principal component analysis

we apply pca to our feature vectors  and use expanding window cross validation to pick the number
of principal components p  p            n   that minimizes cross validation error 
     

correlation heuristic

we hypothesize that only keywords whose trends share a high positive or negative correlation with
the hi low  are useful features  thus  we computed the correlations  between the features  svi
values for all keywords  and the hi low   we then ran cross validation from weeks w       to
w       on increasingly large sets of the top k most correlated features  keywords   k            n 
and chose our final feature set to be the set that produces the lowest cross validation error 

 

experiments and results

in financial trading  any prediction model which often has low error but occasionally makes large
errors proves to be loss making  to strongly penalize larger errors  we use the mean squared error
 mse  as our metric  while it is typical in time series analysis to underweight old training examples 
we consciously did not do that  since our dataset is small  total of     examples   many combinations of features will occur only a few times  and discounting would prevent the model from learning
many of them 
as a baseline  we use the mean over the training weeks  over all constant predictions for the training
set  the mean minimizes the mse i e  e ytrain     arg max e  y c     
c

   

experiments

as explained in section      dimensionality reduction must be done  we explore the use of pca
and the correlation heuristic  both separately use a single hyper parameter  called k  for pca  k
represents the number of principal components and for the correlation heuristic  it represents the
number of features used 
figure   shows the results of using pca  as we can see  low dimensional pca projection is worse
than the naive baseline  we think the reason for this is that only a few of the     features are useful 
and are not necessarily orthogonal to the subspace spanned by noisy features  good features also
might not explain most of the variance in the feature space data  thus  the most important principal
components  which are linear combinations of the features  will very likely give a low weightage to
the important features  hence  pca is ineffective for our problem  we also see that nearly all of the
fitting taking place is overfitting 
 

we used spearmans correlation and rank correlation  and both agreed 

 

fifigure    train vs cv error while searching for the best value of number of features to use  k    
gave the least cv rmse of       the corresponding test rmse was      
figure   shows the learning curve for linear regression using the correlation heuristic as described
in section        it plots rmse vs   of features used  there are two very interesting observations 
firstly  we can see that cv error reaches a minimum at just k      out of     features  qualitatively 
this might imply that very few features are actually important  secondly  for the first half of the graph
 k        the cv error is lower than the train error  we spent a lot of time in checking our code and
look ahead  and that observation is not a mistake  we believe this might be related to the nature of
our cross validation setup  the test set are the last    weeks  which is just all of       this implies
that the weekly volatility of      is well explained by a model trained on data from           

 

conclusion and future work

we show that market volatility can be reasonably predicted using google trends data for financially relevant keywords  and we propose a method for finding such keywords and ranking them
by predictive power  we also show that using a very small subset    words  of the most correlated
keywords produces the best results  a possible extension to this work is the elimination of noise in
google trends data by removing the effects of unrelated searches  this would further increase the
correlation of the trends with actual market movements  another interesting direction involves preclustering the financial data into volatility classes  high  medium  low  to determine if we observe
different correlations for different classes 
acknowledgments
we are grateful to acm systems for granting us access to proprietary data for this project  we
would also like to thank our project mentor  derek lim  and prof  andrew ng 

references
bollen  j   mao  h   and zeng  x          twitter mood predicts the stock market  journal of
computational science          
corrado  c  j   miller jr  t  w   et al          the forecast quality of cboe implied volatility indexes 
journal of futures markets               
fleming  j          the quality of market volatility forecasts implied by s p     index option prices 
journal of empirical finance              
preis  t   moat  h  s   and stanley  h  e          quantifying trading behavior in financial markets
using google trends  scientific reports    
zhang  x   fuehres  h   and gloor  p  a          predicting stock market indicators through twitter i
hope it is not as bad as i fear  procedia social and behavioral sciences          

 

fi