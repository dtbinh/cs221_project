robocop  crime classification and prediction in san francisco
john cherian and mitchell dawson
december         

abstract

analysis problem  for instance  wang et al s pattern analysis suggests that the locations and times of burglars tends
to vary significantly over time      in contrast  the bogomolov paper suggested that demographic data constituted a
particularly useful set of features for his crime classification
problem     
on the issue of time series analysis  a couple different
papers suggested that an arima family model would likely
give us reasonable  though fragile results         however  the
papers we were able to find that were focused on forecasting homicide crime rates  etc  nearly always used univariate
time series models with exogenous variables  we felt that
this was a potential area for improvement because a multivariate model could take correlations between different types
of crimes into account 

in this paper  we employ machine learning and other statistical techniques to the problems of classifying and predicting
crimes in san francisco  drawing upon existing research in
the field to approach these two problems  we employ random forest and var p  models  respectively  for the classification problem  our results across all    crime categories
demonstrate the difficulty of the fully specified crime classification problem  as we achieve a maximum    way classification accuracy of         although our results are perhaps
inappropriate for daily or weekly use in any police organization  the time series model performs adequately at forecasting crime incident averages in the coming weeks and months 
with more data and the use of a time series model already
developed by these authors for discrete time series  our results might be improved upon further 

 

 

introduction

our dataset consists of criminal incidents drawn from the
san francisco police departments crime incident reporting system  and was made available through a kaggle competition      the dataset contains        examples  each
consisting of a timestamp  date and time of day   one of   
crime categories  what we wanted to predict   a short description of the incident  the day of week  the police district
in which the incident occurred  the resolution of the incident 
the address  the longitude  and the latitude  we ignored the
description and resolution fields as both did not appear in
the competitions test set  we ignored the address field  as
it was difficult to work with and redundant given the police
district  longitude  and latitude fields  we also did not use
the competitions test set  as it did not contain crime category information  additionally  we gathered demographic
data such as per capita income  racial composition  and median age from the      american community survey  acs 
to enhance our feature set  specifically  for each criminal incident in the data set we used the given coordinates to find
demographic data associated with the census block in which
the incident occurred 
for the time series analysis  we aggregated the data on
a biweekly basis because data from every other week was
removed from the provided training set so that kaggle could
do out of sample testing  we then constructed time series
covering     two week periods for each crime class with a
total number of crimes exceeding       see figure   for an
example 

criminal activity is inevitably a part of urban life  all city
dwellers therefore have an interest in the improvement of
our understanding of crime and its patterns  police departments in particular could use this improved understanding to
more effectively allocate their resources and better serve their
communities  knowing the spatial and temporal patterns of
criminal activity would allow police to deploy the right officers where and when they are most needed  and being able
to predict criminal activity would allow them to anticipate
and combat surges in crime 
the goal of our project is twofold  first  to able to classify using machine learning techniques   in particular  a random forest model   the type of criminal incident  e g  theft 
dui  prostitution  given data about the incidents location
and time of occurrence  and second  to able to predict increases and decreases in the the rates of specific categories
of crime given past rates using a var p  time series model 

 

dataset

related work

the literature on crime classification is highly limited when
compared to most applied machine learning problems  however  a few papers shed some light on the topic  for example 
bogomolovs mit paper     on crime classification contained
a number of critical insights for our work  the most important of those was the superiority of the random forest classifier for this problem  this assertion  which was confirmed
by empirical analyses of various off the shelf classifiers  was
  methods
also corroborated by another paper from nasridinov  et al 
     which advocated the use of decision trees 
    classification
other papers helped us figure out how to construct our
feature set  although the wang et al  paper is focused on the primary machine learning method we employed for
finding patterns in crime data  it suggested that location and classification was an ensemble method called random
time data by itself would be inadequate for any serious crime forest  a random forest is defined as follows     
 

fiseries exhibits any significant deviations from stationarity 
the critical value observed for each categorical time series is
significant at the   percent level if it exceeds       though
some do come close  none of the critical values exceed that
threshold 

figure    robbery time series

a collection of decision trees  tb   is constructed from
b random samples drawn from the training set with
replacement  each decision tree is grown by repeating the
following steps for each node in the tree until we reach the
limits on the size of the tree 
randomly select m out of the p feature variables  split
the decision tree on the variable that maximizes the gini
impurity associated with the variable  for reference  the gini
impurity is a measure of how homogeneous the set is  it is
defined as follows  fi is the fraction of elements labeled i in
the set  
ig  f    

m
x

figure    critical values from dickey fuller tests on each
time series
it is assumed that the residuals are drawn from a
n          distribution  while the residuals are likely not
truly normal  the average p value of notoriously oversensitive shapiro wilk tests of normality performed on the residuals of each time series is         well above the      or     
commonly used as a threshold for significance 
for the detrended time series model  we detrend the
time series data by computing the linear least squares fit for
each class time series and subtracting the resulting function
from the data  this is implemented in the pracma package
in r     

fi     fi  

i  

after the decision trees are constructed  the random forest
classifies the test data based on whichever class received the
most votes from the b decision trees 

   

 

time series

results

   

classification

the time series model we employed was the vector auto        establishing a baseline
regressive model with lag p  or var p  
for classification  we use as our accuracy measure the number
p
of correctly labeled examples divided by the total number of
x
nt     
aj ntj
examples  unless specified otherwise  all test accuracies were
j  
estimated using   fold cross validation 
k
kk
we first ran a number of standard classification algont  r   aj  r
rithms on our data  including the random forest classifier
using the mts library in r     the time series model was fit as recommended in the literature  the results are shown beusing least squares  i e 
pp the parameters   aj are chosen to low in table    the training accuracies were calculated on
minimize  nt    j   aj ntj     
the entire dataset 
there are two particularly important assumptions made
by this model  the first is that the data is weakly stationary 
table    accuracies of   classification algorithms
i e  the following statements must hold 
algorithm
training accuracy
test accuracy
e nt     

for all t

cov nt   ntj     j

for all t

logistic regression
svm
naive bayes
random forest

an augmented dickey fuller unit root test for
stationarity     however  reveals that no crime class time

      
     
      
      

      
    
      
      

the results for logistic regression and naive bayes
 

fiwere little better than the accuracy of the naive classifier
that always chooses the most common crime category  about
         we found that the svm took much too long to
run on any reasonably sized subset of the data  so the accuracies for svm above are rough estimates based on very
small subsets  the results for random forest were somewhat
promising compared to the others  however 
     

tuning random forest

our next step was to attempt to tune the hyperparameters
of random forest to reduce overfitting  the hyperparameter most relevant to this task was the maximum depth of the
decision trees of which the random forest is composed  we
systematically estimated the test accuracy for different maximum depths while holding the number of trees constructed
constant at     the value that gave the best test accuracy
was     figure   shows how accuracy varied with changes in
maximum depth  we performed a similar search for the hy 

figure    confusion matrix w o demographic data
at the poster session  we were alerted to the existence of
the u s  census acs api      by the other group working
with the same san francisco crime dataset  and we used this
to gather more accurate  fine grained data  the specifics of
these data are detailed in the dataset section above   using
these new demographic features improved the test accuracy
of our model very slightly  resulting in an estimated test accuracy of         the training accuracy dropped slightly to
       
     

we next attempted to reduce the bias of our model by building higher order features   such as longitude  or time of day
latitude   and adding them to our feature set  because of the
large size of our dataset and consequent long training times
with our initial set of features  we chose to only work with
the most important features as measured by the gini importance  which is a measure of the informativeness of a feature
based on how much gini impurity is decreased by nodes that
split on that feature       latitude  longitude  and time of
day were the   most important features  for a given degree n  we calculated all higher order features of degree less
than or equal to n involving those three features and added
them to our feature set  figure   below shows how the estimated test accuracy varied with the degree n  to make this
computationally feasible  we removed the demographic data
and used a smaller      number of trees when estimating the
accuracies 
the variations were small  but the estimated test accuracy tended to decrease as n increased  the slight increase in
accuracy for n      and indeed all the variations in accuracy 
may have been due to the randomness of the random forest 
especially since we used a smaller number of decision trees
in the forest  increasing the variance of the forests outputs 

figure    model accuracy vs  tree depth
perparameter that determines the number of individual decision trees constructed by the random forest  we found that
both test and training accuracy monotonically increased as
we increased number of trees  until we reached a point where
training the model became computationally infeasible with
our available resources  we therefore chose     a value on the
upper end of the range for which we could train our model in
a reasonable amount of time  with these hyperparameters
set at these values  we achieved an estimated test accuracy
of        and a training accuracy of         the confusion
matrix produced by this model appears in figure   
     

adding higher order features

adding demographic data

while tuning the hyperparameters of the random forest
somewhat mitigated the overfitting of the model  the bias
of the model remained high  we next sought to the reduce
this bias by gathering demographic data from the u s  census acs and using this new data to expand our feature set      time series
at first  we aggregated the data by hand from a published
      motivations
profile of san francisco neighborhoods from the          
acs       adding these demographic features to our dataset the var p  model is uniquely capable of finding trends
in this set of crime data  auto correlation function  acf 
did nothing to enhance the accuracy of our model 
 

fi     

model selection

choosing p for the var p  is a feature selection problem 
however  in this case  instead of using cross validation  we fit
the model with different values of p  and calculated the model
aic  akaike information criterion  and bic  bayesian information criterion   as we can in figure    p     appears to
be optimal  although each of the time series passes system 

figure    test accuracy vs maximum feature degree

and cross correlation  ccf  plots between the different categories of crimes revealed significant statistical connections
that the coefficient matrix in a multivariate time series model
like var p  can capture  figure   below shows two particularly informative plots  for example  in this case  the
figure    aic bic
atic stationarity tests  it is evident that certain series exhibit
a trend over time  to account for this  we also fit the model
to detrended data  the process of detrending is described in
further detail in the methods section  
     

fit results

what follows are two charts of the    week forecast  with a
   percent confidence interval  of the unmodified and detrended model trained on robbery time series data 
these are figure   and figure    respectively  as one can
see  this model is unable to capture the week to week variation  but does adequately predict the underlying trend 

figure    acf and ccf plots

cross correlation function between robberies and drug crimes
figure    forecast on unmodified data
might imply that an increase in drug arrests precedes a decrease in robberies  this and other similar correlations are in table    we present the complete fit results for the original
identified and exploited by the matrix coefficients of the time series and the detrended time series model  for error
var p  model 
data  we use normalized root mean square error  nrmse 
 

fispecifically  we use a random forest model with added demographic data about the location of a crimes occurrence
to achieve a    way classification accuracy of         we
believe random forest is better suited to this problem than
other classification algorithms  as it makes no assumptions
about the structure of the data  this is important since our
dataset is highly noisy 
we used a var p  time series model to achieve adequate predictive results for many crime categories  we believe that using the mingarch p  q  model  which assumes
the counts are drawn from a poisson distribution  more appropriate for crime statistics   that john developed in his
summer research would give more useful and accurate results 
however  the currently implemented version of the model is
insufficiently optimized for a larger dataset like ours  also 
adding data from cities like chicago would help train more
accurate  higher lag time series models 

figure    forecast on detrended data
from cross validation on a sliding window of size      to
calculate nrmse  the root mean squared error for each time
series is normalized by the standard deviation of the observed
data 

references

    a  bogomolov  et  al  once upon a crime  towards
crime prediction from demographics and mobile data 
table    time series errors  nrmse 
mit        pdf 
name
unmodified detrended
    a  nasridinov  s  ihm  and y  park  a decision treeassault
      
      
based classification model for crime prediction  inforburglary
      
      
mation technology convergence                 pdf 
disorderly cond 
      
      
    t  wang  c  rudin  d  wagner  and r  sevieri  learning to detect patterns of crime  european conference on
dui
      
      
machine learning and principles and practice of knowldrugs
      
      
edge discovery in databases        pdf 
drunkenness
      
      
    pepper  james v  forecasting crime  a city level analforgery
      
      
ysis  understanding crime trends  workshop report
fraud
      
      
                 google books  web     dec       
kidnapping
      
      
   
klepinger 
daniel h   and joseph g  weis  projecting
larceny
      
      
crime
rates 
an age  period  and cohort model using
missing person
      
      
arima
techniques 
journal of quantitative criminolnon criminal
      
      
ogy
   
       
        
springerlink  web     dec       
other offenses
      
      
   
san
francisco
crime
classification data  kaggle 
prostitution
      
      
https   www kaggle com c sf crime data 
robbery
      
      
    hastie  trevor  robert tibshirani  and j  h  friedman 
secondary codes
      
      
the elements of statistical learning  data mining  insex offenses  for         
      
ference  and prediction  new york  springer        print 
stolen property
      
      
    tsay  ruey s  multivariate time series  mts   program
suspicious occur 
      
      
documentation  r project  vers        the r foundation 
trespassing
      
      
   feb        web     dec       
vandalism
      
      
    pfaff  bernhard  and matthieu stigler  urca  program
vehicle theft
      
      
documentation  r project  vers         the r foundawarrants
      
      
tion    june       web     dec       
weapon laws
      
      
     borchers  hans werner  pracma  program documentation  r project  vers         the r foundation     oct 
      web     dec       
     scikit learn  program documentation  scikit learn org 
vers        scikit learn developers  n d  web     dec 
  conclusion
     
in this paper  we used machine learning techniques to clas       san
francisco
neighborhoods
socio economic
sify the type of crime that occurred given data on where and
profiles 
american
community
survey
    when it occurred and time series analysis to predict future
     
san
francisco
planning
departpatterns of crime given counts of past instances of crime 
ment 
may
     
http   empowersf org wp 

ficontent uploads         sfprofilesbyneighborhoodsf planning dept  pdf  pdf 
     u s  census american community survey api 
http   www census gov data developers data sets acssurvey   year data html 
     louppe  gilles  et al  understanding variable importances in forests of randomized trees  advances in neural
information processing systems       

 

fi