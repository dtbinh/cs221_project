ai meets online dating
predicting attractiveness in profile pictures
tucker leavitt  tuckerl   duncan wood  duncanw   huyen nguyen  huyenn 
stanford university cs     fall     
abstract we construct a learning model to predict the
attractiveness of facial images  we use a face detection api
to tag several facial landmarks and use the pairwise distances
between these landmarks as the features for our model  we
find that our model rates images similarly to humans  making
the same rating that an average human would nearly three
quarters of the time  our work may be useful for the online
dating community and or computer vision community 

i  introduction
although judgments of beauty are subjective  the prevalence of beauty contests  cosmetic surgery and homogeneous
ratings of photos on dating websites suggest there might be
a common denominator for attractiveness perception  this
project is an attempt to quantify this factor 
in this project  we explore the notion of facial attractiveness using machine learning  our model attempts to predict
attractiveness scores for facial images  the input for our
model is a frontal image of a female subject  the output
will be a discrete value between   and   which represents
the attractiveness of the subject in the image 
there have been several experiments with a similar goal 
in       yael eisenthal and his team at tel aviv university 
israel trained their model on   datasets  one contains   
images of austrian females  and one contains    images
of israeli females  using k nearest neighbors and svm
with both quadratic and linear kernels  their model achieved
a correlation of      with the average human ratings    
the same year  amit kagian and his team  also from telaviv university  trained their model on a dataset of   
facial images of american females  by using    principal
components of      distance vectors between    fiducial
point locations  they achieved a pearson correlation of     
with human ratings     in       two students in cs    
hefner and lindsay used a training set of     photos and a
test set of    photos  all obtained from hotornot and got the
maximum correlation with linear regression estimation    
most recently  in july       avi singh from indian institute
of technology  kanpur used guassian process regression 
k nearest neighbor  random forest  svm and linear regression with leave one out cross validation on a dataset of
    asian females      he achieved correlations of           
                 respectively 
its noticeable that most previous experiments use a relatively small number of carefully chosen geometric features
 less than       through this project  we want to begin
with a large number of redundant geometric features  and
systematically reduce the count to a smaller number of more

independent components which will also allow for easier
computation 
ii  d escription of data
we use three different datasets  the scut fbp dataset 
chicago face dataset  and images scraped from the hotornot dating website 
the scut fbp dataset contains     frontal images of
    different asian females with neutral expressions  simple backgrounds  and minimal occlusion  these factors are
conducive to facial beauty perception in both geometry and
appearance  each image is rated by    users  both males and
females  on a discrete scale from   to    we computed the
average rating score for each image and used these as the
labels 
the chicago face dataset includes high resolution frontal
photographs of     male and female targets of varying
ethnicity  each target is represented with a neutral expression
photo that has been normed by an independent rater sample 
each photo is rated by       raters with the average rating
being a continuous value between   and   
the hotornot dataset was scraped off the website hotornot com website  each image has a continuous rating
from   to     which is the ratio between the number of
people who like her and the total number of people who
rate her profile as like dislike  for example  if   out of   
people like a person  that person would have a rating of   
we scaled the ratings of hotornot dataset to       range so
that they can be the same as the ratings of the scut fbp
dataset and chicago face dataset  this dataset is used for
testing purpose only 

fig     a subject from scut fbp dataset with an attractiveness rating of
    

fifig     a subject from chicago face dataset with an attractiveness rating
of     

due to time constraints  we only process frontal images
of female subjects 
iii  m ethods
we used a multiclass logistic regression classifier without
regularization to predict the average rating of each picture  as
an integer between   and     using pythons sklearn library 
as an initial proof of concept test  we implemented a
quick and dirty multilabel classifier using the scut fbp
dataset  to construct our feature set  we manually tagged   
different points on    different faces using pythons ginput
feature  we then used the pairwise distances between all pairs
of points as the input features of our classifier 
to compute the testing error on our models  we used kfold cross validation  choosing k to be m     the results of
this simple model were promising  so we continued to refine
the method to produce the final results 
instead of manually tagging all     of our photos  we
used two existing face detection apis  faceplusplus com 
and facemarkapi  http   apicloud me apis facemark demo   
to tag geometric landmarks on photos  seen in fig      our
apis
 produced a set of    landmarks  giving us a total of
  
         features  this is roughly six times the size of
our data set  to avoid overfitting our data  we used principal
component analysis  pca  to reduce the dimensionality of
this feature space by an order of magnitude or more  our
analysis includes pca reduced feature numbers between   
and      we ran linear regression on this reduced data  this
time with l  regularization 
we ran support vector regression on the data without
much success  this is described in the results section 
iv  r esults
using regularized multi class logistic regression  we
were able to train a model with a maximum classification
accuracy of        this puts our models success rate at
well above chance  i e        and on par with the accuracies
of other existing attractiveness rating models 
tables i and ii show the confusion matrix and precision recalls for our model 

fig     example picture from the scut fbp dataset  with added labels
indicated
table i
c onfusion matrix
predicted
predicted
predicted
predicted

 
 
 
 

actual  
  
  
 
 

actual  
  
   
 
 

actual  
 
  
  
 

actual  
 
 
 
 

note that while rating system allowed for values between
  and    no average ratings of   were observed  our models
most frequent error was in misclassifying  s as  s  in fact 
the     entry in the confusion matrix is four times as large
as the     entry  this might be related to the fact that we
have so many instances of   rated profiles      of the    
data points   indeed  the     entry of the confusion matrix is
also bigger than the     entry 
a  parameter tuning
we attempted training our models directly on our distance
features  but found that the resulting models overfit our data 
with logistic regression  training error hovered at around
     while testing error was      this is likely because we
had many more features         than we had data points
       
to mitigate overfitting  we ran pca on our data and
trained our model on only the top several principal components  figures   and   show the projections of our data
onto the top few principal components 
the number of principal components used had a significant
impact on the training and testing errors  figure   shows that
testing error generally increases with the number of principal
components used  while training error decreases  however 
testing error reaches a local minimum at    principal components  as shown in figure   
in training our model  we performed a grid search to

fitable ii
p recision and r ecall
label
precision
recall

 
     
     

 
     
     

 
    
    

 
     
     

fig    
projections of our data points onto the first two principal
components  darker colors indicate higher attractiveness scores  it is not
immediately obvious what is special about the outliers 
fig    
projections of our data points onto the first three principal
components

optimize the coefficient of the l  penalty term  c  we
found that testing error seemed to increase monotonically
with increasing c  the global minimum was for a value of
c         we chose a slightly larger value  c         to
further discourage overfitting of the data 
b  visualizing the principal components
the well known eigenfaces method allows for a way
of visualizing prototypical faces using principal component
analysis  we were curious if we could visualize the principal
components of our distance features in the same way  each
of our data points represents a planar embedding of points
in the plane  we wanted to see what happens if we tried
visualizing our principal components in the same way 
our features represent pairwise distances between points
in the plane  to reconstruct the set of points from their
distance matrix  we computed the matrix m such that 
 
 
 
di 
  dj 
 dij
 
where dij     xi  xj     then  the matrix m can be written
as m   xx t   where the ith row of x is xi  x    x can
then be found by computing the eigenvalues and eigenvectors
of m     
if the distance matrix d truly represents a planar embedding of points  then all but the first two columns of x will

mij    xi  x      xj  x     

fig     training and testing error as a function of the number of principal
components used

be zero  we computed our entries of d for the principal
components by unnormalizing them  i e  multiplying them
by the datasets standard deviation and adding the datasets
mean 
when we computed the matrix x for each of our principal
components in this way  we found that its first two columns
contained large nonzero values  on the order of       and
the subsequent columns contained smaller nonzero values

fifig    
error 

training and testing error  near the global minimum of testing

 on the order of        surprisingly  plotting the first two
columns of x yields face like images  as in figures   and   

fig     embedding of the last principal component  its visually indistinguishable from the embedding of the first principal component 

here  the s are slack variables that represent how much
our principal component deviates from a planar embedding 
this is a quadratic optimization problem with quadratic
constraints  which is in general an np hard problem  we
attempted to use the cobyla  constrained optimization
by linear approximation  optimization method using
python built ins to solve the problem  but were not able to
obtain good convergence 

fig    

embedding of the first principal component

upon further inspection  however  it was seen that all the
principal components produce nearly identical embeddings 
for example  the coordinate wise variation between the first
and last principal components was on the order of       so 
the embeddings of these principal components all resemble
the average embedding of the dataset 
as an alternative way of visualizing the principal components  we posed the following optimization problem 
x
min
i j  
 x

ij

 
s t    xi  xj      dij
  ij

fig      embedding of the fourth principal component via the optimization
method  showing some signs of structure

it is unknown if visualizing the first principal components
in this manner will yield any meaningful results  the principal components represent the directions in which our data
points vary most significantly  this could correspond to how
much a subject is rotating her head  how assymetrical her
features are  or how significantly her features differ from
some prototypical face  pursuing the planar embedding of

fithese principal components further would be an interesting
topic for further research 
c  attempts at support vector regression
our ratings are continuous  not categorical  so it makes
sense to describe them with a regression model  we attempted to use support vector regression to fit our data  but
found that it drastically overfit our data  even when trained
on the datasets principal components and even when we
performed a grid search to optimize its parameters  as shown
in figure     our models r  value for its training data is
nearly   regardless of the number of principal components
used  while the r  value for the testing data never rises above
   

able to cluster faces based on differentiating features such as
sex  ethnicity  and age  it is conceivable that certain feature
distributions will be more attractive in certain demographics 
perhaps there is a universally appealing facial structure  or
perhaps different groups have different optimally attractive
facial structures 
it is our hope that the results of this paper demonstrate
an effective and relatively simple method for classifying
faces based on attractiveness  as well as contribute to the
understanding of the components of structure that comprise
to this attractiveness  this research could be useful especially
when combined with analysis of other non spacial components mentioned above 
r eferences
    amit kagian  gideon dror  tommer leyvand  daniel
cohen or  eytan ruppin  a humanlike predictor of facial
attractiveness  school of computer sciences  tel aviv university 
http   papers nips cc paper      a humanlike predictor of facialattractiveness pdf        accessed november      
    avi singh  computer vision for predicting factial attractiveness 
http   www learnopencv com computer vision for predicting facialattractiveness   july           accessed november      
    yael eisenthal  gideon dror  eytan ruppin  facial attractiveness 
beauty and the machine  school of computer science  tel aviv university  tel aviv       
    jim hefner  roddy lindsay  are you hot or not   stanford university 
http   cs    stanford edu proj     hefnerlindsayareyouhotornot pdf december       accessed december      
    bruno bruck   http   math stackexchange com users       brunobruck   finding the coordinates of points from distance matrix 
 version               http   math stackexchange com q       

fig     

r  values for training and testing data using svr 

this is consistent with the results found in hefner and
lindsays      machine learning project  more data or
a different approach to selecting features may help make
regression models a viable option 
v  d iscussion
we found that based solely on the set of distances between
key points on a photo  we can predict with as much as    
the score on a scale from     of the attractiveness of a
womans face  this result is extremely significant considering the subjectivity involved in determining attractiveness 
in addition to the many features not captured by this model
including complexion and hair and eye color  this suggests
that structure and positioning is the most important factor in
finding an attractive face  assuming our data is reasonably
distributed in other unmeasured parameters  this means that
for women choosing which photos to post online  the way
the face is depicted spatially can be a principally important
consideration 
the most logical expansion of this project would be to
analyze a more diverse dataset  more data should increase
the accuracy with which one could predict the attractiveness
of any face in general  it may also be interesting to be

fi