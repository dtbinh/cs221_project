predicting stock prices through textual analysis of
web news
daniel gallegos  alice hau
december         

 

introduction

investors have access to a wealth of information through a variety of news channels to inform them on their decisions to buy and sell stocks when managing their portfolios  our
projects goal was to simulate and improve on this process and predict stock price fluctuations of specific companies through a supervised learning approach to textual analysis
of recently published and relevant articles on the web 

 

data

we limited our scope to examine four prominent tech companies  apple  microsoft  amazon and tesla  our assumption was that these companies  because they operate in similar
spaces  would react similarly to the same text features  using python  we scraped the
sites of bloomberg businessweek  techcrunch  and the motley fool for a months worth
of relevant news articles and press releases based on searches by the company name and
ticker symbol  we also computed the stock fluctuation for each company for each day
of news collected  we collected a total of       articles over    days  after collecting the
articles  we wrote scripts to parse their content  convert all words to lower case  remove
punctuation  and stem words to their base form using the porter stemming algorithm 
raw text
htcs one a  has been criticized as an
apple iphone knockoff because of the
resemblance in the exterior design 
however  the ambitious a  will hit the
store shelves today 

processed text
htcs one a  has been critic as an appl
iphon knockoff becaus of the resembl in
the exterior design howev the ambiti a 
will hit the store shelv today

 

fi 

learning algorithms

we treated this as a natural language processing and binary classification problem where
stock price either improved or did not regardless of the percent change  to predict these
two classes we had two types of features  word frequencies and numerical measures derived from sentiment analysis  for each method  we trained our models with holdout
cross validation by training on the data scraped for microsoft  tesla  and apple and testing on the data scraped for amazon  this problem lends itself to supervised learning
algorithms such as svms and logistic regression 
svms are among the best off the shelf supervised learning algorithms  svms excel
at efficiently handling high dimensional feature spaces because of their use of kernels 
also  their margin maximization usually creates very robust predictions  svms are a
good starting point for almost any type of classification problem 
logistic regression is the other natural choice for a classification problem  it assumes
very little information about the training data  and it tends to do well even with small
amounts of data  in addition  it is a very simple learning algorithm to implement  so it is
considered a good first step 

 

feature selection

the primary focus of our project was feature selectionfiltering out the most influential
features in text articles on stock prices  we incrementally tweaked our selection with
two approaches to choosing subsets of features  in our second approach  we also experimented with aggregating content 

   

first approach

the first was a bag of words approach similar to how we implemented spam detection
in class  we created a lexicon of words using frequency tables  and then ran our supervised learning algorithms  limiting our features  in this case tokens  was our primary
concern in this approach  below are different methods we used to select features  which
we then used holdout cross validation to find test error estimates 
 created a lexicon of the     most frequently used stemmed tokens across all articles 
ignoring numeric values and stop words such as the  a  as  etc  
 created a lexicon with sentiment analysis in mind by using an existing sentiment
lexicon to include only the     most frequently used positive and negative tokens  ignoring neutral ones   we sought to test our hypothesis that some words
were more consequential than others  examples of positive and negative tokens
 

fi stemmed to be consistent with our processed data  are  accomplish  amaz  matur 
rich        positive  and abolish  accus  lose  poor  negative 

   

second approach

our second approach used as features the following 
 percentage of positive tokens in an article
 percentage of negative tokens in an article
 percentage of the relevant companys name or ticker symbol mentions  this was
to differentiate between more and less relevant articles  an article that mentions
microsoft    times might be more relevant than one that mentions it once 
 percentages of positive and negative tokens in the sentences immediately surrounding a company name symbol mention  this was to make our feature selection more
contextually sensitive by giving weight to sentiment charged statements in close
proximity to the specific mention of a company 
in this approach  we also took into account bigrams such as not profitable or no revenues to make our sentiment analysis more sensitive to the meaning of pairs of words
in context 

   

aggregating content

we found that  on average  the length of any given article was approximately     words 
given that many of those words would be ignored by our feature selection of either most
frequently occurring words  positive or negative words  and non stop words  we hypothesized that a single article did not have enough information to make an accurate prediction  so  for our second approach to feature selection  we took three approaches to aggregating content to make predictions not on the information of a single article  but on the
information of the content of multiple articles aggregated together 
 aggregate all article content for a company for each day  we did this by summing
up all of the frequencies of positive words  negative words  and company mentions
over all of the articles for each day and for each company  make predictions for the
next day given all content for the current day 
 aggregate all article content for a company for each day and the previous day  then
make each prediction for the next day given its previous two days of content 
 aggregate all content for a company for each day and the previous two days  then
make each prediction given three total days of news content 

 

fi 

results

the following table contains holdout cross validation errors for the models using the first
approach 
feature selection
    most frequently used words
    most frequent positive and negative words

svm
      
      

logistic regression
      
      

the following table contains holdout cross validation errors for the models using the second approach 
feature selection
non aggregated content
single day aggregated content
two day aggregated content
three day aggregated content

svm
      
      
      
      

logistic regression
      
      
      
      

to measure whether our classifier
would fare well predicting the stock
market  if the classifier predicted that
the stock price would rise that day  we
decided to invest money that day in the
market and if the classifier predicted
that the stock price would fall that
day  then we didnt invest money in
the stock  to put it in perspective  we
decided to compare it to the naive
approach which was the case where we
would have invested in the stock every
single day 

 

discussion

through our sentiment analysis of news articles  we achieved error rates fluctuating between    and     and thus were not able to conclusively find a model that accurately predicted the fluctuations in stock prices  however  we at least found that our market return
on investment using our best model was approximately    percent  logistic regression
consistently out performed svms as a learning model 
for our first approach to feature selection  using the bag of words approach  it is unclear whether or not the choice to look at only the most frequent positive and negative
 

fiwords rather than all words was actually improvement since the error rate improved by
roughly   percent for logistic regression  but worsened by   percent for svms  for our second approach to feature selection  where we calculated measures for features it appears
that aggregating content over a day to make predictions performed the best  this could
indicate that the biggest predictor for a days stock price  if it affected by the news  is the
previous days content  further  that might indicate that stock prices are more vulnerable
to immediate rather than long term changes in public sentiment about the company 

 

future work

aggregating data greatly shrank our training data set by roughly a magnitude of    since
it reduced our number of observations to the number of days over which we collected
data  hence  our results from the aggregated data had higher variance than the other
methods  and it would be productive to scrape more data to train and test on 
it would have been interesting to see how effective our models for predicting sentiment
actually were  for example  using the features we selected  could we predict whether an
article was positive or negative  it would probably be beneficial to research and apply
more advanced sentiment analysis techniques in order to be able to capture more of the
nuances in the articles 
further  our models were built on the large assumption that the stock prices for a single company could be predicted by models trained on other companies  while it might
be the case that there is one model common to all tech companies  and that they are all
impacted similarly by news  its also highly likely that companies respond differently to
the same words  for example  amazon might be more affected by the word delivery
or ebay than microsoft  tesla  or apple would because those words are more relevant
to its service  thus  for future work  it might be fruitful to make separate models for each
company  training only on news articles relevant to that company  and making predictions only from articles related to that company 

 

references

    porter  m f  an algorithm for suffix stripping  computer laboratory       
    liu  bing  sentiment analysis and opinion mining  morgan and claypool publishers 
may      

 

fi