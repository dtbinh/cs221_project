stanford university

 

predicting quality of wine based on chemical
attributes
amelia lemionet  yi liu  zhenxiang zhou

abstractordinal data structure is difficult to leverage upon
due to the fact that it has both the properties of regression and
classification  in this paper  we devised a simple new method
that leverages upon ordinal data structure of the data  additive
logistic regression  alr   this method is used to predict the
quality of portuguese white wine based on the chemical attributes
of the wine  we compare alr to non parametric methods  the
results showed that our method  albeit simple  out performs the
non parametric methods 

i  i ntroduction and r elated work
rdinal regression arises frequently in social sciences
and information retrieval where human preferences play
a major role      ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classification  this therefore makes the response more difficult
to model than real numbers or categorical variables alone 
work has been done recently which makes use of support
vector machines to regress on ordinal responses          however  we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found 
the data that we decided to test our method on is from the
uci machine learning repository  in this data  the response
is the quality of portuguese white wine determined by wine
connoisseurs   there are altogether eleven chemical attributes
serving as potential predictors  all predictors are continuous
while the response is a categorical variable which takes values
from   to    
in this paper  we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k nearest neighbors 
the authors acknowledge the fact that work has been done in
field of ordinal logisitic regression  order logit   which tackles
the problem with similar approaches      however  we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be nonintersecting  while this assumption is theoretically true  it
limits the practical application of this method  additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit  in section ii  we will
develop the theoretical foundations of the three methods that
we will compare in this work  k nearest neighbors regression 
weighted linear regression and additive logistic regression 

o

advisor  derek   professor  andrew ng
  ordinal logistic regression finds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest 

in section iii  we will analyze the data through exploratory
analysis  afterwards  in sections iv and v  we implement 
adjust  test and compare the three methods at issue  finally 
section vi and section vii exhibit the main conclusions and
open discussion to other issues of interest 
ii  t heoretical foundations
before tackling the wine quality problem  it is important
to introduce the concepts and ideas that we will be using
throughout this work 
a  notation and formal statement of the problem
to be consistent  let 
 m be the number of examples in the dataset  this may
change as we divide our whole data set into training and
testing sub sets  
 n      be the number of features 
 y be the array containing wine quality for each of the
m examples 
 x be the m  n matrix containing one example on each
row and one feature on each column 
further on  we will note y  i  the prediction we make of y  i   
b  k nearest neighbor classification
one method used in ordinal classification in the industry is
k nearest neighbors  an ordinary k nearest neighbors involves
finding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes  however  in the case of ordinal classification  in order
to make better use of the ordinal structure of the data  we
have decided to take the mean of the responses instead of the
majority of votes  this is illustrated in the diagram below 

fig    

diagram illustration

fistanford university

 

consider the star in the diagram as the test point  an the
points inside the dotted regions as the k nearest neighbors  k  
  in this example   note that if we consider the majority votes
of the nearest neighbors  we would have predicted that this
is class   instead of class    however  if we take the mean 
we will have class      which is much closer to the actual
class of the model  there are various ways in which k  the
number of neighbors  can be selected  in this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized 
c  weighted linear regression
another method we used in this project is weighted linear
regression  which is an non parametric algorithm  it can often
be used to maximize the efficiency of parameter estimation
by attempting to give each data point its proper amount of
influence over the parameter estimates 
the first weight function we used is 
 xi  x  
 
   
   
where x is the input test point  this weight function captures
the distance between the input point and all training points 
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores 
the second weight function we used is 
w i   exp 

max yi  
   
yi
this weight function considers the magnitude of the response
variable  giving more weight to the training response variables
with high quality scores 
the last weight function we used is 
pn
 
i   frequency of yi
n
w i  
   
frequency of yi
this weight function takes into account the nature of the
distribution of the response variable  because the quality
scores are not uniformly distributed  there are fewer cases for
the scores that are extremely low or extremely high  hence 
this function gives more weight to the training points with low
frequency scores 
w i  

d  additive logistic regression
we generally consider the y to take the form 
y   f  x    
with  a zero mean noise  hence we can predict y as
e y  i   x    f  x  

   

in the case of ordinal responses  we discover a special property
of y that we can leverage  to do that  we need to prove the
following lemma 
lemma      suppose y is a multinomial variable such that
y  i                  n 
 i 

e y  x   

n
x
j  

p y  i   j   x 

   

proof 
e y  i   x   

n
x

y  i  p y  i    j   x 

j  

 

n x
n
x

p y  i    k   x 

j   k j

 

n
x

p y  i   j   x  

j  

so essentially  we are turning a multinomial classification
problem into a series of binary classification problems  this
allows us make use of logistic regression as a method to
estimate p y  j   x   then we propose the following
algorithm  algorithm    
algorithm   additive logistic regression algorithm
   for j     to n do
  
divide the training data into y  i   j and y  i    j
  
estimate p  y  i   j   x i    using logistic regression
 i 
  
j   p  y  i   j   x i   
   end for
pm  i 
   y  i   
j j
there are two reasons why we choose logistic regression
over other methods  first  we note that logistic regression is
simple and computationally inexpensive  this is very useful
since we are producing n logistic regression models at the
same time  if we use more computationally expensive methods
such as neural network  the methods can be too computationally expensive and the time taken for model construction  even
with improvement in the results in the end  cannot be justified 
also  using a linear decision boundary prevents overfitting    
and increases the robustness of our model  this makes logistic
regression a better choice compared to the non linear methods 
second  logistic regression is directly modeling p  y   x 
and does not make any assumption on the true distribution
of the response  as the model between each class is highly
correlated  it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another  hence  we have
decided to forgo the idea of using generative models 
although y  i  will be a continuous real number  and not
a discrete class   this method provides an intuitive classifi i 
cation boundary since
 it classifies
x   to be in class c if
 
 
 i 
 c      c       the reason that this
and only if y
boundary is used is that we generally classify a point to
two different classes at the     point  however  like binary
  note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression finds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest  this requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities  when the probability of the point being lower than i is
higher than the probability of being lower than  i        our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes 

fistanford university

classification  there might be different cost associated with
over estimating and under estimating y  this thought brought
us to the possibility of changing
the decision boundary of

y i  c        c        where               
before we test the method on a real data set  we decided that
we should test this method on a arbitrary data set with no
noise  this helped us understand how the method performs in
ideal settings and is a good test of concept  as illustrated in
the diagram below  this method has proven to yield desirable
results when there is no noise in the data  in a   dimensional
naive case    we can observe that the resulting model fits the
data satisfactorily 
another thing that we have to note is that in the ideal setting 
we can observe the number of points in each class is different 
the fitting of the graph signals that the method is robust even
when having different number of points in each class 

 


















occur naturally in the grapes or are created through the
fermentation process 
volatile acidity  a measure of steam distillable acids
present in a wine  in theory  our palates are quite sensitive
to the presence of volatile acids and for that reason a good
wine should keep their concentrations as low as possible 
citric acid  one of the many acids that are measured to
obtained fixed acidity 
residual sugar  measurement of any natural grape sugars
that are leftover after fermentation ceases  in theory
residual sugar can help wines age well 
chlorides  the amount of salt in the wine 
free sulfuric dioxide  the free form of so  exists in
equilibrium between molecular so   as a dissolved gas 
and bisulfite ion  it prevents microbial growth and the
oxidation of wine 
total sulfuric dioxide  amount of free and bound forms of
so   in low concentrations  so  is mostly undetectable
in wine  but at free so  concentrations over    ppm  so 
becomes evident in the nose and taste of wine 
density  measure of density of wine 
ph  value for ph 
sulfates  a wine additive which can contribute to sulfur
dioxide gas  s    levels  which acts as an antimicrobial
and antioxidant 
alcohol  the percentage of alcohol present in the wine 
quality  subjective measurement ranging from   to   
 although the observed data ranges from   to    

the following table  table i  shows the summary for each
variable  note that all variables are continuous except for the
response variable which is categorical 

fig    

one dimension illustration

to the authors limited knowledge  while similar methods
exists            the algorithm proposed is not found in literature that we know about 
iii  dataset and features
our analyses focus in a portuguese white wine database
consisting of        observations  the data set contains eleven
explanatory variables that measure wine attributes and one
response variable  wine quality  in more detail 
 fixed acidity  a measurement of the total concentration
of titratable acids and free hydrogen ions present in the
wine  theoretically  having a low acidity will result in
a flat and boring wine while having too much acid can
lead to tartness or even a sour wine  these acids either
  the

splitting point is generated at random

table i
s ummary of features and response variable

the histogram shown in figure    shows that the values
for the response variable are not uniformly distributed  and
that we have a very high concentration of average wines 
whereas very low quality and very high quality wines are
under represented 

fistanford university

 

weight function      there are different choices for the tuning
parameter    which is the bandwidth parameter  this parameter
controls how quickly the weight of a training point drops as a
function of the distance between xi and the test point x  the
larger  is  the wider the bandwidth around the test point x 
to determine the optimal value for    we used    fold cross
validation on the training set and found       after taking
the minimum cv error 

fig    

distribution of values for wine quality 
fig     error rate for different weight functions  normal linear regression
for reference 

iv  f itting the a lgorithms
to test the algorithm  we first divided the data into a test set
and a training set  we set aside     data points as the ultimate
test set and the rest of the data points          as the training
set  since        data points does not constitute a giant data
set  we used cross validation to find the respective optimal
tuning parameters for k nearest neighbors and weighted linear
regression 
we did not normalize the data because most of the data are
concentrations measured at the same level and hence the scale
is important to the data set 

c  additive logistic regression
it is important to note that the smallest class is supposed to
be    in this case  however  the smallest class is   which is not
ideal  however  a simple solution is to do a parallel shift to the
response variable in order to make a prediction and then shift
the response back after we make the prediction  this method
is mathematically justified since adding a constant does not
change the probability  
v  r esults

a  k nearest neighbors
the first step in adjusting the k nearest neighbors model
was to fix the number of neighbors k  for that  we used   fold cross validation and chose k such that the cv residual
mean squared error  rmse  is minimized  this yielded to a
result of k     
to adapt the mean k nearest neighbors regression to our
ordinal data  we rounded the resulting value to obtain a integer
number 

fig     k     minimizes the cross validation residual mean squared error
 rmse  

b  weighted linear regression
in weighted linear regression  since it is a regression algorithm and produces quantitative outputs  we rounded the
outputs to the nearest integers to make our final predictions 
in weight function          it is quite straightforward to fit the
model because the weights only depend on the training set
and there are no tuning parameters to choose  however  in

in classification  one of the key attributes that we consider
is the test error rate  in this case  since we are not dealing with
binary classification  it is not possible to use the roc curve
or auc as a criteria to assess our performance  however  in
the case of ordinal classification  we can leverage the ordinal
nature of the response to consider not only the error rate  but
also the percentage of response that is under estimated and
percentage of response that is over estimated  in this case  the
number of times we predicted that the wine had worse quality
than it actually has or vice versa  these are the three key
assessment criteria that we will be looking at 

fig     additive logistic regression minimizes the test error  the error
represents how many times we miss classified an observation  the results
shown for weighted linear regression  wlr  are the ones corresponding to
the weights w          

to analyze the previous table  it is important to keep in mind
the naive scenario  as seen in fig     the naive predictor
would classify all wines as being quality   since that is the
mode class  if we were to use the naive predictor  the error
rate would be of       compared to the naive scenario  all
three methods represent a better alternative 
  i e e x

  a    e x    a

fistanford university

 

now  when we compare the three methods against each
other  additive logistic regression outperforms both k nearest
neighbors and weighted linear regression  regarding the percentage of over and underestimation  all methods seem to have
a tendency to over estimate the response variable 

fig    

fig     predicted values respect order in response variable but most values
are concentrated near   

vi  d iscussion
in light of all of the methods that we have applied  we
are encouraged by the fact that the method that we proposed
is performing much better than the other methods  in this
section  we will discuss in detail some of the potential reasons
for this  it is surprising that additive logistic regression actually
performs much better than k nearest neighbor classification
as k nearest neighbor can be highly flexible     in this case 
we believe that additive logistic regression does better at
leveraging the ordinal structure of the data and hence produces
better results 
as for weighted linear regression  we need to note that
weighted linear regression performs well when the number of
predictors is small      in the case of    variables  the predictor
space may be too sparse to generate good results   this can
also be explained by the curse of dimensionality  
in addition to better accuracy in prediction  we like to introduce an additional benefit using additive logistic regression  its
good interpretability  as we go through the ranks  we notice
that the importance of each of variables can change  we will
illustrate this in the case of our wine data 
we consider the logistic regression model of p  y      x 

the coefficients for the model p  y      x 

we notice that the coefficient for fixed acidity here is
actually positive and close to zero  this means that the impact
of fixed acidity is changing from one class to the other  due
to the special nature of logistic regression  we will be able to
observe these changes and therefore peek into the nature of
wine tasting through looking at these coefficients  this highly
interpretability is not available for the other non parametric
methods that we used in this paper 
in addition  we need to observe that the result separating
hyperplane for each class is far from parallel  although this
is counter intuitive  we need to note that that might be other
unknown variables  in this case  the year and brand of the
wine  which  when working together with our variable  could
produce a more or less parallel hyperplane  however  if we use
probability of the test point being in a class as the criteria of
assigning into that class   i e  assign to class with the highest
probability  we might face the problem of having negative
probability which is not mathematically justified  additive
logistic regression circumscribe this problem and make sure
that the predicted results lies within the range of the responses 
however  we would also like to discuss one key part that
is lost in additive logistic regression  we have lost the logodds interpretation of each coefficient  meaning that we cannot
translate the coefficients directly to probability 

vii  c onclusions and f uture work

fig    

the coefficients for the model p  y      x 

notice that the coefficient of fixed acidity in this logistic
regression is            fixed acidity in this case is also very
important since its absolute z value is large 
however  consider the logistic regression model of p  y 
    x 

in conclusion  additive logistic regression proves to be
more accurate method in terms of prediction accuracy and
model interpretability  this is mainly due to the fact that
additive logistic regression leverages ordinal nature of the data 
however  we still believe that more work can be done in
the future  we can add bagging procedure into our methods 
also more work can be done on picking the classification
boundary instead of the naive one that we have tried  we
believe that such methods would improve the results in terms
of prediction accuracy  in addition  we can also treat additive
logistic regression as part of general additive models      this
might further improves the predictability of our model 

fistanford university

 

r eferences
    jaime s  cardoso  joaquim f  pinto da costa  learning to classify
ordinal data  the data replication method journal of machine learning
research                   
    harmen j  dikkers  support vector machines in ordinal classification  a
revision of the abn amro corporate credit rating system august     
http   www kbs twi tudelft nl docs msc      dikkers h j thesis pdf 
    trevor hastie  robert tibshirani  jerome friedman elements of statistical
learning data mining  inference  and prediction   springer   nd edition 
     
    uc irvine machine learning repository http   archive ics uci edu ml  
    winemakers
academy 
understanding
wine
acidity 
http 
  winemakersacademy com understanding wine acidity  
    wine jargon  what is residual sugar   http   drinks seriouseats com 
        wine jargon what is residual sugar riesling fermentation steven grubbs 
html 
    red and white wine quality https   rpubs com daria       
    wei chu  s  sathiya keerthi  support vector ordinal regression http 
  www gatsby ucl ac uk chuwei paper svor pdf
    brian ripley  aut  cre  cph   william venables  cph  class  functions for
classification http   www stats ox ac uk pub mass  
     gareth james  daniela witten  trevor hastie and rob tibshirani  islr 
data for an introduction to statistical learning with applications in r 
http   www statlearning com
     max kuhn  contributions from jed wing  steve weston  andre
williams  chris keefer  allan engelhardt  tony cooper  zachary mayer 
brenton kenkel  the r core team  michael benesty  reynald lescarbeau 
andrew ziem  luca scrucca  yuan tang  and can candan   caret 
classification and regression training  https   github com topepo caret 

fi