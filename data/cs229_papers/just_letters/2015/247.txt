similar language detection

shenglan qiao
stanford university

daniel levy
stanford university

abstract
language identification is a key component of multiple nlp applications  discriminating between similar languages is one of the bottlenecks of state of the art
language identification systems  in this paper  we present a hierarchical method
that first classifies a written sentence into a linguistically defined language group
and then determines the language  we are able to achieve an overall test accuracy
of                      our method is robust and easily scalable to incorporate
many languages as well as more training data 

 

introduction

language identification has many applications across different domains  translation  language interpretation  etc    while it is trivial to discriminate between disparate languages  it is far from a solved
one for highly similar languages and dialects  our problem is presented in the discriminating similar language  dsl  task  a contest that requires teams to predict the language of sentences written
in similar languages  the categories can be different languages or variations of the same languages 
distinguishing one from another is far from trivial  for example  in the training dataset provided
by dsl  more than     of the sentences in the south west slavic group were written with words
present in all three languages  making it impossible to solve with a simple dictionary look up 
in this paper  we present a hierarchical method that breaks down the problem into two consecutive
stages  languages are grouped by their degrees of similarity into groups  a simple word frequency
method first identifies the group a written text belongs to  a more sophisticated method using an
ensemble of support vector machine  svm  classifiers then determines the language within the
group we have also developed this method with efficiency and scalability in mind  our computing
resources were limited to our personal computers  we had to find a way to keep the size of the data
and the complexity manageable without sacrificing too much accuracy 

 

related work

classifying tests written in disparate languages is considered a solved problem      presented the
word frequency method for language classification  which we used for discriminating among language groups 
discriminating among similar languages is a fairly present subject in literature and still open for
discussion  in september       the lt vardial  a joint workshop on language technology for
closely related language  varieties and dialects was held in bulgaria  this workshop featured the
dsl task  a contest where teams were given a training set to build models that would be tested
on an undisclosed testing set  this shared task inspired several research papers on the subject  an
overview can be seen in      
    introduced us to the idea of an ensemble classifier  however      included all languages in a
single ensemble classifier  we decided this method would be difficult to scale up to include more
languages to multiple languages  it would also be computationally inefficient or impossible for us
 

figiven our limited resources      convinced us that a hierarchical methods could perform almost as
well as the winner of the task while being more robust and scalable 

 

dataset and features

the data sets we used were part of the dsl shared task of       the task provided participants with
training        examples per language   development       examples per language  and testing sets
      examples per languages   the sets consisted of individually labeled sentences extracted from
the journalistic corpora in    different languages  the languages were part of   language groups 
 south eastern slavic  ses   bulgarian  bg   macedonian  mk 
 south western slavic  sws   bosnian  bs   croatian  hr   serbian  sr 
 west slavic  ws   czech  cz   slovak  sk 
 spanish es   argentine spanish  esar   peninsular spanish  eses 
 portuguese  pt   brazilian portuguese  ptbr   european portuguese  ptpt 
 austronesian  aus   indonesian  id   malay  my 
we engineered two categories of features  word and character  and we represented them in
a sparse way  word n grams  for example  given a sentence w        wn   the   grams are
w  w    w  w            wn  wn   and character n grams  the same than above but with characters instead of words  with character cross over between words   for example  the   grams of hello
world are hello ello  llo w  lo wo  o wor   worl  world 
we used words n grams for n         and character n grams for n                 

 

methods

the hierarchical method employs a simple word frequency method that first identifies the group a
test belongs to  it then uses an ensemble of svm models to determine the final output  figure  
shows a schematic representation of the method  the svm models are trained with examples within
the language group and can have different number and combination of features across groups 

figure    overview of our learning algorithm 
in this section  we will describe the two stages of our hierarchical method as well as our protocol to
avoid over fitting  parameters of our method were tested and tuned with the development dataset 
before applying it to the test dataset 
   

the word frequency method

the first block of our pipeline is a classifier to distinguish the language group of an input  we built
this classifier using the following word frequency method  from the training set  for each language
 

fil              we define xl  r     s t  xli is the frequency of the ith common word of the lth
language 
given an unclassified sentence s  we define xl  s   l              s t  xli  s      if the ith word of the
lth language is present in the sentence  the classifier is then h s     argmaxhxl  s   xl i  to obtain
l      

the group  we then just have to return the group the language h s  is part of 
mcnamee showed in     that this word frequency method was extremely effective when it came
to classifying rather distinct languages  this method also proved computationally efficient and obtained virtually perfect result        accuracy  for differentiating among language groups  which
we will go more in depth into in section   
   

multi class svms

in class  we derived the optimization program for   class svm  in      crammer and singer makes
a derivation for a multi class svm which we are using 
lets replace ourselves in the context of the course but with k classes this time 
let  x i    y  i     i           m with x i   rn but y  i            k   crammer and singer       
proposed the following multi class approach by solving the following optimization problem 
k

wl  i  l     k i     m

subject to

m

x
 x t
wl wl   c
i
 
i  

minimize

l  
t
wy i  x i 

 wlt x i   i l  i   i              m 

where i l     if i   l  i l     if i    l 
with the following decision function 
hw       wk  x    argmax wlt x
l       k 

we can derive the dual problem 
x

minimize

li  l     k i     m

subject to

 i jm  lk
k
x

m
p
i  

x

i l il

 im  lk

il   i              m 

l  
il 

where wl  

il jl hx i    x j  i  

i l c  i              m   l            k 

il x i    l           k

we can then solve the dual using the coordinate ascent method seen in class  it is clear that this
extension from   classes to k classes works in the exact same way than before and that this classifier
has the same properties  kernel trick      we used liblinear svm to implement this multi class
svm 
   

feature scoring using tf idf

character n grams for higher values  n    mostly  are very prone to over fitting  in order to make
our method robust  we had to select a subset of features for the ensemble of svm models  due to
the size of our features set  approximately   millions  both pca and forward backward search were
out of the question  to overcome this obstacle  we decided to score the features using tf idf and use
only the top rankings ones 
 

fiwe defined the tf idf score of a feature t derived from training set d to be
tfidf t  d    tf t  d   log

n
df t  d 

   

where tf t  d  is the total number of times t appeared in the training set  df t  d  the number of
examples that contain t  and n the number of training examples  since n df  t  d  if always greater
or equal to    this definition of tf idf is always non negative  it is zero if the feature appears in every
training example  intuitively  this means the feature is common to all languages in the training set
and therefore cannot help distinguish them from one another 
   

ensemble methods

to reduce variance  we also decided to train several svm models on a given language group and
combine them using ensemble methods  for each language groups  we trained p svm using different hyper parameters  features and c constant   each one of these svm i  outputs k weight vectors 
we have  wi l   ip  lk weight vectors at our disposition 
after experimenting with several ensemble methods  majority vote  boosting  and mean confidence 
we settled on mean confidence  which can be written as follows 
p
x
h wi l   ip  lk  x    arg maxh
wt l   xi
 lk

t  

for some language group  we did not need to use the ensemble method as high accuracy classification was straightforward  for the others  we trained and combined   svms  each had a combination
word   grams    grams  and character n grams  n                

 

results and discussions

our final classification results for the test set are shown in figure    our hierarchical method yields
an overall accuracy of                      for       of the test examples  the word frequency
method assigns them correctly to their respective language group  we chose to combine the southwestern and west slavic languages into one group  the sws ws group  since the word frequency
method tended to group these five languages together 

figure    confusion matrix for all    languages  group accuracy means the rate at which the wordfrequency method correctly identified the language group of test examples 
in order to avoid over fitting  we tuned the regularization parameter c and performed feature selection based on tf idf scoring  within each language group  multiple svm models were built and
tested on the development set  test accuracies of these model as a function of c were plotted  for
 

fieach svm model  we chose the value for c at which the testing accuracy gain was marginal compared to the training accuracy gain 
we selected subsets of features for our final svm models by observing how their prediction accuracies on the development set changed as a function of features  we ranked features by category
 word   grams    grams and character n grams  n                using the tf idf scores defined in
the method section  starting with the highest ranking features  we computed test accuracies on the
development set using model trained with increasing number of features  figure   shows an example of plots we used for feature selection in the sws ws group  similar to tuning c  our feature
selection protocol kept the top ranking n features in our final svm or ensembles  in figure   n  
       by which increasing the number of features stopped gaining any significant test accuracy on
the development set 

figure    prediction accuracy on the development  sws ws language group  set as a
function of sets of increasingly lower ranked
svm features 

figure    prediction accuracy on the development set  sws ws language group  as a
function of the number of top ranking features 

finally  we also demonstrated that the definition of tf idf was appropriate for feature selection 
svms built with lower ranking features had lower test accuracies when applied to the development set  for instance  figure   shows that test accuracy of models built with sets of word   grams
decreased monotonically  the first set contained the       top ranking features  and the successive
sets contained the same number of increasingly lower ranked features  this consistent trend proved
that we were avoiding over fitting by retaining features most representative of the training data 

 

conclusion

we have achieved classification of    similar languages with an overall accuracy of        this
is close to the dsl task winning teams accuracy of      more importantly  we have explored
a method that is easily scalable and robust  the word frequency classifier requires maintaining a
database of only one thousand frequently occurring words per language  svm features are derived
within a language group  typically made up of only two languages  generalizing to many languages
therefore does not require changing the modeling for existing languages in the hierarchical model
but simply adds independent branches to the hierarchy  this advantage of scalability stems from the
linguistic knowledge used to divide languages into groups  additionally  our methodical selection
of features and the regularization parameter has rendered our method robustness across the development and test sets  this gives us confidence that given more training data our method will perform
well on a variety of texts 
future work will involve improving performance on certain language groups  our method prefers
to label the test examples as a certain language within the south western slavic  spanish  and portuguese groups  more investigation is needed to explain why this happens  gathering training data
from genres other than journalistic articles can also be valuable and may improve the performance
of our method 
 

fireferences
    mcnamee  p       language identification  a solved problem suitable for undergraduate instruction  journal of computing sciences in colleges              
    franco salvador  m   rangel f  rosso p  taule marti m  a       language variety identification using distributed representations of words and documents  proceeding of the  th international conference of clef on experimental ir meets multilinguality  multimodality  and
interaction  clef        volume lncs        springer verlag 
    zampieri  m   tan  l   ljubesic  n   tiedemann  j   nakov  p   overview of the dsl shared
task       proceeding of the lt vardial workshop 
    malmasi  s   dras  m   language identification using classifier ensembles  proceeding of the
lt vardial workshop 
    franco salvador  m   rosso  p   rangel  f       distributed representations of words and
documents for discriminating similar languages  proceeding of the lt vardial workshop
    acs  j   grad gyenge l   bruno rodrigues de rezende oliveira  t        a two level classifier
for discriminating similar languages  proceeding of the lt vardial workshop
    crammer  k   singer  y       on the learnability and design of output codes for multiclass
problems  machine learning  vol     

 

fi