gradient boosted trees to predict
store sales
maksim korolev  kurt ruegg
mkorolev stanford edu  kruegg college harvard edu

e aim to minimize prediction error for a
data science competition involving predicting store sales  we implement baseline
models that are surpassed by xgboost implementation of gradient boosting trees  we further use
sigopt bayesian optimization to modify the hyperparameters of the gradient boosted trees for
further reduction of prediction error 

w

introduction

competition open since month year discrete  ordinal
promo binary
promo  binary
promo  since year week discrete  non ordinal
promo interval discrete  non ordinal

response variable
sales continuous

machine learning forecasting techniques have a number of
applications for businesses  in particular  these methods
can work quite well for large chain stores that are able
to gather significant amounts of data  using this data
to make informed decisions for the future can have large
financial consequences when scaling to a large number of
stores  specifically  this information can inform businesses
on optimal staff levels  product shipments  and sales promotions at each branch  on the data science competition
website kaggle  the german pharmacy rossmann posted
a challenge to the data science community to predict the
sales of their stores for a   week interval in the fall of     
using data gathered from their stores from the previous   
months     
in detail  each training example consisted of the following
features and response variable 

features
date discrete  non ordinal
day of week discrete  non ordinal
store id discrete  non ordinal
customers discrete  ordinal
open binary

a brief explanation of the non obvious features  store
type indicates   of   store designs  assortment is   of  
levels  basic  extra  and extended  of how large the store
assortment is  competition distance gives the distance in
meters to the closest competing drug store  competition
open since gives the year and month when the closest
competitor opened  promo indicates if the basic promotion
is active  promo  indicates if a store is participating in
a cyclic promotion that runs for   month  then takes  
months off  promo  since year   week indicates when the
store started participating in promo   promointerval gives
the months that promo  is active for a participating store
    
the training set consisted of data on      stores from
jan          to jul            with the exception of a    
stores that had an extended closure from jul          to
dec           all stores had entries for every day in the
time period 
the test set consisted of data from all      stores for
the time range from aug          to sep            we
did not have access to the actual values of the response
variables in the test set  but we did have access to our
score on the test set  for the contest  we were allowed to
make   submissions per day 

state holiday discrete  non ordinal
school holiday binary
store type discrete  non ordinal
assortment discrete  non ordinal
competition distance continuous

contest background
concretely  all our models h x  attempt to map our feature
space x  where d is the number of features  to our response

page   of  

fiour simple mean guess decision tree  we performed a multiway split along      stores and then did a     way split
h x   y
on each day of the year to produce our result  we then
d
took the mean of the members of each leaf to make our
x  r   y  r
predictions  we next chose to pursue an algorithm called
the competition utilizes root mean square percent gradient boosted machines  which have been shown to
perform the best out of all decision tree learning methods 
error as the scoring method 
assuming model parameters have been optimized      
v
u n
 i 
 i 
u   x y  y
  
   
rm sp e   t
 
n i  
y  i 
gradient boosted trees

variable y

linear regression model
the first model we tried was a simple linear regression
model  which we made by transforming all discrete nonordinal features into binary encoding and plugging our
features into the built in multiple linear regression function
in r  as expected  our initial model was fairly inaccurate 
rm sp e         

   

mean guess  a simple decision
tree
we noticed that most stores had similar sales across years 

using this insight we made our simple mean guess model 
in order to predict the sales of a day in      we took the
average of      and      for our given day and given store 
the following equation is a general formulation of our
model  the index set a gives the indices of the features that
must match when we make a prediction  our hypothesis
function takes the average of all the training examples that
match all the features that are specified in a 
q
 i 
 i 
i   y
ja   xj   xj  
pn q
 i 
i  
ja   xj   xj  

rm sp e         

f     
ft  x    ft   x    h x 

pn
h x   

the gradient boosted trees method is an ensemble learning
method that combines a large number of decision trees to
produce the final prediction 
the tree part of gradient boosted trees refers to the
fact that the final model is a forest of decision trees  a
decision tree is defined as a model where each non leaf
node represents a split on a particular feature  each branch
represents the flow of data based on the split  and each
leaf represents a classification  an individual decision tree
is grown by first ordering all features and checking each
possible split for every feature  the split that results in
the best score for some objective function becomes the rule
for that node  the splitting process continues until some
termination condition is met  termination can be caused
by running out of features to split on or reaching pure
sets of training examples  but usually an early termination
condition will be met before this  early termination of
tree growth is important to prevent overfitting  which will
be discussed later  finally  in order to make a prediction 
each leaf must have an associated value  the response
of the leaf will usually be the majority response of its
training examples for classification problems and the mean
of training examples for regression problems 
boosted means that the model is built using a boosting
process  boosting is build on the principle that a collection
of weak learners can be combined to produce a strong
learner  where a weak learner is defined as a hypothesis
function that can produce results only slightly better than
chance and a strong learner is a hypothesis with an
arbitrarily high accuracy      the hallmark of all boosting
methods is the additive training method which adds a new
weak learner to the model in each step  in the case of
gradient boosted tree  the weak learner is a new decision
tree  this is shown in the below equation  where f  x  is
our full model after t    rounds and h x  is the new tree
we are adding to the model 

   
   

from here we examined machine learning literature and
realized that we had created a decision tree  albeit a very
simple version  a decision tree is a tree where each node
splits the input space of the node among its children  in

the gradient portion of gradient boosted trees refers
refers to the method by which the new function is added
to the model  the key idea is that each new function is an
attempt to correct the errors of the model built in previous
rounds  thus  this new function h x  should be fit to
predict the residual of ft   x   for xgboost  this insight
is used during the derivation of the the final objective
function 

page   of  

fiin the xgboost package  at the tth step we are tasked parameters
with finding the tree ft that will minimize the following
xgboost requires a number of parameters to be selected 
objective function 
the following is a list of all the parameters that can be
obj f     l f
  f      f  
    specified 
t

t 

t

t

where l ft   is our loss function and  ft   is our regu   eta  shrinkage term  each new tree that is added has
larization function 
its weight shrunk by this parameter  preventing overregularization is essential to prevent overfitting to the
fitting  but at the cost of increasing the number of
training set  without any regularization  the tree will split
rounds needed for convergence 
until it can predict the training set perfectly  this will
usually mean that the tree has lost generality and will not  gamma  tree size penalty
do well on new test data  in xgboost  the regularization
max depth the maximum depth of each tree
function looks like this 
t
  x  
 ft     t   
w
  j   j

   

min child weight the minimum weight that a node can
have  if this minimum is not met  that particular split
will not occur 

where t is the number of leaves in the tree  wj is the subsample gives us the opportunity to perform bagging 
score of leaf j   is the leaf weight penalty parameter  and
which means randomly sampling with replacement
 is the tree size penalty parameter 
a proportion specified by parameter of the training
determining how to find the function to optimize the
examples to train each round on  the value is between
above objective function is not clear  in order to create
  and   and is a method that helps prevent overfitting 
a concrete way of selecting our next tree  we must make
an assumption in order to manipulate the objective func  colsample bytree allows us to perform feature bagging 
tion  first  we assume a fixed tree structure in order to
which picks a proportion of the features to build each
produce an objective function that will allow us to comtree with  this is another way of preventing overfitpare different tree structures resulting from different tree
ting 
splits  an extended derivation that can be read in detail
in a presentation given by the author of xgboost      we  lambda  this is the l  leaf node weight penalty
produce the following objective 
obj ft     

gj  

x

t
  x g j
  t
  j   hj   

y t   l yi   y  t    

iij

hj  

x

y  t   l yi   y  t    

iij

i    i q xi     j 

target variable transformation
    the scoring method of the competition differed from the
loss function included in the gradient boosted tree library
 root mean square percent error vs  square loss   this
meant that we either had to transform our target variable or
write our own loss function  it turns out that doing a simple
log transformation of the target variable is theoretically
sound as long as the difference between our target values
and prediction values is small  see appendix a for a
confirmation of correctness 

where q x  maps input features to a leaf node in the tree
and l yi   y  is our loss function  this objective function is
much easier to work with because it is now gives a score
that we can use to determine how good a tree structure
is  from here we can choose splits that will minimize the
objective function  also  once we have no splits that can
further decrease the objective function  we will have our
termination point  this equation can be manipulated into
a gain function  which is the final equation that does the
dirty work of picking each feature splits in xgboost at
each step in tree growth 

gain  

g l
g r
 gl   gr   
 


hl    hr    hl   hr    

   

where r and l represent the right and left child nodes 
respectively  that would be created in a given split  gain
is then maximized over all possible split options in all the
nodes in the current tree 

v
u n
u  x
rm sp e y  i    y  i     t
 ln y  i   ln y  i   
n i  

   

cross validation
we noticed sales are similar across years  therefore  we
theorized that a good validation set that could mimic the
test set would be the sales in      during the same time
period as the competition test set  aug            sep     
       while rolling window validation is the recommended
approach to time series  we believed that the consistent
pattern of sales and the computational benefits  rolling
window validation takes significantly longer  outweighed
the cons 
for our first result we simply converted all our features
into numerics and fed them into the xgboost algorithm
and got the following score 

page   of  

firm sp e        

of that store  we did mean response variable replacement
     with store  day  and month 
although imposing order is often better than the random
order of non ordinal data  it can be problematic if the
order introduced is incorrect and we artificially force the
tree building algorithm to only pick splits that follow the
imposed order  we did find an improvement in our model
after introducing these transformations  which indicated
that our imposed order was effective 

time series

the above figure shows the learning rate of the xgboost
training process  in order to pick our optimal model  we
choose the boosting iteration that minimizes validation
error 

external data
our data set also had room for improvement  there were
many external factors that influence store sales that were
not given to us in the original data set  we added information about the weather for a given day and region using the
weatherdata r package  which reads data from weather
underground      we also gathered web search rates for
rossmann from google search trends for each region
and day      although stores are labeled without region
information  there is variation in which days are holidays
between the german states  using the holiday information 
it is possible to label which region of germany each store
is in  this allowed us to link each store the google search
trends and weather of its region    

feature variable transformations
gradient boosted trees are good at handling continuous
and discrete ordinal data  however  unordered discrete
data can be problematic because of the implied ordering
that is created when the data is represented numerically 
we had two options in this situation  one hot encoding
or imposing order  one hot encoding simply creates a
binary variable for each instance of a feature to remove the
implied ordering  the second option was to put our own
ordering on these features  we ultimately chose the second
option because one hot encoding can be prohibitively slow
for features with a large number of possible values     
the order that we chose to impose on our discrete nonordinal features was to take the mean of the response
variables for all instances of each feature  for example  for
each store we took the average sales for every example of
that store in the training set and replaced the store label
with that value  this resulted in unique values for each
store  but introduced an ordering based on the average sales

a final consideration that we had to deal with was the fact
that we had time series data  time series data is any set
of data over a sequential time interval  data of this nature
often exhibits autocorrelation  which is the correlation of
our response variable with itself at different points in time 
part of this was taken into account by keeping track of the
day of week  month  and year  these variables helped to
explain cyclic variation in the data  in order to account for
possible autocorrelation  we decided to add lagged variables
that contained the sales for previous days into each our 
this was problematic because our test set did not have
concrete information about previous days sales  so we were
required to do iterative prediction  feeding our predicted
sales into subsequent days to serve as lagged variables 
this introduced the danger of error accumulation  we also
cross validated using iterative prediction to select a model
that did not rely too heavily on the unstable information
in the lagged variables     
specifically  we cross validated on a date range that
matched the size of our test set     days   we introduced
lagged variables of the sales from the previous   days from
the same store  this turned out to not be effective with
our current set of parameters and will be discussed further
in the results section 

bayesian optimization
in order to tune the parameters of our gradient boosted
trees model  we chose to use bayesian optimization  this
method is much faster than grid search and is able to
arrive at a more optimal solution than random parameter
search  bayesian optimization is a black box method that
takes a list of parameters and a function and finds the
optimal parameter set for the function  the process uses
the information from all previous parameter set guesses in
order to determine the next set of parameters to try 
the use of gaussian processes for modeling loss functions
has long been recognized as a good method for optimizing
hyperparameters      gaussian processes interpolate  i e 
curve fit  data with high regularity  i e  non erratically 
using a proper choice of covariance kernels  using bayesian
optimization  a black box function that is costly to train
can be optimized in the least amount of time  assuming a
prior     
for our problem  given a set of already sampled hyperparameters     x         xn   in rd and corresponding
sampling function y    y         yn     such that yk   f  xk  

page   of  

fifor    k  n we want to find a continuous function s
such that s xk     yk for    k  n  
here  the only choice that makes sense for our sampling
function is the cross validation error for a given set of
parameters  because we cannot assume eta is normally
distributed  we held it constant at a sufficiently low number
     of the default value   we also varied number of boosting rounds internally as it scales based on the validation
set  see learning rate   the rest of the hyperperamaeters
were optimized by sigopt 
also  we estimate noise free observations as we only have
a single validation set  k fold or repeated cross validation
would normally be a good estimate of this error   because we assume a gaussian prior  we must also choose a
covariance kernel k x  z  modeled by covariance matrix k


k x    x          k x    x   


  
  
  
k 

 
 
 
k xn   x         

k xn   xn  

with basis



k x  x   


  
k x    

 
k x  xn  
and s defined as 
s x    k x t k   y

    

we can calculate our posterior 
p y   k        n det k 

 
 

 
exp   y t k   y 
 

    

using this posterior  we can then use an acquisition function to calculate the next choice to sample  this function
must properly balance the tradeoff between exploration
and exploitation  i e  choosing to sample where there is
a lot of uncertainty or maximizing already known local
optima   for instance  the expected improvement acquisition function determines the expected improvement over a
target variable t evaluating the posterior at x 
z



ei x    e max    t  y    

 t  y   p y x dy     


the choice of a kernel for gaussian fitting is not a trivial
process  neither is the selection of an acquisition function
an easy task  fortunately  there is software available that
focuses on optimizing these methods  we choose to use
sigopt for its well implemented api  integration for floats
and integers  parallelism and superb results 

results

sigopt optimization  we were able to get a    decrease
in error to        over     observations  our results with
these methods on the test set  leaderboard  resulted in
                 and        rmspe  respectively  we saw
variation between our validation set and test set  this
small amount of variance  though  can be explained to
random chance 
table    rmspe results

model

validation

linear model
mean guess
boosted tree initial
boosted tree external data
boosted tree optimized
boosted tree kitchen sink

n a
n a
      
      
      
      

test
      
      
      
      
      
      

next  we theorized that trees should be resilient to noise
because each tree is built with optimal features based
on the loss function  therefore  we took a kitchen sink
approach  we added all possible predictors that we had
gathered  number of boosting rounds and the time it took
to train the model at each step significantly increased with
this increase in complexity  our validation error decreased
to          unfortunately  due to resource constraints  we
were not able to optimize this parameter set with sigopt 
furthermore  our test error did not decrease from our
previous low of        
finally  we noticed autocorrelation in our training set 
therefore  we theorized that addition of lagged variables
would help explain the data  we attempted adding one
lagged variable as well as one week worth    days   which
would account for the spikes seen in figure    we tested
with a few parameters of the successful trees found for the
data without the one day lagged variables  performance
was similar  although one day lagged variables were far
more costly to compute  one week of lagged variables  on
the other hand  performed far worse  around      rmse  
we account this for the fact that the algorithm is weighing
the lagged variables extremely heavily  but when we forecast them we are exponentially multiplying error  refitting
the parameters using bayesian optimization for the model
with the lagged variables turned out to be highly time
intensive  we hope to rerun sigopt to see if the lagged
variable model outperforms our current best with optimal
parameters when we have the time to recompute 
another future avenue is to implement rolling window
cross validation  currently our cross validation set is quite
small and leads to overfitting on the narrow window of
time that it covers  rolling window cross validation would
increase the size of cross validation and thereby improve the
generality of our model  the problem lies in the amount of
time required for rolling window cross validation models 

our initial run with stock parameters and unmodified
references
features of xgboost resulted in a validation score of       
rmspe  we wondered what feature engineering could     http   www wunderground com 
do  and after implementing feature manipulation specified
in methods  the rmspe was reduced to          with     https   www google com trends 

page   of  

fi    shapire  roberty e  the strength of weak learnabildrop the higher order terms of the taylor expansion
ity  machine learning                   
and plugging into equation     and simplifying 
    chen  tianqi  introduction to boosted trees  university of washing computer science  university of
washington     oct        web 
    rossmann
store
sales 
kaggle 
web 
https   www kaggle com c rossmann store sales 
    prettenhofer  peter  gradient boosted regression
trees  pydata     apr        lecture 

f  y  i      f    y  i     f  y  i    

y  i   y  i 
y  i 


t
 
 
f  t  
t
f  y    ln y    c
f    t  

    bontempi  gianluca  machine learning strategies for
this shows that the correct way to make rmspe and
time series prediction  machine learning summer
rmse approximately equal is to take the log transformaschool  ulb  brussels  lecture
tion of our prediction and target variables 
    j  mockus  v  tiesis  and a  zilinskas  the application
of bayesian methods for seeking the extremum  in
l c w  dixon and g p  szego  editors  towards global
optimization  volume    pages          north holland 
new york       
    snoek  jasper  practical bayesian optimization of
machine learning algorithms  university of toronto 
   aug        web
     sigopt
blog 
sigopt 
web 
http   blog sigopt com post              sigopt indepth profile likelihood vs kriging 
     hastie  trevor  robert tibshirani  and j  h  friedman 
chapter     the elements of statistical learning 
data mining  inference  and prediction   nd ed  new
york  springer                print 

appendix
a
show that rmspe is the same as rmse of log transformed
response variable 
v
u n
u   x y  i   y  i 
 
rm sp e   t
  
n i  
y  i 
r
rm se  

   i 
 y  y  i    
n

in order to make rm sp e equal rm se we need to
find a function f  y  that will make 

f  y  i     f  y  i     

y  i   y  i 
y  i 

   

we can approximate f  p  using taylor expansion  with
the assumption that   the difference between our prediction and response variables  is small 
   y  i   y  i 
f  y  i      f  y  i       f  t    f    t       

page   of  

fi