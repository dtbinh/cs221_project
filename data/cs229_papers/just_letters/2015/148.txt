 

predicting the final score of major league
baseball games
nico cserepy cserepy stanford edu
robbie ostrow ostrowr stanford edu
ben weems bweems stanford edu

abstractwe extracted the result of every mlb plate appearance since       these data allowed us to calculate arbitrary
batter and pitcher statistics before every game  leveraging these
data  we simulated individual baseball games  we found that our
simulations  which incorporated only data available before the
start of the game  were a better predictor of true scores than
the accepted state of the art  over under lines in las vegas  in
fact  a bet based on the techniques described in this paper is
expected to yield profit 
index termsmultinomial logistic regression  softmax regression  baseball  scores  markov chain  betting  mlb

b

i  i ntroduction

aseball has always been known as americas favorite
pastime  it is also a favorite speculation for the ambitious
gamblers of las vegas 
this sport is unique in how much data is available online 
statistics on every plate appearance in major league baseball
 the mlb  since      are available for free  given this
abundance of data  we were interested to see whether we could
improve upon odds makers predictions about the over under
of a baseball game  also known as total  
an accurate simulator would be valuable to teams looking
for a slight edge in managerial decisions  mlb managers  as of
      cumulatively make over      million to make decisions
throughout a game  in a league where the most marginal
improvement can make a difference between the world series
and elimination  every pitching change matters 
the input to our algorithm is the roster of the home team 
the roster of the away team  statistics of all players on each
team leading up to a game  and all metadata about the game 
like the stadium or field conditions 
the output of our algorithm is simply a tuple  sa   ah   where
sa is the predicted score of the away team  and sh is the
predicted score of the home team 
rather than attempt to predict the total scores of baseball
games by feeding metadata about the game into a learning
algorithm  we chose to simulate the game on a play by play
basis  in order to do so  we needed to calculate the probability
of each outcome of an at bat 
the input to this sub problem is the current batters statistics  the current pitchers statistics  the current game state  and
metadata about the game  we ran multinomial logistic regression on these features to assign probabilities to each possible
outcome of a plate appearance  like single  double  or hitby pitch 

this project was done partially in conjunction with cs    
there was significant data massaging that had to be done  and
the database that we built was shared among classes  along
with the general infrastructure for game simulation 
ii  r elated w ork
due to baseballs popularity and its fans penchant for
statistics  there has been substantial work published on the
modeling of baseball games  the markov chain model is wellsuited for scenarios we encounter in baseball since each play
can be considered a distinct state  and transition probabilities
from that state can be estimated given historical data 
in       freeze used a markov model to analyze the effect
of batting order on runs scored      his analysis was likely
one of the first that used such techniques to simulate a baseball
game  he found that batting order does not significantly effect
game outcomes  but his simplifying assumptions were significant enough to raise questions about his results  however  his
techniques provided a useful model for future research 
pankin attempted a more sophisticated version of freezes
research to optimize batting orders in     by attempting to
maximize the number of runs per game  he used a markov
process to simulate games  but did his research in      and
thus did not have the resources to calculate sophisticated
transition probabilities  he assumed that batters hit the same
in all situations  and had only one season of data to work
with  his methods were primitive compared to what is feasible
today  but his was the first paper that proved that markov
processes can be leveraged for an advantage in baseball 
bukiet and elliote take pankins attempts a step further  with
more computing power and statistics  and attempt to simulate
baseball games using a small set of offensive probabilities
     this paper leveraged more advanced models  but ignored
what is probably the biggest predictor of runs scored  pitching
statistics  however      suggests that their methods could be
extended to include pitcher statistics 
humorously  perhaps the most relevant paper is about calculating realistic probabilities for a board game  in      hastings
discusses the challenges and techniques he used to help design
the probabilities of the stat o matic board game  a baseball
simulation run with dice  baseball simulation games were very
popular in the     s  and players wanted a realistic experience 
the insights in this paper about interactions between batter
statistics and pitcher statistics were key for designing our
algorithm  luckily  we didnt have to roll several dice for every
at bat  though 

fi 

iii  dataset and f eatures
to convey our need for a large volume of data  we have
included our feature templates 
binary feature templates  each of these features assumes
a value in         brackets signify where more than
one possible feature exists  for example  there is a separate
binary feature for man on base   and man on base   
          out
man on base          
             precipitation
             field condition

      
      
      
batter

     inning
      stadium
      wind direction
and pitcher same handed

real valued feature templates  each of these features
takes on a real value in          e     generic
out  strikeout  advancing out  walk 
intentional walk  hit by pitch  error 
fielders choice  single  double  triple 
home run  
bat signifies a batters statistic  pit signifies a pitchers
statistic  and limit signifies that only the last    events for
that player in that category were considered  for example 
single bat vs diff handed is the number of singles
per plate appearance against a different handed pitcher in this
batters career  and double pit totals limit is the
fraction of doubles given up by the pitcher in the last    at
bats 
 e 
 e 
 e 
 e 
 e 
 e 

bat
bat
bat
pit
pit
pit

vs same handed
vs diff handed
totals
vs same handed
vs diff handed
totals

 e 
 e 
 e 
 e 
 e 
 e 

bat
bat
bat
pit
pit
pit

vs same handed limit
vs diff handed limit
totals limit
vs same handed limit
vs diff handed limit
totals limit

in total  these feature templates generate     features 
we believe that these features provide a state of the art
description of an arbitrary plate appearance  and they provide
deeper knowledge than any model found in the literature 
to calculate these features  we extracted play by play data
from retrosheet org for all regular season baseball games since
the      season      we focused on the data most relevant
to modern day by gathering statistics for players who were
active in the           range  however  players whose careers
started before      have their career statistics stored 
there were about     million plate appearances between
     and       we needed many statistics about each batter
and each pitcher specific to the day of each game  but gathering
these data in real time would be infeasible  gathering the data
during each the simulation would be too slow  so we had to
precompute all of these statistics  in order to perform accurate
simulations  every player needed up to date statistics for every
game  we pre processed the play by play data to calculate
all relevant statistics for every player in the starting lineups
of every game between      and       this database ended
up being about    gigabytes  but proper indexing provided
a significant speedup relative to recalculating features every
simulation 
along with batter and pitcher statistics leading up to the
game  we also had features that depended on the game state 
statistics such as the number of outs  the inning  or the handedness of the pitcher might affect the outcome of some plate
appearance  since these features could  by definition  not be

precomputed  we added them to the feature set during each
simulation 
unfortunately  an example of the play by play data is too
large to be displayed here  however  we used chadwick     to
help parse the data from retrosheet org  and the approximate
schema of the original play by play data can be found at       
to train our classifier  we randomly sampled one million
events along with their results  from the     million at bats
available after       we also sampled         at bats as a
test set  in our case  since there is no ground truth  analyzing
success on a test set is tricky  rather than a percentage
correct  we calculate the log likelihood of our whole test
set given the trained classifier  this log likelihood can be
compared to log likelihoods of other classifiers to determine
which is superior 
iv  m ethods
baseball games are sufficiently random and complex that it
is not reasonable to model a game as a single vector of features
and expect that any machine learning algorithm will be able
to give accurate predictions  instead  we model the game on
a state by state basis by running monte carlo simulations on
a markov decision process  mdp  
to do so  we discretize each baseball game into states that
represent plate appearances  this requires that we assume that
nothing happens between the end of two plate appearances 
as such  our simulation assumes no steals  passed balls  pickoffs  or other events that might happen during an at bat  while
this may seem like a large assumption to make  these plays
turn out to have very little bearing on the score of the game
since they happen comparatively infrequently     
baseball is a unique sport in the sense that the beginning
of each play can be considered a distinct state  and relevant
information about this state can be easily described  in our
program  we define each state to contain the following information  score of each team  which bases are occupied  the
statistics of the baserunner on each base  the current batting
team  the number of outs  the current inning number  statistics
of the player at bat  statistics of the pitcher  and auxiliary information  auxiliary information is miscellaneous information
that does not change from state to state  like the ballpark  home
team  etc 
let action be any way a batter can end his at bat 
from each state  there are up to    possible actions 
 generic out  strikeout  advancing out 
walk  intentional walk  hit by pitch 
error  fielders choice  single  double 
triple and home run   some actions are not possible
from some states  for example  fielders choice is not
possible if there are no runners on base  from each state 
we can calculate the probability of each possible action 
p  action state   this calculation of p  action state  is
fundamentally the most difficult and important part of the
algorithm 
  see

http   chadwick sourceforge net doc cwtools html
ran out of ram using more than this  but we could feasibly
train on more events 
  computer

fi 

once we have calculated the probability of each action  we
take a weighted random sample  having chosen an action  we
can calculate the probability of each outcome given that action
from the historical data and again take a weighted sample  for
this step  we only take into account the positions of the baserunners and the action chosen  for example 
p  f irst and third  no runs scored  no outs made
 f irst  single  is asking the question  whats the probability
that  after the play  there are players on first and third  no
outs are made  and no runs are scored  given that there was a
baserunner on first and the batter hit a single   the answer
     
is
  or a little less than       these probabilities are
      
pre computed to make the simulation run quickly  so they do
not depend on the batter or pitcher  since there are a large
number of states and transitions  many of which have to be
computed on the spot  we cannot specify a full mdp  instead 
we run monte carlo simulations on the states and transitions
specified above  so  at a very high level  to simulate a single
game once  we do the following 
   enter start state  away team hitting  nobody on base 
etc  
   repeat until game end
a  calculate p  actions state 
b  choose weighted random action
c  calculate p  outcomes action 
d  choose weighted random outcome
e  go to the state that outcome specifies
   gather statistics about simulation

figure    this figure demonstrates what the transition probabilities from a given state to the next state given an action
might be  note that these arrows do not all represent the same
player  the transitions from home plate are for a player at bat 
while those from the bases represent transition probabilities
given some action the batter takes  e g  single  
in order to estimate these probabilities  we train these
million examples on a multinomial logistic regression  or
softmax regression  classifier 
softmax regression is a generalization of logistic regression
to n class classification problems  we assume that the conditional distribution of y given a feature set x is
 

in order to achieve statistical significance  we simulate each
game many times  the key to simulating accurate games is
learning p  actions state  

p y   i x     

e i x
k
p
 
e j x

   

j  

to learn the parameters  of the model  we can maximize
the log likelihood 

     

m
x
i  

figure    this histogram represents the scores predicted by
       simulations of a mets cardinals game  the median
total score of the simulations is    the actual total score of
the game was   

a  calculating p actions state 
recall that we have one million training examples each consisting of     sparse features  all labeled with their outcome 

log

 

   y i   l 

   i 
k b
c
y
b e k x
c
b k
c
  p   x i  a
l  
j
e

   

j  

to solve this problem  we leveraged scikit learns implementation of the broydenfletchergoldfarbshanno algorithm          bfgs approximates newtons method  the
algorithm approximates the hessian instead of calculating it
directly  which makes the problem more tractable  training
  gb of data on a laptop is no easy task    
once we know the parameters  it is trivial to calculate the
probability of each action using equation   and substituting
the results in for p action state  
v  r esults and d iscussion
we quantified the success of our complete system based
on two metrics  a  using the binomial test to quantify the
probability of meaningful results relative to random selection
of vegas over under and b  return on investment  roi   we
then analyze the success of c   our softmax algorithm and
d   our simulation accuracy 

fi 

a  binomial test
prediction confusion matrix

predictions

true result
over

push

under

over

ci    
di    

ci    
di    i

ci    
di    

push

no bet

no bet

no bet

under

ci    
di    

ci    
di    i

figure    confidence vs  statistical significance  the y axis
provides the p value of statistical significance for a one sided
binomial test  or likelihood that our results would be obtained
if the null hypothesis p r ci        were true  the x axis gives
the confidence we had in our predictions 

ci    
di    

table i  confusion matrix that determines how to set the
variables ci and di based on a prediction and a result 
to calculate significance relative to random selection  we used
a one tailed binomial test  our null hypothesis h  is that the
probability of success    is      and our alternative hypothesis
h  is        we evaluated our number of successes  k  out
of total non push  game attempts  n  to measure significance
 p   we define ci to be   if the game was correctly identified
and   if the prediction is incorrect  and set di equal to the
amount bet if the result is a push  otherwise it is set to   
k   max 

n
x

ci  

i  

p   p r x

k   

n
x

  

to calculate roi  we first converted odds     to payout    
for a given bet amount    by using the equation 
 
                   
 
   
                  
in our simulations  we always set       but there is potential
to alter the bet amount conditional on confidence 
we can then use equation   to calculate the roi by subtracting
the bet amount  and adding the payout if the prediction is
correct  we get the total return  r   and divide by the amount
bet  a  to get the roi     

i  

k  
x
n
i  

   

ci   

b  roi

i

r 
i

   

 

n i

  a non push game attempt is defined as a game in which the total runs
scored does not exactly match the vegas prediction  if the runs scored equals
the prediction  all money bet would be returned and this is called a push 

ci

i

  di

   

i  

   

using our model  we correctly identified over or under on
     out of      non push games  which is over     but
not statistically significant  if we focus on games in which
we were     confident of our prediction  we find that we
correctly identified      of      non push games  once again
using equation    we find that p          providing statistical
significance at p         this     confidence is not handpicked as the best confidence  as we see in figure    we have
statistical significance of p        at     confidence and
higher  the statistical significance decreases when we reach a
high confidence threshold because fewer games are considered
making achieve high confidence more difficult 

n
x

a 

n
x

i

   

i  

 

n
p

ci i
r
  i  
n
p
a
i

   

i  

roi can easily be converted to percent expected return  per 
  on a bet as below 
             

   

in figure    we see the per at each range of confidence
intervals compared with pseudo random predictions of over
or under for those games  accomplished by pseudo randomly
setting ci to either   or   in equation    a confidence interval
is determined based on the fraction of simulations of a game

fi 

displays a histogram with scores from game simulations which
presents an average score between seven and nine runs  which
is in line with the      mlb average of      runs  which we
calculated  this  along with topical knowledge among group
members  tells us that the simulations are running realistic
game paths with realistic results for games 
vi  c onclusion and f uture w ork

figure    the percent expected return  smoothed across confidence intervals of             confidence is the fraction
of simulations with the predicted result  over under the total  
random pseudo randomly assigned over under to the games
in that confidence interval 
which give the predicted result  confidence     is calculated
as follows 
n
n
p
p
max  ci      ci   
i  
i  
 
    
n
our lowest per is found at the lowest confidence shown 
       this gives           this value continues to rise
as our confidence increases  while the pseudo random return
stays constant near     
c  softmax regression
our results for softmax regression are probabilistic  so we
worked to optimize the log likelihood of its fit  it is difficult
to assign a score of accuracy because a feature set does not
belong to any specific label  its probability of each label is
exactly what we are seeking  instead of a traditional measure
of accuracy  we say that our classifier is optimal if the loglikelihood of its fit to our test set is greater than any other
classifiers 
we did not have the computing resources to train many classifiers on a million features then predict on          or even
to cross validate in a similar manner  but we did make two
important discoveries  first  the removal of pitching data from
the features significantly decreases the likelihood of the test
set  as expected  second  the learned probabilities performed
very well in the larger scope of the baseball simulations  which
suggests that the fit was at least moderately good  additionally 
we optimized the inverse of regularization strength to balance
between strong regularization and weak regularization 
d  simulations
to determine the accuracy of our simulations  we relied on
years of baseball knowledge among team members  figure  

after extracting the results of every mlb plate appearance
since       and simulating thousands of games as markov
chains using a multinomial logistic regression model  we
generated a state of the art tool for predicting outcomes of
major league baseball games 
while we are very pleased with our results  we regret that we
did not have the capability to formally analyze the performance
of our softmax regression versus other possible classifiers and
feature sets  we varied scikit learns parameters for logistic
regression and found the choice that maximized the loglikelihood of the test set  but did not have the computational
power to cross validate or perform forwards or backwardssearch  as each design iteration took hours to days of computer
time  while our results show that the classifier was effective 
we hope to formalize that notion on future iterations of this
project 
simulations of games provide the opportunity to examine
many more statistics than only total score  the granularity
provided yields a powerful tool for coaches and bettors alike 
r eferences
    r  a  freeze  an analysis of baseball batting order by monte carlo
simulation  operations research  vol      no     pp               
    m  d  pankin  finding better batting orders  sabr xxi  new york 
     
    b  bukiet  e  r  harold  and j  l  palacios  a markov chain approach
to baseball  operations research  vol      no     pp             
    k  j  hastings  building a baseball simulation game  chance  vol     
no     pp             
    d  smith et al   retrosheet website 
    t  l  turocy  chadwick  software tools for game level baseball data 
 online   available  http   chadwick sourceforge net
    p  beneventano  p  d  berger  and b  d  weinberg  predicting run
production and run prevention in baseball  the impact of sabermetrics 
int j bus humanit technol  vol     no     pp             
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikit learn  machine learning in python  journal of machine
learning research  vol      pp                 
    d  f  shanno  on broyden fletcher goldfarb shanno method  journal
of optimization theory and applications  vol      no     pp       
     
     h  cooper  k  m  deneve  and f  mosteller  predicting professional
sports game outcomes from intermediate game scores  chance  vol    
no       pp             
     s  van der walt  s  c  colbert  and g  varoquaux  the numpy array 
a structure for efficient numerical computation  computing in science
  engineering  vol      no     pp             

fi