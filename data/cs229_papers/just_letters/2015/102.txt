cuisine classification and recipe generation
juhi naik  vinaya polamreddi
november     

 

introduction

food is one of the most fundamentals parts of life 
we wanted to use machine learning to approach
food and specifically recipes in new ways  we tried
to answer two different questions using recipe data 
given a set of ingredients  what is the cuisine of
that recipe and if we can automatically generate a
recipe 

 

classification
figure    distribution of data

   

related work

classification of recipes into respective cuisines
based on ingredients has been done by     using
techniques like associative classification and support vector machines  a graph like model is proposed by     to model recipes and tries to match
substructures among the graphs to find similar
recipes  on the other hand      tries to model
recipes using a variety of features like taste  smell 
texture  nutritive value etc  for our project  we
have performed a variety of classification techniques
and analyzed how they perform in comparison 

   

   input set    full dataset divided       for train
and test respectively
   input set    dataset divided       train and
test after removing all north american recipes
   input set    dataset subsampled to have an
equal amount of each cuisine  train set of    
recipes from each cuisine  and test set of   
recipes from each cuisine with a total of     
and     recipes for train and test 
   input set   dataset randomly sampled for
         recipes of train and test sets

data

our dataset is recipes each consisting of
a set of ingredients labeled with a cuisine 
it is from a publicly available
dataset sourced from http   epicurious com
and http   menupan com  
sample data 
ingredients
cuisine
vegetable oil  starch  east asian
shrimp  egg  cream
honey  cocoa  butter  north
cereal  gelatin
american
the y labels consist of    cuisines with the following distribution in the data  the data has a
very high skew towards north american cuisine 
to understand how this skew affects our data
we created four different input sets to test our
classification algorithms on 

   

methods

svm  logistic regression  perceptron and pca
were implemented using the library sklearn     
     

baseline

since       of the data is north american  even
predicting north american for every input will give
us       accuracy  so our baseline is the majority
label 
     

multi variate logistic regression

we applied it to each of our input sets tuning hyperaparameters  penalty  inverse of regularization
 

fi   

methods

 

classification

 a  train accuracy across datasets and algorithms

 b  test accuracy across datasets and algorithms

 c    of data misclassified by   of
methods

 d  number of false negatives of full dataset by cuisine

figure    results for classification

 

fi   

results and analysis

 

strength  class weight  etc  tuning the parameters
only changed the test accuracy by a maximum of
    using l  regularization  we have an accuracy
of       accuracy for train and       for accuracy
on the test set for input set   
     

svm

various kernels like linear  polynomial  sigmoid
and rbf were used after tuning various hyperparameters like the penalty term  kernel coefficient 
degree of polynomial etc  using cross validation 
however  except linear kernels  all others overfit
the training data by predicting north american on
all points  tuning the hyper parameters mentioned
above didnt improve the accuracy by more than
   

   

perceptron

perceptrons were similarly trained on the dataset
using l  and l  penalty terms  this was one
of the few methods that was not affected by the
skewness in the data  using pca on the data to
reduce the dimensionality before applying perceptron classification reduces overfitting even more  if
we inspect the confusion matrices by cuisine  north
american cuisines have the highest false negatives
given by the pca   perceptron algorithm on the
full dataset 

   

generative

to use generative models for classification  we implemented a form of naive bayes by calculating
the probabilities of each ingredient appearing in
the cuisines and the fraction of each cuisine in the
data set  it has an accuracy of about     on the
full dataset and about     on the stratified data 
thus  it doesnt do too well at learning the models
of cuisines with the given features 

   

nearest neighbors

we implemented a modified nearest neighbors algorithm which took all recipes of least distance from
the feature vector of test data and took the most
common cuisine among those neighbors as the prediction  this algorithm ran exceptionally slow and
we could only run it on the two smaller input sets
each of which took approximately    hours to run 
the accuracy on the small random input set was
      and the accuracy on the stratified data was
      
 

   

classification

results and analysis

figures  a  and  b  show the train and test accuracy for each method for each of the input sets  the
first bar in each except for input set   is the majority label baseline  taking a closer look at the test
accuracy for each data set  we can see that for input
set   and     those with the northamerican skew in
the input   most of the methods do similarly  however  only logistic and svm with a linear kernel
do noticeably better than the baseline for input set
   while some of the other svm kernels  rbf and
sigmoid  do marginally better  on the other hand 
for the smaller and still skewed data  logistic and
svm linear do no better than the baseline while all
the other svms do exactly as much as the baseline 
looking at the confusion matrix for each method 
it is clear that the rest of the svms are overfitting
and have only predicted northamerican 
for input set   where we have removed the skew
in training data by undersampling  all the methods have overall accuracy that is lower than each
of their accuracies given the other training sets
which can lead to hypothesizing that training on the
skewed data sets made the classifiers perform better  however  if the accuracy is compared against
the baseline  almost all the methods did significantly better when trained on this dataset  this
implies that training on the skewed data is making
the classifiers perform closer to the majority label 
digging deeper into the confusion matrices for input set   vs input set    it can be seen that classifiers trained on the less skewed data predict more
diversely where the false negatives are spread out
more evenly for all cuisines whereas the incorrect
predictions from classifiers trained on input set  
were almost exclusively northamerican 
we have also tried all the svm and perceptron algorithms after using pca on the input and without
using pca  there was no significant difference in
any data set or algorithm due to using pca and
not 
we also ran analyses comparing each of the methods actual predictions against each other  in figure  c  we plotted the results from analyzing how
many methods misclassified each data point for input set          of data was classified correctly
by every method while       of the data was misclassified by every method  this implies that even
if we somehow picked the best prediction from all
the predictions  we cant do better than       error
rate  digging down  we found that the one method
that uniquely misclassified        of the data was

fi   

methods

 

logistic regression  this implies that the way logistic regression learns and is predicting is somehow
different than the way the svms and perceptrons
are learning from this data 
examining the false negatives graph  we can see
that most algorithms are similar in which cuisines
they misclassify the most except for perceptron
which predicted many northamerican recipes incorrectly which clearly shows that perceptron is
not overfitting  however  its accuracy is one of the
lowest and looking at the confusion matrix  it can
be seen that it is randomly predicting cuisines and
hasnt learned much 

 

generation

   

related work

generation

collected above  the input taken from the user consists of a set of ingredients  a subset of which can
be used to generate new recipes 
given the set of ingredients  our aim is to first select a feasible subset of those ingredients and then
generate a sequence of instructions to perform on
them 
     

step    select ingredients

we first calculate pairwise probabilities of ingredients from our training set by counting how many
times each pair of ingredients appears in the set of
recipes  to get a new set  we would ideally like to
maximize the probability of the entire subset  however  the search space in this case would be impossible to search for  so we begin with the null set and
iteratively add new ingredients to the set by taking
the most feasible from the remaining ingredients using the joint probabilities of the new ingredient with
only the last added one  we stop adding new ingredients once the probability of adding a new one
goes below a certain threshold 

    works on understanding different types of pairing between ingredients  while     does the same
for pairing between a dish and a wine      and    
work on sub parts of recipe generation  the former
attempting to predict the amount of a particular
ingredient given the amounts of other ingredients
and the latter trying to synthesize recipes given an       step    generate instructions
initial seed of favourite ingredients 
instructions for a recipe are a sequence of actions 
each of which is a tuple of verb and ingredient  to
generate this sequence  we use   different models
    data
and compare the recipes generated by each of them
      collection
given the same set of initial ingredients 
allrecipes is a popular recipe sharing website 
recipes contain a variety of information  ingredients  quantities  instructions  cooking time  user
ratings  etc  for our project  we scraped the list
of ingredients and instructions for approximately
      recipes 
to parse the list of ingredients  we wanted the
instructions in the form of a list of actions where an
action was made of a verb and noun pair  in order
to get the data into this format  we created a list of
   verbs commonly used in recipes 
     

input

the input for the model is a set of ingredients available with the user out which a subset can be chosen
to generate recipes 

   

methods

we used a generative model based on bayesian
pairwise probabilities calculated from the recipes
 

   action   action probabilities 
similar to our previous step  here  we directly
take the most probable complete action conditioned on the previous action added to the
sequence  the probabilities for the sampling
are calculated using the number of times the
pair of actions appear in the training recipes 
   action ingredient verb probabilities 
here  we first choose an ingredient to work on 
given the previous action performed  then  a
verb is predicted conditioned on both the previous complete action and the new ingredient
chosen  thus  this model assumes a logical ordering of ingredients that we work on during a
particular preparation and a logical set of verbs
that can possibly be performed on a given ingredient 
   action verb ingredient probabilities 
this model is similar to the previous model except that we first predict a verb given the pre 

fi 

future work

 a  ingredient ingredient probabilities

 b  action action probabilities

 c  action ingredient verb probabilities

 d  action verb ingredient probabilities

figure    bayesian networks for generation of recipes
vious action performed and then choose an ingredient conditioned on both the previous complete action and the new verb chosen  thus 
this model assumes a logical ordering of verbs
that we do during a particular preparation and
a logical set of ingredients that each verb can
be done on 

   

results

below are examples of recipes generated by the first
model for selection of ingredients and the   models
for generation of actions 
user input 
potato  parsley  butter  corn syrup  cornstarch  cinnamon  onion  celery  garlic  tomato  chicken  corn 
turkey  paprika  sugar
ingredient selection 
garlic  onion  tomato  salt  butter  celery  potato
model    action   action probabilities
preheat butter  stir onion  stir garlic  stir
tomato  stir salt  cook potato  cook celery
model    action ingredient verb probabilities
preheat butter  stir onion  stir garlic  stir
tomato  stir salt  place potato  stir celery 
stir parsley
model    action verb ingredient probabilities
preheat butter  preheat salt  beat potato 
beat garlic  drain onion  drain tomato  pour
chicken  pour celery  add parsley  add paprika

   

analysis

we will rely on human evaluation of output to analyze the results due to the nature of our problem  given the user inputted list of ingredients 
the model first selects a set of ingredients that make
 

sense together as they belong to a savory dish without sweet ingredients  the first two models result
in very similar recipes with the sequence of ingredients almost exactly the same and only two verbs
that are different  the third model predicts many
different verbs and ingredients which is probably a
result of the model primarily predicting a sequence
of verbs and choosing possible ingredients  it seems
to pick verbs that dont make sense with our ingredients resulting in strange instructions such as
pour chicken  for recipes  generating the sequence on ingredients or the whole actions seems to
make more sense than generating the recipe based
on a sequence of verbs  the first two recipes actually make semantic sense  you would preheat butter
and stir the onion  garlic and tomato which take
longer to cook before cooking the potato  adding
salt or adding parsley  overall  model   seems to be
the better model allowing for more dynamic formation of instructions and still generating a reasonable
sequence 

 

future work

all of our models pick the next most likely ingredient or verb by picking the most probable next
step according to pairwise bayesian probabilities 
however  optimizing over the entire sequence would
result in much more feasible recipes  we can approach this problem as an mdp where the rewards
are the joint probability metrics on the entire set of
ingredients and instructions generated  however 
our data contains hundreds of unique ingredients
and actions  and computing all possible joint probabilities was an unfeasible task  trying to solve
this as an mdp in conjunction with other heuristics would be interesting future work 

fi 

 

references

   su  han  et al  automatic recipe cuisine
classification by ingredients  proceedings of
the      acm international joint conference
on pervasive and ubiquitous computing  adjunct publication  acm       
   wang  liping  et al  substructure similarity
measurement in chinese recipes  proceedings
of the   th international conference on world
wide web  acm       
   xie  haoran  lijuan yu  and qing li  a hybrid semantic item model for recipe search
by example  multimedia  ism        ieee
international symposium on  ieee       
   ahn  yong yeol  et al  flavor network and
the principles of food pairing  scientific reports          
   meier  justin 
n p        

from food to wine 

tech 

   safreno  doug  deng  yongxing  the recipe
learner  tech  n p        
   agarwal  rahul  jachowski  daniel  shengxi 
david  robochef  automatic recipe generation  tech  n p        
   scikit learn  machine learning in python  pedregosa et al   jmlr     pp                  

 

references

fi