is your story going to spread like a virus 
machine learning methods for news popularity prediction
xuandong lei
xuandong stanford edu

xiaoti hu
xiaotihu stanford edu

abstractonline website has been the majority
source for news to spread  an interesting news is going
to be shared thousands of times through the internet 
in this project  we tried different methods to predict
the popularity before its publication  using over      
sample articles from mashable com  we tried to
address the problem both as a numerical and a
multinomial classification problem  specifically  we
applied linear regression  polynomial regression  gam
with smoothing splines and lasso to predict the exact
shares  the best cv error result is        acquired by
gam with smoothing splines  and then we used svm 
random forest and bagging to predict popularity 
which is divided into four categories  for each article 
in this case  random forest gives us the best result 
which achieves       prediction accuracy 

 

introduction

tons of news  stories and articles are published on
website everyday  the author or editor would like their
articles get shared and referred around the world as
many times as it can be  but even the most skillful
journalists cant be completely sure that their news
directly hit peoples tastes  no matter how wellorganized or gorgeous it might be  there certainly
exists a large amount of features contributing to an
impressive online news or article  if one can know what
kind of news people mostly like prior to the publication 
creating an amazing article is just a matter of time and
proper modification 
our project aims to develop an effective learning
algorithm to predict how popular an online article
 especially news or short stories  would be before its
publication
by
analyzing
several
statistic
characteristics extracted from it  measurement of
popularity is the number of shares an article gets  we
use real world dataset from uci machine learning
repository  instead of inspecting each single word or
phases of the contents  we used some derivations 
the input to our algorithm is a large list of features of
articles which were published in mashable  popularity
of referenced articles  natural language features  e g 
global subjectivity and polarity   popularity of articles
used the same keyword  number of digital media  e g 
images and videos  and published time  e g  day of the
week   we use these features to predict the popularity
an article would be prior to its publication  we solve
this problem in two ways  firstly  we predicted the
exact shares using different regression models  e g 

hongsheng fang
hsfang stanford edu

regression  gam  lasso   after that  we split all
articles into four categories  very unpopular 
unpopular  popular  very popular  and apply three
classification methods  e g  svm  random forest and
bagging  to find the best classification for all test
samples 

 

related work

several popularity prediction methods on web content
have been proposed and compared in recently years 
generally speaking  popularity prediction can be classified
into four cases  single domain  cross domain  before
publication and after publication  previously  similar
researches mainly dealt with predicting eventual online
popularity based on early popularity  however  according
to tatar  alexandru  et al       predicting before
publication is particularly useful for short lifespan web
content like online news  to make it concrete  two
different ways to indicate popularity   i  numerical
prediction  predict the exact value of the popularity   ii 
classification  predict the popularity range that an item is
most likely to fall in 
tsakias et al      address the prediction task as a two stage
classification problem  binary classification to identify if
news articles will receive comments and if they do  go
through another binary classification to identify if the
number of comments will be high or low  they used a
random forest classifier based on a five groups of features
and get a solid performance for the former task  while the
performance for the latter degrades 
moreover  a robust rolling windows evaluation of five
state of the art models is demonstrated in      they
collected        articles from mashable website and
labeled those articles with a chosen threshold  the best
result is given by random forest with a discrimination
power of     for binary classification 
bandari et al       tried to address the prediction task both
as a numerical and a classification problem and predict the
number of tweets of news  the research demonstrated that
although predicting the exact number of tweets may have
a large error  it is possible to predict ranges of popularity
on twitter with an overall     accuracy 
another perspective on this problem is described in work
     they used passive aggressive algorithm to predict retweets number  their study showed that the number of retweet is mainly determined by social features like number
of followers and friends while tweet features like number
of urls and tags could be a substantial boost 

fi 

dataset and features

the data set us obtained from uci machine learning
repository  the original set contains    unique features
with over         samples along with the actual number of
shares for that article  those key features can be grouped
into seven categories  i e  words  links  digital media  time 
keywords  and features related to natural language
processing  such as closeness to lda topic  sentiment
polarity etc   
we start from manually filtering those features to decrease
the number of features to    and pre process the feature to
normalize its mean and variance  the features we utilized
in model is listed in table together with their type 

in high dimensional problem  too many features always
lead to a poor regression result  hence it is very important
to select a small amount of features with statistical
significance  best subset selection is one of the most
popular methods used to select those features  for a
feature set

f              p   

 

find the one with minimum bic  bayesian information
criterion   which is defined as 

bic  
type   
number   
number   
number   
number   
number   
boolean   
number   
number   
number   
number   

the distribution of target value is shown in fig   

 

i features and define f  i     p  i     p  i         pi i  
to be the subset of f and the one with minimum rss
 residual sum of squares  among all f s subset with i
   
   
  p 
features  then we compare all f   f       f
and
with

table   list of features
feature
number of links
number of links to other articles
published by mashable
number of images
number of videos
number of keywords in the metadata
data channel  lifestyle  entertainment 
business  social media  tech or world 
worst keyword  max  min  avg 
best keyword  max  min  avg 
average keyword  max  min  avg 
shares of referenced articles in
mashable max  min  avg 

we consider every subset

 
 
  rss   log n d     
n

where n is the number of observations  d is the number
of features and  is an estimate of the variance of the
irreducible error   
   

regression methods

we first try regression methods to predict the shares of a
given article  we run four classical regression models 
linear regression  polynomial regression  gam with
smoothing splines and lasso 
     

linear regression

linear regression is the most basic regression model 
given a dataset  yi  xi     xip ni    assume that the
relationship between target value and features is linear 
thus the model takes the form 
p

y         i xi      g   x    
i   

     

polynomial regression

polynomial regression is an extension of linear regression 
which replace each xi with some polynomial functions of
xi  the model takes the form like 
p

y         i fi   xi       

figure   histogram of shares

 

methods

after getting the dataset  we tried to solve the problem
both in regression and classification methods to predict the
popularity  prior to that  we also use a feature selection
method called best subset selection to choose the most
significant features  to evaluate the trained algorithm    fold cross validation is adopted to compute the cv error 
further  we then use error based on     loss to measure the
result of classification models 
   

best subset selection

i   

where
     

fi   

is a polynomial function with order p 

gam  generalized additive model 

gam     gives us a more generalized form to extend linear
regression model to non linear cases  instead of using the
form of polynomial regression  gam tries the form of 
p

y         i fi   xi       
i   

fiwhere

fi    is the basis function  in this problem  we let

fi   

to be the smoothing spline for quantitative features 

which tries to minimize 
n

   y  g   x   
i

 

i

i   

m

      g   m    t     dt  

where m is the degree of freedom 
     
lasso
lasso is another regression method that tries to minimize 
n

   y  g   x   
i

 

i

i   

p

    j  
j   

the advantage of lasso is that while gives out the
regression fit as linear regression model  it can also show
the statistical significance of features  and lasso always
obtains great results when only a small portion of features
are of statistical significance 
   

classification methods

as we all know the shares of each article is quite variable
and hard to predict  besides  in the real world predicting
the exact number of shares is not that interesting  instead 
people may just want to know whether the article is going
to be popular or not  therefore  we also try three
classification methods called one vs one svm  random
forest and bagging 
     

one vs one svm

     

bagging  bootstrap aggregation 

bagging is a totally different classification method using
decision tree  we first use bootstrap to generate b sets of
training data from the original set of training data  for the
bth set of all b sets of training data  we fit a decision tree
b

 classification tree  f   x    then we generate the final
prediction by putting an observation to the most frequently
assigned cluster 
     

random forest

random forest is very similar to bagging in general 
however  there is some difference when fitting a decision
tree for bth bootstrap training data set  while making a split
in the decision tree  we only take a random sample of m
predictors into consideration  this method increases the
diversity and hence can reduce the variance 

 
   

results
regression result

to start with  let us take a look at the data distribution  fig 
    similar to what is described in      the distribution can
be thought as a log normal distribution 
after that  the best subset feature selection suggests that a
set of   features gives the minimum bic and the bic for
 i  

each f is shown in fig     as in fig    the minimum
bic is           and the   top features are listed in table
  

the svm model is defined as 
n

max w         i 
i   

  n  i     j  
 y y i j k   x i     x  j    
  i   j   

s t      i  c
n

 y

 i  

i

  

 i  

  j 

i   

where k   x   x   is the kernel function  in our
problem  we use both radial kernel and linear kernel 
radial kernel is defined as 
p


k   x  i     x   j       exp      xk i    xk  j        
 k   


the linear kernel is defined as 

p

k   x  i     x   j        xk  i   xk  j    
k   

one vs one svm is an extended version of original svm 
which is used in a multiclass classification situation 
suppose we need to classify all data into k clusters  we
construct

c k  svms  each svm is used to compare a pair

of clusters  the final classification is determined by
putting an observation to the most frequently assigned
cluster 

figure   best subset feature selection
table   features with statistical significance
no
 
 
 
 
 
 
 
 

features
number of links to other articles
article channel is entertainment
min shares of worst keyword
min shares of average keyword
max shares of average keyword
average shares of average keyword
min shares of referenced articles
closeness to top   lda topic

then we use four regression models described earlier 

fifor the linear regression model  we compute the test mse
while using    fold cross validation  the linear model gets
a cross validation error of          
for the polynomial regression  to prevent our models from
a high order which usually over fits the training data  we
only consider the order of   to   for each predictor with
quantitative values  however  since its meaningless to fit
any polynomial functions on qualitative values  we simply
keep them unchanged  the polynomial model has a
minimum cv error of           when using cubic
function  order    

coefficients  hence all those features have quite similar
statistical significance under regression model 
overall  the error range regression model gives us is
generally in            the error itself  however  is unable
to provide us with the intuition of how well the model
works  considering the fact that the exact number of
shares does not make a big difference when the number of
shares is quite high or low  we label the data sets into four
categories to provide an intuitive result 

for the gam model  we fit smoothing splines with degree
of freedom of          while using the smoothing splines
with degree of freedom of    it gives the best cv error 
which is           
the lasso gives the cv error of            table     the
left vertical dash line is the model with minimum mse 
and the right dash line indicates the model chosen by the
one standard error rule  the above figures are the number
of features chosen by the lasso  namely the features with
non zero coefficients  
table   results of regression
regression model
linear
polynomial
gam w smoothing spline
lasso

cv error
        
         
         
         

figure   lasso
   

classification result

in the classification section  since in real world  the
extremely popular and unpopular articles are relatively
rare and unpopular and popular ones are approximately
even  we intuitively divide articles into   categories by
their shares shown in table   
table   popularity categories
interval of log of shares
           
                
               
            

popularity
very unpopular
unpopular
popular
very popular

no 
 
 
 
 

     
     

figure   linear and polynomial regression

frequency

     
     
    

    

    

 
           

                

               

          

log of shares

figure   popularity categories

figure   gam w smoothing spline
as we can see from fig    even by the one standard error
rule  there are still   features that have a non zero

for the one vs one svm with radial kernel  we try
c                     together with                      
combinations in total   it obtains a minimum error of     
when the cost c       and          then we use one vsone svm with linear kernel and also try the same set of
c   it obtains a minimum error of      when c       
for bagging  we use all    features  namely the number of
variables tried at each split  and build     decision trees

fiin total  in the case of bagging and random forest  we
usually do not compute cv error anymore because it will
be too time consuming  instead  we use out of bag error
estimation  oob   notice that in random forest and
bagging  we first use bootstrap to generate b sets of
training data  to predict a single prediction for the ith
observation  we only need to take a majority vote for the
 i  

predictions generated by training sets without x as its
training data  and the oob for bagging is        
for random forest  we try               and    features
 namely the number of variables tried at each split  and
find that the minimum oob  which is         is obtained
when m       use    features at each split  

variance but less bias  the data set we have is actually a
data with highly non linear relationships between features 
therefore  highly non linear models and models with high
variance will give better result  that is exactly what we
obtain in our regression model  gam and polynomial
model give better result than lasso and linear model   the
random forest model performs best in classification
model  random forest model avoid the case that some
strong features makes all trained decision tree to be highly
correlated  which will definitely lead to a worse result  all
in all  random forest is typically helpful when a large
number of predictors are correlated  which is exactly what
we face in our problem 

table   confusion matrix for random forest

 
 
 
 

 
   
   
  
  

 
    
    
    
   

 
    
    
     
    

 
  
  
   
   

error
    
    
    
    

the confusion matrix shows that the class   and class  
have quite good predictions  we can learn from the matrix
that the there is good pattern for predicting popular and
unpopular classes  however  the pattern for very popular
and very unpopular classes are not that clear 

 
figure   bias variance trade off
we also demonstrated that the eight most significant
features influencing the popularity of an online news are
shown in table   
overall  the work done in this project was important in
understanding the main reasons that effects popularity of
online news from a statistical point  not from writing style
and content  author and editors can easily use the
information we get to adjust their articles  in future work 
we intend to do more exploration about the result we got 
for example  we can find specific information on how
important each feature is  so that the editor can focus on
the most important ones and ignore some to save time 

figure   random forest and bagging
table   classification prediction accuracy
model
svm
random forest
bagging

 
 

accuracy
    
      
      

conclusion

in this project  various regression and classification
models were trained for predicting the popularity of an
online news article before its publication  we use shares
to indicate popularity  the models were trained using
mashable data from uci 
the gam model performs best in regression problem  as
we all know  lasso and linear regression are the model
with lowest flexibility  polynomial and gam  on the other
hand  are more flexible  more flexible models have more

references
    tatar  alexandru  et al   a survey on predicting the
popularity of web content   journal of internet services and
applications                  
    tsagkias  manos  wouter weerkamp  and maarten de rijke 
 predicting the volume of comments on online news stories  
proceedings of the   th acm conference on information and
knowledge management  acm       
    fernandes  kelwin  pedro vinagre  and paulo cortez   a
proactive intelligent decision support system for predicting the
popularity of online news   progress in artificial intelligence 
springer international publishing                
    bandari  roja  sitaram asur  and bernardo a  huberman 
 the pulse of news in social media  forecasting popularity  
icwsm       
    petrovic  sasa  miles osborne  and victor lavrenko   rt to
win  predicting message propagation in twitter   icwsm 
     
    james  gareth  et al  an introduction to statistical learning 
new york  springer       

fi