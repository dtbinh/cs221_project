how well does language based community detection
work for reddit 
urvashi khandelwal
stanford university
urvashik stanford edu

abstract
online communities in the modern day era
are becoming more and more important 
this makes it imperative for us to understand the structure of these communities 
in addition  content generation sites like
reddit  tumblr and quora have an abundance of text in comments and posts which
can be used to model the user interactions
and network substructures  in this paper 
we propose to study community detection
in reddit solely based on language features  to better understand how well language informs the boundaries between different communities  we use supervised
prediction tasks and unsupervised community detection to gauge the quality of these
features and find that they provide a fairly
robust signal in trying to understand and
model user interactions in the network 

 

introduction

one of the most important tasks in understanding a social network is community detection  by
understanding how the network organizes itself
into communities  we gain the insight that can explain the various relations between entities  an
important signal that lends itself well to community detection in an online social network is text 
danescu et  al  showed in their work on beer communities that language plays a key role in identifying the life cycle of a user within a community
 danescu         this is an instance of the importance of language in understanding online social
networks  for a content generation site like reddit  we look to utilizing language features from
user comments  in order to predict subreddits for
test users as well as detect communities within the
user population 
traditionally  community detection algorithms
have looked at network structure as well as node

silei xu
stanford university
silei stanford edu

attributes  however  in this study we focus our attention on the language features  how well do language similarities connect users with similar interests  this is an important question to ask when
trying to understand an individual users diverse
interests as well as which communities suit them
best  such an understanding can aid collaborative
filtering tasks  content recommendation as well as
personalized search 
the rest of the paper is organized as follows 
in section    we first introduce the related works 
in section    we explain the dataset  its processing and feature extraction  in section    we give
a brief overview of the learning algorithms  proceeding to explain experiments in section    wrapping up with a conclusion in section   

 

related work

online communities have been studied for decades
and most of the traditional community detection algorithms put their effort in analyzing graph
structure of the data  fiedler        pothen       
newman         however  in the real world  apart
from the topological structure  we also have content information available to us  in recent years 
analysis on community detection on networks
with node attributes has gained more and more attention  zhou        yang         one of the most
significant attributes for nodes  users  in content
generation sites is their language  danescu        
in this project  we go one step further by detecting communities solely based on users language
to see how well language informs the user interactions and ground truth communities 

 

dataset and feature extraction

reddit is an online content generation website organized by topically specific subreddits  where
users can submit posts and have other users com 

fifigure    a subset of unigrams for a randomly selected user
where size of word is correlated to frequency

ment on them  the reddit comments dataset 
is a publicly available compilation of comments
from nearly        subreddits since       the
number of comments in these communities follow
a heavy tailed distribution  such that only about
      communities have at least       comments
in the year of      
in this paper  we only consider comments
posted during       in addition  we consider only
midsized communities which are defined as those
that had at least     thousand comments and at
most   million comments over the entire year  this
choice is driven to avoid extremely large communities where users are likely to feel a lack of loyalty and extremely small communities where the
number of comments are too small to justify a
wholesome community  after filtering in this way 
we end up with     communities 
next  we looked at the user population for the
included communities  first we filtered users by
removing bots  bots were identified as usernames
that ended with the string bot or bot  as well
as users that posted over      comments in      
posted to over     communities in      or over
    comments to a single community  activity
representative of bots upon looking at the data  
once we get this raw list of users  we further prune
all those who posted less than      comments
through the year  in order to have a large number
of features per user  in this study  we include     
users with each having posted in at least    subreddits  overall  we have           comments 
finally  we extract language features for each
user  since the language in reddit is extremely informal  we start by looking at unigrams  for each
user  we start by tokenizing each comment using
nltk    a canonical list of stop words is used
to filter through the more topical and user specific tokens  tokens containing punctuation  urls 
 
 

https   www reddit com r datasets 
www nltk org

numbers and non ascii characters are removed 
finally  nouns and verbs are lemmatized  this
generates a list of unigrams with term frequencies  the overall size of the unigram vocabulary is
          words  with        average unigrams
per user  for the entire set of users  we proceed to
compute term frequency inverse document frequencies as follows 
n
tfidf  w          log tf  w      log
df  w      
where  w is a single token  n is the number of
users  tf is the term frequency  and df is the document frequency  treating each users comments
collectively as a document  we use tfidf scores as
the feature vector for each user  given the sparsity
of the perceived user unigram matrix  we use svd
to map the features to lower dimensions  making
the different users language more comparable 

 

learning

in order to gauge the quality of our languagebased features  we looked at two types of tasks 
supervised prediction and unsupervised community detection 
   

supervised learning   multilabel
classification

for supervised prediction of subreddits for test
users  we use multilabel classification algorithms
since each user posts to multiple subreddits and
belongs to mutliple classes 
      decision trees
decision trees  quinlan        are described as
follows 
   start with the entire dataset 
   split along the attribute dimension that provides the highest information gain 
   divide examples into children based on the
splitting attribute 
   recurse on the children  going back to step  
for each 
information gain on set x given that we know the
attribute value for a is defined as follows  dependent on the definitions of information entropy  h 
and conditional entropy 
ig x a    h x   h x a 
x
h x   
p xi   log  p xi  
i

h x a   

x
i j

p xi   aj   log 

p aj  
p xi   aj  

fiin the context of our setting  decision trees can
be thought as splitting based on different topics 
thereby creating a kind of a topic hierarchy in the
process  the subreddit labels lie in the leaves and
each user can belong to multiple subreddits 
     

random forests

random forests is an ensemble technique that
combines multiple decision trees  brieman        
each tree k         n is constructed based on i i d 
random vectors k   sampled from the feature distribution  the outputs of all trees are combined
based on majority vote  with some threshold for
multilabel settings  this helps to remove variance
by training on different parts of the dataset and averaging over all predictions  in our setting  this is
expected to be helpful in reducing overfitting as
well as the generalization error 
     

ml k nn

multi label k nearest neighbor is an extension
of the k nn algorithm  for every query user from
the test set  we compute the k nearest neighbors in
the feature space  using their class distribution as a
prior  then  the map estimate is used to compute
the label set for the query user  zhang         in
our setting  this would help to demonstrate how
well the distribution in the feature space represents
actual subreddits 
this algorithm should perform better than the
two based on decision trees  since topical hierarchies try to isolate a subreddits language where
multiple subreddits might have similar language
and this may lead to higher error  on the other
hand  ml k nn attempts to find similar users 
which might all be posting to a similar set of subreddits and would have higher confidence in the
predictions 
   

unsupervised learning

community detection can be seen as a clustering
of nodes into sets of similar entities  hence  in order to detect communities in reddit  based on the
users language  we tried two unsupervised clustering algorithms 
      k means clustering
we start with the k means clustering algorithm to
group users into communities  with the objective
of using coordinate descent to minimize the within
cluster sum of squares metric 

j c     

m
x

  x i   c i     

i  

where  j is the distortion function that measures
the sum of squares between each n dimensional
example x i  and the center of the cluster to which
it was assigned c i    in this setting  the x i  s are
feature vectors containing tfidf scores for user i 
this algorithm performs hard clustering  i e  every
user is assigned to a single cluster 
      em algorithm
a soft clustering algorithm which uses coordinate
ascent to maximize the following objective function 
j q     

m x
x

qi  z  i    log

i   z  i 

p x i    z  i     
qi  z  i   

where x i  is the feature vector containing tfidf
scores for user i  z  i  is the subreddit being considered  qi  z  i    is the distribution over the labels
for user i 

 

experiments and analytical discussion

in this section  we present results for the supervised and unsupervised approaches to learning described above  we evaluated both techniques using precision  recall and f   scores  precision is
defined as the fraction of the subreddit labels predicted that matched the ground truth  recall is the
fraction of ground truth subreddit labels that were
predicted and f   score is the harmonic mean of
precision and recall 
   

ground truth

ground truth in this dataset exists in the form of
sets of subreddits for each user  where a subreddit
is included if the user posted to it during the year 
this ground truth is useful for the evaluation of the
multilabel classification as well as the clustering
approaches 
   

prediction task results

the prediction task involves training a model to
learn which subreddits a user belongs to based on
the feature vectors  in order to predict these for a
test set of users  we randomly pick      users
as our training set and test on the remaining     
users using the supervised learning algorithms described in section       decision trees  random

fi a  decision trees

 c  k nn

 b  random forests
figure    supervised learning precision recall curves

forests  and multi label k nn  for random forests 
the number of trees used was    for ml k nn  the
number of neighbors considered was    the precision  recall  and f   scores are shown in figure   
as we can see  the results for decision trees and
random forests are relatively better for fewer features  this can be explained by the fact that after
performing svd and picking the top n features 
for a small value of n this tends to include the more
topically relevant features that help to distinguish
between similar subreddits while classifying a new
user  for large values of n  we tend to include
more noisy features which would cause the model
to overfit  for instance  with fewer features we
get a shallower decision tree  but as it gets deeper
with higher n  performance degrades as picking
the correct subreddits at the leaves gets harder due
to confusion caused by noisy features 
using a higher number of decision trees    in
random forests  helped to increase the precision
in the sense that fewer subreddits were being predicted and a larger fraction of these were correct  but recall was lower since fewer predictions
meant retrieving lesser of the ground truth  overall  fewer noisy results  based on confusion caused
by noisy features  meant that using random forests
served as an improvement over decision trees  just
as expected 
for ml k nn  with a larger number of features  the performance remained fairly stable because the algorithm relies on other similar users
to make a prediction and when the feature vector
dimensions change  it affects all users in a similar
way  showing that similarity of users in the feature vector space is robust in the context of noisy
features 
   

community detection results

baseline  we implemented a naive baseline based
on random assignment to get a sense of how the
clustering algorithms performed in comparison to

random guessing  in this baseline  every user is
assigned a cluster between   and k uniformly at
random 
k means clustering  we adopted llyods algorithm for the k means clustering  the number of
clusters were varied from   to      where each
user is assigned to a single cluster 
em  we used the gaussian mixture model implementation for em  the number of latent variables were varied from   to     and a probability
was computed for each users membership for the
class 
we first evaluate the performance of k means
and em by calculating the average weight of intracluster and inter cluster edges of detected clusters 
where edges are defined as follows  for a user ui  
let ci denote the set of subreddits ui commented
in  and for each subreddit c  ci   let ni  s  denote
the number of comments ui posted in c  then the
weight of the edge between a pair of users ui and
uj   denoted by wi j   is defined as
 p
  p

ni  s 
nj  s 
i cj
 cci cj
  cc

wi j    p
 p

ni  s 
nj  s 
cci

ccj

note that if ci  cj     then wi j      and in
case that ci   cj   we have wi j      the results
are shown in fig   a 
the availability of ground truth subreddits allows us to quantitatively evaluate the performance
of these two unsupervised learning algorithms 
however  its hard to find the mapping between detected communities and ground truth subreddits 
thus  we evaluate the clustering results by calculating the average f   score of the best matching
ground truth community to each detected community and the best matching detected community to
each ground truth community 
 
p
p
 
 i 
ci c f   cmi   ci  
ci c  f   ci   cm
 
  
 c   
 c 

fi    
    

clustering evaluation
k means  avg weight of intra edges
k means  avg weight of inter edges
em  avg weight of intra edges
em  avg weight of inter edges
avg weight of all edges

out the year  however  as shown in fig     users
post comments to different subreddits at different
times of the year  mixing signals from throughout
the year might be adding more noise and it could
be interesting to consider comments and ground
truth from specific time windows  day  week etc   

    
    
    
     

   

   

   
   
number of clusters

   

   

 a  weight of intra cluster and inter cluster edges
    
    

clustering evaluation
k means
em
random

f  score

    
    
    
    
     

   

   

   
   
number of clusters

   

   

 b  average f  score
figure    supervised learning evaluation

where c    c denote the set of ground truth and
detected communities  respectively  mi   m i denote
the best matching  mi   arg maxcc f   ci   c  
m i   arg maxcc  f   c  ci    the results are
shown in fig   b 
from fig   a  one can observe that both kmeans and em algorithms perform much better than randomly guessing cluster assignments 
the average weight of intra cluster edges is much
higher than inter cluster edges  whereas the two
metrics should be similar if we cluster users randomly  from fig   b  one can see that both algorithms perform poorly when trying to match the
ground truth precisely  the algorithms perform
relatively better when trying to discover fewer
clusters  this is because the similar subreddits
have similar language which allows them to be
grouped to form super communities and the clustering algorithms seem to be better at detecting
these clusters than the more fine grained subreddits that we are working with 
   

user activity

the prediction task and community detection results collectively demonstrate that the unigram
tfidf scores do not form a strong set of features 
one reason for this can be that we are constructing
these features by using comments posted through 

figure    event plot showing the comments posted to different subreddits for a randomly selected user who posted    
comments to    subreddits through the year 

 

conclusion and future work

from this study  we see that language in the comments holds signals that inform the process for
modeling user interactions in reddit  if we consider the task of trying to understand the interactions based on similar interests  rather than the precise subreddits  these techniques hold a lot of potential  in this case  it might work to our advantage to try and cluster the ground truth into groups
based on similar topics  another possible future
direction could look at making the language features more robust  i e  considering bigrams  processing the comments using more advanced natural language processing techniques as well as setting up word vectors to match language based on
semantic meaning  finally  it might also be beneficial to consider time windows for the comments 
this would not only improve the current study 
but also allow us to understand how these user interactions change over time  in conclusion  our
project served as a good starting point for looking
at text to model interactions between users in the
reddit network and after confirming that the language features provide robust signals  we can pursue many future directions to make a more compelling argument about how they can be useful 

references
fiedler  miroslav        algebraic connectivity of
graphs  czechoslovak mathematical journal 

fipothen  alex  horst d  simon  and kang pu liou 
     partitioning sparse matrices with eigenvectors
of graphs  siam journal on matrix analysis and applications 
newman  mark ej       fast algorithm for detecting
community structure in networks  physical review
e 
yang  j  and mcauley  j  and leskovec  j       
community detection in networks with node attributes  ieee international conference on data
mining  dallas  tx  usa 
zhou  y  cheng  h  and yu  j        graph clustering
based on structural attribute similarities  proceedings of vldb endowment  lyon  france
zhang  m  and zhou  z        ml knn  a lazy
learning approach to multi label learning  pattern
recognition
brieman  l        random forests 
quinlan  j  r        induction of decision trees  machine learning
pedregosa  f  et  al        scikit learn  machine
learning in python  journal of machine learning
research
danescu niculescu mizil  c  and west  r  and jurafsky  d  and leskovec  j  and potts  c        no
country for old members  user lifecycle and linguistic change in online communities  acm international conference on world wide web  rio de
janeiro  brazil 

fi