language identification from text documents
priyank mathur

arkajyoti misra

emrah budur

adobe systems
stanford university
stanford  california      
priyankm stanford edu

target corporation
stanford university
stanford  california      
arkajyot stanford edu

garanti technology
stanford university
stanford  california      
emrah stanford edu

abstractthe increase in the use of microblogging came along
with the rapid growth on short linguistic data  on the other
hand deep learning is considered to be the new frontier to
extract meaningful information out of large amount of raw
data in an automated manner      in this study  we engaged
these two emerging fields to come up with a robust language
identifier on demand  namely stanford language identification
engine  slide   as a result  we achieved        accuracy in
discriminating between similar languages  dsl  shared task
     dataset  beating the maximum reported accuracy achieved
so far     
index termslanguage identification  lid  slide 

i  i ntroduction
automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for
machine translation  improving the search relevancy by personalizing the search results according to the query language
     providing uniform search box for a multilingual dictionary
     tagging data stream from twitter with appropriate language etc  while classifying languages belonging to disjoint
groups is not hard  disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing  regular
classifiers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classifier performance  in this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language processing tasks to build a state of the art language classifier 
we benchmarked our solution with the industry leaders and
achieved first rank in the dsl test dataset 
ii  p revious w ork
in the past  a variety of methods have been tried like
naive bayes      svm      n gram     graph based n gram    
prediction partial matching  ppm       linear interpolation with
post independent weight optimization and majority voting for
combining multiple classifiers      etc  and the best accuracy
achieved are still in the lower ninety percents 
the researchers have worked on various critical tasks challenging the dimensions of the topic  including but not limited
to  supporting low resource languages  i e  nepali  urdu  and
icelandic            handling user generated unstructured short
texts  i e  microblogs            building a domain agnostic

engine            existing benchmarking solutions approach
the lid problem in different ways where logr      adopts a
discriminative approaches with regularized logistic regression 
textcat and google cld     recruits n gram based algorithm  langid py     relies on a naive bayes classifier with
a multinomial event model 
the outstanding results  of the time  suggested by cavnar
and trenkle became de facto standard of lid even today     
the significant ingredient of their method is shown to use
a rank order statistic called out of place distance measure
      the problem in their approach is that they generated
n grams out of words that requires tokenization  however 
many languages including japanese and chinese have no word
boundaries  considering that japanese is the second most
frequent language used in twitter       there is a need for
better approach to scale the solution to all languages  as a
solution to their problem  dunning came up with a better
approach with incorporating byte level n grams of the whole
string instead of char level n grams of the words      
after a rigorous literature survey  we found no prior study
that applied deep learning on language identification of text 
on the other hand  there are a few number of studies that
applied deep learning to identify the language of speech      
                  we believe this study will be the first in the
literature if published for lid in textual data by means of deep
learning 
iii  dataset description
the data for this project work was obtained from discriminating between similar language  dsl  shared task     
      a set of       instances per language        training
 train txt  and      evaluation  test txt   was provided for   
different world languages  the dataset also consisted of a
subset  devel txt  of the overall training data which we utilized
for hyper parameter tuning  the languages are grouped as
shown in table i  the names of the groups will be frequently
referred in the subsequent sections 
each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin  a
similar set of mixed language instance was also provided to
add noise to the data  a separate gold test data was provided
for the final evaluation  test gold txt  
we applied t sne algorithm to visualize the instances
in  d euclidean space             for feature extraction 

figroup name

language name

language code

south eastern slavic

bulgarian
macedonian

south western slavic

bosnian
croatian
serbian

bs
hr
sr

west slavic

czech
slovak

cz
sk

ibero romance  spanish 

peninsular spain
argentinian spanish

es es
es ar

ibero romance  portuguese 

brazilian portuguese
european portuguese

pt br
pt pt

astronesian

indonesian
malay

bg
mk

id
my

table i  benchmark results of available solutions

 a  easily separable

 b  difficult to separate

fig     t sne visualization of language groups  more
plots including  d animated plot are available at  
http   seeyourlanguage info

fig     naive bayes performance as a function of n for both
word and character n grams 

acters carry little information and therefore the performance
for character n gram improves quite sharply as the number
of characters is increased before saturating at about n    we
experimented with character n grams both restricted at word
boundaries and spanning across word boundaries  the latter
has a marginal performance boost at the cost of longer training
time and memory pressure  the word n gram model peaks
at n   and drops beyond that  while higher order n grams
carry more structure of the language  they become increasingly
infrequent too and therefore the models dont always get a
boost from it  both the character level and word level n gram
models show similar performance where they really excel
at certain languages  czech  slovak  and do poorly at other
 bosnian  croatian  serbian  
b  logistic regression

we vectorized each sentence over   to   grams of the tokens delimited by white space characters  fig    shows
the resulting plot  as can be seen on the plot  the languages in the same group overlap a lot while the languages
in different groups can be linearly separable  a   dimensional visualization of all the languages can be viewed at
https   www youtube com watch v mhrdfc  q   
iv  m ethods
a  multinomial naive bayes
we created a baseline result by training a multinomial
naive bayes model because it is quick to prototype  runs
fast and known to provide decent results in the field of text
processing  we have done no pre processing of the text commonly done in the field like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language  particularly when the same
language is spoken by two geographically disconnected group
of people  e g portuguese spoken in portugal and brazil  
we experimented with both word and character n grams  the
character n grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet 
the character level n gram behaves quite differently from
that of word level n grams as shown in fig     single char 

we next tried a regularized logistic regression and here
too the character level n gram performed a little better than
the word n grams  fig    shows that the model was able to
completely fit the training set but the performance on the
validation set plateaued close to       the best performance
was obtained by a character   gram model that includes all
n grams up to n    these n grams were truncated at the
word boundaries  or in other words these n grams did not
capture two or more consecutive words  relaxing this criterion
significantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent 
c  recurrent neural network
the mnb and lr approaches work really well in distinguishing two languages that have very little in common
because the set of n grams will have very little overlap
between them  this approach does not work very well when
two languages are close to each other and share a lot of words
between them  therefore  it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages  we explored recurrent neural networks  rnn 
for this purpose 
rnns are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units  as

fiof the process  we varied a single parameter while keeping
the other two constant  the plots below  fig     show the
performance of the resultant models on the validation dataset
as each parameter was changed 

fig     the lr model was able to completely fit the training
data but the accuracy on validation data peaked at about    
overall 

fig     variation of the accuracy on validation dataset as we
vary training epochs  number of hidden units and drop off

such  rnns are able to record temporal dependencies among
the input sequence  as opposed to most other machine learning
algorithms where the inputs are considered independent of
each other  hence  they are very well suited to natural language processing tasks and have been successfully used for
applications like speech recognition  hand writing recognition
etc 

fig     visualization of an un rolled recurrent neural network
    
until recently  rnns were considered very difficult to
train because of the problem of exploding or vanishing
gradients     which makes it very difficult for them to learn
long sequences of input  few methods like gradient clipping
have been proposed to remedy this  recent architectures like
long short term memory  lstm       and gated recurrent
unit  gru       were also specifically designed to get around
this problem  in our experiments  we used single hidden
layer recurrent neural networks that used gated recurrent units 
hyper parameter tuning  in our single layer networks  we
had three model hyper parameters to search over
   epochs   the number of iterations over training data 
we generally try to train until the network saturates 
   hidden layer size   number of hidden units in the
hidden layer 
   dropout   deep neural networks with large number
of parameters are very powerful machines but are
extremely susceptible to overfitting  dropout provides
a simple way to remedy this problem by randomly
dropping hidden units as each example propagates
through the network and back      
we used a subset of our overall training data  devel txt  for
hyper parameter selection  this subset was further divided into
    training data and     validation data  in the first step

fig     grid search over the best parameter values found in
the previous step
as we can see in the plot above in fig     increasing the
number of training epochs improves the model performance
up to a certain stage  after which it plateaus  hence  for the
next stage of tuning  we fixed the number of training epochs
to     using the best values for the number of hidden units
and dropout found above  we performed grid search over
all combinations of these parameters  the result of the grid
search is visualized in fig     the  number of hidden units 
dropout  combinations             and             gave us
the best performance on the validation set  the final values
chosen for further experimentation were     hidden units and
     dropout so as to avoid overfitting 
training procedure  our final model is an ensemble of  
rnns  each built using a different feature set  namely  from
character   grams to character   grams and word unigrams  to
train our models  we divided our entire training data  train txt 
into     training set and     validation set  once trained  we
measured the performance of each model individually on the
validation set and is reported in table ii 
as seen in fig     to construct the ensemble  instead of
manually assigning weights to each model  we constructed a
logistic regression model to get the final output  the features
for this lr model were the outputs from the   rnns created
earlier and it was tuned using   fold cross validation over the
    validation dataset 
for training the rnns  we used a python library called
passage       which is built on top of theano  although the

fiaccuracy
validation set
test set

model
mnb  char   gram 
lr  char   gram 
rnn  char   gram 
rnn  char   gram 
rnn  char   gram 
rnn  char   gram 
rnn  word uni gram 
ensemble of rnn model  slide 

      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      

table ii  performance comparison of various models
fig     training procedure for the rnn ensemble

library provides several tools for text pre processing including
tokenization  it lacked the ability to generate character n gram
level features  therefore  we had to extend the library with
custom character level feature generators  in addition  training
neural networks on cpus consumes a lot of time  hence 
for our experiments  we leveraged aws gpu  g   xlarge 
instances that provided a   x boost in time required to train
one model 
v  r esults
table ii shows a comparison of the models we have experimented with  one surprising feature of the result is that
individual rnn models were not able to beat the performance
of the mnb and lr models  even though the latter models
have minimal knowledge of a language structure  however 
when we created an ensemble of rnn models  it turned out
to be the best model and crossed the     threshold for the first
time  it should be noted that for a particular n gram model 
mnb and lr models use all m grams where    m  n 
however  due to the very nature of an rnn architecture  a
combination of n grams cannot be used because that will lead
to an overlapping sequence of content to be fed to the network 
since any given n gram captures only limited information
about a language  it was natural to try an ensemble of n gram
rnn models with different values of n  so that structure of
the language can be captured at multiple different levels 
the boost in performance due to ensemble can also be
attributed to model combination  which aims to achieve at least
as good of a performance as the worst model in the ensemble 
this is because individual models can make mistakes on
different examples  and therefore  by using an ensemble we
are able to reduce this variance  while we tried other model
combination strategies like median and manual weighting 
building a logistic regression classifier on top of rnns really
helped us find the optimal weight that should be given to each
individual model  we could not include rnn models beyond
character   gram in the ensemble because of memory limitation and including the mnb or lr model in the ensemble did
not improve the performance of the model 
vi  d iscussion
the final classification for each language group is captured
in the confusion matrix in fig     it is quite evident from our

fig     confusion matrix

results that the biggest challenge consistently posed to our
classifiers is distinguishing the languages in south western
slavic group  bs  hr  cr   the training set revealed that among
all the words in bs      are common to hr and     to sr 
since fig    clearly showed we didnt underfit the training
set  it made sense to augment the training data in these three
language categories  we incorporated a significantly larger
labeled data for two of these languages and also downloaded
newspaper articles in bs  but the classification accuracy in this
language group did not improve  looking closer to some of
these external datasets revealed that none of the new words
could be uniquely associated to any of the three languages
and therefore  the additional data probably added more noise
than signal 
to understand the failure mechanism of the classifier for
the south western slavic language group  we fed the lr
classifier  which is the best of single models in validation set
according to table ii  different fractions of a document it failed
to classify correctly  for example  the following document
is in bosnian bs  but the classifier predicts its language as
serbian sr   usto se osvrnuo na ekonomsku situaciju u kojoj

fipart of the corpus  we have relied on the ensemble of rnn
models to discover the structure unique to a specific language
but we could not engineer any additional feature due to lack
of knowledge in those specific languages  at this point  we
think  further improvement can only be achieved by designing
rule based features by talking to language experts or native
speakers 

acknowledgment

fig     scenarios where the lr classifier incorrectly predicted
the target language   detailed description in section vi 

je veliki broj novinara u potrazi za poslom  na mizerne plae i
guranje etike strane profesije u zapeak  so we fed the classifier
with usto and noted the prediction  then fed it with usto
se and noted the prediction  and so on until the full sentence is
fed  the classifier prediction at different stages of the sentence
scan is plotted in fig       the top left panel of fig      shows
that the classifier for the most part thinks the document to be
actually bs  until it saw the last word of the sentence when it
switched its prediction to sr  we think this is due to the fact
that the last word associated very uniquely to sr in the training
corpus  the bottom left panel of fig      shows a similar
scenario but in this case the classifier switched back and forth
a couple of times  the confusion of the classifier is very
high in the top right panel of the figure because the particular
sentence was made of words and phrases that are common to
all three languages  we believe that the correct classification
of such documents needs creation of extra features based on
deeper understanding of this language group  another possible
scenario where any classifier can struggle is when the body
of the text contains a quotation of a different language  the
bottom right panel of fig      shows a scenario where a
document in serbian had a comment in portuguese  though
that was not the cause of the eventual classification failure 
removing quotes from a document is a potential option but
it can also have adverse effect if the quote is in the same
language as that of the main document 
vii  c onclusion and next steps
we have presented a deep neural network based language
identification scheme that achieves near perfect accuracy in
classifying dissimilar languages and about     accuracy on
highly similar languages  specifically  the languages in western slavic slavic group posed the highest challenge  and
expanding the corpus of these languages using external sources
did not help much mainly because no n grams of words that
are unique to certain languages were ingested by the expanded

we would like to thank david jurgens of department
of computer science  stanford university for helping with
the initial idea  dataset and previous research  junjie qin
for his mentoring and insightful comments that polished the
outcome of the study  aws educate program for providing
ec  credits and computing resources and microsoft azure
for research program for providing azure credits and full
featured computing resources  and lastly google  yandex and
basis tech for providing free access to their language detection
apis for our benchmarking analysis                

c omparison with other systems

we assessed the performance of slide by comparing its
result with the domain leaders in an unfair test described
below 
we queried the test file of dataset of dsl shared task     
and accepted the resulting predictions even if




the dialect of the language is not distinguished in iberoromance language group due to lack of support  i e 
google always predicts portuguese for sentences both in
brazilian portuguese and european portuguese 
a certain language is not supported at all  i e  rosette
doesnt support bosnian 

table iii shows the resulting accuracies  although slide
had lack of competitive advantage in this unfair test  it
surpassed the industry leaders in terms accuracy 
solution
slide
google translate api
rosette language api
langid py
yandex translator api

accuracy
   
   
   
   
   

table iii  benchmark results in non increasing order of
accuracy

fir eferences
    i arel  d c rose  and t p karnowski  deep machine learning   a
new frontier in artificial intelligence research  research frontier   ieee
computational intelligence magazine                  
    marcos zampieri  liling tan  and nikola ljube  overview of the dsl
shared task                    
    juliane stiller  maria gade  and vivien petras  ambiguity of queries
and the challenges for query language detection  clef      labs and
workshops notebook papers       
    dong nguyen and a seza do  word level language identification in
online multilingual communication     october               
    marco lui and timothy baldwin  langid  py  an off the shelf language
identification tool  proceedings of the acl      system demonstrations 
 july             
    aditya bhargava and grzegorz kondrak  language identification of
names with svms  computational linguistics   june               
    william b cavnar  john m trenkle  and ann arbor mi  n gram based
text categorization  in proceedings of sdair      rd annual symposium
on document analysis and information retrieval       
    erik tromp and mykola pechenizkiy  graph based n gram language
identification on short texts  proceedings of the   th annual belgiandutch conference on machine learning  pages            
    victoria bobicev  native language identification with ppm  pages    
    
     simon carter  wouter weerkamp  and manos tsagkias  microblog
language identification  overcoming the limitations of short  unedited
and idiomatic text  language resources and evaluation           
          
     shane bergsma  paul mcnamee  mossaab bagdouri  clayton fink  and
theresa wilson  language identification for creating language specific
twitter collections  proceedings of the  nd workshop on language in
social media at naacl hlt     lsm                  
     semicoast  half of messages on twitter are not in english japanese is
the second most used language  semiocast  page             
     dick sites  google compact language detector         
     ted dunning  statistical identification of language  computing   november        
     ignacio lopez moreno  javier gonzalez dominguez  oldrich plchot 
david martinez  joaquin gonzalez rodriguez  and pedro j  moreno 
automatic language identification using deep neural networks  icassp      pages                
     gregoire montavon  deep learning for spoken language identification 
nips workshop on deep learning for speech recognition and related
applications  pages          
     bing jiang  yan song  si wei  jun hua liu  ian vince mcloughlin 
and li rong dai  deep bottleneck features for spoken language
identification  plos one                      
     javier gonzalez dominguez  ignacio lopez moreno  hasim sak 
joaquin gonzalez rodriguez  and pedro j  moreno  automatic language
identification using long short term memory recurrent neural networks 
in interspeech       pages                
     lt vardial workshop 
     t sne laurens van der maaten 
     laurens van der maaten  accelerating t sne using tree based algorithms  the journal of machine learning research                 
       
     understanding lstm networks  colahs blog 
     razvan pascanu  tomas mikolov  and yoshua bengio  on the difficulty
of training recurrent neural networks  international conference on
machine learning                     
     sepp hochreiter schmidhuber and jurgen  long short term memory 
neural computation                        
     kyunghyun cho  bart van merrienboer  caglar gulcehre  fethi
bougares  holger schwenk  and yoshua bengio  learning phrase representations using rnn encoder decoder for statistical machine translation 
arxiv  pages                
     nitish srivastava  geoffrey hinton  alex krizhevsky  ilya sutskever 
and ruslan salakhutdinov  dropout  a simple way to prevent neural
networks from overfitting  the journal of machine learning research 
                       
     indicodatasolutions passage 
     google translate api   programmatic translation services google cloud
platform 

     translator api yandex technologies 
     rosette api  basis technology 

fi