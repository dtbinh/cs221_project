model reductions in quantum optical devices
edwin ng  tatsuhiro onodera  and gil tabak
edward l  ginzton laboratory  stanford university  stanford  ca        usa
 dated  december          
a major obstacle in designing quantum photonic systems is the exponential scaling in simulation
complexity as a function of network size  on the other hand  in many applications  the individual
components exhibit relatively simple  low dimensional behavior within the parameter regimes of
interest  in this project  we explore the use of machine learning to identify such structure and
develop reduced models for efficient quantum simulation  we focus on the kerr cavity device  for
which a low dimensional qubit limit exists under weak drive  using pca and manifold learning
algorithms  we show that this analytic limit can indeed also be inferred from a learning approach 

introduction

the state of any pure  finite dimensional quantum sys 
tem is described by a vector   cn   where kk      
while quantum optical systems are technically infinitedimensional  as long as the energy of the system remains bounded  we can typically choose a suitable finite n to truncate the space  usually  n      for
each component   the dynamics of the quantum system
are then generated by a  hermitian  hamiltonian matrix h  cn n along with a set of collapse operators
l    l    l            where each li  cn n  
the most basic form of the kerr cavity device features
two parameters  a nonlinearity   r  and a drive   r 
a formal  slh  description of this model can be found in
     which includes how h and l depend on the parameters  but we omit the details here  we do note  however 
that the qubit limit from     corresponds to a choice of
parameters where       
once we construct h and l from the model  we can
perform exact quantum simulations on the system  up
to truncation errors   we use the python based package qutip      which provides two different solvers for
quantum simulation  the first  quantum trajectories 
is a monte carlo solver that  given h  l  and an initial condition     generates a  stochastic  time series of
vectors                  t    where for simplicity we have
fixed some time step  and run the simulation for a
time t   if we run m quantum trajectory
simula 
tions  then we obtain s                  m     where each
 i 
 i  
 i 
 i                t   so that t is the state of the ith
trajectory at time t  quantum trajectories of this sort
comprise the training data we use for machine learning 
the other solver provided by qutip is a master equation solver  while the quantum trajectories formalism
solves the stochastic schrodinger equation  which is a
quantum stochastic differential equation       the master
equation is a corresponding partial differential equation
describing the probability distribution over the ensemble of those trajectories  that is  if the trajectories t
 as random variables  are drawn from some distribution
dt   the master equation describes

 the time evolution of
a density matrix t   ex t t  cn n   simulating
the master equation provides us with the true distribu 

tion of states in time  which we use in this project to validate the performance of our learning algorithms  illustrations of both quantum trajectories and master equation simulations are shown in figure   
thus  provided some trajectory data s  we want to find
manifolds   cn of dimension k   n to which the dynamics can be projected  while still producing states with
distribution close to dt in some appropriate sense  generally  finding  is an unsupervised learning problem  in
section iii  we use pca to identify principal subspaces 
allowing us to find linear projectors to construct a new
e and l
e for the reduced model  in section v  we use
h
manifold learning to find more general   the tradeoff 
however  is we must transform the stochastic schrodinger
equation to obtain the reduced dynamics 
furthermore  with an eye towards exploring the
physics in a more general way  we also consider in section
iv the supervised learning
problem  where we augment s

with features x   x           x m    comprised of  say 
different settings of the parameters     etc    thus learning the physical regimes  x  

   
   
photon number

i 

   
   
   
   
      

   

   

time

   

   

   

fig     visualizing a set of m      trajectories  n      
 i 
                        to visualize t  cn   we plot
 i 
 i 
 i 
the photon number nt   t diag            n  t   since
 i 
nt does not exceed    dashed line  by much  only the first
 i 
two or three components of t are important  the solid line
shows ex nt     tr t diag            n     with t obtained from
master equation simulations 

fi 
related work

to our knowledge  there are no explicitly machine
learning based methods for performing model reduction
on the stochastic schrodinger equation in particular 
there is a body of literature on model reductions of
the stochastic master equation  which is yet another
formulation of quantum dynamics useful for performing
real time coherent quantum feedback control  many of
these results tend to be analytic  for example      assumes
the quantum optical state is gaussian  which works well
in semi classical regimes of operation  but this assumption  for example  does not hold for the system we are
studying since a one photon state is non gaussian 
in a more sophisticated approach      uses a filtering
framework to constrain the evolution of the quantum
statistics  representing the state  to take place according
to a predetermined  finite dimensional family of probability densities  this latter approach is numerically efficient since the orthogonal projections can be analytically
computed using the fisher metric  however  the algorithm relies on the use of physical intuition to tailor the
underlying family of densities to the physical system at
hand  and as a result under performs in some nonlinear
problems such as absorptive bistability in cavity qed 
these limitations inspired     to use nonlinear manifold learning approaches for model reduction  in an
approach similar to that in section v  this work shows
how to transform the stochastic master equation using a
smoothed mapping into a lower dimensional manifold obtained from a learning algorithm  however  the scheme is
designed for active feedback control and hence integrates
the stochastic master equation  requiring quadratically
more memory than the stochastic schrodinger equation
we use in quantum simulations  as another key difference      is concerned with stabilizing attractors in state
space and hence works in a localized region of state space 
consequently  it does not encounter global problems like
manifolds folding on themselves  which we address in section v by constructing multiple charts 
we do note that some work        has been done on
the simulation of the stochastic schrondinger equation
as well  here  the main approach is to simulate the dynamics on a moving basis  optimizing for a coordinate
transformation which centers the state in phase space
before applying truncation  while this effectively treats
systems with large displacement  it can under perform
when there is non local entanglement in phase space  e g  
the infamous schrodinger cat state  
in this context  we expect our approach of using machine learning from the very start can in general do better
for quantum simulation applications  while making fewer
assumptions about possible system behavior  finally  we
have also not found any prior work on the use of supervised learning to explore different regimes of operation
for quantum optical devices  therefore  we believe these
techniques will prove to be a useful alternative for physicists to gain intuition about the systems under study 

   
   
   
   
   
   
   
   
   

v 

ii 

    
         
         

v 

    
         

    

    
    

v 

fig     the data in s plotted in the pca basis using k     
note that the variation in the data is much higher along v   
which shows that the pca is successful  however  we also see
that the data shows additional nonlinear structure which is
not captured by a linear model reduction algorithm like pca 

iii 

identifying principal subspaces

because of the linear structure of quantum mechanics 
a natural reduction method is to find linear subspaces of
cn in which the dynamics lie  we use this as our starting
point due to its simplicity and the fact that the qubit
limit in     identifies precisely such a subspace  because
we want an orthonormal basis for the reduced space  we
focus on principal components analysis  pca  
we run pca on an input set of m t real vectors
  i 
 
s   t      t  t and    i  m  
where we have taken the absolute value offi each compo i   fi
 i  
nent of the complex vectors  t j   fi t j fi  applying pca on s with produces n principal components
v            vn  rn   with corresponding variance  ratios 
w            wn such that w       wn   these components

t
form an orthogonal transformation v   v     vn
from the original basis to a pca basis  where the state
vectors from s written in the pca basis have components
with generally decreasing amplitude 
one way of performing a dimensional reduction using
pca is select a small set of principal components with
non negligible weight  for any k  n   we can define
a dimensional reduction corresponding to truncating the
last n  k components in the pca basis  represented by
the k  n submatrix of v  

t
p   v  v     vk  
we think of p as a projection onto a k dimensional subspace of cn   producing
 reduced model
  simulation inputs
e   p hp    l
e   p li p    li  l   and e    p    
h
then we say that the manifold found by the algorithm is
the subspace    span v         vk   

fi 
variances from running pca on s  same data as for
figure    are shown in the second column of table i  in
figure    we illustrate some principal components of the
data  in the pca basis  that is  p  for some subset of
  s   finally  figure   illustrates the master equation
simulation inaccuracies incurred by this model reduction
for the qubit limit kerr cavity given various choices of
k  we see k     already provides good approximation
of the dynamics  as expected 
if we were to run quantum trajectories using the ree l 
e and e    we would obtain
duced simulation inputs h 
a set of trajectories et that converge to some density
matrix et in expectation  we are interested in how close
p  et p  i e   in the original basis  is to the true density
matrix t   while it is possible to compute an empirical
estimate of et using reduced model quantum trajectories 
since our system is relatively simple  n      for an exact
quantum simulation  we can in fact run a reduced model
master equation simulation to find it directly and thus
quantify the performance of our algorithm 
to define the error  we introduce the fidelity measure
of quantum states      defined as
p

f        tr
         
which is well defined for density matrices because they
are positive semi definite by construction  the fidelity
is symmetric in its inputs and bounded between   and
   approaching   when  and  become indistinguishable
 i e   close in distribution   thus  we define the error as
  

t
 x
f  e
t   t    
t t  

for the data set s  the errors  k  for different choices of
k are shown in the third column of table i  again  we see
that k    is sufficient for achieving good performance 

   

k  n
k   

photon number

   

k   
k   

   
   
   
   
      

   

   

time

   

   

   

fig    
reduced model master equation simulations obtained using pca with k         and   principal components
compared to the exact case of k   n   as in figure    we visualize the state using the photon number  notice that k    
and k     are almost indistinguishable from full simulations 

index k pca variance wk fid  error  k   k 
 
 
 
 
 

         
         
         
         
     


         
         
         
     

table i  summary of results for using pca to find reduced
models for qubit limit kerr cavity 

iv 

learning on principal subspaces

we next want to learn the principal subspace itself as
a function of model parameters  to do this  we perform
supervised learning using the parameters as features and
the pca projectors as targets  this means automating
the procedure from section iii  then using a regression
algorithm to learn the reduction  from the qubit limit
results  we expect to find a low dimensional  small k 
subspace when        with k increasing with  
more formally  we have a supervised
learning

  problem where we are provided x   x              x m     where

each x i     i     i  for  i     i   r  augmented with
    
 
z   s           s  m     where each s  i  is trajectory data
for the parameter setting     i  and     i    consisting of m  t n  dimensional vectors  m   m generally  
we then compute for each s  i  a corresponding pca decomposition y  i    w i    v  i  where w i   rn are the
 i 
 i  
variances  and v  i    v            vn are the pca basis
    
 
vectors  then  x and y   y           y  m   form the
input to the supervised learning problem 
for our training data  we sample a      uniform grid
on the parameter intervals         and            
we then run m      trajectories for each parameter setting on this grid  keeping all other simulation settings the
same as before  a minor issue is that the pca vectors in
y occasionally flip due to the eigenvector subroutine 
we address this issue by tracking the angles between nearest neighboring samples and correcting the flip 
to run the regression  we use a neutral network provided by the python based theanets package      to obtain regressions on the variances w and pca basis vectors v            vn   more specifically  we train a neural network n  on the mapping x   w  followed by n neural
nets n            nn on the mappings x   v            x   vn  
respectively  the idea is to use n  to predict the variances  then select k using a threshold tolerance  once
k is selected  we then construct the projector using the
predictions of n            nk  
for n    we use a single chain of layers with   input nodes  since x  r    and    output notes  since
w  r      with          hidden layers  for each nk   we
similarly use a single chain of layers with   input notes
and    output notes  since each vk  r      but with
       hidden layers  theanets uses the sigmoid thresholding function by default on its hidden layers 

fi 

 

 

 

 



 

 

 

 

fig     training and regressed variance ratios for neural network supervised learning  for the slice of       the points
show the training variances w i  corresponding to parameters
 i    while the lines show the regressors prediction at    
points in the same interval  note that the piecewise form of
the lines come from the regressor rather than sampling 

having obtained the regression model  we can evaluate its performance by looking at the predictions made 
for example  in figure    we plot the training variances
w i   as computed by pca  alongside the regressed components of w   predicted by n    we fix a tolerance
tol         so that we drop all variances smaller than
tol w    producing a set of k top principal components 
next  from the regressors n            nk   we can obtain

t
p        v          vk        generally  p will
not be semi orthogonal  we solve this issue by performing
gram schmidt orthgonalization on the rows  finally  we
truncate h and l to obtain a reduced model described
e
e
by h  
  and l  
  with the procedure in section iii 
a heatmap showing the fidelity error of these regressed
truncated models vs the true density matrix  again  via
master equation simulations  is shown in figure    as 
increases  the neural network realizes the need to taking
increasing values of k  while k     when     as
expected from the qubit limit  the presence and shape of
these regime transitions can provide substantial insight
into the physics  e g   how  and  scale  etc   

v 

nonlinear embeddings

k   

 

 



    
    
    
    
    
    
    
    
    
    
    

 

 

k   

 

fig     fidelity error of neural network regressor for learning
pca model reduction  the performance over the fine grid
sweep            is quite smooth despite the training data
being         the discontinuities in the error correspond
precisely to jumps taken in k by the trained algorithm 

vectors only  we use the same trajectory data as in section iii  but preprocess it to obtain a set of m t real
 n  dimensional vectors x     re   im       s  
generally  we found ltsa produces results that best
preserved the visual structure of the data  so we focus
on this algorithm here  briefly  ltsa computes local
tangent spaces of dimension k to the data and tries to
align neighboring tangent spaces as it proceeds  thus producing a  sampled  coordinate parameterization of the
k dimensional manifold underlying the data 
as shown in figure    the manifold obtained by ltsa
appears to be   dimensional but folds over itself  suggesting the need for multiple coordinate charts to capture the
structure  we introduce such coordinate charts g         gc  
with each gj a mapping sj  rk   where sj  r n
such that j sj   r n   for this particular training set 
we define sj    x   xj       sc   j    x   xj    for
j              c    using         we illustrate two charts in
figure   
chart   only
chart   only
both charts
neither chart
    
    

u 

despite the utility of pca in identifying linear subspaces for model reduction  quantum dynamics tend to
lie on nonlinear manifolds in general  e g   see figure    
so linear reductions can fail to both find optimal values
for k and preserve low error rates  for this reason  we
next consider nonlinear approaches for model reduction 
here  the goal is to find a smooth map from cn to a
parameterized k dimensional manifold  on which the
dynamics lie  simulation of the system on  is then expected to be more efficient than in cn  
in this project  we explored different methods of nonlinear manifold learning such as variations of local linear embedding  lle  and local tangent space alignment
 ltsa   since manifold learning algorithms work on real

  
   k   
  
  
 
 
 
 
    

fidelity error

   
    
    
    
    
    
    
    
    



variance

 

    
   

   

u     

   
   

u 

   

fig     ltsa output points for k      a two dimensional
surface is discernible  but the topology is different from r   
multiple coordinate charts are needed  we illustrate two such
charts here  points belonging to only either chart are in blue
or yellow  points belonging to both in green 

fiu

    

          

    

u

          

    

u 

    
    
    

error

     
     

order  
order  
order  
order  

x    

     
      

   

           

data index

             
data index

normalized l  error

 
    

greedy coefficient dropping
dropping smallest coefficients

    
    

 

  

   

   

number of coefficients remaining

   

fig     normalized l  error between ltsa coordinates for
x and g  evaluated on x 

now we can perform model reduction by applying the
itos lemma to find the evolution of the coordinates  for
each chart gj   we have a set of coupled reduced sdes
 superscripts denote components 
duk   dg k  u    
ej  u  dt   
ej  u  dw 

fig     top  comparison of ltsa coordinates for x  blue 
with g   left  and g   f   g   right  evaluated on x  for order
  polynomials  bottom  sorted l  errors of above points for
polynomials of different orders  note the errors on the right
are multiplied by     

in addition to the charts gj   we also require a corresponding inverse fj   such that fj  gj  x     x for all
x  sj   now  ltsa only gives numerical values of gj
sampled x  in order to obtain approximate charts gj
and their inverses fj   then  we need to solve a regression
problem  from a number of regression algorithms tried 
the best seems to be ridge  i e   regularized l    regression  for simplicity  we use low order polynomials for
fitting both gi and fi   cross validation results  using a
        split  are shown in figure   
ideally  the maps fj and gj should have as few parameters as possible  i e   perform feature selection   from
the regression above  we find that the vast majority of coefficients are small            removing these coefficients
leave approximately     significant ones  to see how removing more points would affect the regressed maps  we
show in figure   the errors incurred on the training data
for various procedures for feature selection 
a key difficulty incurred by using nonlinear manifolds
for model reduction is that we need to perform a corresponding nonlinear transformation on the stochastic
schrodinger equation in order to perform quantum simulations  briefly  this sde is given in general by
d      dt      d 
where d is a complex vector valued
wiener noise pro

p
cess       ih     li l li li    i li    i  i  
and ij       lj   j  i   here   i     li   in our formulation in terms of real vectors  we can simply double
the dimensionality and can easily derive a similar realvalued sde with corresponding terms   and      taking
   w for a real vector valued wiener noise process 

where by itos lemma      
i


  h t

ej    gjk  t    fj   tr     hgjk       fj
 


k t  

ej    gj     fj
where  is the gradient and h is the hessian  since this
results in a standard real valued sde  we can simulate
this system to obtain the reduced dynamics 
one remaining subtlety is that we need to know which
chart to use for each integration step  since we need to
pick the right set of sdes to use  suppose we have a
point ut on chart jt   and we wish to compute ut     one
possible method for selecting the chart is to maximize
the minimum distance from all points in that chart 


jt     arg max min ks  fjt  ut  k  
j

ssj

this concludes a possible algorithm for performing model
reduction on nonlinear manifolds 
vi 

conclusion

in this project  we found wide applicability of machine
learning methods for model reduction in quantum optical
simulations  using pca  we implemented linear truncations into principal subspaces  which performed well
as measured by low fidelity error  subsequently  we extended this result into a supervised learning problem 
allowing us to systematically identify different regimes
of physics  the kerr qubit limit  etc    finally  we explored reductions using nonlinear manifolds with multiple charts to perform model reduction for the stochastic
schrodinger equation  we derived the transformed equations of motion and devised a prescription for moving between charts  looking ahead  we would like to implement
this procedure in the near future  verify its accuracy in
simulation  and apply the supervised learning approach
to nonlinear reductions as well 

fi 

    see the ipython notebook http   web stanford edu 
 edwin   cs    project kerr qutip html 
    h  mabuchi  qubit limit of cavity nonlinear optics 
phys  rev  a                   doi         physreva 
         
    documentation available at http   qutip org docs   
    index html 
    d a  steck  k  jacobs  h  mabuchi  s  habib  and
t  bhattacharya  feedback cooling of atomic motion
in cavity qed  phys  rev  a                    doi 
        physreva          
    r  van handel and h  mabuchi  quantum projection
filter for a highly nonlinear model in cavity qed 
j  opt  b   s   s            doi                   
        

    a e b  nielsen  a s  hopkins  and h  mabuchi  quantum filter reduction for measurement feedback control
via unsupervised manifold learning  new j  phys    
               doi                               
    n  tezak  n h  amini  and h  mabuchi  quantum information geometry and localized quantum dynamics
 manuscript under preparation  
    t  steimle  g  alber  and i c  percival  mixed classicalquantal representation for open quantum systems
j  phys  a     l            doi                   
         
    m a  nielsen and i l  chuang  quantum computation
and quantum information  cambridge university press 
       sec        
     documentation
available
at
http   theanets 
readthedocs org en stable  
     b  ksendal  stochastic differential equations  springer
berlin heidelberg        

fi