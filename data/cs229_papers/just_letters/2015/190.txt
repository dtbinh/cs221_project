kernel learning framework for cancer subtype analysis
with multi omics data integration
william bradbury
wbradbur stanford

thomas lau
thomklau stanford
december         

abstract
recent advances in multiple kernel learning
 mkl  and unsupervised clustering methods have
each enabled large scale analysis of integrated multiomics data for cancer subtyping  however  efforts
to combine the advantages of a kernel approach with
the flexibility of unsupervised methods have not progressed due to the underconstrained nature of the
problem  in this paper  we present a novel approach to
solving this problem based on methods for unsupervised multiple kernel learning  umkl   we present
the possibility of constraining the problem with available data on clinically determined subtypes for cancer
patients  we successfully demonstrate that modifying
the constraints of the umkl problem to include conditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer subtype analysis by reducing it to an efficient alternating
quadratic optimization problem 

   introduction
the expansion and increased availability of largescale biomedical data in the past decade has led to the
rising use of machine learning as an integral component for providing biological insights   especially in
the domain of cancer research  recently  with the increasing number of large scale genomic projects  such
as the cancer genome atlas  tcga   the amount of
openly accessible patient data is greater than ever  in
order to make sense of this ever increasing quantity of
data  numerous statistical approaches have been developed to analyze tcga and other similar datasets 

shivaal roy
shivaal stanford

cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practitioners to design more finely tuned cancer therapies 
intuitively  the more precisely we are able to classify types of cancer  the more effective the treatments
we design can be  futhermore  cancer subtyping has
the potential to more accurately predict treatment outcomes and survival rates based on rates of cancer progression and other clinical factors  currently  doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their cancer  however  these subtype classifications are slow
to perform  often very broad  luminal a vs luminal
b for brca   and have weak prognostic ability  since
subtyping by physicians can only be refined as the cancer progresses  recent advances in statistical analyses
suggest there is much potential to improve cancer subtyping      
previous approaches      suggest the potential for a
umkl solution which relies on sparse clinical data
to analyze multi omic data in the search of more specific and useful subtype characterizations   thereby
empowering doctors to determine optimal methods of
treatment  we propose a method for such an approach  combining the techniques of previous umkl
approaches with novel constraints based on clinical
data 
tcga contains information about methylation 
copynumber variation  cnv   microrna  mrna 
rppa  and other factors relevant to the onset and progression of cancer  the interaction between these levels of genetic control is referred to as multi omics 
intuitively  by looking at the genomic causes of cancer  we can more accurately predict future outcomes as opposed to clinically looking at phenotypic cancer

fiily separated by an svm  we write this as

progression 
our goal in this paper is to extend the existing unsupervised multiple kernel learning technique in zhuang
et al  to leverage sparse labeling for specific use with
clinical and multi omic data from tcga  we wish to
demonstrate that such an extension is still a tractable
optimization problem 

kconv  

m
x

t kt

t  

where t is the weight assigned to each kernel  in
mkl  we try to learn the kernel combination that linearly separates the data best 

     unsupervised multiple kernel learning

   related work

unsupervised multiple kernel learning also implements a linear combination of kernels to create a distance metric in a higher dimensional space  the distance metric is used to determine groupings using a
k means or alternate clustering algorithm  

this paper builds off of zhuang et al   which proposes a method for umkl that also integrates casespecific constraint functions  this paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well  the algorithm presented in this paper  however 
does not perform well as the number of features increases or as the number of sample points decreases  

     sparse labels
we take advantage of various labels  although
sparse  to constrain our data and aid in the clustering
process  we incorporate the clinical data mentioned
earlier into our cost function to have the resultant combination of kernels reflect this added restriction  we
also impose domain knowledge of the separation between cancer and non cancer patients  this data can
be included in our model as further priors on the subtypes 

supervised multiple kernel learning has been implemented in daemen et al  to classify rectal cancer microarray data  the resulting support vector machine
accurately classified different outcomes  often reaching above       the model also performed better when
using more than one genome wide data set  suggesting
that integrating multiple genome wide data sources allows models to reach higher accuracy  

   optimization problem

the icluster algorithm developed by shen et al 
was used to accurately group cancer outcomes based
on multi omic considerations found in tcga data on
breast cancer  the algorithm clusters different cancer subtypes using multiple genomic features such
as dna copy number changes and gene expression 
icluster  however  does not take advantage of any kernel methods  

     cluster label alignment metric
we design an optimization problem to produce a
kernel and clustering which together yield the best assignment of patients to subtypes  in order to do this 
we employ the cost functions found in zhuang et al 
as well as two new cost functions 

we build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a relatively small number of samples 

     multiple kernel learning

 a good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping  in other words for each
xi we expect that the optimal kernel minimizes
p
xj li kii   kij   kjj   where ci is the set
of all samples with the same known label as xi  
kij   k xi   xj    ci    for xi that do not have
known labels 

multiple kernel learning is the use of a linear
combination of kernels to map points to a higherdimensional feature space where they can be more eas 

 a good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping  when this is the case  we expect

   background

 

fip
the kernel to maximize xj li  j kii  kij  kjj  
where here ci  j is the set of sample points labeled with a different subtype than xi  

     a simpler formulation
instead of optimizing the above function with respect to   k and bi we instead formulate the problem as one of optimizing the cost function with respect
to  and d  this allows us to eventually present the
problem as one of solving a quadratic program on the
vector  and later d 
as in zhuang et al  we define matrices d  s 
      nn where each element is given as

each of these cost functions is designed to affect the
optimization problem in such a way as to yield a kernel which places co labeled points together and differently labeled points apart  these heuristics guide the
search for an optimal kernel based on the prior sparse
clinical information by encouraging the eventual adoption of a kernel which conforms to the clinical data as
well as possible  such a kernel will yield a clustering
which places co labled points in the same or nearby
clusters  while placing differently labeled points in different clusters if possible 

 d ij     xj  bi  
 s ij     xj  li  
 q ij     xj  li  j  
 m ij   xti xi   xtj xj   xti xj

     cost function

so that we can write 

in order to obtain an overall cost function for the
optimization problem  we combine these cluster label
alignment metrics with those found in zhuang et al  to
form the overall optimization problem 

min

b kkconv

   

min
 d

    tr k  d  m   t       kdk   


  tr  k  i    t     k      t   k  i t


 

n 
x
x


 
xi 
kij xj 


 

i   
xj bj
n x
x

   s     q      t  
s t   ij  

   
   

x

t       
    
d        nn

 kii   kij   kjj  

i   xj li

  

n
x
x

the function given is not quadratic and also not guaranteed to be convex  because of this  there are no
off the shelf tools which are able to generally solve
this problem  this difficulty motivates our work to reformulate the problem as an alternating optimization
problem in the next section 

 kii   kij   kjj  

i   xj li  j

where
b is a clustering assignment 
m
m
x
x
kconv    k      
t kt       
t      t     
t  

t k t  xi   xj   

   i  j  n 

 bi  

i
n
x

m
x
t  

kij kxi  xj k 

i   xj bi

x

 
kx i  k  d k f
 

     alternating optimization algorithm
in order to formulate the problem in such a way
that it can be approximately solved  we break the
problem into two components   consider the space
parametrized by  and d  we can reduce the above
problem to one of coordinate descent by alternatively
optimizing  and d while holding the other constant 
if we can show that each of these individual problems

t  

i control constraint trade offs  and
l is given in     
this cost function is notin its present formfeasible
to optimize given the size of the data 
 

fi   discussion

is itself quadratic and therefore tractable  we will have
broken the problem down into a tractable approximation 
     

in deriving a quadratic problem from our initial
complex constrained optimization problem  we have
shown that our new approach for incorporating partially labeled data is a solvable problem  here  we explain the underpinnings of the initial constraints and
derivation of its quadratic form 

solving  by fixing d

we accomplish this separation by first optimizing 
while holding d constant and ignoring the otherwise
difficult constraints on d  fixing d we try to solve for
 by minimizing the cost function 
 t
m x
n
x
j     t
t i tt i  di dti  p
   zt  

     underlying motivations
the original umkl algorithm presents two constraints on the behavior of the kernels  the first was
that of a standard clustering optimization problem 
minimize the sum of the distances between samples in
the same cluster  in this case  however  the distances
were computed by the kernel function being chosen 
thus  a good choice of kernel function would place
points which end up in the same cluster together in the
feature mapping  this is useful for learning an optimal
kernel on a large quantity of training data  only to apply it using something other than a clustering method 
zhuang et al  demonstrates this approach by learning
an optimal kernel for a data set  and then applying that
kernel to do classification with an svm 
the second constraint on the behavior of the kernels
was that of continuity with the given sample points 
this constraint enforces  as far as possible  that the
kernel should have a limited impact on the geometry of
the data  points should not be mapped so that they are
distant from their neighbors in the original space  this
constraint ensures that the kernel does not map points
in an ad hoc manner such that essentially meaningless 
but perfect  clusters form in the feature mapping space 

t   i  

where
 z t  

n
x

    vi  di   pi  di

i  

     ei

x

sij   ei

x

j

j

x

qij   ei

x

    ei

j

sji   si  
qji   qi   t t i

j

t

p x x
t i    k t  xi   x          k t  xi   xn   t
pi is the i th column of p
vi is the i th column of m
si is the i th column of s
qi is the i th column of q
most importantly  this cost function is a quadratic program and so is tractable  even for large x 
     

solving d by fixing 

     our contributions

because we introduced new constraints on the kernel 
but not the cluster matrix d  the optimization step for
d is unchanged from its presentation in zhuang et al 
and so we simply give the result for each column of d 

j d    dt t  p d         v     p t d 

building on this last constraint  this paper introduced two new constraints  the first specified that
points which are given the same labels in the cluster
priors are placed as close to possible in the feature
mapping  combined with the previous constraint  this
ensures the learned kernel will highlight the underlying geometry in such a way as points with the same label are close together  but it will not mangle the space
unnecessarily  this forces the kernel to learn whatever is important to identifying each of the prior clusters 
in a similar vein  the second new constraint requires

which is of the form
j d    dt wd   ct d
and so is also tractable with a qp convex optimization
by iterating over each sample point and using this minimization problem to find the optimal set of neighbors 
 

fiify whether two elements have the same or different
initial clusterings  columns of these matrices can then
be used as masks for selecting only kernel elements
that refer to the distances between samples of the same
lable or of different labels  these correspond to each
of the two constraints 

that points with different prior clusters are placed as
far apart as possible  by the same mechanism as the
previous constraint  this forces the kernel optimization
algorithm to learn a kernel that highlights the features
which distinguish clusters 
both of these new constraints operate on the principle that the goal of a kernel in a situation in which the
sample data is highly dimensional is to highlight the
features of that data which are most important  we designed the optimization problem such that the optimal
kernel does just this 

     implications
we believe that the work presented in this paper
will be beneficial to the bio computation community
by providing an additional tool for making sense of
tcga and other large scale data sets  in particular  we think that applications such as cancer subtype
analysiswhich initially motivated this investigation
are particularly suited due to the the small number of
patients  expansive feature space  complex geometry 
and existence of sparsely labeled subtype data 

     deriving quadratic form
injecting these additional constraints into the problem would help to incorporate sparse training data 
but only if the new problem was solvable  in order to demonstrate that the new problem is solvable
we showed that it could be reduced to an alternating
optimization problem  each step of which was itself
quadratic  there are numerous know methods for efficiently solving quadratic programming problems  an
implementation of this algorithm would simply have to
make use of an available convex optimization toolkit 
this drastically reduces the complexity of the problem
from a general mixed integer problem to a quadratic
programming problem 
in order to demonstrate that the problem could be
reduced to two quadratic programming problems  we
primarily focused on the optimization of  while holding the clusters  represented in d  constant  this allowed us to forgo all the constraints on d  and thus
simplified the problem immensely 
we know that the the kernel matrix k is given by a
summation over the tensor t i j in the dimension indexed by t  weighted by   thus 
k 

m
x

   acknowledgements
we thank prof  olivier gevaert for his help in formulating and guiding this project and for his help in
understanding and navigating mkl and tcga  we
also thank prof  serafim batzoglou for his initial suggestions which led us to cancer subtype analysis 

t t  

t  

thinking about k in this fashion allows for interpreting the additional constraints as selection and scaling operations on columns of   we think of the problem as one of determining which kernel elements to
include and which to avoid for each xi and for each
t   for each frame of the tensor   which elements
must be summed and with which coefficients 
in order to accomplish this filtering step  we translate the initial cluster priors into matrices which spec 

fireferences
   

anneleen daemen et al  a kernel based integration of genome wide data for clinical decision
support  in  genome medicine             p     

   

anneleen daemen et al  improved microarraybased decision support with graph encoded interactome data  in  plos one             e      

   

grg lanckriet and nello cristianini  learning the kernel matrix with semidefinite programming  in  journal of machine learning research           pp       

   

r  shen  a  b  olshen  and m  ladanyi  integrative clustering of multiple genomic data types
using a joint latent variable model with application to breast and lung cancer subtype analysis 
in  bioinformatics               pp           

   

ronglai shen et al  integrative subtype discovery in glioblastoma using icluster  in  plos
one             e      

   

emily a  vucic et al  translating cancer omics
to improved outcomes  in  genome research
             pp         

   

jinfeng zhuang et al  unsupervised multiple
kernel learning  in  proceedings of the third
asian conference on machine learning   
        pp         

 

fi