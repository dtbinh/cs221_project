behind the tv shows 
top rated series characterization and audience rating prediction
team      yushu chai  yiwen xu  zihui liu

   introduction
the television production industry has maintained its vitality over the past few years  tv broadcasters make profits by
selling time to advertising agencies based on the projected audience sizes  failure to predict a tv series popularity to
audience could result in substantial losses to either broadcasters or advertisers  therefore  both tv broadcasters and
advertisers are under pressure to make wise investment decisions about tv series prior to their release  although a large
number of research papers are focused on exploring the factors affecting movie ratings  very few studies have looked
into tv series  while movies and tv series have similarities  tv series are different from movies in many aspects that
worth investigating  the internet movie database  imdb  is a comprehensive online database that has a high degree of
interactions with users  making it a fertile source of machine learning problems      in this project  our team endeavor
to build several supervised learning models to predict the popularity of tv series using viewer ratings on imdb as an
indicator  and then used unsupervised learning to investigate the key features of the tv series  these key features
obtained by unsupervised learning steps were then be used to improve our current prediction models 

   related work
previous relevant research is mostly focused on the prediction of movie ratings and avenues  given the similarity
between movie and tv series  we employed some of the machine learning algorithms from these studies and made
adjustments based on the unique problems we explored  there are two major approaches in movie ratings and revenue
prediction  one utilizes sentiment analysis  and the other runs linear regression and k means clustering on the database 
in the first approach  some studies           applied sentiment analysis and classification using the text data posted by
viewers  comments on social media are information that directly shows viewers preferences  these algorithms could
do a better job than linear regression  however  pre processing the data is quite time consuming  according to these
studies  the authors spent a great deal of time correcting grammar errors and formatting the texts  in the second
approach  apala     and joshi     used k means clustering and regression models  although both algorithms are easy to
implement and interpret  the error rate could be relative high  with r  around      

   dataset and features
the tv series data recorded on imdb websites were extracted and provided by andrej krevl from the stanford snap
research group  we filtered and cleaned the data by excluding the tv series with very small numbers of ratings  say
less than    viewer ratings  and those with missing feature information for the accuracy of prediction  features of the
data set include the genre  vector of booleans   release year  numeric   number of seasons  numeric   number of
episodes for each season  numeric   number of ratings  numeric   number of critics  numeric   number of reviewers
 numeric   runtime  numeric   aspect ratio  categorical   and color  boolean   we randomly selected     complete
samples from the data set for testing and training purposes 

   methods
to predict the tv show ratings  our first intuition was to use regression models  the very basic model is thus the
multiple linear regression  with all features  whether numeric or categorical  included in the hypothesis  to improve the
regression model  we have also fitted a locally weighted linear model and a reduced linear model by backward search
and principal component analysis  another idea is running a classification model  where we segmented the ratings into
several groups and utilized logistic regression for classification  the segmentation was then revised by k means
clustering 

  

 

fi    supervised learning
       model i  multiple linear regression
the hypothesis of the linear regression used here is
                                       

 
     

   

where   are the coefficients that we are seeking  and   is the output used to calculate mse  we used stochastic
gradient descent to minimize the cost function and obtained the   s 

       model ii  locally weighted linear regression
to improve the accuracy of the prediction model and make the choice of features less influential  we performed a
locally weighted linear regression model to minimize
 
     

      

 
 
             

where     exp   

       a
 ba

  

the   s are non negative valued weights  a large   means that  will be picked to make                      small 
the bandwidth parameter   which controls how quickly the weight falls off with distance of its   from   was chosen
to minimize the test error 
the locally weighted linear regression is a non parametric method which  unlike the unweighted linear regression 
cannot reduce the training set as it proceeds  because every time a query is obtained at a new location  the entire training
set is needed to determine what the local neighborhood is 

       model iii  linear regression with selected features and interaction effects between original features
we suspected that some of the features in the original linear model were irrelevant to the learning task  and overfitting
might thus increase the estimate of generalization error  therefore  as another way of improving the linear model and
reduce the dimension of the feature matrix  backward search was performed starting with the full sets including all
possible interaction terms  for each iteration of the backward search  we did cross validation to evaluate the model 
where the measurement is the akaike information criterion  aic   which is
          log   
where  is the number of estimated features in this model  and  is the maximum of the likelihood function  at each
step  we deleted a feature whose elimination gave the greatest reduction in aic value 

       model iv  classification using logistic regression
by using logistic regression  we predicted the probability of the rating being     the highest possible rating on imdb  
and label this observation as fair  rating below     good  rating between   and     and very popular  rating above    
the cost function of logistic regression is
      

 
      o

p 

     further improvement i  principal component analysis  pca 
pca projects the normalized data onto a space where the decrease of variance is the maximum  for the input   we
would like to find a unit length vector  to maximize 
r  

 
  r  
    

the t s are the principle components  pc   we then used the pcs to revise our linear regression 

  

 

fi     further improvement ii  k means clustering
to find a better and more reasonable segmentation of the ratings  we performed k means clustering on the training set
with k         and    after randomly initializing the k centroids randomly  the k means clustering algorithm iterates on
assigning each point to the closest centroid and letting the new centroid be the mean of the points in this cluster  until
there is a convergence  we utilized the clustering result for the classification model 

   results and discussion
to predict the ratings and evaluation our regression models  we used   fold cross validation  we divided the entire
dataset in   equal length subsets  each time we held one of the five as the test set  and trained on the other four as a
whole  for each training set  we built a multiple linear regression model  and made predictions on the test set to obtain
the mean squared error  mse  
  

 

 
      

     

         

where m is the number of observations in each training set  and n is the total number of predictors in the regression
model    is the ratings of the test data      is the predicted rating  i e   output value  the average of the mse for each
step of the   fold cross validation is our estimate of the generalization model 
the logistic regression model is evaluated by the hold out cross validation approach with  of the dataset for training 
and another  for test  the misclassification rate is used as a measurement 
       

       
       

the full linear regression model gave an error estimate of         it is still necessary to diagnose the model 

figure    residual vs  leverage 

figure    residual vs  fitted value 

while figure   shows that there are only two points of high leverage  which is acceptable  the pattern in the figure  
shows that the assumption of constant variance of error terms is not perfectly satisfied  so it is necessary to improve the
linear model by locally weighted regression 
the estimate of generalization error given by the locally weighted regression is         which is slightly lower than the
error of the linear model  this means that the problem of non constant variance is  to some extent  addressed by this
approach  but since the problem of the variance of error terms is not significant  as shown by figure    this locally
weighted regression model does not significantly improve the linear model 
considering the tedious input features    used in our previous two models  we attempted to perform dimension
reduction by backward search feature selection  using pca as a comparison  the following plot  figure    keeps track
of the aics of the process of backward search  the model at the last step gives the lowest aic  and also the most
reduction in estimated generalization error  i e          

  

 

fifigure    feature selection process   

an alternative method we used for dimension reduction is pca  the plots  figure    are projections of the data onto
the first three principle components  i e   the scores for the first three principal components  the observations of
different subgroups lie near each other in the low dimensional space 

figure      d pc projection 

we then used the first k  k               pcs to fit linear models  and the estimated generalization error of different
number of pcs is plotted below in figure   

figure    estimated errors at different   pcs 

when k      the lowest estimated error was         which was lower than the full linear model  and close to the error
by the feature selection method 
table    comparison between regression models

  

linear regression models

features included 

linear regression
locally weighted linear regression
linear regression with selected features
and interaction effects between original
features
linear regression using the first six pcs

all
all
without run time  aspect
ratio and some of the genre
features    
the first six pcs
 

estimate of
generalization error
      
      
      
      

fialthough the methods above reduce the estimated error of the original linear regression model  the improvement is not
very significant  thus  we also considered classification model  which gave a rough range into which the predicted rating
may fall  and could be easier to interpret and use by tv show producers or investors 
based on the current dataset  we segment the ratings into several categories  fair  rating below     good        and very
popular         to fit a classification model using l  regularized logistic regression  when predicting the ratings of an
upcoming tv show  this classification error can give a category as reference for the tv show producers 
table    confusion matrix for classification model

original
rating

class

counts

  misclassified
from to other
groups

total  
misclassified

test
misclassification
rate

  

  correctly
assigned
to this
group
  

  

fair

  

  

    

   
    

good
very popular

   
  

   
 

  
 

the confusion matrix above gives very good misclassification rate  and since the classification model is more
straightforward and makes more practical sense  we recommend that it is used if tv show producers and investors
would like to predict their products popularity 
furthermore  since the segmentation in the original logistic regression model is based on our intuition  we also use the
k means clustering result  k         and    to divide them into subgroups  figure   is an example of k     

figure    k means clustering when k     

for each value of k  we calculate the lowest and the highest ratings for the k clusters  and use them as the cutoffs of
the classes in the logistic regression  when k      the test misclassification rate is the lowest  which is about       the
ranges are  below                   and above       as each range represents the lowest and highest ratings of that
cluster  however  the logistic regression model after k means clustering does not give a better misclassification rate 

   conclusions and future work
this project mainly utilized regression models to predict viewers ratings of tv series based on the existing imdb
database  in particular  the classification model with ratings divided into three subgroups provides the best outcome and
is recommended for prediction  although the outcome falls in a relatively wide range  nevertheless  linear regression
using selected features  either by using backward search or pca  provides improved results compared to linear
regression with all available features  if given more time  we will try using more sophisticated models to run on the data 
for example  neuro network and kernel  we will also analyze the tv series ratings based on different time spans to see
how the importance of features changes over time 

  

 

fi   references
    apala  krushikanth r   et al   prediction of movies box office performance using social media   advances in social
networks analysis and mining  asonam        ieee acm international conference on  ieee       
    augustine  achal  and manas pathak  user rating prediction for movies  technical report  university of texas at austin 
     
    brendan oconnor  et al  from tweets to polls  linking text sentiment to public opinion time series 
proceedings of the international aaai conference on weblogs and social media      
    jain  vasu   prediction of movie success using sentiment analysis of tweets   the international journal of soft
computing and software engineering                     
    m  joshi  d  das  k  gimpel  and n  a  smith  movie reviews and revenues  an experiment in text regression 
naacl hlt       
    oghina  andrei  et al   predicting imdb movie ratings using social media   advances in information retrieval  springer
berlin heidelberg                
    the internet movie database  retrieved from  http   imdb com

  

 

fi