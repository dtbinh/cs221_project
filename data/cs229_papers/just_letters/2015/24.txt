crime prediction and classification in san francisco city
addarsh chandrasekar  abhilash sunder raj and poorna kumar

abstract to be better prepared to respond to criminal activity 
it is important to understand patterns in crime  in our project  we
analyze crime data from the city of san francisco  drawn from
a publicly available dataset  at the outset  the task is to predict
which category of crime is most likely to occur given a time and
place in san francisco  to overcome the limitations imposed by our
limited set of features  we enrich our data by adding information
from the united states census to it  we also attempt to make our
classification task more meaningful by merging multiple classes
into larger classes  finally  we report and reflect on our results
with different classifiers  and dwell on avenues for future work 

i  i ntroduction
many important questions in public safety and protection
relate to crime  and a better understanding of crime is beneficial
in multiple ways  it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime  and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments  with the advent of the big data
era and the availability of fast  efficient algorithms for data
analysis  understanding patterns in crime from data is an active
and growing field of research 
in our project  we use spatio temporal and demographic
data to predict which category of crime is most likely to
have occurred  given a time  place and the demographics of
the place  the inputs to our algorithms are time  hour  day 
month  year   place  latitude  longitude  and police district  
and demographic data  population  median income  minority
population  and number of families  which we get from the
united states census   the output is the category of crime that
is likely to have occurred  we try out multiple classification
algorithms  such as naive bayes  support vector machines 
gradient boosted decision trees  and random forests  we also
perform multiple classification tasks  we first try to predict
which of    classes of crimes are likely to have occurred  and
later try to differentiate between blue  and white collar crimes 
as well as violent and non violent crimes 

they also noted significant correlations in crime across weekly
time scales 
towards the second objective of understanding patterns of
criminal behaviour  significant contributions have been made
by tong wang et al in      in finding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes  their approach was to
identify a common modus operandi across crimes  which could
then be linked to groups or individuals who might commit the
crime  for this  the authors proposed a new machine learning
method called series finder  which was trained to recognize
patterns in housebreak incidents in cambridge  massachusetts 
our approach shares certain similarities with some of the
work described above  in that we use spatio temporal and
demographic information to discover which types of crimes are
likely to have occurred  however  we are notably different in
that  given the data  we seek to predict which category of crime
is most likely to occur  and we are hence concerned principally
with understanding the differences between different types of
crime  which is relatively unexplored territory 
iii  o ur dataset
our dataset is a publicly available dataset that we obtained
from kaggle  which has information about         crimes
that took place in san francisco city over a span of nearly
twelve years  each crime is labeled as belonging to one of   
categories 
a  features
every entry in our training data set is about a particular crime 
and contains the following information 



ii  r elated w ork



much of the current work is focused in two major directions 
 i  predicting surges and hotspots of crime  and  ii  understanding patterns of criminal behavior that could help in solving
criminal investigations 
important contributions towards the former include     by
bogomolov et al  who try to predict whether any particular area
in london will be a crime hotspot or not  using anonymized
behavioural data from mobile networks as well as demographic
data  in      chung hsien yu et al use classification techniques
to classify neighbourhoods in a city as hotspots of residential
burglary  using a variety of classification algorithms such as
support vector machines  naive bayes  and neural networks 
 more work on the usefulness of support vector machines for
hotspot detection can be found in       toole et al demonstrated
in      by analyzing crime records for the city of philadelphia 
that significant spatio temporal correlations exist in crime data 
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces 








date and timestamp of the incident 
day of the week that the crime occurred 
name of the police department district 
address  the approximate street address of the crime
incident 
latitude 
longitude 
category  category of the crime incident  this is the target
variable 
description  a brief note describing any pertinent details of
the crime   this was not used as a feature in our classifiers  
resolution  whether the crime was resolved  with the
perpetrator being  say  arrested or booked  or not   this
was also not used as a feature in our classifiers  
table i  some sample rows from our dataset

dates
          
          
          
          

     
     
     
     

category
warrants
other offenses
other offenses
larceny

descript
warrants
traffic violation
traffic violation
grand theft

dayofweek
wednesday
wednesday
wednesday
wednesday

pddistrict
northern
northern
northern
northern

firesolution
arrest  booked
arrest  booked
arrest  booked
none

address
oak st   laguna st
oak st   laguna st
vanness av   greenwich st
     block of lombard st

x
            
            
           
            

y
          
          
           
           

b  preprocessing
before implementing machine learning algorithms on our
data  we went through a series of preprocessing steps with our
classification task in mind  these included 
 dropping features such as resolution  description and
address  the resolution and description of a crime are
only known once the crime has occurred  and have limited
significance in a practical  real world scenario where one is
trying to predict what kind of crime has occurred  and so 
these were omitted  the address was dropped because we
had information about the latitude and longitude  and  in
that context  the address did not add much marginal value 
 the days of the week  police categories and crime categories were indexed and replaced by numbers 
 the timestamp contained the year  date and time of occurrence of each crime  this was decomposed into five
features  year              month         date        
hour        and minute        
following these preprocessing steps  we ran some out of thebox learning algorithms as a part of our initial exploratory steps 
our new feature set consisted of   features  all of which were
now numeric in nature 
c  feature enrichment
as we plunged into solving our classification problem  we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime  in order to improve
our feature set  we augmented our dataset with additional
features that we scraped from the united states census data 
this included demographic data such as mean income level of
a neighbourhood  racial diversity and so on  we felt that the
addition of such information could improve our performance at
the task of crime prediction  the census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset  and increased our number of features to
   
d  collapsing crime categories
we also felt that the number of output classification labels  i e 
    in the original dataset was too high for accurate prediction 
the labels were too fine grained  and we realized that several
of these crime categories were similar to one another  and could
therefore be collapsed into smaller classes for better prediction 
further  such collapsing could be done in several ways  two
specific ways we went ahead with this were 
 blue collar crimes vs white collar crimes  blue collar
crimes included crimes such as larceny  arson and burglary while white collar crimes included crimes such as
fraud  forgery and extortion 
 violent vs non violent crimes  violent crimes included
crimes such as assault  arson and prostitution while nonviolent crimes included crimes such as traffic violations
and trespassing 
iv  m ethods
after the preprocessing described in the previous sections 
we had three different classifications problems to solve  which
we proceeded to attack with an assortment of classification

table ii  sample rows from the census data 
note  the column headers are as follows  tc  tract code 
til  tract income level  tmfi  tract median family income 
mfi  median family income  oou  owner occupied units 
fu  family units  min   minority
tc
      
      
      

til
moderate
upper
upper

     mfi
       
         
         

tract pop 
    
    
    

distressed
no
no
no

tmfi  
     
      
      

tract min   
     
     
     

mfi
       
       
       

min  pop 
    
   
    

oou
   
   
   

     tmfi
       
         
         
   to    fu
   
   
    

algorithms  the following sections explain the models we used
in detail 
a  naive bayes
as part of our initial exploratory analysis  we implemented
a naive bayes classifier based on a multi variate event model
with laplace smoothing  this is a multi class classification
problem  the target variable y  crime category  can be one of
   classes  represented by numbers from   to     therefore  y
was modeled as a multinomial distribution 
y                    

   

y  y  multinomial 

   

the latitude and longitude data were not used for classification  and all the remaining features are categorical variables 
thus  our feature vector  x  is a   dimensional vector  each
of the features takes a range of values  concretely  month
                     day of week                     and so on 
therefore  each feature is modeled by a multinomial distribution 
xi                 ki  
xi   y   j   i y j  multinomial 

   
   

assuming that there are m training examples  the parameters  y   i y j   are estimated using the following  laplacesmoothed  equations 
pm
y  j    p  y   j   

i  

  y  i    j     
m     

j y l  k    p  xj   k y   l 
pm
 i 
 i 
  l     
i     xj   k  y
pm
 
 i 
  l    kj
i     y

   

   
   

b  random forests
random forests is a very popular ensemble learning method
which builds a number of classifiers on the training data and
combines all their outputs to make the best predictions on the
test data  thus  the random forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overfitting on the training data 
a random forests classifier is an ensemble classifier  which
aggregates a family of classifiers h x      h x        h x k   
each member of the family  h x    is a classification tree and
k is the number of trees chosen from a model random vector 

fialso  each k is a randomly chosen parameter vector  if d x  y 
denotes the training dataset  each classification tree in the
ensemble is built using a different subset dk  x  y   d x  y 
of the training dataset  thus  h x k   is the k th classification tree
which uses a subset of features xk  x to build a classification
model  each tree then works like regular decision trees  it
partitions the data based on the value of a particular feature
 which is selected randomly from the subset   until the data is
fully partitioned  or the maximum allowed depth is reached 
the final output y is obtained by aggregating the results thus 
y   argmaxp h x     h xk   

k
x
 i h x j     p   

lm   argmin

x

 yi   fm   xi      

    

xi rlm

fm  x    fm   x     lm   x  rlm  

    

v  e xperimental r esults
in this section  we detail the results of running the classifiers
we described in the previous section on our data  on both the
full dataset  and on collapsed categories 
a  performance on our original dataset

   

with our original dataset  we ran two different learning
algorithms  i e  naive bayes and random forests classifiers 
to get an initial understanding of the quality of our feature set 
and the amount of predictability in the data 

we used support vector machines for binary classification
in the latter part of the project  where we worked on the
classification problems with collapsed categories  we ran svms
using the gaussian  rbf  kernel to map the original features
to a high dimensional feature space 


  x  z   
   
k x  z    exp 
   

the naive bayes model was tested using cross validation 
i e     percent of the data was used for training and the rest for
testing purposes  we got the following results 
 the classifier gave     accuracy on the training set and
    accuracy on the cross validation set  hence  both
training and cross validation error were very high 
 the above trend was observed even on varying the size of
the training set  the accuracy did not go above      even
on the training set  note that this is still significantly better
than random guessing  since we have not    but     output
classes 
we also implemented random forests for our classification
problem  this was done keeping in mind that most machine
learning algorithms work with numerical features and that our
features are almost all categorical in nature  the random
forests classifier works well with categorical features and does
not need any preprocessing  we got the following results 
 after building the model  we ran it on the training set
itself to get a training error of     which initially looked
too good to be true 
 however  on performing cross validation on the training
data using    folds  we got a test error of      which
was huge compared to our training error  indicating high
variance 

j  

where i denotes the indicator function 
c  support vector machines

the optimal margin classifier with l  regularization was used 
m

x
 
 
i
kwk   c
 w b  
i  

    

s t y i  wt  xi     b      i

    

i     i           m

    

min

d  gradient boosted decision trees
gradient tree boosting    is another popular ensemble
method used for regression and classification  given a training
sample  x  y   the goal is to find a function f   x  that maps x to
y such that the expected value of some loss function  y  f  x  
is minimized  boosting approximates f   x  by the following
equation 
f  x   

m
x

m h x  am  

    

b  performance on collapsed classes 

m  

where the functions h x  am    called base learners  are simple
functions of x with parameters a  starting with f   x   the
parameters m and am are found in a stage wise manner and
the function fm  x  is updated as 
fm  x    fm   x    m h x  am  

    

in tree boosting  the base learner h x  a  is an l terminal node
regression tree  at each iteration m  a regression tree partitions
the x space into l disjoint regions  rlm  l
l   and predicts a
separate constant value in each one  the update rules for
calculating fm  x  given fm   x  are as follows 


d yi   f  xi   
yim   
    
df  xi  
f  x  fm   x 
ylm   meanxi rlm  yim  
h x   rlm  l
    

l
x
l  

ylm    x  rlm  

    

    

after the feature enriching process detailed above  where
we augmented our training data with data from the census 
we ran   different algorithms on two separate classifications of
the data  we split our crime categories into blue collar white
collar crimes in one case and violent non violent crimes
in the other  since blue collar crimes far outnumbered whitecollar crimes  and non violent crimes far exceeded violent
crimes  we decided to duplicate the minority class while
training our classifier  this penalized the classifier more for
mislabeling a training example in the minority class  than
for mislabeling a training example in the majority class 
during the training phase  and over rode the tendency of the
classifier to minimize its error by simply labeling all data as
belonging to the majority class  by doing this  we were able
to improve our precision and recall values on the minority
class  using this classification  we ran the following algorithms 
random forests 
for random forests algorithms  the parameters to tune are the
number of trees and the maximum depth of each tree  in order

fito pick optimum values for these  we tested the algorithm on
the data for different combinations of the parameters  finally 
we picked the set of parameters that gave not only the overall
highest accuracy but also the highest precision and recall values
for both crime classes  the graphs below show the variation
of accuracy  precision and recall values for parameter values
for blue white crime classification  from this  the maximum
accuracy obtained was        for number of trees       and
maximum depth       thus  random forests worked fairly well
on this problem  especially for blue collar crimes 

show the variation of accuracy  precision and recall values for
parameter values  from this  the maximum accuracy obtained
was       for number of estimators       and maximum depth
     

table v  precision and recall for gradient boosted trees on
blue collar white collar crime classification
precision blue
           

table iii  precision and recall for random forests on blue
collar white collar crime classification
precision blue
           

precision white
           

recall blue
           

recall white
           

precision white
           

recall blue
           

recall white
           

the graphs below show the variation of accuracy  precision and
recall values for various configurations of parameter values for
violent non violent crime classification  from this  the maximum accuracy obtained was        for number of estimators
      and maximum depth      

the graphs below show the variation of accuracy  precision
and recall values for parameter values for violent non violent
crime classification  from this  the maximum accuracy obtained
was        for number of trees       and maximum depth  
   

table vi  precision and recall for gradient boosted trees on
violent non violent crime classification
precision violent
           

table iv  precision and recall for random forests on
violent non violent crime classification
precision violent
           

recall violent
           

precision non violent
           

recall non violent
           

gradient boosted decision trees 
for the gradient boosted trees algorithm  the parameters of
interest are the number of trees and the maximum depth of
each tree  once again  we tested the algorithm on the data for
different permutations of the parameters  finally  we picked the
set of parameters that gave not only the overall highest accuracy
but also the highest precision and recall values for both crime
classes for blue white crime classification  the graphs below

recall violent
           

precision non violent
           

recall non violent
           

support vector machines 
we used support vector machine classifier with an rbf
kernel as our final algorithm  the two parameters of interest
for the classifier are the c and  values  once again  we
tested the algorithm on the data for different permutations of
the parameters and we picked the set of parameters that gave
the highest accuracy  the graphs below show the variation
of accuracy  precision and recall values for parameter values
for blue white crime classification  from this  the maximum
accuracy obtained was     for c     and        
the graphs below show the variation of accuracy  precision
and recall values for parameter values for violent non violent
crime classification  from this  the maximum accuracy obtained
was        for c       and         

fivi  c onclusion and f uture w ork

table vii  precision and recall for svms on blue collar white collar crime classification
precision blue
         

precision white
           

recall blue
           

recall white
           

the initial problem of classifying    different crime
categories was a challenging multi class classification problem 
and there was not enough predictability in our initial data set
to obtain very high accuracy on it  we found that a more
meaningful approach was to collapse the crime categories into
fewer  larger groups  in order to find structure in the data  we
got high accuracy and precision on the blue collar white collar
crime classification problem using gradient boosted trees
and support vector machines  the former famously robust
and the latter well suited to a   class classification problem 
especially with an rbf kernel that can translate the data
to a high dimensional space where it is linearly separable  
however  the violent non violent crime classification did not
yield remarkable results with the same classifiers  this was
a significantly harder classification problem  thus  collapsing
crime categories is not an obvious task and requires careful
choice and consideration 
possible avenues through which to extend this work include
time series modeling of the data to understand temporal correlations in it  which can then be used to predict surges in
different categories of crime  it would also be interesting to
explore relationships between surges in different categories of
crimes  for example  it could be the case that two or more
classes of crimes surge and sink together  which would be
an interesting relationship to uncover  other areas to work on
include implementing a more accurate multi class classifier  and
exploring better ways to visualize our results 
acknowledgments

table viii  precision and recall for svms on violent nonviolent crime classification
precision violent
           

recall violent
           

precision non violent
           

recall non violent
           

c  feature selection

we would like to thank our project ta youssef ahrez for
his thoughtful feedback and helpful ideas at every stage of our
project  we also owe a debt of gratitude to viswajith venugopal 
a student of the department of computer science  for helping us
to understand and implement parallel computing  and of course 
our acknowledgements section would be incomplete without a
mention of professor andrew ng  whose excellent class enabled
us to do this project in the first place 
r eferences

more important than just our accuracy and precision is the
interpretation of our model  which of our features actually help
predict the category of crime  analyzing the feature importances of our model gave us interesting insights the following
are the most relevant features used by our svm  which we
identified by running forward search  for classification  for
the case of blue collar white collar crimes  hour  minority
population  day of week  tract income level  tract code 
police district number  tract population  day of month

    bogomolov  andrey and lepri  bruno and staiano 
jacopo and oliver  nuria and pianesi  fabio and pentland 
alex       once upon a crime  towards crime prediction
from demographics and mobile data  proceedings of the   th
international conference on multimodal interaction 
    yu  chung hsien and ward  max w and morabito  melissa
and ding  wei       crime forecasting using data mining
techniques  pages          ieee   th international conference
on data mining workshops  icdmw 
    kianmehr  keivan and alhajj  reda        effectiveness of
support vector machine for crime hot spots prediction  pages
         applied artificial intelligence  volume     number   
    toole  jameson l and eagle  nathan and plotkin  joshua b 
      tist   volume    number    pages     acm transactions
on intelligent systems and technology
    wang  tong and rudin  cynthia and wagner  daniel and
sevieri  rich        pages          machine learning and
knowledge discovery in databases
    friedman  jerome h  stochastic gradient boosting 
computational statistics and data analysis             
        sts

fi   leo breiman  random forests  machine learning       
volume     number    page  

fi