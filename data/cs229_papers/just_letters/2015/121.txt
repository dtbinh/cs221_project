predicting short range displacements from
sensor data
maurice shih and jun ting hsieh

 

introduction

with the advent of internet of things  iot  and mobile devices  a key question is  how much
can we learn about a user by the data from smart devices  there are many interesting topics
in this domain  such as gesture recognition  about assisting users with additional functions 
in this project  we are aiming to construct a model to determine distance travelled by a
worn wearable in a short range displacement by sensor data  we define a short range displacement  gesture  as a natural motion of a stationary users arm  this typically takes less
than a second of travel time  many wearables can capture motion events from many different
sensors  the most basic sensor is the accelerometer  which measures the acceleration felt
by a device in three coordinates  the goal of this project is to predict the distance travelled
during a gesture using data from the accelerometer 

 

related work

work has been done on identifying motion gestures based with accelerometers  but little done
with identitfiy classifying gestures that take a short amount of time  e g  a few seconds  the
goal is to used the acclerometer data to identify the distance travelled  this is especially hard
since the orientation is not possible to determine  since there is no gyroscope 
one straightforward approach would be double integration  but this has mixed results 
as one lab noticed  this isnt because the accelerometers themselves are poor  but because
the orientation of the sensor must be known with a high degree of accuracy so that gravity measurements can be distinguished from the physical acceleration of the sensor  even
small errors in the orientation estimate will produce extremely high errors in the measured
acceleration  which translate into even larger errors in the velocity and position estimates 
   
as you can see in the table below  if the orientation of the device is off by     degrees 
the after    seconds  double integration can me     meters off 

 

fi 

preliminary tests

from the discussion above  we know that using double integral to calculate distance will
result in a huge error  however  there is no guarantee that using a machine learning model
will have a better result  in fact  given a set of acceleration data a         an with a constant time
step t  the result of double integral is just a linear combination of the data points 
d 

n
x
i  

 

vi t   t

n x
i
x

aj   t  na     n    a          an



i   j  

thus  double integral is a linear model  so it is possible that machine learning models
such as linear regression will do no better than using double integral  in order to test if this is
the case  we generated simulated data and compared the performances of double integral and
linear regression with only the acceleration data as features 
for a simple hand gesture  the acceleration data is very close to a perfect sine curve  and
the errors from the sensor and hand orientation can be modeled as gaussian noise  therefore 
we simulated our data using the equation a t    a sin t      where a is a random number
between   and    and  is noise drawn from a gaussian distribution  since a hand gesture
usually takes   or   seconds  we simulated the data over   seconds with a sampling rate of
    second per data point  below is the graph of a simulated example 

 

fithe actual distance can be calculated using double integral on the perfect sine curve without
noise 
zz t
a
 
a sin t dt     t  sin t   
d 


 
we generated      training examples and      test examples for linear regression  and
we used the test examples to evaluate both models  for double integral  the average percent
error is       for linear regression  the average percent error is       this shows that even
though both models make predictions by linear combinations of the data  linear regression
performs much better  it is able to  learn  the fact that the hand gesture is similar to a sine
curve and make more accurate predictions based on that  this also implies that non linear
models are likely to perform even better than linear regression  therefore  we will use both
linear and non linear regression  neural networks  to train on the real data 

 

procedure

an app for the pebble watch and android phone was created to measure and store the acceleration caused by the short displacements  we also created an ipad app to measure the distance
travelled during short displacements  the procedure of each gesture is as followed 
   have the middle finger touch the ipad 
   triggers the pebble to start recording data 
   make the gesture on the ipad 
   finally  touch a button on the phone to stop the recording 
this method assumes that the distance travelled by the middle finger is very similar to the
distance travelled by the wrist  pebble   thus  the motion of the finger and the wrist need to
be parallel during the gesture  in order to increase the accuracy of this  a split was used to
limit the motion of the wrist 
due to the limitations of the pebble  the data from the pebble is first sent to the phone 
and then manually transferred to a computer 

 

data processing

for each gesture  we have the acceleration data of all   directions x  y  and z  the y axis is
the direction that the hand moves  so the y data look like a sine wave  and the x and z data
are roughly constant throughout the gesture  if the data dont look like this  we will disgard
it 
since the pebble starts recording data before the gesture starts and doesnt stop right after
it ends  we have to manually select the data corresponding to the gesture  the gesture part

 

fiof the data  in the y direction  looks like a sine curve and the rest is roughly zero with some
noise  so we plot out the data and manually remove the irrelevant parts of the data 
in total  we collected over     gesture data from the pebble and ipad 

 

results and discussion

we did some initial testing with how double integration and linear regression did on predicting the distance travelled 
figure      linear regression vs actual

figure      double integration vs actual

as expected  we need some sort of non linear regression model  so we turn to neutral
networks  the two models we used were scaled conjugate gradient  which runs fast and
bayesian regularization  which runs a lot slower but tend to do better with smaller and nosier
data  one key factor that we tested with these two methods was the hidden layer size  or the
number of neurons to use  the two figures below are our results 
figure      after training  the average error for test case

 

fiwe fine tuned the bayesian regularization model with the smallest percentage error and
obtained a correlation between the output vs actually to be          below we have a figure 

 

conclusion

relying only on the accelerometer only to determine displacement  traditional methods of
double integration and linear regression do not fit well  neural networks that were implemented were fine tuned to have average errors of around    percent  an observation that was
made was that the hidden layer sizes of   or less tended to yield much higher accuracy in
prediction  more data collection to understand the data may lead to increased accuracy 
potential applications are virtual keyboards  authentication as well as malicious possibilities such as picking up a users keystrokes  this work  with the inclusion of a finger tap
classifier  when a finger taps  based on accelerometer data  the classified determines which
finger tapped  that was done in a former project by the authors  has the potential to capture
users keyboard inputs 

 

future work

we hope to fine tune the model with more data in order to understand more characteristics
of the data  this includes the variation of motion for different individuals  we also hope to
generalize our work with smart devices that offer more sensor data  such as gyroscope  we
are currently looking at using apple watch  which can record both accelerometer and gyroscope data  with gyroscope  we can construct yaw  pitch  and roll data  which can hopefully
increase the accuracy of our model 
 

fireferences
    using accelerometers to estimate position and velocity  http   www 
chrobotics com library accel position velocity

  

fiprop
predicting short range displacements from sensor data with the advent of the internet of
things  iot  and mobile devices  we can capture gesture and motion events from accelerometer and gyroscope data  the goal of this project is to develop a method to predict short range
displacements of hand gestures using these data  the current method of using double integrations of acceleration data to apply simple newtonian models is very inaccurate  in this project
we apply machine learning techniques to outperform existing models that predict short range
displacements  the bulk of the project will be determining a method to calculate ground
truths of displacements over short time spans  our preliminary idea is to track gesture motions by analyzing frame per frame changes of image recordings  at least    frames sec   we
hope to start by measuring these changes in one dimension  after this  we hope to expand to
the xy plane of two dimensions  once we create a method to calculate ground truths  the next
step is to find a way to extract meaningful features  initial feature selections will be based on
accelerometer and gyroscope data  this is an example of supervised learning  an interesting
result might be whether motions of different people vary significantly  after collecting the
training data  we can apply various machine learning techniques to construct a model for predicting displacements  in order to determine the accuracy of our results  we will collect test
examples and check if our model closely predicts the displacements of most test examples 
for example  using techniques for time series data may prove to be more accurate  we will
continue modifying our feature extractions until our model is true  in terms of timeline  we
hope to research and develop a technique for calculating ground truths in two weeks  in the
consequent week s   we will collect data and begin feature selection  if time permits  we
will investigate the differences in the gesture movements for different people  this particular
project stems from a larger project that a member of this team is part of 

milestone
as a refresher  our project is to develop a method to predict short range displacements of
hand gestures using smart devices  we plan on using apple watches  they will be available
to use on         but for preliminary testing we are using the pebble smartwatch  with this
device  the only data we can collect is the accelerometer data  so our objective is to map these
accelerometer values to features  and to map these features to the distance that was travelled
by the watch  the bulk of our work was to develop a system to measure data accurately and
reliable 
our first goal is to determine how much the watch actually traveled  in order to have a
ground truth to test how accurate a model we generated would be  out initial approach was
to have a camera that would record how the watched moved  and then calculate the number
of pixels that moved  however  this proved to be too complex  and we would have to scale  
pixel to how much distance it actually is  eg  inches   so our next approach  our current  is
to measure how much a finger moves during that gesture  our thought is that if the wrist and
finger stay straight  our gestures are only fractions of a second  during the gesture  then the
  

fidistance travelled should be close to the distance traveled by the smartwatch  in order to do
this  we had a wrist brace to limit the movement of the wrist 
we then build an ipad app that can track the movement of a finger to estimate how much
the smart watch moved  we also build a pebble app and android app to measure and store
the accelerometer that the pebble smartwatch experiences  when we get the apple watch  we
will be migrating to using that to collect the accelerometer data  we initially wanted yaw 
pitch  and roll  but apple has prevented  rd party developers from accessing that until the    
of the apple watch ios 
after building the apps  we started collecting data  we iterated through various methods to be able to segment the data better  we want to be able to  from just looking at the
accelerometer data  figure out when a user is moving his hand for the gesture  this is a
challenge that we will have to solve for applications  but for now  we tested different start
positions for the hand and found that having a finger placed on the ipad screen from the start
eliminated a lot of error  we found that the accelerometer data had the x axis points moving
the most in a shape that is similar a sine or cosine shape  you accelerate you hand  then slow
it down   now we move to feature extraction  the biggest problem is that the length of accelerometer data varies in size for each gesture  so we cannot have them be the features  even
if this worked it wouldnt be optimal   however  we are still trying to figure out what it the
best features we can extract  since the  fatness  or  height  of the accelerometer data shape
 the part the data that looks like the hump of a sine wave  increasing correlates to more distance travelled  so we construct a base model just to test how much tweaking is required  we
made our model based on least squares regression  but we need to have invertible matrices of
features  we did this by creating a square matrix of unifomly selected accelerometer points 
we ran leave one out cross validation model to construct the best model  after doing so  we
had a very significant error margin  we expected this to be very off  but we are looking into
feature selection that is meaningful to the data  such as how constant not constant was the
acceleration  duration of the acceleration  max acceleration  etc 

  

fi