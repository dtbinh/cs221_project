prediction of post collegiate earnings and debt
monica agrawal  priya ganesan  keith wyngarden
stanford university

i 

introduction

background
the u s  department of education launched college
scorecard in september      as a means of gathering
more data on degree granting institutions  the demographics of college students  and the status of alumni
of these institutions      by doing so  the u s  department of education hopes to empower students to make
more informed college decisions through a data driven
approach 
considering the soaring cost of higher education as
well as the accompanying rise of student debt  prospective students can greatly benefit from such information 
however  college scorecard has faced scrutiny due to its
omission of over     colleges  particularly community
colleges  in its data set      hence  applying machine
learning to fill in omissions in the data set  particularly
related to earnings and debt  and finding correlations
between characteristics of colleges and the future success
of their alumni has great value to society 
despite the relevance of machine learning to this
issue  fairly little research has been done in this area 
machine learning has been used in several related topics 
such as predicting corporate earnings and predicting
income based on census data about individuals        
however  no research has been conducted on using college data to predict the earnings and debt of its alumni 
potentially because higher education institutions do not
condone a solely numbers based approach to the college
selection process 

goals
we hope to use a variety of machine learning models
to make predictions regarding post college earnings
and debt of alumni who were on federal financial aid
from various institutions based on factors that reflect
the current status of each institution  such as majors
and degrees offered  tuition  and admissions rates  such
statistics are easier to obtain than post college earnings 
so our predictions can be used to fill in gaps in the current data set and potentially unearth interesting factors
that influence alumni earnings and debt  in addition 
alumni earnings can be compared with tuition costs and
average student debt to determine the typical interest
and length of student loans for a particular school 

previous work
as college scorecard is a newly released data set and
is more comprehensive than past college data sets  not
much analysis has been done on college scorecard or
even on the topic of predicting post collegiate earnings
and debt  the most relevant past work in this area was
conducted in the late     s and early     s 
brewer et al  looked at the effect of college quality
on future earnings based on individual and family characteristics of high school students entering into college 
and found that elite private institutions had a higher
return on investment in terms of future wages      james
et al  attempted to predict future earnings  for only male
college graduates  using a mix of individual student
information  institutional information  individual college experience variables  and labor market variables     
they found a general trend that selective private schools
on the east coast generally correlated to higher future
earnings  but also found that the college experience variables contributed to the majority of the variance in the
data  hence  they concluded that each individuals college experience  and what each individual makes of the
opportunities at his or her college  is the best indicator
of future earnings  lewis c  solmon  one of the most
widely cited experts in this field  performed a study
on what features determine college quality and what
impact college quality has on earnings      he used regression analysis to find that variables like college level 
average s a t  scores  and average faculty salaries drove
up alumni earnings the most 
while these papers have made large strides in using machine learning to understand what fuels alumni
earnings  and were very careful in avoiding bias with respect to minority communities and other similar factors 
they also have some shortcomings  all of these studies
were based off of individual alumni data  things like
personal and family background  individual major  etc  
no one has yet attempted to predict alumni earnings and
debt solely based off of anonymized institutional data 
furthermore  these studies focused on the most elite
institutions and did not provide analysis on smaller and
lesser known institutions  which are the organizations
that could most benefit from a study like ours 
as we were working with a new dataset  there were
a number of data quality issues to resolve  these are
largely detailed in the following section  but of particular
note are metrics that had partially missing data  only
some schools had listed values   there is is ample re 

fisearch on missing data problems in machine learning 
marlin        gives an overview of major methods     
the most useful family of methods for our dataset is
statistical imputation  which is detailed in rubin       
in the context of an overview of multiple imputation     
we will return to these papers in the next section 

ii 

data and feature set preprocessing

data
college scorecard provides a publicly available data
set consisting of approximately      metrics for     
degree granting institutions      these metrics include
demographic data  test scores  family income data  data
about the percentages of students in each major  financial aid information  debt and debt repayment values 
earnings of alumni several years after graduation  and
more  we chose to focus on the      data set because
it was the least sparse data set in the last five years
 more future earnings information was available than
for more recent years   our first tasks were to select
variables to predict  transform the dataset into pairs of
features and prediction variables  and segment the data
for evaluation purposes 

selecting features and prediction values
we chose two values for our prediction variables  the
median postgraduate debt and the median postgraduate
earnings for alumni   years after graduation  we then
went through several steps to prune the full feature set
to an initial feature list 
we first eliminated all features that had nonnumerical categorical values  primarily school name  
additionally  we removed unrelated features that should
not be used to make predictions  such as features that
provided the number of students in different data collection cohorts 
we also removed all features related to debt  earnings  and repayment  all metrics in these categories
are highly correlated with the two we chose to predict 
so they would be weighted very strongly compared to
other features and would hurt the ability of our models
to generalize to schools without any of this information
available  which is the motivation for this project 
finally  after the preprocessing steps listed above
and the non standard data value processing described
below  we removed all features  mostly null indicators
and unused categories  which had only one value for all
examples  as they offer no predictive power 

preprocessing non standard data values
some features in the data set were categorical fields 
we chose to turn each category into separate indicator
features  many values in the data set were listed as
 

 null   and a portion of these were meaningful  for example  indicating the absence of a binary feature  rather
than indicative of missing data  in order to transform
the nulls into usable numeric values while preserving
the original meaning of the nulls  we replaced each null
value with   and created an extra feature for each feature
that contained null values  this new feature used  s
and  s to indicate whether the value in the previous
feature was null or non null  for categorical fields that
contained null values  we created just one null indicator
feature in addition to the category indicators described
previously 

handling privacy suppressed values
all values in our dataset that were computed using data
from fewer than    students were listed as  privacysuppressed   privacy suppressed values are more common for smaller schools than larger schools and many
privacy suppressed values occurred in potentially useful
metrics  one approach for handling these values was to
simply remove all features with any privacy suppressed
entries  however  discarding hundreds of features in this
fashion  especially for features with a low percentage of
privacy suppressed values  was undesirable 
in marlins overview of approaches to missing data 
alternatives to case deletion  the above strategy  include
mean imputation  setting missing values to the mean of
observed values   regression imputation  learning regression models based on observed values   and the class of
multiple imputation solutions  sampling multiple values
from a simpler generalized model over observed values
and running analyses on each for later aggregation      
we determined that mean imputation was not appropriate in this case  since many features of schools vary
significantly based on school size  degree level  and so
on 
we implemented regression imputation by training a
linear regression model  with ordinary least squares cost
function  to the fully observed features with respect to
each feature with privacy suppressed values  to avoid
training these models with limited data  we imposed a
requirement that imputed features must have missing
data for less than     of schools  we then replaced
the missing values with predictions of the appropriate
model  this is a single imputation method  though since
the model cost function is convex  it is very similar to
multiple imputation methods with this same choice of
model   as noted by rubin  multiple imputation methods capture variability of the data that is lost with single
imputation      future work might involve using more
generalized models for imputation  such as a mixture of
gaussians  and running multiple imputation 

fiselecting training and testing examples
we removed all examples  schools  that were missing the
values for our two label variables  median postgraduate
debt and median postgraduate earnings   years after
graduation  among the provided options of       and
   years post graduation    years had the least sparse
data   from the remaining examples  we set aside     
for training       for development  and the rest      
for testing 

iii 

prediction models and methodology

linear regression
we pose our learning task as a regression problem  given
a processed list of features for a school  we would like
to predict real values for that schools students median
debt at graduation and median earnings   years after
graduation  linear regression is a natural choice of
baseline model for regression problems  so we first ran
simple linear regression on the full feature set  including
imputation of privacy suppressed features   using our
     training examples and      development examples 
the performance of this baseline was        mean absolute percent error  average of the absolute values of
percent error made on each soon  on the development
set for earnings and        for debt  in addition to
tuning the number of privacy suppressed features to
include in the feature set  we saw two avenues for lowering this error  pruning the feature space and enabling
our model to learn nonlinear relationships between the
features and earnings debt 

feature selection
after data preprocessing and statistical imputation of
privacy suppressed values      features remained  this
is a large number of features in comparison to the training set size of      schools  especially as we moved from
simple linear regression to more complex models  we
therefore explored the use of feature selection to shrink
the number of input features 
to select the most important features to keep  we ran
sequential forward based feature selection on our     
training examples  using our median earnings prediction
variable and median debt prediction variable in turn to
evaluate and select the most relevant features       features were selected based on their mean squared error 
using    fold cross validation  and selection was terminated at the point where the prediction error stabilized 
this procedure yielded     features for earnings prediction and     features for debt prediction  with   
features in common 
the top   features yielded after running statistical
imputation and feature selection were 

table    top features for median earnings and debt 

we also tried using pca on the school feature data
matrix to transform the data into a smaller set of uncorrelated model inputs  after full optimization under
each approach  a model using pca performed only
slightly worse than a model using forward based feature selection  however  the use of pca for feature
selection would require collection of data for all features when adding new schools to the dataset  since the
principal components need to be recomputed when the
data matrix grows  by contrast  after running feature
selection on the existing dataset  adding new schools to
the dataset requires collecting data only for the selected
feature subset  if too many new schools were added  the
feature selection results may become outdated  however 
since the number of examples in our task is limited by
the number of colleges in the united states  and since
the initial dataset is fairly comprehensive and the rate of
school closures openings is low compared to the total
number of institutions  this is not a major concern 

locally weighted linear regression
to capture local nonlinearities between the features and
debt earnings  we added local weighting to the cost
function for our linear regression model  using the euclidean norm  our weight function for a training example
x  i  with respect to an input example x was 
w

 i  

  exp

   x  i   x    

 

 

to make the euclidean distance  the norm in the
equation above  meaningful  we standardized features
 

fito zero mean and unit variance prior to computing the
weights  the parameter  in the weighting function
above was tuned on the development set data for various other model choices  feature selection  inclusion of
privacy suppressed values  
figures   and   show the results of tuning  for each
output variable and model  we found that the best linear regression model on the development set used local
weighting  feature selection  and imputation of privacysuppressed values 

figure   shows the performance of knn regression
on the development set across values of k  inversedistance weighting outperformed uniform weighting 
giving evidence that school with similar graduate earnings and debt are clustering in our feature space  but
knn with optimal k had higher error than the best
weighted linear regression model 

figure    k values plotted against percent error for
median earnings and debt 
figure     values plotted against percent error for
median earnings 

figure     values plotted against percent error for
median debt 

knn regression
we also used the non parametric k nearest neighbors
model in order to capture nonlinearities in prediction
of debt and earnings  using imputation of privacysuppressed values and the same data standardization
technique used for weighted linear regression  the knn
algorithm predicts debt and earnings as a weighted combination of debt and earnings of an inputs k nearest  defined here as euclidean distance  neighbors  the weighting schemes tried were uniform weights and weights
proportional to inverse distance 
 

capturing nonlinearities among features
lastly  we explored using models that can automatically
capture nonlinear relationships among the variables  in
addition to nonlinearities between the variables and outputs 
first  we used a support vector machine with data
standardization and feature selection to make predictions  we used the rbf kernel and l  regularized l loss support vector regression  l  regularized l  loss
support vector regression yielded similar results  we
tuned our regularization term coefficients on the development set and found          and            to be the
optimal parameters for earnings and debt  respectively 
we also trained simple neural networks with a single
hidden layer  using the previous feature selection and
imputation for privacy suppressed values       a single
hidden layer was chosen because there was insufficient
training data  number of schools  to fit a model with
more parameters without significant overfitting  the
network is trained using the levenberg marquadt algorithm for minimization with the logistic function as
the activation function  and it uses a randomly held out
set from the training set as a validation set and ceases
training when improvement on the held out set plateaus 
the number of nodes in the neural network was
tuned by examining the performance on the development set  results were mostly consistent for networks up
to    nodes  after which the network suffered an overfitting problem  a hidden layer with   nodes performed
optimally for debt with        error  and a hidden layer

fiwith   hidden nodes performed optimally for earnings
with        error 

iv 

results

tables   and   show the test set performance of the optimized  with respect to the development set  model from
each class for earnings and debt  the primary error
metrics were mean absolute percent error  which penalizes errors of different sizes and directions equally  and
rmse  root mean squared error   which penalizes larger
deviations superlinearly  for the best model under both
metrics  weighted linear regression  the r  measure between predicted and actual values in the test set was
       for earnings and        for debt  our absolute
error is lower for debt than for earnings  but since the
dollar amounts for debt are typically lower than those
of earnings  we have a higher percentage error for debt
prediction 

table    error for earnings across all models 

table    error for debt across all models 

v 

discussion

overall  much of the variance in earnings and debt information was in fact captured by the static school data
provided in college scorecard  our incremental model
selection process showed that regression imputation
of privacy suppressed values improved overall performance  in addition  local weighting helped adapt linear
regression to nonlinear relationships between school
characteristics and graduate debt earnings  the number of training examples is limited by the number of
schools  but feature selection helped constrain the complexity of our models in this setting  in addition  test set

performance was very similar to development set performance  so optimizing our model parameters through the
development set did not lead to excessive overfitting 
support vector regression did worse than all other
models  even after optimization of regularization parameters  the test set performance was only marginally
worse than errors for the training and development sets 
so overfitting was not an issue  this indicates that learning decision boundaries in our kernelized feature space
is not very helpful for the values we want to predict 
several selected features relate to socioeconomic
backgrounds of the student population  the college
scorecard data set included earning and debt data subdivided by background  but most of this data was privacysuppressed  for future work  partnering with the u s 
department of education to gain access to this data
could help provide more accurate or individualized estimates 
if we examine the predictions made by weighted linear regression for median earnings  approximately    
of the test set schools had predictions within        of
the true value  and almost     of schools had predictions within        of the true value  meaning that the
function did well for the majority of schools  however 
ten of the schools had absolute percent errors above     
in examining these schools  the majority only instructed
specialized skills  e g  cosmetology  massage therapy 
therefore  it seems like the current algorithm has trouble extending to trade schools  in which future debt and
earnings may be best characterized by a different set of
features than those emphasized by college scorecard 
we also explored whether our best model generalized well outside of our training  development  and test
sets by running it on the      schools missing earnings
or debt information  though we have no benchmark
to calculate accuracy for predictions on these schools 
we qualitatively examined the schools with the highest
predicted earnings  the columbia college of nursing
had the highest predicted earnings  the rest of the top
ten included two other health care related schools  six
law schools  and the hawaii technology institute  since
health care related schools dominated the top earnings
schools list for our labeled data  schools which had earnings information already in the dataset   their presence
in the predictions for the unlabeled data is expected 
however  the labeled data had no law schools and law
schools made up only      of the unlabeled data  indicating that our features were general enough to correctly
predict high earnings for law school graduates 
with below     average error for earnings data  our
best weighted linear regression model could be used
to fill in gaps in the current college scorecard data set 
given a proper disclaimer 
 

fireferences
    u s  department of education  college scorecard 
     
    the washington post  hundreds of colleges missing
from obamas college scorecard       
    michael kamp  mario boley  thomas gartner  beating human analysts in nowcasting corporate earnings
by using publicly available stock price and correlation
features       
    center for machine learning and intelligent systems 
census income data set       
    dominic j  brewer  eric r  eide  ronald g  ehrenberg  does it pay to attend an elite private college 
cross cohort evidence on the effects of college type on
earnings        the journal of human resources 

 

    estelle james  nabeel alsalam  joseph c  conaty 
duc le to  college quality and future earnings  where
should you send your child to college         the
american economic review 
    lewis c  solmon  the definition of college quality and
its impact on earnings        the national bureau of
economic research 
    benjamin m  marlin  missing data problems in machine learning        university of toronto 
    donald b  rubin  multiple imputation after     years 
      journal of the american statistical association 
     machine learning and statistics toolbox release
    b  the mathworks  inc   natick  massachusetts 
united states 
     neural network toolbox release     b  the mathworks  inc   natick  massachusetts  united states 

fi