exploring adversarial learning on neural network models for text
classification
isaac caswell
allen nie
onkur sen
stanford university
stanford university
stanford university
dept  of computer science symbolic systems program dept  of computer science
 icaswell  anie  onkursen  stanford edu

 

introduction

deep neural networks have recently achieved
state of the art performance in many tasks  most
notably image recognition  however  many models are also notoriously difficult to train  over the
years  people have used different data augmentation techniques to artificially simulate possible
data  and training techniques such as pooling and
dropout are used to help with this particular issue 
adversarial learning is a reaction to the observation that many models are susceptible to images
perturbed by adversarial noise  such images can
fool a trained neural net into predicting a wrong
label with high confidence  nguyen et al         
adversarial learning combats this problem  it
bears similarity to the aforementioned techniques 
but instead of simulating what could realistically
happen  it focuses on helping the model to explore
the landscape near the decision boundary  leading to a more accurate model  as demonstrated by
 miyato et al          adversarial learning can be
treated as an a priori optimization technique that
can be generalized on all neural network models by directly modifying the objective function 
training may also be done by generating external adversarial examples directly  demonstrated in
 goodfellow et al         

 
   

related work
image processing

adversarial learning first gained popularity in the
realm of image processing  where the heavy use
of neural networks improved performance across
different corpora  most of them are established
in goodfellow et al          in image processing 
where automatically generating variants of data
points is quite common  goodfellow made the
distinction between adversarial learning and common data augmentation techniques such as transformations adversarial examples are not naturally 

occurring examples that need to realistically happen in the world 
adversarial examples add noise to the training
set that is smaller than the precision of the initial
training image  such noise is undetectable by human eyes and also cannot be obtained by the image capturing equipment  goodfellow argues that
training with such perturbations will help neural
networks truly capture concepts and gain accuracy
beyond perceived data  jin et al         pointed
out the adversarial examples generated in goodfellows paper are sampled near a decision boundary  thus making network models particularly vulnerable  miyato et al         further demonstrated
the theoretical implication that adversarial training
examples will help models to better find non linear
relationships in data 
   

natural language processing

adversarial learning with neural networks has performed well in image processing  furthermore 
since neural network architectures are commonly
used for many tasks in natural language processing  we believe this technique would improve such
models as well  such work has yet to be done 
however  natural language deals with discrete outcomes whereas images are easily represented as
continuous data points  an adversarial image is
much easier to intuitively visualize than an adversarial sentence 
we explore the effectiveness of adversarial
learning on recurrent neural network and long
short term memory model  which would be very
different from previous models 

 

adversarial learning

intuitively  it does not makes sense that a classifier should respond to a very small perturbation
to a very small change in an example that is assigned some label with high confidence  this is
especially true for images  where the precision of

fiimplementation note  in order to construct this
cost function  we double the size of the computation  creating an entirely separate computational
graph for the first and second terms in l   s  y  
connected at the bottom by the word embedding
matrix and at the top by the cost function  we forward prop both from the word embedding matrix 
but only backprop through the graph corresponding to the first term 

the features is limited  and certainly also true for
word sentence vectors  which are inherently approximate  as they are learned from data  symbolically  a classifier shouldnt respond differently
to x than it does to x  if each element in the perturbation  is sufficiently small 
x   x              class x    class x  
where  may be the precision of the sensor  in the
case of an image   or the maximum magnitude of
the last update to a word vector  nlp   or an arbitrary small number  in our case  
consider  however  what actually happens to the
activation of such a perturbed example  for some
weight vector w 

 

task definition

among a variety of text classification tasks  we
settled on sentiment analysis to avoid focusing on
relationships of single words  thus giving us more
freedom to generate appropriate adversarial examples  sentiment analysis is also a heavily explored field with many neural network architectures  we experimented with recurrent neural networks  rnns   long short term memory  lstm 
models  and cnns 

wt x   wt x   wt 
notice that a well chosen   e g     w  may
cause the activation to grow linearly with the size
of the weight vector  for our rnn  one sees that
the perturbation will propagate 

   

data

ht    w
xt   u ht   

we chose the imdb dataset  maas et al        
which contains        sentences split equally into
   w xt   w t   u   w xt    w t    training and testing sets  each training instance
contains an entire review written by one individu  w xt    w t          
ual  no post processing is used  the dataset is used
as is  we also load pre trained word embeddings
to determine the adversarial perturbation   we
from google word vecs google news vectors of
use the fast gradient clipping method proposed
    billion words  mikolov et al         
by goodfellow et al          let s be a training
example  w be our word embedding matrix  and
  methodology
l   s  y  be the loss function for s given its true
label y  adapted to our lstm architecture  the
    recurrent neural network
perturbation is 
we implemented an lstm rnn with mean pool    sign w l   s  y  
ing and softmax layers to map to output labels using the symbolic mathematical expression engine
rather than actually creating adversarial examples
theano  bastien et al          our model is gpu
and training on them  we simulate this training by
compatible although alas  the only gpus availbale
modifying the objective function  let  be the facto use did not cooperate 
tor by which we wish to weight the adversarial
   w xt   w t   u ht   

versions of the training examples  the modified
objective becomes 

     

hyperparameters

in the ideal world  we would run parallel random
searches for hyperparameters on some cluster with
l   s  y     l   s  y   
     l   s    sign w l   s  y  gpu  however  as this was beyond our research
budget  and the farmshare machines didnt cooperate   we finally copped out and picked somewhat
for each word in the training data  we simulate
arbitrary hyperparameters  excepting alpha and
the creation of an adversarial example for that seneps  which relate to the adversarial objective  
tence  and have the classifier balance the imporbased on the theano lstm example  pierre
tance of correctly classifying the untainted examluc carrier         we did a manual search over
ple and the adversarial example by  
 

fi 

alpha and epsilon  the important hyperparameters for our model are 

results for a variety of parameters for the adversarial objective are summarized in table   
where      corresponds to the non adversarial
case  since we were not able to tune hyperparameters  our validation and test set may be considered equivalent  and their average is presented
in the third row of the table we discover no clear
evidence that adversarial learning performs better
than the alternative  our conclusion  however  is
that these results are not sufficient to discredit our
approach  there are several reasons why this is
the case  all stemming from our inability to sufficiently tune the net  which stems from our lack of
computing power  see section      travails 
the most important thing to note is that the error
is high in all cases  even had we demonstrated that
adversarial learning performed better in this case 
 say  an improvement of     to     average error   one would have had trouble giving credence
to this result  a support vector machine could easily outperform either of these models 
furthermore  the results are noisy  although the
best adversarial model does outperform the nonadversarial version  this could be due to chance 
in this respect our results are disappointing 
however  one can still do some analysis of the adversarial examples we spawned  and the coming
sections devote themselves to this  furthermore
the architecture is in place to do more powerful
analyses  when the computing resources present
themselves to us 

   wemb init  how to initialize the word embeddings  they are either initialized with
word vec     d  or with each value drawn
uniformly from           because of constraints mentioned above 
   wdim  word embedding dimension  for
word vec vectors  this must be      otherwise  we used     
   hdim  the hidden dimension of the lstm 
we default to     
   reg  the regularization on the weight matrix  we default to     
   clip  the magnitude to which to clip the
gradients  gradient clipping is elementwise 
in a boldly arbitrary move  we default to   
   adv  whether to use the adversarial cost
function 
   alpha  how many adversarial examples to
simulate  as a fraction of the training corpus 
   eps  a constant proportional to the magnitude of the perturbation 
   

results and discussion

travails

as alluded to above  we built a gpu compatible
system with    tunable parameters  and furthermore implemented a script to make random hyperparameter sweeps in parallel between adversarial
and non adversarial models  however  we were
unable to access a machine that was capable of
running this hyperparameter search  most of our
efforts focused on trying to work with the rye machines  those morbidly interested in our fate here
may enquire in person  even the corn machines
tended to randomly cut off a few hours in 
as a result we ran all experiments locally with
friendly hyperparameter settings  furthermore 
due to the constraint on the computing resources 
we switched from a truncation to a filtering approach with our data  meaning we discarded reviews longer than     words  rather than truncating them  leading to utilizing only     of our data 

 

adversarial visualization

we would like to visualize the adversarial examples produced by our system to interpret whats
going on in a human interpretable way  there is 
however  a hitch to this  when adversarial learning is applied to vision  it is easy to interpret what
an adversarial example means  a perturbed an image is another image  natural language  however 
proves to be more difficult  a perturbed embedded
word is some vector in the word embedding space
without any obvious correspondence to an english
word  this section details our approaches towards
dealing with this problem 
   

nearest neighbor visualization

the simplest approach is to find the nearest neighbor via cosine distance in word embedding space
 

fitable    test and development set error given different values of  for        

   
   
   
   
   
   
   
   
   
   
   
dev
                                                      
test
                                                      
average                                                       

    
    
    
    

table    progressively improving visualization of a positive review 
love it   love it   love it   this is another absolutely superb performance from
the divine miss m  from the beginning to the end   this is one big treat   don t
rent buy it now         
knn
love conquistadors conquistadors love conquistadors conquistadors love conquistadors conquistadors conquistadors conquistadors conquistadors conquistadors drama the daftness from the conquistadors miss m  from the conquistadors to the conquistadors conquistadors conquistadors conquistadors pensacolians conquistadors treat conquistadors daftness t rent conquistadors
conquistadors conquistadors now conquistadors  conquistadors    
knn top     
love quit   love quit   love quit passing quit quit quit absolutely quit quit
from the quit miss intentions from the quit to the quit   quit quit quit quit treat
passing quit t rent quit quit quit now passing  quit    
knn top     
love it   love it   love it   quit quit quit absolutely   quit from the   miss
  scaled perturbation
intentions from the quit to the easily   quit quit one big treat   don t rent unk
quit it now    quit    
knn top     
  scaled perturbation
love it   love it   love it   this is another absolutely superb performance from
  bigram smoothing
the audience miss intentions from the quit to the end   this is one   treat   don
t rent unk quit it now    quit    
original

   

to each perturbed example and reconstruct a sentence out of these words  representative results of
this approach are shown in the first row of table   
we see that these visualizations are quite pooror
at least  its hard to make any sort of sense of them 
there are several reasons why this approach
may yield such odd results 

removing uncommon words

to address     above  we restricted ourselves to
the      most common words from the training
data  the second row of table   gives an example
of how this change affected the decoding of our
adversarial examples  as one sees  this was mildly
helpful  and especially when the nearest neighbor
was the sentence padding token 

   rare words with mostly random distributional representations pollute the word space 
and many perturbations end up being closest
to them 

   

scaling the perturbation

to approach      we decreased the magnitude of
the perturbation until the reconstructed sentence
began to look more similar to the original sentence  we tended to decrease the magnitude of
the perturbation until it was weighted by about
          this is significantly lower than the 
value we used for training  which was on the order
of        

   word by word translation does not take context into account  furthermore  in highdimensional spaces  the top several neighbors
often have very similar distances  for one
run  for example  the top ten neighbors were
all at distance of around     
   the direction of the perturbation may be
more important in terms of a human interpretation of an adversarial example than the
example itself 

   

bigram weighted interpretation

to deal with      we trained a bigram model
on the imdb training corpus and incorporated
 

fitable    a plausible adversarial example 
original
sentence

adversarial
sentence

shallow   shallow script     stilted acting     the shadows of     
unk mob over the actors  heads in scenes     worth watching because kate unk plays the most selfish mother in tv movie history
and it s all before ben stretch got his teeth unk  
unbelievable   unbelievable script     stilted acting     the shadows of nazis unk intelligent over the actors  heads in scenes    
worth watching because remarkable unk plays the most imagination mother in tv movie history and it s all before attached
remarkable got his summer unk  
the gradient from the adversarial example  and in
short taught us everything practical that we never
learned as students  chris manning advised us in
our quest to figure out what we were doing 

it into our decoder  as seen from table    the
bigram weighted interpretation model can help
make the adversarial sentences more interpretable 
often snapping perturbed words back to what they
were before  if the perturbation would otherwise
be too linguistically infeasible  often  however  it
does more harm than good  and changes mildlyperturbed words to something erroneous 
   

references
frederic bastien  pascal lamblin  razvan pascanu 
james bergstra  ian goodfellow  arnaud bergeron 
nicolas bouchard  david warde farley  and yoshua
bengio        theano  new features and speed improvements  arxiv preprint arxiv           

interpretation of adversarial examples

even with our best visualizations  its hard to tell a
good story about our adversarial examples  table
  gives a pair of sentence and reconstructed adversarial example into which we can inject some
measure of sense  words that differ between the
two are bolded in the adversarial example  the
original review is negative  meaning that the adversarial review aims to be classified as positive
while still being obviously negative to a human 
the resulting adversarial review is highly ambiguous  and is adversarial in the sense that  to a
human  it still seems moderately negative  while
containing positive words  it calls the script unbelievable  which could be a good thing or a bad
thing  the words substituted in tend to be negative words converted to positive  shallow  selfish   intelligent  remarkable  imagination 
nazi probably too in this case as people like to
give good reviews movies about the nazis   but
none of the words is placed in a context such that
its positive connotation carries to the description
as a whole  of course  this may well be because
it is mostly nonsense  a scientist must be careful
not to read too far into this 

 

ian j goodfellow  jonathon shlens  and christian
szegedy        explaining and harnessing adversarial examples  arxiv preprint arxiv           
jonghoon jin  aysegul dundar  and eugenio culurciello        robust convolutional neural networks under adversarial noise  arxiv preprint
arxiv            
andrew l maas  raymond e daly  peter t pham  dan
huang  andrew y ng  and christopher potts       
learning word vectors for sentiment analysis  in
proceedings of the   th annual meeting of the association for computational linguistics  human language technologies volume    pages         association for computational linguistics 
tomas mikolov  wen tau yih  and geoffrey zweig 
      linguistic regularities in continuous space
word representations  in hlt naacl  pages    
    
takeru miyato  shin ichi maeda  masanori koyama 
ken nakae  and shin ishii        distributional
smoothing with virtual adversarial training  stat 
        
anh nguyen  jason yosinski  and jeff clune       
deep neural networks are easily fooled  high confidence predictions for unrecognizable images  arxiv
preprint arxiv           

acknowledgements

jon gauthier not only conceived this idea  but also
helped us wade through theano syntax  get the
system running on rye  advised us to disconnect

kyunghyun cho pierre luc carrier        theano
lstm tutorial 

 

fi