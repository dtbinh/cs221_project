deep reinforcement learning for
flappy bird
kevin chen
abstractreinforcement learning is essential for applications where there is no single correct way to solve a
problem  in this project  we show that deep reinforcement
learning is very effective at learning how to play the game
flappy bird  despite the high dimensional sensory input 
the agent is not given information about what the bird
or pipes look like   it must learn these representations
and directly use the input and score to develop an optimal
strategy  our agent uses a convolutional neural network to
evaluate the q function for a variant of q learning  and
we show that it is able to achieve super human performance  furthermore  we discuss difficulties and potential
improvements with deep reinforcement learning 

fig     three screenshots of the game flappy bird at
three different difficulties  easy  medium  hard  respectively 

i  i ntroduction
reinforcement learning is useful when we need an
agent to perform a task  but there is no single correct
way of completing it  for example  how would one
program a robot to travel from one place to another
and bring back food  it would be unrealistic to program
every move and step that it must take  instead  it should
learn to make decisions under uncertainty and with very
high dimensional input  such as a camera  in order to
reach the end goal  this project focuses on a first step
in realizing this 
the goal of the project is to learn a policy to have an
agent successfully play the game flappy bird  flappy
bird is a game in which the player tries to keep the bird
alive for as long as possible  the bird automatically falls
towards the ground by due to gravity  and if it hits the
ground  it dies and the game ends  the bird must also
navigate through pipes  the pipes restrict the height of
the bird to be within a certain specific range as the bird
passes through them  if the bird is too high or too low 
it will crash into the pipe and die  therefore  the player
must time flaps jumps properly to keep the bird alive
as it passes through these obstacles  the game score is
measured by how many obstacles the bird successfully
passes through  therefore  to get a high score  the player
must keep the bird alive for as long as possible as it
encounters the pipes 
training an agent to successfully play the game is
especially challenging because our goal is to provide
the agent with only pixel information and the score 
the agent is not provided with information regarding
what the bird looks like  what the pipes look like  or
where the bird and pipes are  instead  it must learn these
representations and interactions and be able to generalize
due to the very large state space 

ii  r elated w ork
the related work in this area is primarily by google
deepmind  mnih et al  are able to successfully train
agents to play the atari      games using deep reinforcement learning  surpassing human expert level on
multiple games           these works inspired this
project  which is heavily modeled after their approach 
they use a deep q network  dqn  to evaluate the qfunction for q learning and also use experience replay
to de correlate experiences  their approach is essentially
state of the art and was the main catalyst for deep reinforcement learning  after which many papers tried to
make improvements  the main strength is that they were
able to train an agent despite extremely high dimensional
input  pixels  and no specification about intrinsic game
parameters  in fact  they are able to outperform a human
expert on three out of seven atari      games  however 
further improvements involve prioritizing experience replay  more efficient training  and better stability when
training      tried to address the stability issues by
clipping the loss to    or    and by updating the target
network once in every c updates to the dqn rather than
updating the target network every iteration 
iii  m ethod
in this section  we describe how the model is parameterized and the general algorithm 
a  mdp formulation
the actions that the agent can take are to flap  a     
or to do nothing and let the bird drop  a       the state
is represented by a sequence of frames from the flappy
bird game as well as the recent actions that the player
took  specifically  the state is the sequence shown in

 

fist    xthistlen     athistlen          xt    at    xt  
h
i
   
i li  i     es a   s    r    max
q s
 
a
 

 

q s 
a 

  
q s 
a 

 
i 
i

i
i
 
a

equation   where st is the state at time t  xt is the pixel
input  or the frame or screen capture  at time t  and at is
the action taken at time t  historylength  or histlen 
is a hyperparameter that specifies how many of the most
recent frames to keep track of  this is to reduce the
storage and state space compared to saving all frames
and actions starting from t      the reason for storing
multiple xs and as rather than storing a single frame x
is because the agent needs temporal information to play 
for example  the agent cannot deduce the velocity of
the bird from a single frame  but velocity is essential for
making a decision 
the discount factor was set to          the
transition probabilities and the rewards are unknown
to the agent  since q learning is model free  we do
not explicitly estimate the transition probabilities and
rewards  but instead directly try to estimate the optimal
q function  this is described further in the q learning
section 
however  we still must define the rewards intrinsic to
the game  ideally  the reward should essentially be the
score of the game  it starts out as   and every time the
bird passes a pipe  the score increases by    however 
this is potentially problematic in that the rewards will be
very sparse  specifically  if the bird dies instantly at the
start of the game  the reward would be similar to if the
bird died right before reaching the pipe  the performance
is clearly better if the bird survives up until the pipe
compared to dying instantly  therefore  adding a reward
for staying alive encourages the agent to think similarly  without this additional reward  the agent should
eventually realize this  but adding the reward  called
rewardalive  speeds up the training process  in total 
we have three rewards  rewardalive  rewardp ipe 
and rewarddead  the agent gets rewardalive for
every frame it stays alive  rewardp ipe for successfully
passing a pipe  and rewarddead for dying 

   
   

bellman equation as an iterative update
qi    s  a    es    r    max
qi  s    a    s  a 
 
a

   

where s  is the next state  r is the reward   is the environment  and qi  s  a  is the q function at the ith iteration  it can be shown that this iterative update converges
to the optimal q function  the q function associated with
the optimal policy   however  this is rote learning  to
prevent rote learning  function approximations are used
for the q function to allow generalization to unseen
states  our approach uses the deep q learning approach
in which we use a neural network to approximate the qfunction  this neural network is a convolutional neural
network which we call the deep q network  dqn  
a common loss used for training a q function approximator is


 
 
   
li  i     es a    yi  q s  a  i   
 
where i are the parameters of the q network at iteration
i and yi is the target at iteration i  the target yi is defined
as
h
i
   
yi   es   r    max
q s
 
a
 

  s 
a
i 
 
a

   

for a given experience e    s  a  r  s     an experience is
analogous to a datapoint such as in linear regression and
the replay memory  a list of experiences  is analogous
to a dataset such as in linear regression  the gradient of
the loss function with respect to the weights is shown in
equation    thus  we can simply use stochastic gradient
descent and backpropagation on the above loss function
to update the weights of the network 
additionally  we take an  greedy approach to handle
the exploration exploitation problem in q learning  that
is  when we are training  we select a random action
with probability  and choose the optimal action aopt  
arg maxa  q s  a     in our implementation  we linearly
change the exploration probability  from   to     as the
agent trains  this is to encourage a lot of exploration in
the beginning where the agent has no idea how to play
the game and the state space is extremely large  it takes a
large number of random actions and as it starts to figure
out which actions are better in different situations states 

b  q learning
the goal in reinforcement learning is always to maximize the expected value of the total payoff  or expected
return   in q learning  which is off policy  we use the

 

fi   filters of size      with stride    followed by a rectified nonlinearity  the second layer is also a convolution
layer of    filters of size      with stride    followed by
another rectified linear unit  the third convolution layer
has    filters of size      with stride   followed by a
rectified linear unit  following that is a fully connected
layer with     outputs  and then the output layer  also
fully connected  with a single output for each action 
to choose the best action  we take the action with the
highest output q value  aopt   arg maxa  q s  a     

it exploits more and tries to narrow down what the
optimal actions are 
c  experience replay
a problem that arises in traditional q learning is
that the experiences from consecutive frames of the
same episode  a run from start to finish of a single
game  are very correlated  this hinders the training
process and leads to inefficient training  therefore  to
de correlate these experiences  we use experience replay 
in experience replay  we store an experience  s  a  r  s   
at every frame into the replay memory  the replay
memory has a certain size and contains the most recent
replaym emorysize experiences  it is constantly updated  like a queue  so that they are associated with the
actions taken with the recent q functions  the batch used
to update the dqn is composed by uniformly sampling
experiences from the replay memory  as a result  our
experiences are no longer likely to be correlated 

g  pipeline
algorithm    deep q learning algorithm for flappy
bird
initialize replay memory
initialize dqn to random weights
repeat
new episode  new game 
initialize state s 
repeat
extract xt from raw pixel data update state
st with xt
add experience
et     st     at    rt     st    to replay
memory
take best action
at   arg minaactions q st   a  with
exploration if training
uniformly sample a batch of experiences
from the replay memory
backpropagate and update dqn with the
minibatch
update exploration probability 
if c updates to dqn since last update to
target network then
update the target q network
q s  a   q s  a 
end
update state st with at
update current reward rt and total reward
totalreward
update game parameters  bird position  etc  
refresh screen
until flappy bird crashes 
restart flappy bird
until convergence or number of iterations reached 

d  stability
moreover  to encourage more stability in decreasing
the loss function  we use a target network q s  a  
q s  a  is essentially the the same as q s  a   the
network has the same structure  but the parameters may
be different  at every c updates to the dqn q s  a   we
update q s  a   this q s  a  is then used for computing
the target yi according to 
i
h
   
q s
 
a
 

  s 
a
yi   es   r    max
i 
 
a

   

this leads to better stability when updating the dqn 
e  pre processing
since we use a very high dimensional state  we actually perform pre processing to reduce the dimensionality
and state space  the pre processing is done over the
pixels  so we first extract the images from the state st  
the original screen size is          pixels in three
channels  but we convert the image captured from the
screen to grayscale  crop it to          pixels  and
downsample it by a factor of      resulting in a        
pixel image  it is then rescaled to        pixels and
normalized from          to         i call this feature
extractor  s  
f  deep q network
our q function is approximated by a convolutional
neural network  this network takes as input a        
historylength image and has a single output for every
possible action  the first layer is a convolution layer with

the pipeline for the entire dqn training process is
shown in algorithm    it is as previously described
earlier in this section  we apply q learning but use

 

fitraining difficulty
easy
medium
hard

experience replay  storing every experience in the replay
memory at every frame  when we perform an update
to the dqn  we sample uniformly to get a batch of
experiences and use that to update the dqn  this is
analogous to sampling batches from a dataset using
sgd mini batch gradient descent in convolutional neural
networks for image classification or deep learning in
general  then we update the exploration probability as
well as the target network q s  a  if necessary 

flap every n
inf
inf
   

human
inf
inf
      

dqn
inf
inf
    

table i  average score of dqn on varying difficulties
compared to baseline and human performance
training difficulty
easy
medium
hard

flap every n
inf
  
 

human
inf
inf
  

dqn
inf
inf
   

iv  r esults
table ii  highest score of dqn on varying difficulties
compared to baseline and human scores

a video can be found at the following link 
https   youtu be  wkbztuspkc  our metric for evaluating the performance of the dqn is the game score
 numper of pipes passed   the reported scores in the
tables are the average scores over    games  unless
otherwise specified  

the flappy bird game was run at    frames per
second  and historylength was set to    the discount factor was      and the rewards were the following  rewardalive         rewardp ipe        
rewarddead        the exploration probability  decreased from   to     over        updates to the dqn 
the size of the replay memory was       experiences 
for training  we used rmsprop with a learning rate
of  e    decay of      and momentum as       these
were chosen similarly to that of      to figure out better
parameters  some were done by trial and error  for
example  we noticed that the learning rate was too high
when the neural network weights began exploding  and
used a binary search algorithm to figure out the best
learning rate  if the learning rate was too low  it would
take longer to train  we did updates in mini batches of
size     experiences   we only begin training after the
replay memory has at least      experiences and update
the target network q s  a  once for every      updates
to the dqn  our convolution weights are initialized to
have a normal distribution with mean   and variance
     this deep neural network was implemented using
tensorflow 

screen  these comparisons are shown in table i  average
score  and table ii  highest score  
the performance of the dqn is much higher than the
baseline and human performance  if the score was higher
than       the score was considered to be infinity  except
for the human case where if they got a score above
     this would be considered infinity   the human case
was generalized to be infinity if the user could play for
forever if he or she could focus and did not need to take
breaks  eat  sleep  etc    although the scores for human
and dqn are both infinity for the easy and medium
difficulties  in reality the dqn is better because it does
not have to take a break whereas the dqn can play for
    hours at a time 
in general  almost all of the failures in the hard
difficulty are because the bird flaps upwards when it
should be letting the bird drop  and then it dies  however 
once in a while  the bird will just barely clip the top right
corner of the lower pipe as it is falling  furthermore  i
noticed that the agent seems to take riskier moves when
it trains more  thus  a follow up test to resolve these
problems could be to encourage the agent to take the
moves with the lowest risk  to do this  we would have
the agent make a random move a small probability of the
time during training  even if the agent is supposed to be
evaluating the optimal action   to maximize the expected
return  the agent would have to play very safely 

b  overall performance

c  training time

the trained dqn plays extremely well and even
performs better than humans  we compare the results
of the dqn with a baseline and humans  the baseline
implementation flaps every z frames to keep the bird
in the middle of the screen  this baseline was chosen
because the pipe gaps locations are uniformly distributed
with the expected location to be in the middle of the

in this section  we discuss how the number of training
iterations affects the performance of the flappy bird
agent  the number of training iterations refers to the
number of updates to the dqn  there is no exact definition of epoch here   our results  see table iii  show that
more training does not necessarily lead to better scores 
in fact there is some instability and the scores fluctuate

a  testing parameters

 

fitraining iterations
     
      
      
      

easy
      
      
      
       

medium
    
     
    
    

hard
   
    
  
    

game difficulty
easy
medium
hard

dqn  easy 
inf
   
   

dqn  medium 
inf
inf
   

dqn  medium 
    
inf
   

dqn  hard 
   
   
    

table v  performance of dqn after training on the
tested difficulty but initialized to random weights

table iii  average score of dqn as a function of
learning rate
game difficulty
easy
medium
hard

dqn  easy 
inf
    
   

dqn  hard 
inf
inf
    

  iterations
     
      
      
      

easy
    
     
      
     

w  rewardalive
      
      
      
       

medium
   
     
    
     

w  rewardalive
    
     
    
    

table iv  performance of dqn on medium difficulty
with weights initialized from dqn trained on easy

table vi  comparison of training with additional
rewardalive and without it

with more training after a certain point  for example  the
hard difficulty had not reached this point of training and
consistently yields better results with more training  this
instability is inherent to many reinforcement learning
algorithms and could be further investigated in a followup project  one potential solution would be to decrease
the learning rate as more training occurs or to increase
model complexity  neural network architecture  

e  removing the reward for staying alive
in this section  we test whether the rewardalive
reward truly leads to faster convergence or better results as suspected  the results in table vi show that
indeed adding a rewardalive reward accelerates the
training process since it provides an incentive which is
directly correlated to the score goal  more importantly 
it prevents sparse rewards to encourage faster learning 
therefore  if a reward is directly correlated with the
intended reward  such as score of the game   then it is
beneficial to use this correlated reward in addition to the
intended reward to speed up the training process 

d  training with initial pre trained network
here  we describe results of a network which is initialized to another pre trained network  specifically  when
training the network to play on the medium difficulty  we
initialize the dqn to have the same weights of a network
that was previously trained on the easy difficulty  this
yielded the best results performance wise compared to
any of our other trained networks  the network was
trained on the medium difficulty for         updates
after being initialized to the previously trained dqn on
easy mode 
from table iv  it is clear that not only does the
dqn perform better on the difficulty it was trained on 
but it also performs better on the easier difficulties  it
remembers how to perform well on the easy mode while
it modifies it weights to also perform well on the medium
difficulty  the same could not be said about the networks
which were directly trained on the easy medium hard
difficulties  as shown in table v   which is a very
insightful observation 
after training for     iterations on each difficulty
 directly  with random initialized weights  we got the
results shown in table v  these networks do not generalize well to different difficulties 

v  c onclusion
we were able to successfully play the game flappy
bird by learning straight from the pixels and the score 
achieving super human results  however  training was
not consistent in that more training did not necessarily
correlate with better score  the model could be overfitting or forgetting so future work could attempt to explore
and resolve this issue  another very important area that
could be refined is the experience replay  we uniformly
sampled from the replay memory  but some experiences
have more impact on successfully training the dqn
than other experiences  being able to prioritize these
experiences would lead to better performance  efficient
training  and faster convergence  moreover  in this game
we removed the background and score to reduce clutter
and increase likeliness of successful training  it would be
interesting to see how restoring the background affects
agent performance  overall  our results show that deep
reinforcement learning is a step in the right direction and
has a lot of potential for further application 

 

fir eferences
    v  mnih  k  kavukcuoglu  d  silver  a  graves  i  antonoglou  d 
wierstra  and m  riedmiller  playing atari with deep reinforcement learning  in deep learning  neural information processing
systems workshop       
    v  mnih  k  kavukcuoglu  d  silver  a  a  rusu  j  veness 
m  g  bellemare  a  graves  m  riedmiller  a  k  fidjeland 
g  ostrovski  s  petersen  c  beattle  a  sadik  i  antonoglou 
h  king  d  kumaran  d  wierstra  s  legg  and d  hassabis 
human level control through deep reinforcement learning  nature 
                        
    t  schaul  j  quan  i  antonoglou  d  silver  prioritized experience
replay  arxiv  http   arxiv org abs           

 

fi