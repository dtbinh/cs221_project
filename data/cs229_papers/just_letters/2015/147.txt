cs    final report  fall     

 

neural memory networks
david biggs and andrew nuttall

i  i ntroduction

ii  r elated w ork

ontext is vital in formulating intelligent classifications and responses  especially under uncertainty  in a
standard feed forward neural network  ffnn   context comes
in the form of information encoded in the input vector and
trained in weight parameters  however  useful information can
also be present in the temporal nature of the input vectors 
or from past internal states of a network  future outputs can
achieve better accuracy by observing transient trends in the
input data  or by utilizing key memories from distant inputs
which could be crucial to formulating a correct output  by
providing a neural network with an architecture for storing
and maintaining memories this additional context can be
effectively leveraged 
a simple implementation of memory in a neural network
would be to write inputs to external memory and use this
to concatenate additional inputs into a neural network  for
noisy analog inputs  memory inputs pulled from gaussian
distributions can act to preprocess and filter the data  fig   
shows a schematic and memory weight distribution of a ffnn
with external memory augmentation 

c

   
   

weights

   
   
   
   
   
   
 
 

 

 

 

 

  

memory locations

 a  network schematic

 b  weights of memory
matrix

fig     architecture of ffnn with simple memory implementation
this architecture was implemented to predict the next
sequence in a noisy sinusoidal signal  ten previous inputs
were stored in memory  from which five additional inputs were
drawn from gaussian weights on the memory  the network is
trained with a single sinusoid then tested with a sum of three
sinusoids of varying frequencies with errors around     for
training and     for testing  the convergence and input output
waveforms are shown in fig    
network architectures with delayed inputs or additional inputs drawn from memory can be useful tools  but are limited in
functionality since the memory architecture is predetermined 
a more powerful memory architecture would store memory
inside the network to allow the network to learn how to utilize
memory via read and write commands 

neural networks were first introduced over    years ago
as one of the first learning algorithms  recurrent neural
networks  rnn  were then introduced in the     s to better
process sequential inputs by maintaining an internal state
between inputs  long short term memory  lstm  improved
upon rnns in the late     s by adding logic gates to read 
write  and forget internal memory states  recently  weston et
al      defined a framework for neural networks to interact
with external memory in order to read and write long term
memory  in their paper  westin implements an rnn with
memory for textual story processing in which several actors
move between rooms and while carrying and deposit objects 
in their results they showed that with memory their network
outperformed a similar rnn and lstm without memory
access at answering questions about a story  around the same
time  work was published by graves et al      in which they
implement an algorithm they call the neural turing machine
 ntm   the ntm algorithm has memory structures called
memory heads that can be accessed in order to read  write  and
erase data  they ran several test on their ntm  one of which
was teaching the network to store then copy sequences of
numbers  they trained their network up to sequences of twenty
  bit numbers and tested up to sequences of     numbers
with good results  this project differs from related work by
implementing internal neural memory by assigning a set of
memory nodes  memory bank  to each standard neuron  to
be discussed in the following section  with this approach a
richer memory can be achieved by not only maintaining key
memories  but by retaining memory histories with associated
temporal information 
iii  m ethodology
the neural memory network  nmn  architecture is comprised of an ffnn with a dedicated internal memory bank
associated with each standard neuron in each of the hidden
layers  the input and output layers are composed of only
standard nodes  while the hidden layers are composed of
standard nodes with their respective memory nodes  in this
formulation  the output of each node is given by a sigmoid
function of an affine combination of its inputs  the output of
a node is used as a linear switch to activate the memories
dedicated to that node  during forward propagation the neural
network can learn to call upon these memories as needed
to provide additional information in making classifications 
figure   depicts the general network layout for a simple
memory neural network 
training and testing of the algorithm is comprised of three
key steps 

fics    final report  fall     

 

training data

  
training error
test error

  

 

  
   

error    

  
 

  
  

 

   

 

   

  

 

   

 

   

 

   

 

   

 

testing data

  
 

  
   

  
 

 

 

 

training points

 

 
  

 

 a  error convergence

input signal
output signal

 

 

   

 

   

 

 b  waveforms

fig     signal prediction results from memory augmented ffnn

the output state vector for each layers standard nodes is
augmented with the output of the layers memory nodes  the
output of each memory node is the product of the content of
that memory slot and the output of the standard node it is
attached to  with this approach the standard node acts as a
linear switch to turn memories on or off as desired  during
forward propagation in the hidden layers the outputs of a
hidden layer only go to standard nodes  memory nodes only
receive input from their respective standard node in the same
layer 
b  backward propagation and gradient descent
fig     a neural memory network has input and output
layers composed of standard sigmoid nodes and hidden layers
composed of both standard and memory nodes  each standard
node in a hidden layer has a memory bank associated with
it  and each memory bank is composed of multiple memory
nodes  the outputs of standard nodes are used as switches to
activate memories when called upon 






forward propagation of the input across each network
layer to form the output
back propagation of the errors across each layer starting
at the output to perform gradient descent updates on
network parameters
memory storage in each layer of nearby neuron outputs
as an orthogonal set in a higher dimensional space

the network is trained by updating the parameters w and
b using gradient descent with the back propagation technique 
in the training process  an input is forward propagated through
the neural network to produce an hypothesis h xj   that is then
compared to a known solution y  the error is calculated as
 


 i 

forward propagation in an nmn functions in a similar
manner to that of an ffnn  with an augmented output vector
from each hidden layer  the output of each layer i is given
by a two step equation containing outputs from both standard
neurons and memory neurons 


x i    f w i   x i     b i  
   
h
i 
 i 
 i 
 i  
 i 
 i 
 i  
x i     x    x  m         xk   xk mk
   

   

performing gradient descent on an nmn requires different
considerations to be given to the set of first and last layers and
the set of hidden layers due to the networks asymmetry  to
perform gradient descent on the weights the partial derivatives
 i 
w r t wj k are taken as 

wjk
a  forward propagation

 
 output 
  h x
   y      
 

 


 i 

 i 

h xj   x i 
j
 i 

h xj   xj

 i 

   

wjk

evaluating this expression for the final layer i of weights
yields



 i 
 i 
 i 
 
h x
 

y
h xj      h xj   xi     
j
 i 
wjk
back propagation is implemented in the standard way by
storing gradients in the variable  and passed to previous
layers  this process starts at the output layer as 
  output     x output   y x output      x output   

   

fics    final report  fall     

 

the errors of the subsequent layers are weighted by the
parameters
  i    w i      i    x i        x i     

   

there are two paths back to the primary neurons  so the errors
of each hidden layer must be summed across memory nodes
x  i 
 i 
 i 
 i 
j   j  
j k mj k
   
k

the updates on w and b are then 
w i     w i     i  x i   
 i 

b

 i 

   b

 

   

 i 

    

c  memory architecture
every node in the hidden layers contains a static memory
bank  each unique memory in a node is orthogonal to all other
memories in that node  such that no information is lost when
the memories are added together  a collection of memories
is then represented as a single vector in a higher dimensional
space called a composite memory  the direction of the vector
dictates what memories are present  and the magnitude of the
vector when projected onto each memorys axis determines
the order in which the memories occurred  when a node
fires the current composite vector that is stored in memory
is scaled according to some scheduled decay and then the
new memory is added  the new memory is created by an
orthogonal mapping of the outputs the node and its  nearest
neighbors  nearest neighbors can be defined in a spatial sense
to be nodes in the same layer with close indices  or it can also
be defined to include nodes in previous and future layers  the
memory bank update equation for each node is given by
j
m
i
h
 i 
xj
 i 
 i 
 i 
 i 
 i 
mj    
mj   g xj          xj        xj    
    
   
where g x    r     r    is a function mapping the
outputs of its  nearest neighbors to      dimensional space 
and  is the given decay schedule which is turned on and off
byj themrounded output of the standard node  when
am node fires
j
x

 i 

x

 i 

 j evaluates to   and if it does not fire  j evaluates
to   
during the training phase memories are allowed to decay to
zero quickly to provide the network the plasticity to converge
to its desired orientation  during use memories can be retained
indefinitely  or according to any desired decay schedule  with
an asymptotic memory decay schedule  transient information
can be retained for the more recent memories  and any remaining memories which have hit the asymptotic magnitude are still
retained  but their time ordering can no longer be compared
to other memories which have also hit the asymptotic decay
magnitude 
iv  e xperiments  r esults
the nmn algorithm was implemented and then tested with
several test data sets 
 sorted classification
 classification and recall

fig     each standard node stores a composite memory in
its dedicated memory bank  a composite memory is the sum
of all of the orthogonal memories associated with that node 
such that the original memories are retained by projecting the
composite memory onto different dimensions  in the nearest
neighbor approach the activation permutations of a cells nearest neighbors is mapped to a higher dimensional orthogonal
memory and then added to the cells composite memory 
memory recall
extended recall
each of the test sets are meant to evaluate the various
functionality of the algorithm  for comparison  a standard
ffnn  nmn with memory turned off  and an rnn available
in the matlab neural network toolbox  layrecnet    were also
trained and evaluated with the test data sets  layrecnet   has
one hidden layer with feedback and one output layer  a block
diagram of is shown in fig    



a  sorted classification
a data set composed of points sampled from    randomly
generated multivariate gaussian distributions  with the goal
being to classify which distribution a point was sampled from 
samples were drawn from the distributions in a stationary
order  due to overlap in the distributions a standard feed
forward neural net was only able to achieve      classification
error  by implementing memory the neural network was able
to identify that there was a pattern to the input points and
reduces classification error to       an improvement of      
an rnn with a single feedback delay was also tested on the
data  with classification error of       convergence results are
shown in fig    
b  classification and recall
the task of this data set was to classify an input and also
state the previous two classifications it made  in the order
it made them  a memory less neural network can perform
no better than converging to the most prolific output on this
type of task  a memory based neural network with the same
parameters was able to achieve    classification error on the
same data set  an rnn with a single feedback delay was
also tested on the data  with classification error of        
convergence results are shown in fig    
c  memory recall
the networks were given sequences of   bit numbers to
write into memory  followed by empty input vectors of zeros

fics    final report  fall     

 

memory vector magnitude

 

memory occurs

   
   

   

component magnitude

composite memory magnitude

   

memory component magnitude

 

no memory permanence
memory permanence

   
   
   
   
   

   
   
   
   
   

   

   

   

   
 

 
 

  

  

  

  

   

 

  

  

  

  

   

node firings

node firings

fig     every time a standard node is activated that memories associated with that node have their magnitude decremented
according to some decay schedule to allow for the retention of temporal information  memories can either decay out of
existence  or can be given some asymptotic decay 

 
ffnn
nmn
rnn

   

fig     block diagram of layrecnet    a layered recurrent neural
network function available in the matlab neural network
toolbox 

training error

   
   
   
   
   
   
   
   

 

  

training error

ffnn
nmn
rnn

 

  

   

   

   

   

   

epochs

fig     a convergence plot for the classification and recall
task  where outputs are given by the current classification and
the previous two classifications 

 

  

   
rnn
nmn  external write
nmn  nearest neighbor

 

  

   

 

  

  

  

  

   

   

   

   

epochs
training error

fig     a comparison of error convergence between an fnn 
nmn  and rnn on a classification task demonstrates the
ability of the nmn to leverage temporal patterns in the input
data 

   

   

   

   

of the same series length to signify to the networks to read
and output the stored sequences  the nmn algorithm was
set with one internal memory layer to process the data 
both memory writing procedures were tested  external writing
which recorded the input layer upon a standard neuron firing 
and nearest neighbor which recorded nearby neuron outputs as
outlined above  convergence results for sequences of length
two are shown in fig    

   

 
 

  

  

  

  

epoch

  

  

  

fig     convergence plot of the memory recall task for the
various networks  rnn  nmn with an input write function 
and nmn with nearest neighbor write function 

fics    final report  fall     

 

vi  f uture w ork

 

nmn total error
nmn long recall error
rnn total error
rnn long recall error

testing error

   
   
   
   
 

 

  

  

  

  

epoch

fig      convergence plot for the extended recall task  in
this data set  some classifications depend only on the current
inputs  but some depend on inputs from    time units ago  a
standard rnn is unable to perform those long term classifications better than randomly guessing  while the nmn is able
to achieve near perfect long term recall 

d  extended recall
in the extended recall data set the task was to perform
classifications  with some classifications depending only upon
the current input  and some classifications depending on
information from    inputs prior  both the rnn and the
nmn were able to achieve perfect testing performance on
the classification task where the outputs only depending upon
the current input  however  in the case of outputs depending
upon data from    inputs ago the rnn was only able to
achieve random guess      error  levels of performance 
while the nmn was able to achieve near perfect long term
recall  the nmn in this setup was composed of   hidden
layers  convergence results for all four error rates are shown
in fig     
v  c onclusion
we have developed and demonstrated a feed forward neural
network algorithm that stores memory associated with standard hidden layer neurons  the benefit of neural memory
is shown to give context to the algorithm while computing
pattern recognition and time series data sets  memories are
stored with a higher dimensional mapping to ensure orthogonality  and memories are decremented upon recall to maintain
a time history  the neural memory network was evaluated
with several data sets of classification and recall and against
a similar feed forward neural network without memory and a
commercial recurrent neural network  the results show that
although indeed the neural memory network indeed learns to
utilize memory in order to solve problems requiring context 
in the current implementation it does not outperform the
commercial recurrent neural network for most tasks  on the
other hand  neural memory allow for the network to excel
at long term memory storage and recall  a functionality that
escapes the standard recurrent neural network 

the algorithms described in this paper are only a first
approach towards the nmn architecture  future developments
will depend upon un linking the read and write operators 
memory scaling issues  training schedules  and the general
optimization techniques used to perform gradient descent 
in this implementation the nmn learns to recall information
from memory as needed  but it is not explicitly trained on how
and when to write information  to combat this  in our current
implementation we have tied together the write and read
commands so that any potentially useful information is saved
to memory in the appropriate memory slots as determined by
the read command  however  to get a more robust performance
the read and write commands need to be learned separately
and independently 
in the current nearest neighbor approach  the size of the
memory banks scale according to     which can cause major
performance issues if  becomes too large  this constraint is
introduced to maintain the orthogonal nature of the memories
to avoid the problem of having to learn to reference specific
memories  with orthogonal memories  a composite memory is
referenced instead of a single component memory  instead of
this approach  the permutations of the nearest neighbors can be
used to address a specific memory location in a nodes memory
bank through a binary to linear mapping  thus eliminating
the orthogonal constraint and the   scaling law  an example
output of a memory node with   nearest neighbors with this
approach could be given by
m        x       x   x  m     
    x   x  x  m     
x      x   x  m     

    

x  x  x  m   
which is a differentiable expression for selecting a memory
from a learned location  in this expression x  and x  are the
outputs of the two nearest neighbors  this expression could
be further processed with some type of focusing technique 
but this approach allows for linear memory scaling and does
impose an orthogonal constraint 
to allow for more accurate comparisons between the nmn
and other off the shelf algorithms the optimization techniques
used to perform gradient descent on the nmn can be improved
upon to allow for better convergence  the implementation of
a dynamic step size achieved via a line search could prove to
be very effective 
r eferences
    weston  jason  sumit chopra  and antoine bordes  memory networks 
arxiv preprint arxiv                  
    graves  alex  greg wayne  and ivo danihelka  neural turing machines 
arxiv preprint arxiv                  

fi