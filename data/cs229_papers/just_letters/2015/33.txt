cs    project final report

 

is the company you are investing trustworthy 
christopher wang

abstractwe studied the trustworthiness of chinese
companies by processing court files  results of svm and
nave bayes are compared  we start with the related work
that has been done in the literature  after that  we describe
the data collection process for this project in detail  next
we explain the methods we used to classify the instance
and their results  finally we discuss the effect of feature
selection 

i  i ntroduction
nvestors usually rely on credit rating analysis provided by credit rating agencies to help
facilitate investment decisions in the us  the credit
rating is usually based on financial variables  which
includes long term debt  net income  total asset  net
worth  etc  however  besides the financial status of
the companies  investors care about the reputation
and integrity of the company too  to help investors
to get another view of the companies that they want
to invest  this project is focused on building a credit
model for chinese construction companies based
on court files  the reason behind it is that if the
company is sued for any reason  and proved by
the court that it is guilty  they break their code of
trustworthy  the more guilty court cases are found
for a particular company  the less the investors can
trust it  court files are just one example of the
resource  we can also use news  magazine articles 
etc 
to be more specific  given a chinese court case
file as an input  it will return if the defendant
company is guilty or not  when the investors have a
specific inquiry about a company  we will summarize how many guilty cases this company has went
through 

i

ii  r elated w ork
the classification problem of determining if the
company is guilty by court file is a nlp learning
problem  similar tasks have been done for different objectives  in      the effectiveness of different inductive learning algorithms  find similar 
nave bayes  bayesian networks  decision trees 

and support vector machines  svm   in terms of
learning speed  real time classification speed and
classification accuracy  in the mean time alternative document representations  words vs  syntactic
phrases  and binary vs  non binary features   and
training set size are explored 
there are also enormous work regarding feature
engineering in the literature  the type of representation that dominates the text classification literature is
known as the bag of words  often  most frequent
and infrequent words are removed  together with
case and punctuation  moreover  stop words that are
functional or connective words are also filtered out 
additionally different morphological forms of the
same word will be mapped to the common stem
using stemming algorithm     
since bag of words representation disrupted the
paragraph  sentence and word order  researchers
have also been seeking other means to represent the
text  such as phase based representations     
iii  data collection
the data used in this project was crawled using
scrappy from http   www court gov cn zgcpwsw  
specifically the court cases are in guangdong
province of china  there are approximately        
court cases    gb   the following are the steps i
took to clean the data 
 parse html files to retrieve the body documents 
 segmented the documents using the stanford chinese segmenter  http   nlp stanford 
edu software segmenter shtml   different from
english  different combination or even orders
of characters can mean completely different
things  in addition  chinese is standardly written without spaces between words  these facts
distinguish tokenization of chinese from that
of english 
 built inverted index with segmented documents this is mainly for finding the exact
defendant company names in a court case  to
do that  we start from the inverted index  and

fics    project final report

then find the company name by searching the
position after the location of defendant  usually
in chinese            
  within bounded distance till the word 
  meaning company   then  we can also
record the number of guilty cases that a specific
company we are interested in has gone through 
 given a designated list of construction companies we are interested in  we filter the overall
datasets  so that all the remaining cases have
at least one of the construction companies as a
defendant 
 till now  we can build the feature set as all
the tokens that appeared in the training data 
and the corresponding feature value being the
number of occurrence for that token 
the process is also depicted in fig    

 

we have a binary classification problem with labels
y         and features x  in general  the svm is
a classifier that can be represented as
hw b   g wt x   b  

   

where g z      if z     and g z      otherwise 
to reflect a very confident set of predictions  svm
tries to maximize the margin  which is the minimum
distance between the decision boundary to the training data points  equivalently  that is to find the w  b
to solve the following optimization problem 
 
min kwk 
w b  
s t y  i   wt x i    b      i              m
there are also cases that data is not perfectly
linearly separable  under this situation  one wants
to allow some data points of one class to appear on
the other side of the boundary  in this case  we can
introduce slack variables i    for each xi   and
the quadratic programming problem becomes 
x
 
min kwk    c
i
w b   
i
s t y  i   wt x i    b        i              m
i    
b  nave bayes

in the na ive bayes model  we assume that the
features xi s are conditionally independent given the
label y  in the multinomial setting based on the
training data  we will built the conditional probabilities for each labeling using maximum likelihood
estimator 
pm
 i 
 i 
    
i     xj   k y
p

 
     
jk y  
m
i
fig     example of data collection and pre processing procedure 
i     y     
pm
 i 
 i 
    
i     xj   k y
p
jk y    
     
m
i
i     y     
pm
iv  m ethods
 i 
    
i     y

 
 
   
y
in this project  we used two algorithms to clasm
sify  svm and nave bayes  in the following  we
then  for given feature values in the test data  we
briefly introduce these two methods 
then simply calculate
a  svm
svm is to find the hyperplane  i e  decision
boundary  linearly separating our classes  imagine

p x y     p y     
p x 
qn
  i   p xi  y      p y     
qn
qn
 
  i   p xi  y      p y          i   p xi  y      p y     

p y     x   
 

fics    project final report

and pick whichever class has the higher posterior
probability 
due to the fact that in the nlp problem  the
feature values are normally very sparse  i e   a lot of
them will be zeros  to prevent the scenarios of     
we use laplace smoothing  which replace eq         with
pm
 i 
 i 
        
i     xj   k y
pm
     
jk y    
i
i     y         
pm
 i 
 i 
        
i     xj   k y
pm
jk y    
     
i
i     y         
pm
 i 
        
i     y
y  
 
   
m k
c  results
in this project  we trains the data manually  there
are     files with labels in total  we split them into
two different ratios for the sake of comparison  one
is     training vs      testing  the other is    
training vs      testing  in all the cases  we use the
features that appears in less than     documents 
in other words  we stripped out the most frequent
features that occur almost in every documents 

fig     comparison of nave bayes and svm using features without
including numbers      training      testing  

fig    and   depict the testing errors vs  feature
sets included in more than x documents  the x axis
indicates the number of infrequent features stripped 
in general  nave bayes performs better when more
infrequent words are removed  while the svm gets
worse 
fig    shows the testing errors impacted by different feature selection  all the features  features
without numbers  and feature without numbers and
single characters  in chinese  most single words are

 

fig     comparison of nave bayes and svm using features without
including numbers      training      testing  

fig     comparison of different feature selections on nave bayes
method      training      testing  

either peoples last name or some connective characters  in general  by more selective and informative
features  nave bayes performs better and better 
fig    shows the impacts of feature selections on
svm  where the patterns are more random  with
more features stripped out  svm performs worse 
it performs better when it includes all the features 
if you cross compare the results on svm and that
on the nave bayes  nave bayes performs more
stable than that of svm 
v  m ore d iscussions on f eature
s elections
it is found from this study that as we stripped
more infrequent features out  the most indicative
words in the nave bayes make more sense in
chinese  fig    shows the actual top    indicative
words  there are still some name  location related
words other than number  however  it is very challenge to remove those words out in chinese  as there

fics    project final report

fig     comparison of different feature selections on svm     
training      testing  

 

infrequent words are very high  one smarter way
would be to gather some nature of chinese language
with infrequency of some of the words  for example  we can strip infrequent single characters  as a
result  we will end up with even less features  that
may enable some other ways that we would not do
with enormous features  such as decision trees 
another complication of the court cases are that
some of the cases may involve more than one defendants  for now  i use one class to label them all 
but in reality  there are scenarios that one company
is guilty  while the other is not  further thinking
needs to be done for those more complicated cases 
vi  c onclusion
in this project  we have classified the company
involved in court cases to be trustworthy or not by
machine learning algorithms  in particular  nave
bayes and svm are used for learning  we conclude
that  nave bayes performs slightly better than
svm  feature selection is a very critical task in
nlp  besides common ways of selecting features 
such as stripping infrequent and frequent words out 
we should combine the nature of the language to do
it more smartly 
r eferences

fig     comparison of different sets of most indicative words by
stripping more infrequent features 

are a lot of location and names  which is very hard
to exclude them all out 

fig    

feature size deduction trend 

in fig     we can see that the deduction speed of

    susan dumais  john platt  david heckerman  and mehran sahami  inductive learning algorithms and representations for
text categorization  in proceedings of the seventh international
conference on information and knowledge management  pages
        acm       
    julie b lovins  development of a stemming algorithm  mit
information processing group  electronic systems laboratory 
     
    sam scott and stan matwin  feature engineering for text
classification  in icml  volume     pages         citeseer 
     

fi