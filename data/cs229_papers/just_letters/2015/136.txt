draft kings and queens  cs    

machine learning applications in fantasy basketball
eric hermann and adebia ntoso
stanford university  department of computer science
ehermann stanford edu  antoso stanford edu
abstract
this paper is an attempt to apply machine learning to fantasy sports in order to gain an edge over the average player 
basketball players fantasy scores were predicted using a linear regression algorithm and stochastic gradient descent as
well as a naive bayes classifier with discretized state space  a team of eight players was then selected by framing the
problem as a constraint satisfaction problem  regression optimizations were complex  but an advantage of around  
percent was gained over regular users of draftkings  meaning with a large enough volume of teams  the algorithm will
make money  future investigation is necessary into factoring in riskiness of players  as well as purposely pursuing
players who are less frequently selected in draftkings contests 

i 

introduction

antasy sports have become increasingly popular over the last    years  and the last two
to three have seen the rise of massive fantasy
sports gambling websites like draftkings and fanduel  these sites allow users to draft a fantasy team
in their sport of choice and pay to enter that team
into a pool of other teams  if a teams total number
of points is higher than that of a large percentage
of its competitors  the creator of that team receives
a multiple of the buy in as payout  and if the team
is in the tiny top fraction of scorers in the pool  it
stands to win much more 
this space seemed like an interesting one in
which to apply machine learning principles  in particular because of the predictive nature of selecting
a high scoring team  the large volume of data that
is publicly available from previous seasons  and the
fun of using machine learning in a relatively new
and unexplored domain 

f

ii 

problem framework

draftkings was selected as the website to work with
 though these techniques could be applied just as
easily to fanduel or any other fantasy gambling
site   and basketball as the sport of choice due to
the larger volume of games  which would give more
data and thus more predictive power 
 with

in draftkings fantasy basketball  users draft a
team each day they choose to play  selecting players
from any team that is playing a game that particular
night  a team is composed of eight players  which
are constrained to a point guard  a shooting guard 
a small forward  a power forward  a center  a general guard  point or shooting   a general forward
 small or power   and a util  any of the five positions   additionally  the players have to be from at
least two different real nba teams  and represent
at least two nba games being played on that day 
each player you draft has an associated cost  usually
ranging from        to         in draftkings  dollars   and each user has         to draft their team 
players score draftkings points by doing things like
making field goals  getting rebounds  blocking shots 
and a couple of other simple categories that are easy
to measure from a box score 
the problem was split into two parts  first was
trying to predict the number of draftkings points
a player will score based on that players previous
performance  this was framed in two ways  the first
was as a regression problem  starting with a linear
regression model based on box score statistics from
that player in previous games  the second was as
a modified multinomial naive bayes classifier  with
discretized parameters to prevent an infinite event
space 
second was choosing a team based on the predicted points for each player who has a game in

help from mikaela grace

 

fidraft kings and queens  cs    

a particular night  this part was framed as a constraint satisfaction problem  the constraints for the
csp include the spending limit of         allocated
to draft a team as well as the fact that every position
must be filled by a unique player  the third constraint  that each team must consist of players from
at least two different teams competing in at least
two different real nba games  was not included 
as the event that this constraint is not satisfied is
highly unlikely  after solving for the satisfactory
assignments  the optimal assignment or the team
with the highest predicted score will be selected and
entered into the pool 
in terms of data  box score and team data from
the           and           seasons were collected
from espn com using python web scraping libraries
like beautifulsoup  salary and position data were
downloaded daily from the draftkings website 

iii 

score prediction

in general  if the generated algorithm to predict
players scores is more accurate than draftkings
predicted scores for players  the algorithm will give
an advantage over draftkings users  assuming typical users use draftkings predicted scores as their
metric  the justification for this is that draftkings
prices have a close to linear relationship with draftkings predicted score for a player  see figure    

figure    draftkings predicted points per game vs draftkings
salary for each player 

because of the nearly linear relationship between
player price and their predicted player score  if the
generated algorithm is better at predicting player
scores than draftkings is  then it will be able to find
 

undervalued players according to their draftkings
salaries  this is the justification for comparing our
prediction algorithms outputs with the draftkings
predicted score to test actual predictive value of the
algorithm 
the           season data was used for training
and testing the score prediction algorithms  and predictions for draftkings contests on a particular night
use data from the           season  the means and
variances for each parameter of the data from each
season were nearly identical  so the theta values
trained and tested in           are just as informative in           

i 

regression

the first attempt at predicting player scores from
the data collected was using a regression algorithm 
namely linear regression  the algorithm found
the optimal theta values for the parameters using
stochastic gradient descent  converging in general
when the theta vector changed by less than      on
a particular iteration through the data  train and
test error were calculated using the standard error
estimate 
initially  the linear regression algorithms error
on a prediction was around       this obviously
didnt provide much predictive value  so several specific changes were made to the algorithm to make it
more accurate 
first  features were changed to be the average
of that features values over the last five games  as
opposed to just the value at the last game  adding a
feature for each of the last five games tended to decrease the accuracy  giving relatively poor predictors 
likely because a feature from any game on its own
is not necessarily informative about future performance  the averages over the last five games were
more informative  decreasing the test error by several points  note that its difficult to provide precise
figures on error changes here because changes were
not necessarily made exactly in order  and other
changes to the algorithms were happening at the
same time  
the data was also normalized to have zero mean
and unit standard deviation  so the predictor could
more accurately take into account differences in each
feature 

fidraft kings and queens  cs    

because the train and test error were so close
together  more features were assumed to be helpful
in predicting score  so the algorithm was changed to
add more features  features outside of the players
direct data turned out to be helpful  like the opponents record as a percentage or whether the current
game was at home  but beyond that adding new
features didnt decrease the test error significantly 

figure    parameter informativeness for linear regression 

figure   shows the informativeness of a subset of
the features we ended up using  as measured by the
absolute theta values for each feature  interestingly 
parameters like field goals attempted and assists
were more indicative of future performance than
were features like previous points scored  defining
a quadratic feature set based on a subset of the features     total features   or even on a full set of the
features  over     features   did not decrease the
error significantly 

figure    train and test error of the regression algorithm in
comparison with draftkings error  versus player
points per game 

the eventual optimal linear regression algorithms train and test error are shown in figure    in
comparison with the error of draftkings predicted
scores  when analyzing on all of the players  the
regression algorithm is about as informative as is
draftkings  however  when looking only at players
with a minimum average point total of    or higher 
the regression algorithms error is around      less
than that of draftkings  and the difference increases
as the minimum average point total for players is
increased  the x axis in the figure  
since the algorithm has the end goal of choosing the best overall team  the predictions need to
be accurate for players that would often be chosen
for a draftkings team  in practice  teams usually
consist of players who have been averaging more
than    draftkings points per game  meaning that
the ideal algorithm would be much more predictive
for players with higher average draftkings points 
since linear regression has much more predictive
value than the draftkings predictions at higher average points per game for players  it should work well
in providing an edge when competing with other
fantasy basketball players in actual competitions 

ii 

naive bayes

the naive bayes model used is actually a modification of the typical multinomial naive bayes algorithm
to fit the data given here  each parameter space is
discretized into k equal sized groups  where k is a
parameter varied to maximize the model  then  each
example in the training set is converted into values
from   to k   for each parameter  all examples with
the same set of values have their actual draftkings
score averaged to provide a predicted output for that
set 
the test set is then converted in the same way 
and each test example takes on the predicted output
for its set of values  this model ignored test examples whose converted parameter values did not have
a match in the train examples 
by intelligently varying k  the number of quantiles  and the number of variables  the naive bayes
models accuracy can be improved  table   shows
a table of values for test errors given different k
and variable numbers  after   variables and   quantiles  decreasing either variable further doesnt sig 

fidraft kings and queens  cs    

nificantly improve the error 
num  variables

quantiles

  
  
  
  
  
 
 

  
 
 
 
 
  
 

test error
   
   
     
     
     
     
     

a player must be assigned  the first set of factors
is a set of binary factors between each permutation
of position pairs to assure that every position has
been assigned a unique player  the second variable
is an n ary variable which limits the total sum of the
player costs to be at most          figure   shows
the detailed factor graph of the csp 

table    naive bayes accuracy as the number of discretized
states and variables are varied 

this technique gets relatively close to the error
rates seen in linear regression  however  looking
at the error as the player set is limited to players
who score more points on average shows that linear
regression has a natural advantage with better players  the ones that the algorithm needs to accurately
predict   figure   shows the comparison  in general 
naive bayes tends to perform about as well as the
draftkings predictions  while linear regression has
a significant advantage for better players 

figure    factor graph of the csp

v 

figure    naive bayes and regression error rates in comparison
with that of draftkings  versus average player score 

iv 

constraint satisfaction problem

after constructing an algorithm to predict the score
for each player  the constraint satisfaction problem
solved for the team that would produce the highest
total score  the csp consists of a set of variables
and two sets of factors to account for each constraint 
the variables are each of the eight positions to which
 

backtracking and beam search

even when assigning most constrained variables
first and enforcing arc consistency  the backtracking
search algorithm took several minutes to find the
optimal assignment from a pool of    players  the
backtracking search algorithm finds every possible
satisfactory assignment for the positions and thus
the runtime increases on the order of o n     given
that there are more that     players to choose from
each night  this algorithm would be too long to find
the optimal assignment 
implementing beam search instead limited the
search space to assignments with only the k highest
scores  this k parameter was varied to determine
how large the search space must be in order to find a
high scoring assignment  for the csp  this parameter
was limited to    since higher values produced the
same optimal assignment  although beam search
isnt guaranteed to find the optimal solution  it finds
a high scoring assignment relatively quickly as the
runtime increases on the order or o n kb  log kb  

fidraft kings and queens  cs    

where b is the branching factor  figure   shows a list
of the top three assignments for the games played
on december  th        after running beam search 

figure    player riskiness as compared with average points per
game 
figure    optimal teams for the games played on dec   th

vi 

future work

the following techniques have not yet been implemented  mostly because the data to concretely test
them is not publicly available so it would be hard to
gauge their effectiveness without investing a large
sum of money into the algorithm to see how they
actually play out in draftkings contests 
drafting the best players will generally result in
a good team  but the real money in draftkings is
paid to a tiny fraction of the players in each contest 
in a typical contest  about half of the prize money
goes to the top ten performing teams  and a contest
will have around         entrants on average  to
win  the team chosen needs to be outstanding in
comparison to others 
to that end  picking players with higher  risk  
where risk is defined as the ratio of a players standard deviation to their mean  is a strategy more
likely to win the largest prizes  this is because
riskier players are more likely to do exceedingly
well on a particular night  and also to do exceedingly poorly  but the idea would be to draft a huge
number of teams and make money back when only
one or two of them are successful   figure   shows a
graph of riskiness ratio  a players riskiness divided
by the average riskiness  versus that players average
number of points per game 

this ratio is clearly skewed to players with lower
average points per game  and would need to be adjusted so all values are closer to    incorporating
riskiness is a necessary and important future step to
optimizing a draftkings prediction algorithm 
additionally  winning at draftkings is not necessarily just about picking the best predicted team
on a given night  if the goal is to beat competitors 
its not advantageous to draft players that just about
all of the competitors have also chosen  because that
player provides the users team with no comparative
advantage  choosing less frequently chosen players
is a better strategy because the uniqueness of the
team goes up  and a more unique team that does
well is more likely to be separate from its competitors and to score in the top     data for percentage
of teams that drafted a particular player is not publicly available  but future work could explore proxies
that represent that data well 

references
 espn   espn   fantasy basketball  web     oct 
      http   games espn go com frontpage 
basketball 
 draftkings   draftkings   daily fantasy sports
for cash   draftkings  web     oct      dec 
      https   www draftkings com  
 cs      liang  perry   lecture     csps ii   artificial intelligence  stanford university  nvidia
auditorium  stanford     nov        lecture 

 

fi