extracting keywords from email data using distributed word vectors
irving rodriguez
department of physics  stanford university  stanford  california        usa
 dated  december          
current keyword extraction methods often use statistical models to select certain words from
a set of documents  which fail to take advantage of the information available in the documents
themselves  we propose a model for identifying semantic relationships between words in a document
to identify keywords that more accurately capture the meaning of the document  specifically  we
use distributed word vectors to learn hypernymy and test the relationship on the contents of emails
to deduce meaningful keywords  we find that this model is capable of producing a list of words that
contain such keywords  though more analysis is necessary on learning the hypernymy relationship
between words to quantitatively narrow this list to only those keywords 

introduction
the goal of keyword extraction is to identify the
most relevant words within a corpus  by successfully
identifying these terms  it becomes easier to condense the
conceptual information contained in these documents
into a short list of unambiguous and specific words 
this is especially useful for extracting information in
large sets of documents  like wikipedia  or very dense 
technical corpora  such as a set of scientific papers 
currently  many methods in keyword extraction rely
on purely statistical models to deduce keywords for every
document  many of the leading algorithms use a variation of the term frequency inverse document frequency
measure as a proxy for relevant terms  eq      for a
corpus with d documents  the tf idf measure td for
term wi in document d is
d
td   fraw  wi    log  pd

j   ij

 

   

where f is the frequency of the term wi in document d
and ij     if wi is in document j    otherwise  the key
assumption made by these models is that  for a given document  its keywords will likely not appear in many other
documents within the corpus  but may occur frequently
in the document of interest 
for example  in a set of documents describing nba
teams  we expect that oakland will likely occur more
frequently under the golden state warriors article
than the chicago bulls piece  and will likely not appear often in any of the other teams documents  hence
this relevant term describing the location where the warriors play would have a low value of t in eq    and would
consequently be identified as a keyword 
however  these methods fail to take into account the
rich semantic information available within the documents  and such an assumption does not hold very well
for less descriptive documents  like emails  an email
from a friend stating that he bought tickets to the warriors game may never explicitly mention basketball 
but such a term certainly captures a lot of the meaning
in the message  a persons inbox can contain hundreds

or thousands of these messages with information meaningful to the user  and can be useful for tagging future
incoming emails by content type 
in order to remedy this disparity in keyword extraction  we use distributed word vectors to learn
semantic relationships between words 
hypernymy 
the relationship between a word and a more general
term  is particularly suited for this task  we use a
large subset of the wikipedia corpus to learn vectors
for a large vocabulary of words using a neural network
implementation of word vec     the publicly available
toolkit  and we proceed to learn hypernymy by feeding
hyponym hypernym pairs  dog  animal  into a
cluster projection model that outputs a potential hypernym vector of a given word 
background
the choice of algorithm for learning word vectors
is important for understanding their use in learning
semantic relationships  as such  we will discuss their
construction in some detail 
one of the first learning algorithms for word vectors
came from bengio et  al     by using a statistical language model  we can predict which word should come
next in a sequence based on the conditional probability
given the previous words in the sequence  these words
were known as the context for a given word  that is 
to predict the next word wi   we condition on all words
prior for a sequence of t words 
p  wi    

t
y

p  wi  wit   

   

t  

bengio proposed building a neural network in order to
capture the grammatical and semantic information contained in the many contexts of a word  using a large text
corpus as an input in order to capture this information
in a vector  bengios neural network consisted of four
layers 
input layer  the input word is represented using a  v  dimensional vector  where v is the vocabulary of distinct

fi 
words in the corpus  whose only non zero entry is the ith
entry corresponding to the words order in the vocabulary 
projection layer  the word vector from the input
layer is mapped onto a real valued vector c i  using a
 v    m  dimensional matrix  c  where m is the number of features used  such that the resulting vectors from
training will be m dimensional  
hidden layer  this layer calculates the unnormalized
log probability  yi of each output vector c i using a hyperbolic tangent function 
y   tanh d   hc i  

   

where h is the h   n    m hidden weights matrix  for
h the number of hidden units and  n     the number of
words used in the context      the hidden layer serves
to compute the probability distribution over the whole of
the vocabulary 
output layer  this layer uses a softmax function to
normalize the output of the hidden layer  allowing for a
probabilistic interpretation of the vector wt  
eywt
p  wt  wt         wtn       p yi
ie

   

conceptually  the neural network learns the matrix c
which maximizes the average log probability of each word
over the corpus 
t
 x
l 
logp  wt  w t    c 
t t  

   

bengios algorithm is very expensive to compute and required weeks to train vectors of size    on a corpus of
only   million tokens    
mikolov et  al took this idea further in their first paper
and modified bengios algorithm to make word vectors a
feasible tool for language analysis  the time complexity
q of bengios neural network showed that the limiting
factor was the hidden layer      
q   n  m   n  m  h   h   v  

   

where each term is the sum is the projection  hidden 
and output layer  respectively  the hidden layer here is
highly non linear 
they provided the continuous skip gram algorithm as an alternative that cut out the hyperbolic tangent layer entirely while preserving the probabilistic assumptions behind bengios architecture    
continuous skip grams expand the context to include
both words that precede and follow the desired word as
specified by a window size  the context is also not limited
to its immediate context  and can skip a constant number
of words  as a result  the algorithm now maximizes the
average log probability of the context given a word 
l 

t
 x x
 
logp  wt j  wt   c  
t t  
cjc

   

for a context range  called the window size  of c 
the input and projection layers remain the same 
with these modifications  the neural network could learn
    dimensional vectors using corpora containing   billion tokens in a matter of hours    
most interestingly  however  the vectors could display
knowledge of various relationships using simple vector
algebra  perhaps most famously  the operation v king  v man    v queen  yields the vector closest to woman 
thus  the model could be tested using an analogy task 
where the recall of the model could be measured by its
ability to successfully return the last word in a string of
four tokens   a  b  c  d  if a is to b as c is to   and the
model can return v d   in a second paper  mikolov et  al
provide two more additions to their word vector learning
models in an effort to improve upon its accuracy 
the first is the ability to subsample from the training
corpus  because there are a multitude of frequent words
that appear during learning that do not offer much grammatical or semantic information  like the and in 
mikolov discards the word wi randomly according to the
probability
r

   
p  wi       
f  wi  
where  is a chosen subsampling threshold and f is the
frequency of the word in the corpus      words with
higher frequencies will then be subsampled more aggressively  this can decrease the training time significantly as
well as bolster the level of information available from contexts since the subsampled tokens will then be replaced
by other words that previously fell outside the window
size 
negative sampling also becomes an option during
skip gram learning  mikolov uses the noise distribution 
u  w  to randomly sample tokens from the corpus into
a noise set d  they use a unigram distribution to the
 
they weight the conditional probability
  th power 
of each word wi by  wi  wl   with a random word wl
drawn from d  to produce higher quality word vectors
can be produced with less training time     the idea is
that if the word pairs well with the context represented
by the drawn vector  the dot product will increase the
calculated probability during training  yielding a better
vector in less time 
methodology

word vectors ability to learn various relationships  like
the capital city country mapping  have motivated attempts to learn semantic mappings through vector algebra  we use similar efforts to learn a hypernymy mapping between a word and a more general counterpart 
many of the previous attempts rely simply on the difference between the hyponym vector  the less general word 

fi 
with its hypernym vector  to learn a linear mapping    
     though high levels of recall  often     or greater 
preliminary tests show that such mappings are not easily
applied to a wide variety of hypernyms  we found  for
example  that the classifier described in weeds will output animal as a hypernym of dog  but not university for stanford or vehicle automobile for car 
among others 
instead  fu et  al have shown that these vector differences form many different clusters in vector space  fig
x      this motivates an approach of using kmeans clustering in order to identify the various kinds of hypernymy before learning a linear mapping between a hyponym to its hypernym 
kmeans clustering seeks to group together n pairs
of  wo   wh   hyponym hypernym examples  respectively 
into k clusters  using the difference between the two vectors as a feature  the algorithm will initialize k centroids
in vector space and iterate through each example until
they are all grouped into the clusters according to which
centroid their difference is closest to 
we will then learn k linear mappings from hyponym
to hypernym  more formally  we hypothesize that there
exists a transformation hi for each of the i clusters that
maps a vector wo to its hypernym  wh   we are looking for
an h that minimizes the norm of the difference between
the two 
t
 x
 h wo    wh   
   
t
armed with word vectors and a hypernymy mapping 
we can identify the tf idf keywords from an email set
and map to their hypernyms in order to output a more
meaningful set of keywords 

h    argminh

experimental set up
word vector training
in order to represent words as vectors  we use half of
the english wikipedia as our training corpus  we use the
wikiextractor  available on the mediawiki website  to
parse the first half of the july      english dump     we
iteratively input each sentence into the publicly available
word vec implementation of the continuous skip gram algorithm 
the word vec model requires a parameter set specifiying the dimension of the desired vectors  the window
size of the context  and the subsampling threshold  we
determine the optimal set of parameters by performing
the analogy task described above with a provided list of
     sets of   string tokens 
hypernymy detector training
baroni et  al have compiled a list of word pairs for
various semantic relationships  including hypernymy  by

parsing for specific syntactic patterns  for example 
corpora that contain the phrase x  such as y are often indicative of hypernymy  i e  fruits  such as apples  
this data set contains      hyponym hypernym word
pairs     
we concluded that this was the best collection of semantic relationship examples for our task due to the
size  variety  and generality of the provided word pairs 
other sets included pairs that  while technically hypernyms  seemed too specific to capture the general relationship using our model  for comparison  the baroni
set includes such pairs as  swallow  bird  and  baseball 
sport      while another similar set from weeds includes
pairs like  hurry  necessity  and  emergency  juncture     
this seems to approach a level of granularity in hypernyms that would be better explored in later iterations of
the model 
we use    fold cross validation  using   subsets of the
samples for training  one subset as a testing sample of
true hypernym pairs  and we flip the order of the remaining samples to have negative test samples  since baseball
is not a hypernym of sport  
not every pair of words that will be input into
the hypernymy detector model will necessarily be a
hyponym hypernym pair  as such  we only classify pairs
whose norm difference between the potential hypernym
and the projection of the word are below some threshold
  in order to find the ideal combination of clusters and
an appropriate threshold  we use the cross validation to
test several pairs of parameters and output the average
percent of correct classification  additionally  we keep
the pair in each cluster whose vector difference was
closest to the centroid and call it the indicative pair for
that type of hypernym 
application on email data
we use the publicly available gmail api to pull     of
the most recent non spam emails from a personal mailbox       we create a vector t whose ith entry is the
tf idf value of the ith word in the email vocabulary
according to eq     we normalize the vector so that the
length of the email does not artificially affect the relative
value of td among emails with differing word counts 
we then note that a keyword can have hypernym
pairs that appear in different clusters  i e  dog animal
may lie in one cluster  but dog canine can be in another   therefore  for each cluster i  we take indicative
pair and use v knownhyponym  v knownhypernym   
v keyword  to generate a potential list of hypernyms for
the keyword  for the top    results  if  hi  keyword  
v potentialhypernym   is greater than our chosen   we
classify it as a hypernym 
for testing the accuracy of our hypernyms  we randomly chose    of the emails that outputted more than  
hypernyms and presented them to a human grader  we

fi 
parameter set
analogy task accuracy
d        c       s    e  
      
d        c       s    e  
     
d        c       s    e  
     
d        c       s    e  
     
d        c       s    e  
     
d        c      s    e  
     
d        c       s    e  
     
d        c       s    e  
     
d        c       s    e  
     
d        c       s    e  
     
table i  word vector training  performance on analogy
task with varying parameter sets  where d is the dimensionality of the vector  c the window size  and s the subsampling
threshold 

also presented them with s    the list of   keywords  and
s    the list of the top   hypernyms  we asked them
the following questions and tallied the results for each of
them 
   do you think at least   words from s  describe the
email well 
   do you think at least   words from s  describe the
email well 
   which set  s  or s    is more descriptive of the email 
results and discussion
for our word vector model  we found the optimal parameter set to be            e       table    the vocabulary size for this corpus after subsampling was        
words 
for the dimensionality  we observed that the accuracy
of the model only marginally increased with large increases in the number of features used to construct the
word vector  as the time complexity suggests  the training time is non linear with the dimension of the vector 
for d        total training time surpassed   hours  and
the run for     lasted over     this is  of course  due
to the exponentially larger number of calculations that
the model has to process during each iteration in the
projection layer  however  it is likely that there is little new information obtainable by the word vector itself 
after all  wikipedia articles are subject to very strict
editing guidelines  and the method of writing is rather
constant across all articles  the literature also suggests
that          is a sufficient number of features for a
variety of tasks  given the time constraints  we chose to
use     to test the remaining two parameters 
we discovered that a larger window size continued to
increase the accuracy of the model  while we did not
expect the accuracy to plateau  it is surprising that a
window size of    still saw a significant increase  considering that we were using     dimensional vectors and it

is hard to believe that the average wikipedia sentence is
longer than    words  especially when we already omit
most articles and pronouns  the addition of the extra
words likely means that the projection output  which averages over all of the skip gram contexts  is still getting
pushed towards a truer mean in order to increase the
probability of the context happening  this makes sense 
as there are more and more unique contexts  especially
when we are reaching out past    words in either direction  there seems to be litlte lost beyond computation
time in using a larger window size  so we stuck to    
as for the subsample threshold  we decided to use the
 e    used in the literature     though there is an increase in using a larger threshold  we fear that doubling
it may start ommiting words that provide meaning to the
contexts they are appearing in  because we are already
using such large vectors  we were conservative with this
measure  despite how heuristic the choice may seem 
in the hypernymy detector  the choice of parameters
was much more heuristic than we had hoped  the negative classification accuracy offers great insight into the
behavior of the threshold  but it is still difficult to gauge
the balance  for too low a threshold  the detector is biased toward classifying examples as negative across the
number of clusters  given that it forms too strict a distance around the centroids  however  there appears to
be more stability for a cluster size of     after    demonstrated a much larger drop in negative classification between the two tested thresholds  it is likely that  in this
case  the high number of centroids began to come arbitrarily close  which affected the classification of some of
the examples  indeed  a few sanity checks showed that
it even misclassified an animal as a type of dog  the
reverse  however  was not true for k       as many of
our sanity checks  which consisted of animals and types
of foods  were correctly classified with the last set of parameters 
the email results were less convincing  the human
judge rendered answered yes to the first question   
times  yes to the second question only   times  and s 
as the more descriptive set in every single instance save
one   upon further inspection  she admitted it was solely
because she did not want to answer all of the questions
with a single answer 
the source is likely the use of tf idf as the first
screen for keywords  further tests showed that  an email
about a club soccer teams regional tournament identified
region as the top keyword  with several other examples
sharing a similar fate  this is due to the appearance of
club soccer in almost every other soccer related email 
which is one of the identified drawbacks of using tf idf 
the model then proceeds to search for hypernyms in a
keyword space that is not actually representative of the
words in the email which hold the most meaning 
a simple remedy would be to simply identify all of the
nouns in the email as the keywords  as these likely carry

fi 
parameter set
average positive classification average negative classification
 k      threshold     
     
    
 k      threshold     
     
   
 k       threshold     
     
     
 k       threshold     
     
     
 k       threshold     
     
     
 k       threshold     
     
     
 k       threshold     
     
     
 k       threshold     
     
     
 k       threshold     
     
     
table ii  hypernym detector accuracy  the positive accuracy shows the number of positive samples that were correctly
classified  and the same for the negative  respectively 

the most information in school and professional emails 
the hypernym detector could then search for better hypernyms instead of being led astray from the start 
the human judge  however  added that the set
s  seemed to contain words that were broader than
some of those contained in s    suggesting that the detector could have indeed succeeded at identifying some
hypernyms 
qualitatively  a browse of the words classified as
hypernyms using the email keywords did identify more
general terms  for example  several of the soccer related
emails contained sport  athletics  and game in the
list of hypernyms  though these did not make the top  
used to create s   

conclusion
in short  we were able to train high quality word vectors quickly using the continuous skip gram implementation offered by word vec  we showed their ability to
handle a variety of natural language oriented tasks  as
evidenced by the high recall rate of the hypernym detector during testing  once the appropriate parameter set
was chosen 
qualitatively  we had mild success in producing a list
of more general words than initially extracted from our
email data  however  there is evidently a need for a
mechanism which can quantitatively identify such words
from among the list of false positive that our detector
could produce  this might improve upon developing a
more rigorous mechanism for choosing the parameter set
of our detector in a way that is more consistent with the
development of our algorithm and not so reliant on its

performance during testing 
in the future  the main work should revolve around exploring the hypernym space further  similar pca analysis as fu et  al could shed more light on the way that
hypernymy could be modeled so as to be consistent with
its linguistic properties  the training method successfully captured its asymmetry  but as the number of clusters suggests  there may still be more complexity in the
hypernym relationship that we do not understand  and
thus is not captured by our model 
there may also be more attainable information from
running the detector on pairs of keywords  there is 
for example  a different output by looking at the most
similar words of  soccer  tournament  than either word
individually 
eventually  the next natural step would be to build a
hypernym generator  as our detector needs a candidate
in order to identify hypernymy 
we have demonstrated that word vectors are a reliable
way of exploring the semantic properties of words  nevertheless  a careful consideration of the context in which
these words are used is necessary in building successful
and linguistically robust models 

   
   
   
   
   
   
   
   
   
    

google   
y  b  et  al     
t  m  et  al     
t  m  et  al     
y  goldberg and o  levy   
j  weeds   
e  a  et  al     
r  f  et  al     
mediawiki   
m  b  et  al     

fi