cs    project final paper

 

learning to play atari games
david hershey  rush moody  blake wulfe
 dshersh  rmoody  wulfebw  stanford

abstractteaching computers to play video games is a complex
learning problem that has recently seen increased attention  in
this paper  we develop a system that  using constant model and
hyperparameter settings  learns to play a variety of atari games 
in order to accomplish this task  we extract object features
from the game screen  and provide these features as input into
reinforcement learning algorithms  we detail our testing of different
hyperparameter settings for two reinforcement learning algorithms 
using both linear and neural network function approximators  in
order to compare the performance of these approaches  we also
consider learning techniques such as the use of replay memory
with q learning  finding that even on simple mdps this method
significantly improves performance  finally  we evaluate our model 
selected through validation on the game breakout  on a test set of
different video games 

i  i ntroduction
video games pose a challenging sequential decision making
problem for computers  video games have a straight forward
visual interface  with a large state space  they often reflect
reduced versions of real world problems  for this reason  algorithms capable of operating on this visual data and learning
to make intelligent decisions are highly desirable  as they can
potentially be generalized to more difficult  real world settings 
in this project  we consider this problem on the atari     
platform  which provides a set of relatively simple video games 
we attempt to lower the complexity of the problem by applying
intuition into the human solution to video games  by detecting
objects on the screen  we allow the learning algorithm to operate
in a reduced state space  we then experiment with multiple
reinforcement learning algorithms and function approximators in
order to determine the algorithm and hyperparameter settings that
could make best use of these features  the only inputs into our
learning system are a    x    grid of rgb pixels representing
the game screen and the current score of the game  this screen
is then processed into classified objects by a combination of
computer vision techniques and the dbscan algorithm  these
objects are then fed into one of two learning agents  either qlearning with replay memory or sarsa    which output an
action according to an  greedy policy  which is then used to
advance the game to the next state  we evaluate the model
selected during validation on a test set of atari games to assess
the models ability to generalize  this report details the results
of these experiments  and the modifications to these algorithms
that increase their performance 
we also used this project for cs     for cs     we focused our efforts on the extraction of features from the game
screen  the hyper parameter tuning of each of our reinforcement
learning algorithms  and the analysis of replay memory  for
cs     we considered additional feature extraction methods as
well as the reinforcement learning implementations themselves 
namely development of both the linear and neural network based
sarsa   and q learning models 
ii  r elated w ork
the idea of enabling computers to learn to play games has
been around at least since shannon proposed an algorithm for
playing chess in            td gammon  introduced in      

helped popularize the use of reinforcement learning   specifically
temporal difference learning in conjunction with function approximation   in enabling computers to learn to play games      
development of a general video game playing agent specifically
for the atari      was introduced in      by naddaf     
bellemare et al      formally introduced the ale framework and
established a set of baselines using sarsa   applied to tilecoded features as well as search based methods that explore the
game through saving and reloading its ram contents  planningbased methods relying on this ability have achieved the stateof the art results in playing atari games      however  we focus
here on methods that use information available to human players
and that can execute in real time  within these constraints 
defazio et al      compared the performance of linear  model free
algorithms including sarsa    q    next step and per timestep maximizing agents  actor critic methods  and others for
atari game playing  finding that the best performing algorithm
differed significantly between games  recently  minh et al        
have applied what they call deep q networks  dqn  to the
task of learning to play atari games  this work uses q learning
with nonlinear  specifically deep neural network  function approximators to achieve state of the art performance on a large
variety of atari games  the dqn avoids the potential instability
of nonlinear models used to approximate action value functions
by randomly sampling from a replay memory in performing
parameter updates and through the use of a semi stationary
target function  using these methods  the researchers were able
to effectively train a convolutional neural network function
approximator to transform image input into estimated state action
values  in this paper we consider an adapted approach to the
dqn as well as the conventional approaches using sarsa  
models 
iii 

dataset and f eatures

the state space of a game is the set of possible screens that
the game can produce  modeling this state space as a grid of
pixels is possible  however  this leads to an large and difficult
to explore state space  with sufficient computational power this
is possible  however  preprocessing these pixels provides insight
into what composes a typical game screen  in order to reduce
the state space of our game models  we elected to extract objects
from the game screen rather than using raw pixel data as learning
input  this follows from an intuitive understanding of how a
human agent would play a game  identify the objects on the
screen  their relative positions and velocities  and then decide on
an action 
a  the game state
we extract the game screen at each frame of the game using
the arcade learning environment  ale       an open source
emulation toolkit with a machine learning interface  this toolkit
provides both the raw pixel data of the screen and the reward
associated with each state  the screen is output in the form of a
   x    grid of rgb pixels  the reward function is based upon
the score of a game  and is extracted from a games ram
contents 

fics    project final paper

 

be useful for some features  the tracking algorithm is a simple
greedy algorithm that matches each entity on the previous screen
with the closest entity sharing the same label on the current
screen 
e  feature output

fig     output of dbscan clustering  where each outline color represents a
unique label 

the extracted and classified object features are processed into
a format easily usable by a linear function approximator  first 
we reduce the state space of the screen into a smaller number
of tiles in order to increase the learning speed of the algorithm 
we also reduce the derivatives from a numerical derivative to
a boolean indicating the direction of motion in the x and y
directions  we additionally include cross features that include
the relative positions and derivatives between each object pair 
as the linear function approximator is not able evaluate object
interactions without cross features 
iv 

methods

b  contour detection
the first step in extracting visual features from the raw pixel
data is identifying regions on the screen that correspond to unique
entities  in most atari games  the screen has very distinct  often
monocolored features  weve elected to use methods available
through opencv      an open source computer vision tool  to
extract these features  opencv is used to detect edges in an
image  then draw contours around distinct closed edges  we
then use information about these contours  such as their length 
enclosed area  and the average color contained in a contour as
input to a clustering algorithm in order to classify features 

a  reinforcement learning algorithms

c  feature clustering
we use dbscan clustering to classify the features on a given
screen  dbscan models clusters as points with many nearby
neighbors  we use the implementation of dbscan in scikitlearn       an open source machine learning library  we selected
this algorithm as it estimates the number of clusters while it
clusters the data  this allows it to discover new classifications
while running  which is important as new entities could appear
on the screen at any timestep 
dbscan relies on two parameters  the minimum cluster size
and the euclidean size of the clusters  weve set the minimum
clusters size to one  so that each outlier is classified into a new
category  this is important as there is often only one instance
of some entities in a game  the euclidean size of the clusters is
automatically estimated by the algorithm 
it is critical for learning purposes that feature labels be consistent from frame to frame  since dbscan is an unsupervised
algorithm  it does not guarantee that labels are consistent  in
order to address this issue  weve applied a consistency checking
algorithm to the output of dbscan  this algorithm works by
storing    examples of each label it has seen so far  and then
checking the classification of these labels every time dbscan
is run  a mapping between the most recent dbscan labels and
the time invariant labels is then constructed  which is used to
re map every label to a time invariant label 

b  q learning with replay memory

d  object tracking
the final step of the object extraction is to track objects from
frame to frame  this tracking allows us to measure the derivatives
of each object on the screen  which is an important feature for the
learning agent  tracking also allows us to differentiate between
different instances of a single entity type on the screen  which can

our review of past research     indicated that model based
reinforcement learning algorithms would likely not achieve high
performance  for example      found fitted value iteration to
perform poorly on atari games due to a rank deficiency of the
feature matrix  as such  we focus on model free algorithms here 
specifically q learning with memory replay and sarsa   
we use function approximation for both algorithms  for the
first we use both linear and neural network state action value
approximators and only linear approximators for the second 

q learning is an off policy  bootstrapping algorithm that learns
a mapping between state action pairs to the optimal q values of
those pairs  it specifically attempts to minimize the following
objective when using function approximation 
minw

p

s  a  r  s   qopt  s  a  w    r
  maxa  actions s    qopt  s    a    w   

with the following parameter update equation 
w i         w i    qopt  s  a    w    r
  maxa  actions s    qopt  s    a    w    s  a 
using a replay memory this algorithm stores a history of
 state  action  reward  newstate  tuples  and then during learning
samples from this memory according to some distribution to
make parameter updates  data is collected according to some
policy  commonly  greedy  with a limited capacity replay memory  past experiences can be replaced randomly or with a more
sophisticated replacement policy 
c  sarsa  
sarsa is an on policy  bootstrapping algorithm that learns a
mapping between state action pairs and the q value of the policy
on which it currently operates  it specifically attempts to minimize the following objective when using function approximation 
minw

x
s a r s   a 

qpi  s  a  w    r   qpi  s    a    w  

fics    project final paper

with the following parameter update equation 
w i         w i    qpi  s  a    w    r
  qpi  s    a    w     s  a 
updating these parameters results in an update to the models
policy  with sarsa    the algorithm maintains a set of eligibility traces    one for each parameter of the function approximator 
these traces determine the extent to which each parameter is
eligibile to be assigned credit for a given reward encountered
by the agent  each  value is updated per timestep by a factor
of  resulting in an exponential decay of the impact of rewards
on given parameters over time  for our experiments we use a
replacing trace  which performs the following update at each
timestep 

et  s   

et   s  if s    st
 
if s   st

d  function approximation
   linear approximation  for linear function approximation 
the models are parameterized with one value for each feature 
the output of the value approximation is then a linear combination of the present features multiplied by their corresponding
values  the parameter updates for each model are provided above
in the sections for the corresponding algorithm 
   feed forward neural network  designing higher order
cross feature templates is time consuming and depends on
domain specific knowledge  it is possible to avoid these issues
through the use of a neural network function approximator
that automatically learns relevant  high level features from the
original input representation  for example  minh et al      used
a convolutional neural network  cnn  to transform image
pixels directly into state action value estimates  their networks
generally contained a set of convolutional layers followed by a
set of densely connected  rectified linear unit  relu  layers 
we hypothesized that we could replace the initial convolutional
layers with traditional object detection and tracking methods
in order to extract basic features  and then pass those features
directly into a set of densely connected relu layers  manually extracted features are clearly less informative than those
extracted by a cnn because they make certain assumptions
about what information is actually important for playing a game 
whereas the cnn features are derived directly from learning this
information for each specific game  therefore  by replacing the
cnn with manually extracted features we expected to achieve
diminished results  though at significantly reduced computational
cost 
a complication of passing manually extracted features into a
neural network is that this demands some manner of ensuring the
neural network is invariant to the order of the input features  to
address this we assume a constant ordering of the input objects 
ensured using the extracted class and within class ids of each
object 
we developed a neural network using theano     that transforms object features representing each state into action value
estimates for each action  this is accomplished by associating
each of the output values of the network with one of the possible
actions of the agent  for a given state  taking the estimated
optimal action is then accomplished through an argmax over
the output values of the network  during backpropogation  the
outputs of other actions are not considered because they do not
contribute to the loss value 
we evaluated a number of different network structures and
activation functions  specifically  we considered networks with

 

two  three  and four densely connected layers both with diminishing and constant number of hidden units over layers  for
activation functions we considered relu  max x       leaky
relu  max x    x   with                and tanh activation
functions
e  training methods
the dqn trained by minh et al  used two primary adaptions to
the training process that they claim enabled their network to learn
effectively  first  the dqn used a replay memory  this approach
allegedly improves training by eliminating correlations between
consecutive screens and by more efficiently using collected data 
notably  using this approach requires an off policy model  which
motivated the researchers choice of q learning  we evaluate these
claims on a simple test mdp in section   
second  during training  the network uses target values derived
from static  previous values of its weights rather than its current
weights  these previous values are then periodically updated to
the current values  by doing this  the researchers claim learning
proceeds in a more stable manner 
v  e xperiments and r esults
a  replay memory
prior to training the different algorithms on atari games  we
compared their performance on a simple test markov decision
process  mdp   as well as evaluated the impact of a replay
memory and static target function for the case of q learing
specifically  we used a simple grid mdp with positive and
negative rewards on the edges  we also incorporated a trigger
location that when encountered by the agent prior to reaching
an edge would significantly increase the final reward with the
hope that this would approximate the delayed rewards present in
many atari games 
we first compared performance using different replay memory instantiations  varying memory capacity and sample size 
specifically  we compared two capacities   one hundred and one
thousand   and two sample sizes   five and twenty   as well
as a baseline model that has no replay memory  all models
operated with the same exploration epsilon value        learning
rate         and frozen target update period        each such
combination was run five times for      trials  and the run that
achieved the maximum reward from its group was selected for
visualization figures   and   
our results showed that  in the case of the test mdp  both
capacity and sample size significantly impacted the performance
of the learning algorithm  even when accounting for the increased
number of updates performed with large sample memories  figure    showing performance over a constant number of trials 
illustrates that the large capacity and sample size replay memory
seems to exhibit a much steeper learning curve  even when
compared to the smaller capacity memory making equivalentlysized updates  of course  this replay memory performs more
updates per trail than the replay memories with smaller sample
sizes  so we also compared the performance of the different
memories holding the number of updates fixed  figure    shows
performance over a constant number of updates  and again
illustrates that the q learning algorithm using the larger memory
seems to learn much more quickly than both the five sample
size update  and sequential update  i e   no memory replay 
approaches 
these findings align with the assertions of minh et al  that
a replay memory helps improve learning  they further assert
that this is the case because the replay memory eliminates
correlations between consecutive updates thereby making each

fics    project final paper

 

fig    

fig     effect of replay memory sample size on average reward with constant
number of trials  the      capacity     sample memory shown in green provides
the best performance  but also executes the most updates 

learning curves for different leraning models playing breakout 

we have tested three different values of  in sarsa    as well
as q learning with replay memory 
the learning curves displayed in figure   clearly show that
sarsa   learning with         has the best performance
on breakout  sarsa   is a well suited model for breakout  as
rewards for breakout are heavily delayed from the actions that
create the rewards  a high value of  successfully assigns credit
to actions that preceded rewards 
one can gain insight into the learned solution by looking at the
weights associated with different feature combinations  figure  
shows a visualization of these weights in the form of a heatmap 
based upon these weights the ball being in the same x location
as the paddle is desirable  as well as the ball being higher on
the screen  this closely corresponds to human intuition for the
game breakout 

c  model evaluation on general atari games

fig     effect of replay memory sample size on average reward with constant
number of updates  the      capacity     sample memory shown in green again
provides the best performance even though it performs the same number of
updates as the other two memories shown 

update more informative  to evaluate this claim  we collected
both the samples used in making parameter updates and those
encountered sequentially during the execution of a q learing
model  i e   those that would be used to update parameters when
not using a replay memory  with a       capacity replay memory on the atari game breakout  we then performed principal
component analysis  pca  over both sets in fifty item batches
and compared the fraction of the variance explained by the
first principal component for each set  averaging over       
episodes  we found that the first principal component of the
sequential updates could explain       of the variance in the
fifty item batches  whereas the first principal component of the
sampled updates could explain only       of the variance in the
fifty item batches  these values indicate that the replay memory
samples are less correlated than those used in sequential updates 
b  model validation on breakout
we use the atari game breakout as the validation game on
which we would select the best hyperparameters for the model 
breakout was selected for its relatively simple gameplay  delayed
rewards  and demonstrated high performance in related work     

in order to test our hyper parameter turning  we conducted
learning experiments on three other atari games  asterix  chopper command  and space invaders  the learning performance of
these games can be seen in figure   
the learning algorithm did not perform as well on the test set
as it did when training on breakout  this is most likely caused
by the increased complexity of the test set  as every game in the
test set has a larger state and action space than breakout  there
is evidence of learning only in the game chopper command 
which has easy to extract features and fairly simple interactions
between features  asterix has a very busy screen state  which
makes feature extraction difficult and this causes slow and suboptimal learning  space invaders is a more complex game than
breakout  and the resulting state space is far harder to explore
fully 

d  neural network function evaluation
despite numerous experiments with different network structures and hyperparameter settings  we were largely unable to
achieve above random results with the neural network function
approximator  we significantly departed in setup from the extant
literature  and this made it more difficult to troubleshoot the
issues we encountered  the opaque nature of neural network
hidden layer features also made it difficult to assess whether any
progress was or could be made in learning directly from the
extracted object features 
  heatmap

images generated by rush moody

fics    project final paper

 

fig     heatmap showing optimal q value attainable for each position of the ball holding position of the paddle fixed  dark red is associated with the largest q
values and dark blue with the lowest  the assumed paddle position in the above image is left  center  right respectively   

fig    

learning curves for asterix  chopper command  and space invaders 

vi 

c onclusion and f uture w ork

in this paper we evaluated a set of reinforcement learning
models and corresponding hyperparameters on the challenging
task of general video game playing  we first found that  testing
on a simple mdp  replay memory sample size and capacity
significantly impact the performance of q learning  we next
showed that sarsa    paired with simple linear function
approximation and object extraction methods  can effectively
learn a simple atari game with significantly above random
performance  and that q learning with replay memory and a
static target function achieves worse performance  furthermore 
we have showed that higher  values tend to correspond to faster
learning for this application 
the algorithm was not able to to generalize effectively to
other games with potentially more complicated screen states or
objective functions  the method of extracting objects from the
game screen is very dependent on stable  fast extraction of visual
features  and for more complex games a more robust system
would be needed to achieve success with this method 
neural networks were not as effective for function approximation as a simple linear function approximator  the approach of
extracting a variant number of objects from the screen is difficult

to translate into an effective input to a neural network  and adds
significant complexity to the computation  future efforts could
investigate different input and layering schemes for the neural
network to test its effectiveness with different parameters 

fics    project final paper

   
   
   
   
   

   
   
   
   
    
    
    

r eferences
marc g bellemare et al  the arcade learning environment  an evaluation platform for general agents  in 
arxiv preprint arxiv                  
james bergstra et al  theano  deep learning on gpus with
python  in  nips       biglearning workshop  granada 
spain       
gary bradski et al  the opencv library  in  doctor
dobbs journal               pp         
aaron defazio and thore graepel  a comparison of
learning algorithms on the arcade learning environment 
in  arxiv preprint arxiv                  
xiaoxiao guo et al  deep learning for real time atari
game play using offline monte carlo tree search planning  in  advances in neural information processing
systems        pp           
justin johnson  mike roberts  and matt fisher  learning
to play  d video games  in     
volodymyr mnih et al  human level control through
deep reinforcement learning  in  nature                 
pp         
volodymyr mnih et al  playing atari with deep reinforcement learning  in  arxiv preprint arxiv          
       
yavar naddaf et al  game independent ai agents for
playing atari      console games  university of alberta 
     
fabian pedregosa et al  scikit learn  machine learning in
python  in  the journal of machine learning research
           pp           
claude e shannon  programming a computer for playing
chess  springer       
gerald tesauro  temporal difference learning and tdgammon  in  communications of the acm             
pp       

 

fi