an application of machine learning to native advertisements
kevin grogan  quinlan jung

abstract
machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content  four different classifiers are employed  logistic regression  poisson naive bayes  multinomial naive bayes  and
support vector machine  the features are selected using a threshold based on the mutual information statistic  additionally  the
tf idf and tf bns transformations are applied to the training set  principal component analysis is examined as an a priori feature
reduction algorithm  the reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classifier to an unacceptable level  the tf idf and tf bns transformations are
found to successfully improve the quality of the classifiers  the logistic regression classifier is shown to produce the lowest test
error of all the classifiers examined 
   introduction
native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience  ideally  the advertisements are as fun
and informative as the websites core content  however 
they are often intrusive and decrease the likelihood of repeat viewings  therefore  advertisers would greatly benefit
from a model that could determine which content is an advertisement and which is not  tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website  additionally  identification
of key parameters that betray advertisements would help advertisers develop more organic content 
hence  this project applies machine learning algorithms
to classify articles as native advertisements or core content  for this project  the naive bayes  support vector machine  svm   and logistic regression algorithms as classifiers are employed  additionally  principal component analysis  pca  to examine its utility in feature reduction 

it is faster and easier to optimize      other popular kernels 
such as the string kernel  show positive results on modestly
sized datasets  for larger documents and datasets  approximation techniques need to be used because non linear kernels are computationally expensive     
   data set and features
the training set consists of         html pages with
class labels provided by kaggle      since the training set
consited of raw html  the following algorithm is used to tokenize the training set 
for each html document 
extract the text in the html body 
stripping away tags
remove all punctuation
remove all stop words
merge similar words

naive bayes and support vector machines have been the
traditional approaches to solving text classification problems  when the dataset is extremely large  logistic regression is sometimes used to approximate an svm because it is
computationally more efficient     
naive bayes performs moderately well in text classification  of the different event models  the multivariate
bernoulli performs well with small vocabulary sizes  but
the multinomial usually performs better at larger vocabulary
sizes  providing on average a     reduction in error over the
multivariate bernoulli model at any vocabulary size     
svms consistently achieve good performance on text
classification tasks  outperforming many existing methods      because most text classification problems are linearly separable  a linear kernel is usually recommended  as

while embedded javascript and links in the html head
along with punctuation in the body could provide more insight to determine whether a document is an advertisement 
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement  hence  this project
focuses on the text in the body 
stop words such as a  and  and the  are removed as
an a priori method of feature reduction  additionally  words
with the same roots were grouped by detecting common suffixes such as  ly and ing  implicitly  this assumes that
the covariance between these words is sufficiently high that
the dimensional reduction will have a negligible effect on the
bias of the models 
the feature set  or dictionary  is the vocabulary found in
the training set  the collection of tokenized training documents is given to a countvectorizer    and it returns an n  m
frequency matrix  a  where n is the dictionary size  and m
is the number of documents  the entries ai j denote the frequency of the word at index i found in document j 

email addresses  kgrogan stanford edu  kevin grogan  
quinlanj cs stanford edu  quinlan jung 
cs     final project

  a python class in the scikit learn library that converts a collection of
text documents to a matrix of token counts
december         

   related work

fihowever  the bounding of this metric indicates that it may
underpredict the contribution of a feature to the classification
of a data set  although  for large feature sets such as the
text classification used for this project  mutual information
is posited to be a reasonable and computationally efficient
criterion to eliminate features and reduce the variance error
for a classifier 

     term frequency inverse document frequency
term frequency inverse document frequency  tf idf  is a
heuristic metric used in text classification to yield better results with classifiers  the functional form of the tf idf statistic used for this project is
tf idf i j   tf i j  idf i
 
n
 
  log     ai  j    log
ni

   
   methods

where n is the size of the training set  and ni is overall frequency of word i in the training set  tf idf makes the reasonable assertion that the importance of a word in a document must be monotonically dependent on the frequency of
that word  i e   the tf statistic   however  this term should
be discounted when it is omnipresent in the corpus  i e   the
idf statistic   therefore  words with little embedded meaning and mostly syntactical relevance  such prepositions  conjunctions  and articles  are neglected by the classifiers under
this transformation 

     principal component analysis
principal component analysis seeks to reduce the dimensionality of a feature set  this is done by finding a unit vector u that maximizes the variance in the data when projected
onto the vector  formally  this can be shown to be given by
u   argmax ut u  

where  is the covariance matrix  through the method of lagrange multipliers  this relation can be demonstrated to yield
the eigenvector corresponding to the maximum eigenvalue
of the covariance matrix  u   vi   i   max          n   
hence  an eigenvalue decomposition of the covariance
matrix gives a basis of eigenvectors  where the variance of
the projection onto each eigenvector is in proportion to the
corresponding eigenvalue 
furthermore  dimensional reduction of the training data
may be obtained by a projection onto the eigenvector basis 

     term frequency bi normal separation
while tf idf is the most widely used representation for
real valued feature vectors for text classification problems 
idf is oblivious to the training class labels and naturally
scales some features inappropriately  alternatively  idf can
be replaced with bi normal separation  bns   which has been
previously found to be successful at ranking words for feature selection filtering      the functional form of the tf bns
statistic used is
tf bnsi j   tf i j  bnsi
  log     ai  j     f    tpr   f    fpr   

   

u kuk  

bk   vktc 

   

where bk  rkm is the reduced representation of the
columns of matrix c  rnm to k  n dimensions  and
vk  rnk is a matrix of the first k principal eigenvectors 
for this project  the matrix c is built from the tf idf transformation of the frequency matrix detailed by eq     and the
rows of c are normalized to have zero mean and unit variance 
additionally  since the initial set of features yield a covariance matrix with o       elements  sparsity is enforced using
the cauchy schwarz inequality

   

where the true positive rate is given by tpr   p word   positive class   the false positive rate is given by fpr  p word
  positive class   and f   is the inverse normal cumulative
distribution function 
     mutual information
mutual information provides a metric to assign an importance a given feature to the classification of the document  it
is defined as
x x
p xi   y 
 
   
mi xi   y   
p xi   y  log
p x
i  p y 
x       y     




cov ci   c j  
i j   

 

for

 cov ci  c j    
var ci  var c j  

o t w 

 d

   

where ci   c j are row vectors in c  and d is a threshold parameter set to      this parameter to yield a matrix with a density
just so that the matrix could be manipulated in a practical
manner on a laptop 

i

rewriting eq    using jensens inequality and applying the
relation p xi   y    p y xi  p xi    the mutual information of a
feature  xi   is shown to be bounded by
 
 
p y xi  
 
   
mi xi   y   log e
p y 

     logistic regression
a logistic regression assumes that the conditional probability of the labeling follows a bernoulli random variable
 i e  y x    bernoulli     by defining the hypothesis
function as the expectation of the conditional probability 
h  x    e y x    and assuming a linear relation between
the feature vector and parameter vecotor    the hypothesis
function is correspondingly found to be given by the sigmoid
function 
 
 
   
h  x   
   exp t x 

where the expectation is taken over the joint probability distribution of xi and y  this furthers the intuition that the mutual information is a bounded and normalized metric related
to the expected contribution of a feature to the successful
prediction of a class label over simply the probability of the
class label 
 

fiwhere the parameter vector is found directly through a maximum liklihood estimate  mle   the mle of the logistic
regression is found using batch gradient ascent where the
learning rate is found empirically after several trials  stochastic gradient ascent was attempted  but the parameter vector
was found to be unacceptably uncertain 

linear svm was employed  but it proved to be too slow due
the high cardinality of the training set  the svm was modified to use a dual coordinate descent method     that defines
the primal problem as follows 
minm  i

     naive bayes
the naive bayes algorithm utilizes bayes rules to predict
a label given a feature set 
p x y     p y 
 
p y     x    p
y      p x y p y 

s t   x   x 
t  i 

p x y   

t  i 

em
i

    

 i m  i

where
     m and m     if y   m  m    
if yi   m  this method reaches an  accurate solution in
o log     iterations  similar to what is done with naive
bayes  tf bns transformation is performed on the vocabulary
found in the training set  the svm is set to have parameters
c        and         the optimal c and  are found using
k fold cross validation  where k      
em
i

   

where the prior distributions p x y      and p y  must be
modeled  the hypothesis function takes the form h  xi    
p y     x  for the naive bayes algorithm  additionally  the
prior p x y  is assumed to be conditionally independent 
n
y

x
 x
  m       c
i
  m
i

   discussion
p xi  y   

    

i  

     feature reduction using principal component analysis

the prior p y  is modeled using a binomial distribution 
while p x y  is modeled using a multinomial distribution or
a poisson distribution 
a poisson distribution is considered under the rational that
the number of appearances of a word in a fixed length document is well approximated by a poisson process  the prior
for a poisson distribution is given by

as described in sec       principal component analysis reduces the dimensionality of the feature set by maximizing
the variance of the projection of the data onto a set of basis
vectors  figure   shows the effect of the reduced feature set
on the test and training errors 

  
p xi  y   j   

ixji exp i j  

test pca  test

    
xi  
where i j is e xi  y   j   additionally  it can be shown that
eq    can be written in the form of a sigmoid function given
by eq     making implementation straightforward  the parameter vector in this case is given by

pn
y


log y     i   i   i  for k    
k   
    

log k   k   
o t w 
where y   p y   the mle of i j is given by the sample
mean of each feature corresponding to the labeling y   j 
assuming a poisson distribution increases the computational efficiency of the classifier but also embeds stronger
assumptions into the model  the comparative performance
of the classifier will be examined in the sec    

  

   

   

   

k
figure    the effect of the reduction in dimensionality through pca on
the test error  the error is normalized by that of the full rank representation
of the data set  error is found using the logistic regression classifier and
k folds cross validation 

the figure shows that error is significantly increased when
the dimensionality of the data is reduced through principal
component analysis  hence  while pca reduces the feature
set  the substantial increase in bias overshadows any reduction in variance  additionally  pca is an unsupervised learning algorithm that is agnostic to the class labels  hence  the
belief that a reduction of dimensionality through this algorithm will provide lower test error may be pollyannaish 
the threshold to enforce the sparsity may have been overly
aggressive to yield a successful implementation of pca 
however  the reduction of this threshold would have been
impractical for this project  additionally  the normalization

    

subject to the constraints 
i     i           m

 

 

m

s t  y i   t x i      b      i   i           m

  

 

     support vector machine
the svm attempts to find the maximum margin hyperplane that separates the dataset in a higher dimensional feature space  finding this optimal margin reduces to solving
the following convex optimization problem 
x
 
min  b         c
i
 
i  

  

    

where i allows for the slack in the event that the data is
not linearly separable  originally  an unoptimized l  norm
 

fi   

 

which

care

stock

un

tonight

sum

digits

cu

cialis

it

now

she

eat

auto

resolution

tue

fi

valencia

pentru

for

would

reply

ca

jan

empire

au

pdt

maddow

spectral

but

world

less

program

universe

factors

skiing

mysteries

quilts

barefoot

us

love

same

youre

el

cake

nj

nicht

monopoly

greenpeace

is

still

united

trending

limit

divorce

roasted

delhi

lager

integers

contact

great

months

probably

permanent

exposed

initiatives

occupational kimchi

sequels

on

down

women

happens

religion

tale

swing

extends

volts

at

year

photo

yes

walking

atlanta

dough

instrumental hostility

jahre

this

through

image

hair

medium

laugh

divide

ashes

dashboards

 

 

 

tf

  

have

 

  

     
  

   

bribery

luglio

   

 

 

 

   

idf

figure    depiction of the tf idf transformation  words are colored and by the tf idf value given by eq    

of the feature vectors may have reduced the useful information contained within causing some of the high training error observed  however  not normalizing the data would have
certainly reduced the clustering of the data diminishing the
efficacy of employing pca 
as an interesting aside  inspection of the top words corresponding to different eigenvectors seem to align with the
suspected orgin of the article  for instance  words with a
british spelling such as organisation and stabiliser corresponded to the third principal eigenvector  hence  pca may
have more promise as a clustering algorithm 

    
   

train
test

    



    
    
    
    

     effect of the tf idf transformation
the effect of the tf idf transformation discussed in sec     
is shown in fig     as illustrated in the figure  the tf idf
transformation naturally selects words with a high relevance
to the document  words such as this and have are shown
to have a high idf  while the term frequency of the high
idf terms are document dependent  interestingly  cialis
is shown to have a large tf and idf in this matrix indicating a high importance  considering the connotation  this term
likely appeared in an advertisement  additionally  the tf idf
transformation is found to reduce the test error by     for
the logistic regression classifier 

    
   

true positives
true positives   false positives

   

   

   

   

   

   

   

tthres
figure    comparison of the test and training error as a function of the
threshold parameter based on the mutual information  error is determined
via k folds cross validation  error is shown for the logistic regression classifier  the tf idf transformation is applied 

the mutual information on the error  the abscissa of the figure corresponds to the threshold parameter t 
t xi   y   

     application of the tf bns transformation to multinomial
naive bayes
originally  the tf idf weighting scheme was used on features of multinomial naive bayes  however  the positive predicted value  ppv 
ppv  

   

mi xi   y   min j mi x j   y 
max j mi x j   y   min j mi x j   y 

    

where all features such that t   tthres are disregarded 
as shown in the figure  the error first reduces for an increasing threshold  i e   as more features are eliminated   and
experiences a minimum near ttres        this is likely due
to the decrease in the variance as features are reduced  subsequently  the error begins to increase as bias error likely
becomes dominant  note that at high thresholds  training error and test error becomes similar indicating a reduction in
variance  hence  the metric of mutual information is shown
to reduce the test error in a computationally feasible manner 

    

is    as it is unable to predict even a single native advertisement correctly  this is because idf is oblivious to the
class labels in the training set  which can lead to inappropriate scaling  upon switching to tf bns  an increase of average
ppv to       where n         is observed 

     comparison of the classifiers

     feature reduction through mutual information
as discussed in sec       the mutual information of a feature quantifies the usefulness of a feature in predicting a classification  figure   shows the effect of feature reduction via

a comparison of the test and training errors for the classifiers used in this project is presented in fig     the logistic regression is shown to yield the lowest test error of the
classifiers at the highest cardinality of the training set  i e  
 

fi   

  



    

  

    

    

  
  

logistic regression
naive bayes  multinomial
naive bayes  poisson
support vector machine

      
  

 

  

feature set 
the tf idf transformation successfully augments words
that are pertinent to a document while discounting those
that are superfluous 
the tf bns transformation showed an improvement over
tf idf in the ppv metric when applied to the multinomial
naive bayes model 
feature reduction through the mutual information metric improved the test error of the logistic model 
the logistic regression is found to yield the best training
error of the classifiers used in this project 

additionally  the authors note that this report is not a native advertisement as determined by the logistic regression
classifier 

 

  

m
figure    comparison of the test and training error as a function of cardinality of the training set m  solid lines denote the test error while dashed
lines denote the training error  the errors are deduced through a k folds
cross validation 

   acknowledgment

m   this is posited to be due to the generality and therefore  adaptability of the classifier  the logistic regression
is a generalization of the naive bayes models and it is not
unexpected that it performs better since the parameters are
selected to directly maximize the liklihood function 
the poisson naive bayes model is shown to perform the
worst of the classifies  this likely results from the strong
assumptions imposed by modeling each feature as a poisson distributed random variable  particularly for small cardinalities of the training set  the statistics are insufficiently
converged to provide a meaningful classifier  and the error
is near      however  the poisson naive bayes is found to
be the most computationally efficient model performing   
faster than the logistic regression 
the support vector machine is not shown to generalize
particularly well for this problem  while the training error
is quite low  zero in some cases   the test error is moderate
  between that of the logistic regression and the multinomial
naive bayes  this is likely due to the classifier emphasizing
a few support vectors that determine the decision boundary 
all classifiers are shown to require at least      training
examples to begin showing a regular slope in with respect to
the test error  in this regime  the logistic regression is shown
to have the steepest slope and can be seen to be approximately first order 

references

the authors would like to thank rakesh ramesh for his
early contributions to this project 

    http   www stumbleupon com 
    https   www kaggle com c dato native data
    andrew mccallum and kamal nigam        a comparison of event
models for naive bayes text classification  in proc  of the aaai   
workshop on learning for text categorization  pages      
    j  zhang  r  jin  y  yang  and a  hauptmann  modified logistic regression  an approximation to svm and its applications in large scale
text categorization  in twentieth international conference on machine
learning  pages              
    t  joachims  text categorization with support vector machines  learning with many relevant features  in claire nedellec and celine rouveirol  editors  proceedings of the european conference on machine
learning  pages         berlin        springer 
    http   www svm tutorial com         svm linear kernel good textclassification 
    h  lodhi  j  shawe taylor  n  cristianini  and c  watkins  text classification using string kernels  in t  k  leen  t  g  dietterich  and v 
tresp  editors  adavances in neural information processing systems 
pages         cambridge  ma        mit press 
    s  sathiya keerthi  s  sundararajan  kai wei chang  cho jui hsieh 
chih jen lin  a sequential dual method for large scale multi class linear svms  proceedings of the   th acm sigkdd international conference on knowledge discovery and data mining  august             
las vegas  nevada  usa 
    george forman  an extensive empirical study of feature selection metrics for text classification  the journal of machine learning research 
           

   conclusions
native advertisements are an effective strategy to market to consumers without damaging the core experience of
viewing a website  it is posited that the quality of native
advertisements may be improved by using a machine learning model to differentiate advertisements from core content 
such would provide a metric for the quality of the advertisement  hence  several machine learning algorithms are
applied to the detection of native advertisements and the following conclusions are drawn 
   principal component analysis showed lackluster performance as a means to reduce the dimensionality of the
 

fi