using spectral clustering to sample molecular states and pathways
surl hee ahn   a  and johannes birgmeier   b 
  

chemistry department  stanford university  stanford  california        usa
computer science department  stanford university  stanford  california       
usa
  

molecular dynamics  md  simulations offer a way to explore the conformational state space of large  biologically relevant molecules  our sampling method  called concurrent adaptive sampling  cas   utilizes md
simulations by letting a number of walkers adaptively explore the state space in consecutive steps  walkers
start in one conformational state  execute a short md simulation and thus end up in a different state  one
of the properties of cas algorithm is that the number of walkers explodes quickly as the full state space of
the biomolecule is explored  hence  we use a technique called spectral clustering in order to cluster similar
walkers whenever there are too many walkers in the state space  after clustering  multiple similar walkers are
replaced by a lower number of walkers within each cluster  this results in a large speedup of the sampling
algorithm 
i 

introduction

exploring the state space of biomolecules using md
simulations is a relevant problem in areas such as drug
design  but how does one exactly start to explore the
state space of a biomolecule  we first look at few important degrees of freedom that the biomolecule has  dubbed
as collective variables  and use them to describe the state
space  for instance  when we study amino acids  we often
look at the two dihedral angles  and  and plot their
ramachandran plots  which tell us the possible conformations and their probabilities  see fig    as an example 
however  existing simulation methods are either too
slow or too inaccurate to deliver valuable information
about large molecules  accelerating methods that reveal information about the state space while preserving accuracy is therefore an important problem  hence 
our goal is to accelerate one sampling method called
the concurrent adaptive sampling  cas  using clustering techniques  both of which will be described in detail
in section ii  specifically  we use the cas algorithm

to explore the state space of penta alanine and clustering techniques to accelerate the exploration process 
the cas algorithm adaptively creates microstates corresponding to certain conformations by running many short
simulations  or walkers  since there will be numerous
microstates after the simulation runs for a while  we
use clustering techniques to cluster the microstates into
macrostates and reduce redundant microstates within
a macrostate  in summary  the input to our algorithm is the set of microstates created from the
cas algorithm simulations  we then use spectral clustering to output predicted macrostates 
or clusters of microstates  and reduce the overall
number of microstates and associated walkers for
computational efficiency 
we describe a simplified version of the cas algorithm
in in section ii a because the project does not make sense
without its description 
ii 
a 

methods
concurrent adaptive sampling

the basic outline of the cas algorithm is as follows 

fig     structure of penta alanine and its ramachandran
plots based on its three middle  and  dihedral angle pairs 
the most probable to least probable conformations are colored from red to white as shown in the colormap  figure
taken from ref    

a  electronic
b  electronic

mail  sahn  stanford edu
mail  jbirgmei stanford edu

   a circular microstate or ball is a collection of similar molecular states of the biomolecule  the ball
is named so because it contains all states that are
within some fixed radius r  also  the balls centers
values are equal to the collective variables values 
in our case  the collective variables are the six dihedral angles  so our ball is a six dimensional sphere
with a center equal to the dihedral angle values  as
we describe the algorithm  it will become clear that
balls are indeed microstates that help us structure
the  infinitely large  state space  all microstates
have the same radius r 
   a walker is an abstract notion of a tiny worker
that starts in a particular conformational state  executes a very short md simulation of length  from
that state and ends up in a different conformational

fi 
state  all walkers reside in balls  which will be explained later 
   a walker also has a weight or probability of ending up in the state that the walker is currently in 
hence  the sum of the weights of all currently existing walkers is always   
   to start the cas algorithm  place a target number of walkers nw in the initial state of a given
biomolecule  for example  if nw is set to be    
each walker in the initial state will have a weight of
  since the sum of all weights is maintained to be
   this target number of walkers nw will be used
for subsequent steps  which will be explained later 

or equilibrium weights of the balls  the second eigenvector represents the probability changes of the balls in the
slowest non stationary process  we then use the normalized second eigenvector to cluster balls using k means 
and this is done because we would like to cluster balls
that are similar dynamically  this is described more in
detail in section iv 
after clustering  we downsample the number of walkers
in each cluster in the following way  each cluster receives
the same  fixed number of walkers nwc   this number nwc
is set to be significantly lower than the average number of
walkers in each cluster  if there are more than nwc balls
in any cluster  then only nwc of the balls are used  which
are picked randomly  so that one walker lies in each ball
and the rest are deleted 

   to start sampling  loop through the following ad
infinitum or timeout 
iii 

 a  for each walker that currently exists  execute
a short md simulation of length  such that
the walker walks from its current state to
the next state 
 b  after the walkers have finished walking   for
each walker  check if they ended up within an
existing ball  for any walker that resides outside any ball  create a new ball around the
walker  that is  create a ball centered around
the first walker and then check to see if the
next walker lie within the ball  if not  then
create a new ball for the next walker and so
on 
 c  after the ball construction  loop through each
ball and check to see if there exists a target
number of walkers nw in each ball  if there
are more or less walkers than nw   then merge
and or split walkers to end up with nw walkers  each walker will end up with weight equal
to the mean weight of the walkers in the ball
and this method is called resampling    it is
essential for the sampling algorithm to maintain a target number of walkers nw in each
ball to constantly observe visited states irrespective of their energy barriers 
b 

spectral clustering

as the number of balls grows  the number of walkers or md simulations also increases and computing one
step of the simulation becomes computationally expensive  hence  we perform a clustering of balls after the
number of balls hits a certain threshold  initially  we create a transition matrix of the existing balls and calculate
their transition probabilities using their weights and previous and current coordinates  then we perform eigendecomposition on the transition matrix of the balls and
obtain the normalized second eigenvector  which is the
second eigenvector normalized by the first eigenvector 

related work

developing accelerated sampling methods for md has
been an ongoing research problem and several methods
currently exist     however  most of these methods alter the real kinetics of the system and therefore  we are
unable to find statistically correct pathways and intermediates from using these methods 
hence  different methods have been developed to preserve real kinetics of the system  one method is the
weighted ensemble  we  method developed by huber
and kim  which is the main method that the cas algorithm is built on top of    in particular  the cas
algorithm is one of the adaptive versions of the we
method     
another method is building markov state models
 msms   which divides up the state space into small microstates and runs a large number of short trajectories
to compute transition probabilities between microstates 
overall slow reaction rates  and other kinetic and thermodynamic properties     however  there is no technique
to efficiently build an msm a priori   the microstates
need to be small and the lag time  needs to be long so
that transitions are markovian but not too long so that
short lived transitions are not captured  in msms  geometric clustering methods are used to build an msm
and perron cluster cluster analysis  pcca  is used to
identify metastable states of the system  our clustering
algorithm  however  aims to cluster microstates based on
their dynamic similarity rather than geometric or energetic similarity 

iv 

dataset and features

we always cluster balls  not walkers or any other entity in the system  the adaptive sampling algorithm previously described generates the dataset that we use for
clustering balls  the input to the sampling algorithm
is very small  it consists only of the initial state s  of
the molecule were simulating  i e   penta alanine  for the

fi 
sampling method  however  by letting the walkers explore the state space  in which every state is represented
by six different dihedral angles  the data needed for clustering is generated  this data consists of 

      
      
      
      
     
      

 the coordinates of a large number of balls after
running adaptive sampling for a number of steps 

      
      
      

 the transition matrix t   which is calculated from
the transitions of walkers between balls in two consecutive steps  e g   if the net flux between balls
    and     consisted of walkers with weight      
then the entry tij         this transition matrix
corresponds to the graph laplacian matrix l described in       
the largest eigenvalue of the transition matrix is always    its corresponding eigenvector reveals the distribution of weights among balls such that after performing
any number of simulation time steps  the distribution
stays exactly the same  this is called the equilibrium
state 
the second largest eigenvalue of the transition matrix
is less than but close to    its corresponding eigenvector
reveals the distribution of weight changes among balls
such that after performing a large number of simulation timesteps  the system will converge towards the first
metastable state  because we are interested in preserving
as much information as possible about the convergence
towards the first metastable state  we cluster based on
the second eigenvector values  however  since the second
eigenvector values differ in magnitudes with each other 
we normalized the second eigenvector values by dividing them by their corresponding first eigenvector values 
hence  the normalized second eigenvector is one of our
features  clustering states based on values in the eigenvectors is classical spectral clustering 
after performing spectral clustering using the normalized second eigenvector  we discovered that very distant
states in the state space would often get clustered together simply because they have similar normalized second eigenvector values  therefore  we also tried expanding our feature set to cluster based on both ball coordinate information and the normalized second eigenvector  we present results based on clustering only with
the normalized second eigenvector  and clustering with
both normalized second eigenvector and ball coordinate
information 
our final feature set therefore consists of 
 the six ball coordinates that form the center of
a ball  for balls   to n  resulting in a matrix of
dimensionality n    
 the normalized second eigenvector  of dimensionality n    
the full feature matrix therefore has the dimensionality n     this feature matrix is used in k means clustering as described in section ii 

   
     balls            balls            balls      
     balls   
     balls      
     balls   
clusters     ps   clusters     ps   clusters     ps   clusters     ps  clusters     ps   clusters     ps  

fig     average dunn indices for clustering based on normalized second eigenvector and ball coordinate information
in various configurations  higher is better  for clustering at
a     balls threshold  the    ps timestep led to a timeout
after    hours before clustering     balls was reached  so we
repeated the experiment with a    ps timestep  clustering at
    balls threshold with   clusters performed worst  but clustering at     balls threshold with   clusters performed best 
we do not show the silhouette scores separately because they
agree with the dunn index 
      
       
      
       
      
       
   
     balls            balls            balls            balls            balls            balls      
clusters     ps  clusters     ps  clusters     ps  clusters     ps  clusters     ps  clusters     ps  

fig     average dunn indices for clustering based on normalized second eigenvector only in various configurations  higher
is better  surprisingly  the configurations that give the best
and worst performance are the same as in fig     this holds
even though clustering was performed on different simulation
replicas  apart from that  it is hard to see trends in clustering
performance 

v 
a 

experiments results discussion
performance

running the sampling algorithm for    hours in various
configurations  which we did repeatedly during development  typically resulted in a timeout at approximately
step       

b 

clustering and outliers

to assess the performance of the clustering methods 
we implemented both the dunn index and silhouette
scores for the clusters  fig    and fig    contain a comparison of the clustering performance in various configurations based on the dunn index 
we also used the silhouette score during spectral clus 

fi 
tering to determine the quality of clustering  it turned
out that sometimes the clustering result was heavily influenced by a few outliers  in these cases  the outliers
would end up in a couple of tiny clusters  and a single
huge cluster would contain the bulk of balls  we observed
that when this case occurred  the average silhouette score
was always higher than      using this as a threshold  we
sought to mitigate the outlier problem in the following
ways 
 we performed outlier detection using an elliptic
envelope with the goal of removing    of the data
as outliers  afterwards  we would perform spectral
clustering as usual on the inliers  the sklearn
implementation relies on the robust estimation of a
non singular covariance matrix of the data  however  the covariance matrix used for outlier detection often turned out to be singular  so we could
not perform outlier detection  failing to remove
outliers and proceeding with the highly imbalanced
clusters degenerated the performance of the cas
algorithm with spectral clustering 

fig     clustering results based on the normalized second
eigenvector using k means with five clusters 

 therefore  we turned to a simpler solution whenever outlier detection failed  forego spectral clustering for this step even though the number of balls
is above the pre set threshold  instead  wait for
one more timestep and perform spectral clustering
afterwards if the clustering results are not influenced as heavily by outliers anymore  this strategy
was mostly successful  even though the additional
timestep often took a lot of time to compute 
c 

a view on the clusters

in all of the following figures  the colors correspond to
the different clusters  we show only clustering results
after using k means with three or five clusters  hence 
there are three or five colors in each plot  when showing
clustering results  we are always showing a plot of the
first two dihedral angles  and   a ramachandran plot  
except for the pca plot 
clustering based on the normalized second eigenvector
resulted in a messy clustering  as seen in fig     however  it is important to keep in mind that all states in one
cluster share similar dynamical properties  even if those
are not readily visible  clustering based on both ball coordinate information and normalized second eigenvector
results in a somewhat more sensible clustering as seen in
fig     in particular  it is visible that all states in the upper left corner of the ramachandran plot are somewhat
similar in both dihedral angles and normalized second
eigenvector values 
one of the reasons that the clustering results still look
very jumbled to the human eye is that we can show only
two of the seven dimensions used for clustering in one
plot  when clustering is based on the first two ball coordinates and normalized second eigenvector values  the

fig     clustering results based on both the normalized
second eigenvector and ball coordinate information using kmeans with five clusters 

fig     clustering results based on both the normalized second eigenvector and the first two out of six ball coordinates
using k means with five clusters 

fi 

fig     clustering results from fig    projected onto the first
two principal components of the feature matrix 

fig     distribution of balls after clustering based on the normalized second eigenvector with three clusters  immediately
after downsampling the number of balls 

ered better results than clustering based on the normalized second eigenvector values only  however  the trends
regarding which cutoffs and which number of clusters to
choose still remain somewhat unclear  clustering based
on the normalized second eigenvector values preserved
the dynamics of the system  but often clustered states
in very different dihedral angle conformations together 
thus reducing the accuracy of the simulation  because
the simulation timestep was comparatively short  some
of the transition matrices were noisy  resulting in outliers when clustering  outlier detection using an elliptic
envelope helped mitigate this problem in some cases 
to validate and improve the results  the main tasks
would be as follows 
fig     distribution of balls after clustering based on the normalized second eigenvector with three clusters  immediately
before downsampling the number of balls 

clustering results look much more pleasing to the human
eye  as seen in fig     however  clustering in fact works
just as well on seven dimensions  to provide a visualization of the seven dimensions usually used for clustering 
we performed pca on the feature matrix and plotted
the clustering results on the first two pca dimensions 
as seen in fig     here  it is evident that clustering does
quite a good job of separating balls that differ greatly
with respect to eigenvector values and dihedral angles 
to visualize the effects of spectral clustering and subsequent downsampling of balls  we show a comparison
of the number of balls before  fig     and after  fig    
spectral clustering and resampling 

vi 

conclusion future work

in summary  clustering based on the normalized second
eigenvector values and ball coordinate information deliv 

 run all simulations with a timestep much larger
than    ps  such as         ps  to obtain better
estimates of the transition matrix 
 run all simulations for a much larger number
of steps  because of limitations on the sherlock
computing cluster  sherlock stanford edu   we
could run all simulations for at most    hours 
which severely restricted the number of timesteps
we could use  to calculate more meaningful statistics and evaluate the clustering results vs  nonclustering controls  running for a much larger number of timesteps is paramount 

acknowledgments

we thank prof  eric darve for bringing up the topic of
spectral clustering in the context of weighted ensemble 
we thank our friend ella kim for mental support during long clustering sessions  we thank kilian cavalotti 
who was instrumental in getting sklearn to run on the
sherlock computing cluster 

fi 
  h 

feng  r  costaouec  e  darve  and j  izaguirre  a comparison of weighted ensemble and markov state model methodologies  the journal of chemical physics                    
  t  schlick  innovations in biomolecular modeling and simulations  royal society of chemistry        
  a  laio and m  parrinello  escaping free energy minima  proceedings of the national academy of sciences               
       
  d  hamelberg  j  mongan  and j  mccammon  accelerated
molecular dynamics  a promising and efficient simulation method
for biomolecules  the journal of chemical physics           
             
  y  sugita and y  okamoto  replica exchange molecular dynamics for protein folding  chemical physical letters            
       
  g  huber and s  kim  weighted ensemble brownian dynamics
simulations for protein association reactions  biophysical journal               
  b  zhang  d  jasnow  and d  zukerman  the weighted ensem 

ble path sampling method is statistically exact for a broad class
of stochastic processes and binning procedures  the journal of
chemical physics                    
  d  bhatt and i  bahar  an adaptive weighted ensemble procedure for efficient computation of free energies and first passage
rates  the journal of chemical physics                    
  a  dickson and c  b  iii  wexplore  hierarchical exploration of
high dimensional spaces using the weighted ensemble algorithm 
the journal of physical chemistry b                      
   j  adelman and m  grabe  simulating rare events using a
weighted ensemble based string method  the journal of chemical physics                    
   g  bowman  v  pande  and f  noe  an introduction to markov
state models and their application to long timescale molecular
simulation  springer science   business media        
   u  luxburg  a tutorial on spectral clustering  statistics and
computing                   
   a  ng  m  jordan  and y  weiss  on spectral clustering  analysis and an algorithm  advances in neural information processing
systems                  

fi