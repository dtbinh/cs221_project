socialsense  quantifying social media linkage across the web
joel shackelford  alexander spizler  yang xu
  jshack     aspizler   yxqc      stanford edu
abstract we present socialsense  a quantification of
social media linkage across the web  we utilize content
sourced from monthly crawls of the entire web to predict
the category of links likely to dominate a website 
the web data is conditioned into a set of interesting
features based on intuition of the data items and ease
and availability of common attributes  these principals
may also be applied to explore the darker side of
intent by identifying nefarious and potentially malicious
links  this is done through the aid of various learning
algorithms and implementations  in addition  we address
the computational expense of the algorithms tested  their
performance and the implications of the results 

i  introduction
socialsense is about characterizing web linkage 
specifically websites that are likely to contain links to
facebook  while demonstrating extensibility to more
insidious and malicious linkage in a computationally
efficient manner  the common crawl corpus     a
nonprofit dedicated to education and prosperity through
an open web  crawls the entire web monthly and
maintains this massive data set for open use  we parse
our input features from http interactions and metadata
associated with crawling a given page  we also take
advantage of amazon web services  aws  to interact
with this data set through cloud based queries  aggregation tools such as mapreduce and cloud computing
instances like ec   we show that the application of
several common machine learning algorithms  such
as logistic regression and support vector machines
 svm   produces a marked differentiation between
websites with social media linkage and those without 
ii  related work
while other work has focused on identifying web
intent based on user expectations or user intent    
socialsenses methodology uses keys and features associated with the underlying implementation of the
site  http headers and retrieval statistics  other work
    focuses more on brand safety  ad safe concerns 
maximizing audience relevance and contextual targeting while also listing malicious detection of urls

as an advertised feature  here we focus strictly on
the intent of a given site and leave application and
monetization to future work  there is a vast amount
of research and development in the area of natural
language processing and several tools             that
focus mainly on text processing and sentiment analysis
to judge intent  while this remains a future area of
study for socialsense it was not intimately explored 
    categorizes web intent  but their intellectual property was not available for us to explore  to define truth
in pursuit of quantifying malicious intent we leveraged
blacklists of known suspect urls      while there
is work in this area  current methods rely solely on
known malicious links  socialsences goal is to use
features of the website other than the links themselves
to understand the fingerprint of a malicious website 
iii  dataset and features
data storage and processing takes place through
aws  therein we use java      python  and hadoop
to pre process the data  i e  extract a feature vector
from each website  it is therefore important to first
define an entry in our data set  the common crawl
repository  from where we glean all our data  is constructed by capturing http requests  their responses
and the associated metadata  this data is stored in
warc     format and is identified by a universally
unique identifier uui       the http responses and
metadata represent data supplied to a querying browser
and are the main concern for us  we therefore key our
training and test data on this same uui 
in order to simplify the massive amount of data
contained in a crawl of the entire web we condition
the data to parse only the features in which we are
interested  initial attempts focused on items that were
of minimal effort to parse and collect  these features
included  retrieval time  expiration and page size as
given by the http response as well as total number
of links on a page summed from the warc format 
as the proficiency of working with the warc files
in the aws ecosystem increased we were able to infer
additional information  such as the presence of amazon

fiads and the extension of google apis  e g  google
maps  google sign in   two additional features  server
type and domain extension were trivial to ascertain  but
took some manipulation to provide as inputs into the
learning algorithms discussed below  the difficulty lie
in the fact that we required individual binary features 
but there was an indeterminate set of values  to solve
this problem we took a general history over a broad
range of samples and found the most commonly used
values  apache and nginx were common server types 
but there were on the order of thousands of different
types  unbenounced a priori  as a reasonable solution
we chose the    most common server types which
identified more than     of our data 
below is a table of all features extracted from the
common crawl web data  note that not all features
in table i are considered in all plots in subsequent
sections 
features
unique site id
number of page links
page size
contains amazon adds
extends a google api
contains expiration
domain extension
server type 
retrieval time
text vectorization 

description
string
integer
integer
boolean
boolean
boolean
boolean     
boolean    
integer
boolean     

of y for any one training example as
p y x       h  x  y     h  x   y

and maximize the likelihood of the m independent
samples  since the parameters that maximize a function
are the same for the log of a function  we can solve for
the log likelihood  written as
     

m
x

y  i  log h  x i          y  i    log    h  x i     

i  

taking the gradient of     with respect to  and setting
it to zero  for each j we get

 i 
       y  h  x i    xj
j

finally  the method we use to maximize     is stochastic gradient ascent  we iterate until convergence  updating each j in  given the old value of j   the above
equation  and a learning rate   which controls the step
size for each iteration  this is given by
 i 

j    j    y  i   h  x i    xj  

once we solve for   we make predictions for y given
new values of x using equation     
for svms  we want to find a separating hyperplane
by optimizing

table i

min w b

f eatures extracted from each warc file

s t 

iv  methods
for this binary classification problem  we apply our
own implementation of logistic regression as well as
svm with different kernels available in lib svm      
given i              m websites  with j              n
features extracted from the website  and a feature
vector  x   n     where x       we want create a
classifier that predicts whether or not a new website
links to facebook  y       
in order to do this  we can use the sigmoid function
as our hypothesis function 
h  x    g t x   

 
    et x

   

where if h  x        then y is most likely    and most
likely   otherwise  to do this  we define the likely value
 

features not vectorized in time for inclusion in this paper

   

 
  w   
 
y  i   wt x i    b     

   
i              m

where w and b are parameters of the hyperplane    w   is
the orthogonal distance between the support vectors and
separating hyperplane  and  is the geometric margin 
by applying the kernel trick  we can then map high
dimensional feature vectors to a much lower dimensional space  allowing us to use a higher dimensional
hyperplane for binary classification 
v  experiments
in order to find the best binary classification algorithm for this problem  we run logistic regression
and svm on        samples taken from one slice of
the common crawl     we use simple cross validation 
training each classifier on the first     of the samples 
of the samples         of training data have links to
facebook while        do not  the testing data has
similar proportions at        and        
from the testing results  we count the number of
true positives  tp   false positives  fp   true negatives

fi tn   and false negatives  fn   we then calculate the
accuracy  recall  and precision  defined as
tp   tn
   
tp   fp   tn   fn
tp
recall 
   
tp   fn
tp
   
p recision 
tp   fp
here  defining sites that link to facebook as relevant 
accuracy is the fraction of samples that are labeled
correctly  recall is the fraction of all the relevant sites
labeled as relevant  and precision is the correctness of
a result labeled as relevant 
for logistic regression  we need to choose two
values  the learning rate    and the threshold for
convergence  we select an initial value of  to be      
then  after every   iterations  we reduce  such that
        this gives a better estimation than using
a constant large  and a quicker estimation than a
constant small   we define convergence to be when
  old  new              through trial and error  we
found that this set up converges to the same results
more quickly than running additional iterations at each
learning rate or having a lower threshold for the l 
norm of the change in  
we also produce plots for the area under the curve
 auc  and the area under the precision recall curve
 auprc  by varying our classification threshold between   and    and measuring the tp  fp  tn  and fn
rates  for the auc we can plot the tp rate versus the
fp rate  and for the auprc we can plot the precision
versus recall as the threshold varies  this allows us
to see the effects of different values of the threshold 
allowing users of this algorithm to select a threshold
based on their priorities  e g  choosing to find nearly
all websites that link to facebook despite having many
false positives 
with svm  to obtain better prediction accuracy  we
first use the data scaling tool provided by lib linear to
scale our feature data  since some of the features have
significantly higher values than others  data scaling
allows the maximum value for all features to be the
same  improving our prediction accuracy compared to
using unscaled features  additionally  we assure that we
have equal numbers of positive and negative samples
for both testing and training data 
we then run svm using the various kernels available
in lib svm  listed in table ii below  we also use the
lib linear classifiers to test the performance of other
accuracy 

standard classifiers  we use the default parameters for
both lib svm and lib linear 
classifier name
lib svm 
linear
polynomial
radial basis function
sigmoid
lib linear 
l  regularized logistic regression  primal 
l  regularized l  loss support vector classification  dual 
l  regularized l  loss support vector classification  primal 
l  regularized l  loss support vector classification  dual 
support vector classification by crammer and singer
l  regularized l  loss support vector classification
l  regularized logistic regression
l  regularized logistic regression  dual 

id
t 
t 
t 
t 
s 
s 
s 
s 
s 
s 
s 
s 

table ii
lib   linear and lib   svm classifiers

vi  results
training our logistic regression classifier on the first
    of samples yield the following confusion matrix 
table iii  which compares the classification of testing
samples against their targets  here positive  p  values
represent predictions and outcomes where a site links
to facebook  while negative  n  values do not  the accuracy of logistic regression is         while the recall
and precision are        and        respectively 

true value

predicted
p
p
    
n
    
total     

outcome
n
total
         
         
    

table iii
c onfusion matrix for logistic regression

we next calculate the auc and auprc in order to
understand how different thresholds affect tp and tn
rates  precision  and recall  both plots in figure   show
a knee in the curve when the threshold is close to     
the default threshold for logistic regression 
next  we measure the testing and training errors as
the sample sizes increase in figure    for each data
point  we take three random collections of samples
from the full data set  and calculate the mean and
standard deviation for error      accuracy   recall  and
precision 
we are also interested in the values at which each
parameter converges  table iv shows the weights of

fifig     plot of auc  top  and auprc  bottom   non monotonic
decrease at the beginning of the auprc is possibly due to precision
of small numbers 

several of the more influential parameters for logistic
regression 
finally  we look at the accuracy  recall  and precision
of the various svm kernels provided in lib svm  t  t  
the results for the methods in lib linear  s  s   are also
shown in figure   
vii  discussion
despite lower than desired accuracy  we are able
to distinguish to some degree which sites are likely
to link to facebook  by evaluating the values of our
parameters in table iv  it is clear that the number of
links on a page has the most influence by far  indicating
that the more links a page has  the more likely it is
to link to facebook  other weaker indicators  such as
retrieval time and having a  com domain  show negative
correlation with linking to facebook 
the performance of our logistic regression classifier
is poorer than desired  with        error  while a
linear svm performs similarly  the best performance 
with l   regularized logistic regression  still has       
error  in order to understand how we could improve
performance in the future  we look at the testing and
training accuracy versus sample size  if training error
were much lower than testing error  we might suspect
that the classifier was overfitting to the data  however 

fig     plot of error  top   recall  mid   and precision  bottom  for
training  blue  and testing data  red  as the sample size increases 
notice that all three converge at the largest sample size

because testing and training error converge  we believe
the classifier has a bias problem 
viii  conclusion
with the features available through the common
crawl corpus  we are able to differentiate between
sites with links to facebook and those without  the
strongest indicator by far is the number of links on a
site  where sites with more links are more likely to link
to facebook  one explanation for the strength of this
feature  is that sites with many links  such as news sites
and blogs  often also have facebook pages to which
they link 
logistic regression has similar performance to

fiparameter
x   offset 
number of links
page size
contains amazon ads
extends google api
contains expiration
domain
 com
 org
 net
 gov
 edu
 mil
 int
 co
 ac
retrieval time

value
       
        
      
     
     
      
      
      
     
     
      
     
     
     
     
       

table iv
parameters and their values for logistic regression

fig     plots of accuracy  top   recall  middle   and precision
 bottom  of svms with various kernels and linear classifiers on
training and testing data  method definitions can be found in table
ii

svms  at near     accuracy  the results of our classifiers are biased  and improvements could be made
by either adding features or selecting more informative
features 

that represents a           word vocabulary with a
    dimensional array per word  this array is based
on copious english text from multiple news sources
over time  in an attempt to gain additional contentrich features we could sum these values for every word
on a given page  normalize this vector by accounting
for page size  and include this as a feature in logistic
regression and svm 
an extension to the current approach to defining
truth in the malicious realm is to match a link to a list
of known suspect urls  future endeavors could also
utilize unsupervised learning and clustering to avoid
the postmortem analysis and even prewarn users and
content distributors when suspicious activity is logged
over time 
additional gains may be realized by optimizing the
data conditioning and machine learning algorithms
execution in the aws cloud  given the cost associated
with every could compute cycle and the size of the data
set we deal with  this optimization would be requisite
if heavy future use is predicted 
acknowledgments

ix  future work
as previously mentioned  our final results left socialsence with a small bias problem and less than
desired accuracy  one correction for high bias is the
addition of features  one fertile ground for intent mining is analysis of a websites raw text  google provides
pretrained vectors as part of its word vec package

we thank professor andrew ng and the teaching assistants for their work in cs    and for the opportunity
to build this system  we also recognize the common
crawl foundation and their dedication to a truly open
web as well as amazon for providing educational and
research grants 

fir eferences
    improving semantic consistency of web sites by quantifying
user intent
    https   developer similarweb com website 
categorization api
    http   saplo com technologies
    http   www datumbox com 
    word vec 
https   code google com p 
word vec 
    https   zvelo com 
    http   www malwaredomainlist com 
    common crawl corpus  http   commoncrawl org 
    cc main                      ip             ec  internal warc
     iso iec             information and documentation 
warc file format
     leach  p   mealling  m   and r  salz  a universally unique identifier  uuid  urn namespace  rfc
      doi          rfc      july       http   www 
rfc editor org info rfc    
     http   cs    stanford edu materials html
     hicklin  j   boisvert  r   et al  jama  a java matrix package 
http   math nist gov javanumerics jama 
     internet storm center  suspicious domains  https   
isc sans edu suspicious domains html

fi