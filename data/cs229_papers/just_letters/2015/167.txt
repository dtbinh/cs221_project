dont feed the trolls 
insult detection in online communities
matthew volk  arun kulshreshtha  and stephanie tsai
stanford university  cs    project

 

introduction

online comments have changed the way that internet
users engage with online content  they allow users to express their beliefs  provide criticism  and engage in discussions  rather than just passively read or watch online content 
however  the anonymous nature of online comments can
sometimes cause users to be more readily hostile towards
others indeed  most internet users have likely encountered
hateful or insulting comments  these kinds of comments
create an unwelcoming environment that is harmful for the
communities in which they are posted  as such  it is in the
interest of website owners to identify and remove insulting
comments 
typically  this is accomplished by having moderators
manually review comments  or relying on users to report insulting content  however  this is inefficient and doesnt scale
well due to the need for human input  in this project  we
explore automating the process of identifying insulting comments 
in particular  our goal is to build a machine learning system that takes as input a comment  and classifies this comment as insulting or not insulting  this system will be trained
on a set of training corpora that include known insult and
non insult comments  and then tested on another set of comments whose labels  insult or non insult  are also known in
order to measure its effectiveness 
we start with a simple baseline model using naive
bayes  and build on top of it by incorporating several improvements  including laplace smoothing  stupid backoff 
and threshold variation  in addition to naive bayes  we
explore using several other classification schemes on top
of naive bayes to achieve better results  this is done
by first building a feature vector for the comment that includes things like similarity scores between the comment and
clean insulting subsets of the training set  the sentiment analysis score for that comment  the frequencies of various parts
of speech  and so on  we then experiment with using several classification algorithms  namely  logistic regression 
svms  and random forests  to classify the comment  by
combining several feature extraction and classification techniques  we were able to obtain reasonably good classification
accuracy 

 

related work
there have been prior studies on using machine learning
to classify online comments  the most common approaches
seem to involve using classifiers such as logistic regression
or svm on a feature vectors derived from computing several
metrics about the words in each piece of text to be classifier  in      yin et al  used an svm based classifier along
with features extracted using tf idf word frequencies  sentiment analysis  detecting curse words  and looking at parts
of speech  particularly pronouns  to automatically identify
online harassment  a similar approach was employed by
chavan et al in      who used a similar set of features  namely
tf idf and n grams  combined with svm and logstic regression  likewise  dinakar et al      use tf idf  n grams 
and basic sentiment analysis using a pre built dictionary of
negative words and phrases combined with an svm and decision trees 
overall  it appears that the state of the art is to start with
a bag of words representation of the online comments  and
to use tf idf and n grams based approaches for selecting
features  combined with classifiers such as svm  most common  or logistic regression 
some other approaches focused more on sparse vectors
of words or patterns  for example  in      razavi et al  combined several flavors of naive bayes with a prebuilt dictionary of text patterns indicative of online flaming  by using a
multi level classification approach  they were able to achieve
promising results 
in an older piece of work by spertus      the author relies on manually specified language based features  such as
the presence of second person pronouns followed by a noun 
and runs the resulting feature vectors through a decision tree
based classification system  this approach does not seem as
common in newer literature  but we explore the use of decision trees  particularly random forests  in our project while
using the sorts of features more typical of newer studies 

 

dataset and features
we obtained our data set through a kaggle competition
     and generated features using nlp techniques we thought
appropriate from relevant literature 

fi   

data set
our data set consists of       comments scraped from
online forums  these are pre labeled as    insulting comments  or    neutral comments   it came pre split into a training set of       neutral comments and       insult comments
and a test set of      neutral comments and     insult comments  as a pre processing step  we parsed these raw text
snippets into vectors of individual words  splitting on whitespace and punctuation  an example of each type of comment
is below 
insult  oh  you are such an idiot     you just confirmed that
you cant read     dumb rse 
neutral  you get the gold star the best post ive seen on
here in months  hilarious   great job canadian ps i think
we need you to come down here ill sponsor you  
   

features
we used several features in an svm  a logistic regression model  and a random forest  these features are each the
result of their own algorithm and or preprocessing  so a more
detailed description will be given in the methods section  the
table below lists the features with a short description of each 
feature name
naive bayes
pos tagging
tf idf
sentiment
misspellings

description
nb log probabilities
fraction of nouns  verbs  adjectives
term freq inverse document freq score
sentiment analysis score
fraction of misspelled words

 

methods
the following is a discussion of the methods we employed in our project  it falls into three main sections  first 
we implemented and improved upon the standard naive
bayes algorithm  second  we defined a more complex feature set  including part of speech based and tf idf based
features  third  we ran the resulting feature sets through
three different models  an svm  logistic regression  and a
random decision forest 
   

naive bayes
we used the unoptimized algorithm as a baseline 
specifically  the algorithm generates two probability distributions of words in the neutral and the insult training corpora by dividing the frequency of individual words by the
total number of words in each corpus  it then iterates over
the test set and sums the log probabilities of each word in the
comment to generate an insult score and a neutral score  it
then classifies the word based on which score is higher 
 i 
formally  let x i  be the ith training comment  x j be the
jth word in that comment  and y i  be the class  neutral or insult  that x i  belongs to  neutral and insult are the two class
priors  and k neutral and k insult are the probabilities of individual words k given that y i  is neutral or insult  respectively  following from this  we use the following equations

in our model 
 i 

k insult  

i  j   x j   k  y i    insult 
i  j   y i    insult 

log pinsult  x i     log insult    log 
j

 i 

x j  insult

and similarly for the neutral comments 
      nb improvements
we added several optimizations to the naive bayes
baseline      ignoring words not found in either training corpus      using laplace smoothing over both probability distributions to create a single combined vocabulary      removing stop words      implementing the stupid backoff optimization  and     using threshold variation 
for laplace smoothing  we ran the model with several
different laplace smoothers and settled on the value of     
after plotting the outcomes  we also removed stop words  the
most common words in the english language  like the  a 
and is  because these appear commonly in both language
models and as such arent particularly discriminatory 
we also implemented stupid backoff      an optimization for naive bayes that tends to generalize well with large
training corpora  this model looks at n grams instead of just
unigrams  our implementation generates probability distributions for bigrams and trigrams  as well  using frequency
counts with laplace smoothing  then  without loss of generality  to calculate pinsult  x i     we iterate over all indices j
 i 
 i 
in the sentence x i    generating unigrams u j    x j   bigrams
 i 

 i 

 i 

 i 

 i 

 i 

 i 

b j    x j   x j      and trigrams t j    x j   x j     x j      we
skip the bigrams and trigrams that run past the end of the
training examples  then  we calculate an insult score and a
neutral score for each sentence by adding the class priors to
the sum of    i 
as before  just for a new definition of  
x j  insult

that takes into account the trigram and bigrams 

 i 
 i 

 p t j  insult  if t j in insult lm
 i 
   i 
    p b i 
j  insult  else if b j in insult lm
x j  insult

  
 i 
  p u j  insult  otherwise
effectively  this tries to use trigram probabilities first  given
that they are most specific  then backs off to a discounted
 by       in our model  bigram probability if the trigram is
unseen  then backs off one last time to a doubly discounted
unigram probability 
our last optimization was to run threshold variation  in
particular  instead of classifying a test example in whichever
class had a higher calculated score  we introduced a weight
factor  to vary the threshold  meaning we now compare
log pinsult  x i    and   log pneutral  x i     we chose the value
of  that maximized our f  score  note that i adopt a different notation than in the cited paper  as this notation is simpler
for our particular application 

fi   

non nb features
we then moved on to generating a more rich feature
set  taking the two score generated by our optimized naive
bayes implementation as the first two features  we also generated the following   feature classes 
      part of speech based features
one of the first language based features we tried experimenting with part of speech tagging  using the partof speech tagger provided by the ntlk  natural language
toolkit  a python nlp library   we tagged each word in every comment with its part of speech  we then bucketed the
parts of speech into nouns  verbs  and adjectives  and used
those counts to calculate the fraction of each part of speech
in our comments  we added the fraction of nouns  verbs  and
adjectives to each comments feature vector 
      tf idf
another feature we wanted to use was to measure the
similarity between a given test comment and the insult and
clean training corpora  ordinarily  this could be done by simply using some similarity metric  say  cosine similarity   and
computing this between the test comment and each comment
in the training set  however  this is not likely to be effective
because the bulk of the similarity will be due to words that
are common in the english language  which are likely to be
common in both the clean and insult corpora 
to work around this  for each of the training corpora we normalized the word frequendies using tf idf
 term frequency inverse document frequency   a normalization strategy that adjusts each words frequency count by the
invervse of that words frequency across both corpora  ensuring that the influence words that are common in both corpora
will be reduced      the formula used for computing the
tf idf score of a token t is as follows  where ft c denotes
the frequency of the given token in a given corpus  either
clean comments or insults   n denotes the number of tokens
in both corpora  and nt refers to the number of occurences of
the term across both the clean and insult corpora 
t f  id f   ft c   log    

n
 
nt

using these normalized word frequencies computed for both
corpora  we defined two new features that consisted of the
sum of the tf idf normalized frequencies of each of the
words that appear in the comment being classified  we compute this for both the clean and insult corpora  the idea is
that whichever of these scores is higher should be indicative
of which class the comment belongs to  as will be seen in
the results section  this feature proved to be very helpful 
      sentiment
since insults and inappropriate comments may be more
likely to use negative words than neutral comments  we decided to look at the sentiment of the comments  we used a

builtin python sentiment classifier     that gave each comment a score between    and   depending on how negative
or positive the words it contained were  we then used these
numbers in our sentiment feature 
      misspellings
we believe that insults and inappropriate comments will
have more misspelled words than neutral comments will  so
we compute the fraction of misspelled words a comment has
and use this as one of our features  we use the enchant
spellchecking library      to check if our words are misspelled  the resulting feature was the fraction of misspelled
words in that comment  fraction instead of raw count to normalize for comment length  
   

models
we used three models  svm  logistic regression  and
the random forest 
      svm
the first classifier we tried using our new feature vectors was an svm  as our research indicated that this was one
of the most commonly used and promising classifiers in this
area  we used the implementation of svm provided by the
scikit learn python library       using the default parameters 
the implementation of svm in the library solves the following primal problem 
minimize
w b 

n
   
w w  c  i
 
i  

subject to yi  w   xi     b      i and i   
by default  this implementation uses a gaussian kernel
function  however  after exploring the literature on the subject  it appears that linear kernels are more commonly used
for text classification due to the large number of features involved  and upon trying both  we found that the linear kernel
significantly outperformed the gaussian kernel for our purposes  as such  for our project we are using a linear kernel
with our svm 
      logistic regression
we tried using a logistic regression model next because
we saw based on the learning curve in the result section that
our svm model had high variance  we tried this model  using the dual logistic optimization problem with l  normalization  in an attempt to minimize the high variance  specifically  the objective function it sought to optimize was 

subject to    i  c  i           l  where c     is a penalty
parameter learned by the library and q was a matrix defined
by qi j   y i  j  j  x i   x  j   

fiwe used the logistic regression library provided by
scikit learn to do this       which in turn uses the liblinear
library  from which we took the dual optimization problem
      we chose the dual optimization problem  per the advice
in the logistic regression library documentation  because
we had substantially more training examples than features 

discussion
the following is a disclosure of the results of our various
models and our analysis of them 

before we attempted to feed our data into various classification algorithms  we wanted to visualize our feature space to
ensure that our choice of features was reasonable  since our
feature vectors have many dimensions  in order to make a
scatter plot of our data we reduced the dimensionality of our
features to   dimensions using principal component analysis
 pca   at a high level  pca works by finding the projection of the data that maximimzes the variance between data
points in the reduced space  if the pca plot groups the insult
and non insult comments into distinct clusters  then we believe we can be reasonably confident that our chosen features
would enable a classification algorithm to correctly classify
the data 
upon running pca on our dataset  we obtained the
above plots  it appears that there is only one small point
at which the insult comments are located  if we zoom in on
those data points  as seen in the second plot  it appears that
the insult comments actually form a very small line upon being projected into this lower dimensional space  this preliminary result is promising as it suggests that we should be able
to classify the dataset somewhat accurately using our choice
of features 

   

   

      random forest
the last model we tried was a random forest classifier 
we believe each of our features can be important in classifying comments  but we also believed random forest could
algorithmically determine which features are most important
better than we could  random forest helps solve that problem  since at each step in one of its iterations  it picks a random subset of features to try  thus  it will eventually pick a
subset of features that performs best in a tree classifier 

 

bias and variance

results of different models
below we show our precision  recall  and f  scores of
our different models  and analyze why they performed as
they did  our key metric was the f  score  we decided
to take an aggressive approach to removing harsh content 
meaning we weigh getting all insults about the same as we
weigh incorrectly classifying a neutral comment 
     

the graph pictured above shows our training and validation
scores after running our svm with a gaussian kernel  we
can see that our data does exhibit some variance  since more
training samples seems to help  because of this  we chose logistic regression and random forest models for our next two
models  as these can help reduce variance  we noticed significant improvements in our models after making these decisions  as described in our results 
   

picturing the data

naive bayes results
feature added  cumulative 
baseline
ignore unseen words
laplace smoothing
remove stop words
stupid backoff
threshold variation w o sb
threshold variation w  sb

precision
     
     
     
     
     
     
     

recall
     
     
     
     
     
     
     

f  score
     
     
     
     
     
     
     

the baseline performed quite poorly for several reasons  but
primarily because it did not handle unseen words well  resulting in many log probabilities of    once we ignored
these words  we saw a substantial improvement  however 
instead of ignoring all unseen words  we decided to laplace
smooth across both language models   adding words seen
only in insults with very low probability to the neutral vocabulary and vice versa   this also helped  allowing us to use
these highly discriminatory words that were once skipped 
next  we tried to remove stop words  thinking it would
remove nondiscriminatory words  but it proved to not help
here  most likely because some of the comments are quite
short  such that removing any context was not helpful  the
last major modification was to implemented stupid backoff as described in the methods section  though it did not
help at first  as it only tends to work well with large corpora  to generate enough bigram and trigram samples   we
found that once we introduced threshold variation it could be

fimore discriminatory  in particular improving the recall substantially  more concretely  the combination of these two
features helped us reduce the number of false negatives by
    with a substantially smaller increase in the number of
false positives  we believe this to be because higher order
n grams capture more context  in particular helping discern the purpose of modifier words like very in cases like
very good and very bad  whereas very might previously have had a slight probability towards either end  very
good can have a strong neutral score and very bad can
have a strong insult score 

     

svm results
feature added  cumulative 
naive bayes  with stupid backoff 
part of speech tagging
tf idf
sentiment
misspellings

precision
     
     
     
     
     

recall
     
     
     
     
     

f  score
     
     
     
     
     

after incorporating all of our new features  the svm
performed better than naive bayes in terms of precision 
but performed slightly worse in terms of recall  resulting in
an overall higher f  score  of our features  it appears that
tf idf normalized similarity scores were the most helpful 
causing the largest increase in precision  recall  and f  score
of all of the features used  this makes sense because these
scores emphasize words that appear more commonly in one
corpus set or another  and in particular  since one would expect the frequency of curse words to be much greater in the
insult corpus  one would expect that this would result in tfidf scores would have a reasonably high accuracy 

     

logistic regression results
feature added  cumulative 
naive bayes  with stupid backoff 
part of speech tagging
tf idf
sentiment
misspellings

precision
     
    
    
     
     

recall
     
     
     
     
     

f  score
     
     
     
     
     

logistic regression performed substantially better than
the svm did  in both precision and recall  we attribute this
primarily to the use of l  regularization  we believed our
model was overfitting based on the learning curve shown in
section      l  regularization helped ameliorate this problem
by adding a penalty term to the objective function that penalized models with large weights  forcing the algorithm to be
more conservative in the learned weight vector  this translates into a smoother decision boundary  preventing overfitting 
in terms of individual features  though sentiment analysis and part of speech fractions seemed to have introduced
noise into the model instead of helping it  using tf idf
scores seemed to help this model substantially  as before 
tf idf helped due to the fact that we expect certain terms
to appear far more frequently in the insult corpus than in the
clean comment corpus 

     

random forest results
feature added  cumulative 
naive bayes  with stupid backoff 
part of speech tagging
tf idf
sentiment
misspellings

precision
     
    
     
     
     

recall
     
     
     
     
     

f  score
    
     
     
     
     

our random forest does the best  with every additional
feature improving the scores  in the bias variance graph
above generated from our svm scores  we can see that svm
exhibits higher variance that bias  random forest helps remove this variance without increasing bias by its bagging
step  which could explain its outstanding performance 

 

conclusion future work
the highest performing model from this study was the
random forest run on the full feature set we built up throughout the paper  we had four major numerical results      we
improved the baseline naive bayes classifier f  score from
      to        we then used the nb scores as features in
a larger feature set including nlp features such as tf idf
scores  sentiment scores  and part of speech tag fractions 
we got f  scores of           from an svm            from
logistic regression  and           from a random forest 
we expected all three models to outperform our naive
bayes implementation  as these models had more features to
work with  we believe the random forest performed the best
because it was able to discern most effectively which features mattered  as it randomly generates decision trees  note
that the random forest was the only model to monotonically
increase f  score with new features  the other models sometimes regressed as we added features  most likely because
they added more noise to these models than help  further 
though logistic regression and svm performed similarly  we
attribute the slight edge the svm had to the fact that its decision boundary is motivated only by the points close to it
 the support vectors  whereas logistic regressions decision
boundary is affected by all examples  in this particular application  that means we hypothesize that very clearly insult
  neutral comments were skewing the logistic regression decision boundary such that it misclassified things close to the
decision boundary  whereas the svm was able to avoid this
problem 
given more time  we would have liked to move in three
directions      there were several other promising nlp features we read about in literature that we didnt have enough
time to implement  in particular  we would have liked to
use syntax trees to build more structure out of the comments      we would also have liked to try using variancereducing methods on our svm  because it exhibited some
variance that we we didnt mitigate  if fixed  its entirely
possible that the svm could outperform the random forest 
    lastly  we would have liked to run our models through
forward backward feature selection and hyperparameter tuning algorithms  because the purpose of this project was exploratory  we did not invest time in optimizing the models 
but would like to in the future 

fireferences
    d 
yin
et
al  
detection
of
harassment
on
web
    
available 
http   www cse lehigh edu  brian pubs      caw  
harassment pdf
    k dinakar et al  modeling the detection
of
textual
cyberbullying 
available 
http   www cl cam ac uk  rr    papers             pb pdf
    v  chavan et al  machine learning approach for detection of cyber aggressive comments by peers on
social media network  august      
    a razavi  et al  offensive language detection
using
multi level
classification 
available 
http   www eiti uottawa ca 
diana publications flame final pdf
    e  spertus  smokey  automatic recognition of hostile messages  in iaai    proceedings        available  https   www aaai org papers iaai      iaai      pdf
    imperium 
detecting
insults
in
social
commentary 
kaggle 
     
available 
https   www kaggle com c detecting insults in socialcommentary data 
    t  brants et al   large language models in machine translation  in joint conference on empirical methods in natural language processing
and computational natural language learning  prague  june       pp           available 
http   www aclweb org anthology d        pdf 
    tf idf
weighting 
available 
http   nlp stanford edu ir book html htmledition tfidf weighting   html
    p 
kathuria 
python
sentiment
classifier 
march
     
available 
https   pypi python org pypi sentiment classifier 
     r  kelly  python enchant  june       available 
https   pypi python org pypi pyenchant 
     pedregosa et al   scikit learn  machine learning in
python  jmlr     pp                   available 
http   scikit learn org stable index html 
     r  fan  liblinear  a library for large
linear classification  journal of machine
learning research     august       available 
http   www csie ntu edu tw cjlin papers liblinear pdf 

fi