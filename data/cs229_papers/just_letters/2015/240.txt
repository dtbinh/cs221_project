predicting the rate of progression of the als disease
michael black  durga ganesh  neeharika madduri
i  introduction
als is a neurodegenerative disease with few known
causes  the rate of progression of the disease varies
significantly among patients  and it is not known which
features in patients correspond to these differences  the
identification of predictive factors and then the subsequent
development of models can have a positive impact on a
patient s diagnosis        people are diagnosed with als each
year and an estimated        americans are currently living
with the disease  many of whom are close to us 
our goal is to try to predict the rate of progression of the
disease in patients given a set of predictors that include
everything from demographics to medical history to lab test
outcomes  the features we obtained were taken during the first
three months of a clinical trial  and then the rate of progression
of the disease was measured during the following nine months
of the trial  the progression of the disease is our response
variable for this problem and is measured using the als
functional rating scale  which is a measure from   to    of a
patients ability to carry out ordinary tasks such as walking 
talking  etc  our training set was taken from older trials  while
our test set included all very recent subjects  the reasoning
behind this was that we want try to predict future disease
progression and this test set gives us the most similar subjects
to test our procedures on 

symptoms were a very important factor for prediction while
features such as past clinical history and physical activity were
less so  an additional feature shown to be very predictive of
shorter survival times was whether the patient began his or her
disease with a bulbar onset  which is the degeneration of motor
neuron cells in the bulb      additional lab result features and
their descriptions are summarized in     
one very successful approach to this problem detailed in
    made use of random forests to get good predictive results 
the reasoning behind this was that there is a lot of variability
in the data  and random forests aim to reduce the variance of
the predictions by resampling the data set and only considering
a subset of the features at each decision tree node  another
benefit of using sets of variables in a random forest is the
notion of variable importance that comes out of constructing
many trees from the sampled dataset  with an idea of the
important features used for accurate predictions  the authors
could augment their current models  from the previous
approaches used in tackling this problem  we noticed that a
very flexible and nonlinear model would be necessary to make
reasonable predictions  as a result  we knew that variance
would be the biggest roadblock in our way of achieving a low
mean squared error  to try to relieve this problem  we looked
into many statistical methods such as bootstrapping and then
cross validation and hypothesis tests to compare models     

we broke this problem into a regression and classification
problem to get better predictive results  in the regression
setting  our goal was to predict the exact rate of change of the
disease  so we used various techniques which included random
forests  boosted decision trees  and neural networks  in the
classification setting  we first ordered and categorized our
dataset into a fast progression of the disease and a slow
progression of the disease  we then used softmax regression
and the svm to classify each patient into categories  with the
predictions from both settings  we were able to add a
confidence to our predictions  i e  if both methods predict
similar results we were more confident   this topic was also
part of a kaggle competition for stats      for this class a
simple boosted decision tree was used for regression  but all
the models described in this paper were used for this class 

iii  dataset and feature selection
we acquired a training set of      training examples and a
test set with     examples with     features  the test set was
acquired from more recent patients and since the goal was to
predict the progression of the disease on new patients  this test
set gives us the best way to test how our model will perform on
patients we havent seen before  we obtained the raw data
from prize life and the pro act database      the initial
data had many unmeasured values  na  if the doctor didn t
collect measure a particular feature from a patient  as an initial
step we threw out all features for which the fraction of training
examples that contain a not recorded value was more than
     with the reasoning that there wasnt enough data to be
useful for prediction  with the remaining features that had less
than     of examples not recorded  we replaced the missing
entries with the median of that feature 

ii  related work

after this simple clean up  we still had     features 
adding additional features that are not useful for prediction to
a model causes the training error to go down  but also increases
the variance of a particular method  since test error can be
decomposed into a bias and variance term  increasing the
variance without decreasing the bias much will increase the
overall error  because of this  we wanted to reduce our feature
set further  the most powerful method we found for doing this
was regularization  since penalizing the size of the   norm of
the coefficients in linear regression results in small but nonzero coefficients  we decided to try penalizing the   norm  this

since als is a disease that affects many individuals and is
not understood entirely  there have been many attempts to try
to predict how the disease will progress in patients  in order to
obtain accurate predictions  we needed to first understand our
features and how they contributed to the overall progression of
the disease  one proposed method to measure the association
of the features with als shown in     was to perform
univariate logistic regression  with this model  the authors
were able to show that the onset between diagnosis and

firesulted in a sparse set of coefficients which is exactly what we
wanted for feature selection  the l   regularization cost
function we used is shown below in     below 
   

this problem can also be formulated as an optimization
problem  where the coefficient estimates solve     
   

the parameter  s  can be interpreted as the budget you have
for the magnitude of your coefficients  this shows why the l  
regularization technique is so effective for feature selection  if
a feature is irrelevant  it is best to make it zero and not
contribute to your fixed budget  since we really want to solve
the optimization problem in     when doing feature selection to
find the best subset of features  the l   regularization serves as
a computationally feasible alternative     
   
fig     cv error for lambda tuning in l   regularization

iv  methods
we used three main regression techniques to predict the
rate of progression of the als disease in patients  since the
relationship between the features and the response was very
nonlinear  we used methods that allowed us to try to model
such data 
after running least squares with l   regularization  we got
a mean squared error of        and narrowed down our feature
set to just    features  since lambda is a tuning parameter in l  regularization  we used cross validation to find the best
lambda  figure   shows the mean squared error  and its
standard error  for different values of lambda evaluated on our
test set  the best lambda value we obtained was         
the final technique we used to narrow down our feature set
was forward and backward selection  with only    remaining
features  these methods were computationally feasible so we
were able to obtain our final dataset of    features which we
used for all our prediction methods 

the regression prediction method that gave us the best
results was decision trees because trees can deal with data that
is clustered together well whereas polynomial of different
order fits cannot  at each node of a decision tree  a binary split
of the data is performed so as to minimize the training rss 
this is accomplished by ordering each feature and then
considering all possible splits of all features to determine the
one split of a single feature that minimizes the training error 
depicted in      where the regions rj are the regions
corresponding to the leafs of the tree after each split  this
process continues at every node until there are only a handful
of observations in each leaf of the tree  a piecewise constant
value corresponding to the average of the observations in a
given leaf is then predicted 

fi   

since this is prone to overfitting  we wanted to try a couple
techniques that would reduce the variance of this procedure 
one such method was with a random forest  a random forest
makes use of a technique called bootstrapping that amounts to
resampling the dataset with replacement to obtain a series of
bootstrap replicates  with each sampled dataset  we can build a
decision tree and then average the final predictions in order to
reduce the variance of our method  furthermore  a random
forest does not consider every variable at a split like a
traditional decision tree  by only considering a subset of the
features at each split  each bootstrapped decision tree will look
very different  so the average of the trees will do a better job at
reducing the variance  by implementing these modifications to
the traditional decision tree  we were able to significantly
reduce our test mse 
an additional technique we employed was boosting in
order to augment the predictions of our decision tree  boosting
is a method that in each iteration trains a decision tree on the
residuals of predictions made by the previous tree  by training
on the residuals  the method is able  learn  from what it could
not accurately model in the last iteration  as shown in      by
summing up shrunken sub trees      we obtain a better
predictive tree  the number of trees used in this approach is an
important parameter since it can lead to overfitting  the rate at
which the method  learns  is controlled by the shrinkage
parameter  lambda  and can be found using cross validation    
   

   

our next goal was to transform the regression problem into
a classification problem  we did this by categorizing our
training and test sets into three categories  fast  moderate  and
slow progression of the disease 
we implemented a softmax regression model on the data
set and replaced the logistic sigmoid function with its multiclass equivalent  softmax function given by    
   

   

w is a matrix of the weights for each class  with wi being
the i th column  in order to map from the output of the
softmax function to a class label  the maximum value of the
softmax function can be used   i e  the estimated label is given
by     which corresponds to maximum a posteriori  map 
class labelling for the generative model that has been learnt  to
define our indicator function  t  in this case  we simply found
the  nd quantile of the training targets  and split them into three
classes as follows 
  

y           

t          

  

         y         t          

  

y        

t          

to optimize the feature weights we made use of the
stochastic gradient descent algorithm  this calculates the loglikelihood gradient for small batch      data points  and then
updates the current estimate of the parameters by taking a step
in the direction of the calculated gradient according to
     where ei w  is the error function we want to minimize  i e 
the negative log likelihood  calculated for each data point  or
batch  i  at the current weight values w  for all classes  
   

our last method for regression involved building a neural
network model  our neural network model had one hidden
layer and an output layer  the neurons in the hidden layer have
the sigmoid transfer function and the output neuron has a linear
transfer function  the model has    inputs which is the number
of features of the dataset  the model is shown in the figure   
we used     of the data for training      for validating the
model and     for testing the model 

after implementing the softmax regression model  we
realized that the classes could not be separated using a linear
boundary  to get better classification results we tried two
additional methods involving the svm  both models used just
two classes for classification instead of the three used in the
softmax regression model  in the first svm  we threw out the
observations belonging to the middle class  corresponding to          y         so that the two classes that were more likely
to be separable and used a linear kernel  in the second svm 
we used the entire dataset  splitting the two classes down the
middle  and then applied an svm with a gaussian kernel 
our best classification results came from using an svm
with a gaussian kernel       the gaussian kernel maps the
features into an infinite dimensional feature space  thereby
increasing the likelihood that the two classes are separable     
since the kernel trick allows the dot product of infinite

fig     artificial neural network model

fidimensional vectors to be computed using the original feature
vectors  the svm is still computationally feasible 
    

    from the training set versus the number of neurons in the
hidden layer for the three different algorithms is shown in
figure   

v  results   discussion
the results shown in this section were reported using our
test set  we held out     observations from more recent
clinical trials with which to test and compare our methods 
the random forest and boosted decision tree variants of the
classical decision tree proved to do very well in prediction  the
reason for this is that both of these methods aim to reduce the
variance using a single tree by averaging the predictions of
many trees  the random forest does this by averaging the
predictions of many bootstrapped samples  the boosted
decision tree accomplishes this goal by adding together many
shrunken sub trees trained using residuals 
the parameters for the random forest are the number of
trees and the number of parameters to consider at each node
split  the number of trees to choose is not an interesting
parameter since there is a point of diminishing returns  so a
sufficiently high number of      was chosen to ensure that we
converged to the lowest mse  the number of parameters to
consider at each node was chosen using    fold cross
validation and was determined to be five  this corresponds to
roughly the square root of the number of features 

fig     test mse as we vary the number of neurons in the hidden layer

from this plot  we chose the bayesian regularization
algorithm with six neurons in the hidden layer for our model 
the last parameter we needed to select was the number of
iterations performed during training  the training  validation 
and test mse are shown in figure   for different numbers of
iterations  we then trained our final model with four iterations
and inserted our actual test set to get the final test mse for this
method  reported in table    

the parameters needed for the boosted decision tree are the
number of trees  the shrinkage parameter  and the interaction
depth  the interaction depth is simply how many levels we
want each sub tree to have  again       trees was chosen and
the shrinkage parameter and interaction depth was chosen
using    fold cross validation  an example of the test and
training mse for different shrinkage parameters is shown in
figure  

fig     mse for train  validation  and test sets

fig     training   test mse for shrinkage parameter with      trees

when training the neural network model  we tried three
different algorithms and varied the number of neurons in the
hidden layer to determine the best model  of our original
dataset  we used     of the data to train the model      as a
validation set  and     as a test set  the test mse using the

the mean squared error on the test set is reported in table
  for the three regression methods detailed above  the mean
squared error using a linear regression and also for just
predicting the mean response  of the training set  are also
shown for comparison 

fitable i 

test mse for regression methods

method

test mse

predict mean

      

linear regression

      

random forest

      

boosted decision tree

      

neural network

      

the accuracy of the classification methods we used are
reported using confusion matrices shown in figures       and   
the softmax regression model with three classes had an
accuracy of        this showed that simple linear decision
boundaries did not perform well with this dataset  using the
svm with the linear kernel and two classes  we achieved an
accuracy of        however  in this model  we discarded the
middle class in order to increase the likelihood of separability 
using all of the data  the best accuracy we achieved was      
using the svm with the gaussian kernel 

fig     confusion matrix for softmax regression model

fig     confusion matrix for svm with gaussian kernel

vi  conclusion
als is a disease that still is not well understood  with past
medical data taken at clinical trials  we hope to be able to better
understand the underlying factors to lead to the progression of
the disease and thus diagnose it better in the future  as we
continue to collect data  the algorithms will continue to
perform better and learn patterns and structure that can prove to
be very beneficial to doctors and the families of those affected 
in this paper  we explained our methods for regression and
also classification  when performing regression  the decision
trees worked the best  in order to get the best performance out
of the trees  we used techniques such as bootstrapping and
boosting to reduce our variance and thus improve our test
mse  in the classification setting  the svm with a gaussian
kernel was very accurate  using both the regression output and
the classification category  we could give a confidence level to
our predictions  for example if the rate of progression of the
disease was predicted to be a small number by the random
forest but the svm classified it as a fast progression  we would
be less confident and have to run that example through
different regression methods to get a more accurate result 
for future work  we would like to continue to gather data
and modify the algorithms we used to make better predictions 
we explored many algorithms in the time we had  and got a
very good feel for which ones worked well and why they
worked well  a next step would be to ensemble all the
regression methods into one and used that one to make better
predictions  if one method performs well on data points in
which another does not  we could use the two models together
to improve our accuracy  one way to do this would be to train
a global model that selects which of the three models to use
depending on the test observation 

fig     confusion matrix for svm with linear kernel

acknowledgment
we would like to thank pro act and prize life for the
raw data  and also ji park  yash deshpande  kenneth jung 
lester mackey  kris sankaran  and vatsal sharan for the
derived features 

fireferences
   

   

   
   

qureshi mm  hayden d  urbinelli l  ferrante k  newhall k  myers d 
et al  analysis of factors that modify susceptibility and rate of
progression in amyotrophic lateral sclerosis  als   amyotroph lateral
scler                
gordon ph  cheng b  salachas f  pradat pf  bruneteau g  corcia p  et
al  progression in als is not linear but is curvilinear  journal of
neurology                 
prize life  neurological clinical research institute  how to use proact        url https   nctu partners org  proact  
torsten hothorn   hans h  jung        randomforest life  a
random forest for predicting als disease progression  amyotrophic
lateral sclerosis and frontotemporal degeneration                 

   

   

   

hothorn t  leisch f  zeileis a  hornik k  the design and analysis of
benchmark experiments  journal of computational and graphical
statistics               
g  james  d  witten  t  hastie  r  tibshirani  an introduction to
statistical learning with applications in r  springer  new york  isbn 
                     
m  akay  support vector machines combined with feature selection
for breast cancer diagnosis  expert systems with applications  march
      volume     issue    part  

fi