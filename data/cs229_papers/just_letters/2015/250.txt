predicting future employment  productivity and
income  finding patterns in economic data 
jake mckinnon   
 

computer science department  stanford university
 

jakemck stanford edu

abstractwe implement and evaluate several methods
for making predictions using  or finding patterns in 
heterogeneous economic data  though both supervised
and unsupervised algorithms are used  the primary focus
is on k means clustering and logical extensions for use
with non continuous data  by altering the dissimilarity
function and centroid update procedure  k means can be
made to perform reasonably well on mixed or categorical
data while retaining the other characteristics of k mean
clusteringnamely  relatively good speed on larger data
sets  such a dissimilarity function can be defined either
using domain specific knowledge  or in a generalizable
manner 
i  introduction

whether making decisions about individual career
paths or about overarching government policy  people
often want to know what causes there are for a number
of different economic outcomes  people commonly
choose their majors and careers with the belief that they
will be economically secure on such a path  further  our
society promotes education  marriage  home ownership
and more on the basis that we believe such factors to
contribute to or cause wellbeing for individuals and
society  but are we correct in these assumptions  and
moreover  on the margin  where we are forced to trade
off a dollar spent on one program for a dollar spent on
another  which program makes best use of the additional
dollar  these are questions that ought to be answered
thoroughly  rather than assumed or ignored 
better mathematical evidence using economic data
may be one way to help robustly answer these questions 
and from technology companies to baseball teams     
statistical methods have helped many organizations be
more effective in recent years  if certain factors can be
shown to be strong predictors or causes of desirable
outcomes  then perhaps better policy decisions can be
made  grounded in hard evidence rather than intuition 
to help find such factors  i attempt to use census data
to predict individual outcomes and to look at how
different observations are associated with each other 
for instance  if there is a cluster of healthy  happy 
wealthy people who share certain unique characteristics
or history  we would like to know about it  though this

is far from perfect justification for policiespushing on
these characteristics may break the association with
good outcomes for any number of reasonsat a
minimum  it provides a way to make predictions about
existing people  which is itself valuable  perhaps further
work could prove which relationships are robust to
external pressure  such that policies might move push
both the characteristic and the outcome in the desired
directions 
ii  related work

there are two dimensions to this project  one within
economics and one within machine learning  from the
economics perspective  it is hard to point to specific
papers as a starting point because this is such a broad
target  there is a good deal of discussion and confusion
about the causes of worker productivity  which seems to
be a major point of both confusion and research        
further  there are a number of papers about applying
clustering methods to the problem of market
segmentation     
from the machine learning perspective  though we
briefly tried a number of prediction methods  the
primary focus of this paper is on k means and extensions
into settings where traditional k means breaks down 
therefore  most of the machine learning papers we
referenced have to do with discussions of k means and
modifications 
huang        discusses a method for extending kmeans to use on categorical or mixed data  called kmodes      many of our approaches follow from his
work or related work      or draw ideas from it  the
main idea is to find a way to measure category
dissimilarity  and use this metric to cluster samples 
this method retains the speed of k means 
another method called k medoids updates using
sample data points in place of centroids  and can be
made to work with categorical data and arbitrary
distance functions      though it is more expensive 

fiiii  data set and features

many countries  particularly in the first world  collect
a good deal of information about their citizens through
some form of census  this was the data we aimed to
make inferences from  as it was most available and had
very large numbers of samples to compensate for the
relative imprecision of the data  the primary data set
we used some partially cleaned us census data from
     with         samples  though some of the later
methods might have been more successful on a different
data set  the focus of this application project not so much
the particular results  but the process of applying
machine learning to the overall category of problem 
extension to another data set was relatively low priority 
with the significant exception of our applicationspecific distance metrics  our methods could be readily
extending for use with other census data 

feature
education num
age
capital gain
hours worked

mean
  
  
    
  

standard dev 
   
  
    
  

fig     sample features  note that edu num   is high school grad 

b  preprocessing

of the        samples in the data set        had
missing features  we considered dealing with this a
number of ways  but ultimately decided to remove them
from the data set as many samples with missing features
seemed to be very anomalous in other ways as welle g 
working very low hours or exceptional mismatch
between education and income  this left       
samples  we justify choice  which arguable skewed our
data samples to be more average than reality  by noting
a  features
the data set used had    features  six were that trying different methods for dealing with this
continuous features  the remaining nine were categorical  missing data would be relatively straightforward and
including binary and non binary categories 
the that having exceptional or unreliable data points may
features included  education level  gender  race  have made our clustering results unreliable or otherwise
occupation  age  income level  marital status  capital  more difficult to assess  particularly when custom
gain  and native country  not that the particular features distance metrics were used 
further  we normalized the data to have unit variance 
are not important  except insofar as interpreting findings
when
dealing with k means and extensions  large
on this data set  an example from the data set is
difference
in variance can dramatically overweight
provided below  as is basic information about some of
certain
features
depending on dissimilarity measure that
the more relevant features 
is chosen 
if the data set had had more features  we may have
used principal components analysis to reduce
dimensionality while retaining most of the useful
variation in the data set  though we did not require this
for this particular data set  it is worth noting if one finds
a much higher feature data set on which to apply similar
methods  which may be useful   note that this would
complicate the discussion of several distance functions
later discussed 
finally  from these processed points  we randomly
selected      as our training set  with      remaining for
testing  for clustering these distinctions were less
important  though they helped provide a means of
testing whether or not the clusters actually existed in all
economic data  or was merely a by product of clustering
fig     graph of age vs  educational attainment       phd 
on a particular data set  e g  by clustering on one set 
then on the other  and comparing clusters  

fiiv  methods

with a project goal of finding interesting relationships
in census data  our choice methodology was relatively
open  initially  we looked at simple relationships in the
data and tried the classic task of predicting income  with
reasonable success using nave bayes  but the primary
work was diving into how to apply k means like
algorithms to census data  which tends to be large  with
clustering  our goal to find what similar clusters of
people existed in the country  and how this might be
associated with desirable features such as income and
family stability  the clusters also provide a reasonable
partition of that data such that other less efficient
methods of evaluating data might be applied going
forward     
a  nave bayes classifier

by defining one of the features collected in the census
as the target label  and removing it from the feature set 
we have the common setup for our supervised learning
problem  we straightforwardly apply classic nave
bayes to the problem of predicting income  initially
using the matlab implementation for continuous features
and later our own  tinkering with inputs  e g  by
removing features  provides some information about
which features are most consequential in predicting
income  or any other targeted category 

   the algorithm effectively uses euclidean distance  the
square of the l  norm to group samples with centroids 
   k means is polythetic  so it may not be clear from
looking at any   d plot why the clustered points are
similar to each other  and indeed this held  
   it has no objective measure of performance built in 
at best  it says  these samples are similar as judged by
this particular measure of similarity  which may or may
not make any sense 
c  k modes clustering

the most glaring deficit  for our purposes  in classic
k means is that it doesnt make use of the majority of
our census data  which is mostly categorical  this is
unfortunate because k means is exceptionally fast on
very large data sets such as the us census  as far as
clustering algorithms go      however  by looking more
carefully at the two observations about k means above 
we can see how one might logically extend k means into
categorical data  by defining a new function by which to
assign samples to centroids  one that makes sense in the
categorical context  and change our centroids update rule
correspondingly 
specifically  we define the similarity of two objects as
follows  where m is the number of samples 
m

d x y   

j

j

j  

  x j   y j  
  x j   y j     
  x j  y j  

b  k means clustering

we use k means to see if we can discover any
relationships based on the centroids or the clusters
created  similar to how one can see relationships with
simple graphs such as the age vs  education one above 
ignoring categorical features  as well as continuous
features for which euclidean distance makes less sense 
such as education number   we apply k means to the
data samples  we further apply k means with different
initialization and with differing numbers of clusters 
for the sake of the following discussion  recall how
k means operates  imagine the  continuous  data
samples as points in a high dimensional euclidean space 
where axes correspond to features  we place a number
of centroids either randomly or by some other more
intelligent process in the space  then we group all
samples based on whichever centroids they are closest to
as judged by the square of the l   or euclidean  norm 
finally  we move the centroids to the mean of all points
in its group  then  we repeat this process with the new
centroids  the algorithm terminates when no samples
shift grouping 

note a few things 

  x  y  

further  we change our update rule to a frequency
based update rule  for each feature  the label of the new
centroid becomes the mode of all samples in the group 
notethat this applies to purely categorical data now 
to most greatly benefit from this extension  we must
reintegrate it with k means 
d  k prototypes clustering

combining k means and k modes back together  so
that we can now deal with heterogeneous data  gives us
the k prototypes clustering algorithm  strictly speaking 
this was defined as flows  where features p through m
are categorical and  is a weight to avoid favouring either
attribute type 

p  

d p  x y   


m

  x   y  
j

j  

j

 

    x j y j  
j p

in practice  we just choose  it such that the standard
deviation of the categorical features  as defined by the kmodes dissimilarity metric  is the same as the standard
deviation for the continuous features  effectively  just
normalizing the variance
of categorical data  


fie  application specific dissimilarity functions

further exploring different dissimilarity functions or
update rules for k means like algorithms  we look
towards application or domain specific distance
functions that provide more information about similarity
or dissimilarity than the simple equivalence based
metric used in k modes and k prototypes  note that
defining a good custom dissimilarity metric may require
domain specific knowledge  and has a number of
potential issues that the other methods discussed do not 
   linear or other euclidean metrics for category
dissimilarity  one suggestion for custom distance
metrics was a distance function that functioned by
placing all the categories on the line  and treating
distance as the distance between the category points 
such a metric naturally fits for categories like education
level  but not others  e g  a stock analyst might be
similar to a doctor  both wealthy and educated  and
doctor similar to caretaker  but the stock analyst is not
similar to the caretaker  in this example  categorical
entries may be similar or dissimilar to each other along
many dimensions  which cannot be effectively projected
onto a single dimension  putting categories in a higherdimensional euclidean space can help  as it does here 
but even this may not be the case for less transitive
categories  however  we still made use of this strategy
for some categories  it corresponds to assigning each
label in a category a higher dimensional vector and
calculating euclidean distance between any two labels
using their assigned vectors 
   changing dissimilarity measurements for
continuous features  in certain settings  it may make
sense to use a measurement other than the l  norm to
assign samples to centroids  though this may break
convergence guarantees  it can be made to work in
certain settings  for example l  norm  or manhattan
distance  may be used if you were clustering locations in
a city based on travel time  this is effectively k medians  
though we played with such measurements  we didnt
explore them deeply 
   other custom dissimilarity metrics 
given
sufficient knowledge about the domain in which the data
is being used  one might have a better metric for judging
category differences 
in certain cases  manually
defining all the relationships between categories might
work  but only for very low number of labels 
   the difficulty in judging dissimilarity metrics 
note that  for such these dissimilarity measurements  the
second problem noted in the k means clustering section
is particularly potent  we are defining a distance metric 

and we cannot judge our success by this metric  as that
would be circular 
instead  the metrics must be defined and judged as
being reasonable in the context of the application  e g 
returning clusters that make sense  or by other means 
experts might decide what is a good metric for
dissimilarity as judged by their experience and prior
research in that particular field  and on whether the
clusters produced are sensible 
   a note about convergence  note that the
procedures described here break the convergence
guarantee that k means typically has  while working on
the application  we did not realize this at first because in
many cases the algorithm kept working  however  we
have no proof of this in the general case and there are
likely ways one might use the methods covered here in a
way that might break convergence  we acknowledge
this  but note that empirically most of the methods
discussed seem to converge  there seem to be such
guarantees for k medoids  though the algorithm is
slower     
v  results

brief summary of the results generated by applying
the above methods to the us census data set  most of
the discussion is above  although some of the problems
discussed do return in results 
a  nave bayes classifier

when trying to predict income using the    other
features of the data set  the nave bayes classifier
produced correct labels reasonably well  it had     
training error and      generalization error  on the    
random untrained samples   furthermore  removing
categories revealed that marital status  relationship was
the most important feature in predicting higher income 
b  k means clustering

the only continuous variables available on which to
k means cluster were age  education number  with
higher being more educated   capital gains  capital loss 
and hours worked  despite the low number of features 
k means still revealed some interesting relationships 
specifically  it had the following clusters 
 near     well educated  high cap gains  high hours
 mid   s  college  medium cap gains  medium hours
 old  medium cap gains  little college  low hours
 young  no capital gains  lower education  modest hours
rerunning the algorithm several times we find that the
clusters are consistent even with substantially different
initialization points  increasing the number of clusters
helped equalize the number of samples in each cluster
by further differentiating between the large mid age and

fiyoung groups  the same basic clusters still present 
note that  though we solved the question of how many
centroids to use by trying a variety of numbers  there are
more intelligent methods of deciding how many clusters
are present     
c  k modes and k prototypes clustering

given the low number of categories  there were only a
few relationships that showed clearly in the categoricalonly k modes clustering 
marital statuses were
clustered with the corresponding relationship statuses
 e g  wife was also married civ spouse  and genders
exceptionally well  as and work class and certain
occupations  e g  no self employed armed forces  
extending to k prototypes gave richer clusters  but
they primarily confirmed expected trends  we saw
similar clusters to k means  with the additional
information that  the high cap gains earners were likely
married  in particular occupations  and white  the
young people were least likely to clustered with married
people  other obvious clusters
d  application specific distance function

didnt finish this implementation  though with the
partial complete we got better clusters around doctorate
and profession school people with high income as well
as divorced  widowed  and separated clustered with low
incomes 
may have had difficulty due to the
convergence concerns mentioned previously 
vi  conclusion and future work

though most of the results were nothing new  we still
managed to replicate a number of well known findings 
it turns out that  indeed  stable marriages generally
promote good life outcomes  as does being white 
educated  male  and non manual labour occupations  a
somewhat interesting finding  though not exactly
counterintuitive  is that people who had high capital
gains incomes generally worked quite longover   
hours on average  at least it wasnt something i was
expecting to find as much as those other relationships 
overall  it was interesting to explore several approaches
to clustering that we didnt cover in lecture 
e  future work

 had i had more time or other team members  with is
what i would have worked on going forward  
though this data set was very friendly to work with 
an obvious next step would be to apply these methods to
a data set with more features and that is more recent 
many of the methods outlined generally perform better
with a higher number of categorical features  as the

means by which they differentiate categories is entirely
binary and therefore grainy 
this contrasts with
continuous data  which can more easily produce
intermediate measures of similarity  it would also be
interesting to see any changes over time  e g  women
trending towards becoming more educated and higher
earning  by comparing the results using this data set
with a more recent one 
another extension to this application that might have
improved the observations would have been using the
clusters generated by k means or k prototypes in further
algorithms  by reducing the number of samples on
which to run  k prototypes might allow more
computation intensive learning algorithms to find more
original relationships in certain subsets of the population 
finally  one method of finding many patterns in
census or other economic data that maps very
straightforwardly onto this setting is bayesian networks 
however  i avoided this topic because there appeared to
have already been a good deal of work in bayesian nets
that used or referencing such census data  in contrast 
there seemed to be less work with k meansone
couldnt apply classic k means effectively to most
census data  which is primarily categorical  however 
though bayesian networks have been used in this area
often before  they would still be a good place to explore
recreationally or in any setting where originality is not a
priority 
references
    j s  armstrong  predicting job performance  the moneyball factor 
foresight  the international journal of applied forecasting            
pp         print 
    h  rezankova  cluster analysis of economic data  statistika  vol    
no     pp              
    a  k  jain  r  c  dubes  algorithms for clustering data  prenticehall       
    huang  zhexue  extensions to the k means algorithm for clustering
large data sets with categorical values  international journal of
data mining and knowledge discovery  vol     no     pp          
     
    michael k  ng  mark junjie li  joshua zhexue huang  zengyou he 
 on the impact of dissimilarity measure in k modes clustering
algorithm   ieee transactions on pattern analysis   machine
intelligence  vol     no     pp           march      
doi         tpami        
    milligan  g w  and copper  m c        an examination of procedures
for determining the number of clusters in a data set  psychometrika 
              
    bewley  truman f  why wages dont fall during a recession 
cambridge  ma  harvard up        print 
    judge  timothy a   christine l  jackson  john c  shaw  brent a 
scott  and bruce l  rich  self efficacy and work related
performance  the integral role of individual differences  journal of
applied psychology                       web 

fi