a personal conversational model
lawrence lin murata
ana maria istrate
computer science department  stanford university
lmurata stanford edu
aistrate stanford edu

introduction
conversation with machines has been an important topic in
both research and fiction for a long time  the possibility of
having conversations with machines has always captured the
imagination of people  in       alan turing proposed the
famous turing test to answer the question   are there
imaginable digital computers which would do well in the
imitation game    since then  the turing test has become a
highly influential concept in the philosophy of artificial
intelligence 
the idea of talking with machines has become popular with
movies like       space odyssey and her  and with product
like apples siri  microsofts cortana  facebooks m and
google now  it makes sense for natural language to become
the primary way in which we interact with devices in the
future  because thats how humans communicate with each
other  thus  the possibility of having conversations with
machines would make our interaction much more smooth 
natural and human like  in this project  we explored different
methods that model distinct aspects of language 
in our conversational model  we take the human utterance
as an input  for instance  how are you    we then use a
support vector machine  svm  with a linear kernel to
generate the machine response word by word  where we start
off by predicting the first word in the response  and then each
predicted word  together with the human input  is used to
predict the next word until we generate the entire response 
the final output is the machine generated response  for
instance  i am good  
the main method we develop  the recurrent statistical
model  rsm   relies heavily on machine learning knowledge
and is being used for cs      machine learning   our other
methods  logic model and n gram search  while also
relevant for a machine learning class  are more focused on cs
    artificial intelligence  material 

i 

ii 

related word

a  chat bots and conversational agents
over the last decades  researches have worked on creating
chat bots and conversational systems  but these complex
systems often require a processing pipeline of multiple stages
 lester et al         will        jurafsky   martin        
besides requiring a complex processing pipeline  many of the
classical approaches for modelling conversations rely on
language rules or domain specific knowledge  usually
provided by the creator of the system  our goal was to create a
conversational model that could work reliably without
requiring huge data sets or any previous knowledge about the

domain  the language  or any other sort of pre computed
knowledge 
b  neural networks for natural language processing
the more recent success with statistical approaches
have inspired us to develop our own probabilistic model  the
works of bengio et al   bengio et al          mikolov et al 
  mikolov et al         and mikolov  mikolov        have
successfully demonstrated the application of recurrent neural
networks to model languages  long short term memory
 lstm  recurrent neural networks have become prominent
choices for learning language models  in       sordoni et al 
 sordoni et al         and shang et al   shang et al        
were able to model short conversations using recurrent neural
networks that were trained on short chats 
c  a purely statistical approach with recurrent neural
networks
more recently  google researchers o  vinyals  q v  le  o 
vinyals  q v  le        developed a conversational model
that makes use of the sequence to sequence  seq seq  model
described in  sutskever et al          using recurrent neural
networks to read the human input  token by token  and predict
the machine generated response  token by token  its an
interesting paper because it proposes a system that is simple
than most of the models proposed before  the neural
conversational model is a purely statistical model that is
general and doesnt require any domain knowledge  the
model  however  relies on rather large data sets  an it
helpdesk troubleshooting data set with   m tokens and an
opensubtitles data set  tiedemann        with   m sentences
    m tokens  for training and   m sentences     m tokens 
for validation  since the implementations we were able to find
online did not perform as well as expected with smaller data
sets  we decided to create a simpler statistical model that
could generate conversations with considerably less data  as
you will see  our data sets have     to        lines  
a drawback that all statistical approaches naturally have is
that they oversimplify many aspects of communication in
order to frame a conversation as simply finding the most
likely sequence as a response  for example  it does not
capture general knowledge about the world  human logic
 which our logic model captures   the essence of human
communication  with its goal to exchange information   or a
consistent personality 
iii 

data and feature engineering

a  datasets
we used three main data sets  all of them containing
conversations from friends  the tv series  the first smaller

fidata set  data set a  contains a small portion of the script from
the second season of friends  the raw data set contains    
lines  the second data set  data set b  is composed of the first
and second seasons together  the raw data set b contains
       lines  the third data set  data set c  has seasons     
and   combined  the raw version of data set c has a total of
       lines  we used the three data sets and compared the
results for each of them in order to understand how our model
improved as we increased the size of the data sets 
for testing  we used a set of    questions in   different
domains  general conversation     questions   philosophical
questions    questions   feelings     questions  and closing   
questions   its important to remember that our data set was
very general and not specific to any of these domains  because
our goal was to develop a general purpose conversational
model that can perform well under different domains and
situations 
b  processing
we parsed the data sets in order to remove character names
that were present in the beginning of each line  punctuation
marks  remove introductions or section titles that were not
part of the dialogues  and descriptions of the scenario or of
visual aspects of the scene such as body movement  we also
chose to lower case all of the text 
closing credits
 scene  chandler and joey s  joey is reading a script as
ross enters 
ross  all right i ve been feeling incredibly guilty about
this  because i wanna be a good friend  and dammit i am a
good friend  so just  just shut up and close your eyes  kisses
joey  
janice  oh     my     gawd    chandler rushes over
and kisses her 
fig    example of our raw data  it contains punctuation marks  upper case
characters  and descriptions of the scene

all right i ve been feeling incredibly guilty about this
because i wanna be a good friend and dammit i am a good
friend so just just shut up and close your eyes
oh my gawd
fig    example of the processed data resulting from the raw data in fig   

following a bag of words approach  we then used a
combination of uni grams and bi grams  together with
tf idf  as the inputs in our support vector machine  using
tf idf helped us understand how important each word is in
the documents  we experimented with different n grams to
capture important long range correlations between the words 
iv 
a  logic model

methods

at an abstract level  one fundamental thing a good personal
assistant should be able to do is to take in information from
people and be able to answer questions that require drawing
inferences from the facts  in some sense  telling the system
information is like machine learning  but it feels like a very
different form of learning than seeing   m images and their
labels or   m sentences and their translations  the type of
information we get here is both more heterogenous  more
abstract  and the expectation is that we process it more deeply
 we dont want to have to tell our personal assistant     times
that we prefer morning meetings  
our logic model parses human utterances into first order
logic rules  by enforcing logic rules  the model is able to draw
inferences from facts given by the human  then the model
generates responses and answers questions according to the
inferences it can draw from facts it has previously learned 
our logic method models language according to logic  it is
very limited compared to statistical approaches  yet efficient
for some and fast  another advantage of the model is that it
doesnt require any data or training since it relies solely on
logic to generate responses  disclaimer  the starter code for
this model comes from cs     an artificial intelligence class 
but it was extended for this project 
b  n gram search
we defined our model as a search problem  where we are
searching for the words that will be part of our reply  thus  in
our framework  a state is defined as an ordered set of words
that could be added to the reply  each state has one more word
than the previous one  we have started tackling the task of
generating replies by using a very basic n grams model  we
start by choosing a word that starts the reply and a number of
words we want our reply to have  the starting word is our
initial state  right now  we are choosing the starting word
ourselves  but in the future  we plan on generating the starting
word based on the last message  the one that we are replying
to   we are also manually inputting the numbers of words we
want in our reply ourselves  but in the future we want to
account for that by using grammar and stopping the reply
when a punctuation mark is encountered 
given a starting n gram  this model finds the next n gram
that is most likely to follow the previous n gram in the
sequence until the response reaches a certain predetermined
length  for example  to start with  we find all n tuples  in our
case    or    in our dataset that begin with the starting word 
then  we choose the n tuple that happens with highest
probability  then  we do the same thing  but match the tuples
that start with the previous n   words and take the one that
occurs with highest probability  we keep repeating the process
until we generate the number of words that we want  to know
when to stop  we are keeping a level that tells us how many
words we have added to the reply  we are stopping when the
level equals the desired number of words in the sentence 
c  recurrent statistical model
our statistical conversation model is the one that allowed
us to generate the most flexible conversational models  even
though responses naturally didnt always follow a correct
grammatical structure  we developed the rsm model  which
uses the words in the human utterance as inputs in a bag of

fiwords  tf idf and a mix of uni grams and bi grams   with
the bag of words  a support vector machine  svm  with a
linear kernel predicts the next word in the machines response 
it then adds the predicted word into the bag of words in order
to predict the next word  the process is repeated until the
 eos  tag is predicted  indicating the end of the machines
response 

  subject to

yt     

  t
 q  et 
 
      i  c i         n
where  e is the vector of all ones  c
      is the upper
  min

bound  q is an n by n positive semidefinite matrix 
  qij

 yi y j k xi   x j     where  k xi   x j       xi  t   x j   is

the kernel  here training vectors are implicitly mapped into a
higher  maybe infinite  dimensional space by the function     
the relation between c  alpha and the number of samples is
given by   c

 

n   samples
 
alpha

the decision function is 
n

 

  sgn 

fig    rsm is a purely statistical and recurrent model for building
conversations  in this example  rsm receives a human input a b c  where
 eos  indicates the end of the utterance  and  token by token  predicts the
machine response d e f in four steps

in the example portrayed in figure    the human input is a
b c and the machine generated reply is d e f  the  eos 
token indicates the end of an utterance  the bag of words is
actually a mix of uni grams  bi grams and tf idf used as
inputs for our support vector machine  this algorithm
constructs a hyper plane or a set of hyper planes in a high or
infinite dimensional space  in figure    the three points under
the two dotted lines are called support vectors  the distance to
the nearest training data points is called the functional margin
and  in general  the larger the margin  the lower the
generalization error  thus  intuitively  a good classification
 with higher confidence scores  is achieved by a hyper plane
that has the largest functional margin 
fig    a visualization of how support vector machines work

 

given training vectors  xi

  p   i        n  in two classes 

and a vector  y         our support vector machine
solves the following primal problem 
n

  min w b 

n
  t
w w   c  i
 
i  

yi  wt   xi     b      i  
   i    i         n
  subject to
its dual is

 y  k x   x      
i

i

i

i  

v 

experiments  discussions and results

a  parameters and techniques
there are three major models that we tried in approaching
our problem  n grams search  a logic model and multi class
classification  we focused on the multi class classification for
this project 
   multi class classification approach
our model fit a multi class classification problem approach 
at each step  the classified variable was a sequences of words 
the bag of words that we initially generated after each reply
and kept updating by appending the previous generated word 
the labels were words that the bag of words could map to 
thus  for each reply we would have several possibilities of
choosing the next word from the set of possible words  since
multi class classification is a model that works for
classification problems with more than two classes  this
approach fit our problem the best  in order to get the best
results  we have tried different classifiers with datasets of
different sizes  we have obtained the best results by using a
support vector machine  svm  kernel and a dataset
consisting of the first three seasons of the friends tv show 
below we talk about the two important steps that we took 
training and testing and how the results varied with each of
the factors 
we tried three different classifiers  support vector machine
 svm   multinomial naive bayes and logistic regression 
below  we give an overview of each of them and explain the
benefits and limitations 
i  support vector machine svm  with a linear kernel
the model that gave the best results was the linear support
vector classifier  which is a support vector classifier that has
a linear kernel  the reason why we chose a support vector
machine classifier with a linear kernel is because it scales
better to large number of samples  indeed  we saw better
results with increasing the size of dataset and the number of
text samples  this is because linear svc has more flexibility
regarding the penalties and loss functions it uses than the
other classifiers  we tried this classifier under by setting the

fimulti class strategy parameter to a one vs rest  a model that
fits one classifier per class  basically  for each classifier  the
class is fitted against all other possible classes  a benefit of
one vs rest classifier is its efficiency  being able to generate
predictions in considerably less time than all the other
classifiers we tried  we run the algorithm for a maximum
number of      iterations  and we have used an l 
regularization penalty that prevents overfitting and a squared
hinge loss that optimizes the loss function 
ii  multinomial naive bayes
naive bayes is a supervised learning algorithm that
assumes independence between pairs of features  multinomial
naive bayes is an algorithm usually used for multinomially
distributed data such as in text classification  for each class y 
the distribution is parametrized by a vector
 
  where n is the number of features
that are being used  in our case  the features being used are the
sequences of words  the bag of words  that we are trying to
predict  so it would roughly be equal to m   where m is the
number of replies of the dataset 

 
this learning algorithm worked relatively well on small
datasets  and considerably worse on larger ones  the major
problem with applying naive bayes to our model is that it
assumes of independence between words  which is not a valid
assumption to make on larger datasets  in this case the
prediction would just be composed of a set of words that
happen individually with the highest probability  not
accounting for the context in which the words appear  which
becomes very important especially with increasing the size of
the dataset  however  when working on smaller datasets 
assuming independence between words is a semi valid
assumption to make  because there are fewer choices to make
when choosing a class  so the probability of predicting the
choice that makes sense in the context is higher 

   structure   does this sentence have structure 
   context   does this reply make sense in this context 
   humanness   does this reply seem like its coming from
a human 
each reply is given a score of   or   in each of the four
categories  the total score of a reply is computed by summing
up the scores in all the four categories  thus  a reply gets a
score between      the total score of an entire conversation is
computed by taking the average of the scores of all the replies
generated by the computer 
 

where si   score of the reply i
n   total number of replies
besides being a score  gct can also be viewed as a
percentage of how many generated replies made sense  thus 
if we modify the formula for the score a little  we get another
measure 
 
where si   score of the reply i  but p is now a measure of
success how many generated replies made sense  thus  it can
be viewed as a quantitative metric of success of our model 
we have created our own set of questions  fig      that
targeted four different types of categories  general smalltalk  philosophical questions  feelings and closing  and
then tested and compared the results among three major
categories of objects  available chatbots  different classifiers
and datasets of different sizes 

iii  linear regression
we have also tried multinomial regularized logistic
regression under the one vs rest approach  the model did
not give very good results  always predicting  eos   an
element that we chose to denote the end of sentence 
metrics
quantitative
granular conversation test  gct 
the traditional  well renowned qualitative metric for testing
a chatbot is the turing test  which looks at its level of
humanness  since this is a very binary test that does not
account for other qualities of the model apart from seeming
human  we created our own qualitative metrics  in order to
assess better a conversational instance in several areas that we
found relevant  apart from humanness  we have created and
used the granular conversation test  gct   that tests a reply
given back by the computer in   areas 
   semantics   does this reply make sense by itself 

 
  
testing different classifiers
we have tested multinomial naive bayes  logistic
regression and linear support vector classifiers on friends  
dataset and computed both the in category and overall score
of the granular conversation test  while multinomial naive
bayes didnt perform very well  it still performed better than
logistic regression  that predicted  eos  for every input  an
explanation of this behavior can be found in the section above
where we describe classifiers  the scores are 

fihere are some examples of conversations we were able to
have with our conversational model 

 

 

  testing datasets of different sizes
we have tested trainings datasets of different sizes from the
friends tv show and computed both the in category and
overall score of the granular conversation test  we have
definitely seen an increase in our results by increasing the size
of the training dataset  the scores that we obtained are 

 

 
ii  qualitative

 
vi 

conclusion and future work

using our recurrent statistical model  rsm   we were
able to build a simple and generalistic conversational model 
throughout the project  we were able to tackle the challenge
of measuring how well a model works  which led to the
development of our own test  the granular conversation test
 gct   which aims at breaking down human conversation into
four main components that make a conversation human  using
a support vector machine  svm  with a linear kernel was the
approach that gave us the best result 
the other models we experimented with were able to
capture other important aspects of human conversation  but
these results came at high costs  the logic model successfully
captured basic logic and was able to draw inferences from
facts given  despite its obvious drawbacks since its only able
to parse and understand human utterances that follow a few
hard coded templates  and its only able to respond with a
limited set of inferences based on the enforcement of logic
rules  the n gram search model could capture long range
correlations between words but was extremely prone to rote
learning overfitting  the n gram method was very limited 
because it requires the user to enter the starting n gram of the
sentence and it can only generate responses for known starting
n gram 
rsm was able to generate basic conversations by
extracting knowledge from noisy  relatively small and opendomain data  there are  however  drawbacks that result from
the simplification of aspects of human communication  such
as a consistent personality  general world knowledge  and
human logic  future improvements can help us capture more
aspects of human communication and human nature  for
personality  a new model could use multi class sentiment
analysis techniques together with rsm to create a coherent
personality  our logic model  which is able to make basic
inferences based on logic rules  could be combined with rsm
to build a system that models both human logic and human
communication  both combinations could model and simplify
important aspects of human nature 
references

fi  
  
  

turing  a  m  computing machinery and intelligence  mind  pp     
          
automatic capacity tuning of very large vc dimension classifiers
i guyon  b boser  v vapnik   advances in neural information
processing      
support vector networks c  cortes  v  vapnik  machine leaming 
                  

fi