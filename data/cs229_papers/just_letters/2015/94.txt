 

learning summary statistics for approximate
bayesian computation
yiwen chen 
email  yiwen   stanford edu

abstractin high dimensional data  it is often very
difficult to analytically evaluate the likelihood function  and thus hard to get a bayesian posterior
estimation  approximate bayesian computation is an
important algorithm in this application  however 
to apply the algorithm  we need to compress the
data into low dimensional summary statistics  which
is typically hard to get in an analytical form  in
this project  i used radial basis function network
and neural network to learn the summary statistics
automatically  i constructed a time series example
and a hidden markov chain example  and i find that
neural network performs better 

i  i ntroduction
in bayesian estimation  one of the most important
step is to calculate the likelihood function p x   
   where  is a vector of random parameters and
x is the training set  however  in many important
applications  this is infeasible 
an interesting question is thus how to compute
the posterior distribution  approximate bayesian
computation is such an algorithm to compute the
posterior distribution with the advantage that no
explicit form of likelihood function is required  this
is extremely useful in settings where the dimension
of data is huge  but the underlying parameter has a
relative small dimension 
the basic idea of abc algorithm is very simple 
one can use rejection sampling to obtain draws from
the posterior distribution without computing any
likelihoods  as shown in algorithm    we draw
parameter data pairs      x     from the prior p  
and the model m  and accept only the   such that
  s x      s x        where x is the dataset we
have 

algorithm   abc rejection algorithm
   for i              n do
  
repeat
  
propose    p  
  
draw x    m given  
  
until   s x      s x      
  
accept   and set  i     

one of the key step in abc algorithm is to judge
whether the data x   generated by parameter   is
close enough to the sample data x   when x is of
high dimension  the probability p   x  x         
is very low  the sparsity of high dimensional space 
i e  the curse of dimensionality   so it is easy to go
to extreme when choosing   either the rejection is
of extreme inaccuracy  or accuracy at the expense
of a very time consuming procedure  
so a central question is to construct a summary
statistic s which can efficiently get most information in the data and has low dimension  with such
a summary statistic  it is easy to get a good balance
between rejection accuracy and computation efforts 
in an abstract way  i want to use the posterior
mean e  x  as a summary statistic  which has
been proved to capture the first order information
when summarizing x   however  we should note
that although mean  median etc  are very commonly
used  they can be very different from e  x  
so i want to apply some automatic statistic selection methods to learn the form of e  x  given the
statistical model  to combine what i have learned
in the course machine learning  i propose the
following question 
how different machine learning methods perform
 
when accuracy is high  i e   is low  it takes many samples
for enough acceptations 

fi 

for choosing the summary statistic 
although abc algorithm is well known  the automatic generation of summary statistic is a recent
research topic  and there are few papers on it 
blum        and fearnhead and prangle       
use regression methods  and jiang et al        
use deep neural network  however  non of them
compares the performance of different machine
learning techniques in generating summary statistics  in this project  i aim to use different machine
learning methods to generate summary statistics
and compare the pros and cons of each method 
this will provide guideline for the applications of
this novel method 
ii  m ethods
i denote m as the statistical model  x  rp
as the random values  and x  rp as data  
moreover  to make the methods work  i assume
that x  p     and it is possible to sample from
this conditional distribution  let l     generalized
linear regressions  neutral network       be the set
of supervised learning algorithms  and    l to
be an element of these algorithms  then the main
problem is to construct a low dimension and informative summary statistic s for high dimensional
x   steps are as follows  for each    l 




generate a data set d      i    x  i        i 
n   where x  i  is a random sample from the
distribution p x     i    
use the data set d to train the supervised
learning model    to minimize the objective
function
n

  x  i 
    s x  i       
n

   

i  



 

this objective function can be viewed as an
approximation of the posterior mean e  x 
because the solution to an squared error minimization problem will results in such an estimation 
run the abc algorithm with the summary
statistic s   to get a posterior distribution 

in typical applications of abc algorithm  p is very large 
e g  p       

then check the calculated posterior distribution with the posterior distribution derived
from the sufficient statistics to judge whether
the machine learning method   performs well
  
there are two sources of potential errors  one is
from the construction of statistics  where s   is
not necessarily a sufficient statistic so that there is
some information loss  the other is from the abc
algorithm  which contains errors because of the
acceptation criteria   x    x       to guarantee
that there is indeed a meaninful comparison among
different machine learning algorithms  i need some
convergence results for the estimation methods  the
following theorem is from jiang et al         
theorem    assume that e       abc procedure with observed data x   summary statistic s  
any finite vector space norm         and tolerance
 produces a posterior distribution p     then we
have
  ep     s x      
and
lim ep      e  x 

 

this theorem guarantees that when  is small
enough  the error from abc algorithm can be
neglected  and thus the evaluation of different supervised learning algorithms is valid 
moreover  we can use abc algorithm to get
e g   x  for any continuous functions g    and
thus get the whole posterior distribution  because
the objective of this project is to evaluate different learning methods  it is easier if e g   x  is
sufficient for the posterior distribution  so i use a
one dimension  with a distribution that only has
e      as the parameter  this is also the reason
why i use the quadratic objective function 
iii  t he m odel for e valuation
in this project  i have to choose a statistical model
p    to evaluate different methods  the following
property is needed for evaluation 
 
from the purpose of this project  i have to use models where
the sufficient statistic is known 

fi 

   

 

   
   

 

   

 

   
   

 

   

   

 

xt

 

   
 

   
   

   

 

   

 

   
   
 
   

 

  

  

  

  

  

 

 

 

 

   

 

 

 

 

 

 

t

 a  moment

 b  linear regression

fig     typical ma    process with                  
   
   

   

 
   



the dimension of parameter  is small  but the
dimension of data is very large 

   

   

 
   

   

   

 
   

   

 

   

i will use different machine learning methods to get
a summary statistic and compare the performance 

   

   
 
   
   

let x  rp be a vector of observations in an
ar q  time series  assume the underlying model
is
xi   i   i    i 
where zj s are unobserved random noise terms 
to get an analytical formula for the likelihood
function  i assume that j  n        and are i i d 
and the true parameter is             assume
the prior is a two dimension standard normal distribution  so that i can compare the posterior directly
with the true distribution  typical ma    processes
with                  are shown in figure   
then i compare the following methods 


a natural moment estimation
n 

  x
   
xi   xi
n 
i  





   

   

   

   

 c  rbf

a  moving average model

using linear regression model 
using radial basis function network 
using neural network 

i simulate the ma    process with total time length
t         and replication number n         the
observation is one sample from the true parameter 
then i separately use the above methods to estimate
the statistic and posterior distribution  as shown in
table i and figure   

   

 
   

   

   
 

 

 

 

 

 d  neural network

fig     posterior estimation for the time series model

i find that there are significant differences among
the four methods  because the prior has mean       
and the true value is             a good updating
should be somewhere between the above two values  based on this criteria  i find that the simple
moment summary statistics performs the best  this
shows that if we have some summary statistics with
nice properties in theory  it is better than other
learning algorithms  this can be seen from the
heat map in figure   a   where the posterior is
centered around  while other methods produce a
scattered posterior  moreover  this also results in
lower standard deviation  comparing figure   b 
 c  and  d   i find that linear regression produces
a very scattered posterior  while neural network
produces a more centered posterior  moreover  the
rbf result is even more scattered  this suggests
that neural network performs better 

b  hidden markov chain model
i use a bistable system that can be characterized by
a hidden markov model  hmm  subject to measurement noise  as illustrated in figure    there are
two states    and    the probability of a transition
from one state to the other is defined as  in both

fi 

table i
p osterior summary statistics for the t ime s eries m odel
posterior statistic

moment

linearregression

rbf

neural network

mean  
mean  
std  
std  
corr    

     
      
     
     
     

      
     
     
     
     

      
     
     
     
      

      
      
     
     
     

  


  

  


  

system 



  

  

measurements 

 

  

fig     an illustration of the hidden markov chain

directions  and the probability to remain in the
same state at each time step is      moreover 
the probability to measure the state correctly is 
 so the probability of an incorrect measurement is
       it is easy to get the stationary distribution
as

table ii
p osterior summary statistics for the h idden
m arkov c hain model
posterior
statistic

linearregression

rbf

neural network

mean  
mean  
std  
std  
corr    

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

 
   

   
 

 
   
 

   

 

   

 

                                         
   

 

   

assume the starting state is always    note that we
cannot estimate both  and  by a simple moment
estimation  because in stationary state they are
unidentifiable  we can only use the non stationary
sequence to infer both of them 
let the prior distribution be two independent
u         and the true values be                   
i will compare the following models 

 

   

   

 

 
   

   

   

   

 

   

   

 a  linear regression

   

   

   

   

 b  rbf
   

   

   

   

   

   

   

   
   





linear regression
radial basis function network 
neural network 

   
   
   
   

   

   

   

   

 c  neural network

the results are shown in table ii and figure    i
find that the results are very similar  except that
the standard deviation is slightly smaller for linear
regression 

fig     posterior estimation for the hidden markov chain
model

fi 

iv  d iscussions
in this project  i propose methods for learning summary statistics via approximate bayesian computation  including radial basis function network  neural network  and some simple methods like linear
regression and moment condition as a benchmark 
the theorem that abc algorithm asymptotically
approaches the posterior guarantees the validity of
the method 
to evaluate  i designed two example  one is the
typical time series model  and the other is a hidden
markov chain model  both of them have the feature
that the dimension of the data is much higher than
the dimension of the parameter  this is the typical
application of abc algorithm  then i use different
methods to learn the summary statistics and compare their performance  i find that a simple moment
estimation outperforms the more complex methods 
moreover  neural network produces a more robust
result 
the drawback of the current approach is that i cannot accurately evaluate the posterior as i dont have
an analytical form of it  a better way to evaluate
different methods would be doing abc updating
for multiple samples  instead of on one sample 
using the property that posterior converges to the
true distribution when the sample size is large 
i can measure the posterior from abc with the
true distribution as a goodness indication  however 
currently i dont have a reliable way to do multiple
updating accurately using the abc algorithm  as
the rejection sampling method loss a great fraction
of data in each updating  probably i can use nonrejection sampling method for the updating part  but
that will be a different algorithm than abc 

r eferences
blum  m  g          choosing the summary
statistics and the acceptance rate in approximate
bayesian computation  in proceedings of compstat      pages       springer 
fearnhead  p  and prangle  d         
constructing summary statistics for approximate
bayesian computation  semi automatic approximate bayesian computation  journal of the

royal statistical society  series b  statistical
methodology                
jiang  b   wu  t  y   zheng  c   and wong  w  h 
        learning summary statistic for approximate bayesian computation via deep neural network  arxiv preprint arxiv            

fi