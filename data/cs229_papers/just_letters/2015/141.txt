forecasting rossmann sales figures
chris jee  cjee   tejinder singh  tejinder 
scpd  stanford university
cjee stanford edu
tdsingh qti qualcomm com
cs     project

 

   introduction
in any supply chain  an ability to accurately predict sales has a direct impact on its operating expenditure 
being able to accurately predict the sales validates understanding of the factors influencing it  a good
understanding of these underling factors enable in taking decisions that can improve sales 
rossmann operates over       drug stores in   european countries  currently 
rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance  store
sales are influenced by many factors  including promotions  competition  school and state holidays 
seasonality  and locality  with thousands of individual managers predicting sales based on their unique
circumstances  the accuracy of results can be quite varied 

   related work
time series prediction is a regression problem  for which there are several popular methods  some of the
methods include linear regression  softmax regression  and support vector regression 
a 
linear regression
linear regression uses ordinary least square optimization to fit a polynomial of degree n to the given data
set to create a model  there are numerous literature discussing linear regression method with various
techniques used for estimating the parameters  including gradient descent  newtons method  etc  one such
literature is      this model performs well for dataset that are mostly linear in nature  however  this method
is not explored in this paper  as it is understood that this model will perform poorly for the given data set
which has non linear components 
b 
softmax regression
softmax regression was presented in cs    lecture notes as a method of generalized linear models for
multinomial distributions  in essence  it applies classification to regression data where the output labels is
not a binomial but spans multiple values  if the label to be predicted has a known range of values with
discrete set of values it can take or the discretization error is acceptable  then this method can be applied as
regression model for continuous data  this method is further explored in later section of this paper 
c 
support vector regression
support vector machine  svm  concepts can easily applied to regression problems while keeping many of
the benefits of svm such as use of mapping functions to map seemingly nonlinear data into mostly linear
data in higher dimensions  use of kernel tricks  individualizing hyperplanes  limiting the model parameters
to support vectors that contribute  and maximizing the margins  because the label is a real number  a
margin of tolerance is set to allow an approximation to the svm 
different kernels and regularization parameters can be chosen to suit the particular data set to improve the
results      further discussion of svm and its application to the given problem is discussed in later section 

   dataset and features

fithe data provided for training tags the stores into four different categories  sale data  the target variable
of every store is provided along with various features like the distance of the nearest competitor 
promotional activities and number of customers to name a few  data provided is a mixed set of continuous
and discrete variables 
training was done across the stores resulting in one prediction model for every store  this made us drop
the features like competition distance and store category which remain constant for any given store  the
four features chosen for training are day of the week  month  promotional activity and school holiday  let
       represent the feature vector at time   as the number of features is small  feature reduction
using pca was not considered  all the approaches tried used the same feature subspace 
sales data  the quantity to be predicted across time  is preprocessed  preprocessing consists of two stages 
in the first stage the minimum recorded sale of a given store      is subtracted from the sale data making
zero as the minimum value of result  the second stage quantizes the sales data into discrete quantization
levels  histogram of the sales data is computed to divide it into multiple bins  where the number of
quantization levels  is controlled by adjusting the width of the bins  

   methods
softmax algorithm
the preprocessed sales data  is quantized such that it can take any one of  values  so that           
 is modelled as distributed according to multinomial distribution  let the parameters          specify the
probability of each of the outcomes  following is the hypothesis for softmax regression
 

  

 

   

     

 

       
 
    

where


 

is the feature vector at time  and 

 

  

 parameterizes the model and       
  represents the     column of 
 is the number of quantization levels
 is obtained by maximizing the log likelihood function is defined as follows

fi 

   

log
 

   

 

     

     

 

       
 
    

a value of  is trained for every store  making one model per store  the predictor estimates the bin   for
every query point  the sale data is reconstructed using the following      in the equation is used to align to
the center of the bin 
                

support vector regression
support vector regression is an application of support vector machine principles applied to regression
problems  the main objective function and the constraints are similar to that of support vector machines 
but it adds new parameter epsilon as the margin of tolerance and c for penalty factor for errors  the
objective function and constrains are shown below     
     

 

 

 
 

        

 
   

            
             
        
the kernel functions transform the data into higher dimensional feature space  to make it possible to
perform the linear separation  several different kernel functions are available  but two of the popular
kernels are polynomial kernel and gaussian radial basis function  rbf   which are shown below     
 
             

 

  
         
         exp
   
the main idea for support vector regression remains the same as that of support vector machines  which
are to minimize error  individualize the separating hyperplane by mapping the input to higher dimension
feature space while maximizing the margin  the introduction of the parameter epsilon and c allows control
of how much error is tolerated and how much errors are penalized respectively 

   experiments discussion and results
softmax algorithm
gradient ascent was employed to estimate the value of  for every store  initially for the ease of
implementation gradient of log likelihood function was computed using first principles  the execution time
for this was extremely slow to the point that the approach was prohibitive in spite of making a vectorized
code  this lead to deriving and computing the following gradient in the closed form 
 
 
  
where

      
 

fi 

  

     

 

 

       
 
    

       

 



 

       

 

 



 

 



 

       
 
    

gradient ascent employing this closed form expression for gradient was approximately  x faster than the
implementation using first principles  the implementation of closed form gradient was tested against the
originally developed gradient using first principles  the error vector magnitude  evm  between the two
implementations was observed to be close to    db 
our initial approach was to try multiple fixed values of learning rate  but the results were not encouraging
and the cost function to be maximized would start approaching in a couple of iterations  this led us to
using a heuristic that would adapt the learning rate in every iteration  every step that resulted in a lower
value of cost function than the current value was reverted and the value of  reduced   was increased in
the steps that showed an increase in the value of cost function  to make the learning rate biased towards
decreasing the factor by which  is increased is chosen to be less than the factor that decreases it  this
approach alleviates the burden of choosing  and convergence can be achieved by executing sufficient
number of iterations 
the minimum number of bins and the width of the bins are the tuning parameters for this approach  large
execution times prevented us from sweeping across various values of these parameters  minimum number
of bins tried was    and     while the width of the bins was set to not greater than     

support vector regression
an important aspect of applying support vector regression is the choice of parameters    c   it is not
immediate apparent what values should be chosen for these  a typical approach would be to do some form
of tuning process to find the optimal value  such as grid search on different combinations of these values to
minimize the root mean square norm error  rmse  of the estimated label 
the svr was tuned by sweeping over various values of c and  and computing the rms error of
predictions  this is a compute intensive process 

    
    
    
   

c     
          
          
          
          

c     
         
          
         
         

c  
         
        
          
          

c  
         
         
          
          

firesults
the following plot summarizes the results obtained by employing the various approaches described above 

   conclusion and future work
for the two methods tried on the problem  svr algorithm performed significantly better than softmax
algorithm  it is also worthwhile to note that the execution time for svr was typically around    minutes
while softmax algorithm took nearly    hours 
given more time and resource  it may be worthwhile to fine tune svr model further for the given problem 
the areas to improve the svr model would be 


group the stores by some combination of common features and choose different set of features for
each group to train svr model optimally  one simple way to group the stores would be to use kmeans clustering algorithm 



given that dataset is time series distribution  introduce weighting factor that puts more emphasis on
more recent results than the older results     



explore use of local support vector regression that allows svr to automatically adjust the parameter 
    

   references
   
   
   
   

bjrck  ke         numerical methods for least squares problems  philadelphia  siam  isbn              
chih chung chang and chih jen lin  training  support vector regression  theory and algorithms 
national taiwan university
alex j  smola   bernhard scholkopf  klaus robert muller  the connection between regularization
operators and support vector kernels  neural networks                   gmd first  rudower
chaussee          berlin  germany
alex j  smola  bernhard scholkopf  a tutorial on support vector regression  neurocolt  technical
report series nc  tr           oct       

fi   
   
   

charles
h
martin 
kernels
part
  
what
is
an
rbf
kernel 
really  
https   charlesmartin   wordpress com            kernels part   
alex j  smola  k r  muller  g  ratsch  b  scholkopf  j  kohlmorgen  using support vector machines
for times series prediction  ruower chausee          berlin  germany
rodrigo fernandez  predicting time series with a local support vector regression machine  lipn
institute galilee universite paris   

fi