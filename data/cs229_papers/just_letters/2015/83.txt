predicting billboard top     songs
julien kawawa beaudan and gabriel garza
julienkb and ggarza    stanford edu
december         

 

introduction

the goal of this project was to create a machine learning project which could
successfully predict whether a song could achieve mainstream commercial success  the motivation for the project was that we are both interested in music 
and were curious whether a machine could successfully predict something that
seems to be based on purely subjective human judgment 

 

dataset

for this project  we used a        song subset of the publicly available million
song dataset  the million song dataset  created through by columbia universitys labrosa and the echo nest  contains data about a million songs
sampled from many music genres  time periods  and places  however  due to
the size of full dataset  we opted to use the smaller        song subset 

   

labels

positive  songs were labeled as positive if the song had appeared on a billboard
top     chart from any month since       when the billboard top    
was created 
negative  songs were labeled as negative if the songs artist had never appeared on a billboard top     list  the rationale for this labeling was
that there were many songs in our dataset by hit artists  which might have
been too similar to that artists hit songs for any classifier to distinguish 
our final dataset included     positive examples       negative examples  and
     non hit songs by hit artists  which we excluded  

   

features

the million song dataset contains both metadata and audio data for each song 
in fact  one of our main challenges was deciding which features to use  since each
song had more data than we could reasonably train on if we included all the
audio data  the features included genre labels  metadata  and    x n arrays for
the pitches  timbres  and loudness at each segment  which typically represents
the beginning of a new note     
 

figenre  each song was hand labeled with many genres by the creators of the
million song dataset  we collected the six most common labels  and then
created a feature vector for each song where each value was an indicator
for whether that label was applied to the song 
pitches and timbres  each song contains a    x n array containing the pitches
at each time slice in the song  in addition  each song also includes two   
x n arrays containing a corresponding numerical value for the timbre and
loudness of those pitches 
metadata  the database contains metadata for each song  we chose to keep
   of the features from the metadata  including song duration  musical key 
mode  major or minor   tempo  year of release  and time signature  we
also chose to include the artist and song hotttness features  a metric
created by the echo nest which represents the popularity of the song
when the database was created  originally  we hoped to use the metadata 
including the hotness feature  as a control or baseline feature to compare
our models trained on other feature sets against 
since we were unsure which features would be the most helpful  we ran all
the models with several different sets of features  just the genre label  just the
metadata  the pitches and timbres from   equally spaced points in the song  the
pitches and timbres for    time slices at    and    seconds into the song  and
finally a combined set using genre  metadata  and the audio data at    seconds 

 

machine learning algorithms

the first classifiers we experimented with were simple linear regression and logistic regression  however  because our data was very imbalanced  only     
positive   both of our regression models performed poorly  so  we also experimented with increasing the penalty for misclassified positive examples  as suggested in a paper we discovered while researching training models on imbalanced
datasets      
we also ran other standard machine learning algorithms  which were nave
bayes  support vector machines  gaussian discriminant analysis  and decision
trees 

 

results and discussion

one of our major challenges was that our dataset was very unbalanced  which
made it very simple to achieve a highly accurate classifier simply by always
predicting false  in particular  svm always labeled all our data as negative 
which is why it is omitted from the following figures  although it was not
extremely surprising that there was no clear decision boundary when trained
on a linear kernel  since there was not a stand out feature that we intuitively
believed would separate hit songs compared from non hit songs   it was a little
surprising that an svm trained on a gaussian kernel also returned the same
results 
because high accuracy was easy to achieve without creating a meaningful
classifier  we choose to measure classifiers by the positive recall and precision
 

firates instead  the results of training on all the combinations of features and
models are shown in the graphs below 
the low precision but
very high recall for both
linear and logistic regression illustrates the reverse problems of the
svm   labeling almost
all the points as positive 
which is equally unhelpful  in order to fix this
problem  we also experimented with an implementation of stochastic
gradient descent for logistic regression  in which
we penalized misclassified
positive examples more
than misclassified negative examples  in practice  however  this did
not improve the overall
positive precision of the
classifier  the weight only
changed the overall fraction of predictions which
were positive  but without actually making improving the positive precision  the values shown
in the chart to the left
are for the weighted regression when the penalty
for misclassified positives
is    times that for misclassified negatives  but
in practice the weight had little effect on the positive precision 
one surprise we had when running the experiments was that metadata 
which included an artist hotttness and song hotttness feature  did not significantly outperform the other feature sets  our original intention was to use
the metadata feature set as a baseline to compare other feature sets on  but
in practice only the naive bayes and gaussian discriminant analysis classified
more accurately on the metadata than on the actual audio 
another interesting result of these experiments was that the naive bayes
classifier performed roughly the same on the random audio   the pitches and
timbres from   equally spaced points in the song   as it did on the sequence of
audio at the    second and    second marks  the two other useful classifiers 
gaussian discriminant analysis and decision trees  significantly improved when
using the sequential audio instead of the random audio  which matches the
human intuition that a human could recognize a song given a   second clip of a
 

fisong  but not based on several random moments in a song  however  this makes
sense in some respects  because the positive precision for naive bayes on all the
audio features is fairly low  about     which is only two times more accurate
than randomly guessing  since the critical assumption with naive bayes is that
the features are independent   which is clearly not the case for audio features
of music   this could mean that the independence assumption is equally bad for
all sets of audio features 
feature set
metadata
metadata
audio     sec
audio     sec
audio   metadata
audio   metadata
audio   metadata

algorithm
gda
decision tree
gda
decision tree
gda
naive bayes
decision tree

accuracy
      
      
      
      
      
      
     

  recall
      
      
      
      
      
     
     

  prec 
    
      
      
      
      
      
      

f measure
     
     
     
     
     
     
     

table    the results of our best classifiers on the testing data  models trained
on audio at    seconds performed very similarly to audio at    seconds 

one last observation about our results is that we were actually able to classify
songs quite well using only the audio information  represented as an array of
frequencies and pitches  this was quite surprising to us  because even the
authors of the million song dataset reported that the audio information encoded
in the dataset is not sufficient to accurately reconstruct the song  we assumed
that because the song would not be clearly recognizable to a human  a computer
would not be able to classify hit songs  in addition  these results are surprising
considering that popular music today is very different from popular music when
the billboard top     list first began in       it seems very plausible that a
hit song from the   s would not be successful at all in the   s  there are
two possible explanations for the success of our classifiers despite this obstacle
  either our models identified some universal features for detecting hit songs 
or the models actually learned different clusters of features corresponding to
different types of music popular throughout the years  since the two algorithms
which consistently outperformed the others were gaussian discriminant analysis
and decision trees  which are capable of capturing multiple peaks or modes 
it seems much more likely that the second option is the case 

 

conclusion

we successfully trained several classifiers to recognize billboard top     songs
using data provided in the million song dataset with significantly greater accuracy than randomly guessing or always guessing false  overall  our best classifier
based on recall and precision rate was a gaussian discriminant model on the
metadata features  however  we also had success classifying using the audio
pitches and timbres data provided by the million song dataset  using gaussian
discriminant analysis and decision trees 

 

fi 

future work

although our project successfully classified hit songs using the data provided
by the million song dataset  there are many directions for future work  in
particular  both academic papers     and past cs     projects     have focused
on working with music in the midi format  which contains information about
the instruments and actual notes being played  as opposed to an array of pitches
being played  the midi format is cleaner and easier to analyze than a simple
array of pitches  because many instrument sounds are combinations of different
frequencies  for example  percussion sounds often span many frequencies  and
make it difficult to analyze the main components of music when using an array
of pitches  while the midi file would isolate the percussion track from other
instruments 
in addition to making analysis of single instrument voices much easier  the
midi format would also allow using time series analysis of the music  a major flaw of representing music as an array of pitches at each time slice is that
this representation fails to capture the time variation of music  midi files  in
contrast  would allow extraction of features such as rhythms  recurring note
patterns  and harmonic structure 
finally  another research possibility which seems promising is using lyrics
to predict song success  the use of lyrics  either in as a bag of words or as
n grams of words  could be very interesting as a feature for predicting song
success  however  we decided against using lyrics for this project because the
task of collecting lyrics for each song would be time consuming and also require
us to remove many songs from our dataset which contain no lyrics or were sung
in another language 

 

acknowledgments

we would like to thank prof  andrew ng and the cs    teaching staff for
teaching the machine learning course 

references
    ardnt  carl fredrik and li  lewis  automated transcription of guitar music  cs     projects       dec        web 
    s  dubnov  g  assayag  o  lartillot and g  bejerano  using machinelearning methods for musical style modeling  ieee       august      
web 
    provost  foster  machine learning from imbalanced data sets  new york
university  new york university  web  accessed dec          
    thierry bertin mahieux  daniel p w  ellis  brian whitman  and paul
lamere  the million song dataset  in proceedings of the   th international
society for music information retrieval conference  ismir             
    wang  kedao  predicting hit songs with midi musical features  cs    
projects       dec        web 

 

fi