 

cuisine classification from ingredients
boqi li  mingyu wang

abstract
in this report  the team aimed to classify    types of
cuisines by analyzing the ingredient lists  the team approached this problem by building several feature sets and
trained them with different classification algorithms  the
result shows that the logistic regression is able to achieve
the highest performance with an accuracy of        the
error distribution of each class is analyzed for further
investigation 
i  introduction
yummly has provided a large dataset of recipes in which
each entry has the type of cuisine and the list of its
ingredients  the goal of the project is to categorize a
dishs cuisine type  indian  italian  chinese  etc    by
analyzing its ingredient list  since there are    classes in
the cuisine set  the team will use several multiclass classification techniques including ova multiclass naive bayes 
ova r  regularized logistic regression  ova multiclass
svm  ova multiclass svm with crammer method and
k nearest neighbor and compare the performance among
these methods  the input of our algorithm is a list of
ingredients of some recipe  and the output is the predicted
cuisine  the team will also modify the features to improve
performance  this project has an intriguing application 
image when people search on yummly for certain type
of cuisine or browsing related recipes  this algorithm can
help yummly decide which recipe to display or recommend
recipes that people may be interested in to improve user
experience 
ii  related work
after a literature search  the team found several
ways to approach multi class classification problem 
one well studied method is called the error correcting
output coding method  which divides the multi class
classification problem into a series of binary classification
problems  which is suggested by rennie and rifkin    
the advantage of this method is that  by reducing the
problems to binary classification  many well known
algorithm can be applied  such as naive bayes and
svm  the problem is that since this method trains the
classifiers independently  it doesnt take the correlation
between classes into account  another newer method that
considers the correlation is introduced by crammer and
singer  which is a modified multi class svm     apart
from these two methods  trudgian and yang suggests
using k nearest neighbor to approach the classification
problem  the advantage of k nearest neighbor is that it

can operate quickly without sacrificing accuracy greatly    
there are rigorous researches in text classification  in
     a variety of feature selection and learning schemes are
discussed  to reduce dimension of feature space  document
frequency thresholding  information gain  mutual information and term strength are compared  in our paper 
we applied tfidf and mutual information on our dataset 
in      tfidf is discussed in detail  while this method is
borrowed from information retrieval  it doesnt utilize the
privilege of labeled training set in supervised learning  a
new term weighting schemes is proposed and compared
with other schemes  in this paper  the group adopted the
classical tfidf term for simplicity 
iii  dataset and features
our dataset was obtained from kaggle competition
whats cooking  https   www kaggle com c whatscooking data         examples from    cuisines are
available in total  the training set has      unique
ingredients which are considered as features in our
dataset  uniqueness is in the sense that the strings are
different from each other  one recipe is represented
by a feature vector  in constructing feature vectors 
we considered different approaches  notations in
this paper follow the convention of our class  let
s     x      y         x      y         x      y              x m    y  m    
be the training set  and each x i  is a subset of the
vocabulary v  which means that it contains some
 i 
ingredients combination  xj     indicates feature j
occurs in example i and   otherwise  j                    n 
y  i                       which correspond to the    cuisine
types 




binary representation  if the ith ingredient appears in the example  then the ith term in the
feature vector is set to one  and zero otherwise 
in the recipes context  a ingredient cannot appear
more than once  so this representation can be
regarded as a frequency of the ith feature 
tf idf frequency  tf  xji   is the term frequency of
ingredient i in recipe j  tf     if recipe contains this ingredient and   otherwise  idf  xi   s 
is inverse document frequency to measure how
common a ingredient is in our dataset  idf  xi    
 s 
where m is the number of examples
log  xs x
i    
in the training set and  x  s   xi      is the
number of recipes which certain ingredient appears  then  tf idf is calculated as tf idf  xji   s   
tf  xji    idf  xi   s 

fi 

having observed that our dataset is highly biased as
in figure     the group also assigned different weight to
achieve a uniform cuisine distribution  however  it doesnt
show improvements in the prediction result 

mutual information of      ingredients
    

    

mutual information

   

distribution of examples in each cuisine
    

    

    

    
    
    
    

  of examples

    

 

    

 

    

    

    
    
ingredient

    

    

    

    

fig     plot of mutual information of      ingredients

    

    

 

 

 

 

 

 

  
  
cuisine

  

  

  

  

  

fig     distribution of cuisine examples in training set is highly
biased  italian  mexican and southern us cuisines have the most
examples while others have much less examples 

then  before selecting features  the group removes any
digits  measurements  punctuations and content inside
parenthesis  the brands in the dataset are also cleared
and all strings are converted to lowercase  in reducing
the number of features  several alternatives are studied 






one major problem with the feature data is that
it contains many low content or synonyms of descriptive words  for example   low fat reduced
fat  fat reduced  mean the same thing and
 natural  fresh  artificial  are considered
to be low content and should be removed to
combine ingredients  after these steps  strings
like    low fat chocolate milk are converted
into chocolate milk  to stem different variants
of a word  stemming algorithm is performed  in
this paper  porter stemming algorithm is used to
remove plural form of words  these steps let us
reduce the number of features to      
an alternative way is mutual information of
features  plot of mutual information is shown
in figure    only the      ingredients with the
largest mutual information are taken into account
in the following results 
the group also considered selecting the features
according to the number of occurrence  only
ingredients that appeared more than   times are
considered  which gives us      features  this can
help us exclude ingredients with typos  brands or
redundant descriptive information  to study the
influence of number of features on our prediction
model  different number are chosen and studied 

furthermore        examples are used as training set
and      examples are used as testing set to evaluate the
classification models 

iv  methods
multi class classification involves labeling among m
classes with n      and in this project n       the team
then applied error correcting output coding introduced
by dietterich and bakiri  as is mentioned in previous
section  the error correcting output coding method links
multi class classification with binary classification  it allows approaching multi classification problem by training
a series of binary classifier  in this project  the team
choose the code matrix r                     to be
one vs  all matrix  ova  with dimension of        the
matrix has   on its diagonal  and    on all other places 
the columns of r corresponds to    binary classifiers
 f  f     f      and the rows of r corresponds to the   
classes  for each classifier  there will be only one class
labeled as    with all the rest labeled as     for each
training instance  x i    y  i     the label trained with class
fj is r y  i    j   each classifier will be trained with all the
training instances  when predicting the label of a new
instance x  each classifier will predict on x  and form a
row vector rx    f   x   f   x   f   x   f   x      f    x    the
row vector will be compared with every row of matrix r
 r  r     r    by computing the dot product between rx and
ri   i               
si  

  


rx  k   ri  k 

k  

if the k th elements of rx and ri have the same sign 
a positive number will be added into the summation 
otherwise a negative number will be add  therefore the
row ri that is closest to rx will have the largest dot
product 
when comparing between rx and matrix r  if fk  x      or
   then only the signs are considered and the magnitude
information t x is lost  which means the confidence of the
label predicted is not taken into consideration  therefore 
the team managed the output of each classifier to be
t x for the case of svm  this becomes  t x   b  and for
naive bayes  p  y     x   p  y     x  is used  and the

fi 

hinge loss function is applied to predict the class  finally 
the predicted label of instance x is 
h x    argmin

  


g rx  j   r c  j  

c            j  

where g s        s     binary classification algorithms
including naive bayes  logistic regression and svm are
applied based on error correcting output coding  the
description of each algorithm is described in the remaining
part of this section 

b  logistic regression
logistic regression is a robust classifier that does well
with a larger training set  which makes weaker model
assumption  it generally performs well even if the model
distribution is non gaussian  the hypothesis function still
uses sigmoid function but in the end the logistic regression
classifier will return the value of t x instead of g t x  
in order to be fetched to the hinge loss function  the
hypothesis function is 
 
h   g t x   
    et x
and the stochastic gradient ascent rule is 
j    j    y  i   h  x i    x i 

a  naive bayes classification
naive bayes is a binary classification algorithm  which
is good for text classification purpose  let function f be
the classifier that can achieve the labeling of   labels  and
with a new example x  the predicted label is 
f  x    p  y     x   p  y     x  
where
p  y     x 
p  x y     p  y     
 
p  x 

 i y      y     
i

  
 
 i y      y         i y      y     
i

i

p  y     x 
p  x y     p  y     
 
p  x 

 i y      y     
i

  
 
 i y      y         i y      y     
i

and after training every parameter of naive bayes with
the training set of size m  the parameters are shown as 
m


 j y     

 

i  

m


 j y     

 

i  

m


 y     

 

 i 

  xj      y  i      
m


  y  i      

i  
 i 

  xj      y  i      
m


  y  i 

s t 
y  i    t x i    b      i   i         m
i     i         m
according to yang and liu      the linear kernel works
better than non linear kernel on text classification application  therefore  linear kernel is used on svm 

and

i

c  support vector machine
support vector machine is binary classifier that finds
the optimal margin between two classes of data  the
hypothesis function is h   g  t x   b   where g   when
 t x b    and g    when  t x b     still the classifier
will return the value of  t x   b  which will be used in the
hinge loss function  in this project  the regulated support
vector machine is applied and the optimization problem
becomes 
m

 
min  b         c
i
 
i  

d  multi class support vector machine by crammer and
singer
the next model is a multi class support vector machine
classifier based on crammer and singer  the difference
between their svm method and the ova svm method
is that  in ova svm the correlation between each class
is not considered  because the classification is divided into
training multiple independent binary classification problem  in the svm by crammer and singer  this correlation
is taken into account  the multi class svm is trained via
the following optimization problem 

 
max q      
k xi   xj   i  j     
i   yi
  i j
i
subject to 
ii   yi
i        

    

i  

  y  i      

i  

m

laplace smoothing is used and  v  is the size of the
vocabulary 

and the function h is 
h x    argmax
r

 


 
i r k x  xi    

i

the team used the liblinear module with linearoption  s
   which corresponds to the multiclass svm classifier 

fi 

e  k nearest neighbor
the last algorithm used is the k nearest neighbor
algorithm  the k nearest neighbor compares the new
instance as query point with all other instances in the
training set and selects the k nearest instances to the query
point  using ova coding matrix  there will be    k nearest
neighbor classifiers  each classifies one binary classification
problem  and
  
l   
d  j
j
l   


j

 
d  j

where d  j is the distance between the jth neighbor of the
query point that has label    and d  j is the distance
between the jth neighbor of the query point that has label
    the output is
l   l 
  which will be used in the hinge loss function  since every
instance x in the training set is a set of a combination
of the feature set  it is tested that the jaccard distance
is the desired distance metric for k nearest neighbor that
outputs the best performance  the jaccard distance is one
minus the jaccard coefficient  which is shown as 
jdistance      j a  b   

 a  b    a  b 
 a  b 

neighbor  the number of nearest neighbor k is equal to   
the team tested with several values of k ranging from
  to     it appears that the resulting accuracy doesnt
increase much with k increases from   to     however the
computation time of the code increases a lot  therefore
k   is selected for knn 
for every classification algorithm and feature set       
examples in the dataset is selected to form the training set 
and      examples is selected to form the test set  this
data allocation is pre determined and the same for every
classification algorithm and feature set combination  no
randomization is utilized  the result is shown in table i 
table i shows that the best performance observed is
from the logistic regression with the combination and
reduction feature set  the logistic regression and the
two svm classifiers have much better performance than
naive bayes and k nearest neighbor  different feature
sets have very limited influence on the result of logistic
regression and the two svm classifiers  the improvement
compared to the original feature set is very low  the
mutual information feature set increases the accuracy of
the naive bayes classifier by     which is the highest
improvement among all feature sets in the naive bayes
case  the combination and reduction feature set has the
highest increase of accuracy to the k nearest neighbor 
which is about     the test error of logistic regression

v  result and discussion
using the classification algorithm discussed in the previous section  the team is able to carried out results with
  different feature sets on each algorithm  every feature
set is compared to the original feature set and the primary
goal is the find the feature set and classifier that yields the
highest test accuracy 
the first feature set contains all of the original     
features  this is set as a baseline for comparison with other
feature sets created as described in the feature selection
section 
the second feature set contains      of original features
selected with the highest occurrence in the whole training
set  every feature in this feature set appears at least  
times in the training set 
the third feature set contains      of the original
features selected with the mutual information between the
features and classes higher than      
the fourth feature set contains      new features created by combining all the same key words  regardless of
any descriptive words 
the last feature set contains      new features created
by first remove all the low content descriptive words and
then combining the same key words which is manually
controlled so as not to be over simplified 
for the two svm classifiers  the regulation coefficient c
is equal to      the team tested with several coefficient c
ranging from      to    the result shows that with c     
the svm classifiers yields best accuracy  for k nearest

fig     test error for logistic regression with different feature sets

with increasing training set is shown in figure    it can
be seen that applying any of the feature selection other
than combination and reduction doesnt improve the result
compared to the original feature set  and the combination
and reduction feature set only improves the performance
slightly  with increasing training set size  the test error
can still decrease 
the confusion matrix is shown in table ii  the element
in row i column j shows the percentage at which cuisine i
is classified as cuisine j  blue cell boxes show the correct
classification rates and red cell boxes show the error
classification rates greater then      compared with the
cuisine distribution in figure    it can be seen that the
classes with high mislabel error rate are those with fewer
numbers of examples  it also shows that several classes are
often mislabeled as class       and     whereas class    
   also have a high accurate rate  which means that those
classes are similar to these classes 

fi 

table i
overall performance for each classification algorithms
original features
reduced occurrence
mutual information
key words combination
combination reduction

ova nb
     
     
     
     
     

ova svm
     
     
     
     
     

ova logistic
     
     
     
     
     

svm by crammer
     
     
     
     
     

knn
     
     
     
     
     

table ii
confusion matrix  unit    
 
    
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

 
   
    
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   
   
   
   

 
   
   
    
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

 
   
   
   
    
    
   
   
   
   
   
   
   
    
   
   
   
   
   
   
    

 
   
   
   
   
    
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

 
   
    
   
   
   
    
   
   
   
   
   
   
   
   
   
    
   
    
   
   

 
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   
   
   
   
   
   

 
   
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   
   
   
   
   

 
   
   
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   
   
   
   

vi  conclusion and future work
throughout the project  the team made five different
feature sets including the original feature set  and put
them into tests with ova naive bayes  svm  logistic
regression  svm by crammer and singer and k nearest
neighbor 
the result shows that logistic regression yields the best
performance the team believed that the reason logistic
regression has the highest accuracy is related with the fact
that logistic regression performs well with large training
set with non gaussian distribution 
even though it has the highest accuracy  the performance of logistic regression is closed to the performance of
the two svm classifiers compared with the performance of
naive bayes and k nearest neighbor  it is make sense that
svm tends to yield better performance since it is a robust
algorithm  the fact the different feature sets doesnt change
the performance too much is probably because changing
the features doesnt effect the forming of support vectors
a lot 
the confusion table shows that some classes are highly
correlated  this correlation reduces the accuracy of the
algorithm  given more time  the team would investigate
deeper into the correlation and came up with way to better
distinguish these specific classes 
the table also indicates that the classes error rate is
also influenced by the number of instances of these classes
in the training set  since the training set is not uniformly

  
   
   
   
   
   
    
    
   
   
    
   
   
   
   
   
    
   
    
   
   

  
   
   
   
   
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   
   

  
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
   
   
   
   
   

  
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
   
   
   
   

  
    
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
    
   
   

  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
   
   

  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   
   
   

  
    
    
    
   
   
   
   
   
    
   
   
   
   
   
   
    
    
   
   
   

  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
   
   

  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
    

distributed with each class  the team would find a better
way to balance them  since simply duplicating the classes
with small instance size didnt yield better result 
references
    j  rennie  r  rifkin  improving multiclass text classification
with the support vector machine   revised  april      
    k  crammer  y  singer  on the algorithmic implementation of
multiclass kernel based vector machines   journal of machine
learning research       
    d  trudgian z  yang  spam classification using nearest neighbour techniques   proceedings of the  th international conference on intelligent data engineering and automated learning 
pp        revised       
    yang  yiming  pedersen  jan o  a comparative study on feature
selection in text categorization   icml  vol      pp          
     
    ko  youngjoong  a study of term weighting schemes using class
information for text classification   proceedings of the   th international acm sigir conference on research and development
in information retrieval  vol      pp                  
    yang  yiming  and xin liu  a re examination of text categorization methods   proceedings of the   nd international acm
sigir conference on research and development in information
retrieval  acm       

  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    

fi