the price is right  estimating medicare costs with machine learning
sarah rosston and samantha steele

i  introduction
historically  healthcare costs in the united states have
been difficult to quantify as there have been no incentives for
hospitals to publicize their pricing  however  in       medicare released the inpatient prospective payments system that
for the first time publicized price lists for hospitals across
the united states  this dataset reveals vast discrepancies
in pricing for similar procedures across different hospitals 
even within the same broad geographic region  consider
one particularly prevalent condition  heart failure  which
affects more than three million americans annually  patients
in baltimore  maryland are charged on average         
for treatment of heart failure and shock  at a suburban
hospital    miles away  patients are charged only         
a five factor discrepancy  figure   highlights the broader
geographic discrepancies for this disease across the united
states 
although insurance agencies generally negotiate these
prices to more standardized payments  the problem is particularly relevant for patients without insurance  they lack
the resources to negotiate and may pay exorbitant prices 
consequently  our goal is to identify the predictive factors
behind these discrepancies and create a model for more
accurately predicting the price of a procedure for the patient
and the medicare system  we explore four potential models for cost prediction  linear regression  regression trees 
support vector regression and neural networks  we compare
the performance of each model  finding that neural networks
most accurately model healthcare prices 

fig     heatmap of prices of treatment for heart failure and shock across
the united states  red areas represent the highest prices 

of specialty physicians in the area in the first stage  and
predicting costs in the second stage  medicare costs of cancer
are predicted with r  values ranging from      for prostate
cancer to      for breast cancer  although we also use
demographic data  we use it at a different aggregate level 
focusing on average hospital costs and producing a better
linear regression predictor  see results below  
to our knowledge  no previous papers have applied nonlinear methods to the examination of healthcare prices on a
per hospital basis  however  on a per patient basis several
papers have used non linear approaches  bertsimas et al     
show that more complex models are better for cost prediction
than linear regression  using clustering and classification
trees  they achieve optimal r  values of       although most
of their models produced much worse results  sushmita et
al       use regression trees  m  model trees  and random
forests to predict cost data from medical and claims history 
as a baseline  they use simple linear regression with previous
cost as the predictor  which had a lower rmse than their
more complicated models in most scenarios 
the most successful application of non linear approaches
that we uncovered is lee et al      which improves on
the work of penberthy et al by using regression trees
and neural networks to predict payments for colorectal
cancer patients at a single hospital in korea  their r 
values are substantially higher than penberthys        for
regression trees and       for neural networks   however 
their work focuses on a tightly controlled patient set at a
single hospital  incorporating only patient records  while
its conclusions cannot be directly generalized to a highly
diverse  large scale healthcare system like that of the united
states  lee et al is suggestive  as our work confirms  that
appropriate non linear techniques perform better 

ii  related work
previous healthcare studies have predominantly used linear
regression to model healthcare costs  smith et al      
attempts to solve a similar problem to ours  predicting
payments to charge ratios  pcr  for medicare by hospital 
the paper uses a regression model with features including
casemix variables  hospital characteristics  and state characteristics  however  their models do not take into account
the fine grained  county level demographic data used in our
approach  using their variable set  smith et al  modeled pcr
with rmse of      for medicare  penberthy et al      use
similar variables to smith et al  to predict the costs of cancer
in individual patients  penberthy et al  use patient specific
variables including cancer stage  but also factor in county
level data including percent of people with below secondary
education  percent of population    or older  percent of
population that is not white  and income data at the zip
code level  they use a two stage least squares model  using
many of the population statistics to estimate the number
 

fifig    

fig    

normal q q plot for linear fit

residuals vs  fitted values plot

iii  dataset and preprocessing
while many costs associated with treating disease are not
available in public datasets     data about medicare costs is
freely available from the center for medicaid and medicare  a rich dataset on the inpatient prospective payment
system from      chronicles costs of the most common
    medical procedures from medicare patients  overall 
the dataset includes information on about     of medicare
discharges in that year  to narrow our focus within the
dataset  we concentrate on the costs of treating heart failure
and shock with comorbid conditions  one of the four most
common health issues in the dataset  the data covering
heart failure and shock contains information about costs at
      hospitals across all    states  accounting for        
discharges and                   in payments  in addition to
payment data  the center for medicaid and medicare has a
wealth of data on hospitals  drawing on this  we supplement
our initial dataset with information about each hospital 
including type  ownership and patient hospital ratings  the
patient hospital ratings datasets reported not available for
a fraction of hospitals  to include this potentially important
feature  we limit our dataset to       hospitals that reported
complete survey results 
we also include economic and demographic data drawn
from the us census  each of these datasets were preprocessed to remove incomplete data and joined to the
hospital data on a county by county basis  they include
data on income  race  education  poverty level  and age
distribution  these combined datasets provided    potential
explanatory variables across the       hospitals 

fig    

histogram of heart failure and shock costs

we corrected for heteroscedasticity in two ways  we standardized the features of our data set to have zero mean and
unit variance  this counteracts variation between features of
difference scales  in models that permit weighted training
inputs  heteroscedasticity can be corrected by using weights
that are inversely proportional to the variance of each point 
wi  

 
i 

however  the true values of the i are unknown  to determine a proxy  we examined potential root causes of heteroscedasticity  our data give equal weight to hospitals where
data has been collected from only one patient or from over
     data backed by fewer patients is likely to have higher
variance  since smaller values of the variable total patients
likely correspond to higher variance  we approximate weights
with
wi     of patients per datapoint
these values are used on the inputs to both weighted least
squares and neural networks 

a  heteroscedasticity

b  outliers

from early results  we hypothesized that our data was
heteroscedastic  heteroscedasticity is the property of having non uniform variance among the elements of the data
set  because linear regression relies on the assumption of
homoscedasticity  datasets that violate this assumption tend
to perform poorly  to evaluate this conclusion  we plot
residuals vs fitted values  fig     while a homoscedastic plot
would show evenly distributed values  this cone shaped data
confirms strong heteroscedasticity 

even from government sources  data about medical costs
is often inaccurate or incomplete      because of these
innacuracies  as well as the large number of hospitals in our
dataset  we expected to see outliers in costs  a normal qq plot  shown in figure    confirms these suspicions  and
shows that data in the highest cost quantiles is not normally
distributed  the histogram in figure   shows  among mostly
evenly distributed data  a few cases where average costs
 

fiare above         for a condition that averages         
these points correspond to the ones identified as outliers
in the normal q q plot  and indicate either a few very
expensives cases or inaccurate data  because these few data
points are so far from the rest of the dataset  they exert high
leverage on the model  decreasing its accuracy  to account
for outliers  we excluded the top and bottom decile of costs
from our analysis as the data points may either be innacurate
or include only a small number of patients 
iv  metrics
to assess the validity of our models  we chose two main
metrics  the r  coefficient measures correlation between
variables to assess goodness of fit 
adjusted r   

fig    

boxplot of average payments by state

thus  the optimal  satisfies the following equation 

p
 
i  yi yi     np  
p
    n  
 y
y 
i
i

   arg min  k      

it takes on a value between   and    with   indicating
a perfect fit  the r  value can only increase as additional
features are added  consequently  we use adjusted r    to
correct for this bias 
to evaluate our generalization error on the cross validation
set  we use the root mean square error 
qp
 
 
i   yi yi    
rmse  
n



in forward selection  features are added greedily until a
new feature would no longer decrease aic  using forward
selection     features were selected from the original set of
    and using backward selection    features were chosen 
after weighting average costs by the number of discharges 
both forward and backward selection chose significantly
more features     and    respectively  while the feature set
was substantially reduced with model selection  the adjusted
r  value only changed slightly  increasing to        with
forward selection and        with backward  one reason
for the small changes is that the hospital state contributes
substantially to the number of features and is a major factor
in cost as shown in figure    showing a boxplot marking
the   th and   th percentiles of cost for each state  we then
explored weighted least squares using the weights described
in section iii  the features selected for the weighted model
included more economic data than the unweighted model
which included a more even array of economic and demographic data  weighted least squares significantly improved
our estimates  raising the adjusted r  value to       and
lowering the rmse to         

this measures the distance between estimated and actual
values and must be compared with the mean value to
contextualize scale 
v  models
our baseline nave model implemented simple linear regression in r with all    features used to predict prices 
this model achieved an r  value of        and a rmse
of          in a feature dataset where the average price is
         
a  model selection
the initial regression model used all of the variables in
our dataset  but many of the coeffecients have high p values 
an indicator that they are not significant and that the model
may be overfitting  feature selection is a clear solution to
this overfitting problem and we explored using both forward
and backward selection  because our data set contains a large
number of features  some variables are likely correlated  we
therefore hypothesize that forward selection would be the
better choice because it is biased towards choosing fewer
variables  since one major research goal is to identify the key
features that affect hospital price  we investigate using both
forward and backward selection  this permits us to examine
differences in which features were selected 
to implement feature selection we use the akaike information criterion  aic   aic provides a method for comparing models to chose whether to add or subtract a feature 
balancing goodness of fit with model complexity  the aic
measure for a model is calculated according to the following
function
aic    k      

b  regression tree
because lee et al   bertsimas et al   and sushmita et al 
discuss the value of tree based models  we hypothesized that
a regression tree model would better explain medicare costs
than linear regression  the model would make sense for
predicting medicare cost because some features  such as a
hospitals state  could be significant enough to change the
weights on every other variable  an interaction not captured
by linear regression  to create a regression tree  we use the
cart algorithm  which initially groups all nodes together 
and then finds the split that minimizes the node impurities as
measured by the sum of squared errors  if the reduction in
node impurity is above a threshold  the node is split and the
process is repeated on the child nodes  individual regression
models are created for each leaf node 
we found that using regression trees did not substantially
improve our r    shown in figure    the final tree  shown
 

fifig    

fig    

test and train r  with a regression tree

color matrix for the svr parameter sweep

i   i    

fig    

the  and   act as slack variables that permit certain
outlier data points to exceed the epsilon curve boundary
without forcing a drastic change in the model  because   w  
is non convex  this primal objective is difficult to optimize 
instead  the lagrangian dual formulation is used  finally  a
gaussian kernel permits support vector regression to model
non linearity by mapping the data to a higher dimensional
feature space  we conducted a parameter sweep over      
          to determine the optimal values for for c and
  figure     the optimal parameters of      and     
produced a rmse of     

regression tree using cart  n   number of points in each node

d  neural networks
 

neural networks are known to exhibit superior behavior
in modeling non linear problems  we utilize a feedforward
neural network that implements backpropagation via the
levenberg marquardt algorithm in the matlab neural networks toolkit  the levenberg marquardt algorithm varies
between gradient descent and the gauss newton method via
the following update rule 

in figure    has    leaf nodes and has an r value of     
and rmse of       the tree performed similarly to linear
regression  which indicates that the interactions between
features may be too complex to model with a decision tree 
while other papers found trees useful for cost prediction 
they were predicting costs for individual patients and features
that make decision trees successful for predicting costs for
individuals may not be as relevant for predicting hospitallevel costs  while the tree did not provide substantially better
results  it did identify state  hospital ownership  and race as
some of the strongest factors  which is consistent across all
of the models we applied 

i    i   
where  is the solution to 
 j t j   diag j t j     j t  y  f     
the value of lambda causes the algorithm to vacillate
between gradient descent and gauss newton like behavior 
this permits the algorithm to use the efficiency of gaussnewton near local minima and the robustness of gradient
descent  gavin        
the neural network is composed of an input layer  one to
two hidden layers and an output layer as shown in figure x 
initially  a mean square error target cost function was used 
p
j     n  i ei

c  support vector regression
despite modifications to our linear regression models and
the use of regression trees to better model the data  linear
models consistently demonstrated relatively high training and
test errors  our relatively low r  values  which are similar
to those in multiple studies that use linear models to predict
costs on a per patient basis  suggest that us healthcare costs
are nonlinear  consequently  we explore models that capture
this behavior  support vector regression fits a model such
that all data points lie within  of the prediction curve  while
maximizing the smoothness of the prediction curve  support
vector regression optimizes the primal 
pl
minimize      w      c i    i  i  
s t  yi    w  xi   b     i
  w  xi    b  yi     i

 
however  this model tended to overfit the training data
across a variety of parameters  discussed below   consequently  a regularization parameter was introduced to correct
this high variance 
p
p
j       n  i ei            n  i wi   
 

fifig     

fig     the rmse for neural networks with   to    neurons per layer 
network configurations are show for   hidden layer    hidden layers   
hidden layers with     regulation parameter  and   hidden layers with    
regulation parameters

neurons
 
 
  
  
  
  

  layer
train test
    
    
    
    
    
    
    
    
   
    
    
    

  layer
train
    
    
    
    
    
    

test
    
    
    
    
    
    

    reg
train test
    
    
    
    
    
    
    
    
    
    
    
    

    reg
train test
    
    
    
    
    
    
    
    
    
    
    
    

error histogram for optimal neural network

vi  results
for linear regression  regression trees and support vector
regression  a random sampling of data was used to create a
    holdout cross validation test set  for neural networks 
random sampling created a     validation set and     test
set  as shown below  support vector regression and neural
networks consistently outperformed our linear models 

    reg
train test
    
    
    
    
    
    
    
    
    
    
    
    

fig      r  values for the training and test set under various parameter
configurations

this regularization parameter penalizes large weights and
tends to correct overfitting 

model

r 

rmse

lin reg
w  lin reg
reg  trees
svr
nn

    
    
   
    
    

    
    
    
   
   

this confirms our hypothesis that us healthcare data is
non linear  hospitals have a high degree of flexibility in pricing their chargemasters  a master list of prices for individual
services that is used to negotiate for reimbursement with
healthcare insurers and other payers  including medicare 
this variability likely contributes to the non linearity of
hospital pricing data  our optimal neural network produced
a rmse of only      roughly      of the mean  figure     

e  parameter fitting
the hyper parameters of a neural network must be tuned
to optimize performance  in our model  the key hyperparameters are the number of hidden layers  the number of
neurons per layer and the regularization factor  one to two
hidden layers are standard for most neural networks  and
networks with at least one layer can reproduce any nonlinear function given sufficient neurons  however  a second
hidden layer can decrease the number of required neurons
and remove neuron interdependencies that may interfere
with fitting  tamura        chester        consequently 
we hypothesized that two hidden layers would give optimal
performance  our results in figured   and    validated that
theory 
the network was tested on configurations of          
       and    neurons per layer  larger concentrations of
neurons tended to overfit the data  leading to almost perfect
fit in the training data and high generalization errors  figure
         neurons per layer were ideal for most configurations 
a regularization parameter significantly improved the rmse
and reduced overfitting  a parameter sweep of              
    was conducted to determine the optimal regularization 
for summary purposes  results for a subset of regularization parameters are shown in figures   and     the    
regularization value produced both the lowest rmse and
highest adjusted r  values  consequently  our optimal neural
network consisted of   hidden layers each with    neurons
and     regularization 

vii  conclusion and future work
due to the non linear structure of medicare data  nonlinear models including support vector regression and neural
networks are better at predicting hospital prices than linear
regression and regression trees  neural networks had the
highest r  and lowest rmse of all of the models we tried 
we hypothesize that non linear models better capture the
complex interactions that lead to pricing  and that neural
networks are more successful than svr because svr is
limited to capturing linear hyperplanes in high dimensional
spaces  despite their different outputs  our models agree
that state  hospital ownership  and race are among the most
highly influential factors in determining the medicare costs
related to heart failure and shock  in future work  modeling
the impact of hospital market density and predicting the
availability of cardiac doctors could improve our models by
providing better supply side data  we also hope to explore
how our results generalize to a wider range of hospital
conditions publicized by medicare 
 

fir eferences
    bertsimas  dimitris  et al  algorithmic prediction of health care
costs  operations research                        
    booth  ash  enrico gerding  and frank mcgroarty  predicting equity
market price impact with performance weighted ensembles of random
forests  computational intelligence for financial engineering and
economics  cifer        ieee conference on  ieee       
    dunn  d  l   a  rosenblatt  d  a  taira  e  latimer  j  bertko  t 
stoiber  p  braun  s  busch        a comparative analysis of methods
of health risk assessment  society of actuaries monograph m hb     
    bertsimas  et al  algorithmic predication of healthcar costs operations research  vol     no    november december      pp          
    chester  daniel l  why two hidden layers are better than one 
proceedings of the international joint conference on neural networks 
vol          
    hancock  jay  attention  shoppers  prices for    health care procedures now online  npr     february     
    lee  sm  kang  jo  and suh  ym  comparison of hospital charge
prediction models for colorectal cancer patients  neural network
vs  decision tree models journal of korean medical science     
october      pp         
    mackenzie  andrew  et al  predictive healthcare cost modeling using
regression  issue    society of actuaries conference       
    penberthy  et al  predictors of medicare costs in elderly beneficiaries
with breast  colorectal  lung  or prostate cancer  health care
management science       
     smith  et al  predicting inpatient hospital payments in the united
states  a retrospective analysis  bmc health services research       
     sushmita  shanu  et al  population cost prediction on public healthcare datasets  proceedings of the  th international conference on
digital health       acm       
     tamura  shinichi  and masahiko tateishi  capabilities of a fourlayered feedforward neural network  four layers versus three  neural
networks  ieee transactions on                     
     therneau  terry  atkinson  beth  and ripley  brian  rpart 
recursive partitioning and regression trees  https   cran rproject org web packages rpart index html

 

fi