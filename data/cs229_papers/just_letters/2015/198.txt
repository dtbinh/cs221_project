application of unsupervised learning techniques to business
meta data  using yelp data

charles zhang
department of statistics
stanford university
cyzhang stanford edu

eric wang
department of statistics
stanford university
eriwan stanford edu

 

introduction

when designing effective marketing strategies for products  marketers often consider the marketing mix  the value
of the product  the price it might be offered at  how the product is promoted and the places where the product can
be accessed or purchased  in competitive marketplaces  a better understanding of the elements of the marketing mix
might lead to better marketing strategies and be incredibly valuable 
our project attempts to better understand several aspects of the marketing mix in the context of firms that utilize brickand mortar stores  we think that the use of unsupervised techniques to learn  without strong assumptions  directly
from the data may be able to provide valuable insights that help refine existing models or uncover questions previously
overlooked in theoretical frameworks 
in this project  we apply unsupervised learning techniques to a dataset of brick and mortar businesses cataloged by
yelp in order to better understand the types of businesses that offer face to face services  furthermore  we exploit
geographical data and apply self organizing maps to create business neighborhoods  clusters of businesses that are
physically close to each other and far from other businesses  and examine the mixture and levels of different types of
businesses within each of these business neighborhoods  finally  we examine whether the unsupervised clusters we
obtain improve prediction of average yelp stars for businesses 
in section    we describe the raw data and our clustering dataset  in section    we describe our clustering procedure
and highlight the specific techniques that we used  in section    we discuss our results and present visualizations 

 

data

the dataset we used for our experimentation was provided by the yelp dataset challenge  sixth round   this dataset
includes data on    k businesses from    different metropolitan areas from around the world  the dataset included
rich and detailed categorizations of the various businesses categorizations  restaurant  bar  shopping   various
attributes  good for kids  good for groups  price range   etc  in addition  it also provides yelp data on yelp user
engagement     m reviews     k tips    k aggregated check ins   we limited the scope of our project to examining
the two largest metropolitan areas in our dataset  las vegas  with    k businesses and phoenix  with    k businesses  furthermore  we restricted the scope of our inquiry to non professional services  restaurants  bars  casinos
and other entertainment  using the filters provided by yelp 
   

data processing

to create our clustering dataset  we began by extracting and reshaping all of the categorical business data into a
matrix of attributes where   indicated the appearance of the attribute  for example  there are attribute indicators
for nightlife  bars  and asian fusion  from the check in data  we created day of the week and hour of the day
features to capture time of day trends  in addition  for each business  we identified all users who had left a tip  a
short review  about the business and then created aggregate features  median  average  from the user level features of
the users who had visited the business 
 

fiafter creating these features  we then applied principal component analysis to subsets of the attribute matrix  we
selected the principal components that captured     of the variance  we also looked for an elbow in the proportion
of variance explained  and used these as inputs to our clustering algorithm instead of the original attribute matrix
features  
finally  for continuous variables  e g   review count   we scaled these variables to have zero mean and unit variance
in order to approximately balance the influence of different feature vectors 

 

methodology

we used several unsupervised learning techniques in this project and we provide a brief outline of our procedure below 
   

procedure
   create business neighborhoods  given the geographical location of all of the businesses within a city  we
wanted to be able to identify business neighborhoods where a group of businesses were geographically
close to each other and  relatively  far from other clumps of businesses 
to create these neighborhoods  we trained self organizing maps on the longitude and latitude of the businesses in each city  we explain the learning algorithm in more detail in section      
   cluster businesses  using our compiled business level data  we then performed k medoids clustering 
choosing the number of clusters k by using the gap statistic 
   aggregate data on business clusters  next  we compiled summary statistics for the business clusters  such
as average yelp star rating  average number of reviews  and percentage of businesses falling into various
attributes  we created a heat map to examine emergent patterns and arranged the clusters by generating a
dendrogram  i e   hierarchical clustering  of the clusters against these summary statistics 
   cluster business neighborhoods  next  we examined the cross tabulation of business cluster and neighborhood cluster membership for businesses in the las vegas area  we then performed an additional k medoids
clustering  with each neighborhood cluster as a row  and features as the number of businesses belonging to
each of the business clusters   we also assigned clusters to businesses in the phoenix area and compared the
geographical distribution to las vegas 
   supervised learning  we then examined the usability of our business clusters in a supervised learning
exercise using  support vector regression to predict arizona business average yelp star ratings 

   

k means and k medoids

the k medoids algorithm    is a variant of the k means algorithm that we have discussed in class  specifically  kmedoids replaces the cluster centroids of the k means algorithm with cluster medoids  whereas cluster centroids are
calculated as the expected value of all points within a cluster  the cluster medoid is constrained to be an actual training
example  the medoid can alternatively be described as the data point in the cluster that minimizes the reconstruction
error  the use of the medoid is more suitable for our problem because it is more robust to extreme outliers and can
more sensibly handle categorical variables translated into binary features 
   

self organizing map

self organizing maps    is a technique for unsupervised learning that can be used for online learning  in our application of self organizing maps  we begin with a connected lattice of neurons that were initialized evenly over the citys
geographical boundaries  indicated by the minimum and maximum longitude latitude coordinates   we examined
each business location one at a time  found its closest neuron and pulled the closest neuron and its neighbors in
the direction of the training example  after presenting the dataset to the learning algorithm many times  we would
eventually have stable neighborhood centers which would implicitly define a clustering  all of the businesses that are
closest to a certain center are in one cluster  
 

more specifically  we applied gaussian kernel pca after observing that a higher proportion of variance was explained with
fewer principal components using this dimension reduction technique  relative to regular pca 

 

fialgorithm   self organizing maps  online 
  repeatedly present the training data to the algorithm

   repeat
  
for i       m do
  
wi  arg min   xi  nj     

  find the closest neuron

nj

for nj  wi   wj   do
nj  nj    nj   wi   xi  nj  
end for
end for

  
  
  
  

  update the closest neuron  and its neighbors

in this algorithm   is the learning rate   nj   wi   is a decay function that depends on the distance between nj and
wi on the grid and  x  denotes the set of neighbors of x  in our implementation  we chose an alpha of       used a
rectangular neighborhood and allowed the decay function to simply be a function that indicated membership within a
radius r of wi   we specifically chose to present our training data to the algorithm     times as we found that additional
repetitions did not significantly affect the choice of final neuron positions 
   

gap statistic

when using an unsupervised learning technique for clustering  the correct number of clusters is often unknown  in the
case of k means  the metric used to evaluate convergence  the within cluster sum of squares  generally decrease as
you increase the size of k  e g   consider that when k   m  you can achieve   error   the gap statistic    is a method
to choose k in a principled way  specifically  it compares a clusters within cluster sum of squares against the expected
within cluster sum of squares in a similarly sized space  i e   if you generated m data points from a uniform distribution
within the bounds of your data   the suggested choice of k is the first k that satisfies gap k   gap k       sk  
where 
  x

gap k   
log wkb
   log wk  
b
b
p

  is the estimated within cluster sum of
where wk is the within cluster sum of squares with k clusters  b  b log wkb
squares and sk is the standard deviation of the estimated within cluster sum of squares  

 

results

   

business neighborhoods

the generated neighborhoods for the las vegas and phoenix metro areas are shown below 

 a  las vegas metro area

 b  phoenix metro area

figure    business neighborhoods trained by self organizing maps
 

we used the gap statistic to choose our number of clusters whenever we used k medoids  we also checked for the robustness
of these choices of k by examining silhouette plots  

 

fi   

clustering businesses and visualizing clusters

after evaluating the gap statistic for the k medoids algorithm  we selected    clusters  we then generated a heat map
for summary characteristics within each of the clusters  as shown below  the clusters are sorted vertically based on a
dendrogram generated from hierarchical clustering the    clusters using the summary characteristics as features 
the heat map shows that our clustering has picked up relatively coherent clusters   for example  clusters      
represent a non restaurant region of businesses that are hotels  cluster     and bars  nightlife  and shopping related
 clusters         the row arrangements suggest that these non restaurant clusters represent a super cluster apart from
other restaurant clusters  moreover  the clusterings are not driven by single attributes  there are several fast food
clusters as well as several asian fusion food clusters  for example  so that our algorithm has provided a differentiation
of business types that are based on patterns across features 
interestingly  there are also clusters that combine restaurants across different cuisines  suggesting that the unsupervised
learning is leveraging other features such as the check in and tips data 
we then assigned clusters based on this analysis to the phoenix metro area as well to assess whether businesses in
other regions adhere to a similar concentration around the medoids we generated  as shown in figure    the average
characteristics business in each cluster are consistent across regions 

 a  las vegas metro area

 b  phoenix metro area

figure    summary characteristics for business clusters

   

cluster business neighborhoods

next  we examined whether business neighborhoods had systematic patterns in the distribution of businesses across
our    business clusters  for example  if certain neighborhoods were strongly affiliated with ethnic cuisines  or
bars  we might be able to detect this through cross tabulating business neighborhoods with the number of businesses
belonging in each of the business clusters  we used this matrix       neighborhoods x    business clusters  to cluster
the neighborhoods  arriving at   clusters through k medoids and gap statistic analysis 
as shown below  we observe that the neighborhood appear to be clustered by the level of business density  with
a downtown area clearly identified  magenta clusters   we also repeated this process for the phoenix metro area 
by assigning businesses to the    clusters generated from the las vegas data  the clustering here seems to be less
interpretable  although once again high business density areas appear to be captured 
 

we note that businesses are relatively evenly distributed across clusters  with the largest cluster including roughly a tenth of
the observations 

 

fi a  las vegas metro area   clustering somclusters

 b  phoenix metro area   clustering som clusters

figure    grouping som neighborhoods by business cluster
   

supervised learning

finally  we explored the usage of our business clusters as additional features in a supervised learning problem  we
predicted average yelp star rating for businesses in arizona  utilizing our compilation of business attributes  check in
and tip data  we supplemented these features with home price data collected from zillow as well as median household
income data from the us census bureau  finally  we leveraged the business clusters generated from the las vegas
data by assigning the arizona businesses to these clusters based on euclidean distance  we then generated a set of
indicators for cluster memberships to include in the predictive model 
we performed  support vector regression with a linear kernel  after tuning both  and the cost of constraints violation
parameter using   fold cross validation  the resulting rmse for a test set of     businesses were very close  with
and without the indicators for cluster membership        vs         respectively   suggesting that the clusters do not
improve this prediction problem beyond the constituent features  we also explored calculating distances of businesses
to the cluster medoids  which did not result in predictive gains for this particular supervised learning problem 

 

further work

we note that our supervised learning analysis requires further exploration  since we only utilized one modeling technique  svr   other regularized learning methods such as lasso would be worth exploring for example  or kernelizing the svr  we also would want to directly cluster the arizona businesses for feature creation  as opposed to
applying clusters generated from las vegas   furthermore  our business and neighborhood clustering may be better
suited for other supervised learning problems  such as in improving recommendations to users 
another potential avenue for further work is in enhancing our unsupervised learning with textual data from tips and
reviews  thus far  we have used attribute  check in  and aggregated tips data  utilizing the tips and review language data
would likely yield subtler  more refined clustering results for businesses  finally  our usage of k medoids assumed
that businesses fall into one category  an alternative clustering scheme would be a mixture model where businesses
fall into overlapping groups 

references
    leonard kaufman and peter j rousseeuw  partitioning around medoids  program pam   finding groups in data 
an introduction to cluster analysis  pages             
    teuvo kohonen  the self organizing map  neurocomputing                 
 

fi    robert tibshirani  guenther walther  and trevor hastie  estimating the number of clusters in a data set via the
gap statistic  journal of the royal statistical society  series b  statistical methodology                      

 

fi