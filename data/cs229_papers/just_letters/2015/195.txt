probablistic models for visual odometry
boyang zhang
boyangzhang   gmail com
abstractvisual odometry is a process to estimate the position
and orientation using information obtained from a camera  we
test a popular open source implementation of visual odometry
svo  and use unsupervised learning to evaluate its performance 
keywordsvisual odometry  unsupervised learning 

i  introduction
visual odometry is the process of estimating position and
orientation using images from an onboard camera  it is used in
robotics for navigation  and can complement or even
supplement the gps and imu module  gps provides accurate
position  but suffers from jamming  spoofing  multipath error 
and indoor environments because it depends on wireless signals
from satellites  mems imus provides acceleration and rotation
in three axes  but suffers from inherent bias and drift due to
environmental factors such as temperature  humidity  and
vibration  in contrast  visual odometry provides position without
depending on wireless communications  and orientation with
less degrading due to environmental factors  by providing a
redundant source of position and orientatin to the navigation
system  the robot is more robust to failures and will be able to
operate in more challenging environments 
the experiment uses svo      an open source visual
odometry software package  and a motion capture system with
millimeter level accuracy  the camera images are fed into svo
to obtain the position and orientation estimate  a tracking device
is attached to a monocular camera  and the motion capture
system tracks the dongle to report the position and pose estimate 
the camera and dongle is manually moved in the motion capture
system  at the beginning of the test  the svo and the motion
capture system are initialized to the same initial point  since the
motion capture system is the most accurate  its estimates are
regarded as the truth  the difference between the svo estimate
and the motion capture estimate is designated as the svo error 
in addition  svo produces a number of status indicators  and the
most relevant one is the number of observations  we propose
using unsupervised learning to correlate the internal indicator of
accuracy with the true accuracy to evaluate the effectiveness of
visual odometry 
the input to the machine learning algorithm is the position
error  the orientation error  and the number of observations 
mixture of gaussian is used to find four latency multivariate
gaussians that maximizes the probability that the data lies in one
of them  since there are three different types of data  the
multivariate gaussian have three dimensions  the output of the
algorithm is the mean and covariance matrix for each of the four
multivariate gaussians 

ii  related work
a  feature based visual odometry
feature based visual odometry correlate changes in features
to the camera motion  features from camera images are often
extracted using difference of gaussian algorithms  and popular
implementations include surf  harris  or fast  from the set
of features  epipolar geometry algorithms such as ransac are
used to find the change in position and orientation  on one hand 
feature based visual odometry is accurate in the presence of large
numbers of features and handles rapid movements  on the other
hand  feature based visual odometry is slow because of the large
amount of computations needed for feature detection and
tracking  it could be fast enough for ground vehicles  where
weight is not a constraint  and some groups have implemented
this on vehicles travelling   meters per second      however  this
method is not conductive towards flying vehicles  which have
weight constraints as well as real time constraints  position and
orientation estimates often need to be updated greater than once
per second in order to stabilize a flying vehicle 
b  stereo visual odometry
stereo visual odometry differs from monocular visual
odometry by using multiple cameras taking images of the same
object  by triangulating features that appear in both camera  the
distance and bearing to that feature can be calculated  as the
feature moves from frame to frame  the motion of the camera
can be calculated  the disadvantage to stereo visual odometry is
that the distance between the two cameras is one the same order
of magnitude as the distance to the feature  if the distance to the
feature is an orders of magnitude or more larger than the distance
between the two cameras  it degenerates to monocular visual
odometry      in the case of aircraft based systems  cameras may
be mounted several feet or tens of feet apart  but it is expected
to fly at an altitude of several hundred or thousands of feet above
ground  consequently  stereo visual odometry is not the right
solution to drones  svo uses monocular visual odometry 
c  slam
highly related to visual odometry is simultaneous
localization and mapping      slam utilizes onboard cameras
to map the vehicle environment  and place the vehicle in the
environment  while similar equipment and algorithms are used
in both vo and slam  they are significantly different in a
number of areas  visual odometry is concerned with finding the
current position and orientation  and builds a map of the
surrounding in order to aid estimation of the position and
orientation  on the other hand  slam is interested in mapping
the surrounding environment  and may depend on the camera

fitravelling in a loop in order to accurately place the camera
location by completing the loop closure  for robotics  slam is
useful for navigation and guidance because it builds a complete
map of the surrounding  and aids the vehicle in deciding where
to go next  aspects of visual odometry are used to determine the
relative position of the vehicle to the environment  on the other
hand  visual odometry is useful for navigation and controls
because it produces the position and orientation  which is used
by the aircraft to change the roll  pitch  and yaw in order to
obtain the desired position and orientation 
d  semi dense visual odometry for a monocular camera
an alternative to feature based visual odometry is density
tracking  in density tracking  movements are tracked over time
by finding the position and orientation change that minimizes
the pixel difference between frames  in a recent paper      a
proposal was put forth to use a stereo camera to capture the
depth of prominent structures and use the change in the pixel
locations on the depth maps to calculate position and
orientation  while previous papers have proposed using the
depth map of the entire image  the innovation here is to use a
subset of the entire image  namely the features with structures
that can be easily tracked across frames  hence the name semidense  by using only a subset of the density map and by
restricting the computation to pixel comparisons  the algorithm
can run in real time on a cpu  which improves upon previous
versions that have required a dedicated gpu or were not real
time  the biggest weakness with using depth map is that as the
camera moves further away from the surrounding  the less
accurate the depth map becomes  when the camera is mounted
on an aircraft several hundred or thousand feet above the
ground  the depth map will resemble white noise unless
advanced imaging equipment such as lidar is used 
e  robust odometry estimation for rgb d cameras
similar to the algorithm above  this paper     proposes uses
an rbg d camera instead of a depth only camera  the camera
continuously captures the color and distance  and the algorithm
finds the motion that induced the change from one frame to the
subsequent frame  this is done by taking the difference
between two consecutive frames  which results in the
photogrammetric error  and using that to update the state
estimator  while the algorithm using the rgb d data is
expected to perform better than the one above  one remaining
problem i foresee is when large scale bias is introduced in the
image  for example  if cloud movement partially obscures
direct sunlight on half of the image frame  the majority of the
photometric error results from the shade instead of the
movement of the camera 
iii  dataset and features
the data was collected from two source  the first source is
from a monocular camera  which is denoted as the raw image 
the second source is from a motion capture system  which
produces the position in the x  y  and z dimensions  and the
orientation as a quaternion  robot operating system is used
both to organize and store data  as well as to operate on data by
applying an algorithm to a data stream 

svo is installed as a ros package  and used to operate on
the raw image  the output of svo are the raw image overlaid
with green dots that denote the position of observations  the
count of the number of observations  the number of keyframes 
the tracking quality  the tracking stage  the position  the
orientation  and the timestamp  of these sets of information  the
tracking quality  the count of the number of observations  the
position  the orientation  and the timestamp were used as
features 
here is a screenshot of the position orientation data
visualized in   dimensions over time in ros 

one task was to correlate each svo position orientation
estimate with a mocap position orientation estimate based on
timestamp  this is needed because the two are independent
systems and therefore the data overlap in time  but do not have
a one to one correspondence  two approaches were explored 
the first approach is to take the weighted average of the first
preceding mocap timestamp and the first succeeding mocap
timestamp according to the following formula 
       

                                    

the second approach is to take the nearest mocap
timestamped data  while the second approach is simpler  it is
also more accurate because there are times when the svo stops
outputting data because there is insufficient information to make
a position orientation estimate  and the last estimate could be
from several seconds ago and completely inaccurate 

fifinally  we obtain the position error  the orientation error 
and the number of observations at each valid timestamp  the
position error is the euclidian distance between the svo and
mocap estimate  the orientation error is the angular distance
between the svo and mocap estimate which was obtained by
converting the quaternion estimate back into angles along the
three dimensions  these are plotted in a  d graph below 

                               


                          
                  

this is the basis for the e step  and the m step is simply


                             
    

which can be solved by taking the derivative with respect to 
in the case that the qs are gaussian random variables  this
reduce to the following 
e step 

  



the machine learning algorithm used on the data is mixture
of gaussians      the reason this is used is to find the latent
gaussians because we see above that the data is organized into
several clusters  by finding the latent gaussians  a model can be
built to find the likelihood that the combination of the number
of observations  position error  and orientation error is a valid
data point  or an outlier  our goal is to be able to find     
where x is a vector of size three that represents the three inputs 
and  is the mean vector  probability vector  and covariance
matrix for a three dimensional multivariate gaussian
distribution 
the mixture of gaussians is a specialization of the em
algorithm  we would like to maximize the likelihood that the
parameters with respect to the training data


      log      
  

however  explicitly finding the parameters by taking the
partial derivative in the general case is difficult  instead  we can
iteratively find the local maximum using the em algorithm 




  

  

 log                                   
 

using jensens inequality  this becomes 


                             
    

this becomes an equality if the distribution is constant  which is
the case if we set

                

                          

                             

              
iv  methods



 

 

      

 
             
 

m step 


 
    


  

  

  

 

    


   



              


   

v  experiments results discussion
one of the difficulties in using the mixture of gaussians is
picking the initial values  especially the mean  this is because
the  can diverge to infinite if the denominator becomes
approximately zero  this happens when there are data points
that are several orders of magnitude smaller from the initial
mean of all latent gaussians  the exponential used to calculate
the multivariate gaussian probability becomes arbitrarily small 
and this in turn causes the mean and covariance update to
become even bigger because they  this causes a positive
feedback cycle which converges when everything becomes
infinite 
the initial values that we chose for the four latent gaussian
are the following in order to capture the largest range of
possible values and prevent a divergence  the first index
represents the number of observations  the second index
represents the position error  and the third index represents the
orientation error 
              
               

fi               
               

the values for  were picked based on estimates from the
histogram of the number of matches 
 
   
  
 
   
  
 
   
  
 
   
  

we pick the covariance matrix to be the same for all gaussian
because the initial covariance was unknown 
     
        
     

after the mixture of gaussian algorithm converged  the
resulting values were the following 
                               
                                
                              
                              
            
            
            
            
      
       
       
                                
                            
      
         
        
                                     
                    
         
      
                 
                                  
       
          
        
      
           
      

       
         
         

      
         
        

to verify this  we collected another set of datapoint  and used
the same analysis on the data  the histogram of the number of
occurrences matches the overall shape of the training set 
however  the position error and orientation error has been
scaled up by a factor of    additionally  there are two distinct
clumps of data  one with low error  and one with high error  just
like in the training data  however  the test example had
difficulties correlating the number of occurrences with a clump
of data  as it did in the test set 

fi         
            
        

        
        
         

        
         
        

vi  conclusion future work
as seen from the unsupervised learning algorithm  there is
a positive correlation between the position error in meters and
orientation error in degrees  this is what we had expected 
because as the vehicle made difficult maneuvers  the error in
both position and orientation would increase  additionally 
there is negative correlation between the number of
occurrences  which is an svo indicator for its estimated
accuracy  and the error in both position and orientation  the
reason why the correlation is negative is because the number of
occurrences represents the number of depth estimates  depth
estimates occur when the algorithm has a poor estimate  but as
the estimate improves with time  the depth estimate turns into a
 d point  and the number of occurrences becomes reduced 
therefore  we conclude that we were successful in evaluating
the svo algorithm and software package using unsupervised
learning 
vii  acknowledgment
i would like to thank dr  hui li for her assistance and
guidance 

references
   

                                
                                
                                 
                                 
             
             
             
             
                          
                     
        
                 
        

         
            
        

       
           
       

        
           
        
       
       
        

        
        
        

       
        
        

   

   

   

   

   

   

konolige  kurt  motilal agrawal  and joan sola   large scale visual
odometry for rough terrain   robotics research  springer berlin
heidelberg                
nistr  david  oleg naroditsky  and james bergen   visual odometry  
computer vision and pattern recognition        cvpr      
proceedings of the      ieee computer society conference on  vol    
ieee       
scaramuzza  david  and friedrich fraundorfer  visual odometry part   
the first    years and fundamentals  ieee robotics   automation
magazine  december      
engel  jakob  jurgen sturm  and daniel cremers   semi dense visual
odometry for a monocular camera   computer vision  iccv        ieee
international conference on  ieee       g  eason  b  noble  and i  n 
sneddon  on certain integrals of lipschitz hankel type involving
products of bessel functions  phil  trans  roy  soc  london  vol  a    
pp          april        references 
kerl  christian  jurgen sturm  and daniel cremers   robust odometry
estimation for rgb d cameras   robotics and automation  icra       
ieee international conference on  ieee       
forster  christian  matia pizzoli  and davide scaramuzza   svo  fast
semi direct monocular visual odometry   robotics and automation
 icra        ieee international conference on  ieee       
ng  andrew  cs    lecture notes 

fi