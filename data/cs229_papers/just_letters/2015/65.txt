assessing the quantum signature of the d wave one machine
andrew guo  and brian wai 

abstract in our project  we will explore the controversial
question of whether the d wave one machine is a true quantum
computer  d wave systems claims that their machine  a quantum annealer obtains a speed up over classical computers on
combinatorial optimization problems  using machine learning
tools  we hoped to determine features that would classify a
problem as easy or hard for the d wave one  by comparing
these features to those of classical simulations  we would obtain
a metric to measure the quantum signature of the d wave
one machine  our naive bayes classifiers yielded inconclusive
results  due to a lack of adequate features and insufficient data 

edges energies to obtain an estimate of the probability of
success 
we have also noticed that the d wave machine tends to
do very well on some problems and rather poorly on others 
that is  it finds the lowest energy state fairly often given
some initial configurations and very infrequently given other
initial configurations  we used this information to reapply the
given problem as a classification problem  e g  whether we
could classify certain initial configurations as easy or hard
for the d wave machine 

i  introduction
ii  r elated w ork
the nascent field of quantum computation aims to create
quantum devices that possess computational capabilities that
far surpass those of their classical peers  by utilizing the
exponentially larger parameter space of coherent quantum
systems  quantum computer scientists aim to achieve quantum speed ups  they have shown exponential speed up in
the factoring of large numbers via shors algorithm  as
well as other promising applications such as the efficient
simulation of quantum systems and a quadratic speed up in
searching via grovers algorithm     
quantum annealing methods comprise a proper subset of
quantum computational techniques  utilizing such quantum
behaviors as tunneling to obtain a quantum speed up  finitedistance quantum tunneling   a phenomenon whereby a
system can overcome costly energy barriers surrounding
local minima by passing through them   has proven useful
for the discovery of local minima of binary optimization
problems  simulated quantum annealing has been shown
to be more efficient than  classical  thermal annealing for
certain problems that can be modeled by  d ising spin
glasses  the goal is to find the ground state of a hamiltonian
 i e  a cost function  given by     
x
x
hising  
jij iz jz 
hi iz
i j

our inspiration for assessing the quantum signature of
the d wave one was inspired and facilitated by vigorous
academic debate on the arxiv  in march of       d wave
systems released a paper claiming to have achieved quantum
annealing with over one hundred qubits      they justified
their claim of a quantum speed up not via the d wave ones
speed which  at the time  still lagged behind simulated
annealers run on laptop computers   but by the qualitative
aspects of its performance  when run on a variety of problems  d wave one found certain problems hard  average
probability of finding the optimal solution near     and other
problems easy  average probability of finding the optimal
solution near     this meant that the d wave one had a
bimodal distribution of success probabilities  which seemed
categorically different from classical models  the simulated
annealer generated a gaussian distribution of success probabilities  while d waves distribution consisted of two mixed
gaussians  by finding good correlation between the easy and
hard problems with that of a simulated quantum annealer 
and poor correlation with a classical annealer  d waves
supporters argued that d wave was indeed behaving in a
quantum manner 

i

for our project  we assume that each initialization of the
chimera graph uniquely determines the probability that the
d wave machine will succeed on a given configuration 
initially  we treat the input as the starting state for the
chimera graph  and then take the output to be the probability
of success  that is  the probability of getting the lowest
energy state   we then train a naive bayes model on the
 this work was supported by stanford university
  andrew guo is a b a  candidate in physics at stanford university 
class of       aguoman stanford edu
  brian wai is a b a  m s  candidate in math computer science at
stanford university  class of       brianwai stanford edu

fig     distribution of success probabilities of d wave and simulated
annealing

fid waves critics  however  offered differing explanations
for the d wave ones supposed deviation from existing
classical algorithms      a paper by john smolin   graeme
smith argues that the comparison between d wave and
a generic simulated annealer was specious  as the wrong
classical model was used  to test this  we decided to apply
machine learning to test a home brewed classical simulated annealer on the data  which also generated a bimodal
distribution for success probabilities  by using a learning
algorithm that could learn which problems were easy or hard
for the d wave and for the classical annealers  we could
test whether certain features determined the difficulty of the
problem  by comparing the most important features for dwave and for the classical annealer  we hoped to find another
source of data to support or refute the hypothesis that the dwave one is a quantum computer 

for the simulated classical annealer came courtesy of tomas
navarro  a student in the edx course cs    x  quantum
mechanics and quantum computation  taught by professor
umesh vazirani in          

iii  datasets and f eatures

iv  m ethods

our dataset for the d wave behavior was the      training
examples released publicly by d wave  the data consists of
the starting energy and the energy between edges  the data
for the classical model is generated by a c program  which
we run on the same initial starting states as the d wave
data to generate our simulated thermal annealing data  the
features we used all derived from the initial edge energies
of the chimera graph  which is shown in the graph below 

the learning algorithms we used were nave bayes 
 weighted  linear regression  and svm  initially  we began with linear regression and svm  linear regression
formulates the problem by training the classifier as a linear
function of the features  the edges of the chimera graph  
each answer  in this case the probability of success  is
modeled as a linear function of the edges  and we aim
to minimize the average squared error  a support vector
machine works as a linear classifier by weighing the vectors
with the best possible supports for the data  that is  although
the svm uses infinitely many possible vectors  it features a
regularization component that allows it to not overfit the data
set given a sufficient amount of data  generally  a support
vector machine will do better than naive bayes given a
sufficient amount of data because it does not assume each
feature is independent of the others  however  when not
given enough data  a support vector machine will overfit the
training set and not generalize well to other examples  nave
bayes works by modeling each feature as an independent
variable  given that we have either a easy or hard problem 
we can find the probability that each edge is labeled   or   
given whether the problem is easy or hard  given the prior
distribution on easy hard problems  given a problem and its
edge specification  we can calculate the probability of the
problem being easy or hard  and output the answer with
the greater probability  unfortunately  both methods gave
us unacceptable error values of the success probabilitywe
had around    error  which was unacceptable  therefore 
we decided to reformulate our problem as a classification
problem  this was inspired by the fact that the d wave
success probabilities form a somewhat bimodal graph  as dwave does very well on some types of problems and not well
on others  as seen in the graph below 
to establish a baseline  we used a classifier that picked the
majority value  this baseline was     using linear regression
and svm to classify our models did not pass the baseline of
   error  therefore  we decided to use nave bayes  in order
to do this  we first ran nave bayes on all possible edges  this
gave us an error of around      however  we knew the nave

fig    

portrayal of the chimera graph

we attempted to find a subspace of our high dimensional
 n      feature space using pca  but since our features
were sampled from  correlated  bernoulli random variables
with parameter         they already possessed maximum
variance  we used sums of the energies pairs of edge that
coincide at a vertex in order to model first order correlations
between edge 
data examples for the d wave machine were obtained
from the ancillary files on the arxiv      the code and results

table i
s ample of d wave data    rows       
first vertex
 
 
 
 
  
 
   

second vertex
 
 
 
 
  
 
   

edge energy
  
 
  
 
  
 
 

fibayes assumption is inapplicable here because the difficulty
of the problem mainly depends on the interaction between
the edges  because we have difficulty finding which pairs of
edges affect the function the most  we ran nave bayes all
pairs of edges  unfortunately  this failed to converge because
we lacked the required data  therefore  because we could
not do this  we looked for a way to compress the number
of features  because the previous nave bayes algorithm on
each edge weighted each edge about equally  we decided to
try running nave bayes on the sum of the edges  e g  the
total energy  when we did this  we found out that we got the
test error down to     because we are no longer overfitting
the data  the test error and the training error were about
equal in this case  in fact  in some cases  the test error was
actually lower than the training error 
v  e xperiments   r esults   and d iscussion
first  we tried linear regression and an svm  the svm
has low training error and high test error because we did
not have enough training examples to fit the features we
wanted  linear regression did converge  but the mean squared
error       was too high to predict the probability of success
accurately  next  we tried to train nave bays on the initial
energy of each edge as our features  using a training set of
    examples and a test set of     examples  our results can
be seen in the graph below 

fig     misclassification error of naive bayes trained on single edge
energies as features  test size was     examples 

here  the training error and test error converge to around
     but we have yet to converge because we lack the
required data  in other words  even with only     features 
we are overfitting the training set  and therefore  we should
compress our features  because of this  we implemented
nave bayes on the energy alone  because the energy was
a number between   and      we encoded the energy as
  bits of separate observations  this gives us the graph
below  here  we see that when we train only given the energy 
the test error converges almost immediately  in fact  the fact
that the test error is lower than the training error shows
that we have in fact converged and are not overfitting  we
then attempted to run nave bayes on pairs of edges  but the
algorithm predictably failed to converge due to the lack of
available data 

fig     misclassification error of naive bayes trained on the total energy
as the feature  test size was     examples 

we noticed that the naive bayes classifier for the d wave
machine classified the classical data with a similar error of
      while the naive bayes classifier trained on the classical
data classified the d wave data with similar error  below
       however  because both error values are still high 
further research is necessary to draw a conclusion 
vi  c onclusion and possible f uture work
for this project  the main roadblock was the lack of data 
only so many inferences can be made with      data points 
this is a problem because we have     possible edges  so
even if we only count edges alone as features gives us     of
them  let alone accounting for interaction between edges  we
simply do not have enough data to train all our parameters 
another problem was the lack of a quantum algorithm to
formulate quantum data  as we cannot compare the d wave
data to data we do not have 
nave bayes was the best working algorithm due to the
low number of data points we have  as nave bayes does
not require much data to function effectively  however 
naive bayes makes the assumption that all the features work
independently  which eliminates any interaction between the
features  this is important because the interaction between
the edge energies have great physical importance in the
chimera graph  therefore  in order to advance this project
further  we would need either more data  or better features 
unfortunately  we lack the physics knowledge required to
construct better features by hand  therefore  we can only
hope to obtain more data from dwave 
with more data  the next step would be to run pca or a
svm if possible on the increased data  if this is not possible 
we would run nave bayes on pairs of edges  and extract out
the edges which have the highest significance  this would
allow us to find the pairs of edges which correlate best with
the success probabilities  and then we can extract features
given these edges that is  we could look at pairs of these
edges and train a neural network model on them 
to formulate the quantum algorithm  we would code up a
quantum monte carlo algorithm to mimic what the current
classical algorithm does  except it would take into account
entangled states to get to the lowest possible energy state 

fir eferences
    s  boixo  et al  quantum annealing with more than one hundred qubits 
arxiv          
    j a  smolin  g  smith  classical signature of quantum annealing 
arxiv          
    l  wang  et al  comment on  classical signature of quantum annealing arxiv          
    a  harrow  why now is the right time to study quantum computing 
arxiv           
    tomas
navarros
dropbox
link
can
be
found
under the class project discussion page for cs    x 
https   courses edx org courses berkeleyx cs    x      august 
courseware 

fi