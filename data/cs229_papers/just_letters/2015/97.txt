an ai for the wikipedia game
alex barron
admb stanford edu

 
   

zack swafford
zswaff stanford edu

introduction

from the articles to provide more descriptive features
for them  the most successful approaches such as that
of milne  david  et al     and yazdani  majid  et al     
heavily utilized the links in each article for classification  which boded well for our link based approach to
explore the structure of wikipedia  hu  jian  et al    
introduced some representation of the hierarchical nature wikipedia which improved their text classification 
in particular differentiating category and link based articles from the rest  our technique employs unsupervised learning to infer such relationships and build features representing these broader articles to augment our
base bag of words model  intuitively such higher level
articles should be useful in linking diverse concepts and
thus providing minimal path solutions in the game 
many previous papers have investigated various natural language processing methods to optimally featurize
and then cluster wikipedia pages in a similar manner to
our unsupervised learning  hu  xia  et al     proposed
the use of the porter stemmer to most effectively stem
features  a technique we inherited  but also suggested
the removal of date strings as features  for our specific
application  date articles turned out to be very common
on minimal path solutions and so we decided to retain
them in our feature set 

the game

we designed an ai to play the wikipedia game  a
computer game played on wikipedia org  in short  players start on a random wikipedia article and then try
to traverse the site using only the hyperlinks in the articles to get to some common destination article  the
game can be played by humans either for speed or for
fewest links used  but a key component of a humans
strategy is that he never backtracks  i e  his algorithm
is entirely greedy  
an input to our ai will therefore be a random wikipedia article  and the output will be a path from there
to the destination article  which we chose to remain
fixed at the stanford university page  our ai will
navigate from the start article to the goal one attempting to minimize both the link distance and time but also
attempting to minimize states viewed  to emulate the
human player and because in a real application to the
web each article would take non trivial time to download and view 

   

classes

this project incorporates a variety of concepts in artificial intelligence  game theory  and machine learning  because all of the elements are used in concert
and interrelated  the distinction between the subjects
is difficult to make  in general  the search problem and
modelling apply more specifically to the subject matter
in cs      and the rest of the machine learning  i e  the
various ml algorithms  k means  and pca  are geared
toward cs     

   

 
   

data and features
dataset

the first thing we did to collect data was extensive
human trials  we observed enough plays through the
game to establish an oracle for human ability with confidence  a comparison of these results with results from
various implementations of our ai can be found in table   on page   
our data was all downloaded directly from the simple english version of wikipedia  which can be found
at simple wikipedia org  this website  and all wikimedia websites  provides a free  xml dump of the entire
site  we downloaded this single file and parsed it into
a series of pages  each with its article text attached  we
then extracted all of the hyperlink information from the
pages and used this to form a directed graph  with each
page represented by a node and each link from one page
to another as an edge in that direction 
with this exceedingly simple implementation we were

related work

while we were unable to find any academic articles directly investigating the link traversal which forms the
bulk of our project  many groups have applied machine
learning techniques to explore the structure and semantics of wikipedia  much of previous research focuses
on text and document categorization and to that end 
various techniques have been implemented to group
wikipedia articles 
banerjee  somnath  et al     along with a number
of other groups have attempted to create concepts

 

fi   

able to implement basic graph search algorithms such
as uniform cost search  ucs   however  the more interesting problem began when we introduced a model
to estimate the link distance between two articles  to
do this  we first had to generate training sets to train
the models and testing sets to test them  we did this
by running the simple  but slow and costly  exhaustive
ucs algorithm to determine the actual minimum link
distance d between a random given start page pstart
and our pgoal   stanford university  this allowed us
to generate a virtually infinite amount of training and
testing data in pairs  pstart   d  representing the input
and the desired output of our predictor  essentially 
this created an arbitrary dataset s    x i    y  i     for
i              m where x i  is a random article and y  i  is
the distance between that article and the goal 

   

this model was sufficient to use simple state based search
algorithmsnamely  depth first search  dfs   breadthfirst search  bfs   and ucs  these algorithms  particularly ucs  unsurprisingly  performed well and found
the minimal path within a reasonable amount of time 
however  we were interested in trading the guarantee
of the minimal path that ucs affords to decrease the
number of states that the algorithm has to explore  the
algorithms fell short of our goal because although they
all found the minimum path between pstart and pgoal   in
each case the number of states the algorithm had to explore was still very high  to remediate this  we applied
a heuristic to ucs  thereby making it a    which could
estimate the distance from one article to another  this
strategy was roughly based on ours as humanswhen
we play the wikipedia game  we follow a greedy algorithm  clicking on one page and never looking back 
this means that we almost never find the shortest path
 only an approximate one  on the order of the minimal
path   but it also means that we are much more efficient and dont have to explore as many states  all of
this is only possible because humans are able to incorporate knowledge of the content of the articles in their
decisions about which link to follow 
to implement this heuristic  we made the assumption that the link distance between two articles could
be modelled directly and implemented ml algorithms
to find it  as discussed above  we used a series of training examples with the input of a page x i    a bag of
words feature extractor     and the actual minimum
distance from that page to the goal y  i  to train different algorithms 
it is important to note that the output of our algorithms y is not binary  this is a multiclass application
of ml  in principle  y can assume any value            l
 where l is the maximum minimum path length from
any article to the goal  or the value   representing
the case where no path exists whatsoever  in practice 
the range of values y could assume were limited to the
range of the values assumed by any of training examples
y  i    usually  this range was about           
one common strategy to handle mutlticlass classification is one vs  rest  ovr  classification  this
involves training any ml algorithm as usual  but doing so once for each of the different possible outputs 
for each separate model  only the training examples
pertaining to that particular outcome are marked as
positive outcomes  each model is then able to generate
a score for its outcome on a new example x  whichever
of the models gives the highest score to that example
 implying that that outcome is  in some sense  the most
likely result according to the model  predicts the overall result to be the corresponding outcome  we used
the sklearn library to implement ovr logistic regression  stochastic gradient descent  sgd  with hinge

feature extraction

to extract meaningful data about each webpage for the
ml algorithms  we had to featurize each article  we
used a feature extractor    which could operate on
any article x  the page was modelled as a bag of words
 i e  we assumed that word order was irrelevant to our
problem   we therefore implemented the feature extractor using the nltk package to tokenize and stem
each word in the page  so that any pages with similar
concepts might have features in common  we ignored
commonly used words by removing any of nltks stop
words from the feature set  furthermore  we added features for the number of words and the number of links
in a page  therefore  x i    essentially created a sparse
feature vector with only the words in the page and a
modicum of metadata about it 

 
   

machine learning

supervised learning
search

initially we implemented a state based model to frame
the problem  essentially  the goal of the algorithm is to
find a path from the start article to the goal article via a
series of other articles  the state based model is simple 
each state is based on a article  and must store all of the
information that the article does  this is the only value
intrinsic to a state  so it is the sole component of a state 
the actions which can be taken from a given state are
to transfer to a state representing any article that the
given state links to  following this action transfers to
the given state  and the cost of taking this transition is
uniform at   because each transition counts as one link
used  finally  a state is the goal state if the article it
refers to is the goal article 

 

filoss and with perceptron loss  and a support vector machine  svm  with hinge squared loss  none of
these was the most effective algorithm  so we will not
describe them here  see table   on page   for details 
given a random set of      training examples and
random test set of     examples  multinomial logistic
regression was able to consistently outperform all of
the above methods  multinomial logistic regression
actually operates in a similar fashion to ovr  but the
output of each of the classifiers is a probability that that
classifiers outcome is the correct one  normalizing over
the results produces a probability distribution for the
outcome  and selecting the one with the highest probability allows the algorithm to select a particular value 
each of the individual classifiers in the model is implemented as a linear combination of the weights of the example x  note that x     for the intercept term   the
classifiers score therefore varies with w   x   where
w is the weight vector learned in the training phase 
specifically  the classifiers score is
hw  x   s x   

the assignment step  where each page is assigned to a
centroid  mathematically represented by
c i     argmin   x i   j      
j

and the recentering step  where each centroid is moved
to the average of its constituents places with
pm
 i 
  j c i 
i     c
 
j    p
m
 i 
  j 
i     c

figure    k means loss vs  num clusters

 
 
   ew  x

typically  the parameter w is taken to be the maximum
a posteriori  map  estimate
w  argmax l w 
w

  argmax p  y   x   w 
w

  argmax
w

m
y

p y

 i 

 x

 i 

we clustered the data for various values of k and
graphed the resulting total loss from all of the pages 
the graph is displayed in figure    estimating the
inflection point  whose k value is the optimal number
of clusters  based on the shape of the graph  we found
that optimal clustering was achieved by k   

  w  

i  

which can be found by stochastic gradient ascent via
the update rule


 i 
wj    wj    y  i   hw x i    xj  

   

the unsupervised learning algorithm principal component analysis  pca  was also quite helpful in understanding our dataset and feature space  the pca
algorithm runs a singular value decomposition  svd 
on the matrix x of all the training examples  this
creates a list of eigenvectors of the space  ordered by
their importance to the space  creating a transformation matrix c as a slice of the first n eigenvectors and
multiplying c   x flattens x to n space  assisting in
visualization 

when run to convergence  this algorithm produced a
relatively powerful classifier for our data 

 
   

principal component analysis

unsupervised learning
k means

one method we used to improve our algorithm and to
gain an intuition for our feature space was k means
clustering  which we used to develop clusters of pages
under our feature extractor  our theory was inspired
by human players  who frequently maintained the initial goal of getting to articles on the topic of the goal 
then focused on moving from there to the real goal 
the intuition was that if the machine could also take
this topical context into account the algorithm would
improve  clustering the data allowed us to add another
feature to the feature setspecifically  the identity of the
cluster each page was in  k means is run in two steps 

figure    different perspectives of  d pca with more
important nodes emphasized

 

fiare guaranteed to find a minimal cost path when one
exists 
to improve ucs  we undertook to implement an ml
algorithm to predict the future cost of a path  specifically  we tried the methods in table   

for instance  pca showed us that date articles  e g 
     or    january   which intuitively connect sporadic ideas  proved to be a very common route to the
goal for the minimum path  it can be seen in figure
  that certain nodes are disproportionately more valuable than others  represented by the large blue circles  
these are primarily date and list nodes  upon further
investigation  we found that these types of articles are
outliers in the  d space  see figure    

table    comparison of ml algorithms
model

figure     d pca with dates highlighted

accuracy

distance

logistic regression
sgd with hinge loss
sgd with perceptron loss
svm with hinge squared loss

   
   
    
   

   
   
   
   

multinomial logistic regression

   

   

note that all but the ultimate algorithm is ovr 

the distance column in the table represents the
average distance between the predicted value of y and
the actual one  it is clear to see that multinomial logistic regression yielded both the best accuracy and
the lowest distance  so we implemented that for the algorithm 
finally  consider the performance of a    implemented
with multinomial logistic regression as an heuristic 
it did not always find a minimum cost path  because
the heuristic we used was not consistent  on average
less than     of the time the heuristic would be incorrect  by a distance of      on average  and sometimes
this error would result in an overestimate of the actual
distance  overestimating distance contradicts the definition of consistency for heuristics  which meant that
a  achieved an average path length of          more
than the minimum  furthermore  it took a  more than
  x as long to accomplish this result because it had to
use the predictor on each of the states it visited 
a  did  however  drastically improve in our desiderata  the number of states explored  the heuristic allowed the algorithm to advance to pgoal while only examining an average of     states  which is a massive
improvement even over ucs  specifically  it is a twentyfold improvement over ucs and a two hundredfold
improvement over the naive algorithms  this result
was exactly as we had expected and imagined 

the fact that they are outliers suggests that something is different about the structure of these pages 
specifically  list and date articles have a lot of links to
other pages and shockingly little text  these two types
of articles  along with some of wikipedias administrative articles  comprised the vast majority of the outliers
in the  d pca space 

 

results

we found with high confidence that from     of randomly generated start articles  stanford university
could not be reached at all  for     of start articles 
there did exist a path from pstart to pgoal   on average
the empirically minimal path had a cost  i e  length 
of     transitions 
the really interesting results had to do with the
number of states explored under various state search algorithms  which can be seen in table    bfs and dfs
both by definition explored all         states per solution  the time complexity of ucs depends inherently
on the structure and convexity of the search problem 
without an heuristic for the future cost of a path  i e 
the cost from pcurr to pgoal    the transition costs for the
problem are uniform  this structure allowed ucs to
achieve a much better result than dfs or bfs  when
ucs was used to solve the problem it found a minimal
path after exploring an average of        states for a
speedup of more than         all of these algorithms

table    comparison of search algorithms
search type

 

path length

states explored

time

human

   

   

    s

dfs
bfs
ucs

   
   
   

       
       
      

    s
    s
    s

a 

   

   

   s

fi 

discussion

examining this tradeoff between minimality and states
explored is the theme of our problem  if the heuristic is
good enough  especially among low distance states  it
is entirely possible that an implementation of weighteddfs using the heuristic to calculate edge weights might
produce similar results to the ones achieved by a humans greedy search 

comparing these results to our extensive human trials 
we see that a  can still do the problem   x faster than
humans can  furthermore  a  s result is within throwing distance of the minimal one  humans are nearly
twice that  however  the massive advantage that humans obtain in the game from their ability to actually
understand the articles and the context allows them to
perform a greedy search and explore exactly as many
states as their selected path  humans still explore only
     the number of states that a  does  and       the
number ucs does   in a context without the whole of
wikipedia downloaded  i e  with the introduction of
network latency  a fact of life on the internet  humans
could still conceivably beat a  at the game 
nonetheless  the strides which a  exhibits demonstrate massive improvement over any non heuristic based
algorithm  which shows that our implementation really
has learned something about the goal article  one of
the most impressive aspects of this learning is that it
also demonstrates serious latent structure in the data
 structure which it learned   the classification is incorrect by an average of only     for the incorrectly
classified examples  i e  the output is only off by two or
more about     of the time overall   this would not
be possible if adjacent categories were not inherently
very similar 

 

 

references

    banerjee  somnath  krishnan ramanathan  and
ajay gupta  clustering short texts using wikipedia 
proceedings of the   th annual international acm
sigir conference on research and development in
information retrieval  acm       
    milne  david  and ian h  witten  learning to
link with wikipedia  proceedings of the   th acm
conference on information and knowledge management  acm       
    yazdani  majid  and andrei popescu belis  using a wikipedia based semantic relatedness measure for document clustering  proceedings of graphbased methods for natural language processing 
association for computational linguistics       
    hu  jian  et al  enhancing text clustering by
leveraging wikipedia semantics  proceedings of
the   st annual international acm sigir conference on research and development in information
retrieval  acm       

future work

some avenues for continued improvement stand out 
firstly  our multinomial logistic regression classifier
could be improved  our solution is almost certainly
overfit to the data  i e  we have a high variance  because the bag of words model usually errs in that direction  that model has so many features  and the training set is even in our context such a small portion of
the space  that the classifier is extremely likely to overfit to certain words  further investigation is required 
but a simple path to improvement for the accuracy of
the classifier might be to remove rare words from the
feature space or to train on more of the data 
in this vein  implementing other or more complicated ml algorithms might also yield better results 
because the training set is virtually infinite and the
output y is multiclass  this might very well be a good
space for a neural network to be applied  if the neural
net could even better classify and predict the distance 
we could move even further toward a perfect heuristic
for a   
once we settle on an optimal ml algorithm  it would
be interesting to vary the weighting of the heuristic in
a    although this will likely wreak havoc with the
consistency of the estimate  it will produce interesting
results  a  will likely be much less minimal in its pathfinding  but could potentially also explore fewer states 

    hu  xia  et al  exploiting internal and external
semantics for the clustering of short texts using
world knowledge  proceedings of the   th acm
conference on information and knowledge management  acm       
    scikit learn  machine learning in python  pedregosa
et al   jmlr     pp                  
    tiago p  peixoto  the graph tool python library 
figshare         doi          m  figshare        

 

fi