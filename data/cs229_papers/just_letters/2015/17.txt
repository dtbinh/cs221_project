cs    final report

rating the raters  bias analysis on yelp
reviews for improved star rating system
qiaojing yan  jingwei ji  haitong li

i  introduction

y

elp is becoming a vibrant bond connecting people
with local business  the user experience and service
quality of yelp are in fact heavily dependent on the users
text reviews and star rating which aid our everyday
decisions on local business  especially given tremendous
amount of data being generated nowadays  the current
yelps rating algorithm determines the overall rating
without taking into account the reviewers special
characteristics while different people may have diverse
rating tendency  some people are reluctant to give a five
star while some other are the opposite  even when two
people have the same view on a restaurant  one may tend
to give it five star while the other follow his habit and give
three star  this tendency is hidden underneath the simple
star rating system since we dont know what a costumers
real unbiased opinion is  however  review text is good
indication of a costumers real opinion  rating with star
takes less than a second  but writing a review costs much
more and text conveys much more information  in this
project  we address this interesting problem by using
machine learning techniques to bridge the gap between
the personalized rating tendency and the overall rating
statistics  from a business and practical perspective  this
method could potentially help yelp to improve the
fidelity of its rating system by considering the intrinsic
variations of the reviewers personalities 
as a classification problem  it is aimed to predict the
  star rating results based on multiple features 
specifically  the input of this problem is the extracted
from reviewers comments  plus the user id of each
review  previous research work has tried star rating
prediction using different variations of bag of word
features and classification models          in this project 
we use  perceptron  softmax  svm   to perform the
classification task  where perceptron serves as a baseline
to predict the polarity  two class output   we train the
classifier to predict star from review text  since it is
trained on all reviews  this classifiers rating tendency is
the average of all reviewers  so  using this classifier  we

dataset
user reviews   rating

feedback to
rating system

stop removal
word stemming
pos tagging  not x

features
word features
user id
etc 
training

rating
tendency 
polarity
compare

predict

learned
models

fig     project framework  using classification algorithms
with nlp word features and user id feature to reveal the
personal rating tendency polarity  which can be used to
statistically quantify the bias of users as the feedback to
the rating system in order to deliver more accurate star
rating results for a business 
can assess the rating tendency of a user  i e   whether
he she intends to give a higher lower star ratings with a
similar review as the others  the assessment is based on
the comparison of the predicted and actual star ratings
based on reviews of a user  then  the rating bias of users
can be statistically calculated  the project framework is
schematically shown in fig    

ii  dataset   features
a  dataset
in this project  we use the public yelp dataset      the
whole dataset includes        businesses          users 
and           reviews in json format  in the review
object type  individual users review text and star rating
from   to   are given  besides  there are also many other
features of a review such as tags including funny 
useful  cool  overall  the dataset is large and rich
enough for a convincing machine learning task  another
advantage of having a large dataset is that this allows us
to experiment with different scale of subsets to generate

fics    final report
    

 m

total users        
total reviews         

training error
test error

     
          

    

     

error rate

total number of reviews

 m

   k

   k

 
   

    

     
     

    

     
          
     
     

    
    

 k

  k

   k

 m

top k users

fig     total number of reviews  actual dataset size  from
sorted top k users  a representative size   obtained from
original dataset  for each subset      examples are used
for training and     examples are used for testing 

fig     an example of extracted word feature after nltk
preprocessing  stemming  stop word removal  etc   

the learning curves on such task  we sort the original
dataset based on how many reviews a user has given  and
generate the total number of reviews as top k reviewers
give  as shown in fig     for the following experiments
with  perceptron  softmax  svm   we use k to represent
different data scale  where the actual data size should be
on the y axis of this figure because one active user can
generate thousands of reviews  for the full size
experiment  we would have     m examples as the
training       and testing       dataset  which is a
computation intensive task 

b  features
our baseline feature is just the bag of tokens in the
review text  based on that  we try to use nlp technics to
process the tokens 
   stemming and stop word removing 
word in same meaning may have different form  for
example   argue    argued  and  arguing   we used
porter stemmer in the natural language toolkit  nltk 
package to do stemming      after stemming  these three
words are reduced to the same stem  argu   an example
of stemmed word feature is shown in fig    
   part of speech tagging 
using all the english words as our feature may cause

   epochs

      

   

 k

  k

   k

 m

number of users
fig     training and test error rates using perceptron to
predict the polarity of users rating  subsets of different
scale are assessed varying the number of top k users 
over fitting as there are too many words  we initially
assume that words that are adjective and verb carry more
information  so we first do part of speech tagging and
then used only adjectives  jj  and adverbs  rb  as our
features  the pos tagging is done using the stanford
corenlp      however  we found out this technic did not
help to reduce error rate 
   dealing with negations
we first define a set of words that mean negation  n  
 no  not  isnt  wasnt  arent  werent   then if we find a
word n  n  we add a tag  not  to the word after it to
form a new feature  for example  not bad give us
not bad  we also concatenate not to the second word
after it  for example  doesnt taste good give us
not good 
how do we know each user do have a characteristic
tendency  besides the word features on the review
comments  we also include the user id as an additional
feature  error rates when using this additional feature are
compared to error rates when using the word features  if
the error rate drops  then it verifies that each user do have
his own characteristic 

iii  results and discussions
a  polarity prediction
we first conduct the polarity prediction task using
perceptron algorithm  the boundary for polarity is   star 
indicating whether a user feels the service is good or not 
the reviews from top k users are used to train perceptron 
where k is varied to generate different size of subsets for
the training and testing      of the generated subset is
used as training set and     as test set  the metrics for
evaluation is the strict error rate  which simply counts the

fics    final report

   

     

                     

     

mse

   
training mse
test mse

   
   

   

     

     
     

   
     
      
      

   

 k

full size
  k

   k

 m

number of users
fig     mean square error  mse  of training and test for
svm  with data size enlarging 

fig      a  test error as a function of including excluding
user id feature on different data size   b  training and
test error rates on full size dataset 
training error
test error

   

error rate

   
   

as illustrated in fig    a   the learning trend with data size
enlarging is similar between including and excluding user
id feature  which is consistent with the fact that
increasing subset size makes reviews richer regardless of
the status of user id feature  then  based on the
assessment on the full size      m reviews from    k
users  dataset  the test error drops from       to      
after incorporating the user id feature on top of the word
feature  this simple user specified feature has evident
impact on the prediction performance  which further
helps us to validate our assumption on the users rating
bias hidden beneath the large scale review stars and texts 

b  star rating prediction

   
   
   
 

   

   

   

   

   

   

number of users

fig     softmax prediction results on small scale subsets
 yet still tens of thousands of reviews  
number of misclassified cases for both training and
testing phases  the training is done after    epochs for
each dataset  and the results are shown in fig    with error
rate numbers listed inside  as the user number increases 
the actual data size increases even more rapidly as
illustrated in fig     perceptron is implemented by hand in
python  following lecture notes  and we observe from
experiments that the test error drops with data size
increasing  for larger data size  the algorithm is able to
better separate the above   star rating from below   star
rating  the impact of user id feature is then evaluated on
subsets of various scale and also on the full size dataset 

for the   class star rating prediction task  we use
softmax and svm which will be discussed in this section 
softmax is first quickly evaluated before we focus on the
comprehensive results of svm  softmax algorithm is
implemented by hand in python  following lecture notes 
and the test results on small scale subsets are shown in
fig     where around     test error is obtained for   class
prediction  for large scale dataset  since svm is a
widely used standard algorithm  we will then focus on
svm to comprehensively analyze the rating prediction
and the underneath bias tendency  svm is implemented
using the scikit learn package     in python  we used
support vector classification  svc  model with linear
kernel  and here the performance metrics is the mean
square error  mse  for both training and testing  the
training and test mse of svm on various data size are
shown in fig     the trend of the learning curve is that
with data size increasing  overall test mse will decrease
whereas the training mse increases  for svm  it is
important to make sure that the training examples exceed

fi  

  

  

 

              

  

 

                  

 

                   

 

  

              

 

 

  

  

 

 

 

           

   

  

 

                     

 

                      

 

                      

 

                    

 

 

 

 

 

top       users

 

top      users

top     users

cs    final report

 

 

    

    

   

   

   

 

    

    

    

    

   

 

   

                

    

 

   

                      

 

   

   

 

 

                

 

 

 

fig     classification confusion matrix for  left  top       middle  top  k  and  right  top   k users reviews  the
vertical   to   are the actual star rating while the horizontal   to   at the bottom indicates the predicted star
rating  the gray scale map is generated based on independently normalizing the numbers in each row  the darker ones
indicate dominant prediction stars given by svm  a    low star rating pattern can be observed for the
misclassification under small data size 
training mse
test mse

   

     

     
     

     

   

mse

table i
mse on full size dataset

     

training mse

test mse

stemming   word removal

     

     

not x

     

     

not x w  user id feature

     

     

   
   
   

     
     

     

   
original

not  x 

adj adv stem stop

text processing
fig     training and test mse after different kinds of text
preprocessing methods 

the number of features  otherwise the performance will
not be acceptable  on the full size dataset        mse is
achieved  which is a reasonable result given such kind of
prediction task and the performance of svm algorithm 
we further analyze the statistical results of the svm
  class classification on various data size  and then
generate the confusion matrices as shown in fig     for
relatively small datasets  the misclassification has some
structured pattern for low star rating region  as illustrated
by yellow circles in fig    a  and  b   the algorithm tends
to give    star rating  or more specifically  predicting
actual star   to star    and predicting actual star   to star   
such pattern is not valid for higher stars like   or    then 
after further increasing the data size  such    pattern is
no longer valid either for low stars or for high stars  a dark
diagonal line is shown   due to the increase in the review
dataset  the rating bias of part of users can be neutralized
by others  so the algorithm wont give higher stars this

time for low star rating region  as shown in fig    right  
since nlp is involved  we also experiment with
different text preprocessing methods to investigate the
impact on the   star prediction task  here  we use top
     users reviews and fix the data size  four different
group of features are assessed and compared   original 
not x  adj   adv  stem stop   as can be seen in
the result  word stemming or adjective and adverb
extraction does not help in reducing the error rate  this is
in contrast to our initial expectation  this may mean that
the non stem part of the word  and the words with pos
other than jj and rb are all good indication of sentiment 
also  since the dataset is very big  feature size is allowed
to be large  while reducing feature dimension may lead to
high bias 
different feature choices are assessed for the full size
dataset as well  not x method performs well on
small size dataset  but for the full size dataset it can be
observed that it helps only with user id feature included 
as illustrated in table i  here  including the user id
feature gives a significant improvement in training and
test mse  we will then use this best performance
classifier to perform the statistical analysis on the users
bias 

fics    final report

word dependencies  incorporating word vectors  or do
sematic parsing  we dont have time to do this in our project
since the dataset is very big and parsing would need a long
time  text understanding is still an active research frontier
and we expect to use new technologies to get better results 

    

number    

    
   
   

references

   
   
 

 

 
  

  

  

  

 

 

 

 

 

bias

fig      statistical distribution of bias of      users  most of
the non zero counts range between    and   

c  user bias analysis
with svm classifier trained on full size data  we could
compute each reviewers bias br by 
  



        


where nr indicates the total number of reviews the
reviewer has written  stari represents the reviewers real
star rating of the i th review  and stari predict is the
predicted star  in short  br is the average of the reviewers
star deviations of every reviews he wrote  with our
definition of bias  we can get the statistical distribution of
users biases  as shown in fig      we only calculate the
bias of reviewers with more than   reviews  which
correspond with experienced yelp reviewers  from fig 
    we can see that most reviewers are neutral  which
agrees with natural intuition  most of the non zero counts
range between    and    indicating that experienced yelp
reviewers are not likely to be too extreme 
given the bias of a reviewer  when calculating the star
of a business  we should first subtract the bias from the
reviewers original star  and then calculate an arithmetic
mean  by which we think the final star would be more
objective 

iv  conclusion
in this project  we have applied machine learning
algorithms such as perceptron  softmax  and svm on
large scale public text dataset to perform classification tasks 
the impact of data size and feature selection on the
prediction performance is quantitatively studied  the rating
bias is statistically investigated based on the learning results 
for the future work  we plan to further enhance our feature
extractor and incorporate more high level features such as

    linshi  jack   personalizing yelp star ratings  a semantic
topic modeling approach   yale university       
    a  maas  r  daly  p  pham  d  huang  a  ng  and c  potts 
learning word vectors for sentiment analysis  acl
     
    fan  mingming  and maryam khademi   predicting a
business star in yelp from its reviews text alone   arxiv
preprint arxiv                 
    yelp dataset challenge  data available online  
http   www yelp com dataset challenge  last updated 
july          
    natural language toolkit  nltk   http   www nltk org  
     
    manning  christopher d   et al   the stanford corenlp
natural language processing toolkit   proceedings of   nd
annual meeting of the association for computational
linguistics  system demonstrations       
    fabian p   et al   scikit learn  machine learning in
python   the journal of machine learning research    
pp                 

fi