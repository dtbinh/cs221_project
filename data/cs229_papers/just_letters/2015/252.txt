solving the prerequisites  improving question answering on the babi
dataset
vincent su   john miller   jack zhu 

 

introduction
the aim of this project is to make progress towards building a machine learning agent that understands
natural language and can perform basic reasoning  towards this nebulous goal  we focus on question
answering  can an agent answer a query based on a given set of natural language facts 
we combine lstm sentence embedding models with an attention mechanism and obtain good results
on the facebook babi dataset      outperforming     on   task and achieving similar performance on
several others 

 

dataset

the facebook babi dataset is a synthetic dataset of    toy question answering tasks  each task targets
a specific skill that a general reasoning agent would be expected to have  such as answering yes no
questions or performing deduction over multiple sentences 
each of the    tasks consists of       training examples and       test examples  the tasks are
generated from a simulation of characters and objects interacting in a small  closed world  which produces
ground text describing the scene and question answer pairs  supervision is provided in the form of
answers for each of the questions and the location of relevant sentences in the input required to answer
the question  for development  we randomly partition the       training examples for each task into    
example training sets and     example development sets 

   

example

each training example consists of a sequence of supporting facts  a question  an answer  and labels
indicating the sentences relevant to answering the question 
 
 
 
 
 
 
 

mary moved to the bathroom 
sandra journeyed to the bedroom 
mary got the football there 
john went to the kitchen 
mary went back to the kitchen 
mary went back to the garden 
where is the football  garden    

support sentences are numbered  sequentially   and questions are labeled with the correct answer as well
as the index of the relevant sentences  this structure is repeated for each different tasks 
each
  formally 
  example is represented by a   tuple  q  f  a  r   where q denotes the question  f  
f   f             f k is a list of supporting sentences  a denotes the answer  and r    r            rn   denotes
the set of relevant sentences  further  fij denotes the i th word in the j th supporting sentence  and qi
denotes the i th word in the question 

 

related work

question answering is a long studied topic in natural language processing and machine learning  there
are a variety of approaches to the problem ranging from constructing and reasoning over knowledge bases
     inferring and reasoning over latent logical forms      to using neural networks     
several authors have studied using neural networks and attention for question answering  our work
and conceptual framework is inspired by the memory networks of      though our implementation of the
attention mechanism and inference modules is different  the work of kumar et  al     applies attention
and recurrent neural networks including the babi tasks and outperforms     on several of the tasks 
  partners

in cs    but not in cs   

 

fifigure    lstm sentence embedding model

 

approach

each of our models can conceptually be broken down into   parts  the first two parts are representation
modules  one module converts the supporting facts into a vector  f    and another module converts
the question into a vector  q   the third module is a reasoning module  it takes as input  f   and
 q  and outputs a distribution prob a   q  f   over words in the vocabulary  v   we describe several
implementations of this framework below 

   

lstm sentence embedding model

to better capture the semantic structure of the supporting facts and questions  the second model uses
sentence representations generated from a long short term memory network  lstm      
lstms are recurrent neural networks that can be trained to compose vector representations of
sentences from word embeddings       in our models  at time t  the lstm takes as input a new word
wt and updates its hidden state according to
ht   h wt   ht    ct    
where h is a variant of the lstm     with update equations 
it     wxi wt   whi ht    wci ct    bi  
f t     wxf wt   whf ht    wcf ct    bf  
ct   ft ct    it tanh  wxc wt   whc ht    bc  
ot     wxo wt   who ht    bo  
ht   ot tanh ct   
the vectors it   ft   ot and ct are called the input gate  forget gate  output gate and cell activation
vectors  respectively  and  is the sigmoid non linearity  there are all the same size as the hidden vector
h 
to construct  f    we first embed all of the supporting facts word by word using the lstm model 
then   f   is constructed by averaging the intermediate hidden states using a mean pooling layer
 f    

t
 x
ht  
t t  

where t is the number of words in the supporting facts  constructing  q  uses an identical procedure 
given  f   and  q   we compute prob a   q  f   by concatenating  f   and  q  and using this as
input to a feed forward neural network with   hidden layers  the network uses relu non linearities and
outputs a categorical distribution over all  v   words in the vocabulary  this model is depicted graphically
in figure   

 

fiobjective 

the model is trained by minimizing the regularized negative log likelihood
x
j    
 log prob a   q  f       kk  
 q f a r dtrain

where  is the parameters of the model 

   

attention mechanism

one of the weaknesses of the lstm sentence embedding model is handling questions with large numbers
of supporting facts  especially with the number of relevant sentences is small  in these cases  the lstm
model likely experiences difficulty with preserving information across time steps and learning long term
dependencies 
to address this issue  we extend the lstm embedding model with an attention mechanism  the
attention mechanism is implemented as a lstm that  at each time step  takes as input the question
and the current supporting fact and predicts with the supporting fact is relevant  only the relevant
sentences are then fed as input into the previously described lstm sentence embedding model  the
mechanism is trained using the relevant sentence supervision signal provided with each example 
more concretely  let
  s  denote
  the bag of words representation of sentence s  consider an example
 f  q  a  r   where f   f             f k   for t              k  we form


 f t  
xt   u
 q 
where u  rd  v   is a learned embedding matrix  at step t  the attention model takes as input xt and
outputs prob rt   x  t    an estimate if fact f t is relevant 
during inference  any sentence f t with prob rt   x  t         is deemed relevant  if no sentence
has prob rt   x  t          then we take the two sentences f i   f j with the highest probability under the
model 
the model takes advantage of the relevant sentence labels and can be trained in a supervised fashion 
we found that joint training of the attention mechanism and the question answering module yields the
best results  let f  denote the set of sentences declared relevant for some example  the full model is
trained by minimizing


tf
x
x
j    
 log prob a   q  f        
 log prob rt   q  f       kk    
t  

 q f a r d

   

implementation and training

all of our models were implemented from scratch using theano       
in all of our experiments  we use adagrad      to minimize j    which is in general non convex 
we initialize the word embeddings with pretrained glove vectors of dimension          the other model
parameters are all initialized with an i i d  gaussian of variance       in every entry  we use a mini batch
of size     an initial learning rate of                and by default  use     lstm cells and hidden
layers of dimension     in the feed forward model 

 

experiments

we evaluate the models introduced in the previous section on the synthetic tasks in the babi dataset 
for each of these models  the reasoning module computes prob a   q  f   over single answers only since
it is implemented as a feed forward neural network rather than an rnn  therefore  we only evaluate on
   of the    tasks in babi since the remaining tasks  list     and path finding       require a sequence of
answers in list form 

   

evaluation metric

all of the models are evaluated individually on each task using accuracy  defined to be the number
of correct answers divided by the number of questions  following      we consider the answer to each
question to be a single word  this makes evaluating correctness of an answer unambiguous 
 

fi   

results

model accuracy on the test sets for the    tasks are reported in table    the first three columns display
our baseline bag of words model  our lstm sentence embedding model  and the lstm model with
attention  for each task  we perform early stopping based on performance on the held out development
set  we also include metrics from the highest performing augmented memory network implementation
in      and the results of a human oracle  since our models are incompatible with tasks that require
multiple answers  we designate those with a question mark 
as expected  our lstm and lstm   attention models generally outperform the baseline on most
tasks  adding attention gives an additional performance boost on many tasks  possible explanations
for weakers performances are given in the next section  while our lstm attention model performs
worse the memnet implementation in      we approach its accuracy in many tasks  in particular  our
performance approaches within    of the memnet on tasks             and even exceeding it by   
on task    we believe that more careful cross validation of model size and hyperparameter tuning can
slightly improve our reported metrics 
table    preliminary results on development set

  single supporting fact
  two supporting facts
  three supporting facts
  two arg  relations
  three arg  relations
  yes no questions
  counting
  lists sets
  simple negation
   indefinite knowledge
   basic coreference
   conjunction
   compound coref 
   time reasoning
   basic deduction
   basic induction
   positional reasoning
   size reasoning
   path finding
   agents motivations

 
   

lstm
  
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
  
  
 
  

lstm with attention
  
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
  
  
 
  

memnet    
   
   
   
   
  
   
  
  
   
  
   
   
   
  
   
   
  
  
  
   

discussion
overview

in this section  we discuss and analyze the performance of our lstm embedding models relative to the
baseline bag of words model  our model featured two improvements  an lstm model which embeds
the stories  and then an attention model  we find that the base lstm model with word vectors gives
significant boosts for most of the tasks over the bag of words 
the attention mechanism provided an additional boost on many of the tasks  but it also severely
impacted performance on tasks which required more complex reasoning  this is potentially because the
attention mechanism failed to mark enough important sentences as relevant  making it impossible for
the reasoning module to draw correct inferences  we conclude that attention mechanisms have a lot
of potential to help with natural language processing  but our particular implementation was not well
suited for certain tasks  we provide a concrete example with discussion for our best task performance
relative to    
task     counting
 

fisupporting facts 
daniel took the milk there  
john moved to the hallway  
daniel left the milk  
daniel journeyed to the office  
question  how many objects is daniel carrying  
true answer  none
predicted answer  none
predicted relevant facts  daniel took the milk there  
daniel left the milk  
the lstms capacity for sequence learning is most clearly demonstrated in this task  our lstm
model with attention boosts performance to      which outperforms the stated result in the memnet
paper  the example above shows how the lstm can correctly infer the effect of compounding sentences
sequentially  whereas a bag of words classifier might simply have counted three instances of daniel in
the story and predicted three as the answer 

 

conclusion

we applied lstm embedding models with attention to a classic ai task  question answering on the
babi dataset  the attention mechanism greatly improved performance on many tasks  for questions
which required more inferential reasoning  the attention model was often too greedy and fed the lstm
inference model insufficient information to correctly answer the question  thus  we conclude that the
performance boost from the attention mechanism is highly sensitive to its specific implementation  our
experimentation found that the lstm based attention mechanism was well suited to tasks with a very
sequential structure like counting      for which we report a better accuracy than the optimized memnet 

references
    weston  jason  bordes  antoine  chopra  sumit and mikolov  tomas  towards ai complete question
answering  a set of prerequisite toy tasks  corr  abs                  
    weston  jason  chopra  sumit  and bordes  antoine  memory networks  corr  abs                 
    a  yates  m  banko  m  broadhead  m  j  cafarella  o  etzioni  and s  soderland  textrunner  open
information extraction on the web  in hlt naacl  demonstrations        
    j  berant  a  chou  r  frostig  p  liang  semantic parsing on freebase from question answer pairs 
empirical methods in natural language processing  emnlp        
    a  bordes  x  glorot  j  weston  and y  bengio  joint learning of words and meaning representations for open text semantic parsing  aistats       
    a  kumar  o  irsoy  j  su  j  bradbury  r  english  b  pierce  p  ondruska  i  gulrajani  and
r  socher  ask me anything  dynamic memory networks for natural language processing  corr 
abs                  
    hochreiter  sepp and schmidhuber  jugen  long short term memory  neural computation 
                    
    f  bastien  p  lamblin  r  pascanu  j  bergstra  i  j  goodfellow  a  bergeron  n  bouchard  and
y  bengio        theano  new features and speed improvements  deep learning and unsupervised
feature learning nips      workshop 
    j  bergstra  o  breuleux  f  bastien  p  lamblin  r  pascanu  g  desjardins  j  turian  d  wardefarley  and y  bengio        theano  a cpu and gpu math expression compiler  in proceedings of
the python for scientific computing conference  scipy  
     k  tai  r  socher  and c  d  manning  improved semantic representations from tree structured
long short term memory networks  acl      
 

fi     j  duchi  e  hazan  and y  singer        adaptive sub  gradient methods for online learning and
stochastic optimization  in conference on learning theory  colt  
     j  pennington  r  socher  and c  d  manning        glove  global vectors for word representation 
in empirical methods in natural language processing  emnlp  

 

fi