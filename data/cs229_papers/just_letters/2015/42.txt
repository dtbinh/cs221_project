classification of high grade vs low grade gbm tumors
vincent pierre berges  victor storchan  kevin luo
december     
abstract
this paper address glioblastoma  gbm  disease  which is a very frequent brain cancer in adults  classifying these tumors between two clusters of high grades and low grades tumors by noninvasive methods
would improve the accuracy of targeted therapy and personalized treatment  we dealt with a partially
processed data set of     patients  consisting of     patients affected by high grade tumors  very dangerous  and    affected by low grade tumors  less dangerous   we performed classification using the logistic
regression algorithm  after cross validation  the resulting overall error is about      the confusion
matrix and receiver operating characteristic  roc curve  are provided 

 

introduction and related work

traditionally  medical imaging has been approached as a qualitative science  new advances in medical
imaging acquisition and analysis enable the extraction of imaging features to estimate the differences between biological tissues  glioblastoma  gbm  is a notorious brain cancer with a high rate of death for
adult patients  according to p y  wen and santosh kesari  see       malignant glioblastoma accounts
for approximately     of the        new cases of malignant brain tumors that are found in adults in the
united states each year  as this disease is very common and performing invasive methods such as biopsies
can be harmful for the patient  it is of high importance to be able to recognize when immediate treatment
is necessary  we are thinking of using noninvasive methods such as imaging and machine learning to help
radiologists determine how dangerous a gbm tumor is  this approach is related to an expanding and
promising field called radiomics  although some papers dealt with multi modality imaging in cancer classification  they based their analysis on huge balanced data sets  see       what if someone only had access
to a smaller data set  or even to an unbalanced data set between low grade and high grade tumors  our
work tries to provide insight to this question 

 
   

data processing and features extraction
analysis of the data set

decoding tumor phenotype by noninvasive methods is an emerging field and literature provides the first
attempts of converting imaging data into a high dimensional workable feature space  see             we used
data from miccai brats provided by professor olivier gevaert and his students darvin yi and mu
zhou from the stanford center for biomedical informatics  we have     patients consisting of     patients
affected by high grade tumors  very dangerous  and    affected by low grade tumors  less dangerous   that
data has already been partially processed to identify the location  size  and segments of the tumor  within
a tumor  four regions can be emphasized 
necrosis

 edema

 non enhancing tumor
 

 enhancing tumor

ficlassification of high grade vs low grade gbm tumors
the matrix of predictors has to keep track of their specificities  each patient has five volumetric images
  d scans   four concern different modalities of the brain  t   longitudinal relaxation time  t c  longitudinal relaxation time after administration of contrast agent  t    transverse relaxation time  flair  fluid
attenuated inversion recovery mri  and one just focuses on the tumor  note that we do not follow a patient
overtime through his therapy  even though it does not reflect the exact reality  we count each data as being
related to a different patient 

   

features extraction

the data takes the form of   lists  one high grade and one low grade  of folders of patients each containing
  tomographic images  the function readmha m uses the library readdata d and allows us to extract
 d matrices of pixel intensities from  mha files  from the volume computed tomography images  we are
extracting histograms of the pixel intensities for each patient  which have to be preprocessed to be comparable  indeed  because of the improvement of software overtime  our data set contains images extracted
by different devices so the mean contrast and intensity are not comparable  to deal with this issue  we
converted the intensities to their z scores on the whole brain  this is the role of normalizevolume m 
then we extracted histograms for each of the four regions and four modalities and used them to generate
the features  there are some brain scans which do not contain certain regions and cannot produce the
histograms  one of the challenges of this project was to determine how to deal with lack of information
on a small set of training examples  see the next subsection for the resolution of the issue   the function
patientiterator m iterates over all the folders and images and calls getpredictors m on each patient to
obtain the vector of predictors  when patientiterator m terminates  it generates and saves a matrix id
containing the names of the patients  a matrix of predictors x and a vector of output y that can be used
later for the classification 

   

choice of the predictors

for this project  our first step was to use n      features  from the first four images of the brain  the
histograms of the pixel intensities were computed on the four available regions  and we extracted both
the mean and the standard deviation from them  this yields    features  from the tumor image  we can
extract the size of the different regions of the tumor  this produces four more features  after training our
algorithm in a first attempt  this naive approach led to complete separation of the set  we obtained the
following warning in matlab 
 
 

warning   iteration limit reached  
warning   the estimated coefficients perfectly separate failures from successes   this
means the theoretical best estimates are not finite  

actually  the    features separated entirely our subset of training low grades tumors  approximately    
of      we concluded that the number of predictors was too big compared to the number of low grades of
our training sets 
for this reason  we decided to reduce the number of features  we tried to focus on getting regions necrosis
    and enhancing tumor     most accurately  non enhancing tumor     is important for low grade
tumors  but due to its scarcity  this should lead to lots of nan in the predictors  edema     represents
swelling  as a consequence  it may not be extremely clinically relevant  therefore we reduced to a set of
   features  the size  pixel count  of the different regions were removed  and the information  mean and
variance  related to edema on each image was removed too  with only    features  there was no longer
a perfect separation of low and high grades  we reduced the flexibility of the model   at this point  one
can produce a matrix x of predictors  filled with floats and nan values at the places were there was

 

ficlassification of high grade vs low grade gbm tumors
no information provided by the volumetric images  the idea is then to replace the nan values by a
combination of known values of the same kind  this is the role of the script cleandata m 
 nan of the standard deviation are replaced by  
 nan of the mean of the histograms are replaced by the mean value of the predictor
on the picture  a  below  one can see that the histogram extracted from the healthy tissue does not have the
tiny bump existing on picture  b   one need to investigate on this bump and to try to extract information
from it  the path we followed was to consider the characteristics of the   regions of the tumor  the
information is extracted by counting the frequency of the colors which appear on the volumetric images of
the regions 

 a  a histogram from a non tumor slice

 

 b  a histogram of a slice of the tumor

validation of the model and consistency of the results

we tried two approaches  logistic regression and svm  we compare their performances in this part 

   

the hold out cross validation implementation

we used the hold out cross validation to predict what is the best threshold to use with our logistic regression 
to do so  we separated the matrix of predictors into      training sets and associated test sets for each value
of the threshold  the threshold ranges from   to    and used the glmfit   set to logistic regression and
glmval   functions of matlab  we then averaged the values found for the true positive  true negative 
false positive and false negative to obtain the hold out cross validation values  we designed our test sets
to be picked at random but we wanted them to contain the same number of high and low grade  for this
reason  out of the     patients  each test set contains    low and    high  this means that a purely random
prediction would predict with     error 
to compare different methods  and their efficiency  we ran a svm implementation on our data set with
matlab toolbox liblinear      in this svm approach  we carry out the same experiment except that the
tuning parameter is the  decision parameter   or  decision value    in matlab  it takes a value in       
and corresponds to the distance of the separating hyperplane to the origin 

 

ficlassification of high grade vs low grade gbm tumors

 a  the roc curves for both regressions

   

 b  precision recall curves for both regressions

the roc

with different values of threshold  or decision parameter   we were able to compute the receiver operating
characteristic curve and the area under it 
 the area under the roc curve for logistic regression is        
 the area under the roc curve for svm is        
we can notice that svm gives better results than the logistic regression 

   

the precision recall

with different values of threshold  or decision parameter   we were able to compute the precision recall
curve and the area under it 
 the area under the precision recall curve for logistic regression is        
 the area under the precision recall curve for svm is        
once again  svm gives better results than logistic regression 

   

the confusion matrix

after computing the threshold that minimizes the hold out cross validation  we can fit a logistic regression
using this threshold  in our case  the optimal threshold is around      and gives the following confusion
matrix 
real false real true
predicted false
     
    
predicted true
    
     
using this threshold  the test error is         random guessing is      
there is another way to fix the threshold  we could use the threshold that equates specificity and sensitivity 
in our case  determining the low grade patients is as important as determining the high grade ones  for
this reason  this method seems appropriate for our project  looking at the sensitivity vs specificity curve 
we see they cross for a threshold value of       the confusion matrix obtained is 

 

ficlassification of high grade vs low grade gbm tumors

predicted false
predicted true

real false
     
    

real true
    
     

as expected  the confusion matrix is now more symmetric  with more balanced false positive and false
negatives  the error increased only moderately to        which makes this method for choosing the
threshold very encouraging 

 a  error  specificity and sensitivity analysis for logistic

 b  error  specificity and sensitivity analysis for svm

comparing these results with the svm approach  the decision value that minimizes the error is       the
error is         for this value  the confusion matrix is 
predicted false
predicted true

real false
     
    

real true
    
     

similarly  when we equate specificity and sensitivity  the decision value is      and the error only increased
to reach        the resulting confusion matrix is 
predicted false
predicted true

real false
     
    

real true
    
     

we notice that the svm prediction is better  once again than the logistic regression 

 

conclusion

to conclude  after carefully cleaning the data set  then studying it both in terms of understanding the
relevant features  and in terms of extracting these features we handled two classification algorithms  the
results showed that even with an unbalanced data set which provide few information on the low grade
tumor  the learning phase generates predictions which out perform the nave approach of      although
the point of view adopted in this work stays at the level of phenotypic information  other approaches
could be handled  for instance  radiogenomics  tries to establish links between image features and gene
expression  olivier gevaert published innovative research on this field  see           
note that all the source code is available online at

https   github com cs   classif 
 

ficlassification of high grade vs low grade gbm tumors

 

references

    patrick y  wen  m d   and santosh kesari  m d   ph d  n engl j med                   july    
    doi          nejmra       
    aerts  h  j   velazquez  e  r   leijenaar  r  t   parmar  c   grossmann  p   cavalho  s         lambin  p          decoding tumour phenotype by noninvasive imaging using a quantitative radiomics
approach  nature communications    
    lambin  p   rios velazquez  e   leijenaar  r   carvalho  s   van stiphout  r  g   granton  p      
  aerts  h  j          radiomics  extracting more information from medical images using advanced
feature analysis  european journal of cancer                 
    itakura  h   achrol  a  s   mitchell  l  a   loya  j  j   liu  t   westbroek  e  m         gevaert 
o          magnetic resonance image features identify glioblastoma phenotypic subtypes with distinct
molecular pathway activities  science translational medicine             ra       ra    
    gevaert  o   mitchell  l  a   achrol  a  s   xu  j   echegaray  s   steinberg  g  k         plevritis  s 
k          glioblastoma multiforme  exploratory radiogenomic analysis by using quantitative image
features  radiology                  

 

fi