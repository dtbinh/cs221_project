cs     machine learning
event identification in continues seismic data
please print out  fill in and include this cover sheet as the first page of your
submission  we strongly recommend that you use this cover sheet  which will
help us to get your graded homework back to you more quickly  as well as help
us with tracking submissions 
please mark the submission time clearly below  it is an honor code violation to
write down the wrong time 
if you are submitting this homework late  each student will have a total of seven
free late  calendar  days to use for homeworks  project proposals and project
milestones  once these late days are exhausted  any assignments turned in late
will be penalized     per late day  however  no assignment will be accepted
more than four days after its due date  and late days cannot be used for the
final project writeup  each    hours or part thereof that a homework is late uses
up one full late day 
on campus  non scpd  students  please either hand in the assignment at the
beginning of class on wednesday or leave it in the submission cabinet on the  st
floor of the gates building  near outside gates     and     

name   alex hakso
sunet id  ahakso           
name    fatemeh rassouli
sunet id  frasouli           

fisection    introduction
injection of saline wastewater by the oil and gas industry has resulted in earthquake rate increases in much of the
central and eastern united states  the changes have been significant  oklahoma has experienced an approximately
    fold increase in frequency of earthquakes greater than magnitude     since       walsh   zoback         the
causal mechanism is poorly understood  complicating risk assessment and mitigation  the earthquakes themselves
provide the primary window into the nature of the relationship between wastewater injection and induced seismicity 
accordingly  robust and efficient earthquake detection is an important component of addressing increased seismic
risk associated with wastewater injection  the input to our algorithm is a large number of short time series
amplitude recordings  we then use a variety of machine learning algorithms to output a predicted classification  the
time series does does not contain seismic energy originating from an earthquake 

section    related work
earthquake detection is a longstanding problem in geophysics  and the research comprises hundreds of approaches 
spanning decades  indeed  the features driving our algorithms draw directly from the literature  early event detection
algorithms searched for anomalous amplitudes over short time periods in the time series  using  for example  a ratio
of short term to long term average amplitudes  as documented by freiberger         this method rarely produces
false positives when the monitoring location is relatively free from impulsive noise  seismic monitoring locations are
chosen largely to accommodate this consideration  a weakness of this approach is that low magnitude events are
often below the detection threshold for this method  producing many false negatives 
to compensate for this difficulty  a particularly effective method of identifying low amplitude signals in a relatively
high amplitude noise environment is template matching with cross correlation  known as a matched filter  anstey 
       the shortcoming of this method is that the results are sensitive to the form of the master waveform used as a
template  the form of the target wave is a product of the nature of the source producing the signal  as well as its
location  generally  these are unknown quantities  making the success of this method dependent on informed
template selection  templates are often chosen using a high amplitude event as a model or by developing a synthetic
waveform based on the greens function associated with an estimated earth model  schaff   waldhauser       
used a dataset near parkfield  ca to demonstrate that a matched filter technique can allow for an improvement for
earthquake detection threshold to approximately an order of magnitude lower energy seismic events 
gibbons   ringdal        employed an array of seismometers in combination with cross correlation techniques to
achieve a further improved detection threshold  when available  using multiple seismometers is advantageous  the
magnitude of completeness threshold for our study area  the magnitude at which all events of this size or larger are
detected  is estimated to be approximately mm      llenos   michael         this provides a benchmark goal 
albeit a rather conservative one  the goal of this research project is to build on existing techniques to achieve high
event detection reliability using a single seismometer 

section    dataset and features
the data used in this study was gathered by the united states geological survey in guy arkansas using a
continuously deployed seismometer in guy  arkansas  network ag  station whar   the data was cleaned and
low passed to    hz by clara yoon and greg beroza  acceleration amplitudes were sampled at     hz on  
components  vertical  n s  and e w  the portion acquired for this study was   month of recording  although a week
was found to be acceptable for training and testing the algorithm  this week contained            samples  which
were divided into   seconds windows  which we overlapped by   seconds  resulting in a total of         candidate
time windows        events had been identified by clara yoon and greg beroza using their patented efficient
similarity search of seismic waveforms  beroza et al         methodology  as discussed in section    the chosen
four features drew on existing literature for event detection  they were  variance  ratio of short term average
amplitude to long term average amplitude  ratio of low frequency energy to high frequency energy  and maximum
match filter value 
   match filter value
matched filters are an effective method of identifying signals buried in noise  providing a reasonable approximation
to the target signal is used  to calculate this feature  each window is cross correlated against a set of templates  this
work employs a set of templates that is highly representative of the waveforms produced by earthquakes in the area 
the templates were developed using unsupervised machine learning to cluster the training events based on
similarity of waveforms  agglomerative hierarchical clustering was used to group the events into maximally

fidissimilar groups based on a dissimilarity matrix produced by cross correlating each event against every other  the
three components provided three dissimilarity dimensions  which were weighted by the signal to noise ratio as
determined to sta lta  once the event had been clustered by similarity of waveform  the templates were formed
by stacking several representative events within each group at maximum cross correlation value to reduce the impact
of noise  once each template had been cross correlated with a window  the maximum matched filter value was
taken  viewing the values assigned by classification clearly shows that the feature calculated by this rather involved
method produced exceptional results for discriminating between events and non events 

figure    event windows of the left  non event windows on the right are a small  random subset of total non event
windows 
   variance
fundamentally  seismic energy arriving on seismometers increases the magnitude of the amplitude recorded 
variance is a convenient measure of the average amplitude  and the squared component of the term brings it to a    
correspondence with energy in the ambient seismic field  as energy is proportional to   the arrival of
seismic energy originating with an earthquake is one of many causes which may cause an increase in amplitude  a
common examples is nearby vehicle or foot traffic 

figure    left  large events have unambiguously high variance  while smaller events are distributed more similarly
to non event windows  right  y range is reduced
   sta lta
this feature identifies anomalous amplitudes relative to immediately adjacent time signals  this feature is designed
to distinguish between long term high amplitude noisy periods and impulsive transient amplitude increases
 freiberger        

fifigure    here  the average abs  amplitude  of the window indicated by the green bracket is substantially higher
than the average abs amplitude  of the window indicated by the yellow bracket  this yields a high sta lta value 
   spectral content
noise on seismometers located on the surface of the earth is generally relatively high frequency  using

  
          
  
           

where   is the energy in the f hz range  a measure of the relative energy in the signal is obtained  a simple fft with
a hamming window is employed in matlab to calculate spectral content 

section    methods  an introduction to statistical learning  james et al   springer       
in this work  we used classification methods  including logistic regression  nave bayes  knn and svm 
as our learning methods  these methods are briefly described as follows 
   logistic regression
in this method  p x   pr y   x  is calculated using a logistic function  eq     outputting a value between   and  
for all values of x  maximum likelihood is then used to fit the model  eq    
   
e    x

p x   

log 

   e    x

p x 
    e    x
  p x 

   

   nave bayes
this model follows bayes rule  eq    to model p x y  using the strong assumption that the features are conditionally
independent given y  eq    
p x   y  p y 
   
p y   x   
p x 
n
   
p x   y     p xi   y 
i  

   knn  k  nearest neighbor 
since conditional distribution of label y given a feature x is mostly unknown for real data  computing the bayes
classifier is impossible  many approaches attempt to estimate the conditional distribution of y given x  and then
classify a given observation to the class with highest estimated probability  one such method is the k nearest
neighbors  knn  classifier  given a positive integer k and a test observation x   the knn classifier first identifies
the k points in the training data that are closest to x  in feature space  this set is termed n   knn then estimates the
conditional probability for class j as the fraction of points in n  whose response value equals j 

pr y   j   x   x     

 
 i  yi   j 
k in 

finally  knn applies bayes rule and assigns the test observation x  to the class with the largest probability 
   svm  support vector machine 
for this project  we used svm methods with a variety of kernel and objective functions  as listed below 

   

fimin

c classification    

n
  t
w w   c  i
 
i  

s t  yi  wt   xi     b     i and i    i          n
min

  t
  n
w w     i
 
n i  

nu classification    

s t  yi  wt   xi     b     i and i    i          n and    

k xi   x j      xi t x j     q

k xi   x j     e

polynomial kernel    

  xi x j

laplace radial kernel    

section    experiments  results  discussion
excluding the agglomerative hierarchical clustering associated with feature calculation  which was carried out in
matlab  we performed all of our data analyses using r language  since our data is very skewed  we used precision
and recall functions to compare the results of different methods 

 
 

 

true positive   true positive    false positive
 
true positive   true positive    false negative
 

precision     

 

recall     

the first challenge was picking the right size of test and training datasets  to find the best size of the data  we used
e     package in r to run nave bayes model on different sizes of training data set ranging from     to     of
the total data set  the purpose of running the nave bayes is that  because of its strong assumptions  this method has
a low computational cost  the calculated precision and recall for this method is shown in figure    note that in this
figure  we tested two types of features  original and normalized 

figure    nave bayes model fitting over different sizes of training datasets  using original and normalized features 
the most changes are observed for          and     of the total data  we used these fractions for the rest of our
study  also  the values of recall for the predicted labels from normalized features are very small  considering the
trade off between precision and recall values  we decided to use our data with original features 
next  we tested various svm methods  figure   shows the results of our classification using c classification svm
and polynomial kernel equation  here  we have changed both the training data subset number and the degree of
polynomial  the recall of these methods is still low  showing that the model is not detecting a good fraction of
events  this is in fact not acceptable in our context  as false negatives are costly in characterizing earthquake
swarms  attempting to improve recall  we varied the kernel functions  additionally  we implemented nu
classification svm  which has to the potential to handle skewed data more robustly  results of these methods  in
addition to the results of logistic regression  are provided in figure   

fifigure    polynomial svm classification over different subsets of training dataset 
the recall value of all these methods ranges from     to      showing the low performance of these methods 

figure   precision and recall of the low performance methods
finally  we ran knn method over various subsets of the dataset  we chose k values of         and     remarkably 
in each case  the precision was       the recall value as a function of chosen k is shown in figure    note that for
each value of k  the recall value is in excess of        since we achieved impressive recall and precision with k   
which incurs acceptable computational cost  we concluded that the optimal model is found using knn with k   

figure    the resulting recall from performing knn classification 

section    conclusions future work
applying knn with the features outlined has been demonstrated to be an efficient and practical approach for robust
earthquake detection  exceeding the baseline detection threshold in arkansas by three orders of magnitude  these
results have been well received in the geophysics department  and we expect to present internally next quarter  our
methodology holds promise for enabling a more detailed analysis of induced seismic events in oklahoma and
arkansas  moving forward  we intend to explore data normalization effects more thoroughly  and fine tune
parameters involved in feature calculation 

fireferences
anstey  n          correlation techniquesa review   geophysical prospecting                 retrieved from
http   onlinelibrary wiley com doi         j                tb      x abstract
beroza  g  c   oreilly  o  j   yoon  c  e     bergen  k          efficient similarity search of seismic waveforms 
united states 
freiberger  w  f          an approximate method in signal detection  dtic document 
gibbons  s  j     ringdal  f          the detection of low magnitude seismic events using array based waveform
correlation  geophysical journal international                  doi         j         x            x
llenos  a  l     michael  a  j          modeling earthquake rate changes in oklahoma and arkansas  possible
signatures of induced seismicity  bulletin of the seismological society of america                   
doi                   
schaff  d  p     waldhauser  f          one magnitude unit reduction in detection threshold by cross correlation
applied to parkfield  california  and china seismicity  bulletin of the seismological society of america 
                  doi                   
walsh  f  r     zoback  m  d          oklahoma  s recent earthquakes and saltwater disposal  science advances 
 june       doi         sciadv        

the facilities of iris data services  and specifically the iris data management center  were used for access to
waveforms  related metadata  and or derived products used in this study  iris data services are funded through the
seismological facilities for the advancement of geoscience and earthscope  sage  proposal of the national
science foundation under cooperative agreement ear        
the concatenated data was provided by clara yoon 

fi