automatic image colorization
harrison ho  h o stanford edu  and varun ramesh  vramesh  stanford edu 

abstract image colorization is a difficult problem that often
requires user input to be successful  we propose and evaluate a
new approach to automatically colorize black and white images
of nature without direct user input  our approach uses support
vector regressions  svrs  to predict the color of a region
locally  then a markov random field  mrf  to smooth color
information across the image  we find that our algorithm is
able to recognize textures that correspond to objects such as
trees and water  and properly color them 

i  i ntroduction
fig    

an example usage of the algorithm 

image colorization is the process of adding color to
grayscale or sepia images  usually with the intent to modernize them  by hand  this can be a time consuming and
repetitive process  and thus would be useful to automate 
however  colorization is fundamentally an ill posed problem
  two objects with different colors can appear the same on
grayscale film  because of this  image colorization algorithms commonly require user input  either in the form of
color annotations or a reference image 
we propose an algorithm to automatically colorize black
and white images in restricted circumstances  without requiring any user input  our technique works by training a model
on a corpus of images  then using the model to colorize
grayscale images of a similar type  we use support vector
regressions  svrs  and markov random fields  mrfs  in
our approach 
in order to define the problem  we represent images in the
yuv color space  rather than the rgb color space  in this
color space  y represents luminance and u and v represent
chrominance  the input to our algorithm is the y channel
of a single image  and the output is the predicted u and v
channels for the image 
ii  background
a  previous work
existing work with image colorization typically uses one
of two approaches  the first approach attempts to interpolate
colors based off color scribbles supplied by an artist  levin
et al develop an optimization based approach which colors
pixels based on neighbors with similar intensities      luan

et al further build on this work by not only grouping
neighboring pixels with similar intensities  but also remote
pixels with similar texture      this addition was designed
to facilitate colorizing images of nature 
the second approach to colorization has the user supply
a reference image  the algorithm then attempts to transfer
color information from the reference onto the input grayscale
image  these algorithms typically work by matching up pixels or image regions by luminance  bugeau and ta propose
a patch based image colorization method that takes square
patches around each pixel      they then extract various
features such as variance and luminance frequencies in order
to train a model  charpiat et al develop a color prediction
model which takes into account multimodality   rather then
returning a single prediction  they predict a distribution of
possible colors for each pixel      they then use graph cuts
to maximize the global probabilities and estimate the colored
image 
for our algorithm  we expand on the second approach 
training a model over a corpus of images rather than a single
image  our goal is that  once the model is trained  users will
not need to provide any input at all to the algorithm 
b  related work
our work here is largely inspired by the work of f  liu 
et  all  who used deep convolutional neural networks  dcnns  to estimate a depth channel given individual monocular
images      their approach constructs an mrf over the
superpixels of a given image and estimates the potentials
of the field using a dcnn  we apply a similar approach 
modeling an mrf over the superpixels in an image  and
using svrs to provide local estimates 
iii  dataset and f eatures
a  dataset
to train and test the classifier  we decided to focus on
images of yellowstone national park  we downloaded photos
on flickr with the tags yellowstone and landscape  as this
gave a good selection of photos that did not include animals 
humans  or buildings  we scaled the images in our dataset to
a constant width of     pixels  and eliminated any already
grayscale photos  additionally  we split the images into a
training set     images  and a test set      images  
b  image segmentation
in order to constrain the problem  we segment the images
into sections using the slic superpixel algorithm      we
chose the slic algorithm over other segmentation algorithms due to its effectiveness in creating uniform  compact

fisegments  we used the implementation in scikit image     
figure   shows the results of slic clustering on one of our
test images 
fig    

the results of image segmentation on a grayscale image 

c  image representation and training
for our model  instead of predicting color pixel by pixel 
we predict two real values  u and v channels  for each
segment of the image  this allows us to color segments based
on image structures  additionally  we assume that the u and
v channels are independent given an image segment 
we utilize support vector regressions  a generalization of
support vector machines  given training vectors  x i    
 
rp for i       m and a vector y  rm   a support vector
regression specifies the following minimization problem 
where c and  are chosen constants 
n

x
 
 i   i  
min  w  w   c
w b    
i  
subject to yi  w   x i     b     i  
w   x i      b  yi     i
i   i     i           n
this optimization problem can be performed efficiently using
scikit learns svr function  which performs epsilon support
vector regression optimization with an underlying liblinear
implementation     
for our model  we train two separate svrs   one for each
output channel  to extract feature vectors for each of the
image segments  we take the square of      pixels around
each centroid  we then perform a  d fast fourier transform
 fft  on these squares  giving us our feature vectors  x i    
for both regressions  the feature vectors  x i    are used as
the input  while the average u and v values of the segment
x i  are used as the output  we used the default gaussian
kernel for training the svrs 
fig    

conversion of superpixels to feature vectors 

d  image testing and icm smoothing
to estimate the chrominance values of a test image 
we perform the same process of segmentation and feature
extraction described above  we then run the two svrs over
the segments  giving us an initial estimate of the color 
to smooth out the shading of similar  adjacent superpixels 
we model every test image as two markov random fields
 mrfs   with one mrf per predicted channel  we represent
local potentials for the hidden chrominance value of a
superpixel as a gaussian  n  i      where i is the result
of the svr on that superpixel  we additionally represent
neighboring potentials between hidden chrominance values
of adjacent superpixels with a distance function 
the total energy ei of a single superpixel x i  with a
hidden chrominance ci can hence be represented as
ei  

  ci  i     
 
   

x

  ci  cj     

x j  n  i 

where  weights the relative importance of neighboring
pixels compared to single pixels  and n  i  denotes the set
of adjacent superpixels to x i    for any two pairs of adjacent
superpixels x i  and x j    we only include x j  in the set
n  i  if    x i      x j       lies below a threshold value 
finally  to minimize the total energy of the mrf  we run
iterated conditional modes  icm  until convergence 
at this point  we have the original y channel and estimated
u and v channels for a target image  converting these
channels to the rgb space yields our final colorization
estimate 
fig    

flowchart of the algorithm 

fie  hyperparameter tuning
we evaluated the hyperparameters of the svr and icm
over a range of values  and selected those that produced the
minimum error  in our formulation we define the error as the
average distance between our predicted rgb values and the
actual rgb values  after evaluation  we selected         
and c        for the svrs  and      for the icm 
iv  r esults
fig     the original color image  the de colorized input  and our algorithms
prediction  respectively 

our method still has room for improvement  our method
fails to reconstruct the full range of colors present in the
original image  and fails to include more shades of yellow
and brown  this may be due to the limited range of colors
present in our data set  which primarily consists of shades
of blue and green  training the model on images with a
balanced variety of colors may alleviate this behavior 
another issue with our method is that shades intended
for one section of the image often bleed into neighboring
sections  for example  in the third image  the water adjacent
to the large tree on the right is shaded similarly to the tree
itself  this may be because for small superpixels  taking
       squares around the centroid of the superpixel often
takes in more pixels than contained within the superpixel
itself  thus  the estimated color is strongly affected by neighboring superpixels  which can introduce error if neighboring
superpixels are significantly different shades of color 
finally  our method often produces images with low
saturation  an issue particularly noticeable with the second
example  this may be a result of our assumption that each
superpixel has a single most likely coloring  despite the fact
that superpixels may take on one of several  equally plausible
colorings  this assumption can cause the svrs to average
the chrominance value outputs  decreasing the saturation of
the estimated color  a multimodal approach may help to
increase the saturation of the predicted colors  which has
been explored in depth by charpiat et al     
v  c onclusion

despite the ill posed nature of the problem  our algorithm
performs well on the test set  we display some of our results
in figure    while the method does not reproduce all colors
correctly  it in general produces plausible coloring results 
moreover  our method successfully colors environmental
features differently  in the third example  our method notably
shades the reflection of trees in the water differently from the
actual trees 
in figure    we compare the average error for the grayscale
image with the original  the average error prior to running
icm smoothing  and the average error after running icm
smoothing  implementing the svrs decreased the error by
       while implementing the icm smoothing decreased
the error by an additional     percentage points  visually  the
icm smoothing helps to reduce anomalies within an image 
as an example  in figure    segments which are erroneously
colored a red shade are changed to blue and white shades in
our prediction 

automatic image colorization by training on a corpus of
training data is a feasible task  we envision a potential system
where a user can colorize grayscale images by simply declaring relevant tags for a target image  a hypothetical system
can then automatically retrieve photos with the relevant tags
and train models specific to these tags 
our project code is available at https   github 
com harrison     recolorizer 
vi  f uture w ork
there are a number of measures than can be taken to
improve on the performance of the algorithm 




fig    

average error comparison before and after icm smoothing

before
processing
         

with
svr only
         

with
svr and icm
         

currently  we simply use a gaussian distribution for local potentials in our mrfs   we use the svr prediction
as the mean  and a fixed variance  a gaussian is too
simplistic  as a given texture can be indicative of two
or even three colors  for example  leaves vary between
green and red  but are unlikely to be other colors  in
order to capture this  we need to predict a distribution
of values for each color  not just a single value  svrs
are insufficient for this task 
we can represent images as a pixel wise mrf  with
local potentials on each pixel and pairwise potentials
between adjacent pixels  with this model  we may be
able to prevent color bleeding and provide other visual
improvements  such as allowing backgrounds to poke

fi





through trees  this could also be coupled with pixelwise svr prediction  and the elimination of the image
segmentation step 
while svrs were reasonably successful  many state of
the art algorithms use dcnns to great effect  early
experimentation with dcnns did not prove promising
for us  but adjustments in layer definitions and an
increase in training data might make them more viable 
we can incorporate additional features in the svrs
to increase the models expressiveness  bugeau et al
incorporate three features  the variance of a pixel patch 
the  d discrete fourier transform  and a luminance
histogram computed over the patch     
we currently use the yuv color space to separate luminance and chrominance  alternate color spaces exist
that may do this more effectively  cielab  
r eferences

    radhakrishna achanta  appu shaji  kevin smith  aurelien lucchi 
pascal fua  and sabine susstrunk  slic superpixels compared to state ofthe art superpixel methods  pattern analysis and machine intelligence 
ieee transactions on                        
    aurelie bugeau and vinh thong ta  patch based image colorization 
in pattern recognition  icpr          st international conference on 
pages           ieee       
    guillaume charpiat  matthias hofmann  and bernhard scholkopf  automatic image colorization via multimodal predictions  in computer
visioneccv       pages         springer       
    anat levin  dani lischinski  and yair weiss  colorization using
optimization  acm trans  graph                 august      
    fayao liu  chunhua shen  guosheng lin  and ian d  reid  learning
depth from single monocular images using deep convolutional neural
fields  corr  abs                  
    qing luan  fang wen  daniel cohen or  lin liang  ying qing xu 
and heung yeung shum  natural image colorization  in proceedings
of the   th eurographics conference on rendering techniques  pages
        eurographics association       
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot  and e  duchesnay  scikit learn  machine learning in python  journal of machine
learning research                    
    stefan van der walt  johannes l  schonberger  juan nunez iglesias 
francois boulogne  joshua d  warner  neil yager  emmanuelle gouillart  tony yu  and the scikit image contributors  scikit image  image
processing in python  peerj    e            

fi