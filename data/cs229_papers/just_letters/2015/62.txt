object classification using rgb d data for vision aids
apples and oranges
kyle chiang  anthony pelot  and trisha lian
i  introduction
with the increased availability of cheap and reliable depth
sensors  imaging systems can now use depth information to
better detect and locate objects in a scene  new augmented
reality  ar  systems  such as the microsoft hololens  are
now mounting depth sensors on their glasses to improve
functionality 
the motivation behind our project is to perform object
recognition using rgb d  color and depth  data for a vision
aid being developed at stanford  this vision aid consists of a
pair of ar goggles with an asus xtion depth sensor mounted
on top of it  we would like to use the rgb d data from this
sensor to train a classifier that can recognize household items
from a database  information on the classified object can
then be relayed to a user through the vision aid  this system
has the potential to help the visually impaired navigate and
perform everyday tasks more efficiently 
our training data consists of an rgb d data set from a
team at the university of washington  we used two different
 d descriptors to extract feature vectors that describe
each frame of rgb d data  the input features to our
algorithm are these descriptor vectors  we then use svm
to train a classifier that can output the predicted class
of new rgb d images  using test data we collected with
our own experimental setup  we evaluated the effectiveness
of our classifier 
the best cross validation results gave     accuracy while
using the shotcolor descriptors and a subset of tested
items  experimental results from this model had a prediction
accuracy of     when presented with testing data obtained
from our experimental setup 
ii  related work
object detection and recognition using rgb d data is an
ongoing research area in computer vision  in particular  there
are a wide range of  d descriptors that can be used as feature
vectors     for example  lai et al  used a combination of
spin descriptors on point clouds and sift descriptors on
corresponding rgb images to train three classifiers  linear
support vector machine  linsvm   gaussian kernel support
vector machine  ksvm   and random forest  rf   using
rgb d data      their classifiers  however  were only tested
on their own data set and not on data collected from different experimental setups  in addition  their feature vectors
required extensive pre processing of descriptors 
instead of choosing standard descriptors  bo et al  introduced a collection of novel features that utilize the depth
map  the rgb image  and their corresponding point cloud

     these features capture specific aspects such as shape 
edges  or relative object size  these features performed better
then standard descriptors when trained with linsvm  in
a different paper  the same authors used sparse coding to
learn hierarchical feature representations from raw rgb d
data      in addition to object recognition  classifiers using
rgb d data have also been used for other computer vision
tasks  for example  goswami et al  use hog descriptors
and entropy maps extracted from rgb d data to train an
rf classifier to recognize faces 
iii  dataset and features
a  training set
we use a dataset collected by a team at the university of
washington for research on rgb d data      this dataset
contains     household objects grouped into    categories 
the dataset was recorded using a microsoft kinect while the
objects were rotated on a turn table to obtain multiple angle
views  the camera was also mounted at different heights
to obtain viewpoints of different angles from the horizon 
objects are segmented from the background and presented as
textured point clouds  each viewpoint and its corresponding
point cloud acts as a single training example 
we trained on a subset of the     object categories in
this set  each object category contains data from a variety
of different objects  for example  in the apple category there
are different types of apples  e g  red  green  fiji   in total 
we have around      training examples per category     
view points from     different objects in each category  
figure   shows a small example of items contained in the
training dataset 

fig    

example images from our training data 

fib  testing set
in order to test our classifier with completely new inputs 
we collected our own rgb d data  first  we mounted a
calibrated asus xtion rgb d sensor on a raised platform
overlooking a table  next  we performed a calibration step
to obtain average depth and rgb images of the static
background  we then placed objects in front of the sensor
and captured rgb d images  the object was automatically
segmented from the image by selecting points that differ
significantly from the static background  multiple objects
were separated into individual point clouds using connected
component approaches  the depth and rgb information was
then projected back out into  d space to form a textured
point cloud  this data matches the type of input we obtain
from our training data set 
we collected textured point cloud data with our own
objects from each category  we rotated the object by hand to
obtain different viewpoints of each object  like the training
data  each viewpoint and its corresponding point cloud was a
single testing example  in total  we had around       views
of each of the   objects 
figure   shows a few examples of our testing data gathered
from our experimental setup  figure   shows one textured
point cloud of a mug from the testing data 

fig     examples of several objects and different viewpoints from our test
set 

c   d descriptors
in order to represent our textured point clouds with a
feature vector  we use  d descriptors   d descriptors reduce
a point cloud into a vector or histogram that captures the key
aspects of the object  although there are many available  d
descriptors  we focus on two  viewpoint feature histograms
 vfh      and signature of histograms of orientations with
color  shotcolor       descriptors are calculated and
manipulated using the point cloud library  pcl      
vfh captures both the viewpoint information of the point
cloud as well as its geometry  it does the former by first
calculating a vector between the viewpoint and the point
clouds centroid  it then bins the angle between this vector
and the normals of each point into a histogram  to capture

fig    

example of a textured point cloud from our testing set 

the geometry  it calculates a fast point feature histogram
 fpfh  descriptor using the objects centroid  fpfh pairs
neighboring points and bins each pairs euclidean distance
and angular difference between their normals  this results in
  histograms    for the viewpoint    for each angle in fpfh 
and   more for the distance in fpfh  the final feature vector
contains     values 
while vfh captures both geometry and viewpoint  it does
not utilize the rgb data  shotcolor is a descriptor that
incorporates both rgb texture and geometry  a spherical
support structure is constructed around the point of interest
 keypoint  and divided into    volumes  for each volume 
the angle between the keypoint and each point is binned
into a histogram  a similar procedure is performed for the
texture information  color is converted into a vector  using
the cielab color space  and the angle between the keypoint
and each point is also binned into a histogram  this results
in a feature vector with      values 
vfh is a global descriptor and therefore encodes the
entire  d geometry of a point cloud  on the other hand 
shotcolor is a local descriptor and only describes the
geometry around a single point  in order convert our local
descriptor into a global one for our implementation  the
centroid of each point cloud was found and the maximum
distance between the centroid and each point was calculated 
we then use the centroid as a single keypoint and set the
radius of the descriptor to the be the maximum distance 
both descriptors are invariant to scale and shotcolor is
also invariant to rotation 
we expect shotcolor to perform more accurately in
situations where object geometry is similar but color is not 
for example  apples and oranges are both spherical  but the
former tends to be red  and occasionally green  while the
latter is always orange  despite the lack of color  vfhs
inclusion of viewpoints might be advantageous when an
object has unique geometry when viewed from a single angle 
in addition  our method of converting a local descriptor to a
global one for shotcolor is not standard and may reduce
the effectiveness of the descriptor 

fiiv  m ethods
a  svm
to train our machine learning algorithm  we decided to
use a multi class svm  a binary svm classifier works by
establishing a separating hyperplane  or decision boundary 
within the n dimensional representation of n features  this
hyperplane is located such that it creates a constant margin
between positive and negative training examples  this hyperplane can then be recreated for prediction purposes by only
considering those training examples that exist exactly along
the margin for the hyperplane  these training examples are
called the support vectors  the optimal margin classifier can
be solved via the following equation 
min

 w b

s t 

 
  w   
 
y  i   wt x i    b      i           m

this constrained optimization problem can be solved by
utilizing the lagrangian dual problem where the primal
consists of the form 
min p  w    min max l w     
w

w

  i  

the dual can be similarly formed as 
max d       

  i  

max min l w     

  i  

w

by finding w      and   that satisfy the karush kuhntucker  kkt  conditions  we find the solution to this dual
optimization problem and the support vectors are found to be
any training point in which the kkt condition of gi  w     
is actively constrained 
additional complexity can be introduced by utilizing
kernels that map features into a higher dimensional feature
space  this allows much more flexibility in the fitting of the
features  especially when the dimensionality of the feature
set is low or irregular enough that a linear classifier cannot
capture the complexity present in the training data 
to expand the binary classification to a multi classification
system  two approaches can be used  in the first  called
one vs all  each class is compared to all the other classes 
resulting in n classifiers for n classes  during prediction  the
classifier with the greatest margin is determined to be the correct class  the second method  called one vs one  the classes
total classifiers for
are divided into pairs resulting in n n  
 
n classes  this method can be much more computationally
intensive  but tends to give better performance 
after obtaining descriptors of the training set  we implemented a multi class svm network using libsvm  which
uses one vs one and supports a variety of kernels      kernels considered were the radix  rbf  kernel  a polynomial
kernel  the sigmoid kernel  and a basic linear classification 
of the various kernels  the rbf kernel was determined to be
the best to start with since it performs very similarly to the
sigmoid kernel which can be invalid under certain paramaters
and has fewer hyperparameters than a polynomial kernel     

however  when investigating the usage of rbf kernels  the
support vectors would take hours to compute  furthermore 
with a training set of around        data points of dimension
      we would obtain over      support vectors  the high
number of support vectors and poor performance on cross
validation data suggested that we were overfitting the data 
to fix this problem  we switched to a linear classifier using
liblinear  which uses one vs all  and the performance
on cross validation data improved significantly      
b  cross validation
because of the way our dataset was collected  there is a lot
of structure and correlation between the various datapoints 
for this reason  naively setting aside the last     of our data
or a random     of our data for cross validation would not
accurately represent the quality of our classifier  because our
data was comprised of a small number of objects with many
views of each object  we implemented a version of k fold
holdout cross validation by holding out all views of each
item  this way we can be assured that the cross validation
data is independent of the training data 
looking at the visual of class predictions for the   different
descriptors we used in figure   and of the confusion matrices
in figures   and   for shotcolor and vfh respectively 
we confirmed our initial hypothesis that the additional color
data captured by the shotcolor descriptor would give us
better prediction results than that using the vfh descriptors 

fig     visualization of vfh and shotcolor classification  each row
is a point of training data and the color represents the class in which that
training point was classified as in the holdout cross validation

v  l ab t ests
a  testing model
after training an svm model on the training set  we
proceeded to validate the quality of the model using our
own test data captured in the lab  unfortunately  we could
not obtain all the objects in our original training set and
our camera was unable to pick up objects with too many
transparent or shiny surfaces like the water bottle and soda

fito differentiate them correctly with about a     to    
accuracy  this difference can be improved if the categories
were more finely split among colors  with green apples and
red apples all in the training set under the same category 
color doesnt help separate apples and oranges as well as if
red apples and green apples were categorized as separate
classes  unfortunately doing this would give poor results
because there are too few independent red and green apple
data points in the training set 

fig     visualization of confusion matrix for cross validation when using
shotcolor

fig    

fig    
vfh

visualization of classification of test data

visualization of confusion matrix for cross validation when using

can  for this reason  the model we used for our test set was
smaller than that used to select the linear kernel for the svm 
classifying   objects instead of   
b  results
testing on our own data  we indeed showed that the
shotcolor descriptor for training resulted in more accurate predictions compared to vfh  a visualization of the
test predictions corresponding to each of these descriptors
is shown in figure    figures   and   show the confusion
matrices for categorizing our test data  overall experimental
accuracy was     for vfh and     for shotcolor 
while the difference in accuracy between the two is not
as drastic as the difference in the cross validation      to
    in experimental tests versus     to     in crossvalidation tests   it can be seen that shotcolor does
a much better job categorizing globular fruit than vfh 
in the testing set  vfh simply categorized nearly all the
globular fruit as oranges  whereas shotcolor was able

fig    
visualization of confusion matrix for test data when using
shotcolor

vi  f uture w ork
in order to accurately identify a larger amount of household items  a much larger training set must be used  this can
be done by generating additional data in the same way that
the training set currently used was generated  another option
is to investigate the usage of widely available  d models of
objects  to use standard  d models  they would first need
to be converted to  d point clouds before they could be

fifig    

visualization of confusion matrix for test data when using vfh

converted to the appropriate feature set with  d descriptors 
one important difference in this process would be that a
single model would represent a     degree view of the object
and thousands of training examples per object for each view
would no longer be required  this precludes utilizing view
based descriptors such as vfh and may introduce additional
complexities that need to be investigated  if successful  this
would allow a training set to be compiled using freely
available databases of  d models 
another important work to be addressed in the future is
recognition within a scene  currently  due to background
subtraction  only the object introduced is considered for
recognition  in a real time system  calibration for background
subtraction of each individual object in the room is not
feasible  additional algorithms must be introduced that will
allow recognition of objects within the entire scene in order
for true real time operation to be possible 
vii  c onclusion
this investigation sought to implement a machine learning
algorithm to accurately recognize household objects for the
purpose of assisting the visually impaired using an ar
system  a training set of  d point clouds for household items
was converted to appropriate features using  d descriptors
in pcl  both vfh and shotcolor  d descriptors were
tested for comparison with and without considering color  a
model based on a linear svm multi classifier was trained 
cross validated  and tested against data collected in our own
experimental setup for each item  using vfh  which does
not consider color  cross validation results were     and experimental results were     accuracy  using shotcolor 
which takes advantage of rgb data  cross validation results
were     and experimental results were     accuracy 
r eferences
    alexandre  lus a   d descriptors for object and category recognition 
a comparative evaluation  workshop on color depth camera fusion
in robotics at the ieee rsj international conference on intelligent
robots and systems  iros   vilamoura  portugal  vol     no          

    lai  kevin  et al  a large scale hierarchical multi view rgb d object
dataset  robotics and automation  icra        ieee international
conference on  ieee       
    bo  liefeng  xiaofeng ren  and dieter fox  depth kernel descriptors
for object recognition  intelligent robots and systems  iros       
ieee rsj international conference on  ieee       
    bo  liefeng  xiaofeng ren  and dieter fox  unsupervised feature
learning for rgb d based object recognition  experimental robotics 
springer international publishing       
    rusu  radu bogdan  et al  fast  d recognition and pose using the
viewpoint feature histogram  intelligent robots and systems  iros  
     ieee rsj international conference on  ieee       
    tombari  federico  samuele salti  and luigi di stefano  a combined
texture shape descriptor for enhanced  d feature matching  image
processing  icip          th ieee international conference on  ieee 
     
    rusu  radu bogdan  and steve cousins   d is here  point cloud
library  pcl   robotics and automation  icra        ieee international conference on  ieee       
    chang  chih chung  and chih jen lin  libsvm  a library for
support vector machines  acm transactions on intelligent systems
and technology  tist     no          
    hsu  chih wei  chih chung chang  and chih jen lin  a practical
guide to support vector classification       
     fan  rong en  kai wei chang  cho jui hsieh  xiang rui wang  and
chih jen lin  liblinear  a library for large linear classification 
the journal of machine learning research          

fi