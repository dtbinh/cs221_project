machine learning classifier for preoperative diagnosis of benign
thyroid nodules
blanca villanueva  joshua yoon
  villanue  jyyoon    stanford edu

abstract
diagnosis of a thyroid nodule is the most common endocrine problem in the united states  approximately        of thyroid
nodules evaluated by via the most common method  fine needle aspiration biopsies  fnabs   are indeterminate  individuals with
cytologically indeterminate thyroid nodules are often referred for diagnostic surgery  these surgical consultations can be costly 
dangerous  and in most cases leave patients requiring levothyroxine replacement therapy for life  most of these indeterminate
nodules prove to be benign  this project aims to provide individuals a means to avoid surgery by building upon existing machine
learning techniques for preoperative diagnosis of thyroid nodules  in order to be successful our classifier should yield results
comparable to the current benchmarks of     sensitivity      negative predictive value  our best result was achieved using a
combination of random forests and neural networks  which yielded     accuracy      sensitivity  and     negative predictive
value for a held out test set  this result was achieved using a feature set an order of magnitude smaller than the original feature
space 

i

set itself consists of     fnab specimens     benign 
   malignant  and     indeterminate   each of these
biopsies contains gene expression data on     genes 
each of these gene expression data is represented as a
numerical value in the data set  we imported this data
from the national center for biotechnology information  ncbi  gene expression omnibus  geo  website
    individual cel files for each sample were unzipped
from a tar file and were then extracted  reformatted 
and then saved in a convenient matrix format using the
getgeodata matlab function  we represented each
sample via a feature vector with its length equal to the
number of genes that were examined with every element corresponding to its respective gene expression
value 

introduction

diagnosis of a thyroid nodule is the most common endocrine problem in the united states    up to    of
adult females and    of adult males have thyroid nodules detectable by physical examination  and approximately     of adult women have nodules detectable
by ultrasound  although thyroid nodules are common
and usually benign  they prove to be cancerous in      
of cases  a fine needle aspiration biopsy  fnabs  is the
diagnostic tool of choice for thyroid nodule evaluation
as the technique has shown to be safe and effective at
producing accurate results  however         of these
biopsies yield an indeterminate result  patients with
indeterminate fnab reults are usually referred for diagnostic surgery  most individuals with thyroid nodules
larger than  cm in diameter are referred for surgical
consultation  these individuals are exposed to a      
risk of serious surgical complications  and many individuals who undergo the procedure will require lifelong
levothyroxine replacement therapy thereafter        
of thyroid cancers bear at least one known genetic mutation  and several classifiers based on gene expression
data have shown promise  see references   thus  this
study aims to reduce the number of individuals who
undergo diagnostic surgery due to indeterminate fnab
results by implementing a machine learning classifier
for preoperative diagnosis of benign thyroid nodules 

ii

iii

methods and related work

several classification algorithms  both parametric and
non parametric were tested on the dataset  naive bayes
 which has been shown to outperform logistic regression on smaller datasets   ensemble methods  random
forests  boosting  linear kernel svm  and neural networks  each of these methods was trained on the same
random subset of     samples  out of the original      
veracyte  south san francisco  ca  g c k   d c   j d   l f   p s w  
j i w   r b l    the departments of pathology  z w b   v a l   and
medicine  s j m    perelman school of medicine  university of pennsylvania  philadelphia  the department of medicine  ohio state university college of medicine  columbus  r t k    the department of
pathology  university of washington school of medicine  seattle
 s s r    centro diagnostico italiano  milan  j r    the department
of surgery  university of cincinnati college of medicine  cincinnati
 d l s    the department of surgery  johns hopkins university school
of medicine  baltimore  m a z    and the department of medicine 
university of colorado school of medicine  aurora  b r h   
  http   www ncbi nlm nih gov geo query acc cgi 
acc gse     

data

the data were collected from a study conducted by scientists from multiple centres over    months     the data
  ferry 

robert  jr   thyroid nodule   medicinenet  n p   n d 
of medicine  e k a   and pathology  e s c   
brigham and womens hospital and harvard medical school  boston 
  departments

 

fiwe can then apply our model to our testing sample
to classify each as benign or malignant by choosing the
prior condition that gives the higher probability 

iii ii

figure    overview of techniques used
each was then evaluated on classification accuracy on a
held out test set of    samples  out of the original     
    these methods were chosen based on literature reviews  see references  which indicated that they would
perform well on our dataset given the following factors 
    large feature space relative to number of observations      previous literature using these methods and
classifiers applied to gene expression data  we build
on the existing literature through our feature selection
methods  and through the methods used for tuning
our final models  in particular  we hope to emulate the
results of the alexander et  al  study of benign thyroid
nodules with indeterminate cytology 

the random forest classifier creates many decision
trees from bootstrap replicates of the original dataset 
in the case of rf  when building each decision tree for
its respective bootstrap sample  each time a split in
the tree is considered  a random subset of predictors
is used to create said split  in this case  a subset of

p predictors     features out of      was used from
the original feature space  this technique effectively
decorrelates each of the constructed decision trees to
reduce overfitting  a tuning parameter b determines
the number of trees to construct  after b trees are
constructed  the trees are averaged in order to create
the final model 
 
f  x    
b

baseline model  naive bayes

our baseline model is a naive bayes classifier implemented in matlab trained via splitting up the data
set into training and test sets and using the entire set
of     genes  as per the preceding literature  and their
gene expression values as our feature set 
for this particular model  in order to tame the number of parameters  we make a strong assumption where
all features are conditionally independent given the
response for that sample  malignant or benign   we
experiment with different training set sizes         samples  where we calculate all parameters relevant for our
model  incorporating laplace smoothing in the process
 i e   add   to the numerator and the number of genes
to the denominator for each parameter   by using a
multinomial model where the overall probability of a
message is given by 
   

p   y    p   xi   y  
i   

  one

sample was corrupted and removed from the dataset

b



fb   x  

b   

because each tree is created out of a random sampling
of the training data  the random forest is robust to
overfitting despite very large values for b  thus  in
practice we use a sufficiently large b for the error rate to
have settled  for a given test observation  the predicted
value is chosen based on the most commonly occurring
class among the b predictions 

iii iii
iii i

ensemble methods i  random forests

ensemble methods ii  boosting

boosting is also an ensemble method for classification 
it differs from the rf method in that each tree in the
constructed forest is not independent from the last  and
each tree is fit on a modified version on the original
dataset  versus a bootstrap sample   each boosted tree
is fit to the residuals from the model rather than the
response  this tree is then added into the fitted function
to update the residuals  boosted trees tend to be quite
small  depths of   or   are typically used in practice  
by fitting these small trees to the residuals  boosting
improves the fit in areas where the previous iteration of
the model did not perform well  boosting involves three
tuning parameters  number of trees b  although the
boosted trees are not independent as in rf  overfitting
tends to occur slowly if at all  such that we can also
choose large values for b   shrinkage parameter  which
controls how quickly the boosted model learns  and
interaction depth d 
   set f  x        ri   yi for all i in the training set 
   for b            b repeat     
 

fi   fit a tree fb with d splits  d     terminal nodes  to
the training data   x  r   
   update f by adding a shrunken version of the new
tree f  x    f  x      fb   x   
   update the residuals ri  ri   fb   xi  
   output boosted model 
f  x    

b

  fb  x 

b   

we tuned these parameters using    fold cross validation on our test set via rs caret package 

iii iv

linear kernel svm

the linear kernel svm is a parametric method constructed by identifying observations to define the classification boundary  these observations are called support
vectors   several svm models were implemented using
different kernels  although the linear kernel performed
best  the cost parameter c was chosen using    fold
cross validation 

iii v

neural network

for this algorithm  one  neuron  takes an input vector of
features and is fed through a weight vector with some
bias  usually one neuron is not enough to produce an
accurate model of our training set  so we have a layer
of neurons wherein every single feature for a different
sample is fed into multiple neurons  this results in our
argument equal to the weight matrix w multiplied by
the input feature vector x  a bias vector b is then added
on  within the hidden layer of neurons  a sigmoid
function takes the result from the previous computation
and computes a value that is between   and    for
model selection  we measure results using classification
error on the held out test set  as well as cross entropy 
c 

 
n

 y ln a       y  ln    a 
x

weights and bias values are updated with each simulation until the cross entropy of the overall model
reaches a very small value ideally as close to   as possible 
the number of neurons in the hidden layer and the
regluarization parameter  were chosen using    fold
cross validation on the training set using rs caret package 
neural networks usually apply sigmoid transfer functions in the hidden layers  sigmoid functions are useful
for differentiating inputs especially when they are either
very large or small since these are the regions when the

figure    tuning rf nn hidden layer using    fold cv
slope approaches zero  however  this characteristic is
problematic when using gradient descent to train a multilayer network with sigmoid functions since they may
not produce large changes in the weights and biases
when attempting to find their respective optimal values 
in order to circumvent this problem  back propagation
training algorithms are used where only the sign of
the derivative is used to determine the direction of the
weight update  i e   if the weight continues to change in
the same direction  e g  negative  for several iterations 
the magnitude of the weight change will be increased
 rumelhart  

iii vi

feature selection using random forests

we were able to reduce the feature space by an order of
magnitude via variable selection using the rf variable
importance plots  see fig      for the best boosting
model  we were able to reduce the feature space by two
orders of magnitude  several studies  see references 
show that rf serves as an effective feature selection
method for both svms and neural networks  variable
importance is calculated based on classification error
rate on the held out test set 
e      max  pmk  
k

and gini index 
k

g 

 pmk     pmk  

k   

which is a measure of total variance across the k
classes   note that mathematically  the gini index and
cross entropy metrics are quite similar  
backward stepwise selection  bss  using the top   
variables based on the rf variable importance measures
 

fiwere used to construct the final svm and nn models 
variable selection using bss also significantly improved
the performance of both the rf and boosting models 
this indicates a large amount of noise in the data set 
we opted for feature selection instead of dimensionality reduction techniques such as pca in order to find
specific genes that were correlated with the response 
reducing the feature space in this manner reduces overhead and computational complexity  in addition  identifying particular genes correlated with thyroid cancer
could prove useful for further studies 

iv

results and discussion

out of the models implemented  rf nn proved to
be the most effective with an accuracy of     on the
held out test set 
method
nb
rf
boosting
bss rf
bss boosting
svm
nn
rf svm
rf nn

acc 
    
    
    
    
    
    
    
    
    

sn 
    
    
    
    
    
    
    
    
    

spc 
    
    
    
    
    
    
    
    
    

ppv
    
    
    
    
    
    
    
    
    

npv
    
    
    
    
    
    
    
    
    

figure    results showing accuracy  sensitivity  specificity 
positive predictive value  and negative predictive value on the
test set for each method used

figure    most significant variables according to rf

figure    linear kernel svm on test data using v    and
v   as predictors  as selected by rf

that variable selection improved each model indicates that the raw models were heavily overfitting the
test data  in particular  the small sample size and large
feature set size  relative to sample size  made it difficult
to train our models without incurring variance vis a vis
the test set 
for all algorithms tested  classification error on the
held out test set was minimized using     predictors 
these correspond to    genes from the original dataset
provided by the ncbi  using a smaller set of predictors
reduces variance of the model and increases model
interpretability  from a clinical standpoint  there is a
real world cost difference incurred by collecting a larger
number of gene expression data  both in terms of time
and money   we deem this a significant result that could
be useful in further studies exploring the correlation
between specific gene expressions and the presence of
thyroid cancer 
the roc curve for rf nn  fig     shows true positive rate versus false positive rate  or equivalently  sensitivity versus   specificity  for different thresholds of the
classifier output  this curve indicates that overall our
model is able to accurately classify most of our samples 
however produced false positives for samples that were
classified as malignant when in fact they were benign 
the neural network model calculates the crossentropy of each set of data for every run  fig      here
it stops running on the twelfth run as the validation set
cross entropy reaches a minimum value at that point 
all subsequent runs check to validate this result 
from the results  our neural network model produced the most accurate results out of all classifiers
tested  see fig     
 

fiavoid any learning slowdown  the rate at which both
the weights and biases learn is controlled ultimately by
the error in the output  which seems like an appropriate way to determine the best model for our data set 
    neural networks take correlations between features
into account when producing its model  by having the
neural network make use of aijfeature learning ai
the model we are able to produce becomes much more
versatile  of course  we can see that we are still producing mismatches  part of this may be explained by the
fact that the data can be noisy and taking into account
the original feature set size of     

v

conclusion

figure    roc curve of rf nn on the held out test set

figure    training rf   nn

the dataset collected from the ncbi consisted of mostly
indeterminate results      out of     samples   our
sample size was also smaller than that used in the referenced literature  alexander et  al    despite these
constraints  the performance of our best performing
model has an overall classification accuracy comparable
to published results  this further validates the use of
rf as a feature selection method to complement svm
and neural networks for applications in analysing gene
expression and cancer data  as well as machine learning
classifiers in analysing biomedical data in general  further we we able to produce these comparable results
using a feature set an order of magnitude smaller than
that of the original dataset  this translates to identifying specific genes that are correlated with predicting
thyroid cancer  and in particular  could aid further studies in validating this result by reducing cost of collection
of these gene expression data  as well as reducing computational complexity in producing results 

vi

acknowledgements

the authors would like to thank professor olivier
gevaert for his guidance and for providing the original dataset  as well as professor andrew ng and the
cs     teaching team for their guidance 

vii

figure    confusion matrix for rf   nn on held out test set

we believe that this may be due to two factors     
using the cross entropy expression  we were able to

references

   alexander ek  kennedy gc  baloch zw  cibas es  chudova d diggans j  et al  preoperative diagnosis of benign
thyroid nodules with indeterminate cytology  n engl j
med                    
   ghanem  muhammad  yair levy  and haggi mazeh 
 preoperative diagnosis of benign thyroid nodules with
intermediate cytology   gland surgery                   
pmc 

 

fi   chen  yi wei  and chih jen lin   combining svms
with various feature selection strategies   feature
extraction  foundations and applications  studies
in fuzziness and soft computing                 
web   https   www csie ntu edu tw  cjlin papers  features pdf  
   li  wenjuan and meng  yuxin   improving the performance of neural networks with random forest in detecting network intrusions   proceedings of the   th
international conference on advances in neural networks   volume part ii        springer verlag  dalian 
china 
   rumelhart 
d e  
hinton 
g e  
williams 
r j 
learning
representations
by
backpropagating errors 
nature     
       
       
 http   www nature com nature journal 
v    n     pdf       a  pdf  

 

fi