 

predicting win percentage and
winning features of nba teams
cs    final project report
evan giarta  nattapoom asavareongchai

i  i ntroduction

t

he nba is the premier basketball league of the world 
it is an enormous business  with an estimated revenue of
nearly    billion in       teams within the nba with more
wins will gain popularity and increase income  beneficial to
the league and its players  thus  predicting the win percentage
of an nba team based on the previous performance their
roster is quite valuable to general managers  coaches  players 
fans  gamblers  and statisticians alike  moreover  knowing
which particular stat or feature is most influential to winning
games is also desirable  our project aims to do exactly that 
we built a learning algorithm that takes in a data set of
team features  pre processed from previous statistics of each
player in the current seasons team roster  and outputs an
estimate of the win percentage of that team during the regular
season  the input of our algorithm is a team feature vector
of the    different nba for several seasons  each team in
each season is considered one input data point  each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating 
the stats of the players in the team are however taken from
one season prior  for example  if we want to compute the
feature vector for the golden state warriors during the
          season  we will look at all the players in the
          gsw team  then find the statistics of each of these
players during the           season and pre processed them
to create the required feature vector  the win percentage
output would be the win percentage of the current  or the
          win percentage of the gsw in our example 
the learning algorithm model that we use is a second
order polynomial fit regression model  unlike linear regression
where the feature vector is used as it is  all the elements of our
input feature vector will first be square and both the original
value and the squared value are used as the final input features
within the input vector  for example  we have    input features
in our vector  without squaring   before inputting this into
our learning model  we will square the features to create a
   feature vector  this vector includes the original    features
plus the squared of each of the    features 
input f eature vector x i     x    x         x     x     x          x     t
output w in p ercentage   y  i 
for i           n with n   dataset size

ii  r elated w orks
there have been many previous research papers on
predicting nba team wins  however  most of them tries
to predict wins of each individual nba games  then sum
them up to then create the win percentage of a season  two
papers  one by bernard  l et al            and the other by
pedro et al            used network based algorithm to predict
nba wins  because mere statistical features do not produce
as well of a result  as can be seen in our model results  a
network based feature model can definitely improve learning
algorithms  more specifically  bernard  l et al  used neural
networks with feature selection to predict nba games  in our
opinion  this is a very good model to use  since it takes in
many sophisticated nature of many features  ones we werent
able to incorporate into our model  
another interesting model done by na wei          
utilizes naive bayes to predict playoff records from regular
season stats  even though this is a different problem  the
nature of the problem is similar  the use of naive bayes
however is not a very good approach  since the model relies
on a lot of assumptions  the model is not as accurate 
online  statistical resources for sports fans span an enormous spectrum  basketball aficionados of all kinds and skills
can enjoy interacting with the buckets visualized shot chart
from bball breakdown    as well as studying the complex
decision making process of markov state machines on big
league insights    as applied to the game  using data from
thousands of basketball plays  the model analyzes and predicts
how multiple in game states and transitions influence each
other and the overall outcome of a game 

iii  dataset and f eatures
a  data sources
we collected our data from two main sources  the
first main source where we collected most player statistics during each seasons  as well as team statistics  team
roster data of each seasons and team win percentage
of each seasons   is the main nba statistics website
http   stats nba com league player   the second source that we
obtained our feature stats from  especially the team salaries
stats is https   www eskimo com  pbender  

fi 

b  data set pre processing
as described in the introduction  our input data is the set
of team statistics for each season taken from player roster
statistics from the season before  the raw data we obtained
are therefore every player in the league during the          
season to the           season  we then pre processed this
to obtain    team feature statistics for the year           to
          
these features include 
   age
   games played  gp 
   wins  w 
   minutes  min 
   field goals made  fgm 
   field goals attempted  fga 
   field goal    fg  
     points made   pm 
     points attempted   pa 
      points     p  
    free throws made  ftm 
    free throws attempted  fta 
    free throws    ft  
    offensive rebounds  oreb 
    defensive rebounds  dreb 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

rebounds  reb 
assists  ast 
turnovers  tov 
steals  stl 
blocks  blk 
personal fouls  pf 
double double  dd  
triple doubles  tp  
points  pts 
efficiency      
  of rookies
  of allstars
  of rebounders
  of double doubles
  of triple doubles

the number of allstars is defined as the number of players
with points per game of   standard deviation above the mean
of a season  similarly  number of rebounders is the number
of players with rebounds higher than   standard deviation 
likewise with number of double doubles and triple doubles 
the  st feature up to the   th feature are obtained from
averaging the stats of each player in the team per season
together  the last   features are obtained from counting the
players in the team per season with the feature requirements 
we then added the last or   st feature  called team salaries 
using the teams available salaries during the current season  so
from           to           seasons   all of the    features
are then normalized per season by whitening  with a total of
   seasons and    teams  we have a total of     data points
in our data set 

c  extra data manipulation
we tried to run pca or principle component analysis to
remove any dependent features  a new feature vector  with
reduced dimension size  is created  the size of the new feature
was determined by how many dominant
of the
pm eigenvalues
 
 i   i t
x
x
there
is
empirical co variance matrix    m
i  
 where m   the number of train samples or data points and
x i  is the input feature vector of data point i  
suppose there are k dominant eigenvalues  k        then

the new input feature will be 
 t  i  
u  x
ut  x i  


z  i       
    
utk x i 
where uj are the eigen vector corresponding to the j th
eigenvalue of 
however  when the learning model is run using the reduced
feature vector from pca versus the original feature vector 
there was no improvements  the generalized error even went
up  and so we decided not to use the reduced vector from pca 
instead  we decided to reduce the feature vector using feature
selection  more specifically backward search  this method
proves to give us a better result  we will describe how we
did feature selection in more detail in the methods section 
iv  m ethod
the learning algorithm that we chose to use for our
learning model is the second order polynomial fit regression
model  however  before we decided on this model  we ran
our data through three different regression models  linear
regression  locally weighted linear regression and polynomial
fit  with different orders   we validated each method model
using k fold validation and chose the model that gave the
best generalization error  lowest rms error of the test set  
for each regression model  we measured their performances
by measuring the cost function j   of both the training set
and the test set  the cost function we used is defined as
follows 
v
um
ux
j     t  h  x i     y  i    
i  

for locally weighted regression  we varied the parameter 
between       and        for polynomial fit  we ran our
data through second order and third order polynomial fits 
our data set includes     data points     teams during   
seasons  we decided to validate our learning algorithm by
separating this data set into the training set and the test set 
we separated     data samples as the training set and the
other    as the test set  for simplicity we used one season
as the    data test set and the other seasons as data to train 
because of the nature of how we separated the data and
because there are    seasons  we decided to use    fold
validation  therefore  each season becomes a test set once 
the pseudo code for this    fold validation then becomes 
   split the data into    disjoint sets  s    s         s     where
each set contains data from each of the    seasons 
   for j             
train the model m on s        sj   sj         s  
 train the model on every set except set sj   to obtain
the hypothesis hj   then test the hypothesis on sj to get
the generalization error j  

fi 

   the generalization error  for model m is then the
average of all the j for j              
   fold validation is done with every regression model for
comparison 
a  regression model descriptions
in this section  we will briefly describe how each regression
model  linear regression  locally weighted linear regression 
and polynomial fit regression  works 
we will start with linear regression  in linear regression 
we are using the data to create a linear model in the form
y   h x    t x where          x          x   with
xp
 r     the model aims to minimize the objective function
m
 
 i 
 i   
i    h x    y    
 
to minimize the object function  we used the normal
equation defined by 

    x t x   x t 
y

 x     t

 x     t

x 
  

 







 x      t
     
y
 y     




y     
    
y

    

next for polynomial fit with different orders  we ran
our data on second order and third order  for each
polynomial fit regression  we are trying to minimize
the same objective function as the case for linear
regression  however  the only difference is in the
hypothesis h x   for second order polynomial fit  we have
h x    t x         x             x        x              x     
for this case    r   and x  r     for the case of a third
 
polynomial fit  we have h x    t x         x         
   x        x              x         x              x      for this
 
case    r   and x  r    
lastly  for locally weighted linear regression  we try to
fit
our model to minimize the objective function 
p   i to  i 
w
 y
 t x i       to do this we also used a modified
i
normal equation in the form 

we ran the following algorithm 
   initialize f                with our    features 
   repeat  
 a  for i               delete feature i from the set f and
call it the set fi   then use cross validation to evaluate
the generalization error of the new fi  
 b  set f to be the set fi in  a  with the least
generalization error 
 
   end the repeat loop when the generalization error does
not improve any longer or if it starts to increase again
with fewer features 
v  r esults   d iscussions
we will split our results discussion into three parts  the first
part is the results obtained while choosing the best regression
learning algorithm between the ones we have mentioned  the
second part will be the results obtained from trying to improve
the model using different techniques like increasing the data
set size and feature selection  also pca even though it did not
improve the model   the third part will be the end result or
the final model we achieved and how we utilized our model
on the current           nba season 
a  different regression model results
as mentioned  we first learn our data on different regression
models  we measured the cost function of each model using
k fold validation and decided on theq
model with the least rms
pm
 i 
 i   
generalization error or j    
i    h  x    y    
initially  we were working with data from   seasons instead
of    seasons  from           to            with this
dataset we validated each model using   fold validation 
we first ran our dataset with linear regression  we obtained
a training rms error of      and a test rms error  or
generalization error  of around     the result gives a good
variance but a very bad bias problem  we therefore decided
to fit our data using different polynomial fits  second order
fit gave us better rms training data error of      and third
order gave us an even better rms training data error of
    however  both second order and third order fits produced
worse rms test error of      and      respectively  therefore 
we improved our bias problem with the higher order fit but
created a high variance problem  an over fitting problem  


    x t w x   x t w 
y
 
with wii   w i    wij     for i    j
 
b  learning model improvements
after we decided on second order polynomial fit regression 
we worked on improving our learning model by trying to
reduce our variance using feature selection  the type of
feature selection we chose was backward search  to do this

we tackled our high order fits variance problem by increasing the dataset to    seasons  this reduced the rms test
error of all the polynomial fit orders  including first order or
linear regression  down  the resulting rms test error of linear
regression  second order fit  and third order fit are            
and      respectively  although the rms test error decreased 
the rms training error increased for all three models with
errors of             and      respectively  even if the training
error increased  the test error decreased which means that the

fi 

generalization error improved and so increasing the dataset
did make a good improvement  a plot showing this result is
shown below 

the different order polynomial fits to decide which model is
best  we varied the parameter  from    to    and measured
the rms train and test error  the plot is shown below 

fig     plot showing the rms test and train error with using    season
datasets of different order polynomial fits

fig     plot showing the rms test and train error of locally weighted linear
regression as  is varied

we then varied the dataset size from   seasons to    seasons
for second order and first order polynomial fit and plotted a
graph to see the difference  the graph is shown below 

the rms test error or generalization error did not fall below
     with the  varied and so the model with the best rms test
error  the lowest error  is therefore second order polynomial
fit 
b  model improvements   feature selection
with second order polynomial fit model  we have a rms
training error of      and rms test error of       we have
already mentioned how we improved our test error and
variance problem by increasing the dataset size to    nba
seasons from   nba seasons  or from     data samples to
    data samples  
we further improved our generalization error by doing
feature selection using backward search  we managed to
reduce our feature vector of    features down to    features 
this means that our input vector would now have a size of
   instead of    for second order polynomial fit  the size of
the feature vector reduced by more than half  the    features
that were reduced are shown below in order of removal 

fig     plot showing the rms test and train error while changing the dataset
from   nba seasons to    nba seasons

with    nba seasons of our dataset  we then ran locally
weighted linear regression to compare our rms test data with

  
  
  
  
  
  
  
  
  

num rookies
pf
team salaries
tov
dreb
fga
fg 
ft 
fta

   
   
   
   
   
   
   
   

 p 
pts
min
ftm
age
reb
oreb
blk

fi 

has the largest corresponding  weight of         the
next most influential feature is the number of players with
double doubles of   standard deviation more than the mean in
a season  the corresponding  weight of this feature is        
with our model  we predicted the win percentage of current
nba regular season teams as follows 

fig     plot showing the rms test and train error changes as features are
removed

with the features removed  we managed to reduce the rms
test error or generalization error down to        with a training
error of       
fig     plot showing our win percentage predictions of nba teams during
the           season

c  end results
our resulting hypothesis h x    t x with x    r     with
all our improvements computed  has  corresponding to the
first order and second order weights of the feature as follows 
features used
 x  to x    
gp
w
fgm
 pm
 pa
ast
stl
dd 
td 
 allstars
rebounders
num double
doubles
num triple
doubles

first
order
weights   
to     
     
     
      
     
      
     
     
      
      
      
      
     
      

second order
weights    
to     
      
      
     
     
      
      
     
     
     
     
      
      
      

     

      

as seen in the table of the values of  obtained from
our model  the feature that is most influential on win
percentage of nba teams in a season is the number of
all stars  this feature is the number of players with a point
per game of   standard deviation above the average  it

the season is still ongoing and thus the accuracy and errors
cannot be determined yet 
vi  c onclusion   f uture w orks
with our chosen learning algorithm model and tools to
improve our model  such as feature selection  we got our
generalization rms error down to        this is a big
improvement from an error of more than    in the beginning 
the second order polynomial fit is the best model we found
because linear regression seems to under fit the data  whereas
higher order polynomial fits  i e  third order or higher  over
fits the data 
with more time and computational resources  we believe
that the error could be reduced further  to do this we would
need to gather more detailed data such as different plays run
or individual team head to head games and schedule to create
more relevant and detailed features  some examples of these
features are coach stats  different plays run by team coaches
and their frequencies  etc  these data are hard to get and hard
to pre process before being able to add to the model  also
with better knowledge on other learning algorithms not taught
in class  we believe that using a neural network algorithm
approach would give a better result overall 

fi 

vii  r eferences
    loeffelholz  bernard  earl bednar  and kenneth w 
bauer  predicting nba games using neural networks  journal
of quantitative analysis in sports            
    vaz de melo  pedro os  et al  forecasting in the
nba and other team sports  network effects in action  acm
transactions on knowledge discovery from data  tkdd 
               
    wei  na  predicting the outcome of nba playoffs using
the nave bayes algorithms  university of south florida 
college of engineering        
    beshai  peter  buckets  bball breakdown    dec       
web    dec        http   bballbreakdown com buckets app  
    simulating an nba game  building a markov
model  big league insights     jan        web    
nov         http   bigleagueinsights com simulate nba gamemarkov model  

fi