predicting congressional bill outcomes
over the course of this quarter  we built a predictor that identifies which us bills will succeed in both the house and
in the senate  the predictor is modeled on congressional data from the    th      th congresses  spanning the
years              from a wide range of sources  we evaluated our system based on whether or not we were able to
predict the outcome of a particular bill correctly 

data sources   infrastructure
data from govtrack   thomas gov
we downloaded metadata of all bills from the    th      th congresses from the  unitedstates projects bulk data downloads
page  documentation on the structure of these json files can be found on this page in the groups wiki 

maplights congressional bills data
we relied most heavily upon maplights data about the contributions given to members of congress from interest groups that
support or oppose various bills  we pulled information on each session for each kind of bill with a python script that scraped
the maplight data for the    th      th congressional sessions  where each bill had an outcome  success or fail  from
govtrack associated with it we have made the structured data available here  for bills from previous congresses  i e      years
old   if the status is referred to committee or introduced or reported to committee then its a failure 
bill types  hr  house bill  s  senate bill  hj res  house joint resolution  sj res  senate joint resolution  h con res  house
concurrent resolution  s con res  senate concurrent  h res  house simple resolution  s res  senate simple resolution

approach
we modeled the challenge of classifying bill outcomes as a classic machine learning task  using the data on congressional
bills from the sources listed above  we were able to experiment with several machine learning approaches for the
classification task 
baseline  using a conjecture from a previous study from michigan state on bill advancement  see the section on prior work  
we established a baseline prediction of a bills success using just one feature  the total sum of lobbying money in support for
that bill 
oracle  our ground truth for bill outcomes is taken from our bill status data with govtrack  in order to interpret bill status as a
binary outcome  success or fail   we mapped all possible statuses to their most likely outcomes   see the section on
govtrack data  
main approach  our first step after collecting and cleaning the data was representing it in an appropriate feature form 
because we wanted to include unique lobbyists  legislators  organizations  and industries in our features and their effect on
the bill outcome  we took on a text classification bag of words approach to representing these features  for our particular
data  we associated lobbying amounts  either given or received for  and in support or against a bill  to each feature instead of
binary indicators  feature is present vs  not present   a few of the different numerical representations we considered were
absolute value of money for a bill  ratio of money for and against a bill  and the net sum of the money for a bill  i e  money
given received in favor of the bill   money given received in opposition to the bill   some examples of resulting features are
given below 
if senator grinch received         total in support for bill        world peace and         against  then the feature
senator grinch would map to          
if the sum for representatives in a certain voting district of california was        in support and        against  then the
feature ca   would map to       
if the manufacturing industry contributed a total of         against and    in support  then the feature manufacturing
would map to          
with our aggregated training examples  our resulting data included over        features for just under       bills 

fiour second step was to take our newly formed feature representation and assess a second baseline accuracy from naive or
linear supervised classification algorithms  we used naive bayes and logistic regression respectively to accomplish this  as
they are relatively quick and simple classification algorithms 
our final step was to move to more sophisticated approaches that were more suitable for dealing with high dimensional
feature spaces  similar to methods in classc text classification  we decided to use models like support vector machines
 svms  that are robust but also capable of modeling non linear boundaries  we also experimented with perceptron to see if
this single layer network could more effectively capture relationships betwee our various features  in order to perform both
classifications  however  we had to first reduce our feature space using principal component analysis  pca   in addition to
reducing our feature space from         to       pca also helped us discover that these     features capture the most
variation in the data 

literature review   prior work
statistics from previous congresses   govtrack provided an important overview of the bill success rates of previous
congresses  which you can find here  according to govtracks data  nearly    of bills are considered failed legislation  the
vast majority of bills that dont become law simply die in committee or some other intermediate step 
lobbying   congressional bill advancement   a michigan state university paper titled lobbying   congressional bill
advancement found that the amount of interest group lobbying is associated with majority party sponsorship  wide
cosponsorship  and high profile issues and that lobbying helps predict whether bills advance through committee and each
chamber  independent of congressional factors typically associated with bill advancement 
predicting congressional bill outcomes  in       stanford students took on a very similar question in predicting
congressional bill outcomes where the team aimed to answer the question can we predict the voting behavior of a
representative on a given bill using only the frequency of words in the bills text   and they relied on text classification 
lobbying on effective legislation  revolving door lobbyists targeted legislative influence  a university of texas at
austin paper titled lobbying on effective legislation  revolving door lobbyists targeted legislative influence studied the varying
levels of influence exerted by lobbyists on each step of the legislative process 

error analysis
naive bayes with principal component analysis  pca 
summary
we began by using naive bayes as our initial predictor  which performed poorly in the case of a pca transformed space  the predictor
correctly classified points in the test set just over     of the time  far worse than the     that one would expect with chance  our initial
reaction was one of shock  but after some reflection we realized that the source of the problem was likely the approachs naive
assumption of conditional independence between each of the features  this joint probability model can be expressed as 

its intuitively obvious that many of the features are closely correlated  in particular  we included a wide variety of features representing
many different aspects of the lobbying processes  and as a result one would expect there to be many features that communicate
overlapping information about a given bill 
in comparison to the classification in untransformed space  we can see that naive bayes suffers after pre processing with pca  this can be
explained by the fact that pca extracts features that maximize variation within the data  and is heavily dependent on covariations between
features  naive bayes  on the other hand  applies a fundamentally different assumption with the data when creating its models and could
even derive distortions from the pca selected features 

ficorrectly classified instances

   

         

incorrectly classified instances

    

         

detailed accuracy by class

weighted avg 

precision

recall

f measure

class

     

     

     

success

     

     

     

failure

     

     

     

confusion matrix
a

b

  classified as

   

  

a   success

    

   

b   fail

naive bayes without pca
summary

correctly classified instances

    

         

incorrectly classified instances

   

         

in comparison to the classification in untransformed space  we can see that naive bayes suffers after pre processing with pca  this can be
explained by the fact that pca extracts features that maximize variation within the data  and is heavily dependent on covariations between
features  naive bayes  on the other hand  applies a fundamentally different assumption with the data when creating its models and could
even derive distortions from the pca selected features 

detailed accuracy by class

weighted avg 

precision

recall

f measure

class

     

     

     

success

     

     

     

fail

     

     

     

confusion matrix
a

b

  classified as

  

  

a   success

   

    

b   fail

logistic regression after principal component analysis  pca 

fisummary
logistic regression was quite effective at predicting bill outcomes  coming in second place with an       success rate  the feature space
was too large to perform logistic regression with reasonable time and computation power  so we had to first transform the data with pca
before creating the models 
intuitively  logistic regression in conjunction to pca should perform relatively better than naive bayes  logistic regression is dependent on
covariances and does not make the same probabilistic assumptions about independence as naive bayes  in fact  aguilera et al  proposes
that pca actually enhances logistic regression of high dimensional  multi collinear data  given this  it is no surprise that logistic
regression results in a significant improvement from the baseline performance 

correctly classified instances

    

         

incorrectly classified instances

   

         

detailed accuracy by class

weighted avg 

precision

recall

f measure

class

     

     

     

success

     

     

    

failure

     

     

     

confusion matrix
a

b

  classified as

  

   

a   success

   

    

b   fail

support vector machine after principal component analysis  pca 
summary
support vector machine performed the best out of the four methods we used  correctly predicting the outcome of almost     in    fold
cross validation  this is not surprising  as svm is generally a highly effective algorithm for large feature spaces  robust due to
regularizations  and fast due to kernel use  in addition  svm is capable of finding non linear decision boundaries  which is ideal for features
like ours with complex or dynamic relationships  due to the performance and speed of this algorithm  we would most likely favor svm over
other approaches in this analysis 

correctly classified instances

    

         

incorrectly classified instances

   

        

detailed accuracy by class

weighted avg 

precision

recall

f measure

class

     

     

    

success

     

     

    

failure

     

     

     

ficonfusion matrix
a

b

  classified as

  

   

a   success

 

    

b   fail

voted perceptron after principal component analysis  pca 
summary
voted perceptron achieved an accuracy of     on our    fold cross validations  one explanation for the sub par performance is that this
algorithm is a shallow single layer variation of the far more powerful neural networks  in order to achieve higher accuracy in our
classifications  a better approach would be applying full neural networks instead of the voted perceptron  one disadvantage to this
however is that perceptron was by far the slowest of all our algorithms  and neural nets even slower   which also presents a significant
challenge in computational power and time 
number of perceptrons       

correctly classified instances

    

        

incorrectly classified instances

   

        

detailed accuracy by class

weighted avg 

precision

recall

f measure

class

     

     

     

success

     

     

     

fail

     

    

     

confusion matrix
a

b

  classified as

  

   

a   success

   

    

b   fail

contributions  this project was a joint project between devon zuegel and emma marriott from cs     roles played 
arushi jain   data scraping  cleaning and formatting
devon zuegel   project writeup and static website setup
emma marriott   data formatting and preliminary analysis
all three of us worked on applying different models and algorithms to the data for analysis 

fi