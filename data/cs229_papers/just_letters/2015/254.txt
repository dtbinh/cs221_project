dynamic eye gaze tracking for foveated rendering and retinal blur
kushagr gupta  suleman kazi  terry kong

abstract this project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and hidden markov models 
respectively  for users wearing head mounted displays  hmd  
eye gaze location can enhance the virtual reality experience
inside an hmd  in this report  we discuss how we collected
a dataset for eye gaze tracking and the features we extracted 
then  we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear regression  support vector regression  svr   and neural networks 
we also briefly discuss our approach to the prediction problem 

probabilities of where the user is likely to look at  the user
views to estimate eye gaze instead and is able to achieve an
accuracy of        state of the art eye tracking hardware    
is advertised as being able to achieve a typical accuracy of
accuracy of       in our work  instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one  we also compare
different learning algorithms and quantify their results as
described in the sections below 

i  introduction

iii  dataset and features

the ability to track and predict eye gaze accurately can
have a huge impact in virtual reality  vr  headsets since
it gives content developers the flexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye gaze information  a recent
hmd project with integrated eye tracking     raised well in
excess of its funding goal via kickstarter  foveated rendering
is a technique in which the image resolution is not uniform
across the image  retinal blur refers to blurred perception of
objects outside the center of gaze  in the peripheral vision  
in order to apply these techniques the eye gaze location
must be accurately known  to provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted  this requires
high accuracy so that the scene is rendered and blurred at
the right locations  we use machine learning techniques to
output an estimated eye gaze location in terms of screen x y
coordinates 
this is a joint project for cs    and cs     the work
shown in this report explains the eye gaze tracking portion
of the project which was done for cs     the approach to
predict the eye gaze location constitutes the cs    portion
of the project and is very briefly summarized at the end to
connect the two projects  for a much more in depth analysis
of the prediction approach  we refer the reader to our report
for cs    
ii  related work
a large number of the modern eye tracking algorithms we
came across        use signal image processing in conjunction with geometry to accomplish eye gaze tracking  in fact
    which is a survey of models for eye gaze tracking does
not list any machine learning methods      uses artificial
neural networks for eye gaze tracking and is able to achieve
an accuracy of           also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at images of the users eyes to do so it relies on precomputed saliency maps of the videos  showing the prior

a  dataset acquisition
the dataset consists of images of the users eye looking at
points displayed on a screen with a resolution of     x    
pixels and a size of     x     cm  these points serve as
the ground truth data  the user sits   cm away from the
screen  wearing a pair of glasses with a webcam mounted
on them  cm from the eye and off axis as to not obstruct
the screen  by using this method we did not have to worry
about inadequate illumination inside a vr headset whilst
also getting images similar to what a camera inside a vr
headset would produce  we also looked up online datasets
but they did not fit our desired input data requirements  for
example the dataset in      gives the eye gaze location  x y 
values on the screen but doesnt give images of the eye  the
dataset in      gives images of eye but with camera right in
front of the eye thus obstructing content on screen  our data
collection setup is shown in fig    

fig    

setup for data collection

   as a one time procedure user is asked to look straight
at the camera and a reference image is captured  a box
is manually drawn around the position of the users eye
and the image is cropped to this box and saved 
   proceeding to collect a dataset user must take another
reference image  the master reference obtained in step
  is cross correlated with this reference image to find
the location of the users eye and a box of the same size
as the master reference is drawn around the eye automatically  this step removes slight variations of user
head position between different dataset acquisitions 

fi   the user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen  each captured image is
cropped based on the bounding box calculated in step
  and saved along with the location of the point shown
on screen 
b  feature selection
initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm  different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive  we thus chose a
downsampling factor that performed reasonably well giving
a final feature length of     as compared to the original
image size of        pixels as shown in fig    
we introduced two additional sets of features namely the
inner product with the eigen eyes and the inner product with
the fisher eyes      which help as they increase variance
using principal component analysis  they are explained as
follows 
   eigen eyes   given the vectorized training images the
mean of the entire set  is removed and output is
concatenated into a matrix  say s  of dimension number
of pixels by number of training images  eigen eyes are
essentially the eigen vectors of the scatter matrix ss    
this gives us the directions in which there is higher
variance based on the eigen values with an illustration
in   
   fisher eyes   in eigen eyes we consider the entire
dataset and discriminate based on it  we can improve
this using fisher analysis  the aim here is to maximize
between class scatter while minimizing within class
scatter  for the purpose of eye gaze tracking  we
considered a finite set of classes based on the true
locations of target point where the eyes are looking
at and each image  mean removed  is binned into one
of these classes  the generalized eigen vectors of the
problem are called fisher eyes with an illustration in   
while the initial computation of all the eigen eyes and
fisher eyes may take some time  they will only need
to be computed once at the beginning using all the
training data 

fig    

raw image  downsampled image  eigen eye  fisher eye

iv  methods  tracking approach
to evaluate our tracking approach  we compared three
different learning methods  regularized linear regression 
svr  and neural networks  in all three cases we treat the

x and y dimensions of the eye gaze location independently 
while we could consider the dimensions together  there is
no added value  as the results will show  using least squares
where the labels are multidimensional  x and y  actually
performs worse which justifies our assumption 
a  least squares with regularization
least squares  or linear regression  was used as a baseline
due to its simplicity and straightforward implementation 
least squares assumes that the data can be modeled as
p   xw
where p is the eye gaze location  x or y   x is the design
matrix and w is the weight vector  since least squares can
lead to over fitting  we also considered regularized least
squares  which has an extra term augmented to the least
squares objective  with      it reduces to linear regression
without regularization 
jlsr  w    kp  xwk     kwk  
the hyper parameter  controls how much over fitting we
will tolerate  this modified objective function is equivalent
to assuming a gaussian prior on the weights with zero mean
and non zero variance 
taking gradients yields the closed form solution  
wlsr    x t x   i   x t p
where i is the n by n identity matrix and n is the dimension
of a feature vector 
both of these results assume that x is full rank and skinny 
while this isnt always the case  we can always perform svd
or qr factorization to remove the redundant columns in the
x  another approach is to pass the optimization problem
into a convex solver  which will likely perform newtons
method which doesnt require any priors on the data 
b  support vector regression
another approach is to use svr  the problem inherits
part of its name from the popular support vector machine
problem  like most regression problems  the hypothesis
function is of the form h x    wt x   b where w is the
weight vector and b is the constant offset 
to use this model  one must select a tolerance    and
a hyper parameter for the slack variables  c  and then the
weights are learned to minimize the objective  the physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function 
this problem can be solved more efficiently in the dual
domain and is explained more thoroughly in       the
corresponding dual problem is
 
pm
    i j    i  i   j  j     xi   xj  
maximize
pm
pm
 i    i  i     i   y  i   i  i  
m
x
subject to
 i  i       and i   i      c 
i  

  this

also assumes that the design matrix is skinny and full rank 

   

fiwhere x i  is a training feature vector  y  i    m is the number
of training examples  i and i are the dual variables  one
nuance to the dual problem is that there is no longer a
mention of the constant term b  thus  it needs to be inferred
from the solution of the dual problem  for the purpose of
this report  we have set 
bmax   bmin
b 
 
 i 
where bmax   max    y    w  x i   ai   c or ai  
    bmin   min    y  i     w  x i   ai     or ai   c  
and        represents a kernel 
using the solution of the dual problem  i and i    we
can construct the weight vectors as follows 
w 

m
x

fig    

 i 

i  

 i 

 i 

  x  x

neural network

 

i  

v  results and analysis

like svm  svr can also be kernelized 
c  neural networks
neural networks consist neurons which take in inputs
 weighted by a weight vector which may be different for
each neuron  and process them using a transfer function
to give an output  a neural network consists of a number
of interconnected neurons  we use a feed forward neural
network in which the information flow is unidirectional  if
we have a neuron j with a sigmoid transfer function  a weight
vector wj    w j     wnj  t   bias term b and inputs ok   from
the previous neurons then the output for neuron j will be 
 
oj    z     netj    
    exp z 
where 
z 

n
x

wkj ok

k  

the network is trained using a back propagation algorithm
which uses gradient descent to optimize the weights  the
squared error in this case is 
 
e    t  y  
 
where t is the desired output for an input vector and y is
the actual output from      we see that if we take the partial
derivatives using the chain rule we get 
e
   j oi
wij
where 

 oj  tj   netj       netj   




if j is an output neuron
e oj
j  
p
oj netj 
l wjl  netj       netj   


 ll
if j is an inner neuron
then the weight update equation is simply 
e
wij   
wij
  this

where  is the learning rate  we used matlabs neural
network framework to design  train  visualize  and simulate
the network  we have a two layer feed forward neural
network  the hidden layer neurons have a sigmoid transfer
function and the output layer neurons have a linear transfer
function  a diagram of the final neural network selected is
shown in fig   

is inspired from     

all our algorithms find average mse in terms of the x and
y locations of the points shown on screen  this is generalized
to yield values in terms of viewing angle error  horizontal
and vertical field of view is calculated using the screen size
and distance of user from the screen which came out to be
   and    respectively  using the discretized number of
bins for both directions we convert the l  error in terms of
viewing angle error as explained for one axis in    


avgl dist
x   tan  tan horf ov   
 horbins

   

the dimension of the feature vector for all of the testing
and training data used for all cross validation is      the
makeup of the     features is 
     features make up the vectorization of the   x  
downsampled image of the eye from the original
   x   
    features are the coefficients of the top    eigen eyes
    features are the coefficients of the top    fisher eyes
a  hyper parameter selection
to select the hyper parameters for each model of each
approach  we perform hold out cross validation where we
train on a random subset of     of the data and test on
    on the data  we then select the hyper parameters that
correspond to the model that has the smallest test error 
we use hold out cross validation instead of k fold cross
validation because the time to train the svr model and the
neural network on matlab was prohibitively long 
   regularized least squares  for regularized
least squares  there is one hyper parameter   which controls
how much the model cares about over fitting  the trade off
curve of hyper parameters to test error is show in fig    the
minimum on this curve is achieved with the model where
             note that the model where      corresponds
to the least squares model  without regularization  the fact
that the minimum occurs when  is non zero means that
the regularization drove some of the weights down and that
improved the model  this means that there are a number of

firegularized linear regression tradeoff

we have less than    neurons both training and testing error
increase which means the network is under fitting 

   
testingerror
trainingerror
best model choice

   

average l  disterror

 

   

   

   

   

   

   
 

     

     

     

     

    

     

     

     

     

    



fig     trade off curve for least squares of testing and training error vs 
  the minimum test error is marked with a red indicator 

redundant features  which was expected  this also means that
our choice of downsampling the image to a   x   resolution
still produced redundant features meaning that we have not
downsampled the image too much 
   support vector regression  there are two
hyper parameters to select for svr are c and   because we
must optimize over two variable models  we present multiple
trade off curves superimposed in fig    

fig     trade off curve for neural nets  showing number of neuron vs  test
and train error

b  sample output

svr model tradeoff
  

c     
c     
c     
c      
c      
c        

average l  disterror

  

  

fig     visualization using neural nets for training  showing actual points
and estimated points

  

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  



fig     trade off curve for svr  linear kernel  of testing and training error
vs    different values of c are presented as different lines  the minimum
test error is marked with a black indicator  

the minimum over all of the curves is achieved with
the model where         and c          this result is
surprising since the largest c value that was tested was used
in the best model  this illustrates how important the slack
variable is in svr 
we did not present the results for other kernels  e g  
polynomial kernels  since the results seemed worse  rather 
we instead focused on using neural networks which seemed
to yield a better result 
   neural network  with neural networks  our variable parameter was the number of neurons in the hidden
layer  we tested out different values for the number of
neurons and plotted the test and train error vs  number of
neurons as shown in fig     as we see  the training error is
minimized at when we have    neurons in the hidden layer 
beyond this number although the training error decreases the
test error increase which means the network is over fitting  if
  the

training error is not shown in the svr trade off plot in order to
magnify the difference between the hyper parameter settings 

fig     visualization using regularized least squares for training  showing
actual points and estimated points

fig    and   show the     of the outputs of the testing
set to avoid a cluttered illustration  the blue points are the
ground truth points shown to the users and the red points
are the ones estimated by our algorithm  in matlab plot
units   the green line between points is the distance between
the actual and estimated points 
one thing we observed was that error is typically larger
near the edge  regardless of which algorithm is used  we
suspect that this is because the subject used to create the
dataset needed to strain his eyes to reach the points in the
far corners  as a result  the subject could have possibly not
rotated his eyes all the way to view the indicator 
c  ablative analysis
we also carried out the analysis of varying the number of
features as well as including the eigen eyes and fisher eyes

fifor one of the tests  the images were downsampled at four
scales which gave different feature vector lengths and then
training and testing was carried out on the baseline algorithm
i e  regularized least squares  according to table i  the
performance improvement beyond feature vector length of
    i e  after doubling feature vector length was marginal but
with adding    pca components there was improvement and
hence     features were finally used for different algorithms 
table i
t esting e rror   in degrees   varying feature length for r eg  
l inear r egression
  feat

  

error

     

no pca
     

   
    pca
     

   

   

     

     

d  algorithm comparison
using the optimal model for each algorithm  we have
tested all three algorithms on the     validation data  the
results are tabulated in table ii  as can be seen from the
table the training and testing error are of the same order and
comparable which implies that we are not over fitting the
training data 
table ii
t esting e rror   in degrees   of all a pproaches
algorithm
linear reg
regularized linear reg
svr
neural net

train error
     
     
     
     

test error
     
     
     
     

one conclusion we can make from these results is that
neural networks work best compared to linear regression and
svr  using a linear kernel   the purpose of neural networks
is usually to implicitly solve for relationships between the
features  which makes it a more sophisticated algorithm
compared to least squares  which simply minimizes the
square loss objective  we suspect the reason svr performs
worse than least squares is that the data is not linearly
separable in the regression sense 
vi  future work and extensions
a  prediction approach
this is a part of our cs     project wherein using past eye
gaze locations we predict the future eye gaze location  our
models of prediction depend on the framework of hidden
markov models  much like       our approach is also to
model the true motion of the eye as a hidden random variable 
where our models differ is that our observed variables
are  noisy  measurements from the eye tracker and the
hidden variables are the true  noiseless  measurements  an
illustration of a discrete hmm as a bayesian network is
shown in fig    
  also

called evidence in some literature 

h 

h 

e 

e 

   

ht

ht  

et

fig     an hmm with hidden variables hi which each emit one discrete
observed variable  ei   here the variable t is an integer that indexes time 

as fig    suggests  each variable hi and ei are distributed
as follows 
hi  p  hi  hi   
   
ei  p  ei  hi  

   

when a bayesian network has the form in fig    and
has variables distributed as above  it is called an hmm 
for the rest of this report we will refer to p  hi  hi   
as the transition probability and p  ei  hi   as the emission
probability 
an hmm is completely specified by giving the hidden
and observed variables as well as the transition and emission
probabilities  first we will cover the positional hmm  which
uses eye gaze location as variables and then we will cover
the angular hmm which uses the angle of the eye gazes
velocity as variables 
   positional hidden markov model  the first
hidden markov model implemented uses true eye gaze
locations  i e    x  y  positions  in the scene as the hidden
states and the sensor output as the noisy observations  i e 
  x  y  positions  of these hidden states 
   angular hidden markov model  the second
hidden markov model used for prediction uses true discretized directions  or angles  of the eye gazes velocity as
hidden variables  hi   and the estimation of the eye gazes
velocity from the eye tracker as the evidence variables 
vii  conclusions
the purpose of this paper was to evaluate different learning approaches to the task of eye gaze tracking  our results
have shown that a simple two layer feed forward neural
network outperforms linear regression  and a seemingly more
complex approach  svr  as we expected from using an
image of the eye as a feature  there is are many relationships
that can be made between pixels  which is the kind of data
that works well in neural networks 
for future work  we would like to combine the cs   
approach for eye gaze prediction with the neural network
approach to build a fast real time application  we would also
like to explore what performance gains we can expect by
increasing the neural net layer  or by using convolutional
neural networks 
r eferences
    li  dongheng  david winfield  and derrick j  parkhurst  starburst 
a hybrid algorithm for video based eye tracking combining featurebased and model based approaches  computer vision and pattern
recognition workshops        cvpr workshops  ieee computer
society conference on  ieee       

fi    merad  djamel  stphanie mailles viard metz  and serge miguet 
eye and gaze tracking algorithm for collaborative learning system 
icinco ra            
    ji  qiang  and xiaojie yang  real time eye  gaze  and face pose
tracking for monitoring driver vigilance  real time imaging    
                
    kim  kyung nam  and r  s  ramakrishna  vision based eye gaze
tracking for human computer interface  systems  man  and cybernetics        ieee smc   conference proceedings       ieee
international conference on  vol     ieee       
    hansen  dan witzner  and qiang ji  in the eye of the beholder  a
survey of models for eyes and gaze  pattern analysis and machine
intelligence  ieee transactions on                      
    https   www kickstarter com projects fove fove the worlds first eyetracking virtual reality description
    sensomotoric instruments gmbh   september       smi eye tracking glasses   wireless  available at  http   goo gl nzjvwo
    baluja  shumeet  and dean pomerleau  non intrusive gaze tracking
using artificial neural networks  no  cmu cs         carnegiemellon univ pittsburgh pa dept of computer science       
    chen  jixu  and qiang ji  a probabilistic approach to online
eye gaze tracking without explicit personal calibration  image
processing  ieee transactions on                        
     yunlong feng  gene cheung  wai tian tan  yusheng ji  hidden
markov model for eye gaze prediction in networked video streaming 
in multimedia and expo  icme        ieee international conference
on   vol   no   pp            july     
     h  hadizadeh  m  j  enriquez  and i  v  baji  eye tracking database
for a set of standard video sequences  ieee trans  image processing 
vol      no     pp           feb       
     o  v  komogortsev and j  khan kalman filtering in the design of eyegaze guided computer interfaces proc    th int  conf  hum  comput 
interact   hci   pp            
     john findlay and robin walker        human saccadic eye movements  scholarpedia            

     a  smola and b  sch lkopf  a tutorial on support vector regression
statistics and computing      
     yunlong feng  cheung  g   wai tian tan  le callet  p   yusheng ji 
low cost eye gaze prediction system for interactive networked
video streaming  in multimedia  ie ee transactions on   vol    
no    pp            dec      
     h  hadizadeh  m  j  enriquez  and i  v  baji  eye tracking database
for a set of standard video sequences  ieee trans  image processing 
vol      no     pp           feb       
     b a  smith  q  yin  s k  feiner and s k  nayar gaze locking 
passive eye contact detection for humanobject interaction acm
symposium on user interface software and technology  uist  pp 
         oct       
     backpropagation 
wikipedia 
the
free
encyclopedia  wikimedia foundation  inc     december      
http   en wikipedia org wiki backpropagation
     https   www web stanford edu class ee    handouts lectures      autumn   eigenimages   x  pdf

fi