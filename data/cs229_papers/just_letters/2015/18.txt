cs    final paper

predicting sales for rossmann drug
stores
brian knott  hanbin liu  andrew simpson
abstract
in this paper we examined four different methods for time series forecasting  random forests  gradient
boosting  hidden markov models  and recurrent neural networks  we found that using gradient
boosting yielded the best results with root mean square percent error  rmpse  of              of       

i 

he goal of rossmann kaggle competition is to forecast the daily sales of rossmann stores located across germany using store  promotion  and competitor data  it
is known that store sales are influenced by
many factors  including promotions  competition  school and state holidays  seasonality 
and locality  a reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase productivity  the data are provided by rossmann
through kaggle 

t

ii 

iii 

introduction

related work

the models we use in this paper have been
widely studied  the framework for gradient
boosting is laid out in     and      random
forests are studied in      recurrent neural
networks in      and hidden markov models
in      these papers describe how parameters
effect model prediction 
additionally       groups have submitted
results to this kaggle competition  the code
for many of these submission is available on
the competitions scripts page  the majority of
the submissions used random forest or gradient boosting approaches  likely because there
are reliable packages for their implementation 
results of these approaches vary widely  however  due to different choices in parameters 
preprocessing  and data splitting 

dataset and features

rossmann provided data with    features
for      stores  not all stores have data for
each feature on each day  so we filled in missing data with median data for non ensemble
methods  ensemble methods did not require
data for every field   we then extended the
data to    features since some features  like the
date  captured multiple variables  like month
of the year and day of the month   the resulting data had the following feature set 
table    feature set

feature id

description

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  

store id
day of week
open   closed
promotion active
school holiday
store type
assortment
distance to competing store
month competitor opened
year competitor opened
promo  active
week promo  started
year promo  started
interval of promo 
month
year
day
competition age  gb only 

we then normalized the data to have zero mean
 

fics    final paper

before applying our learning algorithms 

iv 
i 

state transitioned from another stated  we used
laplace smoothing to replace zero probabilities with a small probabilities  eq    where k  
number of states  

methods

random forests

random forest regression is an ensemble
learning algorithm that operates by aggregating many random decision trees to make predictions while avoiding overfitting  we started
by using the random forest algorithm for
black box prediction because its bagging techniques are robust to data anomalies  like missing data  and because random forest packages
are widely available 
in particular  we used rs randomforest
package to carry out the training and prediction  we then used parameter optimization
to improve on our prediction model  to see
our parameter optimization method  and the
parameters used  see the results section 

ii 

hidden markov modeling is a sequence   state
estimation algorithm that assumes that the
dataset derives from a markov process with
hidden state information  hidden markov
models are appropriate for this dataset since
its data evolves over time  a phenomenon unaccounted for in ensemble learning 
   

hidden markov models rely on two sets of
markov assumptions  the first is the limited
horizon assumption  this assumes that the
probability of being in a state depends only on
the most recent states  for a first order markov
model  the current state zt depends only on the
state zt   eq     
the second assumption is that the state transition process is stationary  this means that
the conditional distribution of state transitions
does not change over time  eq     
p zt  zt      p z   z     t      t

   

the transition between states can be estimated by averaging the number of times a
 

tt     zt    si  zt   s j      
tt    zt    si     k

   

a hidden markov model is made of two
sets of states one hidden and the other observed  we want to find the most likely series of hidden states given the observed states
and known transitions between the observed
states and the hidden states  this can be done
naively by keeping track of all the provabilities 
however  this is computationally expensive  a
dynamic programing approach  known as the
viterbi algorithm  is used instead  this algorithm instead keeps track only of the maximum
probabilities through a recursive definition 

iii 

hidden markov models

p zt  zt    zt       z      p zt  zt   

p si  s j    

recurrent neural networks

recurrent neural networks work similarly
to standard artificial neural networks with the
addition of feedback loops  the internal recursive states allow this kind of neural network to
exhibit dynamic temporal behavior  making it
appropriate for processing time series data 
a neuron is made up of two parts a
weighted sum  weights  inputs  and a bias 
and an activation function  in order to learn
on non linear data  the hidden layer neurons
usually use an activation function that is in the
sigmoid family  logistic  gaussian  etc    the
output of each neuron is the weighted sum
passed into the activation function  the output of each neuron is passed to each neuron in
the next layer  when predicting a continuous
value  a single linear output neuron is usually
used  learning is usually done on neural networks through the back propagation algorithm 
this algorithm splits the error  squared error 
between each weight it the network  in order to
do this  we use the chain rule  on the hidden
layers the sum of the derivative is part of the
chain that is multiplied to find the error  an
example of how error flows back to the hidden

fics    final paper

errortotal
 
whidden



higherlayer

outputhigher
nethigher
errortotal


outputhigher
nethigher
outputhidden

 



outputhidden nethidden

nethidden
whidden
   

layer is given in eq     net is net sum of weights
times inputs and bias  
in recurrent neural networks the output of
the hidden layer neurons feeds back as another
input  a basic layout of a recurrent neural network is given in figure    since the input from
the previous time is passed as input to the current time  recurrent neural networks have a
form of memory  with a recursive definition 
this memory covers every input set 

validation  the algorithm keeps track of the
cross validation error and determines that an
appropriate fit has been found once the cross
validation error increases for a given number
of iterations 
we used the r package xgboost to train
our gradient boosting models  then used parameter optimization to find the best solution 

v 
i 

figure    an example recurrent neural network  outputs from the hidden layer neurons from the
previous time step are fed back in as input to
the next time step 

iv 

gradient boosting regression

gradient boosting is an ensemble learning
algorithm that uses a weighted average of simple models to learn a more complex model 
the algorithm first uses a simple model to fit
the data  then a simple model to fit the residuals between the data and the first model  this
process continues on each models residuals
until an appropriate fit is found 
as the model increases in complexity  overfitting tends to start occurring  to avoid overfitting  the gradient boosting algorithm keeps
a portion of the data  different in each iteration  out of the training set and uses it for cross

results and discussion

random forest

for our random forest approach  we used
the r package randomforest for training models and making predictions  this package allows for the variation of three major parameters  each parameter is mainly used to balance
the tradeoff between runtime and fit quality 
because random forest regression is a very resource hungry algorithm  parameters must be
set with feasibility in mind  the parameters
given to the model are as follows 
 mtry   the number of features allowed
in each decision tree  for a given value
k  trees may not use more than k regressor variables from the    features given
to the model  higher values for mtry
allows each tree to model more information from the features  but requires more
trees to generate an informative average 
 ntrees   the number of trees averaged in
the model  a higher value means runtime will be longer  but with a better fit 
 sample size   because the training algorithm is so resource hungry and rossmann provided so much data  it is not
feasible to run random forest regression
 

fics    final paper

on the whole dataset  therefore this parameter sets the number of random data
samples that the model is trained on 
table   shows the parameters for our best
two random forest models as well as the prediction rmspe as scored by kaggle  kaggle
scores on data for which we only have access
to the feature data  

discretized to work with the hidden markov
model  a smaller discretization leads to the
potential ability to be close to the true value 
however  it also decreases the probability estimation between states as there are more states 
figure   show the results of different levels of
sales discretization  this experiment showed
that dividing sales by      and rounding produced the lowest training error 

table    parameter selection for random forest regression

mtry
 
 

ii 

ntrees

samplesize

rmspe

   
   

      
      

      
      

hidden markov model

figure    hmm error using laplace smoothing and
month data

in order to account for seasonality effects 
the month was also included as an experiment 
however  when tested the training error was
higher than not including this data  see figure
    increasing the number of observed states
by a factor of          states  likely decreased
the estimation of state transitioning 
figure    hmm error using laplace smoothing

for testing with hmms  the stores were
treated separately  the decision to use the
stores separately came from an experiment
normalizing the data to zero mean and combining the stores to estimate transitions  combining all this produced a much higher error
 over     rmspe  and was computationally
expensive 
the observed states  what we know in advance  is everything except sales and customer
data  there are     state for different observations    days in a week  store open closed 
promotion active    state holidays  and a
school holiday   the sales data needed to be
 

iii 

recurrent neural network

due to the temporal nature of the data
hold out cross validation was used      of
the data was left out for validation  in order
to work with the matlabs functionality  the
data was normalized  each store was treated
separately for reasons similar to the hidden
markov method  during testing it was found
that the predicted values for closed days did
not always go to zero  so we set values for
closed days to zero  an experiment was to test
the effect the different numbers of neurons had
in the hidden layer  the experiment show that
  neurons produced the lowest training error
 see figure    

fics    final paper

to       however the model wight      trees
performed worse than the previous model  we
believe this to be due to overfitting since the
cross validation error does not improve from
     to      trees while the training error goes
down  as seen in figure     therefore  we chose
to keep the number of trees set at       we
then added feature    to our feature set  competition age  

figure    rnn error averages of different number of
hidden neurons  closed days forces to    

the actual error from kaggle had much
higher error of around      this may be due
to over fitting  the neural network may have
learned the pattern leading up to the test date 
but the kaggle test data started at a different
time 

iv 

gradient boosting

similar to random forests  gradient boosting is an ensemble based regression  efficient
for black box prediction  however  gradient
boosting is much faster to train than random
forests  it has been used in several kaggle
competition winning solutions and has been
developed into a the r package xgboost 
table    model performance

ntrees

parallel trees

  features

rmspe

    
    
    
    
    
    

default   
default   
  
  
default   
  

  
  
  
  
  
  

      
      
      
      
      
      

as shown in table    our first run of gradient boosting generated a rmspe of          a
significant improvement on our best random
forest model  to further improve our model 
we increased the number of trees from     

figure    learning curve for      tree model

adding competiton age to our feature set
did not improve our model on its own  but
combining models  runs       and    lead to a
model with an improved rmspe of         
our best result 
table    model performance

model

rmspe

gradient boosting
random forest
hidden markov model
predict median sales
recurrent neural nets

      
      
      
      
      

vi 

conclusion

we found that though hidden markov
models and recurrent neural networks are
better in theory for time series data  ensemble
methods performed much better in practice 
we believe this to be due to their robustness
in black box optimization considering the data
provided is unstructured and some features
may not correlate well with sales 
 

fics    final paper

references
    figueredo  a  j  and wolf  p  s  a         
assortative pairing and life history strategy   a cross cultural study  human nature 
          
    a  liaw and m  wiener         classification and regression by randomforest  r
news            
    friedman  jerome h         greedy function approximation  a gradient boosting machine  the annals of statistics 
                

 

    friedman  jerome h         stochastic
gradient boosting  computational statistics   data analysis                
    samarasinghe  sandhya         neural
networks for applied sciences and engineering  from fundamentals to complex
pattern recognition  auerbach publications
    zucchini  walter and macdonald  iain l 
       hidden markov models for time
series  an introduction using r  chapman   hall crc monographs on statistics   applied probability  chapman and
hall

fi