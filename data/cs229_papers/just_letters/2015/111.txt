predicting and evaluating the popularity of
online news
he ren

quan yang

department of electrical engineering
heren stanford edu

department of electrical engineering
quanyang stanford edu

abstractwith the expansion of the internet  more
and more people enjoys reading and sharing online
news articles  the number of shares under a news
article indicates how popular the news is  in this
project  we intend to find the best model and set
of feature to predict the popularity of online news 
using machine learning techniques  our data comes
from mashable  a well known online news website 
we implemented    different learning algorithms on
the dataset  ranging from various regressions to svm
and random forest  their performances are recorded
and compared  feature selection methods are used to
improve performance and reduce features  random
forest turns out to be the best model for prediction 
and it can achieve an accuracy of     with optimal
parameters  our work can help online news companies
to predict news popularity before publication 
keywords   machine learning  classification  popularity prediction  feature selection  model selection

i 

introduction

in this information era  reading and sharing
news have become the center of peoples entertainment lives  therefore  it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication  for social media workers
 authors  advertisers  etc   for the purpose of this
paper  we intend to make use of a largely and
recently collected dataset with over       articles
from mashable website  to first select informative
features and then analyze and compare the performance of several machine learning algorithms 
some prediction approaches are based on analyzing early users comments      or features about
post contents and domains      another proposed
method     predicted the articles popularity not
only based on its own appeal  but also other articles that it is competing with  prediction models
with svms  ranking svms      naive bayes    

are investigated  and more advanced algorithms
such as random forest  adaptive boosting     could
increase the precision  this paper however  incorporates a broader and more abstracter set of features 
and starts with basic regression and classification
models to advanced ones  with elaborations about
effective feature selection 
this paper has the following structure  section ii
introduces our dataset and feature selection  section
iii gives our implementation of various learning
algorithms  we analyze the result and compare the
performances in section iv  in section v  we discuss
possible future work 
ii 

dataset   feature selection

a  data collection
our dataset is provided by uci machine learning repository      originally acquired and preprocessed by k fernandes et al  it extracts    attributes  as numerical values  describing different
aspects of each article  from a total of       articles
published in the last two years from mashable
website  the full feature set is mainly categorized
as in table   
b  features
a full feature set may include much noise  we
first attempted pca for dimension reduction but it
did not provide any improvements for our models 
then we used filter methods  mutual information 
and fisher criterion  for feature selection and improved our prediction accuracy 
pca is a commonly used dimensionality reduction algorithm  which could give us a lowerdimensional approximation for original dataset
while preserving as much variability as possible 
however  the pca results could only made our

fitable i 

all available features

aspects

features

words

number of words of the title content 
average word length 
rate of unique non stop words of contents

links

number of links 
number of links to other articles in mashable

digital media

number of images videos

publication time

day of the week weekend

keywords

number of keywords 
worst best average keywords   shares  
article category

nlp

closeness to five lda topics 
title text polarity subjectivity 
rate and polarity of positive negative words 
absolute subjectivity polarity level

target

number of shares at mashable

models perform worse  this is because the original
feature set is well designed and correlated information between features is limited 

us better performance than other values of k and
mutual information based criterion  this observation also applies to other of our models  therefore 
we used this feature set as default for the rest of
project 

fig     top    features with highest fisher scores in feature
selection

filter methods
   mutual information  we calculate the mutual
information m i xi   y  between features and class
labels to be the score to rank features  which can be
expressed as the kullback leibler kl  divergence 
m i xi   y    kl p xi   y   p xi  p y  

   fisher criterion  fisher criterion is another effective way in feature ranking  the fish score  for
data with two classes  for jth feature is given by 
f  j   

where

 skj     

 x j  x j   
 s j       s j   
x

 xj  xkj   

xx k

the numerator indicates the discrimination between popular and unpopular news  and the denominator indicates the scatter within each class 
the larger the f score is  the more likely this feature is more discriminative  then we used crossed
validation  with logistic regression  finding that a
feature size of k       see fig     using f score gives

fig      d histogram of all data with respect to top   features

iii 

machine learning approaches

a  linear regression
first we used linear regression to get a quick
start  linear regression represents a least square fit
of the response to the data  it chooses the hypothesis
n
h  x   

x
i  

i xi

fiby minimizing the cost function
j    

m
 x
 h  x i     y  i    
  i  

due to the high variance of the target variable
 number of shares   direct application of linear regression was not acceptable  specifically  on testing
samples  only     prediction values  number of
shares  are within      of the actual results  we
discretized the target value to binary categories  as
in table     and consider a prediction correct if
its value and the actual result have the same sign
 both   or     it gave us     accuracy  although
we applied the regression model on a classification
problem  the result is quite desirable 
table ii 

and the prediction result is the class with the
largest probability  both generalization and training
error increases with the increasing k   e g   for k  
   logistic regression gives       accuracy   since
we mainly focused on predicting whether a news
would be popular or not  we did not go further in
multiclass problems 
c  support vector machine
we started svm with linear kernel  which use
the following formula to make predictions 
wt x   b  

       

      

linear rg  categories
logistic rg  classification

  
 

 
 

i  
m
x

 t

i y  i  x i 
d

x b
e

i y  i  x i    x   b

 

categories and classifications for linear rg 
and logistic rg 
number of shares

m
x

i  

in the equation  the kernel can be replaced with
more complex ones  as long as 
k x  y     x t  y 
table iii 

svm kernels we used

b  logistic regression
we then use classification model trying to improve our accuracy further  for logistic regression 
the hypothesis is
 
h  x    g t x   
    et x
and parameters are chosen as to maximize their
likelihood
m
y

p y  i   x i     

i  

we classified the data as in table   and used
stochastic gradient ascent rule to implement it  and
we got similar result as linear regression model 
for multinomial classifications  say k classes  
the model uses logarithmic function to estimate the
relative probability of each category with reference
to the kth category  for example  for k      i  e 
we classify the data into   categories  unpopular  
popular   very popular    we comparing the follows 
log

log

p  y  i      
p  y  i      

 

p  y  i 

 

p  y  i 

    
    

 

n
x

 i 

k   xk 

k  

 

n
x
k  

 i 

k   xk 

kernel

parameter

expression

linear

none

k x  y    xt y

polynomial

degree d

k x  y     xt y     d

gaussian



k x  y    exp 

 xy  
 
   

the kernels we used are in table    the reason
we use different kernels is because linear kernel
has high bias problem  polynomial and gaussian
kernels can operate in a high dimensional  implicit
feature space without computing the coordinates of
the data in that space  in this way  they can offer
more flexible decision boundaries 
d  random forest
in bagging  bootstrap aggregation   numerous
replicates of the original dataset are created
to reduce the variance in prediction  random
forest use multiple decision trees which are built
on separate sets of examples drawn from the
dataset  in each tree  we can use a subset of all
the features we have  by using more decision
trees and averaging the result  the variance of the
model can be greatly lowered  given a training set

fix      x           x n  with responses y       y            y  n   
bagging repeatedly  b times  selects a random
sample with replacement of the training set and
fits treees to these examples     

table iv 

performance of different algorithms
algorithms

accuracy

recall

linear regression

    

    

logistic regression

    

    

svm  d     poly kernel 

    

    

 for b           b  
   sample  with replacement  n training
examples called xb   yb
   train a decision or regression tree fb on
xb   yb
 after training  predictions for unseen examples
 
x can be made by averaging the predictions from
all the individual regression trees on x   

random forest      trees 

    

    

k nearest neighbors  k     

    

    

svr  linear kernel 

    

    

reptree

    

    

kernel partial least square

    

    

b
  x
fb  x   
f  

kernel perceptron  max loop     

    

    

c    algorithm

    

    

b

b  

or by taking the majority vote in the case of
decision trees 
for random forest  there are two main
parameters to be considered  number of trees and
number of features they select at each decision
point  theoretically  accuracy will increase with
more trees making decision  we use cross validation
to see how the performance changes with these
parameters  we ensured that every value within
a certain range that our computer can support is
tested  and the result is plotted  in this case we
are able to see exactly the relationship between
performance and parameters 
iv 

results

in this project  we implemented    different machine learning models  in this section  we apply  fold cross validation to models and compare their
performances  their accuracy and recall  sensitivity 
are listed in table    we went deeper into how parameter affects performance for svm and random
forest  since they have more parameters to consider 
as a classification model  logistic regression
achieves a decent accuracy  better than most of the
models  its receiver operating characteristic curve
is shown in fig     the auc value is       which
means logistic regression gives fairly good result 
by observing training and test error  we saw
that svm with linear kernel has high bias problem  therefore we used more complex kernels and
trained svm on full feature set to solve the problem
     the result is shown in table   

fig    
bounds

roc curve of logistic regression with confidence

even if we use high degree polynomial kernels 
high bias problem is slightly mitigated while accuracy seems to reach a bottleneck  as we plot the
training set on various combinations of features 
examples from two classes always mix together  and
it shows no potential boundary  we infer that the
data is not separable enough for svm to handle 
even for extremely high degree polynomial kernels 
for svm with   degree polynomial kernel 
which is empirically an optimal setting here for
svm  fig    shows how test error and training
error change with increasing number of training

fitable v 

svm results with various kernels

kernel

linear

poly  d     

poly  d      

gaussian

test error
training error

    
    

    
    

    
    

    
    

fig     random forest training and test error with respect
to number of training examples  left  and number of decision
trees  right 

fig     svm error with increasing number of training examples

examples  this is the best result svm can give  and
its accuracy doesnt improve with more training
examples 
random forest has the best result for this
classification problem  it can have different number
of decision trees and different number of features
used for each decision point  the number of
training examples can also change  therefore 
implementation should be done in a systematic
way  we change only one variable at a time  we
first use a default setting for random forest and
increase the number of training examples  the
error decreases to a certain level as shown in fig 
   left   then we set the number of trees to be
constant  and change the number of features used
for decision  it turns out that log nmax   is the
best value  finally  we change the number of trees
continuously from   to      and plot the result in
fig     right   the accuracy reaches a limit of     
which is the best among all algorithms 
v 

future work

as is seen from the result  no algorithm can
reach     accuracy given the data set we have 
even though they are state of the art  to improve
accuracy  there is little room in model selection but
much room in feature selection  in the preprocess 

ing round     features were extracted from news
articles  and our later work is based on these features  however  the content of news articles hasnt
been fully explored  some features are related to
the content  such as lda topics  feature           which are convenient to use for learning  but
reflect only a small portion of information about the
content 
in the future  we could directly treat all the
words in an article as additional features  and then
apply machine learning algorithms like naive bayes
and svm  in this way  what the article really talks
about is taken into account  and this approach
should improve the accuracy of prediction if combined with our current work 
references
   

   
   

   

   

   

tatar  alexandru  et al   predicting the popularity of online
articles based on user comments   proceedings of the
international conference on web intelligence  mining and
semantics  acm       
 predicting the popularity of social news posts       
cs    projects  joe maguire scott michelson 
hensinger  elena  ilias flaounas  and nello cristianini 
 modelling and predicting news popularity   pattern analysis and applications                      
k  fernandes  p  vinagre and p  cortez  a proactive intelligent decision support system for predicting the popularity of online news  proceedings of the   th epia     
  portuguese conference on artificial intelligence  september 
coimbra  portugal 
chang  chih chung  and chih jen lin   libsvm  a library for support vector machines   acm transactions on
intelligent systems and technology  tist                 
james  gareth  et al  an introduction to statistical learning 
new york  springer       

fi