audio segmentation
ashutosh kulkarni  deepak iyer  srinivasa rangan sridharan
stanford university  stanford 
 ashuvk  ideepak  rangan  stanford edu

abstract
in this paper  we propose a novel algorithm to segment an audio piece into its structural components 
the boundaries of the homogeneous regions are decided based on various time and frequency domain
features  the algorithm has been designed in   stages 
in the first stage  a vocal non vocal silence classification is done using multinomial softmax regression  the
second stage uses a hidden markov model to smooth
the previous output as well as enforce the time dependent structuring  the training and testing was done
specific to songs of the popular english rock group 
the beatles with an average error window of     ms 

gion where the information content is maximum and
can be used best for classification  this  when coupled
with a hidden markov model which not only tries to
negate the classification error of the previous algorithm  but also incorporates musical rules which are
time dependent makes our model capture almost all
rules from the audio domain 
this paper is structured as follows  section   gives an
insight into the various feature vectors that were tried 
while section   deals with the algorithm development
and section   gives a detailed description of the final
algorithm and the results  concluding remarks  acknowledgements and references follow in section   

  introduction
the internet has proved to be  by far the most effective
medium for music distributors to reach out to musiclovers and enthusiasts by providing them with immediate access to music across geography  genre  artists
and eras  domains like amazon com  itunes com are
popular names in this context  a key component of
their marketing strategy in this process is to allow a
hearing of the songs before the users finalize their purchase  any optimization of this process could help
enhance user experience and thereby help bring in
greater returns for distributors 
one such optimization in online music retailing would
be to allow users to have direct access to different
segments of a song  like the intro  verse  lead  outro  
in this paper  we present an approach to solve the
problem of segmenting a song into these homogeneous
regions 
most existing algorithms differ from our approach in
the sense that we do not consider the full range of the
frequency spectrum  and instead narrow in on the re 

  feature vectors
 henceforth  the term frame refers to a sequence of
     audio samples  this frame size is popular in audio analysis due to the tradeoff it provides between the
audio content and computational complexity 
there can be numerous metrics which can be used
as feature vectors in audio analysis  a few of which we
have incorporated into our model depending on their
relevance to the problem at hand  which was classifying a frame into the classes we aimed in the first step
of the algorithm  these feature vectors were calculated  as mentioned earlier  on frames of size     
samples  this section gives a brief description of the
various feature vectors that were tried in the design of
the algorithm 

    mel frequency cepstral coefficients  mfccs 
a spectrum is a positive real function of a frequency
variable associated with a stationary stochastic
process  while a cepstrum is the result of considering
the spectrum  in mel scale  as the process  an mfcc

firepresentation mathematically put  is the discrete cosine transform  dct  of the mel scale representation
of the frequency spectrum and simply put  is the spectrum of a spectrum  mfccs are very effective in
representing the audio content as a whole  their disadvantage is that they do not represent the perceived
loudness accurately  this  in fact  worked in our favor
because we desire an amplitude independent analysis 
additionally  the number of mfccs is a tradeoff between resolution and computational complexity 
hence  we narrowed down on    coefficients 
    spectral flux
spectral flux is a measure the rate of change of the
power spectrum of a signal and is obtained by comparing the power spectrum of the current frame against
previous frames  we took average spectral flux over
   frames to be our feature vector  because most instrumental onset times are lower than this time  the
spectral flux is expected to be larger for non vocal
content because of the onset and decays of instruments 
    zero crossing rate
it is the rate of sign changes of a signal in time  i e 
the high frequency content  human vocals are known
to exhibit a high zero crossing rate  because speech
can be modeled as a random process 
    average energy
energy is the square of the amplitude  the
energy of a frame gives an average of the total frequency content in a frame  this metric differs for vocal and instruments like the drums  hence  it was selected as a feature vector 
    centroid
the centroid is a weighted mean in an algebraic
sense  in audio analysis  it is that frequency which divides the spectrum into   parts with equal energy 
    rolloff
a percentage rolloff can give more information
about the frequency distribution than the centroid  we
tried various percentage rolloffs to try to capture the
distribution 
apart from these we also tried using the average energy over the past few frames  we settled upon mfccs 
spectral flux  zero crossing rate and average energy as
our feature vectors because of their relevance to the

problem and their ability to effectively incorporate
most of the features 
data
data collection in itself is a tricky problem in
the domain of music because the feature vectors are
entities which are not directly visible  in order to keep
the genre consistent over training and testing and to
avoid boundary conditions due to song arrangements 
we limited our study to the sound tracks of the
beatles  the training set was created by editing the
soundtracks and creating pieces which contain purely
instruments  vocals and silence  structural components
like outro  intro and bridge were not incorporated in
the training stage because of a lack of distinction between them in any domain other than time  the training set requires a wide range of samples  an important
thing to note here is that in audio   wide range  is not
mathematically but aesthetically  for e g  in the training set for vocals  we would need samples with vocals
in the low pitch  medium pitch and high pitch  also 
tracks with different levels of vocal to instrument amplitude ratios further add to the variety  hence the
training set of soundtracks was decided by careful
analysis of all the soundtracks to give us a wide range
of training samples  the data used for training of the
vocal and non vocal parts consisted of over     
frames  these frames were created from our database
of songs by selecting small  representative clips  aesthetically  of audio  for silence      frames were
used 

   algorithm development
this section deals with the first step in the algorithm  here  we classify each frame of the song as belonging to one of three classes  non vocal  n   vocal
 v  or silence s   these classes were chosen based on
the fact that they help characterize the components that
we seek to find in the song  for e g   the intro part of
a song consists purely of non vocal content while the
verse usually contains a good mix of both vocal and
non vocal content  the silence class represents portions of the songs which can be ignored and is usually
found to be at either the start or the end of the song
the problem of classifying each frame of the
song as vocal  non vocal or silence was tried using
various algorithms like multinomial softmax regres 

fision and support vector machines  with different kernels like linear  polynomial  weighted linear  weighted
polynomial  rbf  but we selected multinomial softmax
regression because of its performance over the other
classifiers  each frame of the song was thus classified
based on the maximum probability class returned by
the algorithm  the feature vectors mentioned in the
previous section were used with all the classifiers 
existing algorithms compute the feature vectors based on the whole frequency spectrum  fig a  
this is the basic initial approach we took  the results
were far from satisfactory largely because of equal
important being given to all parts of the spectrum  also  the maximum vocal frequency formants in case of
singing occur at around      hz  the next obvious
approach was to boost the vocal range  around         hz  in the spectrum  see fig  b   this approach
did not work even though vocals were boosted because
of the influence of other frequencies in the spectrum
being higher  hence  we decided to apply a steep
band pass filter on the training data sets with a lower
cut off frequency of     hz and various higher cut off
frequencies like     hz  fig  c       hz  fig  d  and
    hz  fig  e   the primary reason for choosing the
lower cut off frequency at     hz is to avoid using the
base frequencies which occur uniformly throughout 
the higher cut off frequency of      hz gave the best
results because it modeled the vocal range best  the
performance errors were based on finding the error in
milliseconds between the segment boundaries produced by the model and those obtained manually 

b  vocal boost

c           hz band pass

d           hz band pass

a  unaltered spectrum
e           hz band pass

fithe results for an unaltered spectrum vs  a band pass
of          hz are as follows 

error in ms for different songs 

   future work
non vocal error

   algorithm
as mentioned in the previous section  a steep
band pass filter with cut off frequencies of     and
     hz is used on the training data sets before computation of the feature vectors  the output of this stage
is a sequence of a combination of the classes vocal 
non vocal and silence 
now given a time sequence  where each member of
the sequence represents the class of the corresponding
frame  the next step is to obtain the structures in the
song  this is achieved by using a hidden markov
model  where the hidden states were instrumental
and verse  with each state emitting a non vocal 
vocal or silence frame  the output of the hmm therefore represents the sequence of states which are most
likely to produce the sequence of frames that were observed 
once the sequence of states is obtained  the
instrumental portions of the song are renamed as intro  outro or lead depending on whether they occur at
the beginning  end or between two verses in the song 
such a procedure is followed because the distinguishing feature between the intro  outro and lead is along
the time scale rather than in any other easily discernible feature 

the most important issue we intend to work on to
make the algorithm commercially is to expand the
range of songs which are classified by the algorithm 
the decent result for songs of different genres currently is because the model hasnt been trained for different arrangements in songs  we would like to expose
our model to many more arrangements of songs and
aesthetic variety of data so that it can work on any generic sample 

acknowledgements
we would like to thank asst  prof  ge wang and
kyogu lee from the stanford university center for
computer research in music and acoustics
 ccrma  for their assistance and support right from
the nascent stages of the project  we would also like to
express our gratitude towards prof  roger dannenberg
of carnegie mellon university for his insightful guidance  we are thankful to prof  andrew ng and the
tas of the course who were always available and
ready to help 

references
    r  dannenberg and m  goto  music structure
analysis from acoustic signals  music structure   
april      
    a  l  berenzweig and d  p  w  ellis  locating
singing voice segments within music signals  proceedings of ieee workshop on applications of signal
processing to audio and acoustics  pp          new
york  oct       
    tong zhang  semi automatic approach for music  hpl       

fi    goffredo haus  luca a  ludovico  music segmentation  an xml oriented approach  lim dico
university of milan 
    kristoffer jensen  multiple scale music segmentation using rhythm  timbre and harmony 
eurasip journal on advances in signal processing 
volume       article id       
    tom zhang and jay kuo  content based classification and retrieval of audio  integrated media systems and electrical engineering systems  university of
southern california 

fi