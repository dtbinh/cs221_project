term project report  cs     machine learning of fall      

 

seeing invisible properties of subsurface oil and
gas reservoir through extensive uses of machine
learning algorithms
kwangwon park
abstractcurrent geostatistical simulation methods allow generating multiple realizations that honor all available
data  such as hard and secondary data under certain geological scenarios  e g   d training image based models 
multi gaussian law  boolean models   however  it is difficult to simulate large models that honor highly nonlinear
response functions  e g  remote sensing data  geophysical
data or flow in porous media data   the large cpu cost to
evaluate the response function imposes limitations on the
size of the model  this is particularly the case when one
desires to generate a sizeable set of realizations matching
the same data  the objective of this study is to generate
multiple realizations all of which honor all available data
 hard and secondary data and especially the non linear response function  under certain geological scenarios  first 
we generate a large ensemble of possible realizations describing the spatial uncertainty for given hard and secondary data  any fast geostatistical simulation methods
can be used for generating these prior models  secondly 
using multidimensional scaling  we map these models into
a low dimensional metric space by defining a distance between these prior models  we propose to make this distance
a function of the response function  next  kernel principal
component analysis is applied to the coordinates of realizations mapped in this metric space to create a kernel  or feature  space with linear gaussian type variability between
the input realizations  in this space  we can apply optimization algorithms  such as gradient based algorithms or
ensemble kalman filtering or gradual deformation method
to generate multiple realizations matching the same data 
a back transformation  first from the kernel space  then
to metric space and finally to the actual space of realizations allows then the generation of multiple geostatistical
models that match all data  hard  secondary and non linear
response  we apply the proposed framework to generate a
realistic model which honors geologic information and dynamic  time varying  response data  a flow simulator is
used as the non linear response function and may require
several hours of cpu time per simulation  we show how
this technique applies to non gaussian  e g  multiple point
or boolean  geostatistical models  we also demonstrate the
importance of using a distance function tailored to the particular response function used in creating a low dimensional
parameterization of the ensemble of geostatistical model in
feature space 

tion  solvers with various reservoir properties as input parameters  however  we can seldom have enough information about these input parameters due to the limitation
to access very deep subsurface formation  those parameters which can not be measured directly are often obtained
by solving optimization problem because we can measure
output data only  such as production rates or pressure variation at production wells 
however  optimization problems for ascertaining reservoir parameters are containing a lot of tough limitations 
first  we usually have to determine     to     number of
input parameters  which means input space is extremely
high dimensional  second  single evaluation of objective
function  difference between observed and simulated  in
other words misfit function  usually takes several hours to
several days  moreover there are lots of local minima in
input space due to highly nonlinear and extremely large
system of partial differential equations in reservoir simulation  third  our optimization is limited by very few information about reservoir  as we have very few wells  which
are the only way for us to measure the reservoir input and
output data  fourth  since all those parameters are related
with geologic subsurface structures  the input parameters
should be geologically realistic 
in the process of this tough optimization problem  various machine learning algorithms can be introduced extensively  the problem  theory  methodology  and simple
results will be followed 

index termscs     machine learning  optimization 
reservoir

where  x is the unknown input parameters  reservoir properties  and y is the output  solution variables    represents the known parameters  f means a pde as equation   

i  introduction

i

n order to estimate oil and gas reserves and maximize
their production  it is essential to find out unknown
reservoir properties  such as geologic subsurface structures 
reservoir engineers often use a reservoir simulator that
provides future prediction about oil and gas flow performance  which is a kind of pde  partial differential equakwangwon park is with the department of energy resources
engineering  stanford university  stanford  ca        phone 
                e mail  wonii stanford edu

ii  problem statements

t

he ultimate objective is to solve an inverse problem 
which can be represented by equation   
x   f   y 


 y      xy        
t

   

   

where  t represents time  equation   can be solved by
various numerical methods with the initial and boundary
conditions that are controlled by   equation   and   have
several interesting aspects 
nonlinearity  since  is a function of y  equation   is
a highly nonlinear equation  which also increases the
simulation  besides  although we know all the ys in

fi 

term project report  cs     machine learning of fall      

the domain  actually rarely possible in field   it is very
difficult to solve the inverse problem  equation    
large number of parameters  usually we have to determine several unknown properties at each node  which
means the dimension of x is     to       in other words 
we have to solve a highly nonlinear inverse partial difference equation in extremely high dimension  it is
needless to say that a lot of local minima exist in the
space 
large number of nodes  in order to get the desired and
meaningful resolution from equation    we usually
have to make     to     nodes at each of which the
output y has to be calculated in   dimensional space 
large number of nodes result in dramatic increase in
forward simulation time  hours to days per single simulation  
limited output data  actually  we can obtain very limited information of y  as we have to install a kind
of measurement device in the deep subsurface  which
costs a lot  in a reservoir  the measurement device is
equivalent to wells  injectors or producers  and drilling
one well costs usually millions to tens of millions of
dollars  in many cases  we know a few  tens to thousands  measurements of y among     to     values 
lots of constraints  in this optimization problem  we
have a lot of constraints on x  in a reservoir  x is
reservoir properties  such as permeability or porosity  which are highly dependent upon subsurface rock
properties  therefore  the main constraint is that x
should be geologically realistic 
with all these challenges  it is often impossible to solve
this optimization problem using conventional optimization techniques themselves  therefore  somewhat different
methods will be applied and in each step the appropriate
machine learning algorithms are going to be applied effectively 
iii  distances

t

he distance is a measure of dissimilarity between two
realizations  simply  we can evaluate the dissimilarity between realizations xa and xb  discretized into ngb
gridblocks  through the minkowski model  such as euclidian space or city block  manhattan  space  although the
minkowski model is easy to calculate  it may not be wellcorrelated with dynamic data because the forward simulated dynamic data may change dramatically by perturbing a small portion of the realization  figure   depicts
the correlation between euclidian or manhattan distance
and the dissimilarity between dynamic data  the difference
between watercut curves   it turns out that both euclidian and manhattan distances are not correlated with the
dynamic data 
in order to optimize an inverse solution efficiently in the
distance space  it is necessary that the dynamic data are
spatially correlated in the space  for this  various distances
may be utilized  if we need euclidian distance  actually
we sometimes have to deal with euclidian distance for satisfying the metric axioms or the positive definiteness of the

 a  euclidian distance

 b  manhattan distance

fig     the distance and dissimilarity of dynamic data  watercut  
on the y axis is the difference in watercut between any two realizations  on the x axis the distance between any two realizations 

distance matrix  for example when employing the rbf kernels   then any similarity distance can be easily converted
to euclidian distance by means of multidimensional scaling
 mds  or principal coordinate analysis 
iv  parameterization

p

rior to the parameterization of the geological model
space  we start from an ensemble of realizations  xj  
 j           nr   if we generate nr realizations   x could
represent a facies porosity or permeability model or any
combination of those  the initial ensemble can be generated by various geostatistical algorithms honoring the
geologic information and conditioning to the static data
 hard and soft data   for simplicity  define the matrix for
the ensemble x as
 x   j   xj

 

   

where   x i j means  i  j  element of matrix x and     j  is
j th column of matrix x  the covariance of the ensembles
is calculated numerically by equation   
nr
 
  x
xj xt
xxt
   
c 
j  
nr j  
nr
when we perform an eigenvalue decomposition on the
covariance  equation     then a new realization can be
obtained by equation    karhunen loeve expansion  
cv   v
xnew   e    new

   
   

where  v and  is the eigenvector and eigenvalue  respectively  e is a matrix each column of which is the eigenvector of the covariance   is a diagonal matrix each diagonal element of which is the eigenvalue of the covariance 
new is the parameter vector for the realization xnew   the
parameter  is gaussian random vector and the size is determined by how many eigenvalues are chosen to retain 
we do not have to use all the nonzero nr eigenvalues 
typically a few large eigenvalues are retained  by equation    we can generate many models based on the same
covariance 
in order to consider higher order moments or spatial correlation beyond the point by point covariance  the feature
expansions of the realizations can be introduced  let  be

fipark  specification for common ieee styles

 

the feature map from realizations space r to feature space
f  equations   and    
 rf

   

nr covariance matrix  equation     
 z    e      

nr
nr
nr
x
x
x
   
   
  i i
 ai  j  xj  
  i i vi  
i  

i  

x        x 

   
 

where  is the feature expansion of realization  with the
feature expansions of the ensemble  x   defined by equation     the new feature expansion can be generated in the
same manner above  equation      the covariance of the
feature expansions   xj    of the ensemble is calculated by
equation    
  x    j    xj  
nr
  x
 
c 
 xj   xj  t  
 x  x t
nr j  
nr

 xnew     e    new

   

    

    

however  since the feature expansion is often very highdimensional and sometimes infinite dimensional  the eigenvalue decomposition of the covariance matrix is almost impossible  the kernel trick makes it possible to obtain the
exactly equivalent solution to the eigenvalue decomposition of the covariance  if we define a kernel function as a
dot product of two feature expansions  equation      the
kernel function can be evaluated without representing the
high dimensional feature expansions explicitly  then  the
kernel matrix  equation     can be calculated efficiently 
k xi   xj          xi     xj    
k      x t  x 

    
    

where   k i j is k xi   xj   and     means the dot product 
the main idea of the kernel trick is to assume that the
new feature expansion is a linear combination of the feature
expansions of the ensemble and represent all the elements
in the equations as dot products of two feature expansions 
actually  equation    means that a new feature expansion is a linear combination of the eigenvectors  since the
eigenvectors lie in the span of the feature expansions of the
ensemble  equation      it is true that the new feature expansion is a linear combination of the feature expansions
of the ensemble  it turns out that the coefficient ai is the
i th eigenvector of the kernel matrix and nr i is the i th
eigenvalue of the kernel matrix  equation     
vi  

nr
x

 ai  j  xj  

    

ka   nr a

    

j  

therefore  we can acquire the new feature expansion
without the costly eigenvalue decomposition of the nr 

nr
x
j  

j  

 n
 
nr
r
x
x
   
  i i  ai  j  xj    
 b j  xj      x b
    
i  

where   b j  

n
pr

j  

   

  i i  ai  j

i  

once the new feature expansion is acquired  the new realization can be calculated from the new feature expansion
 xnew      new     since   cannot often be calculated
explicitly  we have to calculate the new model such that
xnew   arg min k xnew     x bk
xnew

 
  arg min  xnew  t  xnew      xnew  t  x b   bt kb
     
xnew

this is another optimization problem which is called the
pre image problem  this optimization problem can be
solved by the fixed point iteration method   scholkopf and
smola          we find xnew such that

 
xnew  xnew  t  xnew      xnew  t  x b   bt kb    
    
by iterations  equation     
n
pr

xnew  

 b j k    xj   xnew  xj

j  
n
pr

    
 b j

k    x

j   xnew  

j  

where k   means the differential of k  since we have the
kernel functions not the explicit feature expansion  these
iterations can be done efficiently  in conclusion  the new
realization can be obtained by a nonlinear combination of
the ensemble members  note that the nonlinear weight
sum to unity 
we can use various types of kernels  but the kernel matrix should be positive definite  mercer theorem   some
widely used kernels are 
d
 polynomial  k x  z       x  z    c 




gaussian  k x  z    exp  kxzk
   

 

sigmoid  k x  z    tanh    x  z     
figure   shows the correlation between the polynomial
kernel and the dissimilarity between dynamic data  the
difference of watercut curves  of any two realizations  it
turns out that polynomial kernels are not correlated with
the dynamic data in this case  since a kernel is the measure
of similarity  it is desirable for the kernel to be negatively
correlated with the dissimilarity between dynamic data 
likewise the gaussian kernel  the kernel that is based on
the euclidian distance is called rbf kernel  even though
we know the euclidian distance only  the rbf kernel function can be evaluated  also  although the dissimilarity distance is not a euclidian distance  we can map the ensemble


fi 

term project report  cs     machine learning of fall      

into the metric space by multidimensional scaling  once
the euclidian distance in the metric space is well correlated with the dissimilarity distance  we can evaluate the
kernel function by replacing the distance to the euclidian
distance in the metric space  figure   depicts the correlation between the gaussian kernel and the dissimilarity between dynamic data of any two realizations  the hausdorff
distance  suzuki and caers        and connectivity based
distance  park and caers       after mds  ten eigenvalues are retained  are used for the euclidian distance in
gaussian kernel  the connectivity based distance shows 
to some extent  negative correlation with the dissimilarity
of dynamic data 

with

n
p
i  

 i      hu and blanc        

considering for instance the gradual deformation of  
random vectors   and     we have a single gradual deformation parameter as equation    
 new     

 
x

i i     cos       sin  

    

i  

for the calibration of a stochastic model  the following iterative optimization procedure is often used  equation     
n      n  cos    n sin 

    

where n  is the optimized parameterization vector at iteration n     and n are a series of independent gaussian
random vectors  then by minimizing the objective function with respect to parameter   we get a new parameterization vector n  opt   that improves  or at least maintains 
the calibration to the nonlinear data  hu        
vi  the proposed workflow
 a  k x  z     x  z  

 b  k x  z       x  z       

fig     the polynomial kernels and dissimilarity of dynamic data
 watercut   on the y axis is the difference in watercut between any
two realizations  on the x axis the kernel between any two realizations 

 a  hausdorff distance

 b  connectivity distance

fig     the rbf kernels and dissimilarity of dynamic data  watercut   on the y axis is the difference in watercut between any two
realizations  on the x axis the kernel between any two realizations 

v  further parameterization of 

t

he parameterized feature expansions of realizations
 s  are gaussian random vectors  so the optimization can be accomplished by the sequential calibration with
gradual deformation 
hu        developed the gdm for performing history
matching on stochastic reservoir models  it consists in iteratively updating a combination of independent realizations of a random function until both static and dynamic
data are matched 
consider a gaussian random vector i with zero mean
and unit variance  the gdm consists in writing a new
random vector  new as a linear combination of n independent random vectors  equation     
new  

n
x
i  

 i i

    

b

ased on the theories that are stated above  the proposed procedure for conditioning ensemble to dynamic
data under realistic geologic scenarios is as follows  figure    
   generate the initial ensemble  realization space 
first we generate an initial ensemble  the initial
ensemble should include models that are honoring geologic information and are conditioned to all available
static data  that is  hard and soft data  to do this  we
can choose a proper geostatistical algorithms  such as
sgsim  sisim  dssim  and so on as variogram based
methods and snesim and filtersim as multiplepoint  mp  simulation methods  generating the ensemble  we may have to consider the uncertainty in
the static data  for example  if our geologic information is uncertain  we can use multiple training images
for mp simulations 
   calculate the dissimilarity distances  distance space
to metric space 
from the initial ensemble  we calculate the dissimilarity distances and construct a distance matrix 
at this step  it is important for the distances to be
correlated with the dynamic data that we want to
condition  if needed  we can apply multidimensional
scaling to lower the dimension and get euclidian distances  which make it possible to use rbf kernels 
   calculate the kernel matrix  to feature space 
based on the euclidian distances  we calculate
the kernel matrix  rbf kernel matrix would be easily calculated but a proper kernel should be chosen
cautiously 
   parameterize the initial ensemble  to parameterization space 
after obtaining the eigenvalues and eigenvectors
of the kernel matrix  each member of the initial ensemble is parameterized to relatively short gaussian
random variables 

fipark  specification for common ieee styles

   optimize the parameters  in parameterization space 
the optimization process would be done on the
parameterization of the initial ensemble  since the parameters are low dimensional gaussian random variables  we may apply various optimization methods 
such as gradient based methods using the sensitivity
coefficients  probability perturbation method  gradual
deformation method  ensemble kalman filter  and so
on  since we already have an ensemble  enkf would
be applied effectively and provide multiple models
which show the same dynamic data response  additionally  the optimization might be accelerated by
an efficient selection method through kernel pca 
   solve the pre image problems  to realization space 
now  the optimized parameters are converted into
model state vectors  using a proper minimization algorithm  such as a fixed point iteration  we solve the
pre image problems for all the optimized parameters 
   analyze multiple models
finally  we obtain multiple models which satisfy
all available data and geologic scenarios  we can use
these multiple models in a variety of purposes  since
we generate an initial ensemble reflecting the uncertainty after conditioning to static data acquired so far 
these final multiple models indicate the uncertainty  a
posteriori   after conditioning to static and dynamic
data  the multiple models may suggest which type
of data should be acquired additionally  or a value of
information question can be posed 

 

 a  reference

 b  final

fig     the reference and final realizations 

 a  decrease of the
mismatch of the dynamic
data with iterations 

 b  dynamic data of the
initial  green   final  red  
and reference  blue 
realizations 

fig     decrease of the mismatch of the dynamic data with iterations
and the dynamic data matching 

viii  conclusion
the objective of this research is to generate multiple
models which are satisfying all available static and dynamic data  for the objective  multiple optimization methods will be combined and applied in kernel feature space
based on a dissimilarity distance  the proposed method
will be verified in both theoretical and experimental ways 
this research has potential for applications in various areas
of reservoir modeling and real time production optimization 

acknowledgements
the author would like to thank prof  andrew ng and
five tas for great lectures and helps 

references
fig     the proposed workflow 

vii  evaluations

w

e applied the proposed work flow to a simple case 
within     iterations  we could find a realization
which are conditioned to the dynamic data  figure     the
final realization indicates similar channel locations and directions  figure    

    deutsch  c v  and journel  a g   geostatistical software library and users guide  oxford university press  ny        
    hu  l y   gradual deformaion and iterative calibration
of gaussian related stochastic models  mathematical geology
             
    park  k  and caers  j   history matching in low dimensional
connectivity vector space  proceedings of eage petroleum
geostatistics      conference  cascais  portugal 
    scholkopf  b  and smola  a j   learning with kernels  support
vector machines  regularization  optimization  and beyond 
the mit press  cambridge  ma        
    suzuki  s  and caers  j   history matching with an uncertain
geological scenario  paper spe       presented at the     
spe atce  tx 

fi