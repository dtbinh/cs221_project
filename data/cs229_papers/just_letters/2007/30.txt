extending wordnet using generalized automated relationship induction
lawrence mcafee

nuwan i  senaratna

todd sullivan

lcmcafee stanford edu

nuwans cs stanford edu

tsullivn stanford edu

this paper describes a java package for automatically extending wordnet and other semantic lexicons  extending
these semantic lexicons by traditional means of hand labeling word relationships is a very expensive and laborious process  we used machine learning techniques to
automatically extract relationships between words from a
given text corpus  the package is made to be very flexible  allowing for various modules  such as new classifiers
and semanitic lexions  to be plugged in  the power of
the package comes form its ability to seamlessly integrate
the stanford parser to wordnet  results obtained for
various tests  particularly those done for a nave bayes
classifier  are promising 

 

introduction

wordnet like semantic lexicons are vital for research and application development in natural
language processing and related fields  however 
attempts at extending such lexicons using traditional methods such as hand crafted patterns or
human insertion of new words into the taxonomy
to cover more general vocabularies have proved to
be very expensive and tedious  recently  there has
been some work into using machine learningbased methods to automatically learn word relationships from large collections of text 
the goal of our project is to use machine
learning techniques to build a framework that is
capable of identifying a range of word relationships  our package is general enough to be able to
handle most types of relationships that can be observed within single sentences  the large amount
of flexibility in our package is due to its extensibility characteristics  allowing the user to add their
own custom classes to certain packages 
our package is also very powerful because of
its ability to integrate the stanford parser and
wordnet into a single package  the stanford
parser is used to parse sentences into tree structures  from which we can extract patterns that are
used in our feature vectors 

figure    example of a typed dependency tree
generated by the stanford parser    

previously  an algorithm was developed by
snow et al  to automatically induce new word
pairs from text that exhibit the hypernym relationship      we build upon work in     by creating a
generic word induction algorithm that can operate
on any word relationship where the word pairs exist in the same sentence  additionally  we invoke
the induction process described in     repeatedly 
training on the previous training set and the new
word pairs generated from the previous iteration 
snow showed that a good technique for the hypernym relationship  as is also for our algorithm 
to capture structured relational knowledge is to
use dependency trees  dependency trees represent
dependencies between individual words in a sentence 
our project improves on the general layout of
this system by    increasing accuracy through the
use of a better parser  the stanford parser     
broadening the use for such an algorithm by incorporating a wider range of word relationships 
and    by being able to train our algorithm on a
much larger corpora than was previously available  our package will make use of the stanford

fiparser  for creating sentence tree structures from
text   wordnet  for supplying word relationship
example data  among other semantic lexicons  and
various classifier packages to compare and determine what works best for word pair induction  including svms  nave bayes  and logistic regression  among others 

 

design

the design layout of the generalized automated
relationship inductor  gari  see figure    is
based on the following process 
given a general semantic relationship  an initial training set of both known to be related and
known not to be related word pairs for a given relationship and a large corpus of text  gari will
use the training pairs to discover patterns in sentence tree structures derived from the corpus that
indicate the relationship  the two tree structures
we tested were the context free phrase structure
trees  also known as penn trees  and typed dependency trees 
once such patterns are discovered  a classifier
is trained to predict whether an arbitrary pair of
words occurring in the corpus is related by the
given relationship  this process will allow the
discovery of yet unknown instance pairs of the relationship 
the newly discovered  or induced  pairs
will then be added to the training set and the
above process will be repeated in a chain reaction fashion  see section feedback loop   the
process continues until we observe a reduction in
the quality of the induced results 

   

component libraries

we have designed gari as a set of component
java libraries  these component libraries may either be used separately or combined as a whole 
our design consists of four principal packages 
stanfordparser  relationshipverifier 
feature  and classifier  the packages are
combined to provide garis core functionality 
alternatively  if the user wishes to exploit them
individually  they may be used separately 

figure    layout of gari

   

package stanfordparser

given a corpus of plain text  we use the stanford
parser to generate a corresponding set of either
penn trees or typed dependency trees  we then
find the dependency path between pairs of relevant words for tree  for example  the path connecting two nouns in the case of the hypernym relationship   this package interacts with the stanford parser java api 

   

package relationshipverifier

this package interfaces gari with sources that
provide the initial training data needed to initiate
the induction process  it contains functionality to
interface gari with wordnet  as well as interfaces that would enable gari to obtain initial
training data from alternative semantic lexicons 
and other data sources including plain text files 
the interface with wordnet interacts with the
wordnet java api  the wordnet interface generates pairs of words from wordnet that are
known to have and known not to have a given relation  this set of word pairs is pruned to those
that exist in the corpus  and the resulting set is
saved to disk for later use 
this is a flexible part of the system  as it allows the user to plug in any semantic lexicon
that contains example word pairs for a given relationship  given the relation specified by the user 
any word relationship contained in a single sentence can be learned  we have  as default functionality  supported relationships of antonyms 
holonyms  hyper hyponyms  meronyms  participles  and synonyms 

fi   

package feature

once we have converted our text corpus into tree
structures and we have obtained the initial sets of
training data  we then extract relevant sub trees in
the dependency trees that indicate patterns relating
the training pairs  the sub trees are then used to
derive generic indicative patterns  that are independent of the specific training pair instances that
discovered the pattern   we then use an appropriate subset of all such patterns discovered to define
a set of features that indicate  given a pair of
words  the frequency of occurrences in the text
corpus of the set of words with respect to each
specific pattern  hence  for any pair of words in
the text corpus  we derive a feature vector that is
representative of the frequency of occurrences of
the pair of words with respect to the patterns  note
that we generate feature vectors for both positive
and negative training pairs  this is the most cpu
intensive package  as there are thousands of trees
that must be analyzed  therefore  part of this
package is threaded so as to make use of a userdefined number of cpus 

   

package classifier

after deriving the feature vector for each training
pair  we use the feature vectors to train the classifier  using word pairs from the corpus and their
corresponding feature vectors  we then use the
classifier to induce new pairs  the classifiers
already implemented in the package are logistic
regression  nave bayes  support vector machines   and a nave entropy score  classifier
that we designed 
this  again  is another flexible part of the system in that the user can plug in any classifier
he she would like to use 

   

feedback loop

the induced pairs generated by the classification
stage are added to the set of training pairs and the
process is repeated  figure     this loop can be
run until degradation in quality of relation induction is observed 

figure    feedback loop showing how newly found
word relationships are used to retrain the classifier 

   

additional packages

in addition to these four principal packages  we
have also included a skeleton package gui that
may implement a set of front ends that would allow exploiting gari s functionality in a userfriendly manner 

 

methodology and results

   

deriving training data

we derived positive training pairs by generating
all of the word pairs in wordnet with the given
relation and then removing the pairs that do not
occur in the corpus  we derived negative training
pairs by generating all possible noun pairs from
the corpus  and then testing them on wordnet to
filter out pairs that were related by the selected relationship 

   

deriving patterns

we used two types of patterns in our testing  patterns based on conventional dependency trees and
patterns based on context free phrase structure
trees  also known as penn trees   in both cases we
used a tree search  combined with several pattern
matching steps to derive the pattern  we also included several preprocessing stages including
word stemming  the penn tree method seemed to
produce more consistent patterns than the dependency tree method and also required less preprocessing  however  the penn trees required significantly more physical memory during the computations and produced longer pattern strings 

 

using libsvm     
this classifier works as follows  for each relation pair  each pattern
in the feature vector is assigned a score in the range         depending
on how indicative anti indicative the pattern is of the given relationship  where zero indicates little information about the pattern for that
relation pair   classification is done by summing over the patterns
scores weighted by the patterns frequency of occurrence 
 

 

we used a corpus consisting of nuclear development abstracts from
     through      from the center for nonproliferation studies
 cns  at the monterey institute of international studies  the corpus
was obtained through the aquaint program and contains around
        sentences 

fiwe also found that our most frequent patterns
were well known patterns  for example  for the
hypernym relation  the two most common patterns
were word  and other word  and
word  such as word   which agree with the
commonly used hearst     patterns for hypernym
relation induction 

   

generating features

we used the number of times each unknown relation pair matched each pattern as the feature vector  with one feature for each pattern  

   

nave
bayes
w  margin
nave
entropy
w  margin
svm
log reg
w  margin

deriving useful patterns

we used the n most commonly occurring patterns
as our useful patterns  restricting the number
of patterns  i e   the number of features  mitigated
the effects of overfitting  but also resulted in some
word pairs not being discovered later on  to find
the optimal value of n  we ran our classifiers for
different values of n  where n was swept between
  and      and recorded the accuracy and recall
from each run 
varying n had the most effect on recall  recall generally decreases as n increases  this is
most likely due to the fact that as n increases  our
classifier model becomes more complex and starts
to overfit the training data  in turn  more relation
pairs in the test set fall into the unknown category  decreasing the recall  recall is a function of
the size of the test set minus the number of relation pairs classified as unknown 
however  since using too low of values for n
would lead to low accuracy  a tradeoff must be
made between these two factors  in our tests  for
training on     penn trees  we found n      to
yield reasonable accuracy without a significant
drop in recall 

   

classifier

testing

we successfully ran our algorithm for training
set sizes up to      penn trees  however  we
were able to compile significant statistics for testing done only with    to     penn trees  about
         word pairs  in the training set  full testing on larger training sets resulted in infeasibly
large computation time 

positive
relation
accuracy

negative
relation
accuracy

recall
 pos neg 

     

     

             

   

     

             

     

    

            

     

     

             

table    accuracy and recall results for each classifier
on a single    sentence corpus  recall calculated for
each labeling as the number of induced pairs not in the
training set divided by the training set size 

we learned from our experiments that using
the penn trees resulted in much better test accuracy than with the dependency trees  we tested
our system by hand tagging about      randomly
chosen word pairs from the corpus for the hypernym relation  we then compared these handtagged pairs against the induced relation pairs  
although our training sets and test sets were
limited in size  we can still infer information
about the classifiers relative to each other  since
all classifiers had more than an order of magnitude ratio in negative to positive training pairs  all
classifiers induced a large number of negative relations that were virtually all correct  the accuracies in table   are for correct labeling of relation
pairs classified as yes by each classifier  as
compared to our hand labeled list 
nave bayes had the best accuracy of all the
classifiers  it is well suited to this sort of application  where we have very sparse feature vectors
consisting of small positive numbers  naive
bayes had to be modified for this use by putting a
margin between the yes no labelings  leaving
a group of unknown labelings in the middle 
generally  accuracy tends to decrease as recall
increases  in addition to characteristics of the individual classifiers  recall and accuracy tend to
have an inverse relationship because the accuracy
is partially set by the classifiers margin  as the
margin is decreased  the recall will increase  but
the accuracy will be lower because pairs that had
 

accuracy was calculated only from the subset of induced pairs that
were in the hand tagged set 

fi   

inducing new relation pairs

for each iteration of the feedback loop  the algorithm was able to induce around    of the original training set size  using a larger training set and
small feature vector with the nave entropy score
classifier   in other words  on each iteration  the
number of relation pairs classified as yes or
no  i e   not unknown  increased the training
set for the next iteration by    
despite these very positive results  the induction step was also very processor intensive and
time consuming  and hence prevented us from
compiling a larger set of test results  however 
parallelizing several computation steps  in package feature  did give some improvement in performance  we threaded our algorithm to work
across multiple machines  and were able to
achieve significant speedup for up to   machines
 figure    

   

terminating feedback loop

each classifier terminates its feedback loop when
the classifier begins to induce incorrect relation
pairs  this can be observed by the fact that the
classifier will start incorrectly classifying known
positive and negative relation pairs  due to computational complexity  we were not able to run the
feedback loop enough iterations to induce a good
breaking point  however  this can be set by the
user to meet accuracy constraints 

 

future work

there are   primary objects that we recommend as
the next steps for this project     generalizing the
relationship extraction algorithm  and    optimizing the algorithm for speed  generalizing the system includes making it capable of inducing
across paragraph and across multiparagraph relationships  certain word relationships  such as
verb verb relationships  are difficult to induce
 

from single sentence parsing because multiple
verbs in a sentence do not occur as frequently as
multiple nouns 
time to complete  sec 

been previously classified as unknown are now
being set as yes or no 
logistic regression gave the lowest accuracy
of all the classifiers  in general  it is not well
suited to this type of application due to its hessian
matrix being very sparse  

we used a modified version of logistic regression where we set all
zero values in the hessian to very small positive values 

   

no par allelization

   

 

 

 

   
   
   
 
                                  
         

thread

figure    execution time

our other recommendation is to work on optimizing the code such that it is not as processor
intensive  this specifically applies to the feature package  which in the current state would
require several cpus to be feasible in a real world
application  this optimization most likely would
include using a different form of storing the decision trees such that extracting patterns and new
word pairs from the corpus is less cpu intensive 

 

acknowledgements

we would like to thank stanford graduate student
rion snow  who advised us and shared his work
with us during this project  we would also like to
thank profs  dan jurafsky and andrew ng for advice they gave us along the way 

 

references

    snow  r    jurafsky  d    ng  a  y        
learning syntactic patterns for automatic hypernym discovery  nips      
    de marneffe  mc    maccartney  b    manning 
cd        generating typed dependency parses
from phrase structure parses  proceedings of  th
international conference on language resources
and evaluation  lrec       genoa  italy 
    hearst  m         automatic acquisition of hyponyms from large text corpora  proc  of the fourteenth international conference on computational
linguistics  nantes  france 

fi