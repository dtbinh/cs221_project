topic correlations over time
david haley dhaley    david hall  dlwh    and mike rodgers  mikepr cs 
abstract
d   the number of documents in the corpus
topic models have proved useful for analyzing
large clusters of documents  most models developed  however  have paid little attention to the
analysis of the latent topics themselves  particularly with regards to change in their correlation
over time  we present a novel  probabilistically
well founded extension to latent dirichlet allocation  lda  which can explicitly model topic
drift over time  using this extension  we analyze
the correlations of topics over time in a corpus
of acl papers 

 

introduction

a well documented statistical revolution in natural language processing  nlp  took place in the early
    s  gaz     leading to a very rapid increase in the
use of statistical methods to build natural language systems  obvious examples include machine translation
and parsing  which have become largely synonymous
with their statistical variants  however  no work has
been done to analyze this shift  despite the recent proliferation of topic models designed for large document
corpora 
in this paper  we develop a model to examine this
feature of topic shift over time  we then use the model
to analyze the texts of about        papers from the association of computational linguists  available via the
acl anthology  bir   this corpus covers nearly all papers from      until       we first automatically extract
a number of topics  we then infer correlation among the
most salient of these topics  finally  we present the results of our analysis  concluding with promising directions for future work 

 

previous work

a significant amount of attention has been given to topic
modeling  and most recent work in this area has followed

k   the number of topics in the corpus
ni   the number of words in document i
i  dir    i      d 
k  dir    k      d 
zi n  mult i    i      d   n      ni  
wi n  mult zi n  
figure    latent dirichlet allocation
from lda bnj     which merits a brief review here 
 figure     lda is a mixture model of latent topics z
in documents d  which have words w  each document is
characterized by a multinomial distribution over topics
d   and each topic is characterized by a multinomial distribution over words z   finally  both the d s and z s
have  usually symmetric  dirichlet priors with hyperparameters  and  respectively  these hyperparameters
can be fixed or sampled from a gamma distribution 
thus far  this model has no explicit representation
of time  all documents are exchangeable  several models have been proposed to remove this assumption  most
similar to lda is the topics over time model  tot 
 wm     tot assigns a time stamp t in the range      
to each document  and repeatedly samples the timestamp
according to a topic dependent beta distribution z   to
conserve space  we forgo reproducing the entire specification  simply adding the extra parameter here 
ti n  beta zi n    i      d   n      ni  
representation of time as a continuous variable has several useful properties  first  it is a rather minimal addition to the model  making it easy to implement  moreover  it avoids the problem of markovization  there is
no need to divide time into a specific number of epochs 
this negates the need to determine the proper dividing
line for documents and the need to decide whether or
not to cluster natural epochs  however  it makes finding

fit     epochs

t     epochs

dt     documents in the corpus for the tth epoch

dt     documents in epoch t

k     topics in the corpus

k     topics in the corpus

nt i     words in document i for epoch t

nt i     words in document i in epoch t

t  norm t      i 

t i  gamma       t      t    i      k 

t  norm t      i 

t i  dir t    i      dt  

t i  norm t     i   i      dt  

k  dir    k      dt  

zt i n  mult  t i     i      dt    n      nt i  

zt i n  mult i    i      dt    n      ni  

wt i n  mult  t zi n   

wt i n  mult zi n  

figure    dynamic topic model

figure    unmarkovized time model

the change in relationship between any two topics over
time much harder 

a normal distribution  we opt instead to concentrate on
the hyperparameter t   which we will take as representative of the mean topic strength  expectation of t  
for a given epoch t   strictly  t is a multiple of the expectation of the dirichlet  but normalizing it solves the
problem   first  we remove the symmetry of the dirichlet distribution for   allowing the components of the s
to vary  finally  we add a prior on these s  which we
call   the specification for this non markovized model
is in figure     is the scale parameter of the s prior 
this ensures conjugacy  but still maintains the same expectation for the s   this parameter can be sampled
from a gamma distribution  though in our experiments
we found that     produces good results 

on the other extreme  the dynamic topic model
 dtm  represents documents as points on the topicdistribution simplex about a centroid  bl     the dtm
uses a normal distribution and a normalization function
to constrain the points to be on the simplex  moreover  each epochs centroid is sampled from the previous
years centroid  enabling drift of topics from epoch to
epoch  similarly  the vocabulary distributions for each of
the topics are chosen as a centroid  the model is specified fully in figure    this model requires a substantial
change to the inference procedures  and it requires renormalizing topic and vocabulary distributions by a transform function  
finally  we mention briefly the correlated topic
model  blar   which is more or less identical to the
dtm  except that there are no times  only correlations
between topics  in principle these models could be combined to give the desired results  but with great mathmatical overhead  in this paper we propose models that
accomplish the same result but require slightly less complicated machinations 

 

temporal model specification

in this section we develop an unmarkovized  but still
discretized  dynamic model with minimal disruption to
lda  like the dtm  we split time into discrete epochs
 which  for now  are exchangeable   but instead of using

 

topic correlations

we considered a number of techniques for determining
correlations  but in this section we develop an extension
to the utm  or  simply  lda  that has support for modeling topic correlations as part of the inference  however  going forward  we will use correlation in the informal sense  in particular  we will model correlation
as confusability  with what probability can one topic
be confused for another topic 
   

the correlated dirichlet distribution

in this section  we briefly discuss a new distribution 
which we term the correlated dirichlet distribution
 
we omit the details of deriving the gibbs sampler in this paper
due to space constraints 

fit     epochs
dt     documents in epoch t
k     topics in the corpus
nt i     words in document i in epoch t
t i  gamma       t      t    i      k 
btt i  dir i    t      t    i      k 
t i  cdir t   bt    i      dt  
k  dir    k      dt  
zt i n  mult i    i      dt    n      ni  
wt i n  mult zi n  
figure    correlated unmarkovized time model
 cdd   to begin  we note that the dirichlet has neutralj
  unfority  that is  if    dir    then i   
i
tunately  this property indicates that the dirichlet has no
natural means to specify the strength of correlation  formally or informally  between any of the components of
its samples  therefore  we propose the cdd  which explicitly adds a mixture component to the dirichlet distribution  intuitively  we can think of samples from a cdd
as samples from a standard dirichlet distribution that
have been distorted as the result of one step on a random
walk specified by some stochastic matrix  formally  we
say that if b is a stochastic matrix  and
    dir  
    b
then   cdir   b   we should note that the cdd
bears some resemblance to the dependent dirichlet distribution in  llwc     however  the requires that the
matrix b take a specific form relating to the markov transition probabilities defined by their problem  we specify the matrix as a parameter to the distribution  in this
sense  the cdd is a generalization of the ddd 
   

the correlated unmarkovized time model

integrating the cdd into our model is fairly straightforward  the correlated unmarkovized time model
 cutm  is specified in figure    in particular  we introduce the cdd and a diagonally biased prior on the
rows of b  thus  we require that each of the rows of b

be a probability distribution  we also introduce a strong
diagonal bias in the  hyperparameters to help ensure
that topics do not get washed out in the mixing  that
is  bii  bij for bii     bjj for inference  however 
this model becomes slightly more difficult  we cannot
use gibbs sampling to sample the matrix b  though with
fixed b it is straightforward to use gibbs sampling for  
to update b  we use the metropolis hastings algorithm
 has    to update the columns of b individually  details
of the proposal distributions we used are discussed in the
appendix 

 

results

we implemented both the utm and cutm and ran the
model on        papers of the acl anthology over a
wide range of topics until convergence  for k      the
utm ran in about     the time of the cutm  in the interest of space  we report only topics from the cutm 
figure   lists the top    words chosen by mutual information for the top   topics  by word count  as well as
topic    which we use to illustrate correlations  we caption the topics with titles based on our interpretation of
the topics distribution over the vocabulary 
figure   is a smoothed plot of the s for those topics over time  as you can see  the graph is incredibly
jagged  leading us to conclude that treating epochs as
exchangeable should be reconsidered  however  there
is a reasonable correspondence between peaks on the
graph and the topics themselves  topic    peaks in the
early     s  and the topics themselves are constrained
to dates in the     s  and words we associate with papers from that era  topic   has peaks exclusively in
the     s  which corresponds to the words listed  we
see      as a prominent year   we also mention that
            and      also are in the top    words for
topic     topics    and    have peaks later on  and
have words like probability  evaluation  and performance  words we associate with the statistical revolution 
figure   is a smoothed plot of the directional confusability of topic   with topic    and of topic    with
itself  again  we note that there is a considerable amount
of variance in both graphs  indicating that the b matrices
should also be markovized  however  we see that topic
   the     s  and topic         s  are most corre 

fitopic       s parsing
university
grammar
class
trees
semantic
language
process
constraints
parser
event
np
syntactic
set
elements
    
wordnet
number
utterance
book
topic        s
computer
object
science
speech
    
program
network
discourse
report
structure
objects
federal
act
    
    
information
research
semantic
location

topic    statistics
features
model
document
task
data
lexical
language
precision
ranking
verb
score
relation
probability
research
semantic
evaluation
labeled
goal
dialogue
topic    stat  parsing
discourse
target
lexicon
information
verb
constraints
machine
language
state
similar
rule
entity
clause
feature
head
performance
definite
module
function

figure    topic    words for   topics  chosen by mutual
information

figure    t i over time for   topics
lated in the     s  again  this matches our expectation 
the older two topics become  the more confusable they
are  these topics tend to arise in historical papers  or
they are used in papers that cite older work 

 

conclusion and future directions

we have developed an unmarkovized model for directly
learning topic prominence and correlation over time using an additional level of hyperparameters and a stochastic matrix  respectively  we then examined the results
of fitting that model  and observed that the topics and
their strengths were in fact meaningfully related  we
also looked at confusability of two topics and found a
similar relation there 
however  much more work is needed  the assumption of the exchangeability of epochs is clearly incorrect 
the jaggedness seen in the graphs is too pronounced  on
the other hand  that we saw localized peaks and not several disjoint eras indicates that the topics being extracted
were in fact located in certain eras  so this model is still
valuable  regardless  for practical purposes  it is probably best to condition epochs on their ancestors 
more work should also be done to compare this
model with lda and the dtm in terms of log likelihood  moreover  as with all topic models  a better metric
of performance is necessary  log likelihood is useful for
training  but still we could use a better method of evaluating these systems 

figerald gazdar  paradigm merger in natural language processing  pages        cambridge university press 
     
w k  hastings  monte carlo sampling methods using markov
chains and their applications       
xuejun liao  qiuhua liu  chunping wang  and
lawrence carin  neighborhood based classification 
http   www ee duke edu lcarin ddd talk yale   pdf 
     
xuerui wang and andrew mccallum  topics over time  a
non markov continuous time model of topical trends 
     

a
figure    t ik  confusability  over time for   topics
finally  there are still other ways to look at correlations  one could argue that correlations should be
inferred post hoc  and thus an approach like independent component analysis should be useful  some initial
analyses  which do not fit here   seem to find reasonable
post hoc topics 

a  

derivation of proposal distributions for
metropolis hastings sampling
resampling 

a highly effective proposal distribution for  can be
computed fairly easily  we proceed by relaxing the posterior  let  z be the topic counts for a document 
y
y
 k  k
p  z   p     
  bk  zk
k

k

yx
 
 
i bki  zk  k  k  
k

i

acknowledgements

y

 k bkk  zk  k  k  

we would like to thank dan ramage  chris manning and
dan jurafsky for their help in thinking through the issues
here  we would also like to thank hal daume for the
hierarchical bayesian compiler  dau   which we used
to implement early prototypes of the utm 

y

 k  zk  k  

k

references
steven bird  association of computational linguists anthology  http   www aclweb org anthology index  
david blei and john d  lafferty  dynamic topic models 
icml       
david blei and john d  lafferty  a correlated topic model of
science  annals of applied science  to appear 
david blei  andrew ng  and michael jordan  latent dirichlet allocation  journal of machine learning research 
                
hal daume 
hbc  hierarchical bayes compiler 
http   hal  name hbc       

k

  dir     z 
the step in which we dropped the summation takes advantage of the assumption that bkk  bkj   for k    j 
which we enforce on our priors  this result matches our
intuition  draws from a dirichlet distribution should be
roughly the same as draws from a correlated dirichlet
distribution with a strong diagonal bias 
a  

resampling btk

resampling the rows of b is a bit different  while a
similar calculation could be used  in practice we find that
is it is often better to choose a weak diagonally biased
prior and weight the current sample more highly when
conditioning the dirichlet  that is 
 

t
bt
k  dir k   cbk  

where c is large 

fi