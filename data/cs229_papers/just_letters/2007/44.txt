patent cases docket classification
ioannis antonellis



panagiotis papadimitriou



abstract
we contribute to the intellectual property litigation
clearinghouse  iplc  project by providing an extensive
experimental evaluation of two classification techniques 
namely classification trees and support vector machines 
we focus on dockets belonging to the claim construction order class and we build classifiers that achieve
up to     precision with     recall  this provides
a     improvement on the best rules based domainspecific classifier that iplc possess 
 

problem description

our work was a contribution to the intellectual property litigation clearinghouse  iplc       iplc aims
to be a comprehensive online information source on ip
lawsuits and will host general statistical information  as
well as text searchable dockets  complaints  select motions  judicial opinions  and related data 
the dockets that are going to be searchable comprise of     lines of human written text and are accompanied by some pdf documents with       pages
of images and text  these dockets are indexed by the
patent case they refer to and can be classified to approximately    different classes such as order  motion 
patent  judgement  etc   in fig    we provide an example of the dockets that belong to a specific patent
case  the information that the docket text conveys
is unstructured  since there are no rules for writing a
docket  people seem to follow general conventions for
the docket text of particular classes  but this is not true
for all the cases  for example  most of the docket texts
of class answer start with the word answer in uppercase letters  this convention results in an easy rule
for the identification of dockets from a particular class 
however  there are classes where simple rules cannot be
derived  an example of such a class  marksman  is
shown in the same figure 
a human can classify any docket to a class  since
he has access to the pdf documents that are attached to
it  however  since the pdf documents are scanned  we
 computer science dept  stanford university 
antonell cs stanford edu
 electrical engineering dept  stanford university 
ppapadim stanford edu

email 
email 

figure    dockets of a patent case

need to apply some ocr method to get their content
in machine readable format and provide it as input to a
learning algorithm  given the cost of such a procedure
we cannot consider it as feasible and any classification
algorithm should constrain its input to the rest of the
features that are available  the goal of our project
was in the first place to investigate whether there is
enough information in the rest of the features for the
classification of a docket and then build an optimal
classifier if that was possible 
  dataset
    description one of the non trivial classes for
classification is the claim construction order  cco 
class  our dataset consists of      dockets that were
classified by an editorial team      of them are claim
construction order  cco  dockets and the rest     
are dockets of other classes  the dockets of the dataset
refer to      different cases and in general case they are
not related to each other  e g  the non cco dockets are
not responses to cco dockets 
    preparation the dockets preprocessing steps
performed are the following  i  lexical analysis  ii  stopword elimination  that is the removal of very frequent
words such as articles  prepositions  conjunctions  et 
that carry little information about the contents of the
processed dockets  iii  stemming  that is the replacement of all variants of a word with a single common

fi 

symbol
t
b
l
a
n
x
e
f
g
n
p

table    local and global term weighting schemes
name
formula
local term weighting  lij  
term frequency
fij
binary
b fij  
logarithmic
log       fij  
alternate log
b fij       log  fij  
augmented normalized term frequency  b fij      fij   maxk fkj     
global term weighting  gi  
none
 
p
entropy
      j  pij log   pij     log  n 
p
inverse document frequency  idf 
log    n  j b fij   
p
p
gfidf
  j fij     j b fij   
qp
 
normal
  
j fij
p
p
probabilistic inverse
log    n  j b fij     j b fij   

docket we divided our dataset into a training and a
test set  the training set included     of the positive
training examples and     of the negative examples of
the original dataset  and the test set included the rest
of the examples  we performed the partition of the
dataset using reservoir sampling to guarantee the size
of the resulting subsets 

figure    data preparation

stem  iv  index term selection  that is the selection of
a subset of words encountered in the dockets to form
the docket index v  index construction  these steps are
illustrated in fig     the output of this process is a
term document matrix a  each element ij of the term
document matrix a measures the importance of term i
in docket j and in the entire collection  there has been
proposed various term weighting schemes using alternative functions for the local and global weighting for
a term  table   tabulates the various local and global
weighting schemes we considered in our experiments 
another usual practise is the use of normalization for
each docket vector  this normalization factor is used for
the obliteration of bias towards longer documents  for
the implementation of the preprocessing step we used
the matlab toolbox term matrix generator  tmg  
the dimension of our feature vector is n        
after the extraction of the feature vector for each

  baseline approach
the algorithm that is currently used for the classification of the dockets is rule based  these rules derive
from human heuristics and take advantage of domainspecific knowledge  to evaluate the performance of this
algorithm  as well as the classification methods we propose  we use standard performance measures such as
precision  recall and f   measure  we present the corresponding formulas for the classification of cco documents in the following equations 
classified in cco  dockets in cco
classified in cco
classified in cco  dockets in cco
recall  
dockets in cco
precision  recall
f   
precision   recall

precision  

the performance measures for the used classification method and the class cco are shown in eq      
we observe that precision is high and recall is small 
this is so that the classification method can return a few
false negative dockets  an editorial team reviewed the
positive dockets returned by the algorithm and filtered
out the true cco dockets  we use the performance
measures of this classification method as a baseline for
comparison with our proposed methods 

fi 

precisionbase       
     

recallbase       
fbase       

  classification trees
tree based methods for classification and regression
partition the feature space into a set of rectangles and
then fit a simple model in each one  the most common
such model is the constant function  for our problem 
we used cart      a popular method usually used in
regression and classification problems 
the conceptual simplicity of the cart yields its
limitations  for example  the partition of the featurespace into rectangles fails to capture non rectangular
distributions of the data  trees can approximate other
distributions only with a big number of small rectangles that increase the complexity and  consequently  the
variance of the model  there are several proposed methods that handle carts limitations such as bagging    
and boosting      but they are out of the scope of this
paper 
despite their limitations  in our work we used
only naive tree based methods  because of the great
interpretability of the classification algorithm decisions
that they offer  as we discuss below  such trees
reduce the classification problem into binary decisions
on values of feature vectors dimensions  our goal
using classification trees was to gain an insight of
the significant dimensions of the feature vectors that
actually have an impact on classification  since cart
is conceptually simple  we could convey this insight to
people with little or no knowledge of machine learning 
    description in our case we deal with a binary
classification problem  the target variable has value  
if a training example belongs to the class claim construction order and value   if the example does not
belong to this class  our feature vector xi shows the
frequency of the words of our dictionary in docket i  corresponding to the weighting scheme txx as described in
section      
let r  be the feature space of our training examples  in our case r    rn and it contains all m    m
training examples  let p   be the ratio of examples in
r  that do not belong to claim construction order
class  similarly  we define the ration p   for examples
that do belong to the class  in general  for region rk
we define 

  x
i yi     
mk
xi rk
  x
 
i yi     
mk

     

pk   

     

pk 

xi rk

we start the training of our algorithm by classifying
all the training examples of region r  to the majority
value of the response variable y  since most of our
training examples do not belong to class cco  the
most naive classifier would predict that a given docket
does not belong to this class  since it tries to minimize
the classification error   we define the impurity q    of
area r  using the gini index that we show in eq     
for area rk  
     

q k    pk      pk      pk      pk   

the gini index is an indicator of how pure an area is
in terms of the different values that the target variable
takes in that area  we get q k      if all training
examples of one area belong to the same class 
the next step of the algorithm depends on the value
of the complexity parameter cp and the gain q    
 q      q     we obtain if we partition our training set
into two distinct subsets that span regions r  and r  of
our feature space  so that r  r     and r  r    r   
first we explain how we determine the optimal partition
of region r  and then we give more details in the role
of cp  we partition the region r  into regions r  and
r  in a way such that we can determine whether an
element xi  r  belongs either to r  or r  with a
binary decision based on the value of only one specific
dimension of its feature vector  the binary decision is
whether xi s value for the specific dimension is le or  
than one splitting value  we select the dimension and
the splitting value so that the gain q      q      q    
we have in the purity of the new regions r  and r  is
maximized  in our case  the splitting of a region based
on a value of a specific dimension of the feature vector
yields partition of the dockets into two subsets based
on the frequency of one specific word  for example  if
word constru  derived from construct  construction 
constructed  etc   is the word that divides the dockets
into two as pure as possible subsets  we will divide the
dockets in these two subsets  in the best case  where
constru appears in all dockets of our class and no
document that does not belong to our class has the word
contru in it  we will divide the dockets into two pure
subsets and we get the maximum possible gain 
it is easy to see that without penalizing a region
splitting we would stop splitting only in cases where
we have pure areas  since it is easy to prove that we can

fi 
always split an impure region into two  having a positive
gain in the purity of the resulting regions  however 
we limit the number of possible distinct regions by
adding to the impurity of area rk the term cp  hence 
in our case it would be worthwhile to split region r 
into regions r  and r  only if the gain q      cp 
 q      cp   q      cp    q      q      q      cp
was positive 
after splitting a region into two  the next step of
the algorithm requires to find first which region would
yield the maximum gain and then find the optimal
split for this region  in this optimization problem we
should take into consideration that a region splitting
that seems redundant may give the opportunity for
subregions splitting with great gain  to deal with
the recursive nature of the optimization problem we
construct the classification tree in a bottom up fashion
rather than a top down that we described so far  we
build initially a very large tree and then we prune it in
such a way so that we maximize the purity of the nodes
taking into consideration the complexity parameter that
is associated with each terminal node 
a slight variation of the algorithm above uses a
weighted version of the gini index of eq       this
variation is used to penalize more the impurity of
regions with respect to one class than the other  we
show the updated formula in eq      
     

q k    l  pk      pk      l  pk      pk   

the new formula can result in different decisions for
node splitting  since if l    l    for example  we obtain
a greater gain if we split areas where the majority of
the examples belong to class   and  hence  we have
misclassified examples of class      probably give more
detail  
the algorithms finally returns a decision tree where
each branch illustrates a decision on specific dimension
of the features vectors  as a result  the dimensions of
the feature vectors that do not appear in any branch
are completely ignored during the classification of a
new feature vector  in our context  that means that
docket words that do not appear on the tree do not play
any role in the classification of new docket that does
not belong to our training set  hence  the words that
actually appear in the tree branches can be considered
as the significant ones in our classification problem 

   

    

   

    

   

    
   

precision
recall
f
 

   

 

l 

   

 

   

 

figure    performance measures of classification tree
for varying l 
fixed and we varied the parameter l  in the interval
         for each value of l  we varied the complexity
parameter cp to all possible values that are greater than
        these values are finite if we consider only one
value in every interval that results in a different pruning
of the large tree we initially construct  finally  for each
l  we kept only the tree  an the corresponding value of
cp  that had the smallest misclassification error on the
test set 
    results we show the performance measures for
the optimal tree we obtained for each value of l  in
fig     the x axis of the plot show the values of l  an
the three curves correspond to the precision  recall and
f performance measures  we see that the value of l 
that maximizes the f measure is      we see that as l 
increases we see an increase in the precision  because
our classification tree classifies only the pure nodes as
cco and they tend represent approximately     of the
total number of cco dockets  the change of the weight
l  cannot change the classification decision for these
regions  sine there are no misclassified dockets that are
not cco  on the other hand  recall decreases because
we misclassify more and more positive examples as
negatives in non pure nodes  since this is not penalized
because of the increase of l  with respect to l   
in eq      we provide the performance measures
for the optimal tree  we see that with the appropriate
tuning of this simple classifier we get an improvement
of        with respect to our base case 

    tuning classification trees we fitted different classification trees to the data of our training set and
test their performance measures in the classification of
the test set  to construct different trees we varied the
     
parameters l  and cp 
in particular  we considered the value of l      as

precisiontree

 

    

recalltree
ftree

 
 

    
   

improvement

 

      

fi 
precision recall f  values for svm with different cost factors  linear kernel 

   
  

 

precision recall f  values

  
motion     

  

motion     

  

 

  
  

hear     
hear     

  
  
recall txx
precision txx
f  txx

  
 

 

   
 
   
 
   
 
   
cost factor by which training errors on positive examples outweight errors on negative examples

judg     

opinion     

judg     

opinion     

 
 

 

 

precision recall f  values for svm with different cost factors  linear kernel 

   

brief     
brief     

  
 

precision recall f  values

  

 

  
  
  
  

figure    pruned optimal tree

  
  
f  tpx
precision tpx
recall tpx

  
 

 

   
 
   
 
   
 
   
cost factor by which training errors on positive examples outweight errors on negative examples

 

we show a pruned version of the optimal tree in fig    
the words that were actually used in the construction
of the optimal tree are  brief  constru  coordin  deni 
disput  file  hear  hrg  judg  motion  manag  notic 
opinion  parti  patent  plaintiff  schedul  set  staff 
stipul  strike and summari  we see that out of the
approximately      dimensions of our feature vectors 
classification trees achieve an       by utilizing only   
of them 

 

  support vector machines  svm 
    description support vector machines  svms 
are a set of related supervised learning methods used
for classification and regression  they belong to a family of generalized linear classifiers  a special property
of svms is that they simultaneously minimize the empirical classification error and maximize the geometric
margin  hence they are also known as maximum margin
classifiers  support vector machines map input vectors
to a higher dimensional space where a maximal separating hyperplane is constructed  two parallel hyperplanes are constructed on each side of the hyperplane
that separates the data  the separating hyperplane is
the hyperplane that maximizes the distance between the
two parallel hyperplanes  an assumption is made that
the larger the margin or distance between these parallel
hyperplanes the better the generalisation error of the
classifier will be 

precision recall f  values for svm with different cost factors  linear kernel 

   
  

precision recall f  values

  
  
  
  
  
  
  
f  tix
precision tix
recall tix

  
 

 

   
 
   
 
   
 
   
cost factor by which training errors on positive examples outweight errors on negative examples
precision recall f  values for svm with different cost factors  linear kernel 

   
  

precision recall f  values

  
  
  
  
  
  
  
f  lxx
precision lxx
recall lxx

  
 

 

   
 
   
 
   
 
   
cost factor by which training errors on positive examples outweight errors on negative examples

 

precision recall f  values for svm with different cost factors  linear kernel 

   
  

precision recall f  values

  
  
  
  
  
  
  
f  lpx
precision lpx
recall lpx

  
 

 

   
 
   
 
   
 
   
cost factor by which training errors on positive examples outweight errors on negative examples

 

figure    comparing precision recall for svm classifiers with different cost factor and for five different indexing schemes   i  term frequency  txx   ii  term frequency probabilistic inverse  tpx   iii  term frequency  
inverse document frequency  tix   iv  logarithmix  lxx 
 v  logarithmix probabilistic inverse 

    tuning svms we performed a set of experiments using support vector machines  and specifically
thorsten joachims svmlight package  as binary classifiers for dockets  we used a linear kernel function and
we varied the cost factor  the factor by which training

fi 
errors on positive examples outweight errors on negative examples  we report results for cost factors in the
interval          with a step of      we did not modify
any of the parameters of the svms we trained based
on the test set results  so we did not do a second level
development set test set split 
also  for each docket we used all combinations of
local and global term weighting schemes from table  
to derive the vector space representation of a docket 
    results figure   illustrates the precision  recall
and f   measure for five different combinations of term
weighting schemes and variations of the cost factor used
in svms  overall we noticed that simple term frequency
can yield the maximum f   value  specifically the
performance results in that case are 
precisiontree

     

recalltree
ftree
improvement

 

      

      
      
      

this could be justified from the relatively small size
of the content of each docket  notice that overall svms
improve the rules based baseline by a factor of     
 

conclusions

in this paper  we focused on building classifiers for
the dockets belonging in a specific class  this was
done  since iplc provided us training data for only this
class  we provided experimental evidence that we can
efficiently classify dockets of non trivial classes using
only the docket text  our optimal classifier yielded
an improvement in the current classification method
      however  we think that we can build upon
our classification techniques and further improve the
classification accuracy by taking advantage of docket
feature that we were not provided with  these features
include the names of the attached documents to each
docket and the sequence of docket classes that are
formed in a patent case 
future work includes the investigation of classification methods that take into consideration the aforementioned additional features 
references
    l  breiman  j  friedman  r  olshen  and c  stone 
classification and regression trees  wadsworth and
brooks  monterey  ca        new edition     
    leo breiman  bagging predictors  machine learning 
                   

    j  friedman  greedy function approximation  a gradient boosting machine       
    mark a  lemley and j  h  walker  intellectual property litigation clearinghouse  data overview  ssrn
elibrary       

fi