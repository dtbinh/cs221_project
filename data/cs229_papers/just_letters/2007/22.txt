adaptive optimization of hyperparameters in l  regularised
logistic regression
ahmed abdel gawad

simon ratner

ahmedag stanford edu

sratner stanford edu

december         
abstract
we investigate a gradient based method for adaptive optimization of hyperparameters in logistic
regression models  adaptive optimization of hyperparameters reduces the computational cost of selecting good hyperparameter values  and allows these optimal values to be pinpointed more precisely  as
compared to an exhaustive search of the hyperparameter space 

 

introduction

supervised learning methods often include many additional hyperparameters besides the parameters being
optimized  such as the regularization parameters in regularized methods  and kernel parameters  the effectiveness of a particular learning algorithm is strongly influenced by the choice of such hyperparameters  a
common method for selecting good hyperparameter values is via an exhaustive grid search over the hyperparameter space  while this yields acceptable results  it is often computationally expensive  and outright
infeasible if the number of hyperparameters is large 
recently  several methods have been proposed for optimizing hyperparameter selection  including crossvalidation optimizations for kernel logistic regression      svm     and conditional log linear models      these
methods rely on re training the model in the inner loop of the algorithm  after each hyperparameter update 
further  gradient based methods have been successfully used in neural networks to optimize regularization
parameters concurrently with the parameters of the model         in this paper  we investigate applying similar
gradient based techniques for adaptive hyperparameter optimization to l  regularized logistic regression 

 

preliminaries

we consider l  regularized logistic regression  a solution to which maximize the log likelyhood c over the
training data set t  
 t  
x
c    
log p y  i   x i            
   
i  

additionally  we use a stretched sigmoid approximation to the step function s  x           exp   t x  
to define a validation function  v   as a smooth approximation to the generalization error over the hold out
validation set v 
p v 
 
 i 
 i 
v       v 
i     s  x      y   
   
p
 v 
 
 i 
 i 
 i 
   v 
i   s  x       y     y
 

fiwe use newtons method for optimizing the model parameters 
 k       k   hc    k    c  k   

   

where  k  is the value of  after k iterations of the algorithm  let diagonal matrices    diag            and

t
w   diag h k   x i       h k   x i      i            t      and input matrix x   x          x n    i            t    
then the gradient of the objective function can be written as
 c  k     

 t  
x

 y  i   h k   x i    x i     k 

   

i  

and the hessian matrix is
hc   k      x t w x   

   

training data set t and validation data set v are chosen to be non overlapping subsets of the available data 

 

derivation of hyperparameter update rule

we similarly use newtons method to update the regularization parameter    with the validation function
used in place of the objective  note that  k    is a function of the updated model parameters  k      while
 k  is treated as a constant 
 k       k   hv    k      k     v   k      k    

   

derivation of the gradient and hessian in equation     is based on the method in     

   

gradient

the gradient of the validation function with respect to  is given by 
 v   k      k      


  k     t   v   k      k    
 k 

   

differentiating     with respect to  k     
 v 

 v   k      k      

  x
  x i       y  i   s  x i       s  x i    
 v  i  

differentiating     with respect to  k   







   k 
   k 
 k   
 k 
 k 
 
 
 

h
 
 

c 
 

h
 
 

c 
 


c
 k 
 k  c
 k 

   

   

to find the derivative  k  hc    k     we use the property of matrix inverse hc   k   hc    k      i  differentiating with respect to  k   






   k 
   k 
 k 
 k 
hc     hc       hc    
h        
    
 k 
 k  c

 

fidifferentiating     with respect to  k   

hc   k       i
 k 

    

h
i 

   k 
   k 
h
 
 
 
 
h
 
 
c
 k  c

    

from           and      we now get 


 c  k        k 
 k 



  k         hc    k    hc    k    c  k       k 
 k 
   hc    k    k   

    

    

substituting     and      back into      we arrive at the equation for the gradient of the validation function
with respect to  
 v  

   

 k   

 

 k 

 v 
  h    k   k    it x
hc    
  x i       y  i   s  x i       s  x i    
     
 v 
i  

    

hessian

the hessian of the validation function with respect to  can be written as 
h
    k   k    t i p v 

 
 i 
 i 
 i 
 i 
h    
hv   k      k         v 
 k 
i     x      y  s  x      s  x   
h  p c
i
 v 
   k   k   

 
 i 
 i 
 i 
 i 
  v 
 
i     x      y  s  x      s  x    hc  
 k 
using      and      
    k   k    t

hc    
 
 k 
 


hc    k   

h k 
    k   
  hc    



 k      hc    k     k   k   
it
 k   



    

t
    

returning to equation       the remaining differentiation can be obtained through the chain rule 
p v 

 i 
 i 
 i 
 i 
i     x      y  s  x      s  x   
 k 
p v 


 k    t
 i 
   k   
   k    i     x      y  i   s  x i       s  x i    
    
p v 
  x i       y  i   s  x i       s  x i    
  k    p i  
 v 
  i      x i t x i       y  i   s  x i       s  x i         s  x i    


substituting      along with      into       we get the final form of the hessian of the validation function
with respect to  
    k     k    t p v 
 
 i 
 i 
 i 
 i 
hv   k      k        v 
hc     
i     x      y  s  x      s  x   


t
 
   v 
h     k    k   
hpc
i

 v 
   i t  i 
 i 
 i 
 i 
 i 


x
x
  

 y
 s
 x
   

s
 x
    

 s
 x
  
hc    k    k   



i    
    
the quantity hc    k    is already available from the parameter update step  so this calculation can be
performed efficiently 
 

fi      v         

             v        

 

 

 

 

 

 

 

 

 

      v       

 

 

 

 

            v        
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

        v          
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

        v        

 

 

 

 

 

      v        

 

 

 

 

 
 

 a    d  min error at               

 

 
 

 

 

         v        

 

 
 

 

 
 

          v        

 

 

           v        

 

 

 
 

 

 

      v          

 

 

 
 

 

 

 

 

 
 

 

            v        

         v          

 

 

 
 

 
 

          v          

 

 

 

 
 

 

           v          

 

 

 

 

 
 

             v        

 

 

 

 

 

 

 

 

 b    d    plotted   min error at              

figure    grid search over the hyperparameter space in  a  two  and  b  five dimensional data set 

 

results

we compare the accuracy and performance of our algorithm to a grid search over the logarithmic space of
hyperparamer values  such a grid search over  is shown in figure    where each graph corresponds to
training using a certain constant value of   the data sets we used for these results are generated from
multi variate gaussian distributions with two label classes  we added a small amount of noise to the data to
make the use of regularization beneficial  further  the five dimensional data set contains irrelevant features 
in these data sets  there appears to be an optimal value of  which minimises the generalization error 
figure   a  shows the adaptation of  during the training phase of our algorithm on a two dimensional
data set  the minimum value of generalization error reached is         at a            figure   b  shows
the algorithm running on a five dimensional data set  converging on an error of       at            these
results match the approximate values obtained through grid search 
comparing both methods in terms of speed  the grid search method took    and    iterations to converge
for the two  and five dimensional cases  respectively  on the other hand  the adaptation method took   
and     iterations  respectively  we comment here that the grid search was done for just   values of 
where a larger number might be needed to get more accuracy  additionally  this comparison is for a single
hyperparameter  we expect that applying the same method to adapt multiple hyperparameters  such as the
kernel parameters  would maintain a comparable level of performance  while the cost of grid search grows
exponentially with the number of hyperparameters searched 

 

fi   

     

   

     

   

    

   

     

     

   

   

     
 
   
   

    

 k   

     

v  k     k   

   
   

 

     
v

 k   
   

  

  

  

  
  
iteration  k 

  

  

v  k     k   

     

     

   

     
 

    
v

   

    

     

   

 a    d  optimal           after    iterations

 

  

  

  
iteration  k 

  

   

   

 b    d  optimal           after     iterations

figure    learning  for  a  two  and  b  five dimensional data set 

 

conclusion and further work

gradient based method of adapting hyperparameters during the learning phase was previously used successfully in neural networks  this work aimed to study the ability to extend the method to other supervised
learning methods  since the results of adapting the regularization parameter in l  regularized logistic regression are promising  we think this method can be further extended to adapt multiple hyperparameters 
such as the kernel parameters in kernel logistic regression 
it is not possible to apply this technique directly to kernel svm  as svm is usually solved in its dual
form  where the optimization problem is constrained and not smooth with respect to the hyperparameters 
recently  however  there has been work on solving svm in its primal form while maintaining the convenience
of the kernel method      it may be possible to apply the gradient technique in that setting 

references
    m  seeger  cross validation optimization for large scale hierarchical classification kernel methods  in
nips       
    s  s  keerthi  v  sindhwani  and o  chapelle  an efficient method for gradient based adaptation of
hyperparameters in svm models  in nips  pp               
    c  do  c  s  foo  and a  ng  efficient multiple hyperparameter learning for log linear models  in nips 
     
    d  chen and m  t  hagan  optimal use of regularization and cross validation in neural network modeling  neural networks        ijcnn     international joint conference on  vol     pp            jul
     
    r  eigenmann and j  nossek  gradient based adaptive regularization  neural networks for signal
processing ix        proceedings of the      ieee signal processing society workshop  pp        aug
     
    o  chapelle  training a support vector machine in the primal  neural computation  vol      pp      
      mar      

 

fi