semantic taxonomy induction from semi structured text
jui yi kao  yi hao kao  and ian yik oon quek
advised by  daniel jurafsky and rion snow

  introduction
many computational linguistics and natural language processing tasks require a large lexical
database of word relations  an interesting direction to explore in this line of research is to use
more structured texts such as dictionary definitions and encyclopedia entries  these data sources
have much more consistent structures that can be exploited by a computer to recognize word
relations  dictionary definitions  in particular  may also contribute to the recognition of certain
groups of relation instances that are difficult to come by in general texts  they can also have a
much more uniform coverage because each strives to be comprehensive 
in this project  we investigate applying automatic taxonomy induction techniques as presented in
      to more structured texts  in particular  we show using merriam webster dictionary
definitions that applying a priori knowledge of the text structure vastly improves results over
treating the corpus as free text  our general approach is a variation of the approach taken by
snow  jurafsky and ng      adapted to the specific semi structured text corpus we consider 
preprocessed
dictionary data
 complete sentences
with the definitions 

parser

extract patterns
corresponding to each
word pair  each word
pair is a training
example

   x     y       x     y       
 x m   y m    
training set

machine
learning
algorithms

query wordnet for labels   y     y       y m   
is the wordpair a known hypernym 

figure    high level flow chart of our process 

  treating dictionary as free text
    preprocessing dictionary data
before utilizing the data from the dictionary  we have to rewrite it into a useful sentence
structure  so that the stanford parser will be able to parse the sentence effectively  dictionary
data is typically in incomplete sentences  with the word itself left out of the sentence  we want to
include this word in the sentence  so that we can use the parser to detect word pair relations from
the sentence structure  we have observed some rules to preprocess the dictionary data 




if the noun ends in phy  ing or tion  we append  word  is  the  to the
beginning of the definition  for example  absorption is the process of absorbing or of
being absorbed 
if the definition starts with the  we append the  word  is to its beginning  if the
definition starts with an or a  we append  a an   word  is 

fi

if the meaning of a noun does not have the  an or a in front of it  we add the to
the beginning of the definition before appending the word 

    generating attribute vectors  discovering patterns
the following procedure is used to generate an attribute vector for each word pair under
consideration 
 parse the text using stanford parser 
 order of words in a list should be ignored because the order has no semantic significance 
we distribute all relationships across conjunct relations      for example  in the sentence
i love fruits like apples and peaches  one would like the extracted pattern relating fruits
to apples to be the same as the extracted pattern relating fruits to peaches  so distributing
across conjunct relations  the typed dependencies 
prep like fruits    apples   
conj and apples    peaches   

are replaced with the typed dependencies 
prep like fruits    apples   
prep like fruits    peaches   





figure    an attribute vector 

construct a dependency tree from the collapsed dependencies 
identify all distinct noun pairs related by a short enough dependency path  for each of
these pairs of words  identify the minimal paths in the parse tree that contains instance of
both words 
record the number of times the pair is related by each pattern throughout the whole text
corpus  this is the set of attributes of each word pair considered 

  applying a priori knowledge of the text structure
to apply a priori knowledge of the text structure we do not form each defined word and its
associated definition into a sentence  as presented in subsection      the same procedure outlined
in subsection     is followed  with the following exceptions 
 in parsing  only the definition is parsed  the word being
defined is left out  e g  in the definition weekend   the end
of the week  only the phrase the end of the week is
parsed 
 after a dependency tree is constructed from the collapsed
dependencies of the parsed definition  the word being
defined is attached as the new root by introducing a special
metadefine dependency and making each original root
child to the word being defined through this
figure    adding the
metadefine dependency 
metadefine dependency 

  experimental paradigm
the goal of this project is to build a classifier that decides  given an ordered pair of nouns
 ni  nj   whether it is related by hyponym hypernym relationship 

fithe experiments are based on a corpus of approximately    thousand dictionary definitions in
the merriam webster dictionary  
       of the unique word pairs extracted by the definitions were labeled as known hypernym
or known nonhypernym using wordnet   for each noun pair  ni  nj  in our data set  we label it
as y   ni  nj     which is a binary value defined by 
y   ni  nj      known hypernym  if

nj is an ancestor of the first sense of ni in the wordnet
y   ni  nj      known nonhypernym  if

ni and nj are contained in the wordnet

neither one is an ancestor of the other in the wordnet for any senses 
if  ni  nj  does not satisfy either of the conditions set above  we consider it unkown and will not
include it  in our known set  the portion of known hperynym and known nonhypernym are
around    and     of all known pairs 
to evaluate our classifiers  we perform cross validation on the known set  using a portion to train
and a portion to test 

  results
we train several classifiers on two different sets of data  dictionary word definitions pairs
preprocessed into free text sentences  section    and dictionary definitions injected with a priori
knowledge of the definition structure  section     the classifiers include  multinomial logistic
regression with ridge estimators      multinomial nave bayes  complement nave bayes     
and the smo      the classifiers are evaluated using cross validation and the best f scores were
obtained by multinomial logistic regression with ridge estimators  our results are summarized in
figure   
notably  treating dictionary data as free text results in a very poor f score  while the precision is
very high  its recall is negligibly low  this means that the patterns extracted contributes very
little information for deciding hypernymy  the classifier essentially predicts the most likely label
according to the prior 
injecting the dataset with a priori knowledge of the text structure results in much higher f scores 
the best f score obtained in the case of treating dictionary as free text is       compared to the
best f score of      obtained on the dataset injected with structure knowledge 
    comparison with previous work
our best results obtained are inferior the basic hypernym classifier obtained by snow  jurafsky 
and ng      at the same precision of     as obtained by our logistic regression classifier  their
best hypernym only classifier obtained a recall of approximately     compared to
approximately     obtained here  our results here are nonetheless still respectable considering
the much smaller dataset used 

  the corpus contains noun definitions from the online merriam webster dictionary 
  we access wordnet     using mit java wordnet interface       as our wordnet query tool 

fi  conclusions
our experiments show that taking advantage of a priori knowledge of the text structure in
structured natural language texts vastly improves the resulting hypernym pair classifier  by
injecting knowledge of the structure  we obtain a respectable hypernym pair classifier using a
relatively small corpus 

figure    min  means only attributes occurring in at least   distinct word pairs are considered 
zero r  predicts the most common label  slightly perturbed to produce a valid f score  prior 
predicts randomly using prior distribution  disregarding attributes  free text  best result
obtained treating dictionary as free text 

fi  future work
    immediate continuation of this work
due to machine time and memory constraints  our best classifier  multinomial logistic
regression  was trained on a training set that is less than one sixth of the full training set  so we
expect better results when we have the opportunity to use the full training set 
we also lacked the resources to perform wrapper feature selection  when we have the
opportunity to perform systematic wrapper feature selection  we anticipate eliminating the
currently observed overfitting and improve results significantly 
we also intend to train our classifiers to appropriately trade off precision and recall in order to
maximize the resulting f score 
    longer term future directions
in this project we merely scratch the surface in using a priori knowledge about the structure of a
corpus to improve classifier performance  in the merriam webster dictionary definitions alone 
there are many avenues for future investigation  there is often a hierarchy of several definitions
given for the same word  in this project  we treat each definition completely independently  but a
promising future direction is to leverage interrelations between these parallel definitions using
knowledge of the hierarchical structure  furthermore  there are many other sources of structured
natural language text for further exploration  including encyclopedia entries  design documents
and legal documents 
acknowledgments
we thank daniel jurafsky and rion snow for advising us on this project  they gave us the
original idea of investigating semantic taxonomy acquisition in semi structured text and provided
guidance in adapting the existing techniques to semi structured text to take advantage of the
added structure  we thank them especially for their support and encouragement throughout this
project 
references
    rion snow  daniel jurafsky  and andrew y  ng  semantic taxonomy induction from heterogenous evidence  in
proceedings of the   th annual meeting of the association for computational linguistics       
    rion snow  daniel jurafsky  and andrew ng  learning syntactic patterns for automatic hypernym discovery  advances in
neural information processing systems          
    ian h  witten and eibe frank         data mining  practical machine learning tools and techniques    nd edition  morgan
kaufmann  san francisco       
    le cessie  s  and van houwelingen  j c          ridge estimators in logistic regression  applied statistics  vol      no    
pp         
    rennie j   shih  l   teevan  j     karger  d         tackling the poor assumptions of nave bayes text classifiers  proc  of
iclm      
    j  platt         fast training of support vector machines using sequential minimal optimization  advances in kernel
methods   support vector learning  b  schoelkopf  c  burges  and a  smola  eds   mit press 

fi