semi supervised learning with sparse distributed representations
david ziegler
dziegler stanford edu
cs     final project
 

optimization problem as formulated by     

introduction

for many machine learning applications  labeled data
may be very difficult or costly to obtain  for instance
in the case of speech analysis  the average annotation
time for a one hour telephone conversation transcript
is     hours     to circumvent this problem  one can
use semi supervised learning algorithms which utilize
unlabeled data to improve performance on a supervised learning task  since unlabeled data is typically
much easier to obtain  this can be an attractive approach 
in this paper  we combine semi supervised learning
with two types of sparse distributed representations 
local and global  in a local sparse distributed representation we attempt to find local sparse features 
while in a global sparse distributed representation we
attempt to represent our test set using a sparse combination of the training set  in both cases  sparsity
turns out to have desirable properties which we can
leverage in semi supervised learning 

 

local sparse distributed representations

sparse coding is very well suited toward the extraction
local sparse features  the goal of sparse coding is
to represent input vectors as a sparse approximate
weighted linear combination of basis vectors  that is 
for input vector y  rk  
y

 

bj sj   bs

   

j

where b         bn  rk and s  rn is a sparse vector of
coefficients  unlike similar methods such as pca  the
basis set b founded in sparse coding can be overcomplete  n   k   and can represent nonlinear features of
x  to find the optimal b and s we solve the following

minimizeb a

 
i

s t

 

  x i  

 
j

 i 

aj bj          a i         

  bj         j          s

   

the structural self taught learning
algorithm

traditionally  semi supervised algorithms have assumed that the unlabeled data and the labeled data
are drawn from the same distribution  a recent
semi supervised framework proposed by raina et al   
called self taught learning abandons this assumption 
and only requires that the unlabeled data be within
the same problem domain  or modality  the approach
taken by the authors in     is to learn a set of bases on
the unlabeled data via sparse coding  and then represent the labeled data as a sparse linear combination
of the learned bases  this allows them to map their
labeled data into a higher level represenation  which
they can then perform supervised learning on  the
algorithm is summarized below  this algorithm falls
algorithm   self taught learning via sparse coding
   input 
labeled training set t
 
       
 m   m 
 xl   yl          xl   yl    unlabeled data
   
 k 
xu        xu  
 i 
   using unlabeled data xu   solve the optimization
problem     to obtain bases b  compute features for the classification task to obtain a new
 i 
labeled training set t     a xl    y  i    m
i     where
 i   i 
 i 
 i 
a xl   y     argmina i    xl  sumj aj bj       
  a i        learn a classifier c by applying a supervised learning algorithm to the labeled training set t  
   ouput  learned classifier for the classification
task 
under the more general case of self taught learning

fialgorithms which we call structural self taught learning  in structural self taught learning  we attempt to
extract an abstract underlying structure from the unlabeled data  use this underlying structure to project
our labeled data into a higher level represenation  and
then perform supervised learning on the labeled data 
this approach to self taught learning opens up two
important questions which we will attempt to address 
   how does the choice of our structural mapping
function affect performance on our supervised learning task  in this case  sparse coding  
   how does the similarity between the labeled and
unlabeled data affect performance on our supervised
learning task 
while there has been good theoretical work done
recently in semi supervised learning    and transfer
learning     none of these approaches are applicable
to the case of self taught learning  where we do not
assume anything about the distribution of the unlabeled data 

figure    sample bases learned from natural images
nounced for the linear classifier  this could be that
the gaussian kernel is better suited towards raw pixel
intensities  and we note that to allow for a fair comparison between the raw and sparse coding features 
we did not use the sparse coding kernel derived in     

we define the compatibility function    du  dl 
f         relating the compatability between unlabeled distribution du   labeled distribution dl   and
structural mapping function f   as  du   dl   f    
   exl dl   xu   xl   f     in the case of sparse coding 
 xu   xl   a       e xl  ba xl   

 

   

a different approach which we call a global sparse distributed representation is to represent features from
a test example using a sparse combination of features
from the training set  unlike local sparse distributed
representations  it turns out that within the context of
a global sparse distributed representation  the selection of features becomes completely irrelevant  recent
research has shown that for face recognition  if the
representation is sparse enough  random linear projected features perform just as well or better than
feature selection for via eigenfaces  laplacianfaces 
or fisherfaces      building off of the pattern recognition algorithm used in     we devise a simple semisupervised learning algorithm which utilizes a bootstrapping approach to expand our training set 

essentially  we measure the compatibility between an
unlabeled distribution  a labeled distribution  and a
mapping function  by the reconstruction error with
respect to the the optimal basis found over the unlabeled data 

 

global sparse distributed representations

mnist experiments

for our experiments with self taught learning  we
used the mnist handwritten digits dataset for labeled data  unlabeled data was taken from three
sources  natural images  computer font characters 
and mnist  the number of bases was kept fixed at
    and all images were resized to        pixels  we
used two classifiers  a linear and gaussian svm and
compared the performance between the raw pixel intensities and the sparse codes 

we assume that images within the same category
lie on a low dimensional linear subspace  the k individual subspaces can be represented by matrices
a    a         ak where each column in ai is a training sample from class i  let y be an image from the
test set  and a    a  a     ak    then ideally 

the results generally support our hypothesis that a
higher compatibility function yields better results on
the sparse coding classification relative to the raw
pixel intensities  figure     the results are more pro 

y   ax   rm

 

   

fi              
              

                         
            
                                            
   
 abc d
  befd
  bghd
i bcid
     
c  
i bgid
ihb fd
icbeid
ifbi d
      
    
i beid
igbced
ieb hd
ieb gd
      

              
     

                         
            
                                            
   
 abc d
  bcad
  bghd
  bg d
      
c  
i bgid
igbc d
icbeid
ifb ed
      
    
i beid
ifbgc
ieb hd
icbai
      

              
jk    l   m    

                         
            
                                            
   
 abc d
 cb cd
  bghd
 gbchd
      
c  
i bgid
i bi d
icbeid
ihbeid
      
    
i beid
i bi d
ieb hd
ifb id
      

figure    mnist results
where x                i     i          i ni               t 
rn is a coefficient vector whose entries are mostly
zero except those associated with category i  thus  a
new test image should be predominantly represented
using training images from the same subject  and this
representation will be naturally sparse if the number
of categories k is reasonably large 

 

we define the sparsity concentration index as in     
sci x   

let r  rdxm be a transform matrix with d   m 
multiplying both sides of     by r yields y   ry  
rax    ax    recent developments in compressed
sensing have shown that the desired x  is the solution
to the l  minimization problem 

min   x   

s t  y   ax

k
 maxi   i  x        x              
k 
   

the sci provides a measure of how concentrated the
coefficients are on a specific category  when sci     
the test image is represented using only images from a
single subject  and when sci      the coefficients are
spread evenly over all categories  we also define r to
be the ratio between the smallest and second smallest
residual 

   

the residuals calculated in algorithm   measures how
well the the sparse representation approximates the
test image  while the sci and r give us a measure of
how good the representation is in terms of localization 

furthermore  if x has p   n nonzeros  then
d   p log n d 

semi supervised learning in global
sparse distributed representations

algorithm   recognition via sparse representation
   input  a matrix of training images a  rmn for
k subjects  a linear feature transform r  rdm  
a test image y  rm   and an error tolerance   
   compute features y   ry and a   ra and normalize y and columns of a to unit length 
   solve  min   x    s t    y  ax      
   compute residuals ri  y      y  ai  x     for i  
        k
   output  identity y    argmini ri  y 

   

random measurements are sufficient for sparse recovery with high probability     thus even with randomly selected facial features  if the dimension of the
image is sufficiently large then we should still be able
to classify the test image 
let i  x   rn be a vector whose only nonzero entries
are the entries in x associated with category i  the
classification algorithm is then given by algorithm  
     note that the minimization problem in step   is
slightly different than in     to model noise and error
in the data 

after running algorithm   on our unlabeled data  we
then run algorithm   to perform the supervised learning task  alternatively  we can run algorithm   on
the test set  augmenting our training set with images
 

fialgorithm   bootstrapping a global sparse representation classifier
   input  a matrix of training images a  rmn for
k subjects  a linear feature transform r  rdm  
a matrix of unlabeled images y  rmq   and an
error tolerance   
   loop
  
numlabeled    
  
for all y  y do
  
compute features y   ry and a   ra
and normalize y and columns of a to
unit length
  
solve  min   x    s t    y  ax      
  
compute residuals ri  y      y ai  x    
for i           k
  
if sci x    and r   then
  
j   identity y    argmini ri  y 
   
append y to aj and remove y from
y
   
update  a    a  a     ak  
   
numlabeled     
   
end if
   
end for
   
if numlabeled     then
   
break
   
end if
    end loop
    output  augmented training matrix a  rmp  
pn

original image

random projection
feature

figure    random feature example
from the test set  and then run algorithm   on the
remaining test images 
this algorithm has two potential drawbacks  the first
is that if  and  are set improperly  then we might
add misclassified images to our training set  which
would then cause compounding errors in later classifications  thus obscuring the categories in our training
set  in our experiments this problem did not arise 
but this is an inherent weakness in any bootstrapping
algorithm  however  so long as the conditions in    
are satisfied  the probability of a sparse incorrect reconstruction occurring has very low probability  thus
by setting  and  properly  the probability of a misclassified images being added to the training set is
very low 
the other concern is raised by the theory underlying      which is that by increasing our training set
while keeping the dimensionality and number of categories fixed  we reduce the probability that we can
successfully reconstruct the test image since we are
potentially decreasing the sparsity of x  for category
i with ni examples  and r  rmn if
ni    support       m  

   

then we should not expect to perfectly recover x and
       however  this is generally not a problem since
so long as the dimension of r is large enough  if one
suspects that this might be a problem  one could stop
the algorithm once the sparsity started getting close
to the bound in     

 

experiments with face recognition

experiments were conducted the extended yale b
database  the database consists of        cropped and
normalized frontal face images of    individuals under
various laboratory controlled lighting conditions  the
feature space dimension chosen was     

figure    top residuals  bottom  x obtained from
   

we first randomly split half the data into a training
 

firecent development  with many attractive properties 
in particular  the ability to classify using random features  the bootstrapping algorithm shown here is
fairly simplistic but surprisingly robust and helpful
in boosting performance  to explore this algorithm
further  it would be useful to perform experiments
where the parameters were set too low  causing misclassifications to enter our training set  and see how
performance degrades as a result 

figure    training set augmented with unlabeled set

 

figure    training set augmented with test set

thanks to honglak lee for helping me with my sparse
coding implementation 

set and half into a test set  then a portion of the
training set was randomly split off and randomly permuted into a third unlabeled training set  we then
ran our bootstrapping algorithm on this unlabeled set
to augment our training set  and then tested our augmented training set with the separate test set  figure
    the algorithm does demonstrate significant improvement on the test set  although the amount it
improves performance seems to decrease asymptotically as the initial training set size is increased 

references
    m  balcan and a  blum  a pac style model for
learning from labeled and unlabeled data       
    j  baxter  theoretical models of learning to learn 
     
    honglak lee  alexis battle  rajat raina  and andrew y  ng  efficient sparse coding algorithms  in
neural information processing systems       

we also ran a third experiment where we used the entire training set and ran the bootstrapping algorithm
on the test set  augmenting the training set with the
test set  we again get a performance improvement 
although the effect is small since we start out with a
large training set  figure     in all of the bootstrapping experiments   and  were set conservatively to
ensure that no unlabeled data was added to the training set and misclassified  in all the above experiments 
post experiment analysis showed that the unlabeled
data which was added to the training set was all correctly classified 

 

acknowledgements

    rajat raina  alexis battle  honglak lee  benjamin packer  and andrew y  ng  self taught
learning  transfer learning from unlabeled data  in
icml     proceedings of the   th international
conference on machine learning  pages        
new york  ny  usa        acm 
    john wright  arvind ganesh  allen yang  and
yi ma  robust face recognition via sparse representation       
    allen y  yang  john wright  yi ma  and
s  shankar sastry  feature selection in face recognition  a sparse representation perspective  technical report ucb eecs          eecs department  university of california  berkeley  aug
     

future work and conclusions

we provided a way which allows one to quantitatively
determine how the compatibility between the unlabeled and labeled data will affect performance on the
supervised learning task for a given structural mapping function  to make this result more robust  it
would be nice to show this compatibility with nonimage data  and to experiment with different mapping
functions such as pca  also  it would be interesting
to see at what point of incompatibility the unlabeled
data stopped helping performance on the supervised
task 

    x  zhu  semi supervised learning literature survey 
technical report tr       computer
sciences department  university of wisconsin madison  june      

the global sparse distributed representation is a fairly
 

fi