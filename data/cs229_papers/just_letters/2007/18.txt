emotion detection from speech
   introduction
although emotion detection from speech is a relatively new field of research  it has many potential
applications  in human computer or human human interaction systems  emotion recognition systems
could provide users with improved services by being adaptive to their emotions  in virtual worlds 
emotion recognition could help simulate more realistic avatar interaction 
the body of work on detecting emotion in speech is quite limited  currently  researchers are still
debating what features influence the recognition of emotion in speech  there is also considerable
uncertainty as to the best algorithm for classifying emotion  and which emotions to class together 
in this project  we attempt to address these issues  we use k means and support vector machines
 svms  to classify opposing emotions  we separate the speech by speaker gender to investigate the
relationship between gender and emotional content of speech 
there are a variety of temporal and spectral features that can be extracted from human speech  we use
statistics relating to the pitch  mel frequency cepstral coefficients  mfccs  and formants of speech as
inputs to classification algorithms  the emotion recognition accuracy of these experiments allow us to
explain which features carry the most emotional information and why 
it also allows us to develop criteria to class emotions together  using these techniques we are able to
achieve high emotion recognition accuracy 

   corpus of emotional speech data
the data used for this project comes from the linguistic data consortiums study on emotional
prosody and speech transcripts      the audio recordings and corresponding transcripts were collected
over an eight month period in           and are designed to support research in emotional prosody 
the recordings consist of professional actors reading a series of semantically neutral utterances  dates
and numbers  spanning fourteen distinct emotional categories  selected after banse   scherer s study of
vocal emotional expression in german      there were   female speakers and   male speakers  all in
their mid   s  the number of utterances that belong to each emotion category is shown in table    the
recordings were recorded with a sampling rate of      hz and encoded in two channel interleaved   bit pcm  high byte first   big endian   format  they were then converted to single channel recordings
by taking the average of both channels and removing the dc offset 
neutral
  
hot anger
   
elation
   
shame
   

disgust
   
cold anger
   
happy
   
pride
   

panic
   
despair
   
interest
   
contempt
   

anxiety
   
sadness
   
boredom
   

table    number of utterances belonging to each emotion category

   feature extraction
pitch and related features
bzinger et al  argued that statistics related to pitch conveys considerable information about emotional
status      yu et al  have shown that some statistics of the pitch carries information about emotion in
mandarin speech     

fifigure    variation in pitch for   emotional states

mfcc and related features
mfccs are the most widely used spectral representation of speech in many applications  including
speech and speaker recognition  kim et al  argued that statistics relating to mfccs also carry emotional
information     
st
 rd mfcc coefficient  nd mfcc coefficient   mfcc coefficient

pitch hz 

for this project  pitch is extracted from the speech waveform using a modified version of the rapt
algorithm for pitch tracking     implemented in the voicebox toolbox      using a frame length of
  ms  the pitch for each frame was calculated and placed in a vector to correspond to that frame  if the
speech is unvoiced the corresponding marker in the pitch vector was set to zero 
pitch vs frame number
figure   shows the variation in pitch for a female
   
elation
speaker uttering seventy one in emotional states of
despair
despair and elation  it is evident from this figure that
   
the mean and variance of the pitch is higher when
seventy one is uttered in elation rather than despair 
in order to capture these and other characteristics  the
   
following statistics are calculated from the pitch and
used in the pitch feature vector 
   
 mean  median  variance  maximum  minimum  for
the pitch vector and its derivative 
   
 average energies of voiced and unvoiced speech
 speaking rate  inverse of the average length of the
voiced part of the utterance 
   
 
  
  
  
  
  
  
  
hence 
the pitch feature vector is    dimensional 
frame number

   

elation
despair

for each   ms frame of speech  thirteen standard
  
mfcc parameters are calculated by taking the
 
absolute value of the stft  warping it to a mel
 
  
  
  
  
  
  
  
  
  
   
frame number
frequency scale  taking the dct of the log melelation
  
despair
spectrum and returning the first    components     
 
figure   shows the variation in three mfccs for a
female speaker uttering seventy one in
   
 
  
  
  
  
  
  
  
  
  
   
emotional states of despair and elation  it is
frame number
despair
evident from this figure that the mean of the first
elation
  
coefficient is higher when seventy one is uttered
in elation rather than despair  but is lower for the
 
second and third coefficients  in order to capture
   
 
  
  
  
  
  
  
  
  
  
   
these and other characteristics  we extracted
frame number
statistics based on the mfccs  for each
figure    variation in mfccs for   emotional states
coefficient and its derivative we calculated the
mean  variance  maximum and minimum across all
frames  we also calculate the mean  variance  maximum and minimum of the mean of each coefficient
and its derivative  each mfcc feature vector is     dimensional 

formants and related features
tracking formants over time is used to model the change in the vocal tract shape  the use of linear
predictive coding  lpc  to model formants is widely used in speech synthesis      prior work done by
petrushin suggests that formants carry information about emotional content       the first three
formants and their bandwidths were estimated using lpc on   ms frames of speech  for each of the
three formants  their derivatives and bandwidths  we calculated the mean  variance  maximum and
minimum across all frames  we also calculate the mean  variance  maximum and minimum of the mean
of each formant frequency  its derivative and bandwidth  the formant feature vector is    dimensional 

fi   classification
we tried to differentiate between opposing emotional states  six different opposing emotion pairs
were chosen  despair and elation  happy and sadness  interest and boredom  shame and pride  hot anger
and elation  and cold anger and sadness 
for each emotion pair  we formed data sets comprising of emotional speech from all speakers  only
male speakers  and only female speakers because the features are affected by the gender of the speaker 
for example  the pitch of males ranges from   hz to    hz while the pitch of females ranges from
   hz to    hz       this corresponds to a total of eighteen unique data sets 
for each data set  we formed inputs to our classification algorithm comprising of feature vectors from 
pitch only  mfccs only  formants only  pitch   mfccs  pitch   formants  mfccs   formants  and
pitch  mfccs   formants  hence  for each emotion pair  the classification algorithm was run on
twenty one different sets of inputs 

k means clustering
for each emotion pair  all input sets were clustered using k means clustering  k      for all twelve
combinations of the parameters listed below 
distance measure minimized  squared euclidean  l  norm  correlation  and cosine  the correlation
and cosine distance measures used here are as defined in the matlab kmeans function  
initial cluster centroids  random centroids and user defined centroids  udc   a udc is the
centroid that minimizes the distance measure for the input features of one emotion in the emotion pair 
maximum number of iterations     only when the initial cluster centroid is a udc  and      for both
random and udc centroids  
the error used to obtain the recognition accuracy is the average of the training errors obtained by   fold cross validation and is an estimate of the generalization error  the variance of the recognition
accuracy is the variance of these training errors  for each experiment  the highest recognition accuracy
achieved  its variance  the inputs features and clustering parameters used  is listed in table   
all speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness
male speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness
female speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness

features
mfcc
mfcc
pitch
mfcc
mfcc
mfcc

distance measure
l  norm
l  norm
l  norm
l  norm
l  norm
l  norm

centroid
udc
udc
udc
udc
udc
udc

iterations
   
 
   
 
 
 

recognition accuracy
      
      
      
      
      
      

variance
     
      
     
     
      
     

features
mfcc   pitch
mfcc
mfcc   pitch
mfcc   pitch
mfcc
mfcc

distance measure
correlation
l  norm
cosine
correlation
l  norm
l  norm

centroid
udc
udc
random
udc
udc
udc

iterations
 
 
   
 
 
 

recognition accuracy
      
      
      
      
      
      

variance
     
     
     
      
      
     

features
distance measure
centroid
iterations recognition accuracy
mfcc
l  norm
udc
 
      
mfcc
l  norm
udc
 
      
mfcc
l  norm
udc
 
      
mfcc
l  norm
udc
 
      
mfcc
l  norm
udc
 
      
mfcc
correlation
udc
 
      
table    highest recognition accuracies using k means clustering

variance
     
      
      
      
     
      

fisupport vector machines  svms 
a modified version of the   class svm classifier in schwaighofers svm toolbox      was used to
classify all input sets of each emotion pair  the two kernels used and their parameters are 
   linear kernel  with parameter c  corresponding to the upper bound for the coefficients is  ranges
from          with multiplicative step     
   radial basis function  rbf  kernel  parameter c  corresponding to the upper bound for the
coefficients is  ranges from         with multiplicative step    
the recognition accuracy and variance was calculated using the same technique as for k means  for
each experiment  the highest recognition accuracy achieved  its variance  the inputs features and
clustering parameters used is listed in table   
all speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness
male speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness
female speakers
experiment
despair elation
happy sadness
interest boredom
shame pride
hot anger elation
cold anger sadness

features
mfcc
mfcc
mfcc   pitch
mfcc
mfcc   pitch
mfcc

kernel
rbf kernel
linear kernel
linear kernel
rbf kernel
linear kernel
rbf kernel

c
   
 
  
   
 
   

recognition accuracy
      
      
      
      
      
      

variance
     
      
      
     
      
     

features
mfcc   pitch
mfcc
mfcc   pitch
mfcc   pitch
mfcc
mfcc

kernel
linear kernel
linear kernel
linear kernel
linear kernel
linear kernel
linear kernel

c
 
 
  
   
  
   

recognition accuracy
      
      
      
      
      
      

variance
     
      
      
      
      
      

c
features
kernel
recognition accuracy
 
mfcc   pitch
linear kernel
      
 
pitch
linear kernel
      
   
pitch
linear kernel
      
 
pitch
linear kernel
      
  
mfcc   pitch
linear kernel
      
  
mfcc
linear kernel
      
table    highest recognition accuracies using   class svms

variance
      
      
      
      
      
     

   discussion
the results obtained by the experiments performed allow us to make the following observations 
using the formant feature vector as an input to our classification algorithms  always results in suboptimal recognition accuracy  we can infer that formant features do not carry much emotional
information  since formants are used to model the resonance frequencies  and shape  of the vocal tract 
we can postulate that different emotions do not significantly affect the vocal tract shape 
using squared euclidean as a distance measure for k means always results in sub optimal recognition
accuracy  using this distance metric effectively places a lot of weight on the magnitude of an element in
the feature vector  hence  an input feature that might vary a lot between the two opposing emotions may
be discounted by this distance measure  if the mean of this feature is smaller than that of other features 
tables   and   indicate that the recognition accuracy is higher when the emotion pairs of male and
female speakers were classified separately  we postulate two reasons for this behavior  first  using a
larger number of speakers  as in the all speaker case  increases the variability associated with the

fifeatures  thereby hindering correct classification  second  since mfccs are used for speaker recognition 
we hypothesize that the features also carry information relating to the identity of the speaker  in addition
to emotional content mfccs and pitch also carry information about the gender of the speaker  this
additional information is unrelated to emotion and increases misclassification 
tables   and   also suggest that the recognition rate for female speakers is lower than male speakers
when classifying emotional states of elation  happiness and interest  the higher number of female
speakers than male speakers in our data set may contribute to this lower recognition accuracy  further
investigation suggested that in excitable emotional states such as interest  elation and happiness  the
variance of the pitch and mfccs increases significantly  however  variance of the pitch and mfccs is
higher for female voices than male voices  hence  this increase in variance is masked by the natural
variance in female voices  which could make the features less effective at correctly classifying agitated
emotions in female speakers 
of all the methods implemented  svms with a linear kernel give us the best results for single gender
classification  especially in male speakers  this indicates that this feature space is almost linearly
separable  the best results using k means classification are usually obtained when the cluster centroids
are udcs which we think indicates that unsupervised learning algorithms such as k means cannot pick
up on all the information contained in the feature sets  unless we add some bias to the features 

   conclusion   future work
although it is impossible to accurately compare recognition accuracies from this study to other studies
because of the different data sets used  the methods implemented here are extremely promising  the
recognition accuracies obtained using svms with linear kernels for male speakers are higher than any
other study  previous studies have neglected to separate out male and female speakers  this project
shows that there is significant benefit in doing so  our methods are reasonably accurate at recognizing
emotions in female and all speakers  our project shows that features derived from agitated emotions
such as happiness  elation and interest have similar properties  as do those from more subdued emotions
such as despair and sadness  hence  agitated and subdued emotion class can encompass these
narrower emotions  this is especially useful for animating gestures of avatars in virtual worlds 
this project focused on   way classification  the performance of these methods should be evaluated for
multi class classification  using multi class svms and k means   in addition the features could be fit to
gaussians and classified using gaussian mixture models  the speakers used here uttered numbers and
dates in various emotions  the words themselves carried to emotional information  in reality  word
choice can indicate emotion  mfccs are widely used in speech recognition systems and also carry
emotional information  existing speech recognition systems could be modified to detect emotions as
well  to help improve emotion recognition we could combine methods in this project and methods
similar to the nave bayes in order to take advantage of the emotional content of the words 

   references
    http   www ldc upenn edu catalog catalogentry jsp catalogid ldc    s  
    r banse  k r scherer  acoustic profiles in vocal emotion expression  journal of personality and social psychology  vol                 
    t bnziger  k r scherer  the role of intonation in emotional expression  speech communication  vol                  
    f yu  e chang  y xu  h shum  emotion detection from speech to enrich multimedia content  lecture notes in computer science 
vol                    
    d talkin  a robust algorithm for pitch tracking  rapt   speech coding   synthesis      
    http   www ee ic ac uk hp staff dmb voicebox voicebox html
    s kim  p georgiou  s lee  s narayanan  real time emotion detection system using speech  multi modal fusion of different timescale
features  proceedings of ieee multimedia signal processing workshop  chania  greece      
    http   www ee columbia edu  dpwe resources matlab rastamat 
    l r rabiner and b h juang  fundamentals of speech recognition  upper saddle river  nj  prentice hall      
     v a petrushin  emotional recognition in speech signal  experimental study  development  and application  icslp       vol   
             
     l r rabiner and r w schafer  digital processing of speech signals  englewood cliffs  london  prentice hall      
     http   ida first fraunhofer de  anton software html

fi