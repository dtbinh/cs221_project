final report  object recognition using large datasets
ashwin deshpande
        
object recognition is a difficult problem due to the large feature space and the complexity
of feature dependencies  first  there exist positional complexities resulting from the  d position
and orientation of the object as well as the  d position and orientation of the camera  further 
changes in lighting  background  and occlusion can create dramatically different images for the
same object  in addition  to these complexities  we try to perform object recognition over classes
of objects  mugs  pliers  scissors  etc   which contain variances between objects in the same class 
while there has been a steady trend in representing object characteristics with increasingly
complex models and features  we instead chose a different approach by artificially generating a large
training set and applying simple algorithms on the result  this method has several advantages
and disadvantages  on the positive side  using more data almost always yields better results  on
the other hand  the generated training data may not be an accurate representation of reality and
may create an artificial bias  furthermore  when dealing with millions of images simultaneously 
special precautions must be taken to respect the strict hardware constraints 
this paper first discusses the image generation process  next  it explores two nearest neighbor
related algorithms  the first uses cover trees  and the second implements modified nister trees 
finally  the paper ends with future work and conclusions 
image generation

figure    parts of the image generation process  from left to right  green screen image of object 
object mask  shadow mask  background  combined image
the images were generated in a very similar manner to a previous paper      we used an existing
set of about      photographs and regions of interest of objects       per class  with a green screen
backdrop and a stock collection of about      office backgrounds  masks for each object and its
shadow were extracted using intensity thresholds and smoothing  see fig    for an example   to
generate an image  both the background and object were subjected to perspective changes via
 

fiaffine transformations in the homogenous coordinate space including stretching  translations  and
rotations  next  the object was copied onto a randomly selected region of the background  finally 
a random shadow intensity was projected onto the background  using this method  millions images
of sizes   x   to    x    were created 
cover trees
we first experimented with cover trees to perform nearest neighbor calculation  a cover tree is
an efficient data structure which allows for quick nearest neighbors calculations by hierarchically
organizing the data into a tree with guarantees on both the construction time and query time  as
we performed nearest neighbors calculations on the order of million images  the speedup was both
large and computationally necessary 
the first step involved feature extraction  at first    million    x    images were generated
for each class  features were set to be individual pixel values          as performing pca on the
dataset required the infeasible task of computing the eigenvectors of a      x      dimensional
matrix  we eventually scaled back image size to   x    via pca  the number of dimensions were
safely reduced to      with the eigenvalues of the last component vectors being essentially zero  
following this  these vectors were fed into a cover tree  the first test was object detection 
this problem attempts to answer questions of the form  is there a flipphone in this image  the
training set was divided such that half was composed of positive training examples from a single
class and the other half was a random assortment of images from the other classes as negative
training examples  to test the data  a set of      real test images      per class  were queried
against each created tree  see fig      performance was measured as the sum of proportions of
true positives and true negatives weighted equally  in addition to performing   nearest neighbor
and    nearest neighbor search  we also attempted to discover any consistent distributional bias
between real images and generated images  to do this  we calculated an approximation of the
average and covariance of both the training data and half of the testing data  then  we calculated
the difference of averages and axis aligned variances between the two  by simple linear shifts  the
other half of the testing data was subjected to these transformations 
overall  increasing the training set size did seem to have a noticeable impact on performance 
in addition  for telephones and hammers  bias shifting the testing data caused significant improvements in performance indicating a consistent bias of image artifacts in the generated images 
the second test for cover trees was object classification  this problem attempts to answer
questions of the form  what object is in this image  in this case  the training set was divided
equally among all classes  as we used   classes in these experiments  an accuracy of greater than
    indicates learning  as can be seen in fig     again increasing the training set size did seem to
have a noticeable impact on performance and as before some object classes like telephones enjoyed
a significant accuracy boost by normalizing the testing data  it is also interesting that   nearest
neighbor seems to always outperform    nearest neighbors 
overall  cover trees did yield results showing that using larger datasets of generated images
can improve performance in image detection and recognition  however  cover trees seem to be
bound by an upper limit of training set size due to memory constraints as each the features for
each training point must be remembered  thus  this precludes the use of cover trees for tens of
millions of generated images on commodity hardware 

 

fimodified nister trees
the nister tree is a vocabulary tree specifically designed for distance computations between images
based upon localized image patches rather than entire images  in this model  each image can be
decomposed into a set of interesting image patches  then  the distance between a pair of images
is some aggregate of the distance between every pair of image patches  while this approach can be
formalized to clearly define the distance between a pair of images  it cannot be used in conjunction
with more traditional nearest neighbor algorithms like the cover tree as the distance computations
are too slow  thus  as an alternative  the modified nister tree can efficiently ignore distance
computations between very different feature vectors via a greedy tree search and enable efficient
indexing and querying of large image datasets 
the first step in using nister trees is feature generation  initially  we tried using   x   and
   x    images  however  the features extracted from these small images were unreliable  ultimately  we settled on generating       images for each class of size    x     each image was
broken down into a set of about         features  at first  we used sift to generate features for
the image patches  however  we switched to surf features as surf features frequently outperform sift in the literature and each surf feature takes    b instead of    b or sift to describe
an image patch comparably 
the next step was generating the nister trees  this involved plotting all features  localized
image patches  from all training images in feature space  next  k means was applied hierarchically
to generate a tree of feature clusters  in our implementation  we chose k    as a reasonable
balance between performance and quality      in the literature  there were a set number of levels
for k means clustering  however  we allowed a variable number of levels and instead chose to
terminate the hierarchical clustering if the number of features in the leaf cluster was less than a
threshold or the entropy of cluster with respect to the number of features in each class was less
than another threshold  the first condition stops the creation of a superfluous cluster layer  and
the second eliminates the need for unnecessary hierarchical subdivision 
in building the nister trees  the largest tree built involved using       images from each of
  different object classes  this results in       total images      m features  or    gb of input
data  thus  due to hardware constraints  we had to adopt an incremental approach to building the
nister tree  we built the the first level of a nister tree using only a small fraction of the training
data  this provided adequate clustering for the top of the tree  then     times the number of
features were binned into the preset clustering of the previous round  and the second level of the
nister tree was generated  this procedure was repeated multiple times until all     m features
could finally be inserted into the tree  at which point the tree was allowed to grow unrestricted 
this approach was taken as the effective problem size was kept relatively constant at each level
while the number of problems increased exponentially  this meant that only a small percentage
of the data would need to be loaded at any point and memory overflow problems were less likely
to occur  in addition  as this approach limits the amount of data of each subproblem  k means
clustering can be quickly computed in parallel  as the nister tree groups clusters of features
together  the    gb of input data can be compacted into    mb 
the original nister tree is defined to measure the distance between a pair of images to judge
whether the images describe the same object  in our case  we are interested in matching a query
image to an abstract class  thus  we have to modify the scoring metric  for a query image q  we
define qi to be the number of features of q that pass through leaf node i on the nister tree  let
ni x be the number of images of class x stored under the node i  we define the score s between q
and class x to be 
 

fis q  x   

qi
  x
nx i log  nni  
i x

where
nx  

x

ni x

i

ni  

x

ni x

x

this scoring metric is a combination of multiple heuristics  first  the score is normalized with
respect to the number of features for each class  next  each node is given weight proportional to
its prevalence in the query image  last  we reward nodes with low entropy with respect to the
class in question 
we tested nister trees of size up to       images for object classification  see fig      nister
trees seem to perform much better than cover trees for objects with unique features such as
telephones and watches  initially  we believed that using nister trees on generated data would
not be effective as sift surf feature extraction is supposed to be invariant to the minor affine
transformations performed in image generation  to test this  we used     green screen images of
each class of object as a control  while in theory  the nister tree using green screen images should
have performed better as it only learned features which describe the objects rather than features
from the backgrounds  it turned out that the performance of nister trees does improve with large
generated training sets 
future work and conclusions
we are still experimenting with various ways in which to compress the size of nister trees to be
able to store a larger set of information  increasing the subdivision thresholds would allow this 
but may also reduce the quality of query results from the tree  instead  we can tackle the main
source of space usage  the storage of the centroids of clusters  when processing a particular node
in a nister tree  we can perform an outer loop of greedy feature selection with an inner loop of
k means clustering  this will yield only the most relevant dimensions for clustering  thus  the
centroid of a cluster can then be defined as an axis aligned hyperplane with the exception of
the few relevant feature dimensions  furthermore  the distance between a hyperplane and a point
can be calculated in time linear to the number of non axis aligned dimensions  so queries will be
speeded up as well 
it is clear from the results presented in this paper that object detection and classification can
be significantly improved by using large datasets of generated images  by using cover trees on the
pixel values of images and modified nister trees on localized image patches  accuracy generally
improved with training set size  this prompts the question if object recognition will benefit from
tens of millions or hundreds of millions of generated images  the only obstacle in answering that
question lies in transforming the problem and representation for efficient use given memory and
computation constraints 
 datasets are not readily available as they currently consume about    gb of space  i collaborated with ashutosh saxena to discuss many of the ideas and methods presented in this paper  

references
    anonymous  a fast data collection and augmentation procedure for object recognition 
 

fi    h  bay  t tuytelaars and l v  gool  surf  speeded up robust features  eccv       
    a  beygelzimer  s  kakade and j  langford  cover trees for nearest neighbor  icml       
    l  fei fei  r  fergus and p  perona  learning generative visual models from few training
examples  an incremental bayesian approach tested on     object categories  cvpr       
    lowe  david g  object recognition from local scale invariant features  iccv       
    d  nister and h  stewenius  scalable recognition with a vocabulary tree  cvpr       
    t  serre  l  wolf and t  poggio  object recognition with features inspired by visual cortex 
cvpr       
    j  sivic and a  zisserman  video google  a text retrieval approach to object matching in
videos  iccv       
appendices  graphs

 

fiflipphones

forks

  

hammers

  

  
  

  

  

  

  

  

  

  

  

  

  

  

  

  
  
  
  
  
  
  
  
  
 
  

 

 

  

  

 

  

  
 
  

 

 

  

  

mugs

 

  

  
 
  

 

pliers

  

 

  

  

 

  

scissors

  

  

  
  

  

  

  

  

  

  

  

  

  
  

  
  

  

  

  

  

  

  
  
 
  

 

 

  

  

 

  

  
 
  

 

 

  

staplers

  

 

  

  
 
  

 

telephones

  

  

  

  

  

  

  

  

  

  

  

  

 

  

  

 

  

watches
  

  

  

  

  
 
  

 

  

 

  

 

  

  
 
  

  

 

 

  

  

 

  

  
 
  

 

  

 

  

 

  

figure    object detection accuracies for various object classes using cover trees  the x axis
describes the training set size and the y axis describes accuracy  the colors indicate  green regular   nn  cyan   regular    nn  blue   bias shifted   nn  red   bias shifted    nn 

 

fiflipphones

forks

  

hammers

  

  
  

  

  

  

  

  

  

  

  

  

  

  
  
  
  
  
  
 
 
 
  

 

 

  

  

 

  

 
 
  

 

 

  

  

mugs

 

  

 
 
  

 

pliers

  

 

  

  

 

  

scissors

  

  
  

  
  

  

  
  

  

  

  
  
  

  

  
  
  

  

  

  

  

  
  
  
 
  

 

 

 

  

  

 

  

 
 
  

 

 

  

staplers

  

 

  

 
 
  

 

telephones

  

  

  

  

 

  

  

 

  

watches
  

  
  

  

  

  

  

  

  

  
  
  

  

  
  

  

  

 

  
 
 
  

  

  
  

 

  

 

  

 

  

  
 
  

 

 

  

  

 

  

 
 
  

 

  

 

  

 

  

figure    object classification accuracies for various object classes using cover trees  the x axis
describes the training set size and the y axis describes accuracy  the colors indicate  green regular   nn  cyan   regular    nn  blue   bias shifted   nn  red   bias shifted    nn 

 

fiflipphones

forks

  

hammers

  

  

  

  

  

  
  
  
  

  

  

  

  

  

  

  

  

  
  
  
  
  
 
  

  

  

 

  

 

  

 

  

  
 
  

 

 

  

  

mugs

 

  

 
 
  

 

  

pliers

  

 

  

 

  

scissors

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
  
  
  
  
  
  
  
  
  
 
  

 

  

 

  

 

  

 
 
  

 

 

  

staplers

  

 

  

  
 
  

 

  

telephones

  

 

  

 

  

watches

   

   

  

  

  

  

  

  

  

  

  

  

  
  
  

  

  
  
  
  

  
  

  
  
 
  

 

  

 

  

 

  

  
 
  

 

 

  

  

 

  

 

  

 

  

 

  

 

  

figure    object classification accuracies for various object classes using nister trees  the xaxis describes the training set size and the y axis describes accuracy  the colors indicate  red performance using      green screen object images  blue   performance using generated data 

 

fi