 d reconstruction of brain tissue
hylke buisman   manuel gomez rodriguez   saeed hassanpour


 hbuisman  manuelgr  saeedhp  stanford edu
department of computer science   department of electrical engineering
stanford university
abstract

we consider the problem of identifying neuron nuclei in a
stack of  d images of brain tissue of a rat in order to construct a  d model of the neuron bodies  our approach consists of several a combination of image processing techniques 
machine learning algorithms and  d rendering methods  initially  edge detection  prewitt filter   non linear diffusion  morphological opening and black and white conversion are used
to highlight the neuron bodies in the original  d images  in
the next phase  a supervised learning algorithm is used to decide what part of the  d images are neuron bodies and to
remove the noise  evaluation of the algorithms showed that
svm and logistic regression worked particularly well  the
performance of both algorithms is analyzed  finally  a  d
model is generated using vtk 
index terms machine learning   d reconstruction  brain
tissue  neuroscience
   introduction
recent developments in biomedical methods  like serial blockface scanning electron microscopy  make it possible to obtain high resolution images of brain tissue  in these methods 
a cube of brain tissue is cut in thin layers  in this project  rat
brain tissue has been used     
our goal is to detect all the neuron nuclei in every layer
of the cube of brain tissue in order to build a  d model of the
neuron nuclei located in a cube of brain tissue  as a first step
for a  d automated tracking of neural activity  this builds on
previous work in this field      in order to achieve this aim  we
apply several supervised learning approaches  logistic regression  smo  etc   combined with image processing techniques
and vtk 
in a first stage  the layers are processed in order to make
the nuclei more distinctly visible  for each layer  the algorithm outputs a preprocessed image and a mask for every
neuron candidate  thereafter  the relevant features of each
nucleus candidate are extracted from the images using the
masks  and every nucleus candidate in the training set is labeled using a handy labeling tool  in the next step  the machine learning algorithms are trained on this data and we analyze the performance of every algorithm using cross validation 
finally  a  d model of neuron nuclei is built using the

results from these steps  the visualization toolkit  vtk  
that uses opengl  has been chosen to create the  d model
since it significantly speeds up the rendering and the real time
visualization 
this paper is organized in the following way  section  
is devoted to image processing techniques used in the preprocessing of the brain tissue layers  section   elaborates on the
definition and justification of the machine learning features 
section   describes the machine learning algorithms chosen
to solve the classification problem and presents some quantitative and graphical performance results  section   elaborates
on the  d rendering and visualization process  finally  conclusions are discussed in section   

   preprocessing
the preprocessing section of the algorithm is used to transform the rather unstructured input images to a form where it
is possible to extract relevant features  the output of this section is a preprocessed image  and a list of masks indicating
where in the image nuclei are expected to be  in other words
a list of nucleus candidates is returned 
to achieve this  several image processing steps are applied
to the image  it is important to note that each of these steps
is tuned in such a way that the preprocessing will output too
many neuron candidates  this minimizes the amount of false
negatives  at the cost of including more false positives  by
doing this  the classification process is shifted from the image
 pre processing section to the machine learning section 

     edge detection
the first step is edge detection  the edges of the nuclei are
fairly distinct in the input images  and their insides are of irregular consistency  as a result  significantly more edges are
found at the edges of the nuclei and in their insides  than in
between the nuclei  based on experimentation it turned out
the prewitt edge detection works best for this application 
  a false negative in this case  means that a part of the image that contains
a neuron is not present in the list of neuron candidates 

fi a  original image

 b  after the opening

 c  black and white

fig     preprocessing steps
     non linear diffusion
the edge image roughly indicates where a nucleus is likely to
be  but on the wrong scale  a useful output would be a black
and white image that indicates areas where nuclei are likely
to be  to accomplish this  something similar to a blurring
is needed  however  applying a gaussian blur  would result
in losing information regarding the borders of the nucleus 
for this reason nonlinear diffusion is applied  it preserves the
edges  but also blurs edges inside the nuclei  the resulting
image is a more reasonable representation of what is likely to
be part of a nucleus  and what is not 
a positive side effect  is that nonlinear diffusion is also
known to reduce noise      in this case that means that the resulting image will lose some of its  fine grained  noise  which
makes it easier to work with 
     morphological opening
at this point the image is still not ready for conversion to
a black and white image  thresholding at this stage would
result in some nuclei growing together and there would still be
noise present  which would unnecessarily slow down the rest
of the algorithm  to resolve this  a morphological opening is
applied 
a morphological opening is the combination of applying
an erosion  and then a dilation  also known as the minkowski
subtraction and addition   an opening of set a with structuring element b is denoted as 

     thresholding
finally the grayscale image can be thresholded to a black and
white image  to ensure that the values of the image are between zero and one  the image is first normalized using contrast stretching  in our case this is achieved by dividing it by
the maximum value   the threshold is then set to a very low
value  to prevent increasing the number of false negatives 
the value of the threshold was chosen experimentally 
an overview of the preprocessing steps can be found in
figure  

   selection and extraction of features
in the final output of the preprocessing step  all contiguous
areas of white pixels are extracted from the image  each of
these is considered to be a nucleus candidate  for the learning
task of classifying these candidates as being an actual nucleus
or not  a set of relevant features needs to be selected  these
features should provide sufficient information to distinguish
noise from actual nuclei 
after analyzing the set of unprocessed and preprocessed
images  the following features were selected 

   roundness
a  b    a   b   b
where   and  are the erosion and dilation operators 
the erosion of two sets can be interpreted as all points where
the second set  the structuring element  can fit in the first set
 based on some center in b   the dilation is the set containing
all additions of points in both sets  applying the opening operation has the effect of removing all points that are smaller
or not of similar shape as the structuring element 
the structuring element was chosen to be a small disc
 with radius     such that small noise and forms that strongly
deviate from a round form  such as lines in the image  disappear 

   density

   relative density with respect to contiguous layers

   area

in the following paragraphs  these features will be motivated and defined 

fi k
dn  xi   yi   i   is the set of intensity values in the preprocessed layer l      next layer  of the pixels that belong to the
neuron nucleus n in the current layer l 
both values have to be close to   for the set of pixels of a
neuron nucleus because the intensity difference between contiguous layers is not very big  on the other hand  in case of
having noise  the intensity of the set of pixels can be completely different between contiguous layers  then  the previous and next relative intensity can be a relevant feature to
classify neuron nuclei  this feature is expected to be useful
in modeling the continuity between images 



     roundness
for this application  the roundness of a nucleus n in a layer l 
rl  n   is defined as
rl  n   

k
 x
 di  e d     
k i  

   

k

  xp
 xi  xc       yi  yc   
k i  
p
di    xi  xc       yi  yc     

e  d   

   
   


 k
where  xi   yi   i   is the set of coordinates of the pixels that
belongs to a neuron nucleus n in a layer l and  xc   yc   is the
center of n in l  a pixel  xi   yi   belongs to a neuron nucleus
n in a layer l if the value of  xi   yi   in the mask l is   
note that rl  n  will be small for a set of points  for which
the variance of the distance between each point and the centroid is small  rl  n      would mean that n is a disk 
the choice for roundness as a feature is justified by the
relatively frequent occurrence of round nuclei  in addition
noise tends to be non round of nature  like lines in the image  
these observations make roundness a relevant feature 

     area
the area of a neuron nucleus n in a layer l  al   is defined as
al  n    k

   

where k is the number of pixels that belong to a neuron nucleus n in a layer l  as stated before  a pixel  xi   yi   belongs
to a neuron nucleus n in a layer l if the value of  xi   yi   in the
mask l is   
intuitively  too big or too small values of areas are good
reasons to discard a candidate neuron nucleus 

     intensity
the intensity of a neuron nucleus n in a layer l  dl  n   is
defined as
k
 x
d xi  yi  
dl  n   
k i  

   


 k
where d xi  yi   i   is the set of intensity values of the pix
 k
els  xi   yi   i   in the preprocessed layer l  again  a pixel
 xi   yi   belongs to a neuron nucleus n in a layer l if the value
of  xi   yi   in the mask l is   
d n  has a lower value for neuron nuclei than for the
background of the preprocessed images  being a relevant feature to distinguish neuron nuclei from noise 
     relative intensity
we define the previous relative intensity  rdp  n   and next
relative intensity  rdn  n   of a neuron nucleus n in a layer l
as
k
 x
rdp  n     
dp  xi   yi    dl  n  
k i  

   

k
 x
du  xi   yi    dl  n  
k i  

   

rdn  n     

where dl  n  is the intensity of the neuron nucleus n in the

 k
current layer l  dp  xi   yi   i   is the set of intensity values
in the preprocessed layer l     previous layer  of the pixels
that belong to the neuron nucleus n in the current layer l  and

   machine learning algorithms
after building the set of nucleus candidates and extracting a
full set of features for each of them  machine learning can
now be applied  in order to evaluate the performance of many
different algorithms  the weka toolkit     was used  this
machine learning toolkit provides a large set of ready to use
machine learning algorithms  which can also be readily integrated within the matlab environment 
the following methods where applied  decision tree learners  bayesian classifiers  rule based learners  logistic regression and support vector machines 
     labeling
labeling nucleus candidates for every preprocessed layer to
create a training set is a tedious task  in order to accelerate
this task we have developed a tool that handily highlights the
current  previous and next image  this is done by calculating
the convex hull of the mask of every candidate  which is then
superimposed on the preprocessed image    in addition to this 
labeling judgments were also based on the contents of the raw
images within the superimposed mask outline  this method
of labeling proved to be more accurate compared to just using
the preprocessed images 
using this method a total of    images were labeled  which
on average contain    nucleus candidates  thus  a total of
about      instances were used for learning 
  using just the preprocessed images for labeling is the method that was
described and used for the milestone 

fi a  smo

 b  logistic

fig     supervised learning algorithms performance
     evaluation
the initial experiments done with weka showed that logistic regression  with a ridge estimator  and smo performed
particularly well on this task  based on this observation  several more experiments were done to find the best choice of
parameters  for smo a polynomial kernel of degree    gave
best precision  for logistic regression  the ridge parameter
was left unchanged  since no significant increase was achieved
by changing it  simple logistic regression performed just as
well on the task  however it was significantly slower  in figure   the results of the algorithms can be found  these results
where computed with    fold cross validation on the whole
data set 
although the precision of the smo with degree    polynomial kernel performed significantly better  logistic regression can be chosen if time is more important than precision 
model building takes    seconds for smo compared to     
seconds for logistic regression  since logistic regression also
outperforms the linear kernel smo  which exhibits similar
speed   logistic regression would be the best choice if time is
more important 
to be able to appreciate these results  it is worth mentioning the results using the trivial classifier  sometimes referred
to as a zero rule classifier   this classifier always predicts the
majority class  applying this algorithm gives a precision of
     consequently  the results as shown in figure   show
that applying these non trivial algorithms result in a     increase of precision 
finally  comparing the evolution of the training error and
the test error and comparing its values  it is noticeable that
smo has a higher variance  this is an expected result if it is
taken into account that a higher order classification space is
used in smo because of having a polynomial kernel 

    d rendering
after classifying the nucleus candidates with smo  and thus
removing nearly all further noise  the output is used to generate the  d model  for the modeling  the visualization
toolkit  vtk   which is an open source c   library for  d
computer graphics  was used      vtk uses opengl  open
graphics library  for basic computer graphics functions and
presents higher level functions for  d visualization and image
processing 
first  an edge detection algorithm is applied  to specify
the edges and extract the nuclei surfaces  these surfaces are
then covered with polygonal meshes  finally  the meshes are
mapped through the stack to render the  d model using vtk 
figure   shows two  d models  one has been generated
using the  d images before applying smo to reduce the noise
and the other was built using the  d images after applying
smo  these images give a good impression of the improvement the application of machine learning gives  the neuron
bodies can easily be distinguished now that the noise has been
removed 
   conclusions
the method presented in this paper provides a way for inferring the spatial location and structure of neuron bodies and
the spatial relation between them if  d images of a stack of
brain tissue are the only information available     
regarding the overall quantitative performance of the method
presented in this paper  machine learning performance has
been used as an approximation of it  based on the quantitative performance analysis presented on the section    it is
noticeable a significant improvement of the error if a supervised machine learning  approximately      is used instead
of using only a trivial classifier  approximately       it is

fi a   d rendering before applying machine learning

 b   d rendering after applying machine learning

fig      d rendering
assumed that labeling using the preprocessed images and the
original ones is correct  
it is difficult to measure the accuracy of the preprocessing steps and the  d model representation  to measure the
performance of the preprocessing steps  it would be required
to label the data based only on the original images  not the
preprocessed ones  this is not manageable  taking into account the vague nature of the original images  to estimate
the performance of the  d model representation  it would be
necessary to compare the  d model with a reference  which
is not available 
focusing on the relationship between test error and amount
of training data  the results suggest that increasing the size of
the training set  would improve the performance even more 
it was not possible to verify the last statement because of the
unavailability of more data 
   acknowledgments
the authors thank assistant professor mark schnitzer for providing us access to the data set of rat brain tissue and assistant
professor tom clandinin for giving us expert information regarding this data set 
   references
    k d  micheva and s j  smith  array tomography  a
new tool for imaging the molecular architecture and
ultrastructure of neural circuits  neuron  vol      no 
   pp             
    j  h  macke  n  maack  r  gupta  w  denk  b  schlkopf 
and a  borst  contour propagation algorithms for semiautomated reconstruction of neural processes  journal
of neuroscience methods  vol  epub ahead  pp         
     
    p  mrazek  nonlinear diffusion for image filtering and
monotonicity enhancement  prague  czech technical
university       
  the preprocess section has been designed to maximize the amount of
candidate neuron bodies  then  false negatives are minimized by the preprocess itself and false positives are reduced by the expertise of the labeler

    i h  witten  e  frank  l  trigg  m  hall  g  holmes 
and s j  cunningham  weka  practical machine learning tools and techniques with java implementations 
iconip anziis annes  pp               
    w  schroeder  k  martin  and b  lorensen  the visualization toolkit an object oriented approach to  d
graphics  kitware  inc  publishers  vol          

fi