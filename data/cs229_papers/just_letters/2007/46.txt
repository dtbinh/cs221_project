automatic detection of character encoding and language
seungbeom kim  jongsoo park
 sbkim jongsoo  stanford edu
cs      machine learning
autumn     
stanford university

 

introduction

their methods  we aimed at a simple algorithm which can
be uniformly applied to every charset  and the algorithm
the internet is full of textual contents in various lan  is based on well established  standard machine learning
guages and character encodings  and their communica  techniques  we also studied the relationship between lantion across the linguistic borders is ever increasing  since guage and charset detection  and compared byte based althe different encodings are not compatible with one an  gorithms and character based algorithms  we used nave
other  communication protocols such as http  html  bayes  nb  and support vector machine  svm  
using the documents downloaded from wikipedia     
and mime are equipped with mechanisms to tag the character encoding  a k a  charset  used in the delivered con  we evaluated different combinations of algorithms and
tent  however  as native speakers of a language whose compared them with the universal charset detector in
character set does not fit in us ascii  we have encoun  mozilla  we found two promising algorithms  the first
tered a lot of web pages and e mail messages that are one is a simple svm whose feature is the frequency of
encoded in or labeled with a wrong character encoding  byte values  the algorithm is uniform and very easy to
which is often annoying or frustrating  many authors  implement  it also only needs maximum     table entries
especially of e mail messages  are not aware of the en  per each charset and the detection time is much shorter
coding issues  and their carelessness in choosing correct than other algorithms  despite of its simplicity  it achieves
encodings can result in messages that are illegible to the        accuracy  which is comparable to that of mozilla
recipient  therefore  automatically detecting the correct           the second one is a character based nb whose
character encoding from the given text can serve many accuracy is         it needs a larger table size and a
people using various character encodings  including their longer detection time than the first algorithm  but it also
detects the language of document as a byproduct 
native one  and has a good practical value 
the importance of automatic charset detection is not
restricted to web browsers or e mail clients  detecting
the charset is the first step of text processing  there    previous work   mozilla
fore  many text processing applications should have automatic charset detection as their crucial component  web kikui     proposed a simple statistical detection algocrawlers are a good example 
rithm  he used different algorithms for multi byte and
due to its importance  automatic charset detection is al  single byte charsets  a character unigram for multi byte
ready implemented in major internet applications such as charsets and a word unigram for single byte charsets 
mozilla or internet explorer  they are very accurate and  n  gram is the sequence of linguistic units with length
fast  but the implementation applies many domain spe  n   a word unigram means that the unit of statistics is
cific knowledges in case by case basis  as opposed to one word   using different algorithms for multi byte and
 

fisingle byte charsets is a fairly common approach shared
by almost all previous works  this is because multigrams are too long for multi byte charsets and unigrams
are too short to gather meaningful statistics of single byte
charsets  kikuis algorithm is essentially nb except that
it has a pre step which distinguishes between multi byte
charsets and single byte charsets 
russell and lapalme     advocated the simultaneous
detection of charset and language  their probability
model is also nb  but they interpolated trigram and unigram models 
there are not many academic papers about automatic
charset detection  and they are neither theoretically sound
nor supported by a thorough empirical evaluation  at
the same time  open source implementations such as
mozilla     and icu     work extremely well in practice 
since their approaches are similar  we focus on the universal charset detector in mozilla 
the universal charset detector in mozilla     uses three
different methods in a complementary manner  figure    
it first classifies the document by finding a special byte sequence  for example  every utf series character stream
can start with a special byte sequence byte order mark
 bom   if the input byte sequence has an escape character       esc  or    we know that the input is
encoded by one of the escape charsets  gb      hz simplified chinese  iso      cn jp kr   if the input satisfies this escape character condition  the detector executes
a state machine for each possible charset and pick the one
which arrives at the itsme state first  if the input neither
satisfies the conditions above and nor has any byte bigger
than  x f  we know that the input is us ascii 
the second and third methods use a known statistical
distribution of each charset  similar to kikui      the universal charset detector in mozilla uses different methods
for multi byte charsets and single byte charsets  one character as the unit for multi byte charsets and two bytes as
the unit for single byte charsets  however  to fully utilize the encoding rules for each charsets  it also runs a
state machine  the state machine identifies the charset
which is surely correct or which is surely incorrect  according to the encoding rules  for single byte charsets
whose language does not use latin characters  it excludes
latin characters from the statistics to reduce the noise 
in summary  mozillas implementation smartly exploits
various aspects of character encoding rules which are ac 

figure    the flow chart of the universal character encoding detector in mozilla  bom means byte order mark 
cumulated from years of experience  however  the implementation is not easy to understand by novices 

 

design

   

scope

for the purpose of this project  we want to confine the
set of target languages and charsets  to be representative
among single byte and multi byte encodings  we select a
few from both classes as follows 
 single byte 
 us ascii  en 
 iso         en fr 
 multi byte 
 shift jis  euc jp  ja 
 euc kr  ko 
 

fior characters  unigrams  as the features  more elaborate
models can use two or three consecutive bytes or characters  bigrams  trigrams   or even more  the size of the
parameter space is exponential to n   so there is a tradeoff
between the accuracy of the model and the cost of training  computation and storage  this becomes more important as the size of the set of units becomes large  as in
asian charsets with character based approach 

figure    byte based method vs  character based method

   
 iso       jp kr   ja ko 

our first choice was nave bayes  nb  under the multinomial event model  which is similar to the spam filtering
example discussed in class  this algorithm is simple to
implement and expected to work reasonably well 
later  support vector machine  svm  was also tried 
our own implementation of the smo algorithm was used
first  but a library implementation  called libsvm     
outperformed our version  the lengths of the documents
were normalized to    thus  for example  the value at the
feature vector index  x   represents the number of appearances of  x   divided by the total number of bytes of
the document  since the feature space is already large  we
used the linear kernel  we used one vs the rest multiclass strategy  the class with the highest decision values
was selected  we also tuned the regularization constant c
and the tolerance value 
based on the result from these two algorithms  we devised a hybrid algorithm  which first uses the characterbased nb to detect only the charset and then uses the
character based svm to further increase the accuracy of
the language detection 

 utf    universal 
this yields    pairs of  language  charset  
it is a trivial task to add more charsets and languages
as needed in the future  in particular  our algorithm does
not depend on any previous knowledge of any particular
language or charset  except the fact that us ascii is the
common subset of the other charsets  notably iso       
and utf   

   

algorithm

feature

we define our feature as a sequence of units  then the
two axes of the feature are formed by the definition of the
unit and the length of the sequence 
we have two different approaches regarding the definition of the unit  one is byte based  which means treating the raw bytes from the document as the units  and the
other character based  which assumes each charset and
decodes the byte sequence into a sequence of characters
 represented by the code points of the universal character set  to be considered as the units   when the byte
sequence cannot be decoded under the assumption of a
charset  that charset is dropped from the set of candidates 
see figure    the latter is inspired by our intuition that
it will give a better representation of the character distribution in each language than the former  in which byte
sequences spanning the boundary between adjacent characters will perturb the distribution  the character based
algorithm only needs to keep one parameter set for each
language  while the byte based algorithm needs one for
each  language  charset  pair 
the other axis is the sequence length  represented by n
 in n  grams   a simple model can use individual bytes

 

result

     documents per  language  charset  pair have been
downloaded from wikipedia       since wikipedia is in
utf    the documents for other charsets can be converted
from utf   by using iconv   they are used for training
and testing  using a cross validation method that uses    
of the data for training and the rest     for testing 
figure   shows the result for the best cases for each
detection goal and algorithm  for detecting the charset
only  the mozilla implementation gives the best accuracy of         but our svm method with byte unigrams
 

fi 

achieves a similar accuracy of        with a much less
table size of     kb and a much less detection time of
     ms per document 

conclusion

we have seen that 
   the character based approach with the nave bayes
algorithm works well for detection between charsets
sharing a large code space  e g  english us ascii 
utf    and iso        documents  

for detecting the charset and the language at the same
time  our hybrid method combining unigram nb and unigram svm gives the best pair accuracy of        with
the table size of     mb and the detection time of      ms
per document 

   the byte based approach with the support vector
machine works well with a smaller memory usage
and at a faster speed  and

figure   shows the histogram of the correct charsets vs 
the detected charsets when we use the byte based svm to
detect the charsets only  note that the majority of the errors are from detecting utf   or iso        documents
as us ascii  since us ascii is the subset of all the
other character sets and many documents with  english 
utf    and  english  iso         mainly use us ascii
except for very few characters  it is hard to distinguish between them merely by using statistical information  this
is the place where the domain specific knowledge plays
an important role and the universal charset detector in
mozilla uses them very well 

   the hybrid method of character based nb for charset
detection and character based svm for language detection gives the best results 

references
    international components for unicode  http   www icuproject org 
    g  kikui  identifying the coding system and language of online documents on the internet  in international conference
on computational linguistics       
    a
library
for
support
vector
machines 
http   www csie ntu edu tw  cjlin libsvm  
    a composite approach to language encoding detection 
http   www mozilla org projects intl universalcharset
detection html       
    g  russell and g  lapalme  automatic identification of language and encoding       
    wikipedia  http   www wikipedia org  

the character based approach does a better job of distinguishing between us ascii english documents and
non us ascii english documents  if the document is
us ascii  then utf    iso         and us ascii decoders should produce the same character sequence  and
thus give the same likelihoods  by giving the precedence
on the subset charset  us ascii in our case   we can correctly detect a us ascii document   note  however  that
detecting a us ascii document as iso        or utf  is not harmful at all and thus can be considered as a
non error   if the document is almost us ascii but has
exceptional few characters  then the us ascii decoder
fails  thus it is detected as another charset 
we expected that the character based svm would work
very well  because both of the character based nb and the
byte based svm gave a better result than the byte based
nb  however  the accuracy dropped from that of the bytebased svm  this may share the common reason with the
fact that the svm with byte bigrams has a lower accuracy
than the svm with byte unigrams  finding the reason of
the decreased accuracy of the svm algorithm in a larger
feature space could be an interesting future work 
 

fialgorithm
charset only

charset   lang 

nb
byte
svm byte
mozilla
byte
nb
char
byte
svm
char
hybrid

optimal
meta parameters
 gram
 gram  tol  e    c  e  

pair
accuracy

 gram
 gram
 gram  tol  e    c  e 
 gram  tol  e    c  e  
 gram nb    gram svm

      
      
      
      
      

charset
accuracy
      
      
      
      
      
      
      
      

table
size
   kb
    kb
   kb
    mb
    mb
    kb
    mb
    mb

detection time
 per document 
     ms
     ms
     ms
      ms
     ms
     ms
     ms
     ms

figure    accuracy  required table size and detection time of each algorithm  pair accuracy is counted only if the
both of charset and language are correct  the best meta parameters such as n   c and tolerance are chosen for each
algorithm 

correct charset

utf  
us ascii
iso       
shift jis
euc jp
iso      jp
euc kr
iso      kr

utf  
   
 
 
 
 
 
 
 

us ascii
 
   
  
 
 
 
 
 

iso       
 
 
   
 
 
 
 
 

shift jis
 
 
 
   
 
 
 
 

euc jp
 
 
 
 
   
 
 
 

iso      jp
 
 
 
 
 
   
 
 

euc kr
 
 
 
 
 
 
   
 

iso      kr
 
 
 
 
 
 
 
   

figure    the histogram of correct charsets vs  detected charsets from byte based svm  charset detection only 

 

fi