motion planning for the athlete rover with reinforcement learning
patrick mihelich

   introduction
legged locomotion is attractive because it can enable a
robot to traverse far more varied terrain than a wheeled
rover is capable of  in the context of planetary exploration 
this is especially attractive as the sites of greatest scientific
interest tend to be characterized by difficult terrain  for
example  it would be extremely difficult for a wheeled rover
to make its way into a lunar crater in search of water 
planning legged locomotion is  however  a more difficult problem the wheeled lomotion  compared to wheeled
robots  legged robots tend to have a much larger number
of degrees of freedom  a planner for a legged robot therefore has to plan in a high dimensional configuration space 
placing considerable demands on the planners efficiency 
the problem is further complicated when uneven terrain
is considered  classical motion planning techniques applied
to legged locomotion generally assume a flat workspace
with obstacles to be avoided  in the case of uneven terrain 
there are no explicit obstacles  but a motion planner must
take care to maintain the stability of the robot at all times
while moving over sloped and uneven surfaces  for articulated legs  contact with the ground creates a closed loop
kinematic chain  algorithms such as probabilistic roadmaps must be adapted to efficiently handle these closedloop constraints  as a randomly sampled point in the configuration space has zero probability of exactly satisfying
them 
especially when the robot is intended for cooperative
tasks with humans  an additional problem is planning trajectories which appear natural to a human observer  a planner for a humanoid bipedal robot  for example  might generate bizarre looking arm motions which aid in balance 
even on relatively flat terrain where such motions are unnecessary  a planner for legged locomotion should ideally
encode constraints that encourage natural looking motion 
the specific robot considered in this paper is the athlete  all terrain hex limbed extra terrestrial explorer  robot developed by the jet propulsion laboratory
 jpl   athlete is intended to be a lunar rover  and is
especially designed for movement over broken and uneven
terrain  its hexagonal frame is designed for carrying large

fig     the athlete rover

payloads  or even a living capsule  so that it could be used
for both transport and exploration by a lunar base  on
flat terrain it can move quickly using wheeled locomotion 
on uneven terrain it can fix its wheels and walk on its
six articulated legs  and it is this mode of operation with
which this paper is concerned  each leg has six degrees of
freedom  adding six more dof for the position and orientation of the chassis  athlete has a total of    dof 
the goal of the present research is to develop a realtime on line motion planning algorithm for athlete that
enables it to reach a goal location both quickly and safely 
the approach used here is similar to that of urmson et 
al   who plan a global path using a traversability map and
then select from a set of actions how to follow that path
locally     in this paper  the set of actions is defined to be
a set of different gaits  and reinforcement learning is used
to learn the action planner 
   approach
this paper proposes a three part planning algorithm for
legged locomotion over uneven terrain that uses a set of
fixed gaits as a model for generating control actions  the
use of fixed gaits drastically reduces the dimensionality of
the configuration space and also results in natural looking
motion 
the full planning problem is decomposed into a global
planner  an action planner appropriate for any state  and
a set of gait planners 

fi i  field d  search is used to determine an approximate
global trajectory for the robots center of mass between the start and target locations  minimizing a
measure of costs associated with the traversability of
the terrain it crosses 
 ii  an action planner repeatedly selects a gait to use
based on the traversability of the terrain currently
occupied by the robot  action selection is learned
through reinforcement learning 
 iii  a gait planner determines the joint controls to step
the legs according to the chosen gait in the direction
of the next field d  waypoint 

as a measure of the roughness of the terrain  scaling these
measures by a priori maximum allowable terrain slopes
and roughnesses for the robot and multiplying them gives
a measure of goodness  morphin multiplies the goodness
by a measure of uncertainty to determine traversability 
since we assume full knowledge of the terrain geometry  we
simply equate traversability with goodness 

fig     motion planning approach

   field d  path planning
for each terrain cell  we then calculate a weighted
traversability average of surrounding cells  up to approximately the area of the robot  as a measure of the cost
of moving the robot directly through that cell  field d 
search     is then used to plan a route through the terrain 

fig     motion planning approach

for athlete  we choose between two possible gaits  a
wave gait and a tripod gait  the wave gait moves each
leg in turn keeping five legs on the ground at all times  the
tripod gait moves three non adjacent legs at a time  leaving
only three legs on the ground at all times  compared to the
wave gait  the tripod gait trades stability for speed 
   traversability analysis
naturally the robot must be able to avoid obstacles  but
in varied terrain it also essential that it prefer traversing
flat  smooth regions to sloped  rough ones  we therefore
compute a continuous measure of traversability for use by
both the global planner  to maximize the traversability of
the global trajectory  and by the action planner  to select
a gait appropriate to the difficultly of the terrain  we construct a traversability map using a simplified form of the
morphin algorithm     
we discretize the terrain into a grid of cells of fixed size 
due to athletes size  we choose the cell size to be
smaller than the area occupied by the robot to allow for
enough granularity in the global path planned by field d  
for each cell  we sample points from the terrain and perform a least squares plane fitting to those points  the slope
of the plane is used as a measure of the terrain slope  and the
chi squared error of the sample points from the fitted plane

fig     motion planning approach

   action planner
although we assume perfect knowledge of the environment and the robot state  the results of robot actions are
nondeterministic  due to deficiencies of our physics model 
uncertainty in the controller  and unpredictable interactions with the environment  the state resulting from execution of a control action may differ from what was expected 
this makes an open loop planner unsuitable  instead  we
use reinforcement learning to develop a planner which maps
states to control actions 
 

fi     reinforcement learning

   summary

the state of the robot includes its position and orientation  which gait the robot is currently employing  and
the slopes and roughnesses of the terrain immediately surrounding the robot  for the purposes of action selection
and learning  we reduce the state to a single feature  the
minimum traversability of the terrain grid cells currently
underlying the robot 
we take an action to be a single iteration of the chosen
gait  in which all six legs take exactly one step  for example  a full iteration of the tripod gait involves two steps 
each moving three legs in the same direction simultaneously  there is also a special action to select a different gait 
the reward function for an action incorporates the distance travelled  the stability of the robot during the action 
and any mishaps that occurred  stability can be measured by the area of the polygonal support region formed by
the non moving legs  mishaps include exceeding the torque
limits and self collisions induced by over extending one or
more legs 
the planner is learned with value iteration  state transitions are learned by executing many actions in varied terrain in a physics simulation 

this paper presents a framework for motion planning of
legged robots over uneven terrain  reinforcement learning
is used to learn a planner which selects from a set of predefined gaits according to the difficulty of the terrain 
in future work  more choice in actions could be allowed 
in particular  the action planner could be allowed to exercise some control over the heading of the robot  more state
features would also be useful  to allow the action planner
greater knowledge of the terrain and perhaps take into account the current pitch and roll of the robot 
the use of predefined gaits necessarily limits the difficulty of the terrain that can be safely traversed using this
planning framework  it could be useful to incorporate a
full footfall planner as an additional action to be chosen on
particularly difficult terrain 
the framework could also be extended to uncertain environments without considerable difficulty  the full morphin
algorithm supports updating traversability measures when
new data is acquired  instead of using arbitrarily sampled
points from a known terrain mesh  points derived from sensor data could be used  field d  likewise supports efficient
replanning when costs of grid cells are updated  it has been
shown to be at least two orders of magnitude faster than
repeated application of a  in this respect 
algorithm  described in motion planning for a six legged
lunar robot  hauser  bretl  latombe and wilcox  

   gait planner
the selected gait planner accepts a heading and generates the low level control actions given to the motor controllers or physics simulation  the results of these control
actions are used to learn the action planner 
the wave gait planner used moves each leg in turn  going
counterclockwise around the frame of the rover  each leg
is moved in a parabolic arc in the desired direction until it
comes in contact with the ground 
the tripod gait planner used alternates between movements of three non adjacent legs  during each movement 
it attempts to shift the frame of the body a fixed distance
along the plane of the body  to accommodate sloping terrain   inverse kinematics are used to determine where to
place the moving legs so that they end in contact with the
ground 

   acknowledgments
i would like to thank kris hauser for his robotics simulation software  which was invaluable to this research  i would
also like to thank nasa ames research center and jpl
for the opportunity to work with athlete  and jeanclaude latombe for his comments and feedback 
references
    d  ferguson and a  stentz  field d   an interpolation based
path planner and replanner  proc  international symposium
on robotics research  october      
    s  singh et al  recent progress in local and global
traversability for planetary rovers  proc  ieee international
conference on robotics and automation  san francisco  usa 
april      
    c  urmson  r  simmons  i  nesnas  a generic framework for
robotic navigation  proc  ieee aerospace conference  big
sky montana  march      

   results
fig    shows the results of the planning framework in
simulation on a plot of uneven terrain  field d  plans a
global path skirting the hill to the northwest  starting on
relatively flat terrain  the action planner initially selects the
tripod gait  it has learned to prefer this gait on relatively
easy terrain  where additional stability is less essential  for
its speed advantage over the wave gait  on reaching the
bumpier area in the southeast  the action planner switches
to the slower but surer wave gait  upon reaching flatter
ground close to the goal  it reverts back to the tripod gait 
 

fi a  tripod gait

 b  wave gait
fig     athlete selects gait based on difficulty of terrain

 

 c  tripod gait

fi