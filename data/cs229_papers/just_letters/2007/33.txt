enabling a robot to open doors
andrei iancu  ellen klingbeil  justin pearson
stanford university   cs       fall     

i  introduction
the area of robotic exploration is a field of growing interest and
research in the robotics and computer science communities 
ideally  a human would send a robot into an area otherwise too
dangerous for the human to enter  such as a smoke filled
building  a structurally unsound bridge  or a radioactive area 
although robots already exist which can map a building s floor
plan  they can only do so provided the doors are all open  a
robot that can identify and open doors as it maps a building
remains to be developed 
the goal of this project was to use machine learning to enable a
robot to identify the location of a door handle in an image 
classify the door handle type  and finally open the door  the
data set considered consisted of three door handle types  left
and right turn handles and elevator buttons  several machine
learning algorithms were implemented using a variety of
different feature sets 
for both the identification and
classification problems  the best results were achieved using a
derived feature set with principal component analysis  pca 
and the support vector machine  svm  algorithm  accuracy for
identifying a door handle in a scene was approximately     
and the accuracy for classifying the door handle type was
approximately        the algorithms were demonstrated on the
stanford artificial intelligence robot  stair   which was able
to successfully execute the motion of pushing down on a door
handle to unlatch it 
ii  identifying a door handle in a panoramic image
in terms of developing the capability of identifying a single door
handle in a tightly cropped image  we have attempted
implementing several different solutions before converging on
the use of a binary classification svm  as an initial cut at the
problem  logistic regression  nave bayes and svm were
implemented on a data set consisting of approximately       
by    images half of which depicted door handles with the rest
consisting of other varied building scenery  the pixel rgb
values were used as the features  the results were not very
encouraging  nave bayes was clearly the most unsuitable
algorithm with almost     error on most test sets  in retrospect 
this was to be expected as the nave bayes assumptions on
feature independence would obviously be invalid when applied
to image pixels  logistic regression provided a starting test set
error of approximately     to      while the svms test set
error hovered around      based on this initial round of tests 
we decided to pursue the binary classification svm in more
depth 
although multiple implementations of the svm
algorithm were assessed  the software ultimately used was svm
light developed by thorsten joachims at cornell university     
it should be noted that in all svm training from this point
onward  false positive errors were weighted much more heavily
than false negative errors in order to eliminate as many of the

spurious positive door handle identifications in a panoramic
image as possible  the first attempt at refining the svm
algorithm centered on varying the size of the input images and
observing the effect on training test set error with the pixel rgb
values still being used as the features  the image size was
varied from   by   to    by    and the primary observation from
this set of tests was that the training set error would generally be
zero for all image sizes larger than   by   while the test set error
would hover around a fixed mean of      to      this seemed
to indicate that our algorithms were over fitting and that we
needed to either get more training examples or use a better set of
features  we opted to use a set of derived features that were
computed from the pixel rgb values and designed to capture
three types of local cues  texture variations  texture gradients 
and color  by convolving the image with    filters  same features
used for classification  see section iii for a more detailed
description   our test set error dropped slightly so that it now
hovered around    to      however  even with the derived
features  our training set error was still always zero so the overfitting problem seemed to persist 
to attempt to overcome the over fitting problem when using the
derived features  we resorted to pca and attempted to remove
some of the directions in our feature space that did not seem
relevant to our identification problem  the results of the pca
were actually quite interesting in that using the primary
components of only the images containing door handles actually
provided better results  on the order of   to     on the test sets 
however  this seems intuitive in that we are primarily interested
in correctly identifying door handles and the non door handle
images would clearly contain undesired principal components 
to select the optimal filter size when calculating the derived
features  i e  the number of pixels from each image blended
together  as well as the optimal number of principal components 
an iterative algorithm was used to map out the entire
optimization space  the results are shown in figure   and
figure   below 

figure    max test set error vs  filtered downscaled image size
 d  and   of principal components  k  

fiusing this particular algorithm  the identification function
achieved        accuracy at identifying a door handle elevator
button in a scene  this accuracy was much higher than the
tightly cropped image identification accuracy due to the
weighting of the false positives negatives in the latter which 
when combined with the block parsing of the panoramic image 
consistently yielded large clouds only in regions containing an
actual door handle 
iii  classifying door handle type

figure    training set error vs  filtered downscaled image size
 d  and   of principal components  k  
as highlighted by the above data  there is an optimal point with
an average error of     where the over fitting problem
disappears and the error is very close to its lowest observed
value  this point corresponds to blending the image down to  
by   pixels using the previously mentioned filters then picking
the first     principal components of the resulting feature set 
the result of this was a classifier that  given a tightly cropped
image  could identify whether it contained a door handle with  
     accuracy with the vast majority of the misclassifications
being false negatives 
to tackle the problem of identifying a door handle in a
panoramic image  the identification function sequentially steps
through the input image using a variable or fixed size frame that
is approximately the expected size of a door handle  the frame
size is generally chosen based on laser depth data  the rgb
image that is the content of the frame at each step is then passed
through the previously developed svm classifier which
classifies the contents as either a door handle or not  this results
in a cloud of positive classification hits around each door handle
in the image where the centroid of each cloud is returned as a
single  confirmed door handle  as shown in figure   below  in
the case of multiple clouds  the k means algorithm is used to
calculate the centroid of each cloud 

once the robot has identified the location of the door handle in
an image  it needs to identify the handle type to know how to
manipulate it and open the door  the output from the door
handle identification algorithm is an image cropped around the
region containing the door handle  because the size of the
cropped image output from the identification will be variable  it
is re sized to a constant dimension using nearest neighbor
interpolation 
for a first cut  several machine learning
algorithms were tested on an initial training set consisting of
only left and right turn handles  approximately     training
samples were used  each cropped image was re sized to   x  
pixels and the rgb pixel values were used as the features 
logistic regression had a test error of approximately     
nave bayes had a test error of approximately      and svm
had the lowest test error  ranging between   and     
the svm algorithm appeared to give the best results for the
two class training set  so we decided to move forward using this
algorithm  we used the svm multiclass open source software
package created by thorsten joachims  cornell university     
svm was implemented on a three class training set consisting of
approximately      images of elevator buttons and left   right
turn handles  hold out cross validation was used to compute the
average training and test errors  the svm regularization
parameter was varied and the images were re sized to different
values  an image size of   x   and a svm regularization
parameter of     gave the lowest test error of       our goal
was to have classification accuracy of at least      also  the
training error was zero for any image size greater than   x   
this indicates over fitting  reducing the number of features by
decreasing the image size gave non zero train error  but
increased the test error to      
based on the experiments  it appeared that improving the errors
would require a better and reduced set of features  first the
images are re sized to   x    some example training samples
can be seen in figure    the set of features were computed from

figure    example   x   training samples for door handle type
classification 

figure    example of panoramic image parsing on stair robot
 green dots   positive classification   red dot   final output  

the pixel rgb values and are designed to capture three types of
local cues  texture variations  texture gradients  and color  by
convolving the image with    filters    laws  masks and  
oriented edge filters  fig      the image is segmented into  x 

 

fipatches of pixels and the filters are applied to all   color
channels for each of the patches for a total of  x x   features
per patch  the features for the  x  patches are summed over the
entire image to give a total of  x x   features per training
sample  pca is then used to extract the most relevant features
from this set  the algorithm for feature extraction was provided
by ashutosh saxena     

figure    the filters used for computing texture variations and
gradients  the first   are laws  masks  followed by the oriented
edge filters 
finally  svm is run on the reduced set of features to classify the
door handle as one of the three types  an exhaustive search was
used to determine the number of principal components to use as
well as what value to use for the svm regularization parameter
to achieve the best accuracy  the svm algorithm was run  
times on each set of parameters using different train and test sets
to compute the average errors for each run and compute the
variance of the errors  figures   and   show the results of the
training and test errors obtained for varying the number of
principal components  k  and the regularization parameter  c  
from the figures  we see that there are many values for these two
parameters that give test errors at or below       with variance
of less than      

figure    train error vs svm regularization parameter  c  and
  of principal components  k 

figure    test error vs svm regularization parameter  c  and  
of principal components  k 
iv  implementation on the stair robot
the door handle identification and classification algorithms were
implemented on the stanford ai robot  stair    for the
purposes of making the robot physically open a door  this robot 
which has served as a test bed for multiple ai and computer
vision projects  is comprised of a neuronics robotics katana  dof robotic arm  a sony dvi d    ptz camera  a urg
hokuyo laser scanner  and a pc running windows xp  see
figure     the katana robot arm comes equipped with inversekinematic libraries to move the end effector to a desired location
in the arm s reference frame  a user can also query the location
of the end effector as well  the ptz camera was capable of
taking     by     rgb images  and was mounted on a rail
above and behind the robot arm  the laser scanner was also
mounted on a rail behind the robot  about    m above the
ground  and measured distance from itself to objects in its
horizontal plane  the accuracy of this laser scanner was
approximately  cm in the range of distances     m    m  at
which we were operating  the robot was built on a segway
foundation whose motion was controlled using a linux pc 
however  navigation of the robot itself was beyond the scope of
this project  the implementation of the machine learning
algorithms on stair proceeded in three steps  first  the camera
was calibrated and the distances between the camera  robotic
arm  and laser scanner were measured  then  a function was
written to convert the  x y  pixel location of the door handle into
  dimensional coordinates in the robotic arms reference frame 
finally  the robot arm was given commands to move to the door
handles location and open the door  each of these steps is
described below 

 

fi 

 

 

figure    the stanford ai robot  indicated are the robotic arm
     the ptz camera      and the laser scanner     
after photographing     pictures of a checkerboard in various
orientations  the matlab camera calibration toolbox     was
used to analyze those images and compute the intrinsic
parameters of the ptz camera  with this information  one could
convert a pixel in an image into a ray emanating from the
camera s origin and passing through the pixel  expressed with
respect to the camera s reference frame  by measuring the
distances and angles between the camera s and robotic arm s
reference frames  we constructed transformation matrices to
express this ray in the robot arm s reference frame   initially  this
was performed with a simple measuring tape  but more
complicated refining procedures are described below   by using
data gathered from the laser scanner  the distance from the robot
to the door could be determined  and the intersection of this
plane with the ray gave the location of the pixel in the arm s
reference frame 
in order to verify that the function which converts pixel values
into  d coordinates was predicting reasonable  d coordinates  a
test set was constructed  each element of the test set was
comprised of an image of the robot end effector and the
coordinates of the end effector in the arm s frame as reported by
the arm  because the laser scanner was mounted too high to
record the distance between itself and the robot arm  laser
scanner data was  faked  by using the actual coordinates of the
arm to calculate what distance the laser scanner would have
reported if it had been able to measure that distance  this had the
added benefit of removing the laser scanner s  cm measurement
error from the verification procedure 
to get an initial guess for the distances and angles between the
camera  laser  and robotic arm reference frames  the distances
and angles were measured by hand with a measuring tape and
angles were calculated by moving the robot arm along known
horizontal paths and querying the arm s location  however  the
transformation matrices computed from these estimates
produced large     cm  error between the predicted points and

actual points in the verification test set  by physically
positioning the robot arm as close to the camera as possible  we
could query the robot arm s position to learn the approximate
location of camera s origin with respect to the arm  the same
technique was used on the laser  and these new values produced
error of    cm  in order to reduce the error further  the
matlab function  fminsearch  was used to search the space of
possible translation vectors and rotation angles between the
camera and arm to find the values that would minimize the sum
of the norms of the error between the predicted and actual
points  a regularization term was added to the cost function to
penalize the search algorithm for answers that were too far from
the measured points  additionally  we further searched the
parameter space of translations and rotations around our guess
by writing a matlab gui that would permit the user to click
in a series of  d plots to specify pairs of parameters  and the
gui would plot the predicted points and actual points  this
 point and click  method of user driven parameter tuning
produced results that were similar to the results obtained from
using matlab s optimization function  the errors were driven
down to below  cm in the horizontal directions and  cm in the
vertical direction  because there was no danger of damaging the
arm from error in the vertical direction  we tuned the parameters
to reduce the horizontal error at the cost of increasing slightly
the vertical error  figure   shows the results of this parameter
tuning on the predicted points 

figure    the test data we used to fine tune the transformation
matrix parameters  our initial measurements of the translations
and rotations between the robot frame and camera frame
produced transformation matrices which predicted the points
shown in red  after tuning the parameters  we predicted the
green points  which are much closer to the robots actual
position  black  
once we had calculated the  d point of the door handle  the
robot arm was driven to a point slightly above the door handle 
then it executed the  door opening action   it moved its gripper
straight down  pushing down the handle of the door  because the
pixel returned by the identification algorithm was shifted to be
above the handle portion of the door handle  the gripper would
only engage with the door handle itself  and maintain a safe
distance from the pivot point 

 

fiv  results
when the entire system was tested with the robot in front of a
door  the robot was able to identify the door handle  correctly
classify it  find an appropriate point on the door handle to push
on  navigate to that point  and execute the door opening action
on the door handle  however  the pixel to arm coordinate
transformation program required a large amount of parametertuning before it was able to position the arm correctly  that is 
although the image processing algorithms correctly identified
and classified the door handle  the physical implementation
leaves something to be desired  despite having tuned the
parameters to achieve acceptable levels of error in the test set of
images and gripper positions  we experienced a much higher
level of error when testing the robot on an actual door  this
suggests that  while the method for tuning the coordinate
transformation parameters to minimize the prediction error on
the image gripper point pairs worked well  the test set did not
reflect how well the system would perform as a whole  thus  it
would be desirable to construct a new test set of image gripper
point pairs  this time using actual door handles and real laser
data  this would permit us to tune the parameters to perform
particularly well on the regions of the image known to contain
door handles 

the members of the ai lab  specifically ashutosh saxena 
morgan quigley  brad gulko  and rangan srinivasa  for their
help in implementing our software on the robot 
references
    http   svmlight joachims org 
    http   svmlight joachims org svm multiclass html
    ashutosh saxena  sung h  chung  and andrew y  ng 
learning depth from single monocular images  in nips    
     
    http   www vision caltech edu bouguetj calib doc 

vi  future work
there are several extensions we hope to add to this project  for
the door handle type classification  we would like to add more
types of door handles  preliminary results have been obtained
with the addition of a spherical doorknob  for a total of four
classes   using the exhaustive search to find the best values for
the svm regularization parameter and number of principal
components  the test error is approximately       with training
error of          since the features of spherical doorknobs may
tend to look similar to round elevator buttons in a planar image 
it may take more clever features to increase the accuracy above
    
we also need to do much more testing with the robot on
different door handle types  especially types that the
identification classification algorithms were not trained on 
most of the error in experiment was due to the difficulty of
tuning the coordinate transformation parameters  one possible
area of future work would be to implement an online machine
learning algorithm to learn the parameters  such an algorithm
would automatically move the arm to a known location  take a
picture of it  query the laser  and identify the gripper in the
image  with a large test set of this nature  one could implement a
simple learning algorithm such as weighted least squares which
would map the pixel values and laser distances to the  d robot
arm coordinates  another option might be to employ a vision
feedback system for the robot arm positioning 
acknowledgments
wed like to thank the cs department ai lab and professor
andrew ng for use of the stair robot  wed also like to thank

 

fi