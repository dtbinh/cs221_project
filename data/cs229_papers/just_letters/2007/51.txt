multi class object recognition using shared sift features
siddharth batra
 in collaboration with stephen gould and prof  andrew ng 
take into account the fact that sift features work very
well for specific objects but do not generalize well across
that class of objects 
this paper presents a novel approach that combines
elements from the feature and patch based approaches to
provide a method that creates shared sift features which
can be used for recognizing multiple objects of the same
class 
in its first stage termed as descriptor training it looks at
a set of images from the same object class and uses patch
based correlation to find similar areas on the image which
should be shared using the proposed shared sift
technique  this provides a set of descriptors which act as
the dictionary for that object class  such as mugs  bottles
etc  using matching techniques in feature space  matches
are located from the dictionary in positive and negative
training images and are used to train a multinomial nave
bayes classifier  this classifier is then used to iterate over
a given test image to assign to each part a probability that
an object from that object class is located in that part 
filtering based on these probabilities results in localizing
regions within the test image that contain objects of the
same class which the classifier has never seen before 

abstract
the current state of the art object recognition systems
work reasonably well for limited data sets  in the case of
feature based methods  apart from being object specific
the amount of features in the database also grows linearly
with the number of objects  similarly  in patch based
methods if the number of patches are kept constant
irrespective of the number of objects to be recognized  the
performance drops  this paper presents a novel approach
towards sharing features between objects of the same
class to create a dictionary of shared features for that
class  it uses a combination of the feature   patch based
approaches to enable creation of shared sift features  
also recognition of other objects of the same class using
these shared sift features 

   introduction
the problem of object detection within images is an age
old one and the solution to which has tremendous
applications in a variety of fields such as robotics  human
computer interaction and online advertising  one of the
commonly used feature based techniques for object
recognition is sift  scale invariant feature transform 
     sift gives extremely good results for very specific
objects but does not generalize well across a class of
objects  also  the number of features in the database
increases linearly with the number of objects to be
recognized  hence  sift is not a very good choice for
recognizing objects from the same class  especially for
objects it has not been trained to recognize 
a different approach is the use of shared patch based
features for multi class object detection      this approach
works quite well but if the numbers of patches are kept
constant and the numbers of objects are increased the
performance will take a hit  it is faced with a similar
challenge of increasing patch features with the number of
objects though not linearly  similar to these torralba
patches      also uses a patch based approach via the use
of gabor wavelets to enable feature sharing for object
recognition  another interesting approach is to create a  d
model of the object class from training images and share
sift features by simply placing them onto correct
locations of the  d model      this approach does not

   shared sift descriptors
the original sift approach uses difference of
gaussians  neighborhood maximization   minimization
followed by magnitude   other filtering mechanisms to
find unique locations on the image which are consistent
and hence should be found repeatedly under a variety of
scales  orientations and viewpoints  further  the algorithm
uses gradient magnitude and orientation maps to assign
each of these locations  or keypoints  with an orientation
  scale which is used to create a descriptor of the patch
around that keypoint 
the proposed shared sift approach aims to use
gradient and orientation maps from similar looking
patches in a neighborhood of the original keypoint from
all of the descriptor training images to create a shared
descriptor for that point  figure   illustrates this process 
to begin with keypoints  not descriptors  are
individually extracted from all descriptor training images
using the original sift approach  a patch correlation
technique similar to the one described in     is then used
to find the most similar match to the patch surrounding the
keypoint in the source image in all of the other descriptor

 

fitraining images  to avoid having to perfectly align the
images and allowing sharing across images of slightly
different sizes this search is conducted in a neighborhood
of the location of the keypoint in all of the descriptor

training images  in the next step  gradient magnitude and
orientation maps at these similar patches are combined to
give orientation and scale information to that keypoint 

figure    the process of creating shared sift descriptors

figure    sift keypoints found on the rims of two mugs  a
patch correlation algorithm found the keypoints in blue to be
representing a very similar patch

further  instead of creating separate histograms for each
of the individual patches in the images  the keypoint
information computed above and all the gradient maps
from similar patches are used to create a shared or
averaged histogram which provides a smoothened
common representation of all the patches being shared 
the shared sift descriptor for that location is then
computed from this shared histogram 
one major advantage of sharing patches across the
descriptor images is that variance metrics can be used to
judge how well the shared patch represents the original
patches in the descriptor training images 

shown in figure   is the zoomed in view of keypoints
on the upper rims of two mugs  the patch based
correlation found the patch around the keypoints in blue to
be quite similar in both the images  on the other hand
figure   also shows   keypoints in blue which were found
to be similar but the similarity was quite low 

figure    sift keypoints found on the centre of two mugs  a
patch correlation algorithm found the keypoints in blue to be
representing a not so similar patch

 

fithus  it is quite intuitive to see that a shared descriptor
for the keypoints in figure   will be much closer in the
    dimension space  all sift descriptors are     length
vectors  to the keypoints it represents and similarly the
shared descriptor that represents the keypoints in blue in
figure   will be further away from the keypoints it
represents in feature space  the variance metric shown in
figure   computes the average distance between a shared
descriptor and the individual patches in the descriptor
training images that it represents  the distance is a
euclidean distance computed in the     dimensional
space 

 a 

 b 

figure    a representation of a shared object class after sharing
   mug images  a  plots of all the shared descriptors  b  plots of
the shared descriptors after variance based filtering

   

dist  d    d           d i  d  i   

   matching descriptors in feature space

i   

where d  and
used in sift 

in the original sift descriptor comparison approach 
every descriptor in one image is compared to every other
descriptor in the second image via the distance metric
 euclidean distance  shown in figure    the base
approach being the same  the altered matching algorithm
also takes into account the variance of the shared
descriptor  when comparing a new image to the dictionary
of features of an object  the dictionary is scanned to find
the best match for each sift keypoint in the new image 
further  the variance of the shared descriptor which was
found as the best match in the dictionary is used to decide
whether the keypoint in the new image qualifies as a
match or not 
there were a lot of experiments carried out using a
variety of different decision techniques to label a keypoint
as a match or not  one successful technique labeled a
keypoint as a match if in feature space the euclidean
distance to the closest shared descriptor was less than the
variance of that descriptor  which implies that the given
keypoint is close in feature space to the shared descriptor
and the keypoints that it represents in feature space 
the other technique which was quite successful was a
modification of the matching technique described in    
which uses the distance to the two closest shared
descriptors in feature space and checks if these two
distances are within a certain percentage of each other 
this would indicate that the keypoint is also close to
another point in the object feature space and this indicates
a high probability that the keypoint belongs to the object
feature space  figure   shows the matches resulting from
matching the mug dictionary on the right to a new mug
which it has not been trained upon in the descriptor
training stage 

d  represent any     dimension descriptors
m

var d    

 dist  d   f  
i

i   

m

where d represents the     dimension shared descriptor and f
represents the set of m descriptors from the m patches it
represents 
figure    the formula used to compute the variance of the
shared descriptor with descriptors of the patches it represents

computing these variance metrics for each shared
descriptor and taking only the descriptors which have a
variance less than a set threshold results in taking only the
descriptors which are good indicators of that object class
rather than any of the specific objects  figure   a  shows
all the shared descriptors found after sharing a set of   
mug images  figure   b  shows the set remaining after
applying variance based filtering  this filtered set of
shared descriptors now represents the dictionary of
descriptors for the class of mugs 

 

fiwhere x is the feature matrix  y are the labels representing
whether the image belongs to the object class or not  n is the
length of the dictionary of the object class  m is the total number
of training images  x i j  indicates the number of times shared
descriptor j from the dictionary was found in the image i 
figure    representation of the feature matrix x and labels y

the training set for the mug experiment comprised of   
positive images which contained mugs in them and   
negative images which were random patches from the
background with and without other objects  it should also
be noted that the    mugs used for the positive training
samples were all different from the    mugs used for
descriptor training  figure   a  shows some examples of
positive training images and   b  shows examples from
the negative training images 

figure    matching pairs using a variance based threshold
method between the mug dictionary and a new mug it has never
seen before

   multinomial nave bayes  bag of features 
continuing with the analogy to text based search  the
dictionary of an object class is used in conjunction with
distance metrics discussed in the previous section to
tabulate which individual shared descriptors or words
were found in the new image or document 
traditionally the multinomial nave bayes model for
text simply counts the number of times a word from the
dictionary appears in the given document  similar to this 
initially the distance metrics were used to find the count of
the number of times a shared descriptor appears in the
given image  this approach is called the bag of features
model  hence  the feature vector for a single image is the
same length as that of the dictionary of the object class
being searched and each entry in the feature matrix
indicates the count of the number of times that shared
descriptor was found in the new image which that row
represents 

x  r

 a 

 b 

m  n

figure    training set  a  examples of positive images  b 
examples of negatives images

the training set is used to compute the prior probability of
a sample being positive or negative and also the individual
positive and negative probabilities for each shared
descriptor  as is done in the text classification model of
multinomial nave bayes 
after testing the above defined multinomial nave
bayes classifier on a set of test images  the simple metric
of simply counting the number of times a shared
descriptor was found in the image did not turn out to be
very accurate  since a group of false but weak features
could generate a false positive  further experiments were
conducted with using different metrics to create the
feature vector for a given image  one of the successful
ones takes the contribution of every keypoint on the image
to the feature vector instead of just the positive matches 

x ij             n  
y  rm
yi       

 

fithe count of positive matches is now replaced by the sum
of the distances of that particular shared descriptor to the
keypoints in the image for which it is the closest in feature
space  thus  each entry in the feature matrix is now a real
number and not simply an integer count  this metric
allows the classifier to differentiate between a set of
strong keypoints at smaller distances in feature spaces
versus a set of weak keypoints at larger distances
especially when both keypoints are nearly the same
number 
to test the object classifier  test images were collected
using a digital camera of table scenes with mugs in them 
one possible limitation of sift is that objects within the
images need to be of a certain size to get enough features
off them  thus  when attempting to recognize objects
from the sift features of an image the objects within the
image must be at least     by     pixels  hence  the test
images were all      by      in resolution 
in order to localize the objects within the images  the
test image of the scene was dividing into closely spaced
windows  the sift features falling within the window
were then matched to the dictionary of the object and a
feature vector was generated for each window within the
image  the multinomial nave bayes classifier then used
this feature vector and the probabilities computed during
the training step to assign a probability of the occurrence
of the object to each window  thresholds were set onto
these probabilities to localize areas of high probabilities
which ideally would contain the object whos classifier
was run 

 a 

 b 

figure     results on the test images  images in column  a  are
belief maps and images in column  b  are results after applying a
threshold of probability on the belief maps

   results
after the training step     high resolution digital images
of mug scenes were taken in an office environment and
the process of applying probabilities to each window
within those images was used to compute the belief maps
shown in column  a  of figure     where white indicated
high probability and black indicated low probability on a
grayscale mapping to probability  column  b  shows the
result of putting a threshold on these probabilities to
localize the object instances within the image  figure   
shows a precision recall curve which was plotted by
computing the precision and recall on the test set of   
images by varying the threshold of probability used to
localize objects in the belief map 

figure     precision recall curve for the test set of    images

   future work
the current implementation does not support multiview object recognition  hence  the next step shall be to
share images of an object class taken from different angles
and then test them for multi view detection 

 

fiworkshop on applications of computer vision  wacv
     

another aim shall be to test the shared sift algorithm
over a range of objects with different basic shapes to
confirm that the variance filtering approach selects shared
descriptors that represent a generic description of objects
of a class 
to enhance the accuracy of the matching process of
descriptors in feature space  rather than relying on a
combined variance for all the dimensions  a metric similar
to the one in figure   will be used to compute the variance
for each descriptor in each of the     dimensions  this
will also change the distance formula used to in feature
space and distances in each dimensions will be normalized
by the variance of each shared descriptor in that
dimension  further  logistic regression will be used to get
variance normalized distances of matches of each shared
descriptor from positive and negative training images and
then use them to find a threshold between the distance to
an object descriptor and other descriptors  thus  variance
normalized distances of matches in test images will be
used to evaluate the sigmoid function based on the
parameters obtained off the training set to classify the
keypoint as a match or not 
lastly  to improve performance for testing new images 
instead of using the current approach of closely spaced
windows  a better approach will be to cluster the sift
features together into circular windows and assign
probabilities only to each of these clusters  this clustering
technique based on the hough transform will lead to a
significant improvement in performance 

    pingkun y   saad k   and mubarak s     d model based
object class detection in an arbitrary view  school of
electrical engineering and computer science  university of
central florida

   acknowledgement
this project has been made in collaboration with stephen
gould and prof  andrew ng  it would not have been
possible to translate this raw idea into an actual project
without stephens sound mathematical ideas and prof 
andrews guidance and direction  paul baumstarckss
help with the multinomial nave bayes classifier at an
important stage in the algorithms conception is also
acknowledged 

references
    lowe  d  g   distinctive image features from scaleinvariant keypoints  international journal of computer
vision         pp               
    a  torralba  k  murphy  and w  freeman  sharing visual
features for multiclass and multiview object detection  
technical report  csail technical report  mit       
    erik c  and jochen t    shared features for scalable
appearance based object recognition   ieee     

 

fi