state indexed policy search by dynamic programming
charles duhadway
       

yi gu
       

december         
in trajectory following problems actions customized for
local path characteristics are especially useful  requisite
to such exploitation however are  a  a non stationary
policy set i e   changes as the car traverses the
trajectory  b  an accurate correspondence between spatial
position and the policy  used  however  the accuracy in
this correspondence is inevitably degraded by run time
uncertainties  consequently we chose for the policies 
to be indexed over space instead of time increments  as
under such a scheme the correspondence would be
immune from the said uncertainties 

abstract
we consider the reinforcement learning problem of
simultaneous trajectory following and obstacle
avoidance by a radio controlled car  a space indexed
non stationary controller policy class is chosen that is
linear in the features set  where the multiplier of each
feature in each controller is learned using the policy
search by dynamic programming algorithm  under
the control of the learned policies the radio controlled
car is shown to be capable of reliably following a predefined trajectory while avoiding point obstacles
along its way 

the goal of our work is to investigate the feasibility of
employing psdp in trajectory following problems under
a space indexed scheme  more concretely  we aim to find
a suitable parameterization of the system s  action space
a  discount mechanism  reward function r and policy
class   such that the rc car would reliably follow a predefined trajectory in the absence of obstacles  and avoid
obstacles with some margin in their presence 

   introduction
we consider the reinforcement learning problem of
trajectory following involving discrete decisions  casting
the problem as a markov decision process  mdp   we
have a tuple  s  a   psa     r  denoting  respectively  the
set of states  the set of actions  the state transition
probabilities  the discount factor and the reward function 
the objective is to find a policy set opt that is optimal
with respect to some choice of r  in cases where the
system dynamics  r and  are linear or linearizable  opt
can be readily solved for e g  by linear quadratic
regulators  lqr       our current work however focuses
on the case where such linearity does not hold  and
therefore generalizes the class of trajectory following
problems
to
scenarios
involving
non trivial
environmental constraints 

we will first present a space indexed parameterization of
the system  and then define the policy class and feature
set  this leads on to the algorithms implementation 
starting with a brief definition of psdp  followed by a
section by section account of the choices made for the
various functions  parameters and other algorithmic
components  the results and accompanying analysis are
provided in the final part of the paper 

   system parameterization
typically  a reasonable system parameterization for a
vehicle could be

st    xt

we use a radio controlled  rc  car as a test platform 
and introduce non linearities to r and  through the
need to make discrete decisions in trying to avoid a point
obstacle along a pre defined trajectory  we choose the
policy search by dynamic programming  psdp 
algorithm as the means to finding opt  this is because
given a base distribution  indicating roughly how often
we expect a good policy to visit each state  psdp can
efficiently compute opt in spite of non linear r and 
     hence our trajectory following problem fits naturally
into this framework 

yt

t

ut

vt

u t  

vt    

t

where xt and yt are the cartesian coordinates of the
vehicle in the world coordinate system  wcs   t the
orientation of the car in the wcs  and ut and vt are the
longitudinal and lateral velocities in the vehicle
coordinate system  vcs   all values are with respect to
the center of the vehicle  which is assumed to be a
rectangle  the subscripts t and t   indicate the time
instants to which the values correspond  this
parameterization is illustrated graphically in figure   

 

fiall d d   sa come form the same policy class  and
for simplicity we chose each d d  to be linear in the
feature set i e 

note that time is not
included as a state as the
mapping of controller to the
time instant it is used makes
this implicit  under the
space indexed scheme for a
vehicle driving in r 
however  each controller is
mapped to a subspace in r
which we will call a
figure    timewaypoint  we define each
indexed states
waypoint to be a subspace
orthogonal to the trajectory tangent at the point where the
waypoint exists  this is illustrated in figure   

 d   d      d t  g  s d  

 d     d     d      d     d      t

g   s d      g  

ld

d

ud

vd

u d  

g 

g   

t

g  is simply the cross track error ld  g  is the yaw of the
car with respect to the target trajectory tangent at d  for
example if the orientation of the vehicle at d in wcs is
   while the trajectory tangent at d in wcs is     then
g     together g  and g  keep the vehicle close and
parallel to the target trajectory 

to
localize
the
vehicle
along
a
waypoint  we define
l as the cross track
error between where
the target trajectory
intersects
the
waypoint and where
figure    space indexed states
the center of the
vehicle intersects with the same waypoint  this gives a
space indexed system parameterization

s d    t d

g 

g  is the anticipated clearance surplus from the obstacle 
for instance if ld     cm      cm to the right of the
trajectory looking in the direction of travel  as the car is
approaching a point obstacle some distance ahead
situated   cm to the left of the trajectory  and the desired
clearance is    cm to the right of the obstacle  then
g                   cm i e  the car needs to be      cm
further to the right to avoid colliding with the obstacle 
as a conservative measure  g s value is offset by   cm
so that g      cm at     cm real clearance  increasing to
   cm at   cm real clearance  beyond which its absolute
value decays exponentially while remaining negative 

v d    

t

where td denotes the time at space index d along the predefined target trajectory  while d  ud and vd are defined
similarly as before except they now denote state values at
a space index d  note that the position of the vehicle is
implicitly but completely specified 

g  is intended to steer the car off the target trajectory
away and from an obstacle  g    when no obstacle is
within the control horizon  we allow the car to reevaluate which side to pass an obstacle on at each step 
the rule being that if the obstacle is currently to the left of
the car  then pass it on the right and vice versa  and g  is
computed accordingly 

the simple bicycle model s    f   s  a   is used to
describe the vehicle dynamics  where aa is an action 
state propagation from index d to d   is achieved by
first computing t such that at td   td t the vehicle is
precisely at waypoint d    then states at d   are
computed by propagating sd forward through f s a  by
exactly t  for brevity  states capturing historic values of
the steering angle used to model the steering dynamics
are not shown 

g  is the obstacle conditioned yaw  its value is non zero
only when the car is approaching an obstacle and is
steering in a direction that would reduce the clearance  in
such cases  g  is yaw of the trajectory tangent as
measured in the vcs  this feature is included to
preserve any clearance achieved via g   since an existing
clearance would tend to make g  small  and a controller
without g  would be dominated by g  and g  

   policy class

this concludes the definition of the policy class  and we
note in particular that the policy class is clearly nonlinear and cannot be easily cast into a form suitable for a
linear solver 

the ultimate performance of the algorithm depends
critically on the policy class  which is defined by both its
format and the features that constitutes it  as mentioned
previously  the class of policy employed is nonstationary i e            d   d      where
     d     so that we have a one to one mapping
between each index d and stationary policy d d  

 

fitrajectory using a heuristically tuned pi controller over
the target trajectory with no obstacles  the resulting state
sequence is used as the starting point for finding a lqr
controller using differential dynamic programming
 ddp   we finally obtain m samples by using the lqr
controller in step   above 

   algorithm implementation
    definition
we start by considering a trajectory that spans d space
indices and a policy class as defined in    we define the
value function v s 

v t       d    s   

   d  

e  r  s i     s d   s    d        d    
d  i d


 r   s     e s   ps

d  s 

 v

 d          d  

graphically  the psdp algorithm as defined in     and
initialized in     appears as in figure   

 s   

where s   ps d   s   indicates that the expectation is with
respect to s drawn from the state transition
probability ps d  s     the psdp algorithm takes the form 
for d   d    d     

set  d   arg max e s    d  v   d          d    s   
 

where we assume we are given a sequence of base
distributions d over the states  psdp is thus a policy
iteration algorithm that yields an optimal d for every
space index  the dynamic programming part is evident
in that the solution for d starts from the end of the
trajectory towards the beginning 

figure    psdp algorithmic flow
    action space
the control of an rc car involves steering and throttle  if
we choose to drive the vehicle at constant velocity  then
our action space is simplified to comprise of only
steering  the steering angle range of the rc car is a
continuous interval from      full lock left  to     full
lock right   in order to keep the calculation of v s 
tractable however this interval is discretized to    points 
a                           

    initialization
we now turn our attention to d  and note that we do not
know the set of base distributions         d    over a
pre defined trajectory  however if we had a controller
that could simultaneously track a target trajectory and
avoid obstacles  then by running the rc car over the
trajectory using that controller a large m number of times 
we would see a representative distribution of s at d across
all m trials  in this way we have effectively sampled from
d without knowing the explicit form of d  the problem
is that if we had such a controller  we wouldnt have
needed to run psdp to begin with  one way around this
catch    is to start psdp by sampling from a poor
approximation of d i e 

    transition probability
two mechanisms influence the transition from state sd to
sd   viz  system dynamics and noise  the former is
captured by f s a   while the noise is modeled as zeromean gaussian measurement noise  consequently  our
transition probability is

s d      n  s d   f   s d   a d  t       

   run the rc car over the target trajectory without
obstacles m times using a suboptimal yet reasonable
controller  add obstacles to each of the m
trajectories 
   derive a set of policies  using psdp based on
these m samples 
   run the rc car over m trajectories with obstacles
using  
   iterate over steps   and   until  converges  at
which point we deem that we have been sampling
from a good approximation of         d    

where   is chosen to be consistent with variance of state
measurements obtained from the rc cars kalman filter 
    discount factor
recall that v s  is the expected sum of r s  over distance 
and terms for which hr sd h       for some small  are
ignored as they have negligible effect  those more than
h   indices down the length of the trajectory from d   if 
is close to   then the learned policies are anticipatory of
the future and tend to be more globally optimal  however
the closer  is to   the larger p becomes for a given  
with a corresponding longer computation time  if  is
close to    the computation time would be shorter  but

in our implementation  the suboptimal controller is
derived in two steps  first we compute a noiseless state

 

fithe policies would also be less sensitive to future highcost consequences to current actions  making them only
locally optimal 
we found that the geometrically decaying weighting
envelope  imposes could not yield a satisfactory balance
between global optimality and computation time for any
value between   and    consequently we sacrificed
envelope continuity and instead employed a rectangular
weighting envelope  v s  is thus the sum of a fixed
number h of reward functions each of which is weighted
equally 

 a  no obstacle

    reward function
the desired behaviors of the car include  a  close
tracking of the target trajectory  b  reliable avoidance of
obstacles  and c  use of steering actions that are
realizable i e  limited bandwidth actuation  since it is
more natural to treat these attributes in the negative e g 
we do not wish the car to be far from the target trajectory 
r s  will henceforth refer to a cost function instead of a
reward function  the problem remains unchanged  only
now we wish to minimize r s  instead of maximizing it 
from these a   component cost function was conceived 

 b  collision
figure    cost function visualization


b l     b  q   
if no collision


r  s d       
c


 
otherwise
b  q   b     c    b 

  

where q  is derivative of steering angle  c is the distance

note that at this point  we have also fully specified the
mdp in the context of our trajectory following problem 
having defined the tuple  s  a   psa     r  
    optimization
referring to figure    in each iteration of psdp we have
m sample state trajectories each containing d points  for
a dp backup at d 

between the center of the car and a point obstacle  and c 
is the desired minimum clearance to an obstacle  if
c   c   then a collision is considered to have occurred 
this effectively treats the car as a circular object and was
a conscious and conservative choice 

   we start with sd on the first sample trajectory  note
the random number seed  take action a a 
   compute sd    sd h    h as defined in       the
corresponding costs r s  and va   s    the subscript

the term involving l penalizes deviation from the target
path  the term involving q  penalizes rapid changes in
steering  the constant b  penalizes a collision and the
term involving c and c  enhances the repulsion of the car
away from the obstacle during learning 

a  indicates unique correspondence to action a   
   re seed the random number generator with the value
noted in    take action a a and repeat step   
   repeat steps   and   for actions a  to a   
   repeat steps   to   for sample trajectories   to m 

figure   is a visualization aid to r s   which has discrete
regimes and is clearly non linear  note in particular the
expected parabolic appearance of the b  contribution  the
half cylinder and the slight half cone atop of it in b 
attributable to the b  and b  terms respectively  the
weighting parameters b  to b  were chosen
experimentally 

at the end of the   steps  we have a record of how v s 
varies with action at index d for each of the m samples 
we then use the coordinate descend algorithm with
dynamic resolution to find the  that minimizes the
expected value i e  we solve

 d   d     arg min e s     v  
 d   d       

d

d          d   h  

 s   

to expedite the numeric search  the starting point is
chosen to be the least squares estimate init  btb   btb
where b is the vector of value minimizing steering angle

 

fispecific to each trajectory and b is a matrix containing
the feature values associated with each trajectory  this is
based on the observation that the least squares solution of
 to individual steering actions that minimize individual
v s  is often close to the solution that minimized the
aggregate 

   

bounding circle

     

bounding circle

bounding box

  

bounding box

     

x

  

     

collision rate    

  

  

     
  

     
  

     

  

  

     
  

   results and discussion

 

 
 

 

 

 

 

 

 

 

 

  

iterations

 

 

 

 

 

 

 

 

 

  

iterations

figure    bounding box vs  circle collision test

    performance
the learned controllers performed well in both
simulation and the physical system  example trajectories
from both are shown in figure    to quantitatively
evaluate the performance of a learned controller we used
both the cost function and obstacle collision rate  the
controller shown in figure   had a collision rate of      

similar tests were used to choose the specific feature set
and cost function described above  among other things
we learned that the standard integral term was
unnecessary for our learning task  we also found it
important to use a cost function with no flat areas 
    conclusions
we have implemented a system that demonstrates the
feasibility and effectiveness of employing psdp in a
state indexed scheme to solve a reinforcement learning
problem with a non linear policy class and reward
function  our approach takes full advantage of psdps
strengths while eliminating its dependence on accurate
space time correlation 

the target and actual trajectories are shown in blue and
black respectively  obstacles are shown as double red
circles  the inner circle denotes the closest the physical
car could pass by the obstacle without collision if
perfectly aligned  exactly tangent to the obstacle   the
outer circle denotes the farthest the car could be and still
manage to hit the obstacle given worst case alignment
 exactly orthogonal to the obstacle   note the controller
perform wells despite the obvious difference in sampling
rates between the actual and simulated systems 

    next steps
the obvious next step is a detailed comparison of stateindexed psdp with time indexed psdp  we are
confident that such a comparison will clearly
demonstrate the superiority of state based psdp  this
should be especially clear over long or non uniform
trajectories 

acknowledgments
figure    simulated and physical trajectories

the authors wish to acknowledge valuable inputs from
andrew ng and adam coates during the course of this
research project 

circular trajectories were used to test the actual car
system due to physical limitations in our tracking system 
similar performance was observed in simulation for both
straight and oval trajectories and we expect our system to
perform well over a large variety of trajectories 

references
   

    choosing a policy class and cost function
choosing the appropriate policy class and cost function
was non trivial and required a series of tests to choose
between several alternatives  figure   shows the results
of one such test comparing two cost functions  one with
an accurate  bounding box collision test and one with a
conservatively inaccurate bounding circle collision test 
in this case the latters performance far exceeded the
formers 

   

 

abbeel p   coates a   quigley m   and ng a 
y   an application of reinforcement learning to
aerobatic helicopter flight  in nips         
bagnell j  a   kakade s   ng y a   and
schneider j   policy search by dynamic
programming  in nips          

fi