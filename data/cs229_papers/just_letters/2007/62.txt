robot grasping
hee tae jung
december         

   introduction
robots have been widely used in various industries to perform complicated tasks which require
fast and accurate performance  such as assembling the parts of vehicles and welding them
together  however  those tasks can be done only when the robots are hard coded to do so and
even a very simple task such as grasping a newly seen object is still a great challenge to a robot 
various approaches have been made and most of the previous works have been heavily relying
on the   d model of an object to successfully grasp it  however  building the   d model of a
newly seen object itself is a very challenging task  to address this difficulty      proposes the
learning algorithm that does not require building the   d model of an object  rather  they utilize
the features extracted from   d images to predict possible grasping points of the object  they
demonstrate that a robot can successfully grasp the object that it never saw before  but use only
the image data of an object leading to limited performance  hereby  im proposing to use the
depth map of an object in addition to the image to improve performance 
   training
in this project  i use logistic regression and synthetically generated data to train the algorithm  
this set of data includes the synthetically generated images and corresponding depth maps and
correct labels  which are the ideal grasping points  since the set of test data is in     by    
resolution  the synthetic data are resized down to the same resolution and divided into   by  
patches  hence  i end up having                              patches from each image file  i have
      pairs of object images  and the size of training data becomes too large when i utilize all
those patches for training  also  each set of label  the rightmost image of the three below  has
only a handful of positive labels  non black represents positive labels and black represents
negative labels 
 

 image 
 

 depth map 

http   ai stanford edu  asaxena learninggrasp data html

 label 

fithus  i use all the positive labels  the same number of randomly chosen negative labels and the
corresponding features to reduce the size of training data and to balance the number of positive
labels and the number of negative labels 
i come up with four base feature sets  each image file is divided into   by    pixel patches  and
the first set consists of these    pixel values  the second set consists of the edge  texture and
color features extracted from each patch  when extracting the features i use the same filters that
ashutosh used in      in depth map information   two same shapes in different distances end up
being considered to be different due to their different offsets  hence  the    depth values of
each patch are transformed by using z  minminzffff   where z is a   by    patch matrix  this
becomes my third set  the fourth set consists of the edge  texture and color features extracted
from each depth map patch by using the same filters  applying the same filters to depth maps is
somewhat nave approach  i try
ry differentt combinations of these four feature sets  and the results
are given in the next section 
 

   testing
i test on two different sets of data  the first being    held out
out synthetic data  and the second
being the reall data taken by swissranger   intensity information is used instead of image
i
information because fully calibrated
brated images
image from a stereo camera  bumblebee  
bumblebee  are not available
at this time  before using intensity and depth map data  i changed the scale of intensity and
depth map data to make
ke them consistent with that
th of synthetic data by using
 
 
ffffff
  where z is a   by  
 
patch matrix 
fiffff 
 
ffffff
 

 intensity 

 depth map 

 label 

features are extracted in the same manner as before  ideal grasping points are labeled by hands 
among the different combinations of the feature sets i tried  the following five combinations are
 

 

i tried three different options     not using transformation     applying z  minminzffff
min
and
 
ffffff
 
 

finally    using fiffff 
 
ffffff  i performed training and testing with these three cases 
and end up getting the best result when using z  minminzffff  and it has been used for the
rest of the project 
http   www mesa imaging ch 

fidisplayed below 
  
  
  
  
  

 
 
 
 
 

st
rd
st
st
st

feature set    
feature set    
feature set    
feature set    
feature set    

nd
th

rd
nd
nd

feature set  only relying on images 
feature set  only relying on depth maps 
feature set
feature set     feature set
feature set     feature set     feature set
rd
rd

th

the combination   and the combination   show relatively high accuracies as can be seen in the
graph below 

when tested on the    held out synthetic data  the combination   gives about        true
positive predictions and the combination   gives         however  when tested on the real data
taken by swissranger  the combination   gives about         while the combination   gives
about         at the first glance one may think the testing accuracies on the real data are way
too low  but the results are quite reasonable considering that the real data are noisier than the
synthetic data  the real images have a table and walls  the predictions from the combination  
and the combination   are visualized as below 
 

two mugs

 

combination  

             

             
    fi   
 fi       
ff

combination  

fiin general  the combination   gives clearer results
resul visually but the predictions were not that
accurate  on the other hand  the
he combination   gives too many false positive predictions  which is
       while the combination   gives only       of false positive predictions 
with thesee two results  i combined
combin the two predictions
ictions to boost the accuracy  first  each positive
prediction of the combination   propagates to its neighboring patch results  meaning  a patch
becomes positive if its neighboring patches are predicted to be positive  then the result from the
combination   is combined  different weights are tried 
tried and higher true positive
posi prediction rate
can be achieved byy giving more weights to the combination    all the way up to         however 
by giving the same weights to the combination   and the
the combination    more straightforward
results can be achieved sacrificing the true positive prediction rate  which decreases
decrease down to only
        the sample images with the predicted grasping points and the corresponding labels are
provided below  white dots
ots on the objects are predicted candidate grasping points 

bowl

plate

two bowls

cylindrical object

   future direction
this project can be further improved
im
in several ways  first  intensity
ntensity information should be
replaced with real   dd images so that more correct texture
texture and color features from the images
can be extracted  second  smarter features should be used for depth maps to better represent
the   dd characteristics of an object  third  real data can be used even for training 
traini and the last 
svm can be used instead of logistic regression  during this project  svm has been tried but it
couldntt handle the same amount of training data that logistic regression did  i ended up having
either out of memory error or over parameterized warning  hence  the first three should be
resolved before trying the last one 
   acknowledgment
i would like to thank ashutosh saxena and andrew y  ng  for the helpful discussion throughout
the project 

fi   reference
    robotic grasping of novel objects using vision  ashutosh saxena  justin driemeyer  andrew y 
ng 
       
international journal of robotics research  ijrr 

fi