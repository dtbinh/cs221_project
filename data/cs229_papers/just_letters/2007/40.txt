cs    final project
using wordnet and clustering for
semantic role labeling
richard fulton
rafulton stanford edu

ebrahim parvand
eparvand cs stanford edu

december         

 

fiabstract
in this paper  we compare the performance of several clustering algorithms on the
task of semantic role labeling  we use a baseline system based on logistic regression
classifiers  and also a distributional clustering algorithm based on word association lists 
we use a latent semantic analysis system to compare to two previously implemented
clustering algorithms  k means and a more comprehensive discriminative clustering
algorithm  we also focus on features we can extract through specialized corpuses
such as wordnet  overall  we demonstrate that semantically clustering text leads to
significant improvements on semantic role labeling tasks 

 

introduction

semantic role labeling  srl  is the process of annotating the predicate argument
structure in text with semantic labels         to make this slightly clearer  we are
attempting to label the arguments of a verb  which are labeled sequentially from arg 
upwards  arg  is generally the subject of transitive verbs  arg  the direct object  and
so on  consider  for example  the following sentence 

then  arg  john  gave  arg  jim   arg  the apple  
arg  is john  the subject of the verb gave  arg  is the apple  the direct object  and
arg  is jim  the indirect object  the one to whom the apple is given  
while many systems that perform srl use support vector machines  treating the
problem of tagging parsed constituents as multi class classification problems      we use
    regularized logistic regression for our models because of its vastly quicker training
time  we use multiclass logistic regression as our baseline  at points in this paper  we
also compare to a k means implementation  for our k means  the feature we extract
for our logistic model is simply the cluster in which a contituent headword appears 
we also compare to a discriminative clustering algorithm which was developed in our
earlier research  but that is not the focus of this paper  in the discriminative clustering
algorithm  each word is given a score as to how much it is associated with each cluster 
as opposed to discrete assignment 

 

dataset and associated learning tasks

our datasets are drawn from propbank  which is an annotated corpus of semantic
roles      we take the parses from propbank to extract headwords for each verb  then
ignore the parse once we have extracted our data  our dataset consists of a series of
verbs  arguments to those verbs  and a list of headwords of noun  or adjective or verb 
phrases which are examples of those arguments that we have in the data  we divide
the dataset into a training set and a test set  for each verb  for each argument of
that verb  we place in our training set the first     of constituent headwords in the
corresponding list  we test on two data sets        and          is the complete data

 

fiset  whereas in the      set we remove all constituent headwords that occur less than
fifty times and all verb arguments with less than ten constituents 
we wish to extract as much information as possible without the benefit of context  context provides many beneficial features  but if we can show improvements on
semantic role labeling without context  it seems likely that systems using contextual
information as features in their parses or semantic role labeling will benefit from our
findings 

 

extracting features from wordnet

   

overview

wordnet     is a lexicon of the english language that also captures the semantic relationships between words  it also contains information about different senses of words 
combines synonyms into structures called synsets  and facilitates the processing of
these features via an api  wordnet is made freely available for processing and analysis from its developers   maintainers at princeton university 
of particular importance to us are the hypernym   hyponym relationships captured
by wordnet  these terms deserve some explanation 
 hypernym
word a is a hypernym of word b if b is a type of a  the is a relationship  
e g  a car is a vehicle  so vehicle is a hypernym of car 
 hyponym
the converse of hypernym 
e g  a duck is a bird  so duck is a hyponym of bird 

   

features

we hypothesize that two nouns that have a large intersection of hypernyms are likely
to play the same semantic role in a sentence for a given verb  for instance  for the
word backpack  we find from wordnet that a
backpack is a bag  which is a container  which is an instrumentality or instrument 
which is an artifact or artefact  which is a whole or unit  which is a object or phyical
object  which is a physical entity  which is an entity
it comes as no surprise that the noun knapsack and backpack have the exact same
hypernyms and are even in the same synset in wordnet  therefore  we strongly suspect
that the two nouns would almost if not always be the same argument for a given verb 
e g  i wear a  knapsack  backpack  to school everyday  this is the justification for our
wordnet features 
using wordnet  we extracted the distinct hypernym synsets of given nouns  generated unique integer keys for these synsets  and appended the keys to the nouns feature
vectors using a sparse representation  i e  only the indices that are present are actually

 

fiin the vector   since some words have multiple senses  e g  dog has both the sense of a
member of the genus canis and someone who is morally reprehensible according to
wordnet   we ensured that only distinct hypernyms were placed in the feature vectors
 e g  for the above example  both senses of dog have entity as hypernyms  but we only
insert it once into the feature vector for dog  

 

distributional clustering

distributional clustering in relation to nlp tasks refers to the method of clustering
words according to different distributions in particular syntactic contexts      we implement distributional clustering using both a dependency based thesaurus developed
by dekang lin      and a database of semantic distances within wordnet 
as a side note  we mentioned in our project proposal that we would be experimenting with unlabeled data  after further consideration  we decided that distributional
clustering serves the same purpose as unlabeled data  namely the introduction of large
amounts of outside information to the training set  therefore we thought it not necessary to experiment with unlabeled data at this time 

   

dependency based thesaurus

as part of our research  we extracted features from a database of word associations
developed by dekang lin      for a given input word  the database returns a list of
words and scores indicating how closely each word is related to the input word  for
some of the associations for the word car  see table   a  

index
 
  
  
   
   

word
truck
taxi
tank
road
village

score
        
        
        
        
        
 a 

index
 
 
  
   
   

word
linebacker
player
dimaggio
official
republican

score
        
        
        
        
        
 b 

figure    word association lists
for every word x  we compute its association features as follows 
   for every word in the word verb pairs of the training examples  precompute and
store each words association list 
   each time word x appears in a training words association list  add a feature for
that list 
this provides an alternate way of determining relationships between different words 
if we see a word in the test set that we have never seen before in the training set  if
that word appears on another words association list we know that the two words are
similar to some degree 

 

fi   

semantic distance in wordnet

we also explored word associations in the wordnet database as an alternate dataset
for distributional clustering  of the several distance metrics proposed in      we chose
to utilize the jiang conrath measure  which was shown to give the best results  for
word associations for the word quarterback  see table   b  

  latent semantic analysis for semantic role
labeling
we implemented a latent semantic analysis lsa  system that uses singular value
decomposition  svd  to reduce the dimensionality of the training matrix to find meaningful semantic patterns  we ran svd on the matrix of word counts versus training
examples  in the following decomposition equation  m is our data matrix 
m   u v 

     

specifically  we examined the u decomposition matrix that corresponded to the training
words versus training examples  for each training example  if that training example
had a high relation to a particular word according to the decomposition matrix  we
add that training example as a feature  if a relation score was too low  we would skip
it 
not all of the words we are labeling in the training set are nouns  so we also
experimented with running svd on only the nouns  though we did not have time
to try other variations of svd like weighting the terms using tf idf  running svd on
nouns only had a similar effect to that of tf idf  because the nouns are in general the
more important words in the data set 

 
   

results and discussion
wordnet features

as the results show  our extracted wordnet features increased performance from baseline significantly  this confirms our hypothesis that using outside features  i e  features
from the entire english language  rather than extracting features just from the data
set is indeed advantageous 
we suspect that this is the case for the following reason  note that the incraese
from baseline is actually larger in the full data set than in the smaller  we believe this
is because in the testing phase of the larger data set  there is a higher probability of
having seen a word or a synonym of that word or a word with which it shares significant
hypernyms during training  and therefore the classifier has already learned parameters
for the some of the test words hypernym synset features for the given verb 

 

fi   

distributional clustering features

the results we achieved for distributional clustering were comparable to the results
of the wordnet features  on the smaller       data set  we matched the k means
accuracy  however on the full data set  we achieved significant accuracy gains over our
k means baseline  we hypothesize that the reasons are similar to those described in
    

   

latent semantic analysis

lsa was meant to be another baseline to compare with our k means baseline  and
though it improved a great deal over the baseline accuracy  it never reached the level of
k means results  we found that by running svd on nouns only  the accuracy increased
by more than a percentage point 

   

feature combination

though we did not experiment a great deal with the combinations of different feature
sets  we did combine the features we thought would yield the highest score  by combining the baseline logistic regression features  discriminative clustering features  and
the distributional clustering features  we achieved higher accuracy on non contextual
semantic role labeling than all previous attempts 

method
logistic regression  baseline 
baseline   k means cluster ids
baseline   wordnet features
baseline   lsa
baseline   lsa w  nouns only
baseline   distributional clusters
discriminative clustering
baseline   distributional clustering  wordnet 
baseline   distributional   discriminative clustering

       small 
     
     
     
     
     
     
     
     
     

     full 
     
     
     
     
     
     
     
     
     

figure    test accuracy on the small and large datasets using various methods

 

conclusion

in this paper  we have presented results on context free semantic role labeling using
clustering algorithms and feature extraction from outside data sources  we saw that
adding extra semantic information via automatic clustering of constituent words to
our logistic classifier significantly improved performance 

 

fi 

acknowledgments

we would like to thank david vickrey for many useful conversations and algorithmic
advice  as well as providing our datasets 

references
    a  budanitsky and g  hirst  semantic distance in wordnet  semantic distance 
    c  fellbaum  editor  wordnet  an electronic lexical database  mit press  cambridge  massachusetts       
    d  gildea and d  jurafsky  automatic labeling of semantic roles  computational
linguistics                  
    d  lin  automatic retrieval and clustering of similar words  coling acl   
     
    m  palmer  d  gildea  and p  kingsbury  the proposition bank  an annotated
corpus of semantic roles  computational linguistics                 
    f  pereira  n  tishby  and l  lee  distributional clustering of english words  clustering words paper 
    s  pradhan  k  hacioglu  w  ward  j  h  martin  and d  jurafsky  semantic role
parsing  adding semantic structure to unstructured text  proceedings of icdm 
     
    s  pradhan  w  ward  k  hacioglu  j  h  martin  and d  jurafsky  semantic role
labeling using different syntactic views  proceedings of acl  pages              

 

fi