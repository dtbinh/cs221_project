isaac penny
cs    
term project
final report

   introduction
for the term project  i applied machine learning to text classification in ancient documents  in particular  i used a
machine learning algorithm  trained on the pauline epistles of the bibles new testament  to determine the
probability that paul also authored the epistle to the hebrews  of the twenty seven books in the new testament 
only the epistle to the hebrews does not contain an explicit claim of authorship  however  tradition and the writings
of several early church leaders indicate paul as the books author 

   approach
a support vector machine  was chosen for the logistic classification process  an svm was chosen for its off theshelf ease of use and its wide acceptance within the field of text classification   the support vector machine further
utilizes a simplified version of sequential minimization and optimization algorithm   see the referenced papers for
more algorithm details 

    the text
all books were evaluated in greek to avoid the affects of translation  the greek text used is the stephanus edition of
the textus receptus  compiled in      a d  the text itself is in the public domain  however the project utilizes a
proprietary version obtained from hermeneutika software  under an academic license   the hermeneutika version
of the text also provides the greek root word and morphology  part of speech  number  person  tense  mood  and
voice  for each word in the text  an excerpt from the text showing john       for god so loved the world  is
shown below 
john     

bo

dams

nams

dams

viaa s
c

dams

aamsn

dnms
vsam s

c

vppanms
c

vspa s

nams

viaa s
p

rpams
nafs

dnms
c

nnms
rpgms
anmsn

xo
aafsn

each word above appears in triplet  the first word is the original greek word  the second word is the greek root
word  the third word in each triplet is the morphology of the greek word  ex  vsam s means a verb with
subjunctive mood  aorist tense  middle voice  which is  rd person and singular in number  

    training examples
each book in the new testament is divided up into chapter and verse divisions  by scholars to aid in easy
referencing  individual verses from each book were used as training examples  positive training examples were
provided by the    pauline books in the new testament  negative training examples were provided by the    nonpauline books in the new testament 

fi    parsing the text
the data needed to be extracted from its plain text format and stored in a useful data structure  before it could be
used for classification  the data was extracted using a simple text string search  where spaces were treated as
delimiters between words  the data was then parsed into a five dimensional cell array with the following
dimensions 
   book number      matthew      mark  etc  
   chapter number
   verse number
   word number within the verse
   string type      inflected word      uninflected word      morphology 
this structure maintains all of the original relationships between the data  while making it easy to extract the desired
from of a particular word from the text 

    feature selection
n gram frequencies were used as input features for each training example  the density of the data in the feature
space of n grams of size two and higher was deemed too sparse to be useful  thus only unigrams were used 
only root  uninflected  unigrams were used for classification  this approach results in a smaller number of features 
than if all of the inflected forms of a given word were used  the smaller number of features results in a less sparse
set of training data  the more dense training set helps the classifier generalize better to test sets where the test data
set has a predominantly different morphology than the training set  ignoring the morphology in determining
authorship assumes that the choice of root word  ex  play versus compete  is a more significant indicator of
authorship than is the choice of morphology  ex  played versus have been playing  

    creating dictionaries
a dictionary of all uninflected unigrams was created by scanning the five dimensional datastructure mentioned
above  the frequency of occurance for each unigram was also recorded 
one of the main goals of the project was to quantify the effect of feature space size on classification  thus
dictionaries of various sizes were created  d    is a dictionary composed of the one hundred unigrams that occur
most frequently in the new testament  choosing the most frequent unigrams has two benefits  first  the feature
space will be less sparse and therefore more useful for classification purposes  also  frequency of use with common
unigrams  

the 

and  etc   is only slightly affected by a works content  as such they are commonly used

indicators of authorship   equation   shows an example of the aforementioned dictionary 

   

d     

    
   

   
 



     

   

fi    cross validation
k fold cross validation was used to explore the effect of feature space size on classification error  in k fold crossvalidation  the original training data set is divided into k subsets  of the k subsets  a single subset is retained as test
data  while the remaining k    subsets are used as training data  the cross validation process is repeated k times 
with each of the k subsets being used exactly once as the test data set  cross validation error is then the mean
classification error among the k repetitions  

   results
    effect of feature space size on cross validation
the training data set was divided into ten subsets for cross validation purposes  figure     shows the effect of
feature space size on cross validation error 
   
  
  

cross validation error    

  
  
  
  
  
  
  
 

 

  

  

  

  
   
   
   
   
feature space size  number of unigrams 

   

   

   

figure      effect of feature space size on cross validation error 
as seen in the figure  using a dictionary size of more than fifty unigrams does not significantly reduce classification
error 

    percent verses classification
each verse in the book of hebrews was individually classified as pauline or nonpauline  this process was repeated
using each dictionary  the process also repeated for several other books believed to be either pauline or nonpauline 
figure     shows the resulting percentage of verses classified as pauline 

fi   

epistle to hebrews
pauline  romans  galatians  titus 
nonpauline  mark  john  revelations 

percent of verses classified as pauline   pauline

  
  
  
  
  
  
  
  
  
 

 

  

   
   
feature space size  number of unigrams 

   

figure      percentage of verses in hebrews and several other nt books classified as pauline 
as expected given the cross validation study  a feature space size  dictionary size  of greater than fifty unigrams
does not further separate the various book categories  also  it should be noted that the variance in the nonpauline
category is significantly larger than that in the pauline category  this is to be expected  as there is a lurking variable
of multiple authors within the nonpauline category  the resulting analysis shows that for dictionary sizes over fifty
unigrams  the mean percent of verses in hebrews classified as pauline is     

    statistical significance of results
the classification of verses from hebrews certainly appears closer to that of the pauline books than that of
nonpauline books  the question becomes whether this difference is statistically significant  using the central limit
theorem  we hypothesize that the percentage of verses classified as pauline for a book will itself be distributed
normally about the mean value for that books category  thus we can use a standard normal z test to calculate the
probability that hebrews is in the pauline and non pauline categories  for a feature space of size fifty  the pauline
and nonpauline z scores were calculated  a z score measures the distance of a data point from a categorys mean in
units of the categorys standard deviation  the standard normal distribution can then be used to find the probability
that the variation within each category can explain the datapoints departure from the category mean  this calculated
probability is the probability that the data point belongs to the category  the z scores and probabilities are
summarized in table     

fitable      results of z significance test using d    
category

z score

 

  pauline  category

 category

p   hebrews in category  

pauline

   

        

nonpauline

   

        

   conclusions
the z test indicates that hebrews as a whole is more likely to be in the pauline category than it is to be in the
nonpauline category  the low probability that hebrews belongs to either the pauline or nonpauline categories might
also suggest that hebrews was written by a mystery author whose writings are not otherwise included in the new
testament  however  since we do not have training data for the mystery author  one cannot evaluate such a
hypothesis using the current approach 

   future work
this conclusion is premature  multiple authors were lumped into the nonpauline category  thus it could be that the
variation within the writings of an individual nonpauline author is high enough to account for hebrews deviation
from that authors mean classification score  however  training individual classifiers for each of the new testament
authors has the downside of a lack of training data problem  this approach is suggested for future work 
this project only considers the authorship of hebrews as a complete unit  future work might statistically analyze the
distribution of pauline classified verses within hebrews to determine if certain sections of the book are more or less
likely to have been written by paul 

   bibliography
  

platt  john        fast training of support vector machines using sequential minimal optimization 
advances in kernel methods  support vector learning  mit press  boston 

  

boser  guyon  vapnik        a training algorithm for optimal margin classifiers   th annual acm workshop
on colt  acm press  pittsburgh  pp        

  

erasmus  desiderius        textus receptus  public domain  made available by hermeneutika software 
www bibleworks com accessed november          

  

various        context free grammar  wikipedia online encyclopedia  accessed at
http   en wikipedia org wiki context free grammar on december         

  

various        cross validation  wikipedia online encyclopedia  accessed at
http   en wikipedia org wiki cross validation on december         

fi