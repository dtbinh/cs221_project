date          
project members 
elizabeth lingg
alec go
bharadwaj srinivasan
title  machine learning applied to texas hold  em poker
introduction
part i
for the first part of our project  we created a texas hold em poker player that utilizes machine learning
techniques  such as reinforcement learning and supervised learning schemes   poker  being a game of
incomplete information as opposed to chess or checkers  is a more interesting domain to apply machine
learning techniques due to the large amount of uncertainty in the game  opponent cards are unknown
and players may bluff or try to play in unpredictable ways as part of their strategy 
our goal here is that the player must play optimally  with the given information  in   on   poker
games  we also added personality to the poker application  by allowing the player to bluff and behave
differently based on a certain set of heuristics  the player s personality is an important characteristic to
induce uniqueness and randomness in its play  so that it cannot be reverse engineered and beaten
trivially 
we investigated opponent modeling and strategy  some of the variables that we analyzed include
strength of hand  playing style of opponent  size of bets  stage of the game  bankroll and our
aggression level 
we designed a game interface to play heads up       texas hold em poker  with the simplest set of
rules  no  ante   straddling and other complicated events   and created two automated players that
learn how to play the game  both with different strategies and playing styles 
part ii
as a second part of our project  we made our game into a facebook application and opened it up to
facebook users  we collected data from the games that users played against our bot and analyzed the
data to find very useful and interesting results about the players  which our bots could capitalize on  to
play against players with similar playing styles 
in this paper we outline our techniques and state our results and findings 
the learning algorithm
we employed reinforcement learning  with a goal of maximizing the expected value  our bot learns to
play better by repeated training against itself 
we considered game strategy at each step of the poker game  for example  one strategy in poker is to
only take the first two cards or  pre flop  into consideration 
our model consisted of states  actions  and rewards  our states were the different rounds and the sets
of all possible outcomes in those rounds  table cards  player cards and pot sizes  
    pre flop   where each player is assigned two cards 
    flop   where   cards are drawn to the table 
    turn   where another card is drawn to the table 
    river   where the final card is drawn to the table 
our actions at each state were  bet  continuous action with varying bet amount   fold  discrete   and
check  discrete  

fiour rewards were calculated based on the expected value of winnings 
expected value   p win  state    amount in pot        p win   state     amount wagered
typically we use a q learning style algorithm  where we back propagate rewards  dollar amounts
won lost with respect to the bankroll and amount wagered  using a fade out for each step backward 
across all states of the game that contributed to that outcome 
the probability of winning with a given hand was updated after each round of game play  in our tests
shown below  we had         rounds in which our player played another instance of itself  figure  
 appendix  shows the results  in which we graphed the strength of hand vs  the probability of winning
with that hand  on the x axis  the hands are ordered from left to right  beginning with a  high card 
 the worst hand   then a pair  through to a  royal flush   the best hand   we also took into account the
card values in this ordering  for example  a pair of   s is lower than a pair of   s  it is very important
to note that the program did not know these probabilities prior to the application of the learning
algorithm 
figure    appendix  shows clear correlation learned by the bot  between the strength of the hand and
the probability of it winning the hand  the erratic dips towards the end of the curve are due to the fact
that those high strength hands were relatively rarer occurrences and the updates did not lead to a
convergence within the tested time span  training with more rounds would resolve this artifact  one
interesting finding is that the probability of winning closely resembles a logarithmic function in shape 
this is reasonable because after a certain threshold  it is highly likely that the player will win 
we had the bot relearn the probabilities of winning over         rounds  this time only considering
hand type  i e   pair  or  straight   as opposed to all possible hand combinations  figure    appendix 
shows the strength of hand versus the probability of winning at each stage of the game 
here  we notice how the correlation between hand strength learned varies with respect to the stage of
the game  as propagated backwards   obviously  then hand strength cannot be higher than a  pair  preflop  we see how the correlation is lower at the river than at the flop  consistently  which shows that
players do not usually go until the river unless they have quite a good hand  we have tackled this by
adjusting out betting amounts and probabilities depending upon the stage of the game  as explained
elsewhere in the paper 
we also tried an alternative where we include a combination of learning and direct computation of hand
strength in our algorithm  for the first three stages of the game  we learn the probabilities of given
hands as before  for the last two stages of the game  when we have more information  we compute the
exact probabilities of winning  this is done by evaluating all possible two card opponent hands  given
the table cards and our hand  and evaluating the number of cases where we would lose  by simulating
future table cards  
figure    appendix  shows the strength of hand versus the probability of winning in this scenario at the
final stage of the game  although the graph is only slightly different  computing exact probabilities in
the final stages of the game improved game performance significantly  before combining learning and
heuristics  the bot s average bankroll over    games was        against human players  after adding
the probability calculations  the bot s average bank roll against the same two players was        out of
   games  the typical reason for this is that it is the later stages of the game that are usually more
important  and if we calculate exact probabilities in those stages  we have a more accurate estimate 
we represented several heuristics in the form of a utility function  variables such as opponent s bet 
strength of opponent  and pot size were quantified with adjustable weights that were varied as the bot
learned and played more games  and also depended upon its mode of play  strategy and aggression  
for example  we learned the strength of opponent  based on percentage of wins and the size of bankroll
over time and we learned aggressiveness based on the size of the opponents bets  we applied these
findings to tweak the bots strategy  for instance  when playing against a strong player  the bot plays
more conservatively by betting heavy only when it has a monster hand 
in order to determine the parameters for the utility function which when to fold  when to check  and
when to bet  we played the bot against another instance of itself for about         games  if both

fiinstances of the bot ended with about the same bankroll  then we knew the bot was playing
consistently and the parameters were working well  if not  we modified the parameters 
as another step in our algorithm  we aim to teach the bot complicated playing and betting strategies
which would be hard to learn by playing itself  here we play the bot against human players which
follow different strategies  such as slow playing good hands and folding bad ones and the bot learns to
mimic this kind of behavior  we assign high weights to what the bot learns during this stage since we
have a lesser number of examples and also manually change some of the parameters  the bot decides
the appropriate strategy to employ based on a whole range of parameters  which is what makes it hard
to decipher the bot s strategy and counter it 
betting   strategy
an important aspect of creating a poker bot is to give it a  personality   the bot must carry its strategy
and behave in a non deterministic way so that it cannot be reverse engineered and beaten easily  our
algorithm has the bot pick from different levels of aggression strategies and also by teaching it to bluff 
slow play  etc 
typically  our player uses kelly s betting criteria to decide on optimal betting amounts  given the
bankroll  pot size and its estimated probability of winning  strength of hand  as explained above   then
it factors in a variable for aggression and bluff straight play  etc and depending upon the stage of the
game  varies its actions  an example of this is betting on a weak hand pre flop  a bluff  to intimidate
the opponent into folding  checking  or calling on a flush which was hit on the flop until the river  again
to fool the opponent and maximize gain on that hand  simple rules like  never fold when you can
check  are trivially learned by realizing that checking in that case always has an expected value which is
higher or equal to folding 
applying kelly s criteria  we have an expression for the optimal amount to wager  given the pot size 
bankroll and estimated probability of winning  which is  b   b p b    p   b   p b   where b   optimal
bet  b   player s bankroll  p   pot size and p   estimated probability of winning the hand  the
probability of winning at any given stage is calculated by computing all possible   card opponent hands
 typically      hands pre flop       hands at the flop  etc  and calculating how many of those hands
 beat  our hand given the current table cards  that gives us a sense of how many hands can beat us 
and thereby a probability measure  this measure is then adjusted  as mentioned previously  based on
our risk aversion and estimate of our opponent s playing style  frequent bluffer or not  etc   also  we
normalize this probability at each stage of game play  depending on previous actions from the
opponent  for example  if we compute a     probability of winning at the turn  but have seen that our
opponent never bluffs  and has been betting very high the last few rounds and continues to bet high on
the turn  we adjust our probability of winning downwards  accordingly to take that information into
effect 
we thus compute the optimal bet amounts as suggested by kelly s criterion and use that with varying
weights  depending on our strategy  performance and past history  to decide upon the best action  we
back propagate our wins and losses along with the loss amounts to tweak the expected value of any
particular state  that becomes a parameter for the state s utility function which takes into account other
variables such as hand strength and computes probabilities for each action 
for example  for a particular state  the resultant probabilities can be as follows 
fold call
bet
          
the actions are then picked based on these computed probabilities 
figure    appendix  shows the results of the player s performance over        simulated games 
we plotted the average optimal bet amounts for different pot sizes  averaged across all playing
strategies and stages of the game  figure   shows us the trivial result that  return is proportional to
risk or amount wagered  and the bot has learned to bet higher if more money is at stake in the pot 
more importantly  we see how this amount increases with a higher estimated probability of winning as
expected 
figure    appendix  shows the bot s optimum bet amounts for a pot size of        for its different
playing strategies  we notice how the amount wagered increases as the strategy gets more aggresive 

fiinteresting findings here were 
   in the aggresive mode  the bot wagers a full       for a p win            as seen from the
horizontal green line at        this is known to be a good strategy in poker   when you are playing
aggresively  an     chance is more than enough to go  all in  for any pot when the same amount is at
stake 
   while bluffing  the player bets a whole range of randomly oscillating dollar amounts  primarily to
confuse the opponent  we still see an increasing trend with respect to the probability of winning   this
randomness is what makes the player hard to reverse engineer or crack 
   notice how in  slow playing mode   the player bets significantly lower amounts on average than in
other modes  for the same estimated chance of winning  this is a sign that it has learned to slow play
consistently  in slow playing  the idea is to bet lower amounts until the end  to fool the opponent into
thinking that we don t have a very strong hand and extract as much money from the opponent as
possible 
figure    appendix  shows the average amounts wagered by the bot at the different stages of the
game  the highlight of this graph is that it shows how effective slow playing is  in the last column of
bars   slow play  mode   we notice that while slow playing  the bot wagers low amounts pre flop  at the
flop and at the turn but bets heavy on the river  essentially executing the slow play strategy perfectly 
facebook integration and data analysis
we also tested the bot against humans on a web and facebook interface  we performed analysis on
these games  and modified our parameters based on the bot s performance  for example  in one of the
early iterations of our bot  we saw that the bot would lose because the risk level of a human player was
very high  human poker players may be more risky when playing online poker compared to real poker
because real money is not at stake  after this discovery  we tuned a parameter to make our bot more
aggressive which helped mitigate the effects of a risky player  playing against human players also
allowed us to update the values in reinforcement learning 
as a sanity check  we played the online bayesian poker player  bpp  for fifty games  we found that our
player was able to outperform bpp by a net amount of       our player behaved more conservatively
and would fold if it did not have a good hand in order to minimize its losses  however  bpp would often
bluff and would lose by a significant amount if our player had a better hand  on loosing rounds  our
player would lose less money than bpp  and on winning rounds  our player would win more money  due
to its conservative strategy  in order to determine statistical significance  we applied pearson s chisquared test and found the corresponding p value  the formula for pearson s chi squared test is the
summation  from i   to     of  oi ei     ei  our observed values  oi  for each round  were the
amounts won and lost  our expected values  ei  for each round  were the average amount won      
this gave us a p value of      because our p value was less than      we were able to reject the null
hypothesis that the results happened by chance  figure    appendix  shows the results of the fifty
matches 
we integrated our application into facebook  in order to classify user player styles  with the facebook
api  and we turned our machine learning poker application into a facebook application  our goal was
to classify users based on response time  level of risk tolerance  strength of play  and bluffing 
specifically  we used k means to cluster these feature vectors  as a sanity check  we had one player
play under two different identities  both  identities  ended up in the same cluster  table    appendix 
shows the results of applying k means with k   
the interesting result is that four clusters correspond roughly to very good players  good players  bad pl
ayers  and very bad players  this clustering was also able to identify outliers  for example  one of the
clusters consists of one player  who had an average bankroll of      after each game  and bet on avera
ge      
we also performed analysis to see if we could associate more qualitative attributes of players to their
playing style  attributes include school  sex  age  and degree  we applied the apriori algorithm to learn
association rules between these attributes and their cluster  unfortunately  we could not find any
associations between these attributes and their cluster 

fiwe also used linear regression on different attributes of players  we found that the response time is
correlated with the player s probability of winning  for example  a player will take a longer time to
respond if they have a full house as opposed to one pair  figure    appendix  shows this correlation 
this shows a result observed across online poker games  which is the fact that players usually take a
longer time to place a bet when they have a good hand  this can be used as a utility function  thereby
guessing that if a player takes longer to place a bet  then he she probably has a better hand 
conclusions
we made several interesting conclusions 
broadly  we learned that machine learning techniques  particularly reinforcement learning is effective in
tackling partial information games such as poker and in many cases are able to play better than
humans  as they can calculate probabilities more effectively 
adding a personality to the player  making it bluff  slow play  etc gave the player the necessary
randomness and variance in strategy to make it hard to understand and beat 
the player learned the correlation between hand strength and probability of winning as a first step and
then went on to choose the optimal action and bet the right amount  based on a whole range of
parameters  particular to the game  its performance  and opponent 
from our analysis of the facebook game data  we were able to cluster people into different categories
based on their playing style  using several attributes such as their response time  aggression  and
average bet  our findings reinstated popular beliefs such as  people take longer to bet with stronger
hands   we were also able to identify behavioral patterns in players 
future work

after gathering more data  we could use our facebook findings more effectively to model the opponent
in our player  considering features such as player demographics to better understand players  styles
 for example  if we know that males under    are most aggressive players  we can use that fact to
change the way we would play males under     similarly  if we find that stanford players are more riskaverse than berkeley players  we can tweak our player accordingly  
other frameworks that represent the incomplete information  such as partially observable markov
decision processes  pomdps  or hidden markov models  would help state representation and learning
on those states  supervised learning and regression models could be used to make predictions based
on player data  error minimization algorithms can be used to determine the significance of various
attributes  our player can be improved after additional testing and development of new strategies 

fiappendix

figure    strength of hand vs  probability of win

figure    strength of hand vs  probability of win at each stage of the game

fifigure    strength of hand vs  probability of win at river

figure    average optimal bet amounts vs  p win  for varying pot sizes

fifigure    average bet amount vs  p win  for varying play modes

figure    average bet amounts by stage of game for various play modes

fifigure    performance against bayesian poker player

figure    k means graphs for each cluster  the x axis correlates with the   features mentioned in
table   

fifigure    probability of winning vs  response time for a human player

table    k means applied to playing style of all players
number
of
average bet
  hands   hands
people
size   
folded
bet
in
normalized 
cluster

average
bankroll   
normalized 

probability probability probability
of win and of win and of win and
checked
folded
bet

 

      

      

       

      

      

      

     

  

      

      

      

      

      

      

      

 

      

     

      

       

      

      

      

 

 

      

      

      

      

    

      

                 

    

        

       

        

        

average

fi