 

sparse coding of point cloud data
cs    project report

alex teichman
alex teichman cs stanford edu

abstractpoint clouds  made available through laser
range finders  stereo cameras  or time of flight cameras 
are frequently used in robot navigation systems  however 
no unsupervised machine perception algorithm exists to
provide understanding of the data  e g  that a particular
blob of points looks roughly like  say  a car  in this paper 
we take steps towards such an algorithm based on sparse
coding  the work here generalizes to any binary data 
an algorithm for learning the bases and the activations
for point cloud data is derived and demonstrated  given
precomputed basis vectors and an input vector  calculating
the activations is very fast and could be used in real time
applications 

i  background
sparse coding is known to be used in the abstract
representation of input in biological sensory systems     
an efficient method of determining the bases and activations of a sparse coding representation of image data
has been specified already      but the initial assumptions
that are made need to be changed for use in the point
cloud regime 
there are two main reasons for doing this work 
first  the bases and activations may be useful as features
in other machine learning algorithms  second  stacking
layers of these algorithms in a hierarchy may result
in even more abstract and useful features that would
make the job of recognizing frequently seen objects in
the environment easier  this may also provide some
insight into the way the brain generates abstract features 
though this is a secondary goal to developing useful
unsupervised machine learning algorithms 

fig     an example of ashutosh saxenas point cloud data of natural
scenes     

cubes are vectorized to create the inputs x i    the
same reasoning applies for the bases  matrices x  b 
and s have columns of inputs  activations  and bases
respectively  b can also be seen as a matrix with rows
of btl vectors 

ii  o ptimization p roblem d erivation
we use the following conventions 
x i 



b l 

 rk   a basis vector  l       n  

s i 




b

 

      k   an input vector  i       m  
rn   an activations vector  i       m  

because we working with  d point cloud data  the
the receptive fields in this case are cubes  the geometry
of the data is ignored  however  and the receptive field

 

 

 b   
 


 





 
b   
 
bt 
bt 
  
 
btk

 




b n  
 



 





we make the following assumptions to reflect the binary nature of the data and the sparsity of the activations 

fip  x i   s i    b 

m
y

 

   b  b  obj 
t
 i 
m x
k
x
xj aj s i 
 

b  obj    
 i   i t t
b aj
i   j       exp xj s
b

 i 

p  xj  s i    bj  

i  
 i 
p  xj  s i    bj  

 i 

   xj btj s i   
 
 
 i 
    exp xj btj s i   

p  s i    

several other methods were tested for calculating
the bases  l  logistic regression with the constraint on
the bj s and
p gradient descent on the objective function
subject to l   b l          both failed  likely because of
the inadequacy of the constraints 

exp   s i       

starting with the usual map estimate used in sparse
coding and applying the above assumptions  we have the
optimization problem
x

mins b

  s i      

xx

i

i

iii  r esults
the alternating minimization described in the previous
section was run on ten thousand  x x  cubic samples
 discretized at three points per meter  from ashutosh
saxenas laser scans of natural scenes      convergence
was declared when the change in all of the bases or all of
the minimizations  defined by euclidean distance of the
vectors  dropped below     for at least ten iterations
or when     iterations completed  the number of bases
was chosen to be thirty 
all code was written in matlab  the matlab version
of the l  regularized logistic regression solver made
available along with     was used for the calculation of
the activations 
the resulting bases seem to be a mix of planes and
gradients at different orientations and small sets of points
with no apparent structure  an example of typical bases
that result are shown in fig     the average time to
calculate the activations given the bases and an input
vector was       seconds  the average time to compute
the bases with projected gradient descent given the
activations and all the inputs was      seconds  to give
a rough idea of total computation time  the test which
produced the example in fig    took about an hour and
a half to converge on a linux box with an intel core  
duo t         ghz processor and  gb ram 

 i 

log p  xj  s i    bj      

j

  b l           l 

s t 

the norm constraint on the bases is necessary because
there always exists a linear transformation of b l  and s i 
which will not change the reconstruction term but will
make the sparsity term go to zero 
this optimization problem will be solved by alternating minimization  first  consider the case of holding the
bases fixed and finding the activations      can then be
written in the following form 
mins



x

  s i      

xx

i

i

j

min    s i      

x

 i 

log p  xj  bj   s i   
 

 
p

i

s i 

 i 
log p  xj  bj   s i   a

   

j

each of the i      m minimization problems in    
can be solved efficiently using the l  regularized logistic
regression algorithm described in     
now consider the case of holding the activations fixed
and finding the bases  we can then write     in the
following form and use projected gradient descent to
solve it 

minb



xx
i

s t 

j

log

 
 i 

    exp xj btj s i   

  b l           l

gradient descent is run on the objective function only 
at every iteration  b is projected back to the feasible set 
aj is a column vector used to select a column from b t
 i e  one of the bj s   this gives us the update rule

fig     five bases are shown here  since they are a  x x  cube of
real numbers    slices of each base are shown across the rows 

 

fiiv  d iscussion
the calculation of the bases is relatively slow  but
the calculation of the activations is very fast  with the
current code  finding the activations for  say      inputs
that comprise the eye of a robot would correspond
to a processing rate of about  hz  this is approaching
the rate required for useful real time operation  further 
this code was all in matlab  and it is likely that faster
implementations could be produced  this is encouraging 
a  shortcomings
beacuse the activations are positive or negative numbers  there is not a natural interpretation for stacking the
algorithm  i e  making the outputs of one the inputs of
the next  this is a somewhat serious concern 
further  it was inteded that the basis vector activations
correspond roughly to which features are present in
the data  however  this isnt the case when negative
activations are possible  it makes sense to allow positive
and negative values for elements of basis vectors in
the same way that neurons in the early visual system
respond to input with light present in one area and
absent in another  however  reconstructing the input
by subtracting features doesnt have a neural correlate
that i am aware of   but i am not an expert in this 
more importantly  it seems that positive activations  i e 
presences of basis vectors in the input data  could be
more useful in classification tasks  that intuition may be
completely wrong  but it is something to explore further 
for example  look at the activations for the input in
fig     the base in the first row is used to subtract from
all layers except the middle  the next two bases are
gradients and are used to add to the region with the
points present  the input  however  has nothing to do
with planes or gradients 
these two shortcomings could be addressed by changing the assumptions so that s i   rn    the binary input to the next layer could then be generated
from p  s i   x i    b   another approach might be to try
s i         n with a larger set of basis vectors to make
up for the lack of granularity in the activations 

fig     an example of an undesirable reconstruction resulting from
negative activations  for the input  white indicates a point that is
present  the numbers underneath the base labels indicate the value
of the activations 

some tweaking of the current algorithm also needs
to occur  it is possible that the less desireable bases those with just a small set of points with no apparent
structure   can be removed by training on a larger and
more varied dataset or changing the sparsity parameter
  also  cross validation over the number of bases would
be interesting to see  only the pre set choice of    bases
has been tested so far 
finally  it would be interesting to test this algorithm
on binary data of a different sensor modality to see if
the resulting bases are useful 
v  c onclusions
in this paper  we derive an efficient sparse coding
algorithm for binary inputs  it is possible that the resulting bases could be used as better features for other
machine learning algorithms  it is also possible that
stacking modified versions of this algorithm would result
in hierarchies of more abstract features that would be
even more useful 
building on the very efficient implementation of l 
regularized logistic regression in      we show that this
method has the potential to be useful in real time
applications 

b  future work
despite the discussion in section iv a  the first thing
to do is see if the resulting sparse representation of the
data can be used for anything useful  two immediate
opportunities present themselves  using the bases as
features in detecting the presence or absence of cars
in velodyne data from junior and using the bases as
features in finding a grasping point for objects with
stair  suggestions for other applications are welcome 

vi  acknowledgements
many people were helpful in the creation of this
work  thanks to honglak lee  rajat raina  suin lee 
ashutosh saxena  varun ganapathi  dan ramage  quoc
le  paul baumstark  and catie chang for making code
available  making data available  and or discussions on
the derivation 
 

fir eferences
    a  saxena  a  ng  and s  chung  learning depth from single
monocular images  nips  vol             online   available 
http   ai stanford edu  asaxena learningdepth 
    b  a  olshausen and d  j  field  sparse coding of
sensory inputs  current opinion in neurobiology  vol     
no     pp          august        online   available 
http   dx doi org         j conb            
    h  lee  a  battle  r  raina  and a  y  ng  efficient sparse coding
algorithms  in advances in neural information processing systems     b  scholkopf  j  platt  and t  hoffman  eds  cambridge 
ma  mit press        pp         
    s  i  lee  h  lee  p  abbeel  and a  y  ng 
efficient l  regularized logistic regression  in aaai 
aaai press         online   available  http   dblp unitrier de db conf aaai aaai     html leelan  

 

fi