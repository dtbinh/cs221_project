computationally efficient evolutionary algorithms 
enhanced by on line machine learning
jong han kim and taehoon kim
abstract an efficient evolutionary optimization algorithm
of which the convergence is improved is proposed  a machine which learns the parameter cost relations on line is
implemented inside the evolutionary algorithm  and the machine reuses the parameter cost information as training sets to
update the hypothesis functions  as the populations converge
and regression accuracy improves  some portion of the cost
evaluations are substituted with machine learned regressions 
and they are put into the selection process  this significantly
reduces the computational load and running time  because the
training computation of the machine is much cheaper than
the actual cost function evaluations  also  this implies that
the effective number of offsprings can be easily increased 
which leads to improved convergence with little increase of the
computational load  the improved convergence is shown by a
simple numerical examples and a practical design problem 

i  i ntroduction
evolution based optimization methods have a number of
advantages over traditional hill climbing   e g   gradient
descent  newtons method  techniques  unlike the gradientbased methods  evolutionary algorithms can easily escape
from local minima  eventually converging toward the global
minimum  also  the evolutionary algorithms can be applied
to any type of cost functions  since they do not require the
gradient or any higher order information  for cost functions
with discontinuities or peaky noise components  the gradient
is not always well defined or computed with poor numerical
accuracy  which frequently results in failure of gradientbased algorithms    
however  evolutionary computation requires numerous
cost function evaluations  which normally yields to heavy
computational burdens  the cost functions encountered
in practical optimization problems typically involve timeconsuming computations such as numerical integration or
large matrix inversion  therefore frequent evaluation of the
cost function is not desirable  in standard evolutionary algorithms  a population of sample points evolves to select better
individuals based on the fitness measure  by the processes
called selection reproduction  these processes inherently impose repetitive evaluations of the cost functions within the
previously visited domains  of which the ranges reduce as
the population evolves toward next generations  however 
conventional evolutionary algorithms use the computed cost
this work was conducted as a term project for cs    machine learning
class  the authors are grateful to prof  min jea tahk at kaist for his
guidance and discussions on evolutionary algorithms and applications 
j  h  kim and t  kim are with the department of aeronautics and astronautics  stanford university  ca         jonghank 

taehoonk  stanford edu

information only for the fitness evaluation and dispose of
them without storing 
we claim that this wastefulness can be improved by using
machine learning techniques  a machine which learns the
parameter cost relations is implemented inside the algorithm 
as the algorithm evaluates the cost function repetitively  the
machine reuses those information as training sets to update
the hypothesis functions  since the region in which the cost
is evaluated contracts around the optimum as the generation
number increases  the regression performance around the
optimum gradually improves as they evolve  i e   the machine
learns the large scale macroscopic views of the cost functions
in the early stages of the evolution  and the scale reduces as
the population evolves  eventually achieving a very accurate
approximation around the optimum in the local microscopic
views  as the regression accuracy improves  some portion
of the cost evaluations are substituted with machine learned
regressions  and will be put into the selection process  this
significantly reduces the computational load and running
time  because the training computation of the machine is
much cheaper than the actual cost function evaluations  the
proposed algorithm is first shown using a simple numerical
example  then applied to a practical autopilot design optimization problem 
ii  p roposed s cheme
a  evolutionary algorithms
typical evolutionary algorithms find optimal solutions by
iterating the following steps 
conventional evolutionary algorithm 
   initialization
   offspring generation  n  
   cost evaluation
   fitness evaluation and selection for the next generation
 n   n   n   number of parents 
   check termination condition
where n and n represent the number of parents and
offsprings in each generation  increasing n typically leads
to rapid convergence in a small number of generations 
however  the computational demands per each generation
increases linearly with n   thus actual convergence rate
 convergence per unit time  or convergence per number
of function evaluations  does not reduce significantly  the
proposed algorithm introduces a way to increase effective
n without increasing the computational load linearly 

fig y 

initialize

weight vectors
wi

weight vector
v

g y 

generate offsprings

generate additional offsprings

x 



j

g y 
x 

cost evaluations

neural networks

selection

 
g y 

terminate 

initialize

fig     computationally efficient evolutionary algorithm  shaded areas
represent the additional processes 

 

fig    

multilayer feedforward neural networks

output layer update 
b  efficient evolutionary algorithm
in the proposed algorithm  we add a machine which
learns the parameter cost relations  and consequently
several additional steps are needed to train it and use
it  given that the computational cost required by the
machine training computation is much cheaper than the
cost evaluation  a reasonable assumption for practical
optimization problems with complex cost functions   the
load increased by these extra steps is assumed to be
negligible  the flow chart is shown in fig    
efficient evolutionary algorithm 
   initialization
   offspring generation  n  
   cost evaluation  and machine training 
   additional offspring generation  n  
   cost evaluation by additional offspring
 by machine computation 
   fitness evaluation and selection for the next generation
 n   n   n   n  
   check termination condition
c  multilayer feedforward neural networks
multilayer feedforward neural networks with a single
hidden layer are implemented to learn and approximate the
parameter cost relations  the structures are shown in fig    
t
j x      g w t z  g w t z     g wn
z      v
hu

where z    xt   t   x  rn   wi  rn        i  nhu  
v  rnhu      and g y           ey    nhu represents the
number of hidden units 
the neural networks are trained on line as the evolutionary
algorithm evaluates the cost function  the gradient descent
back propagation
algorithm to minimize the error function
p
e   k     y  k   j  k     was used    

    e j  k     y  k   j  k   
vi    vi   g wit z  k   
vnhu       vnhu      

   i  nhu
  learning rate

input layer update 
i   vi g   wit z  k    
wi    wi   i z

   i  nhu

 k 

  learning rate

where the superscript  k  represents the k th training set 
iii  n umerical e xample
the proposed optimization scheme is demonstrated in a
simple numerical example 
a  introductory example
the first example is a single variable optimization problem  this function has its global minimum at x     
with f  x        fig   shows the function on    
x      since this function has many local minima  any
gradient based algorithm may not be a good tool for global
optimization 
 

minimize f  x       ex

   

cos  x 

approximation performances are shown by snapshots at
several generations  in fig      st generation  and fig      th
and   st generation   in both plots  solid curves are obtained
by using neural networks  and dotted are the actual cost
function curves 
fig    compares the convergence histories of both conventional and proposed evolutionary algorithms based on   
runs each  for both of algorithms  n      and n      are
chosen as the evolurionaty algorithm  ea  setup parameters 
and n      and nhu      are chosen additionally for the
proposed algorithm  which effectively doubles the offspring
population   we can observe that the proposed algorithm

figeneration  

conventional evolutionary algorithm

 
 

  
cost

   
   

 

  

   
   

 

  

  

  

  

   

  

   

j

 
proposed evolutionary algorithm

   

 

  
cost

   
   

 

  

   
 
  

 

 
x

 

 

  

  

  
generation

cost function f  x       ex

fig    

  

 

   

generation   

cos  x 

fig    

convergence histories of conventional vs  proposed algorithm

generation   

 

    

   

    

medians based on    runs each
conventional algorithm
proposed algorithm

 

   

  

    

   

    

   
 

  

j
    
    

   

    

   
 
 

 

    

   

cost

j

    
 
   

 

 

 

 

 

 

 
   

 

    

x

fig    

 

    

   

    

   

    

  

 

  

x

snapshots at the   th and   st generation

 

  

  

converges earlier than the original algorithm  while some
runs of the original algorithm failed to converge to the global
minimum 
for this toy example  a function evaluation is just computing simple explicit functions  and does not require
heavy computational load  it is even cheaper than training computing the machine  therefore  actual time to convergence is increased by applying the new technique  however 
in case of practical problems with time consuming cost
functions  the proposed algorithm will reduce the actual time
to convergence  we will see this in the next chapter 

  

  

  

  

  

   

generation

fig    

convergence histories  median 

transfer function

 
e

iv  a pplication   autopilot d esign
a practical autopilot design problem is chosen as the
application problem 

 

az
 
e

a  f    longitudinal dynamics

mu
sq cme s  cme cz   cm cze
 
mu iy  
iy
mu c
 sq   c s    sqc cz   sq  u cmq  s
c
cmq cz  mu
    u
sq cm  
iy
 
sqc cze s
   g
u

c
 u cmq cze s   cme cz  cm cze
 
iy
mu iy  
mu c
 sq   c s    sqc cz   sq  u cmq  s
c
    u
cmq cz  mu
sq cm  



fig    shows the normal acceleration control loop of an
f    fighter     the design parameters are the amplifier gain
samp   and the rate gyro gain srg  
az cmd

table i
n umerical data for a cruise flight condition
h m 
     
u  m sec 
      

cl
    
c d
    

c l
    
cm
     

c l e
   
cme
    

 
 

samp

 
 

cd
    
c
c
 u mq
       

  
s   

e

aircraft
dynamics

az
 


srg

fig    

normal acceleration control loop for f    aircraft

ficonventional evolutionary algorithm
  

   
cost  
inbetween area

 

s

amp

 

   

 
 

z cmd

   

a  a

z

 

   
 

 

  

  

  

  

  

  

  

  

  

   

  

  

  

   

conventional evolutionary algorithm
 

   

reference model
actual response

   

s

rg

 

   

 

 

 

 

 

 

   

 

time
 

fig    

 

  

  

  

  

  
generation

  

cost function for certain samp and srg
fig    

b  cost function

evolution histories for a conventional algorithm
proposed evolutionary algorithm

  

the desired reference model was set by a second order
model with n     rad s  and        

the two parameters  samp and srg   are to be optimized so
that the response signal is as close to the reference signal
as possible  for faithful implication of this objective  the
optimization problem is defined as follows     a graphical
interpretation is presented in fig    
z tf
 az  t   az ref  t   dt
minimize j samp   srg    

amp

s

 
 
 
 

 

  

  

  

  

  

  

  

  

  

   

  

  

  

   

proposed evolutionary algorithm
 

   
rg

az ref
   
az cmd
s    n s   n 

s

n 

 

 

   

ti

 

 

  

  

  

  

  
generation

  

c  improved convergence
the evolution histories of the parameters by a conventional
algorithm are presented in fig     note that some runs failed
to converge to the global optimum  because of the small
offspring population  n      and n        the evolution
by the proposed algorithm  with n      and nhu      
are shown in fig      where all the trial runs converged to the
correct global optimum  we can observe that the convergence
is significantly improved by the proposed algorithm  with little increase of computational load  for this specific problem 
less than     of computational load is additionally required
for neural network training and computation  however this
number depends on the complexity of cost functions  and
will be much smaller for problems with more complicated
cost functions 
v  c onclusion
computationally efficient evolutionary algorithms are developed  neural networks are implemented inside the evolutionary algorithm  learning the parameter cost relations  online training leads the approximation accuracy to improve as
the populations evolve  then additional offspring populations
whose cost values are computed by the neural networks
are generated  because the computational load additionally

fig     

evolution histories for the proposed algorithm

required for these processes is usually much less than that
required for the actual cost evaluations in practical optimization problems  these proceses increase the effective offspring
populations with little increase of computational load 
the proposed algorithm was applied to a numerical example and a practical autopilot design problem  it was
demonstrated to improve convergence characteristics without severely increasing computational load  compared to a
conventional algorithm 
r eferences
    t back  evolutionary algorithms in theory and practice  oxford
university press       
    s haykin  neural networks   a comprehensive foundation   nd ed  
prentice hall      
    j h  blakelock  automatic control of aircraft and missiles   nd ed  
wiley       
    c s park and m j tahk  a co evolutionary minimax solver and its
application to autopilot design  preceeding of aiaa guidance 
navigation  and control conference  boston  usa  pp          aug 
     

fi