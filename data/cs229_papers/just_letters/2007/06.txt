cs    project  tls  using learning to speculate
jiwon seo
dept  of electrical engineering
jiwon stanford edu
abstract
we apply machine learning to thread level speculation  a
future hardware framework for parallelizing sequential
programs  by using machine learning to determine the
parallel regions  the overall performance is nearly as good
as the best heuristics for each application 

   introduction
parallelizing applications has become widely accepted as an
inevitable trend to boost the performance of computers 
major microprocessor vendors have already announced
their roadmap as multi cores  however  the conventional
method of parallelizing applications with locks significantly
decreases programmability  hindering us from effectively
exploiting the increasing computational resources  thus  it
has become crucial to devise a new  but simple framework
for parallelizing applications  recent studies have proposed
several ways to replace the locking method and ease the
work of parallelizing applications  among which threadlevel speculation is a well known candidate     
thread level speculation  tls  is a hardware technique to
parallelize sequential programs by speculation  although
this is a promising approach to further utilize many core
architectures  choosing the speculation points is still a
challenging task  previous researches have attempted to
solve this problem using heuristics         although some
heuristics have shown near optimal performance for a
limited set of applications  their applicability to other
applications is questionable 
we propose using machine learning to find the optimal
speculative regions of method level speculation  more
specifically  our approach attempts to classify each method
invocation site and speculatively fork threads at the correct
call sites  by training the algorithms with various
applications  this approach provides a general framework
which efficiently parallelizes a wide range of applications 
our main contributions are 


this study is the first to explore the potential of using
machine learning algorithms to automatically detect the
parallel regions for thread level speculation 

sang kyun kim
dept  of electrical engineering
skkim   stanford edu




our approach does not rely on runtime information 
using runtime information either involves additional
runtime overhead or time consuming profiling  by
combining machine learning with statically extracted
features  we eliminate this burden 
machine learning offers a general framework that shows
reasonable performance for a wide range of applications 

   method level speculation
    overview
over the last decade  thread level speculation has been
widely recognized as a promising way to facilitate the shift
from sequential to parallel programming  unlike other
parallelizing techniques  such as transactional memory     
tls exploits the parallelism in sequential applications by
speculatively executing segments of code in parallel 
potentially parallel segments of a sequential code are
usually identified and annotated by the runtime system or
tls compilers  using the annotated data  tls hardware
then optimistically forks threads and maintains the flow of
execution  since parallelizing the code must not alter the
application behavior  the tls hardware also keeps track of
memory loads and stores to detect true dependency
violations  if such violation occurs among threads  the more
speculative threads will roll back and re execute their piece
of code 
locating the regions to speculatively parallelize has proven
a difficult task  randomly selecting any segment of code
may actually degrade the performance due to excessive
violations  previous research has mainly focused on finding
parallel regions at the loop level      parallelization in this
case is achieved by mapping iterations to different threads 
it is also possible to parallelize applications at the method
level         where speculative threads are forked at method
invocation sites  due to time constraints  we focus here only
on method level speculation in this project 

    challenges
although previous researchers have demonstrated the
promise of tls  the proposed heuristics are far from
perfect  for example  these heuristics tend to favor different
applications  thus limiting their applicability  whaley    
shows that a heuristic optimal for one application may

fimethod specific features

description

store count

the number of memory stores within the method  the probability of a violation increases
with the number of stores 

code length

the length of the code  this number attempts to capture the correlation  if any  between the
code size and the method execution time 

call count

the number of method invocations in this method  the probability of longer execution time
increases with the number of call count 

loop count

the number of loops in this method  the probability of longer execution time increases with
the number of loops  especially nested loops  

call site aware features

description

distance to conflict

the minimum distance to a potential raw conflict when the call site continues to run
speculatively  the probability of a violation increases as the minimum distance to potential
conflict decreases 

number of conflicting accesses

the number of potential raw conflicts when the call site continues to run speculatively  the
higher this number  the greater the probability of a violation 

remaining code length

the remaining code length of the method containing this call site  the probability of wasting
idle cycles decreases with the remaining code length 

table    the features used for learning
perform poorly on other applications  therefore  given a
specific application  it is difficult to decide which heuristic
to use 
another challenge in thread level speculation is the adverse
side effects when using runtime information  many existing
heuristics require profiling data or runtime statistics 
although profiling provides useful hints about the actual
execution of interest  profiling itself is impractical in many
cases since the user must run the applications beforehand 
collecting runtime statistics and applying heuristics on the
fly may also be undesirable as these methods impose
significant additional runtime overhead 

   machine learning approach
    overview
machine learning is a broad research field that studies and
develops algorithms by which computers learn to extract
useful information from a set of data  more specifically 
machine learning algorithms capture the inherent statistical
characteristics of a given data set and effectively predict the
characteristics of new data  therefore  if some information
extracted from a piece of code can be mapped to a
statistical model  we can use machine learning algorithms to
obtain interesting characteristics of the code  in this paper 
we propose using machine learning algorithms to find the
statistically optimal parallel regions of a sequential code  in
particular  we focus on method level speculation  hence 
speculative regions are chosen by applying the learning
algorithms at each method invocation site 
in contrast to the previous research discussed above  we do
not perform any runtime analysis  we believe that the static
context in a code is likely to contain sufficient information

of potentially parallelizable regions  intuitively  the
functions with similar runtime behavior  e g  execution time 
are expected to share some characteristics in their static
context  thus  although this approach does not guarantee
high performance  machine learning algorithms should
maximize the likelihood of performance optimality 
since the workload size is determined at runtime  this
feature cannot be extracted from static analysis  for
simplicity  we assume that the workload size is always
adequately large with the justification that most applications
of interest require a large amount of runtime  small sized
workloads may also show different parallel behavior even
with the same code  therefore it is infeasible to specify
parallel regions that uniformly apply to every dataset 
unlike previous research  the goal of this project is not to
exceed the performance of existing heuristic approaches 
amdahls law implies that the speedup from parallelism is
limited by the coverage of the speculative regions  for
example  even with     coverage  the speedup is limited to
    with an infinite number of cores      since previous
studies using heuristics have shown results somewhat close
to this limit         only targeting performance would not be
a significant contribution  instead  we demonstrate that the
machine learning approach shows comparable performance
to the best heuristics for a wide range of applications  in
addition  our approach overcomes the difficulty of choosing
the optimal heuristic without the need for burdensome
profiling 
although this approach avoids profiling every application 
it should be noted that this approach requires considerable
amount of time on learning  such learning process needs to
be conducted on every machine as the hypothesis learned

fion one machine may not apply to other machines  however 
unlike profiling  learning is a one time procedure  which
can be done during os installation or any other convenient
time 

    features and training examples
the features used in a learning algorithm should capture the
possibility of both speedup and violation  table   describes
the features used in this project  as can be seen  methodspecific features provide more information on speedup 
whereas call site aware features extract violation
probabilities  in selecting the method specific features  we
assume that the method execution time has a strong
correlation with the potential speedup  that is  methods that
tend to have a certain amount of execution time are likely to
give good speedup  the exact correlation is up to the
machine learning algorithm to decide  table   also shows
features which detect potential raw  read after write 
conflicts of memory access  to avoid complicated pointer
analysis  we simplify the detection process by declaring a
potential conflict when the types of objects loaded by a
speculative thread overlap with those stored by an older
thread  although this naive analysis may produce false
positives  machine learning algorithms are capable of
overcoming such noise 
features are obtained using a two pass java bytecode
analyzer  during the first pass  the analyzer gathers method
information from the body of each method  this includes all
of the method specific features  with this information  the
call site aware features are computed in the second pass 
although we could have also explored sophisticated
multiple pass features  we streamlined this project due to
time constraints 
in addition to extracting features  we need to label the
training examples  determining the label for given data 
however  is not obvious  even to programmers  since
optimal labeling may vary from system to system  one way
to label the data is to exhaustively parallelize every
combination of call sites and select the labeling with the
best performance  however  the time required for this
approach grows on the order of o  n    of call sites   which is
infeasible in most cases  keeping in mind our time
constraints  we adopt a greedy approach  which enables us
to take advantage of previous research on heuristic
approaches and use them as a baseline example  since the
previous results are close to the optimal case  we assume
that their annotations of speculative regions are similar to
those of the optimal case  from this baseline result  we alter
one annotation at a time for each call site until we find the
one that exhibits the best performance  we can repeat this
step for a fixed number of times  r   or until the speedup is
negligible  it should be noted that finding this near optimal
set of annotations only takes o n r  of execution time 

specjvm  

description

     jess

java expert shell system

     javac

java compiler from the jdk       

     mtrt

a variant of      raytrace

     jack

java parser generator based on the pccts 

splash  

description

barnes

gravitational n body simulation

water

system of water molecules simulation

table    java benchmarks

   simulation methodology
    tools and benchmarks
to simplify the implementation  we reuse the trace driven
simulation infrastructure used in      including the
instrumentation tool  trace analyzer  and trace based tls
simulator  this trace driven tls simulation proceeds as
follows  traces are obtained by executing instrumented
codes on a native amd opteron machine  using the
traces  the trace analyzer produces annotations of
speculative regions according to a manually specified
heuristic  the tls simulator then steps through each trace
and speculatively forks threads at annotated locations 
to implement and evaluate our machine learning approach 
we modify the instrumentation tool to include call site
locations in the trace  since no runtime information is
needed  application traces are not analyzed  as mentioned
in section      we instead implement a java bytecode
analyzer to extract features from java applications  these
features  with labels  comprise the training and test data 
speculative regions are then selected by the learning
algorithm for each method invocation site  in addition  we
use the weka machine learning software suite to easily
compare different algorithms 
the tls simulator only detects violations between memory
accesses to heap data  that is  dependencies between stack
variables are neglected  without violation detection of stack
variables  we cannot speculatively parallelize non void
methods since return values are stored in the stack  due to
time constraints  we did not resolve this issue  instead  we
only considered parallelizing void methods  to simplify the
simulation process even further  we only allow forking
speculative threads at the beginning of a method invocation 
delaying thread forks may reduce cpu idle time to further
utilize computation resources  however  without runtime
information  it is unclear how to obtain the optimal forking
point 
we use the java benchmarks from     to create training and
test data  java applications  like other object oriented

fibenchmark

test error

benchmark

test error

jess

      

javac

      

mtrt

      

jack

      

barnes

      

water

      

training error         
table    training and test error rates
and jack exhibit only    to     speedup  in contrast to
    to     speedup in      since we only consider void
methods for speculative candidates  it is very probable that
such low speedup is due to the lack of inherent parallelism
in void methods of those benchmarks  nevertheless  svm
performs similarly to the best heuristics  suggesting that
machine learning offers a promising approach for tls 
figure    speedup and execution time breakdown
programs  exhibit a large number of method calls  which is
desirable for evaluating our method level speculation
approach  since simulating a benchmark is time consuming 
we use only a subset of applications used in     for the
experiment  table   summarizes the java benchmarks used
in this project 

   results and discussion
for each application  we randomly chose half of the call
sites for training examples and used the other half for test
data  using this data  we trained various machine learning
algorithms  including logistic regression  nave bayes  and
svm  among which the svm algorithm shows the best
average test error  since the separating hyperplane is likely
to be nonlinear  we use kernels to work in high dimensional
feature space  after several experiments on the parameters 
svm with normalized polynomial kernel of degree    and
a complexity constant of     yielded the best test errors on
average 

    speedup comparison
to evaluate the performance of our approach  we compare
the speed up of tls driven by heuristics and machine
learning  due to time constraints  we chose two heuristics
from      which are si rt  runtime heuristic  and si sc
 store heuristic   according to      these heuristics have
shown the best results for barnes     jess  mtrt  and water 
although neither si rt nor si sc are the optimum
heuristics for jack and javac  at least one performed
somewhat comparably to the best speedup 
figure   shows the speedups of the heuristics and machine
learning approaches  as can be seen  the svm speedups are
close to those of the heuristics  in particular  svm actually
performs better than heuristics for javac  the results of
some benchmarks differ from those in      i e  jess  mtrt 
 

for barnes  we use eight processors 

    learning results
table   lists the training and test errors of each application 
as can be seen  the error rates are relatively high compared
to those of other typical machine learning uses  at first
glance  this may seem to conflict with the results in section
    since the performance level should drop at each
misprediction  however  it should be noted that not all call
sites may have the same impact on performance  in
software engineering  it is usually assumed that     of the
execution time of an application is spent in     of the code
     therefore  we can assume that only a few speculative
regions strongly influence the overall performance  while
all other speculative candidates have negligible effect  thus 
as long as these critical regions are correctly predicted 
mispredictions at other call sites do not significantly
degrade the performance  for javac  barnes  and water  we
believe that the learning algorithm successfully classified
the critical regions  resulting in good speedup despite the
high error rates  however  for the other applications  it is
difficult to suggest such claim since the best heuristics for
these applications performed poorly to start with 
although high error rates may not necessarily lead to low
performance  skewing toward randomness is unacceptable 
for example  the average test error rate measured above is
around      if we assume that there is only one critical
region per application  then statistically     of the
applications should show low performance  to ensure
reasonable performance for a wide range of applications 
the error rates should be kept low as possible  in addition 
since the call sites are not equally important  this implies
that we need a new cost function with different weights for
speculative regions  and a different measure to evaluate the
accuracy of the learning algorithm 
we attribute the high error rates to several factors  one
factor is that the current features do not completely capture
all necessary information of parallelization  for example 
the features explained in section     include no information

fifor instance  the runtime system can count the number of
violations for a method and correct the mispredictions if the
count reaches a certain threshold  if dynamic binary
translation is included  we can also target non java
applications without their source codes     

   conclusion

figure    error rates vs  training set size
about the number of processors  failure to consider the
number of processors may result in excessive thread forks 
which was the case for barnes  as mentioned in section     
the speedup from svm approached the speedup of
heuristics only after increasing the number of cpus  figure
  also illustrates the high bias of our algorithm  as the
training set size increases  the training error quickly
converges to the test error  suggesting that more features are
needed  another factor is the imperfect labeling  a greedy
approach was used to label the training and test data  thus
increasing the noise in the data 

   future research
in order to show reasonable speedup for every application 
parallelizing non void methods is inevitable  this approach
requires considerable changes to the bytecode analyzer as
well as the tls simulator  since return values of methods
are usually used immediately  we also need to implement
value prediction for the return values      these changes
may affect machine learning algorithms and or require
additional features 
as mentioned above  we need to resolve the high bias of
our model  thus  experimenting with a larger number of
features is one possible direction for future research  we
can also reduce bias by improving the current features using
sophisticated static analysis  e g  alias analysis 
since correctly classifying the critical speculative region is
more important than the overall error rate  we can also
devise a weighted cost function that favors the critical
regions  however  applying such a cost function introduces
new problems  such as defining and locating a critical
region 
another interesting direction is to integrate our approach
with a runtime system  although runtime information was
excluded from our algorithm  the runtime system itself can
be used to compensate for the machine learning approach 

this study is the first to apply machine learning algorithms
to thread level speculation  in particular  we target method
level speculation by learning the optimal speculative points
from static features  features are extracted using a java
bytecode analyzer  simulation results show that svm
performs almost well as the best heuristics for each
application  the high training and test error rates  however 
need to be significantly reduced  parallelizing non void
methods should also be considered 

   acknowledgements
special thanks to john whaley and ben hertzberg for
providing the simulators and analysis tools  we also thank
connie rylance for reviewing this project report 

   references
    sohi  g  s  et al  multiscalar processors  in proc  of the

intl  symp  on computer architecture  june     
    whaley  j  and kozyrakis  c  heuristics for profile 

driven method level speculative parallelization  in
proc  of the intl  conference on parallel processing 
    
    chen  m  k  and olukotun  k  exploiting method 

level parallelism in single threaded java programs 
in proc  of the intl  symp  on computer architecture 
oct      
    herlihy  m  and moss  j  e  b  transactional memory 

architectural support for lock free data structures 
in proc  of the intl  symp  on computer architecture 
    
    chen  m  k  and olukotun  k  the jrpm system for

dynamically parallelizing java programs  in proc  of
the intl  symp  on computer architecture  june     
    warg  f  and stenstrom  p  improving speculative

thread level parallelism through module run length
prediction  in proc  of the intl  parallel and
distributed processing symp       
    oplinger  j  t  et al  in search of speculative thread 

level parallelism  in proc  of the intl  conference on
supercomputing  june     
    hertzberg  b  and olukotun  k  dbt    dynamic

adaptive thread level speculation  submitted to proc 
of the intl  symp  on computer architecture      
    http   en wikipedia org wiki pareto principle

fi