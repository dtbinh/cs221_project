facebook friend suggestion
eytan daniyalzade and tim lipus
   introduction
facebook is a social networking website with an open platform that enables developers to extract and
utilize user information and relationships  our goal was to create a  friendship model  to be used for the
following purposes 
   recommend a new friend to a user given the users existing friends 
   given a users friends  determine which of those friends would be users best friends 
to make these predictions  we gathered user profile information such friends  groups  interests 
music  and activities  we trained our models based on these features using techniques from both
supervised and unsupervised learning  for the first goal  we implemented logistic regression and nave
bayes learning algorithms to predict which users are most likely to be friends  for the second task  we
used pca and k means clustering to compare a given users friends to each other 
it is highly likely that the friendship structure of a certain person would be very different than that of
another person  furthermore  there is a large variation among the profile information of different people
 e g  we will not necessarily have the same features for each person  and the relative importance of
features may be different for different users   therefore  we decided to train our hypothesis separately for
every person whose friends we would try to understand  we use the term central user to denote the user
for whom we are training the algorithm  instead of defining features for individual users  we defined the
features for each sample as a function of a central user and the sample we are comparing against  for
example  a feature would not be the number of friends of a user  but rather the number of friends a user
has in common with the central user 
   data acquisition
we used the facebook api to collect user data  data collection steps were as follows  choose central
users  find their friends  positive samples   choose a set of non friends  negative samples   and gather
profile information for every positive and negative sample with respect to the central user 
limitations of the api and privacy settings created some obstacles in data collection  first  some
profiles allow only limited access  which in some cases makes it impossible to see any data other than the
name of the user  for this reason  we limited ourselves to profiles within the stanford network  since most
non stanford profiles allow us very little access  second  it is not possible to directly query for all of a
given users friends  one can only query whether two particular users are friends  therefore  we needed
to start with a large set of user ids  and it was not possible to find all the friends of a user unless our
starting set contained all of them  the third problem was the lack of an easy way to get a list of users
according to arbitrary criteria  such as stanford students   we had to collect data in a less direct way  we
started with a stanford student group  found all the stanford students in that group  then found other
groups to which those users belong  and repeated until we had around      samples  it is possible that
this method could have introduced some bias into our data set that could have been avoided had it been
possible to select      random stanford users 
because a given users friends make up a small percentage of our network  we chose to include in our
training and test sets all of the central users friends  that we found in our set of users  and up to     nonfriends  we made this choice to include as much information about friends as possible  but we also had to
keep in mind the implications of having a training set with a different distribution than the overall data 
which we discuss in more detail below 
   model evaluation
one of the major challenges of this project was determining how to evaluate our models  one reason
this is difficult is that although the models we used for friend suggestion are typically used for
classification  our application is not quite classification  rather than outputting y    friend  or y    non 

fifriend  for each candidate user  we instead want to select the most likely candidates out of a given pool 
therefore  measuring test error is not as simple as finding the percentage of misclassifications on a test
set  one property that an error metric should have is a higher emphasis on precision than recall  we do not
need to return all likely candidates  but we want the ones we return to be good 
a second problem with the friend suggestion task is that what we are testing is not quite the same as
the goal we are trying to accomplish  our goal is to create an application that suggests new friends that
are not already the central users friends  but we are testing the ability of the model to predict whether a
given user is already friends with the central user  accurately testing the first goal would require a large
set of hand labeled data  we decided it would be better to use the large amount of already existing data as
a proxy  for the second task of determining a central users closest friends  we do not have any labeled
training data at all  so it is difficult to make a precise evaluation of our models performance 
   friend recommendation using logistic regression
we trained a linear logistic classifier to classify non labeled samples as friends or non friends 
initially  we used a training set of    non friends and approximately    friends for each user  for finding
the optimal classifier  we preferred newtons method due to its rapid convergence and the non singularity
of our feature set  features used were the ones mentioned above  i e  number of common friends  groups 
activities etc  we used k fold cross validation  with k     to find our test error  accuracy rates on our
first run were highly satisfactory given the acceptable level of accuracy for our application  classification
error was on average between     and      the test error rate increased as we increased the number of
features included  despite the fact that each feature was individually a decent friendship indicator  figure
  shows how testing error varied with increasing number of training samples  and different graphs shows
error rates for different number of features included 

  

  
  
  

training error
test error

training error
test error

  

  

percent error

training error
test error

  

  
  

  
  
number of training samples

  

  
  

  
  
  

  

  
  
  

  

  

percent error

percent error

  

logistic regression with   features

logistic regression with   features

logistic regression with   features
  

  
  
number of training samples

  

 
  

  
  
number of training samples

  

figure   data reduced to   dimensions for
visualization

as seen in the plots  training error was significantly less than test error with higher number of
features  further  test error declined with increasing number of training samples  we interpreted these
results as indicators of high variance and acquired more data to increase our training set to     nonfriends and around    friends  expanding our training set significantly improved our results  lowering
error rates to          and     for      and   features respectively 
    weighted logistic regression
basic logistic regressions main drawback is that it punishes false negatives the same as false
positives  however  since our application would only be reporting the samples that it would classify as
positive  we should lower error on positive guesses as much as possible  this could be achieved by
weighing negative samples more than positive samples while training  this approach would put more
emphasis on maximizing the likelihood of negative samples and shift the separating line closer to positive
samples  making our hypothesis less likely to guess positive samples  as seen in figure    as the weight
assigned to negative samples increased  the percentage error on positively guessed samples  i e 
mislabeling of negative data  and the number of positive guesses decreased 

fi 
 
 
  
  
negative sample weight
figure  

number of positive guesses
  
  
  
 
 
 
  
  
negative sample weight

total error
  
 error

 error

  

number of guesses

error on negative samples
  

n   friends
n      groups
n      events

  

  
 
 
  
  
negative sample weight

    conclusions on logistic regression
a major challenge is figuring out the optimal weight on the negative samples  we deferred tackling
this issue until we gathered sufficient user feedback to decide whether the number of recommendations or
their accuracy is a bigger priority 
   nave bayes
we also trained a nb model based on features that each person might have in common with the
central user  we trained both a bernoulli and a multinomial model  in the bernoulli model  for a given
feature of a user  e g   groups   we let p xi     y  be the probability that the users ith group was also
shared by the central user  however  we considered the possibility that some groups might be more
indicative than others  therefore  we also created a lexicon of the groups to which the central user
belongs and trained a multinomial model in which we let p xi   j y  be the probability that the users ith
group was the same as the central users jth group  if j      then it is the probability that the users ith group
is not shared by the central user   note that the multinomial model distinguishes between each group to
which the central user belongs  but groups to which he does not belong are treated symmetrically  
    modeling the prior
one difficulty in applying nb is modeling the prior p y     we would normally set this parameter to
be the percentage of the training examples which were positive  but as discussed above  our training set is
not representative of the actual distribution  furthermore  even if we base our prior on all of the data  not
just the data in the training set   it may not match the prior for the question we are actually trying to
answer  the probability that given user is friends with the central user may be different than the
probability that a given non friend is someone with whom the central user might like to be friends 
however  the nature of our goal allows us to solve this problem  as mentioned above  instead of
performing classification  we instead want to select the most likely candidates  therefore  we rank the
users by the likelihood ratio p x y    p x y     which is equivalent to ranking by p y   x   this way  we
can avoid modeling the prior 
    measuring accuracy
unfortunately  as mentioned above  this application of nave bayes makes measuring the test error
difficult  a simple method would be to let f be the number of friends in the test set  rank the test set by
the likelihood ratio  and let the error be equal to the percentage of users in the top f scores that were
actually non friends  this metric  which we call metric a  has the advantage of focusing on false
positives  however  in practice  we would probably only report the top few candidates  so we care the
most about the ones at the top  therefore  we also used metric b  which awards a higher weight to the test
samples that the model ranks higher  specifically  wi   f  i  with weights renormalized to sum to    
we also let metric c be the same as a  but with f   in place of f 
the following plots show the test results for our model  where each column shows the test samples
for a single central user  friends are shown in blue  non friends in red  we found the best model  figure
  b  to be the one that treats common friends by a multinomial model and other features as a bernoulli
model  for comparison  the model with all features treated as bernoulli is shown in figure   a  the error
according to each of the three metrics is given as well 

fifigure   a

a         b         b        

figure   b a         b         c        

   unsupervised and semi supervised learning for best friend suggestion
the goal here is to cluster a users friends based on their similarity  and report the members of the
group closest to the user as the best friends 
     reduction to a single dimension using pca
an obvious challenge in unsupervised learning is defining a metric that determines the cluster to
report  we tackled this issue by assuming that features were positively correlated with friendship level 
to test this assumption  we created a feature set consisting of our friends and reduced the data to a single
dimension by applying pca  this single dimension  the principal eigenvector  indicates the direction of
highest variation  hence  we postulated that the projection of the better friends would lie further away
from the origin  indicating a higher number of common features  using ourselves as the central user  this
metric correctly identified the people we would consider as good friends  although we could not prove
results rigorously  this experiment indicated that projecting on the principal component could indicate
level of friendship 
     k means clustering
we clustered our data into four clusters  the number of clusters was arbitrarily decided  using the  dimensional feature set and reported a cluster based on our metric of closeness to the central user  figure
  a shows the results  it is noteworthy that we applied clustering in higher dimensional space  rather than
reducing data to a lower dimensional space and then reporting points based on their distance from the
origin  the two methods would cluster points in similar but not identical ways  whereas the former
method clusters points based on their similarity with each other  the latter clusters them based on their
similarity with the central user  figure   b shows the results of the latter method  as further discussed
below  it is hard to know which method yields better results without actually getting feedback from users 
     constrained k means clustering
the clustering methods covered so far did not accommodate user feedback  so we implemented a
constrained k means clustering algorithm that would incorporate user feedback as labeling on data to be
adhered to while clustering  user feedback would be in the form of good friend or not good friend 
our algorithm would ensure that sample points with same label would be assigned to the same cluster 
and no cluster would contain points with different labels  in terms of implementation  constrained kmeans differs from regular k means in the assignment of the labeled samples to a centroid  while regular
k means assigns each labeled sample to a centroid with the objective of minimizing that specific
samples euclidian distance from the centroid  constrained k means finds the centroid yielding the
smallest value for the sum of distances of all the samples with a specific label and assigns them to that
centroid  to test this algorithm  we labeled the top    outputs of unconstrained k means and ran
constrained k means on the new semi labeled data set  the results are shown in figure   c 
    conclusions on unsupervised and semi supervised learning
testing was a major challenge on our best friend suggestion algorithms  given the nature of the
problem  we could only test the algorithm on our own friends  and we did not have a coherent metric for
gauging the accuracy of results  however  results were encouraging      of the    people reported by the

fiunconstrained k means algorithm were people that we would classify as good friends  furthermore  a
significant drawback of the semi supervised learning algorithm we used was that it only adjusted clusters 
not the definition of good friends  based on the user feedback  an algorithm that would adjust the
weights of different features  such as distance metric learning      could be more appropriate for the
question at hand 
k means in lower dimensions

k means in higher dimensions

   

   

 th degree  r 
 rd degree  g 
 nd degree  y 
 st degree b 

  
  
  
  
 
   
 

  
  
  
 st eigen vector dimension

  

figure   a reduced to   dimensions for
visualization

 nd eigen vector dimension

 nd eigen vector dimension

   

 th degree  r 
 rd degree  g 
 nd degree  y 
 st degree b 

   
  
  
  
  
 
   
 

  
  
  
 st eigen vector dimension

  

figure   b

constrained k means with    labeled samples

 nd eigen vector dimension

   
   

 th degree  r 
 rd degree  g 
 nd degree  y 
 st degree b 

  
  
  
  
 
   
 

  
  
  
 st eigen vector dimension

  

figure   c reduced to   dimensions for
visualization

   conclusion
in all of our algorithms  we found the best feature to be the percentage of friends that users had in
common  followed by the percentage of common groups and events  one reason for this result is the
sparseness of the data for many of the features  for example  many people have few events listed  so it is
common to see users who have no events in common with the central user  for other features  such as
activities and music  the problem is even worse because the entries for these categories are user generated
 i e  prone to spelling errors or writing the same thing in different ways   which makes it even less likely
to see commonality among users  the sparseness of these features makes them harder to use than features
with denser data  such as number of common friends 
however  we believe the errors we found for our friend suggestion models show promise  depending
on the metric  test errors are roughly in the        range  we believe that friend suggestion could be
useful even if only a much smaller fraction  say  one out of five  of our results were relevant  however 
we also realize that our error measures are by no means perfect  as discussed above  since we were
testing the ability to predict current friends rather than suggesting new ones  we would ultimately need to
test with actual users  and this is even more the case with our best friend predictor  therefore  the next
step in our work would be to incorporate this model into a facebook application and gather user feedback 
   references
    eric p  xing  andrew y  ng  michael i  jordan and stuart russel  distance metric learning  with
application to clustering with side information  university of california  berkeley 

fi