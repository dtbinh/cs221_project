customer review feature extraction
heng ren  jingye wang  and tony wu

abstract
popular products often have thousands of
reviews that contain far too much
information for customers to digest  our
goal for the project is to implement a
system that extracts opinions from these
reviews and summarizes them in a concise
form  this allows customers to quickly get
an overview of a product and manufactures
to efficiently process product feedbacks 
in the past  we focused on the feature
extraction directly on the word level  first 
we would use association rule to extract all
the  noun    adj   rules and then use
pointwise mutual information to judge the
polarization of the adjective  processing
directly on word level neglects the
information contained on the sentence
level  in this paper  we discuss how to
extract features by first investigate on
sentence level and then dive into the word
level 

to judge whether a sentence has any
features  we used several machine learning
methods  we will compare the
performance of each method 

algorithms and implementation
nave bayesian
nave bayesian technique is a powerful
method for classification  in our problem 
the two classes are review sentences with
product feature  c   and review sentences
without product feature  c   
  ci   c   
pr c   
training size
pr c        pr c  
the features we used for training are
individual words within each sentence  we
denote each sentence as si and each word
as wj  the probability of the word si
appearing in a sentence of class ck is given
by 
si

p wj ck  
wj

introduction
extracting features directly on word level
neglects the information contained in the
sentence level  for example  there might be
a sentence such as today is a good day
appearing in the reviews  if we only
perform the algorithm on the word level 
we would consider  day  good  as a feature
of the product  but obviously it is not 

si

  ci   ck   wj in si  

since some word might never appear in
sentences of a particular class  we do not
want to have a sentence with   probability
of being in a particular class  we deploy a
slightly modified version of laplace
smoothing where        instead of   
p wj ck
si

 
wj

here we propose a method that first
determines which sentences in a review
might contain product features  if we are
confident that a sentence does have feature 
we would further process the sentence
using the association rule and pmi to
extract the features 

  ci   ck   wj in si  

si

  ci   ck    wj in si        

  ci   ck    wj in si           w 

finally  we have the following nave
bayesian classifier to determine the
probability of a sentence being in a
particular class  using the words in that
sentence as features 
p c  si  

p c   
ck

p ck  

p wj in si  c   
p wj in si  ck  

fip c  si       p c  si 
to avoid underflow  we can alternatively
use log for calculation 
log p c  si  log p c  si  
  logp c   logp c   
 

logp wj in si  c   



logp wj in si  c   

if the value obtained above is greater than
   we claim that the sentence is more likely
to have product feature s  than not 
the words that appear more often in
featured sentences are indicative of a
sentence likely to have features  the
following list contains the top feature
indicator words from our studies 
fits  convenient  strong  clips  packed 
variable  emails  exceptional  freezes 
glitches
conversely  the following list of words
indicates that the sentence is likely to not
have any product features mentioned 
ordered  olympus  sell  consumers  ran 
shopping  compete  register  replacing 
assume
overall  we achieve an accuracy of     by
training on      labeled sentences and
testing on     sentences 

spy em
sometimes  labeled data may be hard to
come by  this is where spies become useful 
given a small set of positively labeled data
and a large set of unlabeled data  we
designate a subset of the positive data as
spies and unleash them into the unlabeled
set  then we use the same nave bayesian
algorithm that we just discussed to train a

fake classifier pretending that the entire
unlabeled set is negatively labeled  the
spies will enable us to develop a threshold
for being negative  any sentence that is
more unlikely to be positive than all the
spies are considered to be reliably negative
 rn  
spy step in s em 
   rn   null 
   s   sample p       
   us   u  s 
   ps   p   s 
   assign each sentence in ps class label   
   assign each sentence in us class label    
   nb us  ps  
   classify each sentence in us using the nb
classifier 
   determine a probability threshold th
using s 
    for each sentence si  us
    if its probability pr   si    th then
    rn   rn   si  
after obtaining the rn set  we will run an
em algorithm  which is essentially an
iterative version of the nave bayesian
algorithm  the only difference here is that
instead of having class labels be         we
use a continuous range        for the
probability of each sentence being in a
particular class  the indicator functions in
nave bayesian are now probabilities
instead 
em step in s em 
   assign each sentence in p the class label   
   assign each sentence in rn class label    
   each sentence in q    u   rn  is assigned a
probabilistic label  pr   si  
e step  revise p class sentence 
m step  calculate the new p class  and
p word class 
   run the em algorithm until it converges 

fithe following plot shows test set accuracy
as a function of em iterations  the em
algorithm seems to converge after
approximately     steps 

rocchio method could be described as
follows 
 


     

 


   

   let         
   let        

   

 


    

 

 


    

 

    

   if              
classify  as negative
else
classify  as positive

overall  s em achieves an accuracy of     
this is lower than nave bayesian because
we are pretending that the negatively
labeled set is unlabeled  in a more realistic
situation where we do have a large amount
of unlabeled data  only using nave
bayesian will not enable us to solve the
problem 

rocchio method
rocchio method trains the classifier by the
following way  first  each document  is
represented as a vector
                      
each element  represents a word  in
the document and is calculated as
     
where  is the term frequency  the
number of times that word  occurs in  
and  is the inverse document
frequency 
  
   log 
 
   
here    is the total number of
documents and     is the number of
documents where word  occurs at least
once 

here  according to  buckley et al          we
used       and       moreover  we
used the inner product to measure the
similarity of two  vectors  for each test
document  if it is more similar to positive
    we assign it as positive 

svm based on rocchio
since in rocchio  we only used the inner
product to measure the similarity of two
passages  the accuracy rate was not so
good  hence now we switch to using svm 
based on the rocchio formulation  let the
training sample set be
                              where  is
the representation of a passage in
rocchios format  and  is the label for
the passages  to deal with noisy labels  we
used the soft margin svm  which is
 
min     
 




                           
here  is the parameter that controls the
amount of training errors allowed 
we employed different types of kernels in
svm method  linear kernel  polynomial
kernel and radial basis function kernel 
based on our training set  we compared the
performance of each type of kernel as
follows 

fiprecision
     
     
     

recall
     
     
   

f
     
     
     

here we use precision  recall  and f score
as the measure of performance  f score
takes into account of both recall and
precision  f    pr  p r   the numbers above
are all based on the best performance for
each method after tuning the parameters 

biased svm
up until now we assume that all samples
are correctly tagged  however  tagging the
data is tough and tedious and if we cannot
get enough tagged data  the training
algorithm would face potential over fitting
problems  to solve this problem  bing et al
proposed the lpu algorithm  it uses both
the tagged data and untagged data  first  it
assumes that all untagged data are negative 
iteratively  the algorithm would train a svm
to classify the positive and negative
samples until the ratio of positive and
negative samples converges 
the problem is that  since svm is sensitive
to noises  if in any iteration the trained
svm is largely affected by noises  later ones
would be worse 
bing et  al  proposed biased  svm in  bing
liu  et al        biased svm formulation
uses two parameters   and 
separately to weight positive errors and
negative errors  thus the problem changes
to
 
min      
 




                           

we can vary   and  separately 
intuitively  we give a large value for  
and a small value for  because the
untagged set  which is assumed to be
negative  also contains positive data 
we used a separate validation set to verify
the performance of the resulting classifier
with the selected values for c  and c  
we used f score as the performance
measure 
kernel
linear
polynom
gamma

precision
     
     
     

recall
     
     
     

f
     
     
     

evaluation
now that we have presented the five
algorithms we used to determine if a
review sentence contains a product feature 
we would like to compare their
performances 

performance comparison
accuracy

kernel
linear
polynom
gamma

 
   
   
   
   
 

algorithm

it seems that for this particular task  svm
with no untagged samples outperforms all
other algorithms by achieving an accuracy
rate of over      when using spy samples 
the performance of svm decreases largely 
however  since we also used iterative
algorithm in svm  we can see that the
svms performance is still better than s em 

ficombination
to close the loop  we would like to present
some results outside the scope of the
project focus  just for completeness  after
obtaining product features  we would like
to know if customers liked the particular
feature or not  this can be done easily with
an algorithm called pmi 
point wise mutual information  pmi 
algorithm uses mutual information as a
measure of the strength of semantic
association between two words  the
point wise mutual information between
two words  w  and w   is defined as
follows 
p w    w   
pmi w    w    log 
 
p w   p w   
where p w  w   is the probability that w 
and w  both occur in the same phrase 
semantic orientation of a phrase is
calculated as follows 
so phrase    pmi phrase  excellent  
 pmi phrase  poor  
we can use semantic orientation to
determine the polarity of a particular word 
if the word appear more often with
excellent  it is more likely to have a positive
connotation  the following plot shows
some examples 

 

so

   

opinion polarity

 

   

    

opinions

funny

good

difficult

great

stupid

entertaining

  

terrible

    

bad

 

finally  once we have obtained the product
features as well as their relative polarity
from the customer reviews  we can sort
them and present them in a summary form
as demonstrated below 
product file name   d link dwl g   
there are    review s  total 
pros   setup      
connection      
price      
card      
connections      
cons  performance       
port       
sensitivity       

conclusion
in this paper  we showed a two step
method for extracting features from
customer reviews  as the first step  we
would judge whether each sentence has a
feature or not  if it does  we would further
process the sentence and extract the
features  the first step is mainly based on
machine learning methods and the second
step is mainly based on data mining and
natural language processing methods  we
have demonstrated in this project that it is
tractable to determine customer opinions
on individual product features  when all
training data are correctly tagged  we
showed that svm is the best and when not
all training data is tagged  s em and b svm
almost have the same performance 

fi