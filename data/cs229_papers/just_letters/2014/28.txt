classification of higgs jets as decay products of a randall sundrum
graviton at the atlas experiment
aviv cukierman  zihao jiang

   introduction
in       physicists at the large hadron collider announced the discovery of the higgs boson  which earned the moniker
god particle in the popular media due to its far reaching theoretical implications  the standard model  which is by
far the most successful model we have describing matter and its interactions  predicts that the higgs boson exists and
gives mass to quarks  the higgs boson was the final particle predicted by the standard model that had not yet been
discovered and so the discovery was a major triumph for the standard model  however  there are problems with the
standard model  for instance  there is a question of why the mass of the higgs itself is so small  when naive theoretical
extensions of the standard model predict it should be much higher 
among all the more sophisticated models which account for this problem  one model that yields detectable quantities
is the randall sundrum model  which predicts there is a new particle  a graviton  which can decay to two higgs bosons  
a search for such a particle requires distinguishing the higgs bosons from background processes  mostly quark and gluon
showering from quantum chromodynamic  qcd  effects  in this project  we use machine learning techniques to distinguish
the signal of a randall sundrum graviton  rsg  decay to two higgs from the qcd background  using kinematic variables
that would be observed in the atlas detector at cern  in particular  we look at the case where each of the higgs
bosons decay to a pair of bottom antibottom  bb  quarks 
   data   features
the study uses monte carlo simulated data with full reconstruction of atlas detector response  for the signal  we
assume the mass of the rsm graviton to be      gev c    consisting of       events  for the background  we use atlas
tuned qcd simulations consisting of       unweighted events 
in a signal event we have two higgs bosons  each of which decay to a bb pair of quarks  these quarks are are observed as
collimated energy deposits in the detector  which are called jets  we know from physics that in fact the quark antiquark
pair will be detected with small angular separation in the detector  and so the quark jets form the sub structure of a
larger higgs jet  from these energy deposits we can reconstruct the space time four momenta of the two quarks and the
higgs bosons             features   in addition  we have b tagging variables constructed from other observables that
describe the probability that the quarks are bottoms     features   so in total we have           features for each
potential higgs jet 
   models
     naive bayes 
the naive bayes model
qn holds the assumption that the probability of a set of features  fn   conditioned on a variable x
is p f         fn  x    i p fi  x  in our case  x        denoting whether the a jet is or is not a higgs boson  the features
are the continuous kinematic variables of the jets and the sub jets  we model the conditional probabilities as a gaussian
 
i 
p fi  x         exp   x
  where the parameters  and  are calculated directly from the mean and rms of the
   
 i

i

kinematic variables 
heuristically  the features that are most relevant to this classification problem are the invariant mass and transverse
momenta  pt  of the jets  higgs jets have invariant mass around     gev  while the qcd jets have smaller mass because
the showering of gluon and quarks usually produces light mesons  the mass and energy of sub jets  decay products  are
predictive  the higgs tend to decay to heavy flavor quarks like bottom and charm  while the sub jets in qcd come from
radiation of light particles 
for the naive bayes algorithm  we consider the parameters in the following order  mass of the jet  pt of the jet  mass
of the two sub jets  pt of the two sub jets  and b tagging score of the two sub jets  we add these variables to our feature
set one by one  and each time we re run the learning algorithm and testing for     iterations  for each iteration  we use
    of the signal and background to train the gaussian parameters and test the prediction on the remaining     of the
data set  we take the training error and testing error as the mean of these     iterations and rms as the uncertainties 
the results of this algorithm can be seen in figure    the tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set  for the signal  the errors are getting significantly smaller as we consider
 

fi 

aviv cukierman  zihao jiang

more features  however including the sub jets information doesnt improve the performance of tagging background  we
also note that that after including the first two features  mass and pt of the higgs jet   almost all of the discriminative
power is achieved 

figure    naive bayes driven higgs tagger  with   features  the signal error was    and the background error was    
     logistic regression 
in logistic regression we have a hypothesis of the form
h  x    g t x   

 
    exp t x 

where x is a  d      dimensional vector with x      and x      d the features  and   rd   are the parameters of the
model  the prediction for y         is   h  x         
we split the data into     training set and     testing set  we optimize the parameters of the regression via batch
gradient ascent on the log likelihood function on the training set  i e  we run the following algorithm 
repeat for n iterations 
pm
 i 
j      j    i    y  i   h  x i    xj
where in the algorithm we set       and x           x m  is the training set 
we then calculate the error in the testing set  and training set  by calculating
pmtest train
  y  i        h  x i           
test train
signal n   i  
test train
msignal
pmtest train
  y  i        h  x i           
test train
background n   i  
test train
mbackground
test train

n

test train

  signal n

test train

  background n

for each n  
sometimes train
  train
for m   n as the algorithm moves out of local minima  so we look at
m
n
m    argmini       n    train
 
i
since in practice we can choose whatever j minimizes the training error over the course of the algorithm  which isnt
necessarily n  
running the algorithm with all the features  we get convergence of train
m   min at around n        iterations  which
can be seen in fig     in order to compare across algorithms  we are concerned with the minimum total error when
signal        so we define similarly

m  
  argminis  train
 
i

s    i    train
signal i               
the results after n        iterations are shown in table   
train
train
train
m
signal m
background m
m   m
      
      
      

m   m  
      
      
      

test
signal m
      
      

test
background m
      
      

test
m
      
      

ficlassification of higgs jets as decay products of a randall sundrum graviton at the atlas experiment  

train test

figure    n min
as a function of n using all the features  by about n         the algorithm has
converged to within a few percent level of the n   limit 
table    results of logistic regression algorithm 
     k clustering  we examine a significantly modified k means clustering algorithm designed by us  first  we run an
ordinary unsupervised clustering algorithm on the training data 
   choose k centroids          k randomly from the data
   for every i  set c i    argminj   x i   j    
pm
   for every j  set j   i     c i    j   x i 
   repeat the last two steps until convergence
pm
   repeat steps     a few times to globally minimize the cost function j   i     x i   c i       
then we supervise the unsupervised algorithm  for each cluster j we look at what fraction of the data classified in that
cluster is signal 
pm
  y  i        c i    j 
fj   i   pm
i     c i    j 
we then set a signal threshold s and background threshold b to label signal  fj   s  and background  fj   b  centroids 
respectively  then using only the centroids that passed the signal or background threshold cuts  i e  the number of clusters
used in the testing phase ktest  k   we cluster and label the testing data to get signal and background identification
efficiencies 
test train

test train
esignal
 s 

 

msignal

 

x

test train

msignal

y  i    fc i    s 

i  
test train

test train

ebackground  b   

 

mbackground

x

test train

mbackground

    y  i     fc i    b 

i  

and the error       e 
by varying s and b  we get a roc curve  which is shown in figure   
     multivariate analysis  there are correlations between different kinematics variables in the dataset we consider 
for instance  both the higgs jets and qcd background jets will split to two sub jets of the same amount of energy 
therefore  the pt of the sub jets are roughly the same  to account for such correlations we implement more advanced
multivariate analysis tools bdt  boosted decision tree  and k nn  kth nearest neighborhood  which are widely used in
atlas collaboration 
       boosted decision tree  the boosted decision tree  bdt  is an ensemble of weak classifiers called decision trees 
the algorithm involves two steps      building a forest of decision trees  and     adaptive boosting  a decision tree is
a multi layer binary classifier  the training starts with the root node and splits to two subsets  each of which searches
for the best variable and its corresponding cut based on calculation of the gini index  defined as p    p   where p is
the purity of the sample  if in a sample  signal and background are mixed equally then p       and the gini index is
maximized  while it falls to zero if the sample is entirely signal or background  in this search step  we partition each
variable into    equal segments and decide the cut based on the maximum change between the parent node gini index

fi 

aviv cukierman  zihao jiang

figure    the roc curves for the the modified k clustering algorithm  the model improved with
increasing numbers of clusters  but low data statistics prevented making more clusters 
and the sum of the daughter nodes gini indices  the tree keeps splitting until the resulting split subsets have fewer than
    data points  once we obtain     such weak classifiers hi   we pass them to the boosting step 
the goal of the boosting is to construct a strong classifier as a linear combination of the     weak classifiers we
pt     
obtained  f  x    t   t ht  x   for b          wep
come up with a prediction model hb  x      f  x    b   given m
 
training samples  the s are chosen such that b   m
i   hb  xi      yi   is minimized  the minimization scheme we use
is adaptive boosting  which estimates and minimizes an upper bound of   by varying b  we get a roc curve  which can
be seen in figure   
the bdt algorithm is also useful in determining which variables are most useful for separating signal and background 
we calculated the discriminating power of each feature as the number of splits occurred for that variable weighted by the
gain squared and number of events in it  the gain is calculated as the change in information entropy h from a previous
stage to a new stage with more information  as seen in table    we find that the higgs jet transverse momentum and
mass have the most discriminating power 
variable
importance variable
importance
jet pt
    
leading sub jet b tagging score
    
jet mass
    
sub leading sub jet b tagging score     
sub leading sub jet mass     
leading sub jet mass
    
sub leading sub jet pt
    
leading sub jet pt
    
table    importance calculated by bdt algorithm  most indicative variables are the jet pt and jet mass 
       k nn  the philosophy of k nn is neighborhood majority vote  the label of a point is decided by the labels of the
majority of its neighborhood points  to formalize the problem  we need a metric to decide for two points to be close
or neighbors  for this problem  we use the conventional euclidean metric  given a test point p and a set of features
p
   
n  
 f    f         fn    we calculate the metric r  
 a

b
 
  where ai and bij are the ith feature value for p and j th
 
i
ij
i w
i

training point  then we select the closest k training points  in our case  we use k       and assign a probability of p
being signal ps   kks   where ks is the number of signal points of the k nearest points  the weight factor wi in the metric
is the width of feature i for all the data  by varying the cut on the signal probability  we come up with a roc curve 
which can be seen in figure   
   results
the results are detailed in the models section  but we wish to compare the models used  table   shows the minimum
test background error for each model with test signal error     
model
background error
naive bayes
    
logistic regression
      
modified k clustering
      
bdt
      
k nn
      
table    results of the models used here for test signal error     

ficlassification of higgs jets as decay products of a randall sundrum graviton at the atlas experiment  

figure    roc curves for bdt and k nn algorithms  the bdt algorithm outperforms k nn at all
signal efficiencies 
as we can see  the bdt algorithm is the best performing 
   discussion
we examine both linear and non linear classification algorithms  in general we find the non linear classifiers to perform
better than the linear ones  the best performing linear classifier was naive bayes  and the best performing classifier
overall was bdt  we note that the atlas collaboration has generally agreed on using bdt to classify signal versus
background in a wide variety of problems  and our results support that decision 
we also designed a modified k clustering algorithm  our new algorithm was the worst performing of all the ones we
examined  however it was interesting to see that an algorithm with unsupervised components could still do a decent job
of classification 
we also analyzed and measured the importance of each feature used in our algorithms  both our naive bayes algorithm
and bdt find that the two most important features are the higgs jet mass and transverse momentum 
   future work
most of the algorithms discussed here have seen years of research and development  the k clustering algorithm
developed here  on the other hand  shows some promise  even though it was the worst performing of all the algorithms 
with more time it would be interesting to look at this algorithm from a theoretical standpoint to understand its limitations
and under what model it performs best  if we were to work on this project for a few more months  we would also probably
look at other algorithms like svm 
   conclusion
we find that machine learning algorithms can be very powerful in distinguishing higgs signal from background in
simulated data from atlas  its particularly interesting that the algorithms themselves had no knowledge of the physics 
only of the data and training classifications  the prospect of discovering or ruling out a randall sundrum graviton via
decay to two higgs bosons at atlas seems feasible given the power of the classification algorithms discovered here 
from a wider perspective  machine learning algorithms can be used in many physics analyses other than the specific one
studied here  such as searches for supersymmetric particles  dark matter  or exotic particles like axions  we believe that
the future of physics done at experiments like atlas inevitably lies with the use of machine learning techniques like the
ones presented here 
   references
    atlas collaboration         observation of a new particle in the search for the standard model higgs boson with
the atlas detector at the lhc  phys  lett  b         
    m  cacciari  g  salam  g  soyez         the anti kt jet clustering algorithm  jhep         
    a  hoecker et al          tmva   toolkit for multivariate data analysis  pos acat    
    a  ng  cs    lecture notes  retrieved from cs     autumn      course website  http   cs    stanford edu materials 
    l  randall  r  sundrum         large mass hierarchy from a small extra dimension  physical review letters   
    
    t  sjostrand  s  mrenna and p  skands         a brief introduction to pythia      arxiv          

fi