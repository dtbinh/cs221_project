prediction of bike rentals
tanner gilligan  tanner   stanford edu
jean kono  jkono stanford edu

abstract
we constructed a custom linear regression
model to try and impute missing data from a
time series of bike sharing rentals  the
dataset contained some time features  e g 
months       which we translated to
indicator variables  as well as weather
information for that day  we customized the
linear regression model by experimenting
with different loss functions and attempting
to exploit structure in the data  we found
that the data was very noisy  and therefore
it was difficult to make accurate predictions 
but our best results came when we split the
data by month and created a linear
regression model for each month  this
method correctly predicted within    of the
true value on       of the test set 

   introduction
     objective
our objective is to try to accurately impute
missing data from time series bike rental
records  although predicting bike rentals has
very few real world applications  the idea of
trying to impute missing information in a
time series is quite valuable  imputation of
time series information is useful any time
data can become corrupted  or a sensor can
malfunction  leaving holes in the dataset 

     dataset
our dataset contains hourly shared bike
rental information for washington d c  from
     and       we obtained our dataset
from the uc irvine machine learning
repository  but it is also available as one of
the competitions on kaggle  our dataset was
composed of        different data points 
each of which contained    different input
features and   response features 
     features
the dataset was very complete including
features about time and weather  we also
noted the existence of implicit neural
networks  such as feeling temperature
being a function of the temperature 
humidity  and wind speed features  in
addition  many of the feature were already
normalized  divided by the largest value  
allowing the features domain to be fully
contained in         the dataset contained
the following features 
time
  date  mm dd yyyy 
  hour       
  day of the week      
  month       
  year       
time meta information 
  government holiday       
  working day       
  season      
weather 

fi 

precipitation      
temperature  normalized       
humidity  normalized       
wind speed  normalized      
feeling temperature  e g  windchill 

     evaluation metrics
we divided our data into a training set and a
test set  approximately     and     of the
data were used in each set accordingly  the
training set was randomly sampled without
replacement from our full data set and the
test set was the remainder  this was done to
simulate random data removal  all models
were fit on the training set  we used k fold
validation on the training set to generate
validation errors to compare the models and
measured test error by the performance on
the test set 
we chose two different evaluation metrics
for analyzing how well the models were
functioning  our primary evaluation metric is
the mean squared error  mse  of the data
points  which gives us an idea of how well
the regression model is fitting the data  our
secondary evaluation metric is prediction
proximity  ve   which tries to capture how
many of the prediction values were close
to the actual response value  we chose to
have this secondary evaluation metric
because our objective is to try and impute
the missing data  and this provides us with a
way to measure how good the predictions
are  as opposed to just how well we fit the
model  this idea of close is a rather
subjective term  so for our project we ran
our initial baseline once and checked the
proximity of the predictions against various
proximity values  and learned the following 

proximity ve
mse
test error
  
    
     
    
  
    
     
    
  
    
     
    
   
    
     
    
we selected    as the value we would use
for our project since had plenty of room for
improvement  and was a reasonably wide
range  this choice of    was still a relatively
arbitrary choice  however  and one could
select any proximity value they wanted 

   experimentation
     feature generation
the first thing we decided to do was
translate the given time variables  such as
month and day  into indicator variables  this
is because a feature like month should not
assign    times more weight to december
     than to january      they are very similar
and should be treated as such  in addition 
we found that some of the features were
non monotonic  that is  their effect on the
response did not increase as the features
value increased  e g  temperature   in order
to rectify this  we replaced the temperature
and feeling temperature features with a
normalized feature representing the
distance from room temperature    f  
model
ve
mse
te
baseline
               
new vars          
    
we also experimented with clustering the
data and adding the cluster assignment
from k means clustering as a feature 
clusters ve
mse
te
 
    
    
    
 
    
    
    
 
    
    
    
 
   
    
    

fi     feature selection
in order to help us select the most relevant
features  we introduced a lasso penalty
term to our loss function  this term
penalizes the sum of the weights being too
large  causing the algorithm to assign very
low weights to those features that provide
little insight  we ran the algorithm multiple
times with different lambda values  constant
in front of weight sum  in order to find those
variables that were consistently pushed near
   we found that the feature corresponding
to heavy rain storm was consistently push
to    which is likely due to this feature being
active in very few data points  it would also
consistently push either temperature or
feeling temperature to    which is due to
them being so closely related  however 
once we removed these features and re ran
our algorithm  the results we got were so
close to our original results that we didnt
attribute a positive change in our model
performance to the removal of features 
     loss functions
due to the fact that the majority of our error
metrics rely on proximity and closely fitting
the data points  having our linear model
optimize mean squared error did the best of
the basic loss functions we tried  mean
squared error  huber loss  absolute loss  
however  since we also had the second error
metric of being within    of the response
value  we chose to construct a customized
loss function to optimize this  our
customized loss function follows the same
structure as mean squared loss  since this
was already generally optimizing what we
wanted   but we altered it by assigning   loss
to anything that fell within the margin of    
the custom loss function overall performed

comparable to mean squared loss  outperforming it in some cases  and underperforming it in others in terms of test error 
model
ve
mse
te
baseline   m
    
     
    
baseline   c
    
     
    
  clusters   m     
    
    
  clusters   c     
    
    
  c   custom loss  m   mean squared loss
since the difference between our custom
loss function and mean squared loss was so
small  we chose to only use mean square
loss for our testing and comparisons of
hyper parameters  e g    of clusters 
     local regression
the technique that performed the best out
of everything we tried was the usage of
localized regression  we divided the data
into segments based on the month the data
was recorded in  and created a regression
model for each segment  each segment was
also subjected to k fold cross validation as
part of the regression   the reason we chose
to split up the data this way is because each
segment could be modeled independently 
thus allowing closer points  i e  more
relevant points  to have greater effect on the
prediction  we experimented with different
numbers of evenly distributed segments
 starting from january and ending in
december  and obtained the following
results  note that this includes the modified
features  excluding clustering  
segments
ve
mse
te
   baseline      
         
 
    
         
 
    
         
 
    
         
 
    
         
  
    
         

fionce we established that    segments
performed optimally  we introduced cluster
features and re ran the local regression 
clusters ve
mse
te
 
    
    
    
 
    
    
     
 
    
    
    
 
    
    
    
     cluster regression
after we developed our local regression
models  we realized that we were essentially
trying to group data points that were alike 
and perform regression on these groups 
since it turned out reasonably well for
splitting on month segments  we then
decided to take this idea to the extreme and
perform localized regression across the
clusters that we had found when we used kmeans clustering to implement cluster
features 
clusters ve
mse
te
 
    
    
    
 
    
    
    
 
    
    
   
 
    
    
   
 
    
    
    
 
    
    
    
 
    
    
    
 
    
    
    
  
    
    
    
  
    
    
    
  
    
    
    
     hyper parameters
in addition to coming up with techniques to
better fit our data  we also needed to tune
our hyper parameters for optimal results 
the three main hyper parameters we
needed to try and optimize were our
number of iterations for stochastic gradient
descent  the learning rate  and the

coefficient for our lasso penalty term  we
began by trying to optimize our learning
rate  but found that if we increased it to    
or greater  it would periodically diverge and
crash our algorithm  since we must ensure
our algorithm doesnt diverge  we set out
learning rate to      and tried to optimize
our number of iterations based on this
learning rate 
iterations
validation error
  
    
  
    
   
    
   
    
from these results  we were able to deduce
that     iterations was sufficient  as
doubling the run time was not worth such a
marginal increase  we then optimized our
lasso penalty coefficient on the    segment
local regression model 
coefficient
ve
te
 
    
    
  
    
    
  
    
   
   
    
    
from these results  we selected    as our
coefficient value since it had the lowest
discrepancy between ve and te  and it was
large enough that it could push feature
weights to   

   conclusion
     final results
for our final results  we present only the
best result for each category based on
validation error 
algorithm
baseline
baseline w  i
baseline w   c        i
local reg 
local reg  w   c     

ve
    
    
    
    
    

mse
     
    
    
    
    

te
    
    
    
    
    

ficluster reg   c     

    

    

   

  i   indicator variables
   c   x    x clusters used
below is a plot of the best model  local reg 
 localized regression across months   please
note that our model was trained and
predicts on hourly data  however  in order to
make a more comprehensible graph  we
show the cumulative totals for each day
instead of the hourly values  thus  this graph
depicts less variance of the response and less
disparity between predictions and reality 

predictions vs  reality
red line   prediction  black line   reality

     analysis
we see from the table above  that our first
type of localized regression  localized
regression across months  performed the
best on our test set  however  regression
using k means cluster assignments as
indicator features performed the best on our
validation error  considering that localized
regression across months does considerably
better than cluster regression on the test
set  this is surprising and undesirable 
ideally  we want the model with the best
validation error to have the best test error 
one highly possible explanation is that since
the clusters were fit on the entire training set
before it was split into k folds  we

unknowingly gained information that
allowed us to more successfully predict on
the hold out folds in k fold cross validation 
in the future  we will perform clustering on
each iteration of k fold cross validation
instead of performing it once in the
beginning 
judging by the change in validation mse 
 form       to       it seems that the
indicator variables we added were crucial for
success  our interpretation is that the way
we represented our data with indicators
allowed for much more information from
the data to be used  lastly  it seems that
localized regression across months was a
powerful way to model the data  since our
data is a time series where data points that
are close together time wise are highly
correlated  we conclude that doing localized
regression across time variables is an
effective way of capturing the time series
structure 
     future work
if we were to continue working on this
project  we would likely focus on creating
features that are functions of existing
variables  in addition  we would learn about
regression trees  and see how they could be
used to improve our results 
     references
fanaee t  hadi  and gama  joao  event
labeling combining ensemble detectors
and background knowledge  progress in
artificial intelligence         pp       
springer berlin heidelberg 
hastie  trevor  robert tibshirani  and
jerome friedman  the elements of
statistical learning data mining 
inference  and prediction  springer       

fi