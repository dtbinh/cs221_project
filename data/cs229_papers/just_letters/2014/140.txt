classifying wikipedia people into occupations
cs    final report
aleksandar gabrovski  sasho stanford edu 

introduction
wikipedia articles are classified by collaborative authors in categories  each category generally covers a single aspect
of an article   for example common categories are person  woman       births  etc  given the crowd sourced
nature of wikipedia  it is not surprising that the categorization of articles is sometimes incomplete  if not outright
misleading 
wikipedia categories form hierarchical graphs  for example  an article could be marked as in the category
category russian female boxers  which implies that the article is about a boxer  a woman and a russian   each of
which is a separate category  indeed  category russian female boxers is a subcategory with three parent categories 
this is challenging since any automated process on top of an articles categories needs to reason about the specific
category assigned to the article  as well as all category parents of that category  each of which can have its own
multiple parents  
wikipedia categories are extremely useful for automatically generating lists of articles that share a common trait  a
case in point is occupations  if we were interested in finding out more about italian architects  we could go to
category italian architects and discover a very rich listing of architects in italy over the ages  unfortunately  if we
were just interested in architects  we would have to go to category architects which contains the misleading category
category wikipedia categories named after architects   a subcategory about inanimate objects which are clearly
not architects 
the following paper explores the possibility of auto tagging wikipedia articles with some categorical metadata   in
this case the focus is on occupational categories  an accurate occupational classifier could ensure that all wikipedia
person articles are appropriately tagged with the profession of the person in question  making her discoverable from
automatically generated lists by category  we present a classifier on top of a limited set of occupations     total  we
achieve     accuracy with our final dataset of      articles per label 

dataset
to get reliable labels on occupations  we use the automatically generated page lists of people by occupation  as a
seed to a recursive web scraper  in order to avoid blacklisting the authors ip address from wikipedia  we use
wikimedias pywyikibot  for downloading wikipedia pages with automatic throttling of traffic 
starting at the seed page  we proceed to download all links on that page  and then recursively download all of their
links up to a total depth of   hops  as illustrated with the category architects example  this inevitably leads us to
download many pages that are about non persons  to detect which article is in fact about a person  we rely on the
existence of a category of the type  d d d d births  where the  d represents a single digit in base    
in order to avoid the complications of reasoning about a categorys parents  we use a simple heuristic  we assume
that a link on the seed page lists of people by occupation is an actual occupational label  for example  if
 
 

https   en wikipedia org wiki lists of people by occupation
http   www mediawiki org wiki manual pywikibot

 

filists of people by occupation has a link called architects all articles downloaded from architects are marked as
occupation architects 
the first overnight run of the recursive scraper yielded     labels  each label having between    to more than two
thousand person articles  this dataset proved too big for the authors laptop to handle  so for the majority of this
paper we used only   occupations with roughly      articles each  these six occupations are  architect  engineer 
scientist  writer  actor and musician  we briefly explore adding more labels at the end of the report 

features and preprocessing
each wikipedia article contains plain text  links and categories metadata  we start by stripping out all categories from
an article to avoid including the label we are trying to classify in the text of the article  then  we proceed to extract all
links from an article  finally  we apply a standard stemmer  to all plain text words in the article 

words
the plain text words contain a lot of stop words such as a  as  the  the author considered several approaches to
removing the stop words  first  a basic heuristic was used where top n words were removed from the plain text of
each article  unfortunately  the accuracy of the trained models seemed to depend heavily on the value of n if the
number of articles per label was less than     
so  the author experimented with using tf idf   for automatically weighing each word according to its relative
importance  the original tf idf algorithm involves computing the equation 

tf ij  log dfn

i

where tf ij is the frequency of word i in article j   n is the number of articles in the entire corpus and df i is the
frequency of word i in the entire corpus 
in practice  the second term of the above equation led to numbers less than     involving severe loss of precision  in
fact  using the tf idf weighting led to lower accuracy than running the same models with all stop words included 
after some experimentation  the author modified tf idf as follows  leading to much bigger weights for the words
and reduced precision error 

tf ij  logdf i 

a comparison table of not removing stop words and using the modified tf idf approach with a   vs   multi class
svm is presented below 
results for   occupations

 
 

accuracy for each dataset

model

    articles label

    articles label

     articles label

  vs   svm

      

      

      

  vs   svm modified tf idf

      

      

      

https   pypi python org pypi stemming    
manning  c  d   raghavan  p   schutze  h           introduction to information retrieval   p      

 

finotably  as the dataset grows the removal of the stop words from the text seems to matter less and less  this makes
intuitive sense   the more data the model has to train on  the more it is able to figure out that stop words are not a
useful signal 
using the above preprocessing  we generate a feature mapping from plain text words to sparse vectors of modified
tf idf scores for each word occurring in an article 

links
the second raw feature we use are the links a wikipedia article contains  we borrow the concept of the random
surfer from pagerank     intuitively  articles on the same occupation likely point to similar articles  a random surfer on
an article about a boxer is likely to visit other articles about boxers  if we can approximate the probability of the
random surfer visiting a page tagged with a given label  we would have a useful mapping 
following this logic  we introduce the following feature mapping  all links from articles in our training set with label l
are tagged with label l   a link can be tagged with more than one label  for each article we introduce the mapping slj
  the number of links tagged with label l that article j contains  we generate a vector with these weights for each
known label and append it to the word vector described in the previous section  when we get a new article we
havent seen before we compute the slj weights for it by seeing how many of its links were tagged in the training set
and append the weights to its word vector 
a comparison table of the feature mapping without the links and with the links is shown below 
results for   occupations

accuracy for each dataset

model

    articles label

    articles label

     articles label

  vs   svm modified tf idf

      

      

      

  vs   svm modified tf idf   links

      

      

      

clearly the links provide a very good indicator for an articles label 

models
we train multi class svm and logistic regression on the above dataset  in each case we use both   vs   and   vs all
 aka   vs rest   multi class classification  we extend appropriately the logistic regression provided by liblinear  to
  vs all classification  where the model with the highest probability label for a test article is selected as the label
selector  similarly  we extend libsvm  for the svm implementation of   vs all classification  again selecting the model
with the highest confidence  biggest margin  to determine the label 

 

page  lawrence and brin  sergey and motwani  rajeev and winograd  terry        the pagerank citation ranking  bringing order to the
web  technical report  stanford infolab 
 
k  crammer and y  singer  on the algorithmic implementation of multi class kernel based vector machines  machine learning research 
               
 
 r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large linear classification  journal of machine
learning research                     software available at http   www csie ntu edu tw  cjlin liblinear
 
chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm transactions on intelligent systems and
technology                       software available at http   www csie ntu edu tw  cjlin libsvm

 

fifor svm  we determine the value of the regularization parameter c using simple model selection on the dataset with
size of     articles per label  we test the values                                                        for c  the value
of     renders the best accuracy for   vs   svm  for   vs all we choose the value    
we train the above models on datasets of sizes          and      articles per label using simple cross validation with
a       split of the data 

results
following is a table that compares the above described models and features for the previously listed   labels 
results for   occupations

accuracy for each dataset

model

    articles label

    articles label

     articles label

  vs   svm modified tf idf   links

      

      

      

  vs all svm modified tf idf   links

      

      

      

  vs   log reg mod tf idf   links

      

      

      

  vs all log reg mod tf idf   links

      

      

      

the learning curves for the above models are 
  vs   svm modified tf idf   links

  vs all svm modified tf idf   links

log reg mod tf idf   links

 

fidiscussion
interestingly  the   vs all extension of logistic regression had no impact on the accuracy of the model  despite the
increase in the dataset each binary model is running on in   vs all setting  a likely explanation for this is that the linear
separator for logistic regression does not have the same flexibility to adapt to the data as svm  this view is further
supported by the fact that exponential increases in the dataset size barely affect the performance of logistic
regression 
from the learning curves it looks like the   vs all svm has the steepest learning slope  suggesting that this model has
the best odds of improving with an increase in the dataset  running the same model against a dataset with     
articles per label renders accuracy of        suggesting the further improvements might indeed be possible with that
model 
finally  we address some of the concerns regarding the restriction of our labels to   occupations only  we proceed to
gradually increase the labels to      and    adding biologists  chemists and mathematicians respectively   testing the
  vs all svm on these models with datasets of     articles
per label  to the left weve plotted accuracy relative to
the number of labels  note that the y axis ranges from
      to      i e  we do not see a dramatic change in the
accuracy  this makes the author hopeful that the svm
  vs all model can scale ok with the number of labels 
interestingly  the accuracy only drops once we include
mathematicians as an occupation  this is likely due to the
fact that mathematicians  engineers  scientists and
architects all do math  potentially leading to similar
language used across these articles 

conclusions
we managed to reach     accuracy with the   vs all svm
model against our dataset of      articles per label  we also showed that the accuracy of the model does not vary
too much with the number of labels used  both of these results suggest that productionizing this model to actual
wikipedia could lead to very favourable results  unfortunately  the   vs all svm model takes quite a while to train and
performance is heavily dependent on having many articles per label  it seems that if we are willing to slightly reduce
our accuracy to        for a big boost in performance  logistic regression is the a better model to use at scale  such
tradeoffs are of course determined by the business need and infrastructure limitations of the actual product 

future
the above analysis shows promising results for classifying new and existing wikipedia articles on people into
occupations with up to   labels  a natural extension would be to expand the number of labels to something more
realistic and see how that performs  furthermore  the current models only focus on the english wikipedia   it would
be great to see how this scales across languages  finally  for such a project to succeed in the field  serious thought
needs to be given to scaling our models out to millions of articles with a massive feature mapping for each article 

 

fireferences
  

  

  
  
  
  

r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large linear
classification  journal of machine learning research                     software available at
http   www csie ntu edu tw  cjlin liblinear
chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm transactions on
intelligent systems and technology                       software available at
http   www csie ntu edu tw  cjlin libsvm
manning  c  d   raghavan  p   schutze  h           introduction to information retrieval   p      
page  lawrence and brin  sergey and motwani  rajeev and winograd  terry        the pagerank citation
ranking  bringing order to the web  technical report  stanford infolab 
k  crammer and y  singer  on the algorithmic implementation of multi class kernel based vector machines 
machine learning research                 
stijn van dongen  graph clustering by flow simulation  phd thesis  university of utrecht  may     

 

fi