enhancing cortana user experience using machine learning
emad elwany
elwany stanford edu
microsoft research

siamak shakeri
siamaks stanford edu
microsoft

december     

 

abstract

to train our models  we extracted hundreds of
thousands of anonymized cortana query log entries
through a microsoft proprietary internal data warehouse  each log entry corresponds to a user query 
along with relevant information like time  location 
duration  as well as telemetry information indicating how the user interacted with the cortana responses and the overall status of the query  in addition  we are able to group queries by user and
by session using anonymous user and session ids 
this allows us to view the progression of a certain
users queries through a single search session 
while the query logs contain several types of
data  we focused our attention on three 

voice enabled personal assistants like microsoft
cortana are becoming better every day  as a result more users are relying on such software to accomplish more tasks  while these applications are
significantly improving due to great advancements
in the underlying technologies  there are still shortcomings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results 
we analyze the data from millions of user queries 
and build a machine learning system capable of
classifying user queries into two classes  a class of
queries that are addressable by cortana with high
user satisfaction  and a class of queries that are not 
we then use unsupervised learning to cluster similar queries and assign them to human assistants
who can complement cortana functionality 

 

datasets

 general search  queries which cortana redirects to bing search 
 command and control  c c   queries
where the user is trying to execute a task using voice command  such as adding an appointment to the calendar  or sending a text message 

introduction

 enriched search  queries where cortana is
able to provide enriched responses  such as
weather conditions  current traffic  telling a
joke and etc 

cortana is getting better at answering user queries
for more and more scenarios every day  from adding
reminders  to obtaining directions  tracking flights 
and many more  however  the current technology
does not allow addressing more advanced scenarios such as scheduling a doctor appointment or arranging dinner at a restaurant with a friend  these
more complex tasks could however be broken into
simpler tasks  some of which can be automated 
while others require human intervention  human
intervention however is costly  and does not scale
as well as automated task execution  and thus it
should only be utilized as scarcely as possible 

c c and enriched queries align very closely with
the types of queries the cortana enhanced platform
should target to provide an even better user experience  due to computing constraints  we trained
the model using a   day window of data table   
table    summary of datasets

data
category
general data
c c
enriched

in order to achieve that  we need to have a system that can analyze user queries and make a decision on whether a query should go through the
normal cortana flow  or the human enhanced one 
in addition  the system also needs to identify what
category of human assistant is best suited for this
type of query  so that the query can be directed and
handled efficiently 

 

class   to  
ratio
     
     
    

train
size
   k
   k
  k

test
size
  k
  k
   k

data labeling

we employed two different heuristics to label the
data in a pre processing step using the information
 

filaplace smoothing  we test the classifier performance by using simple cross validation  we use
general search data table    for training and testing  the results are summarized in the following
table   as it can be seen  the error rates are high

in the logs  this allowed us to avoid manual labeling and to be able to work with large amounts
of data  our heuristics separate the data into two
classes 
   sufficiently satisfactory cortana response
   unsatisfactory cortana response

table    naive bayes with repetition based tagging

   

repetition based tagging  rbt

test error
  

the simple heuristic works on the weak assumption
that an unsatisfied user re submits the query one
or more times with no or minor tweaks within the
same session  the labeling step goes through all the
log entries and labels an entry   if a user submits a
slightly tweaked version of the query multiple times 
we detect such queries using levenshtein distance
of       

   

training error
     

precision
     

recall
     

and precision is low  which is not promising  there
could be two issues causing this 
 nb is not a proper modeling for this problem 
our intuition here is that the naive bayes assumption does not work well  for example a
word could more likely cause a   classification
if used with question words  this sort of information is ignored in nb by assuming the
features are independent give the output 

feedback based tagging  fbt

this heuristic takes into account more explicit signals from the logs  this information is only available for enriched and c c queries  after cortana
provides the user with answers or actions to these
queries  it asks the user if s he has been satisfied 
we use this signal as the tagging in this heuristic 
however an issue that might cause the labeling  especially with enriched data to be less reliable  is
that the users might just hit a button to move to
another screen without providing feedback to cortana  this will cause cortana to tag that query as
unsuccessful  by looking at the data  we noticed
that for enriched data  the provided feedback on
the status of the query does not truly reflect the
cortana response satisfaction  we will discuss this
later in models performance analysis 

 the tagging heuristic used here does not truly
reflect if the user has been satisfied with the
cortana response or not 
before resorting to other models  we try nb with
fbt  and see if there are any performance enhancements  as the table   shows  the errors are still high
table    naive bayes with feedback based tagging

data
category
c c
enriched

test
error
     
  

train
error
     
  

precision
     
     

recall
     
     

we use the raw query as our primary feature  and
represent queries using the event model  we construct a dictionary of words by pre processing all
the queries  eliminating noise  non alphabetic and
non english words  as well as applying basic stemming to the dictionary terms  we then encode every query into a bit vector of size k  where k is the
number of terms in our vocabulary  the ith bit of
each vector represents the number of occurrences
of the ith term in our vocabulary  the number of
features after all the query modifications turned up
to be        

for c c data  even worse than general data tagged
with rbt  for enriched data  as explained previously  due to high ratio of class   to class   samples table    as well as unreliable feedback used by
fbt  algorithm tends to predict everything as class
   that is why the errors are low  however this does
not mean the algorithm is performing well  we noticed in the predicted test data  only    out of    
class   samples are truly predicted  we will later
design a test to show this flaw with general data
which also suffers from this issue  from now on 
we will focus on c c data which has more reliable
query feedbacks 
since using fbt did not produce any tangible
performance enhancements  we will use other models to train and classify the data 

 

   

 

   

feature extraction

classification models

support vector machine classifier

we now focus our attention on using svm with
fbt and see how the results compare to nb  we

naive bayes

we start by using a bayesian classifier with the
multinomial event model for text classification and

 
the training and test error  precision and recall values
in tables are all in percent 

 

firors also increase  this is expected  as assigning higher weights to class   will cause the classifier to be more biased towards class    this
will end up increasing the number of wrongly
tagged class  s false positives  

used svm with linear kernel implemented in liblinear library      since both our test and training data are unbalanced  we also experimented with
changing the weights in svm  here is the c svm
problem used by liblinear to solve this problem 
x
x
  t
min
    c 
i   c 
i
 b 
 
y   
y   
i

subject to

 precision and recall have opposite trajectories in the figure  with increasing the weight 
a greater percentage of actual class   samples
are classified as class    however this will cause
a higher percentage of class   samples to be
wrongly also tagged as class    thus we will
have higher recall  but with less precision 

i

yi   t  xi     b    b      i
i    

   
we used c c data for train and testing  figures
  and   show the results 
as it can be seen the result are more promising

     

bias vs variance

we will analyze the test and training error versus
training set size change to see whether we are running into over or under fitting issues 
the number of training samples is more than   
times the number of features for c c and general 
for enriched data  this value is      considering
the test and train error rates are very close  we are
not facing variance or bias  figure   shows this for
c c with linear svm  the values of training and

figure    test and training errors for linear kernel
svm with c c data using fbt

figure    training and test error vs  training set size
for linear kernel svm with c c data using fbt

test error are staying very close to each other with
changes in the training size  also  the amount of
the change is near    when changing the size from
       to          thus both our data sets and
number of features are large enough so that are
models are not suffering from either bias or variance 

figure    precision and recall for linear kernel svm
with c c data using fbt

with svm  we can get as low as       test error
with weight       for class   with reasonable precision and recall  the optimal result considering
precision and recall besides the test error  is with
weight     for class    test error         recall 
    and precision         intuitively  this value
for weight is close to the ratio of class   to class   
which is      table     here are some observations 

   

logistic regression

as another popular classifier  we will now use logistic regression model to classify the c c data  we
will be using l  and l  norm logistic regression
implemented in liblinear     

 as the class   weight is increased over the optimal point       both the training and test er 

fi     

table    l  vs  l  norm logistic regression with c c
dataset

l  norm

the unconstrained primal optimization problem
solved for this model is 
min


kk    c

l
x

log     eyi 

t

xi

 

model
l 
l 

   

i  

where  is the the vector of parameters  we will
also experiment with weights as with svm  figures
  and   show the results  the model demonstrates

training
error
     
     

test
error
     
    

precision
  
     

recall
     
     

class    and ran the model over c c dataset  as
it can be seen from table    l  norm model slightly
outperforms l  norm in terms of test error  however the difference is very marginal 
     

logistic regression vs  svm

both models perform very closely  our intuition is
that they both are solving very similar optimization problems  so it is expected that they perform
closely  the regularization done in logistic regression models were not very useful  as we saw that
due to the large size of our dataset  the algorithms
are not experience high variance 
figure    test and training errors for l  logistic regression with c c data using fbt

   

testing queries with aggressive class
ratio datasets

in order to see how well our model performs against
naive algorithms  as well as how resilient it is with
respect to the changes in the input query class ratio 
we choose three different set of test samples 
 test set containing of all  s  this is to test
against an algorithm algbrute  that always
predicts    in this case algbrute will have   
accuracy 
 test set containing of all  s  this is to see how
well our models perform agains an all positive
data set  algbrute will always predict     
here 
figure    precision and recall for l  logistic regression with c c data using fbt

 test set containing of      s and      s 
algbrute will have     accuracy 

very close performance to linear svm  here are
some notes 

we will be using linear svm with class   wight of
     this models performance on the normal test
set has been  test error          recall       
precision          tables   shows the results with
c c data  as it can be seen  the classifier main 

 weights of     to     yield the best results considering test and training errors as well as recall and precision 
 the similar trends in terms of training and test
error as well as precision and recall is seen compared to svm 
     

table    linear svm with aggressive c c test data

test set
all  s
all  s
     

l  norm
l

min


x
t
  t
  c
log     eyi  xi  
 
i  

test error
     
     
     

precision
   
 
     

recall
     
inf
     

   

we experimented with l  norm logistic regression 
and the results did not show any tangible differences with l  norm  we used     as weight for

tains its accuracy well  the precision and recall
are by definition expected to be   and inf when
there are no  s  and precision is      when there
 

fisuch as svm and logistic regression  we could reduce the test errors to as low as      we also
saw improvements in precision from     to     
besides that  we noticed that svm and regularized logistic regression models have very close performances  in addition  we showed that our svm
model with proper tagging fbt  is resilient with
respect to various ratios of class   to class   in the
test samples  different from training set 

are no  s  for the other cases  both precision and
recall maintain very close to normal training set
performance with aggressive test data  this has
the important implication that our model has not
merged to become naive algorithms where one class
is predicted all the time  or choosing one class with
a certain probability every time  these two naive
algorithms will perform very poorly with the aggressive tests 
we performed this experiment with general
search data  which as mentioned before has the
problem of unreliable tagging  we used   as the
weight for class    table   shows the results  as
it can be seen  specifically with all  s data  the
algorithm seems to be doing a coin flip to predict
the samples  thus the training labels have not provided any net valuable information to the model 
this proves our previous claim that the rbt is
not reliable  we saw similar results with enriched
queries tagged by fbt 

 

one of the main obstacles that we encountered was
how to properly tag the data for test and training of the model  inferring user satisfaction is very
complex  and needs a more complex mechanism to
detect  a more advanced way is to follow the actions that user takes after the cortana interaction
is over  this can give more accurate information to
see whether user has been satisfied or not 
another path to expand this work is to run the
models over a larger set of data  we used only a
day of cortana data in this project  our main obstacle was limited computation resources as well as
inability of the implemented models to digest data
sets larger than the available virtual memory of the
machine  for example  the svm we used requires
loading the whole training sparse matrix  which becomes very large considering our large number of
features and queries 
one area that we touched slightly and could be
well investigated further is query clustering  we
did clustering on the data  however we did not see
meaningful clusters  meaning that we were expecting to see clusters of reminders  meetings  etc  this
was not observed by using k means  more advanced
algorithms such as em could yield better results 

table    linear svm with aggressive general search
test data

test set
all  s
all  s
     

 

test error
    
     
     

precision
   
 
     

recall
     
inf
     

query clustering

we start by using k means     to cluster queries together  we used our own implementation of the
k means algorithm and then compared it to the
matlab build in version for validation  both implementation yielded very similar results for the same
value of k and the same similarity metric  we used
cosine similarity     as a similarity metric when performing the clustering step  we tried to use the
em algorithm as well  but it was a challenge to
use our sparse bit vector data representations to
estimate gaussian distributions  we plan on investigating using factor analysis to work around this
in the future 

 

future work

references
    chang  chih chung and lin  chih jen  libsvm  a library for support vector machines  acm transactions
on intelligent systems and technology  volume    issue
        
    rong en fan  kai wei chang  cho jui hsieh  xiangrui wang and chih jen lin liblinear  a library
for large linear classications  journal of machine
learning research       
    levenshtein  vladimir i binary codes capable of correcting deletions  insertions  and reversals  soviet
physics doklady       

conclusion

the purpose of this project was to predict the
queries that existing cortana system will not be
able to handle with satisfactory results  these unsatisfied queries could later be handled by humans
as a premium service  we started by naive bayes
model and a repetition based tagging of general
search queries  our results showed poor performance with errors more than      with improved
tagging of data by using fbt  and better models

    macqueen  j  b some methods for classification and
analysis of multivariate observations   proceedings
of  th berkeley symposium on mathematical statistics
and probability   university of california press      
    singhal  amit modern information retrieval  a brief
overview  bulletin of the ieee computer society technical committee on data engineering       

 

fi