is beauty really in the eye of the beholder 
yun  albee  ling  yling   jocelyn neff  jfneff   and jessica torres  jntorres 
abstract
recent research suggests that high facial symmetry plays a large role in whether or not a person is deemed beautiful 
to test this hypothesis  a beauty classifier was built utilizing various machine learning algorithms in hopes of modeling
societys general idea of beauty  data collection included survey responses and online facial images  facial recognition and
facial feature extraction was performed with haar cascades  beauty classifiers consisted of algorithms such as random
forest  support vector machines  k nearest neighbors  and logistic regression  facial features were detected with     
error  and beauty was correctly classified using the random forest model with      error 

 

introduction

feature detector  in order to train the models  facial features
first needed to be identified  for each of these images  a binary
a common belief is that more symmetric faces are more beauclassification of beautiful or not beautiful was given  these
tiful  if this is true  could it be possible that beauty is not
images  like the celebrity images  were cropped to   x   pixels
subjective  but in fact a result of a mathematical compilation
so that there would be more resemblance between the kaggle
of facial features  by applying machine learning to face detectraining set and the test set 
tion  facial feature extraction  and the quantification of facial
symmetry  a model theoretically could be built that attempts     facial feature extraction
to predict a faces beauty based off features alone  using fea  using the haar cascades facial feature detector provided by
tures that represent the symmetry of a face  a models high opencv      separate models were developed to detect the
accuracy could suggest that societys idea of beauty can be bounding box for the face  eyes  nose  and mouth  opencv
learned by a machine  moreover  a models poor performance had a face and eye detector  but lacked a mouth and nose
could highlight intrinsic qualities of beauty that cannot be detector  in order to detect the nose  a haar cascade model
quantified in model features alone 
was trained  it was trained on    positive images and    
negative images selected from the     of the kaggle data la  materials and methods
beled as training data  for the developed model  a window
    data
size of    x    pixels was used with   stages of training  a
minimum hit rate of       and a max false alarm rate of     
      kaggle
kaggle provided         x   gray scale images with the the positive images were tight bounding boxes of the facial
pupils  eye corners  eyebrow corners  and mouth corners iden  features to be detected 
in a eyeball comparison of the developed and available
tified  each image was a head shot of a face  the images were
model 
the available model had a tighter and more accurate
not necessarily head on shots  and the subjects had varied fabounding
box for the features  therefore  the online model
cial expressions  in addition  there was only a total of   
was
used
for
the test set feature extraction  similar findings
subjects  each subject had multiple photos of him or her with
were
found
for
a nose detector online     
a variety of poses and facial expressions  in order to train the
in
order
to
minimize false positives  the search space for
haar cascades classifier  the kaggle dataset was partitioned
the
classifier
at
each stage was minimized  first  the boundinto     training and     testing 
ing
box
for
the
face
was identified  then  the pixel range for
      survey
searching for the eyes  nose  and mouth was reduced to within
fourteen celebrity images were selected to be distributed in
the box  the eye model resulted in the largest number of false
a survey asking users for their assessment of the celebritys
positives  so additional constraints were implemented  given
beauty  widely held opinions of a celebritys attractiveness
that all faces were   x   pixels and the faces were constrained
could impact responses  so lesser known celebrities were choto all be similar facial shots  the search space was reduced to
sen  in addition  the set of celebrities was limited to cauthe upper    pixels of the face  if the detector still found
casians to reduce biases caused by ethnicity  there were equal
more than two eyes  all boxes were considered questionable
number of males and females  each image was selected so
and the classifier did not classify the eyes  if  however  the
that each facial image was forward facing with little facial
feature detector found one or two eyes  then the eye with
tilt  these qualifications eased facial detection  ensuring that
the lower x axis pixel location  i e  the top left corner of the
the features and distances derived from these facial features
box was further to the left side of the picture  then that box
were correct  each celebrity was ranked on a scale of   to    
was deemed to be the right eye  left and right side of the face
with     responses  there were a total of       rankings of
are from the subjects point of view  
facial beauty  the mean rating across all faces was       the
nose and mouth were also detected  the mouth classifier
rating distribution for each face followed a normal distribuwas likely to return many mouths  so only the bounding
tion where the mean varied from      to       these photos
box that had the largest y coordinate  and thus furthest to
served as the test set for the models later developed  figure
the bottom of the picture  was classified as the mouth 
   
      online images
in addition to the celebrity images      images were collected     feature matrix creation
of faces online to serve as the training set for the models  after facial features were detected  a feature matrix was crethese images also served as the test set for the haar cascade ated as input for beauty classification models  in order to
 

ficalculate various distance statistics  coordinates for the center of boxes that enclose each of the facial features  eyes  nose
and mouth  were identified  the final feature matrix included
basic facial features that are commonly cited in literature     
such as length and width of the nose  in addition  attributes
that are concerned about symmetry were added  since our
hypothesis is that facial symmetry is associated with beauty 
specially  two measurements that were cited in multiple literature        central facial asymmetry  cfa  and facial
asymmetry  fa  were part of our feature matrix  fa is the
sum of pairwise differences between midpoints of each facial
feature in the x axis direction  while cfa is the sum of differences between midpoints of adjacent facial features  figure  
is a visualization of all the distance statistics calculated for
each face 

   

     

svm is a supervised learning technique that finds the maximum margin separating hyperplane between classes  kernel
tricks are often applied to map input features into high dimensional space  especially when data are not linearly separable  function ksvm in r package kernlab was utilized
in all analyses      various svm models and kernels were applied to the entire feature matrix with    features per face
and the reduced feature matrix with only    most correlated
features  the svm models include c svm classification  csvc   nu svm classification  nu svc   and bound constraintsvm classification  c bsvc   the kernels include radial basis
kernel function gaussian  rbfdot   polynomial kernel function  polydot   linear kernel function  vanilladot   hyperbolic
tangent kernel function  tanhdot   laplacian kernel function
 laplacedot   and anova rbf kernel function  anovadot  

classification

four classification algorithms implemented in r  logistic regression  lr       support vector machine  svm       knearest neighbor and  random forest  rf       all classification models were trained on leave one out cross validation
 loocv  method 
     

     

k nearest neighbor

knn is an unsupervised learning algorithm that takes input
integer k  for a given test example  the classification comes
from the majority vote of the k data points that are the closest in euclidean distance  ties are broken at random  this
method was chosen because its excellent performance in a
previous study      function knn in r package class was
used in all analyses using all possible values of k     

feature selection

the original feature matrix included    features  a reduced
feature matrix included    features were the most correlated
with human ratings of beauty in the     images using fselector package in r      all machine learning models were
implemented in both feature matrices for comparison  p values of fa and cfa for each of the    celebrity faces were
output based on the     training images  the p values were
calculated by dividing the number of training faces with the
same or lower values of fa or cfa as the test example by
    
     

support vector machines  svm 

 
   

results
facial feature extraction

the combination of feature detectors collected yielded a successful facial feature detector model  table   summarizes the
results from the haar cascade classifiers developed and used 
the training error was the error from testing on kaggle      
images where faces were not necessarily forward facing  untitled  relaxed expressions  to determine if the algorithm
correctly found the feature  the x y coordinate specified by
kaggle for the feature had to lie within the bounding box
the feature detector found  there were two test sets  the
    images that would then comprise the training set for the
beauty models  and the    celebrity images that comprised
the test set for the models  because neither of these sets of
images had feature locations identified  a correct classification
was eyeballed by running through the results  accuracy was
much higher  as all the testing images were forward facing 
minimal tilt  relaxed faces  in the case that the model did
not find a feature  no prediction was made  given that there
were only     test images  predictions that were not made by
the feature detector were manually tagged in order to provide
all features to the beauty models 

random forest

random forest is a classification method utilizing combinations of tree predictors such that each tree is built independently from the others  a single classification tree is often not
an accurate classification function  thus  rf utilizes an approach which aggregates several inefficient classification trees
using a bagging procedure  at each step of the algorithm 
several observations and several variables are randomly chosen and a classification tree is built from this new data set  for
example  given a training set x   x            xn with responses
y   y            yn  bagging repeatedly selects a random sample
with replacement of the training set and fits trees to these
samples  the result is a final classification decision obtained
by either a majority vote on all the classification trees or
by averaging the predictions from all thepindividual regresb
sion trees for unseen samples  x    f   b  b   fb  x     where
b   b            bn sampling  for our purposes here  majority     feature selection
vote on all classification trees was taken  the package rpart
the    most correlated features are distance between right
from r was used in generating our random forest model 
pupil and top of forehead  distance between midpoint of two
      logistic regression
pupils and top of forehead  distance between midpoint of the
logistic regression  lg  is a classification parametric model forehead and midpoint of two pupils  distance between nose
which is used to estimate the probability of specific observa  tip and left corner of mouth  distance between right pupil
tions to belonging to a particular class  the built in function and right edge of forehead  distance between left pupil and
glm from r was used to build our binary lg model assum  left edge of face  fa  distance between midpoint of two pupils
ing a binomial distribution  loocv was performed with the and nose tip  cfa  and distance between two pupils length
package boot package in r as well 
of face  ca and cfa  which measure the overall asymmetry
 

fiof each face  are among the most associated features  furthermore  the average values for ca and cfa are both smaller
in people who were rated beautiful  this is evident that the
most beautiful people have more symmetric faces  among
the seven celebrities that were deemed beautiful    have significantly symmetric faces  however only   out of the seven
celebrities that were not rated beautiful has symmetric face
based on the p values 

   

by opencv     was optimized  it is still a very slow algorithm
to run  in order to speed up the runtime so iterations on the
model could be made  a smaller window and less images were
chosen  given that the algorithm loops over all pixel boxes
up to the pixel box size specified     x    pixels  increasing the pixel size  while likely increasing the accuracy of the
model  drastically lengthens training time  a drastic hold up
on training was also the number of negative images  while
the algorithm trained instantaneously on the positive images 
negative images were processed more slower 
the results showed that testing precision was perfect 
meaning that all error was created from the predictor not detecting any feature on a test example  given that the models
and symmetry features depended on an accurate representation of the facial features  it was best to sacrifice some accuracy for higher precision  thus  the feature detector would
only consider a feature correct if there was high confidence in
a feature prediction  explaining the high precision 

knn

knn was first implemented using all    features in the feature matrix  it achieved the lowest training error of      with
k    of     eight and nine out of the    final test examples
were predicted successfully  next  knn was applied using
only    features that were the most correlated with human
ratings of the     training images  this classifier performs the
best when k   or   and the training error       using k  
  and    knn was able to accurately classify ten and nine of
the    celebrity faces accurately  as its best  knn reaches a
training error of      and testing error of      after feature se    comparison of classification models
lection by correlation and when three nearest neighbors were
the observation that random forest classifier outperforms
considered  see figure   
support vector machines  logistic regression and k nearest
    svm
neighbor classifiers might be attributed to the presence of
with full feature matrix  the lowest training error was zero  noise within the data  tree classifiers like rf indirectly treat
which is from nu svm with all three different kernels  ra  features unequally by discarding irrelevant attributes and usdial basis kernel function gaussian  laplacian kernel and ing informative discriminative features more frequently  on
anova rbf kernel  amongst the three kernels  anova the other hand  in svm classification all features are treated
rbf had the lowest testing error         using reduced fea  equally and hence svms are more sensitive to high dimenture matrix  the lowest training error rate       came from nu  sional data  interestingly enough  for the case of logistic
svm classification with laplacian kernel  although the testing regression because lr is conceptually simple and has proven
error rate was very high         the lowest training error rate to produce robust results  it was expected to perform much
       was from bound constraint svm classification with hy  better than what was actually observed  some reasons for
perbolic tangent kernel  as well as nu svm classification with this observation could be that lr tries to find broad relationpolynomial kernel function  the overall lowest training and ships between independent variables and predicted classes 
testing error was reported in table   
but without guidance they cannot find non linear signals or
interactions between multiple variables  knns low perfor    logistic regression
mance  evidently highlights that some features do not contain
logistic regression was implemented on the complete feature
enough information of the beauty of faces  this would explain
matrix created  table   describes the top ten features sorted
why rf out performs knn in this project  another drawby most significant p values  as observed there were very few
back of our knn implementation here is that the best model
features that held compelling p values  in training our lg
used a small value of k  k     the bias is low due to the small
model the observed training error with loocv was       for
value of k  but the variance is high      this means that our
the test set our lg model performed worse than our training
model might not perform well with other data sets  similarly
set  only being about to accurately classify images at random
to knn  svm also overfit our data since the best training
with a test error of     
error was zero  much smaller than the test error  nu svm
    random forest
model worked the best amongst the three models because it
random forest was implemented in the binary classification puts boundaries on the number of support vectors and errors
of beautiful or not using again the full feature matrix  the to deal with the variance bias trade off     
training error received was      on     images with loocv 
random forests high performance in both training and
the ca and cfa feature variables were observed to play two test sets allowed for its clear victory over all other algorithms
of the most important roles in classification  see table     tested  rf offers a powerful tool in addition to classification 
the results pertaining to the test error was similar to the variable importance ranking  this is done by estimating the
training error        this suggest that our model held a ade  predictive values of variables by scrambling the variables and
quate amount of trade off between variance and bias 
seeing how much the model performance drops  when applied here both ca and cfa  which measure overall facial
  discussion
asymmetry  are the two most important feature variables to
classification  this correlated well to survey responses  the
    facial feature extraction
more beautiful faces were more symmetric according to cfa
in order to better understand the haar cascades model  a and fa  thus  we conclude symmetry can be positively assomodel was developed  while the training algorithm provided ciated with facial beauty 
 

fi 

future work

curate model  we could do to improve our beauty classifier 

survey results could have been more deeply analyzed to discover unconscious bias that could impact the results  sexual
orientation could also impact a persons perception of beauty 
while the survey was pitched as a beauty rating and not an
attractiveness rating  the two often go hand in hand so that
the attractive person is also deemed beautiful 
with more time to train the haar cascade model  a better
feature detector could be built using more positive and negative images  and a larger window size  a tighter bounding
box would yield a more accurate representation of the face 
in fact  a bounding box that is simply a point that identifies
separate points on a feature would be preferred  then  no
approximation would be needed to find the center of the feature  additional features could also be detected to test other
features effect on symmetry and beauty  for instance  emily
deschanels face was deemed by the algorithm to be relatively
symmetric  but she had a mean beauty score of   from survey
respondents  perhaps other features affected peoples perception of her beauty  for instance  she has a very strong jawline
for a woman  other detectors could be trained to detect these
features as more features would aid in developing a more ac 

 

 

based on our classifier results  there is a good amount of
noise in our feature matrix and only using the reduced feature matrix left out some information  feature selection could
have been done more carefully  i e  compare various other
methods like mutual information or pca to better represent
our data  on a separate note  although the feature matrix
was created to best capture the facial feature in our case 
there could be other statistics that also contains information
about beauty of each face  there were studies that identified
points on each face and used pairwise distances between the
points as the feature matrix      they further reduced the
feature matrix using pca  in order to quickly collect survey
data on the additional     images that served as the training
set for the model  only binary classifications of beautiful or
not beautiful could be gathered  with more allotted time a
more precise detector could have been built i e   a scaled rating rather than binary  other models like svr  glm  em 
and kmeans could also have be evaluated  lastly  because
the models were implemented with default settings  optimizing the model parameters might have improved performance 

bibliography
  

r core team         r  a language and environment for statistical computing  r foundation for statistical computing  vienna  austria  url http   www r project org  

  

haar cascades  url http   alereimondo no ip org opencv   

  

hastie  trevor  et al  the elements of statistical learning  vol     no     new york  springer       

  

opencv github repository  url https   github com itseez opencv

  

kagian  amit  et al  a machine learning predictor of facial attractiveness revealing human like psychophysical biases  vision research                      

  

eisenthal  yael  gideon dror  and eytan ruppin  facial attractiveness  beauty and the machine  neural computation                      

  

grammer  karl  and randy thornhill  human    em  homo sapiens  em   facial attractiveness and sexual selection  the role of symmetry and averageness  journal of
comparative psychology                   

  

hornik  kurt  david meyer  and alexandros karatzoglou  support vector machines in r  journal of statistical software                   

tables and figures

model
developed nose detector
online    nose detector
online    eye detector
online    mouth detector

training error
     images 
    
    
    
    

testing error
    celebrities 
n a
    
    
    

testing error
     images 
    
    
    
    

testing precison
    
    
    
    

table    error results for the haar cascade feature detectors

figure    feature detections made by haar cascade model 

 

fifigure    calculated features represented by purple lines 
figure    accuracy of knn by k using whole feature matrix in    
training images
feature
fa
cfa
nose mouth r
forehead nose midpoint
eye forhead l
nose leye
diff pupil forhead r
diff pupil face l
nose mouse
nose mouth midpoint
eye forhead r

rf importance ranking
     
     
    
    
    
    
    
    
    
    
    

table    top ten features from random forest classification model

model
random forest
logistic regression
knn
svm

training error
    
    
    
    

table    comparison of classification
models for beauty detection

testing error
    
    
    
    

 intercept 
pupil face ratio
nose mouth l
diff nose face l
nose mouth r
d pupil
nose mouth midpoint
diff mouth face l
diff pupil forhead r
mouth chin midpoint

estimate
     
       
     
    
    
    
     
     
     
    

std  error
     
     
    
    
    
    
    
    
    
    

z value
    
     
     
    
    
    
     
     
     
    

pr   z  
    
    
    
    
    
    
    
    
    
    

table    top ten features from logistic regression model

 

fi