obstacles avoidance with machine learning control methods in
flappy birds setting
yi shu    ludong sun    miao yan    and zhijie zhu 
 

department of mechanical engineering  stanford univerisity
december         

 

abstract
object avoidance is an important topic in control
theory  various traditional control methods can
be applied to achieve control of object path such
as pid  bang bang control  sliding mode control  given a known and simple dynamic system 
those classical controls method work pretty well 
however  controls of complex  multi degree of
freedom systems or controls of systems with unknown dynamics have been pushing the limit of
traditional control laws  this report adopts machining learning methods of support vector machine svm  with linear kernels and reinforcement learning using value iteration to solve control problems in the game flappy bird without
understanding the dynamics of the problem  for
comparison purposes  bang bang control is also
implemented and described in the report  the
game is also modified to increase difficulty for
further comparison 

introduction

with the increasing popularity of autonomous vehicle 
unmanned aerial vehicle  humanoid robots and etc   new
methods of controlling complex system to avoid object
are points of interests among control engineers  many
classical control methods are based on accurate models
or concrete knowledge to derive dynamics of systems
and corresponding control laws  for complicated or
unknown dynamics systems  classical control methods
are not very efficient and new systematic methods are
needed to solve such problems  machine learning draws
much attention in last decades and is well applied in
many areas such as shopping recommendation systems 
computer vision  speech recognitions and etc  applying
machine learning to solve complicated control problems
is investigated in this paper 
games are ideal environments for developing  implementing and testing machine learning algorithm 
flappy bird system is chosen to compare control
methods and machine learning methods in general and
also compare different machine learning algorithms 

the machine learning methods are shown to
significantly improve the results got from bangbang control  in the modified game version
with moving pipes  reinforcement learning is
more feasible than svm  detailed implementation of and comparison among each methods
are discussed in this report 

the game flappy bird is a side scrolling mobile game 
which was a very popular in early       the objective is
to direct a flying bird  named faby  which moves continuously to the right between sets of pipes  if faby
keywords machine learning  controls  obsta  touches the pipes  the game ends  each time the player
cles avoidance
taps the screen  the bird briefly flaps upwards  if the
screen is not tapped  the bird falls to the ground due
to gravity  which also ends the game  the game has all
necessary features  a moving object and obstacles  the
game itself is repetitive  hard and addictive  the goal
 yishu stanford edu
is to achieve autonomous flying of birds travelling be ldsun stanford edu
 miaoy stanford edu
tween pipes  when and whether to flap is determined
 zhuzj stanford edu
by control methods and machine learning algorithms 
 

fi 

problem formulation

   

bang bang control

from years experience in classical control  by analyzing the games source code  we consider that bangbang control is one of the most effective classical control
methods that comes to our mind and expect bang bang
control to at least help birds go through a few tubes  so
just for comparison purpose  simple bang bang control
the decision will be based on the relative and absolute is implemented by using only one feature y  the bird
parameters of the pipes and the bird  as fig   shows  only takes action on the basis whether it is above the
there is a multi dimensional attribute space consisting tip of next pipe 
of 
there are two different scenarios  the first scenario
is the standard flappy bird  for the second one  to
raise the difficulty of the game  the pipes are changed
to be able to move upwards and downwards at a specific
speed 

   

 v   bird speed in vertical direction

svm

the svm is applied in the first scenario  so the first
three of attributes are considered  the training set of
 y   axis distance relative to the bottom of next the problem is not linearly separable in the feature space
of raw data  the svm maps the input vectors into highpipe
dimensional feature space and gives the maximum margin hyper plane  svm with a linear kernel is adopted
 vpipe   pipes velocity  modified version 
and liblinear library     is used to implement svm al ypipe   pipes position  modified version 
gorithm  the problem is constructed in the following
way 
 x   axis distance relative to the front of next pipe

s t 

y  i t

 
 
min w b   w  
 


wt x i    b     i       n

data 
the svm is implemented in a static pipe setting  for
simplicity  only the first pipe ahead of the bird is taken
into consideration 

 a  first situation

the training set is generated by several trials of human 
if human gives the command to flap  the current state is
labeled as    otherwise current state is labeled as     at
every sampling step  a current state x i     x  y  v  
and related label y  i          are recorded in the
training set 

 b  second siutation

figure    birds feature selection

 

as illustrated in fig    the training set generated by
human in the original state space is not linearly separable  in fig    red crosses and circles represent training
points labeled with flapping and not flapping respectively  thus  a high dimension feature space is chosen 
nine features from fundamental analysis are selected 
the dimension of the data is increased by adding second and third order of the feature  in fig    blue crosses
and circles represent testing result labeled with flapping
and not flapping respectively 


   x y v x  y   v   x  y   v  

proposed solutions

it is natural to implement the svm and reinforcement
learning  this is a classification problem with discretizable continuous features  so svm is chosen  when the
complexity of the task increases  for instance  the obstacle is moving in a random pattern  it will be hard for
human to supervise the learning process and to generate
training sets  thus reinforcement learning is applied
as an implicit supervised learning algorithm 
 

fifigure    svm training result
k  x  z    t 

   

reinforcement learning

the markov decision processes  mdp  settings of the
reinforcement learning will be presented in details below 

figure    rl state discrete

action taken at every state is binary  flap or not flap 
represented by a     and a     in the program  at
there are four states contributing to the control of the every time step the program will read the current state
of the bird and generate an action based on the learning
bird  x  y  vb p   yp  
result from last iteration 
definitions of x and y are the same as mentioned
in last section of svm  vb p   vb  vp denotes the ver  c  rewards
tical velocity difference between the bird and the pipe 
because the movement of the pipe is also considered in at every iteration  if the bird is in collision with a pipe 
this section  yp denotes the pipes position in y direc  we set the reward of last state 
tion 
rt  s  a         t   t
a  states

in the fixed obstacle case  state vector s    x  y  vb p  
is applied  which is chosen by the intuition of the specific physical system  in the moving obstacle case  state
vector s    x  y  vb p   yp   is applied  the reason why
yp is added is that the movement of pipes is nonlinear 
the y position of the pipes has upper and lower boundaries  so that a feasible solution for the birds trajectory
exists  the pipe will reverse when reaching the upper or
lower boundary  so yp will be necessary for the learning
algorithm to determine future movement of the pipe 

all the states before collision are rewarded with 
rt  s  a       t                 t   
the total reward for state s and action a is updated at
every time step 
total
rt  
 s  a    rttotal  s  a    rt  s  a 

after every iteration is terminated by the collision  the
actual reward for every state is calculated by  if the
state has been visited  

due to the program implementation of the game 
x  y  vb p   yp are naturally discretized  however  due
to the curse of dimensionality  the computation load
for reinforcement learning is incredibly large with the
four dimension state space  thus  for computational
cost concerns  the state space is divided to coarser grids 
the discretization is illustrated in fig   below  areas
further from the pipe gap are discretized with coarser
grids  since the control decision is more natural in the
area 

r  s  a   

total reward in state s with action a
 times we took action a in state s

d  state transition
according to the known dynamics of bird  it seems that
psa  s    denoting the state transition probability from
state s to s  with action a is deterministic and can be
computed explicitly as a priori  however  due to the
switching of target pipe when the bird is going out from

b  action
 

filast pipe and checking for a new pipe  the state transition at this point is not continuous as the case of targeting the same pipe  randomness appears in the state
transition because the relative position of the new pipe
is unpredictable given the state relative to last pipe 
thus  we are unable to compute transition probability
matrix psa with only the knowledge of bird dynamics 
psa is computed on line during every iteration of reinforcement learning  according to the equation below 
psa  s     
figure    learning process of reinforcement learning
for fixed pipes

 times we took action a from state s to state s 
 times we took action a in state s
with increasing quantity of data collected in iterations 
psa  s    will finally converge to the real value 
e  optimization algorithm
due to discretization  the size of state space is limited
to the level of two thousand  for small mdps  value
iteration is often very fast and converges with a small
number of iterations  thus  we use value iteration to
find the value function 

figure    learning process of reinforcement learning
for moving pipes

the value iteration is processed as follows 
for each state  initialize v  s       
repeat until converge
  for every state  update 

pink line show the average results of bang bang control and svm  respectively  moving average of reinaa
forcement learning results are shown in the red line  it
is clear that machine learning methods generate better
 
results than the classical control law  in a short period
is set      which reflects the effect of future actions on  less than two hours   svm is significantly better than
current decision 
reinforcement learning  the video link   is the game
played under svm  it is worth noting that there is an
after several iterations  v  s  and psa are well learnt by up going trend of results of reinforcement learning  it
the algorithm  to apply the control law given by the has been proven that reinforcement learning can help
learning algorithm given a state s  the control input is birds perform better than svm  video link    
given by 
fig   shows the results of modified game  the pipes
a   max psa  s   v  s 
aa
are moving randomly  the game is so hard that scoring more than five is very challenging for humans  in
this case  bang bang control still works but with lower
  simulation results and disscu  scores  svm is not feasible due to the lack of enough
data  the powerful of reinforcement learning comes into
sion
play  reinforcement learning is obviously better than
bang bang control  after training overnight  it looks
the criterion to compare different methods is the num  like moving pipe is not a big problem and the results got
ber of pipes the birds can fly through before the end from reinforcement learning in moving case are as good
of the game  the x axis is the number of iteration the as the standard case  link   shows the game  movgame played  the y axis is logarithm of the times the ing pipe case  was played under reinforcement learning 
the reason why reinforcement learning performs well is
birds fly through pipes successfully in each round 
that the policy we generated can determine the right
fig   shows the results when the standard game is action in a specific state  which is hard to find using
played  the methods are compared  the blue line and traditional control law 
v  s    r  s    max s  psa  s   v  s   

 

fivideo link    svm in standard game 

is used to test different algorithms  distance in x and
y  birds velocity were chosen as standard features  for
https   www youtube com watch v cyfei efaby
the modified version adding moving pipes  two addition features  pipes velocity and pipes position  were
video link    reinforcement learning in standard added  bang bang control  svm and reinforcement
game 
learning were applied to both standard game and modified flappy bird with moving tubes  it turns out
https   www youtube com watch v uwfnunhkccg
that machine learning is immensely useful for controlvideo link    reinforcement learning in modified ling complicated system or system without known dynamics  comparing between svm and reinforcement
game 
learning  for fixed pipes  the svm and reinforcement
learning generally perform similarly  svm is much bethttps   www youtube com watch v um b ihimcw
ter because it has a much higher data space  for moving pipes  it is almost not possible for human to play
smoothly and generate enough data so svm is not feasible due to the lack of training data  reinforcement
  future development
learning still works very well 
in the evaluation of reinforcement learning  it has been to be clear  table   below summarizes the advantages
found that a more refined space of data and longer train  and disadvantages of each methods 
ing time will enhance the performance of the learning
table    comparsions of three methods
algorithm  with limitation of laptop  the full potential
of a refined data space and long training time has yet
bang bang
svm
reinforcement
to be explored 
control
learning
pro
easy
to
implerelatively
less
robust
in
however  when discretization of the states is refined 
ment 
fast
comcomputing
time 
different
circumdata storage and iteration speed become issues  a posputing
work well in stances
sible way to resolve this might be adopting sparse mastandard situatrix to store state transition data in that some states do
tions
not transit to remote states  another option is to find
con inaccurate  lim  un robust
in expensive comthe smart way to discrete the states in the reinforceited
complicated
puting 
long
ment learning  e g  applying sampling based method to
situation
training period
extract states from the original continuous space 
another future improvement of our work is incorporating more complex dynamic models of the birds to the
learning algorithm  the reinforcement learning deals
with the control of the bird without knowing the dynamics of the bird  however  the dynamics of the bird
in the game is merely a simplified version of projectile
motion  in reality  the bird might be a real vehicle that
has various available control inputs and a constraint of
fuel consumption  the vehicle can decide to ascend  descend  accelerate and decelerate  besides  uncertainty
of the environment and dynamic system  such as wind
disturbance and modeling error of the dynamics  also affect the decision model in the learning algorithm  these
factors might bring new challenges to the current implementation of reinforcement learning algorithm 

 

 

acknowledgement

we want to thank professor ng and cs    tas for
teaching this wonderful course  we learned a lot in this
quarter  we also would like to thank mingjing zhang
for providing flappy bird source code so that we can
modify and add machine learning components into it 

references
    andrew
ng
cs   
machine
learning
course
materials 
 online 
     
http   cs    stanford edu materials html      
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang 
and c  j  lin  liblinear  a library for large
linear classification  journal of machine learning
research                     software available at
http   www csie ntu edu tw  cjlin liblinear

conclusion

this report focused on applying machine learning to
solve control related problems  the game flappy bird
 

fi