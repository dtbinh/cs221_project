mining for confusion  classifying affect in mooc
learners discussion forum posts
akshay agrawal
akshayka cs stanford edu

  

introduction

interest in educational data mining  edm  has grown exponentially in recent years      the journal of educational
data mining was established to accommodate the surge in
edm research activity  in the following years  learning at
scale and the international conference on learning analytics cropped up as well  broadly  edm is concerned with
methods to explore unique types of data in educational settings and     to better understand students and the settings
in which they learn       its applications cut across many
domains within the educational setting  ranging from visualization of course activity to automating course construction 
the advent of massive open online course  mooc  platforms edx and coursera in      unleashed vast troves of
data for researchers to mine  much effort has gone into
developing student models  predicting students nextquestion accuracy and modeling their acquired skills  some
argue that such research is reaching the point of diminishing
returns     
in contrast  relatively little research has been devoted to
digging into mooc discussion forums  learners often resort to discussion forums in order to better understand the
subject matter at hand  mooc enrollments  however  are
large  the typical coursera class  for example  consists of
more than        students      these learners posts get
drowned out in a sea of other  non academic posts  especially
since some learners use forums as conversational chatrooms 
as an unfortunate result  instructors might overlook questions posed by struggling learners  discouraging both forum
participation and learner motivation     
ideally  the mooc platform would automatically feed instructors with posts relevant to them  instructors could then
simply answer posts out of this intelligent queue  relieving
them of the burden of sifting through thousands of discussion posts by hand  such a platform would empower learners  too  and would more generally boost the usefulness of
discussion forums 
in this paper  we take an initial step towards an intelligent
queue by presenting a pipeline that automatically surfaces
posts that exhibit confusion  we say that a post exhibits
confusion if it appears that its author either explicitly requests for clarification about a course topic  or if his language implicitly reveals a gap in comprehension  in contrast 
we say that a post exhibits knowledge if it delivers factual
information relevant to the topics studied in the course  if
the post conveys neither confusion nor knowledge  we say

shane leonard
shanel stanford edu

that it is neutral  while edm work on analyzing sentiment
in mooc forums exists      to the best of our knowledge 
no previous work has explicitly attempted to identify confusion in posts  other edm efforts have attempted to predict
instructor intervention patterns      such an approach will
only be successful in generating an intelligent queue if the
instructor is already effective in answering relevant forum
posts  which might not be the case  we instead take the approach of discovering content that we a priori believe will be
relevant to instructors  regardless of their discussion forum
history 
we frame the problem of retrieving confused posts as
a multilabel classification problem  of the following form 
given the body of a discussion forum post p with a true
unknown label l in   knowledgeable  neutral  confused   
apply some hypothesis h that correctly divines l  that the
problem is a multilabel one is of significance  solving the
easier binary classification problem in which posts are labeled as confused or not confused would force us to forfeit
potentially useful information about where a post author lies
on the knowledge confusion spectrum 
the remainder of this paper is organized as follows  we
describe our dataset in section two  detail our preprocessing 
feature extraction and feature selection pipelines in section
three  list our models in section four  present our results in
section five  interpret them and discuss insights gleaned in
section six  and propose future work in section seven 

  

dataset

two sets of nearly        distinct discussion forum posts
each were collected from stanford onlines openedx offerings and scored by three distinct judges  hired from odesk 
one set consisted of six courses categorized under the humanities  while the other consisted of four courses categorized
under medicine 
our dataset has eight relevant columns that convey the
following  the full set of columns is listed in table    particularly interesting columns include the confusion column 
which represents the extent to which the post communicated
knowledge or confusion  confusion was encoded as a number from   through    with   indicating plentiful knowledge
and   encoding plentiful confusion  other columns provided
metadata about the post and its author 
the process   of gathering  cleaning  and preparing the
dataset was labor intensive but necessary  no vetted tagged
 

for more information about this process as well as an anal 

fidataset of confused posts exists  the gold sets will soon be
made publicly available through stanford 

  

feature generation

we have a potentially  d    k     dimensional feature
space  where  d  is the number of unique words encountered in a training set  on the order of thousands   k is
the number of additional features we engineer  and   corresponds to the last six rows in table   above  we call the
entire pipeline from pre processing and feature extraction to
feature selection feature generation  our feature generation
pipeline takes as input the forum post body text and the confusion  up vote  anonymous  anonymous peer  type  reads 
attempts  and grade variables for each forum post  each of
these variables is processed in parallel and converted into a
fractional feature vector  we take the union of these fractional vectures to produce a full feature vector  and finally
apply a feature selection algorithm to winnow the full feature vector down to the feature vector fed to our classifiers 
for each of our variables  there exists a simple mapping that
takes them to an integer representation  processing the text
document  however  requires significant work  the text processing pipeline consists of three stages      preprocessing 
    feature engineering  and     feature extraction 
in the preprocessing stage  we tokenize the text with a
handrolled regular expression  that matches single character
words and punctuation  in addition to multiple character
words  we clean text by mapping the families of latex
equations  numbers  and urls to three unique tokens  and
by converting all tokens to lowercase    finally  we discard
words that are included in modified version of the glasgow
information retrieval groups english stop words list  our
version does not contain the tokens i  can  cant  cannot 
could  and couldnt 
in the feature engineering stage  we artificially manipulate our token space in order inflate the weights of particular tokens or sequences of tokens  in particular  we inflate
duplicate each token that occurs in the first sentence  with
the assumption that students might summarize their intent
early on in their posts  unforutunately  our dataset did not
capture the titles of each post  we also optionally leverage
a chunk parser     and classifier to find noun phrases in
the original text documents  and then duplicate each token
that occurs in the noun phrase  we additionally include the
option to construct k character n grams from the tokens 
in the feature extraction stage  we convert our tokens to
integers  we begin by simply counting occurrences of each
token  on a per model basis  we convert these counts to binary indicators  see the next section for more information  
finally  we optionally replace raw counts with their tf idf
equivalents      intuitively  doing so allows us to generate
a dynamic sieve word list by giving less weight to words
ysis of inter rater reliability  see 
http   bit do krippendorff
 
the
python
regular
expression 
r   u   b w  b          
 
case is sometimes helpful in detecting affect  for example 
in some online forums  frustrated posters may write in allcaps  however  discussions in mooc forums tend to be
fairly civil  and such casing is rarely used 

that occur in many documents  without necessarily rejecting them altogether 
after extracting features from our text documents and
from our metadata variables  we take the union of the generated vectors to produce our full feature vector  we then
optionally apply the chi square univariate feature selection
algorithm in order to reduce the dimensionality of our feature space  the chi square metric is often used in bag of
words feature selection  it performs relatively well and is
computationally inexpensive      in our tests  we experimented with no feature reduction  and reducing to the    k
most relevant features  k from   to   

figure    generating the feature vector  sketched here
is the entire feature vector generation pipeline  from preprocessing and feature extration to feature selection  the
input to our pipline consists of the columns listed in table    each variable is processed in parallel  but the most
work  and the most interesting work  goes into processing
the body of the forum post  the text document is first tokenized and cleaned  an array of feature engineering algorithms are then applied to the sequence of tokens  givin us an
intermediate bag of words representation of the document 
the counts are further processed  and are then unioned with
representations of our metadata variables  finally  we apply
a univariate feature selection algorithm to winnow down our
original feature vector into the one ingested by our classifier  the parenthetical actions in the figure are applied on
a per classifier basis 

  

models

multiple classification models were implemented and evaluated for the purpose of classifying forum post confusion 
these models include multinomial naive bayes  linear svm 
and logistic regression  each of these models was selected on
the basis of several factors  including ease of implementation
and their applicability to a unigram text model  i e  bagof words approach   current research suggests that  while
bag of words is not ideal  classification methods that use
the bag of words feature space often achieve high perfor 

fivariable

description

confusion

a number from   through    with   indicating maximum plentiful knowledge and   indicating plentiful confusion 

timestamp

when was this post submitted 

up vote

how many up votes did this post receive 

anonymous

was the poster anonymous to all persons 

anonymous peer

was the poster anonymous to his peers 

type

did this post initiate a thread  or was it a reply 

reads

how many reads did this posts thread garner 

attempts

how many submission attempts had this posts author made on the courseware before timestamp 

grade

what was this post authors grade at time timestamp 

table    dataset columns  confusion is a weighted average of the judges scores  all other columns were computed from
the data contained in datastage  datastage stanford edu  
mance      in addition  the chosen classifiers have been
shown to be near state of the art compared to other bag ofwords approaches  this is particularly true for linear svm
            
we were able to implement each of these models using the
scikit learn toolkit in python  using the scikit api  we built
a pipeline that handled preprocessing  feature selection  and
classification  the pipeline provided a modular way to easily
interchange and compare the classifiers  as well as the effects
of feature selection algorithms like tf idf and k best selection 
the multinomial event model for naive bayes was chosen over a multivariate bernoulli model because it has been
found to outperform this model when the vocabulary is large 
to briefly differentiate between the two  the multinomial
event model captures the number of occurences of each word
in a document  while the multivariate model does not  both
models use the naive bayes assumption      we chose to implement naive bayes classification as a first approach  and
to provide a baseline for comparison with logistic regression and linear svm  as expected  the naive bayes model
worked reasonably well  but the other models proved to be
more effective 
it is generally well accepted that support vector machines
 svm  provide state of the art performance for text classification  and in particular they perform well with simple
feature spaces such as bag of words or bag of features    
         in addition  svms tend to be more robust than
naive bayes classifiers when applied to a skewed category
distribution  as is the case with our dataset       therefore 
implementing an svm was a natural choice for our application  we ended up using a simple linear kernel  instead of
the rbf  gaussian   polynomial  or sigmoid kernels  this
was partly because we lacked the computational and temporal resources to tune kernel parameters  and partly because
the linear kernel was able to achieve high performance  we
did not consider it necessary to spend time implementing
nonlinear kernels because prior research has suggested that 
for text classification  they offer little performance benefit
over linear kernels          
as a final model  we applied logistic regression  we chose

logistic regression because several sources suggested that it
can achieve similar performance as the svm  for the text
portion of the feature vector  we modified the multivariate
event model  instead of recording the counts of each word 
we simply recorded whether the word was present or absent
from the document  this modification had a positive impact
on the classifiers performance  we found that the logistic
regression did outperform naive bayes  and was similar to
the svm  as expected 

  

results

we evaluated the performance of the classifiers with several different options for the feature selection and feature
space  for the feature space  we compared     a text only
bag of words representation without any post metadata  and
   a union of text features and the post metadata  as described in the feature generation section  for each feature space  we evaluated the logistic regression  svm  and
multinomial naive bayes classifiers using     no feature selection     tf idf  or    k best feature selection with the
chi squared metric  also as described in the feature generation section  figure    displays the results for both feature
spaces using no feature selection  figure   displays the results using tf idf  and figure   displays the results using
the k best feature selection  these figures show the average
precision  recall  and f  scores from the cross validation test
for each model 

  
   

discussion
interpretation of results

we used precision  recall  and the f  score as our primary metrics of success  as reported after applying our classifiers to our test sets  the best classification f  score of
         was achieved by the logistic classifier  this score
was achieved without tf idf or k best feature selection 
applied to the full text plus metadata feature space  using l 
regularization with a penalty parameter of c        every
preprocessing step was used  namely custom tokenization 
lemmatization of numbers  lemmatization of latex equations  lemmatization of urls  and a custom stop words list 

ficlassifier

precision

recall

f 

logistic  all features

        

        

        

logistic  text only

       

        

        

mult  nb  all features

        

        

        

mult  nb  text only

        

        

        

lin svm  all features

        

        

        

lin svm  text only

        

        

        

table    classifier performance on humanities courses 
listed in this table are the precision  recall  and f  score per
classifier for the confusion class  we collected these numbers
by using    fold stratified cross validation on the testing
set  the all features classifiers used both textual data and
the metadata listed in table    the text only classifiers
only used the text body of forum posts to generate features 
figure    classification results using chi squared k best
feature selection      best features  on humanities 

as demonstrated in figures   and    tf idf and chi squared
feature selection did not necessarily help our results  manually inspecting the most informative words suggested that
the feature space reduction caused our classifiers to select
esoteric words that just so happened to be associated with
confused posts in the training set 

   

figure    classification results using tf idf on humanities 

overall  we found that the svm and logistic regression
classifiers performed reasonably well on the data  we also
found that the multinomial naive bayes consistently performed significantly worse than these two models  this is
consistent with our expectations based on prior research 
multinomial naive bayes performed particularly poorly when
all variables were used because it treats its inputs as counts
 e g   a grade of     is equivalent to     occurrences of some
token  so our classifier overfit to these metadata variables 
what was surprising  however  was that the additional metadata only slightly improved the classification results of logistic regression compared to the text only feature space 
indeed  performing ablative analysis on the logistic regression model demonstrated that the only variable that made
an appreciable difference was type  this may be because the
dataset was not sufficiently cleaned for edge cases  e g   we
have reason to believe that grade and attempts were unreliable  but it also may simply be because the features were
noisy or generally less informative than the unigrams 

insights

some interesting qualitative insights arose during the course
of this research  when implementing the pipeline  we were
able to use the scikit learn api to surface the most relevant
features for each model  with naive bayes  the most relevant text markers for confusion were very specificusually
names  elements from an equation  or other tokens that appeared in only a handful of forum posts  they did not reflect
any implicit understanding of confusion in general  instead 
the naive bayes model had overfit to some specific tokens
that appeared only in positive examples  by contrast  the
logistic regression and svm identified text features that revealed a much better understanding of confusion  the features they deemed relevant included words like confusion 
problem  help      understand  and thanks  figure
  illustrates the relevant features picked out by logistic regression during    fold cross validation over the full dataset 
the size of each word corresponds to the number of folds
that identified the word as one of the top    text features 
another interesting finding was that doing cross validation
with leave one course out produced significantly better results than cross validation across all the data  for example 
in humanities  leaving one statistics course out and while
still having another statistics course in our training set let
us achieve an f  of       this suggests that it may be easier
to detect confusion in technical courses  and suggests that an
online machine learning pipeline could work well  especially
if we trained on previous iterations of courses 

  

future work

fifigure    unigrams identified as relevant by logistic regression during stratified    fold cross validation 

we have several suggestions for the future direction of
this research  first  there is much room to investigate more
sophisticated feature spaces that augment or replace the
straightforward bag of words approach  since confusion detection is related to sentiment analysis and text categorization
topics of significant ongoing studythere exist approaches
that will likely surpass the bag of words and bag of features
implementations presented here  building off of the results
here also provides the opportunity to create a new kind of
corpus labelled with confusion markers  as of yet  this corpus does not seem to exist 

  

conclusion

we tackled a novel problem that had not been previously
attempted by the educational data mining community  that
of detecting confusion in forum posts  we had an involved
feature generation pipeline and demonstrated that we can
achieve fairly good results  in addition  we were able to
tease out interesting insights  and found that our classifiers
might work well in an online setting  at least for technical
courses 

  
   

references
references

    beck  joseph e   and xiaolu xiong  limits to
accuracy  how well can we do at student
modeling   educational data mining         apa
    chaturvedi  snigdha  dan goldwasser  and hal
daumal iii  predicting instructoraazs intervention
in mooc forums 
    koller  daphne  et al  retention and intention in
massive open online courses  in depth  educause
review             

    mcguire  robert  building a sense of community in
moocs  campus technology  campus technology 
public sector    sept        web     dec       
    romero  cristasbal  and sebastiaan
  ventura 
educational data mining  a review of the state of the
art  systems  man  and cybernetics  part c 
applications and reviews  ieee transactions on     
                
    wen  miaomiao  diyi yang  and carolyn penstein
rose  sentiment analysis in mooc discussion
forums  what does it tell us   proceedings of
educational data mining        
    zheng  zhaohui  xiaoyun wu  and rohini srihari 
feature selection for text categorization on imbalanced
data  acm sigkdd explorations newsletter    
              
    mccallum  andrew  and kamal nigam  a comparison
of event models for naive bayes text classification 
aaai    workshop on learning for text categorization 
     
    boulis  constantinos  and mari ostendorf  text
classification by augmenting the bag of words
representation with redundancy compensated bigrams 
proc  of the international workshop in feature
selection in data mining       
     pang  bo  and lillian lee  a sentimental education 
sentiment analysis using subjectivity summarization
based on minimum cuts  proceedings of the   nd
annual meeting on association for computational
linguistics  association for computational linguistics 
     
     forman  george  an extensive empirical study of
feature selection metrics for text classification  the
journal of machine learning research          
          
     colas  fabrice  et al  does svm really scale up to
large bag of words feature spaces   advances in
intelligent data analysis vii  springer berlin
heidelberg                
     yang  yiming  and xin liu  a re examination of text
categorization methods  proceedings of the   nd
annual international acm sigir conference on
research and development in information retrieval 
acm       

fi