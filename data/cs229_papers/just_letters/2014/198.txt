evergreen or ephemeral 
predicting webpage longevity
through relevancy features
elaine zhou  lingtong sun
stanford university  stanford  ca      
 ezhou  lsun      stanford edu
i  introduction
with the rapid proliferation of user generated content
available on the internet  one of the biggest challenges
is determining the relevancy of the information
shown  the content often comes in two camps 
ephemeral or evergreen  evergreen content such as
recipes for carrot cake or intro to data structures
frequently dont change with time  whereas ephemeral
content  such as celebrity hot or not trends or local
high school sport scores easily become dated  unlike
apps that harp on ephemerality like snapchat  the
internet doesnt have the luxury of assigning
expiration dates to content  humans can easily
distinguish one from the other  but machines have yet
to do so 
the challenge here is to predict whether or a not a
new piece of web content will be ephemeral or
evergreen  this would be helpful for all sorts of
recommenders to sift through relevant content 
improving web search results  or customer opinion or
review pieces  or general methods for prioritizing web
archives  our goal is to develop a prediction model
and identify most relevant attributes for evergreenness using machine learning techniques 
ii  background   data set
stumbleupon is a user curated web content discovery
content that aims to recommend relevant media and
links to its user base  at the crux of improving their
recommendation engine is the problem of ephemeral
or evergreen content    how can we try and classify
websites before voted on by users 
thus  as presented by kaggle  we use stumbleupons
raw html content scraped from over        
websites  a training set of evergreen or not      
labelled urls  and a test set of       unlabelled
urls      these websites were compiled in a  tsv file
with aggregated text and    numeric meta data fields 

such as the boilerplate text  the ratio of spelling errors 
or ratio of  img  tags vs text  the boilerplate was a
json string format with the title  body text  and url of
the website     
iii  feature sets   pre processing
first  we set out to understand the nature of the data
given and how previous stumbleupon users classified
evergreen websites  at an initial glance  the numeric
data provided by kaggle fell in two camps  those
related to amount of embedded media  e g 
embed ratio  html ratio  image ratio  and the content
of embedded media  e g  numwords in url 
commonlinkratio    linkwordscore   there was very
little related to the interactivity from javascript code
or the markup of body text itself from html tags 
previous research on web page longevity validated
our initial observations    although there is much
variation among web page in terms of top level
domain and by page type  web content itself is more
likely to stabilize      the insight here is that we
should focus more on the content and less on the
html structure or dom itself 
next  we dove deeper into the text data provided by
the page url  body text  and title  we initialized our
text feature selection by using the boilerplate of the
elements  but immediately found some issues  there
was empty body text  empty titles  and the body and
title text often were rife with advertisements  having
any sort of article with a major component as null or
empty labelled as evergreen can be seen as a false
positive  in our preprocessing procedure  we cleaned
up this data  as we later discuss 
from the numeric and text data provided and our
observations and intuitions about html webpages 
we proceeded to focus on our features in two parts 
the    numeric meta data values  with the addition of
converting alchemy category to an integer

firepresentation   and   text features given by the
boilerplate  title  url  and body text 
iv  discussion   results
we discuss our evaluation and use of several different
types of classifiers  and our research into the different
feature sets aforementioned and their effectiveness 

we show the results of these three classifiers with kfolds cross validation  figure     in most  we found
there to be a fairly high bias  since we arent using
many features but it gave us a good understanding on
which models to focus on 
figure    k folds cross validation for
logistic regression  svm  and nave bayes

before we delve into the specifics of classifier and
feature selection  it is important to discuss the
scoring method that we employed  for each feature
set and classifier  we implements k folds cross
validation with a k value of     meaning we divided
our training data into    subsets and ran the classifier
   times  each time using a different subset as the test
set and the rest as training sets   the score for the
classifier and feature set was then determined to be
the mean of the cross validation 
a  classifier selection
we now discuss the three main classification
algorithms that we investigated in detail  naive
bayes  logistic regression  and support vector
machines  svms   we began with the numeric metadata provided to get a better understanding of the
problem at hand  but we iterated and improved on
these models by adding the text features and other
feature sets 
   naive bayes  as recommended in class lecture 
we began with an implementation of the multinomial
naive bayes classifier  we focused strictly on the
content  and just used the alchemy score and
image ratio as a quick baseline 
   logistic regression  from the naive bayes
exercise  we realized that this model tended to
underfit the data  to reduce the bias  our next
attempt was with the logistic regression  the
difference in scores was only marginal 
   svm  we implemented svm and found that c
      gave the best result for the data model  quick
note on other models  we also attempted linear
regression and quadratic discriminant analysis
models  but their yield wasnt close to that of the
nave bayes  logistic regression  or svm  so those
results are not included in this report  we decided to
keep iterating on these as main models for our
dataset 

b  feature selection
from our first implementation of a number of models 
we were getting similar results without much
improvement  after some more exposure to the
dataset  we decided to expand on the input features
and focus on the text data features as well  we began
by letting machine learning algorithms choose the best
features to select through forward feature search 
   forward feature search  our rudimentary
analysis of   features led up to mediocre results  so we
wanted to let the data choose the features instead  we
maintained our three classifiers  naive bayes  logistic
regression  and svm  and ran forward feature search
for the top   features  the features that appeared most
in all the models are  linkwordscore  numberoflinks 
and numwords in url  however  even with the choice
of all    different features  our improvements  feature
over feature  were not significant  from figure    we
can see that for logistic regression and naive bayes
models  we quickly approach a limit of a score of
around       the second realization we had is that
features we were provided were not the most helpful   after   or   of the best performing features  the score
peters out  the maximum features and the resulting
subsequent score is shown in figure    thus  our next
step was to gather more informative features and we
turned to parsing and understanding our text features 

fifigure    changes in scores by adding best features
in each classifier

scraper  we repeated the process but with the full text
of the websites with python boilerpipe  a python
wrapper for the java html fulltext extraction library
     using the top feature determined from forward
feature search  we added the fields for body text
subjectivity and body text polarity  however  this
was unfruitful after much processing power and wait
time  as our accuracy dipped  ultimately  there was
too much noise in the body text 
figure    changing score with sentiment analysis
sentiment analysis forward feature selection

figure    best features from forward feature
search for various models

   sentiment analysis  our background research
and forward feature search indicated that body text
was critical in understanding website longevity  from
our conversations on how best to represent this
information  we moved to sentiment analysis  we
hypothesized that highly intense  emotional reactions
or highly negative or positive text might affect the
ephemerality of a website  using the open library
textblob  we added  the title subjectivity  title
polarity  boilerplate body text subjectivity  boilerplate
body text polarity  and url subjectivity  and url
polarity  this resulted in   new features to our
forward feature search  both the subjectivity of the
title and the sentiment of the body paragraph appeared
in the top   features in the logistic regression model 
however  our hypothesis was not fully realized  adding these features improved our maximum
correctness rate by a marginal difference  as shown in
figure    the logistic regression score improved      
upon further examination  we realized there was
insufficient text data from the boilerplate the body
text could be found in the boilerplate json  next  we
tried to expand beyond the boilerplate data to include
all the body text data from the websites  using a

   term frequency inverse document frequency
 tf idf  features  after trying the aforementioned
sentiment analysis  forward feature search  and
various other techniques to improve the performance
of our classifier to limited success  we did some
further research and determined that the text
frequency of the site contents might yield more
promising results  the feature we decided to extract is
the term frequency   inverse document frequency
 tf idf   essentially  tf idf is the term frequency times
the inverse frequency  the advantage of using tf idf
to term frequency alone is that we can scale down the
impact of tokens that occur very frequently and thus
give us less classifying information  the formula uses
a form of laplace smoothing in that we do tf    idf  
   to not entirely ignore terms with   idf  we
employed scikits built in tf idf extractor on the
content of our websites and ran logistic regression as
well as our naive bayes classifiers with this new
feature set  we obtained a much better classification
accuracy using the same scoring system described
earlier  with logistic regression  we obtained an
accuracy of         with nb  we obtained an
accuracy of        

fifigure    tf idf results

when combined with our earlier results  tf idf seems
to drown out the other features and dominate in the
classification because adding the other features did
not alter the results of classification at all  however 
with tf idf  we were able to build a much more
successful classifier 
v  conclusion
understanding the format and content of a websites
body text is vital to many web mining applications
and we presented a method to effectively solve the
stumbleupon evergreen challenge  however  despite
given the emphasis on the body content  it is
important to filter out the noise and spam from a
variety of sources to present stronger classification
features 
vi  future work
there is still much to be learned in the topic of
website longevity  future work would involve
gathering more and different data sources  as we
learned  the content of the website is critical is
classifying its status as evergreen  however  we are
definitely lacking in the types of raw data that
stumbleupon contains if we take the alchemy
category as a basis  the first thing we recognize is that
there are no articles under the category religion 
indicating there are article areas that are lacking 
considering the timelessness of religious texts like the
bible or the quran  neither author has any expertise
in this field  but in an analog example     m copies of
the bible are sold or given away each year and the
quran is one of the most widely read and recited book
in the world      expanding data sets into more
unconventional sources might yield further insights
into website ephemerality 

another potential exploration could be in the labelling
of evergreen or ephemeral from the get go  it is not
explained how these were determined    whether it
was the opinion of an individual or of a group  either
way  its not the best reliable measurement of website
longevity since it has an inherent bias towards the
interests of stumbleupons user demographic  as our
data perusal showed  hand labelling is also prone to
error  perhaps stumbleupon can implement another
metric to supplement or quantify the label of
evergreen  maybe to track the change or stability in
traffic over time  or the click through rate of articles
when presented to real end users  these are just a few
changes that could help establish and explain
evergreen while reducing the rate of incorrect labels 
vii  references
    kaggle  stumbleupon evergreen classification
challenge  http   www kaggle com c stumbleupon
    kaggle  data   stumbleupon evergreen
challenge 
https   www kaggle com c stumbleupon data
    koehler  w   web page change and
persistencea four year longitudinal study
   
the
battle
of
the
books
http   www economist com node         

fi