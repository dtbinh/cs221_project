estimation of causal effects from observational study
of job training program
dmitry arkhangelsky and rob donnelly
darkhang stanford edu   rodonn stanford edu

 

introduction

in the social sciences  researchers are often interested in measuring the effect of a treatment
or intervention  for example  many labor economists are interested in measuring the effect
of job training programs on increasing salaries or reducing unemployment  measuring the
causal effect of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals  in this case the difference in the average outcome of the
treatment and control groups can be directly compared 
unfortunately in many policy relevant situations economists do not have data from a randomized controlled experiment  instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment  but no directly comparable control group 
we tried two approaches to estimating the effect of a job training treatment on wages 
the first approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population  the second approach matches the
treated individuals to people from the general population based on propensity scores  we
find that the second approach yields more plausible estimates of the effect of job training 

 

dataset  features  and preprocessing

we are using data from the national supported work demonstration  which gave      
months of job training to unemployed adults at    sites around the us  we have data on
    men who participated in the job training  as well as        adults from the general
population    in order to be eligible to participate in the job training  an individual had
to meet certain eligibility requirements and had to volunteer  because of this  the pool of
participants is very different from a random sample of the population    previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the effect of job training programs 
in particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population  naive
estimates often predict that the job training actually lowered wages 
our features include the age  years of education  race  marriage status  annual income
  and   years before training program  and unemployment status   and   years before job
 

this data is from       but is still relevant since it is a commonly used baseline in the economics literature
for methods of evaluating treatment effects from non experimental data 
 
participants in this job training program on average have less education and are more likely to have prior
criminal convictions or to be unemployed 

 

fitraining  we normalized all of our variables by subtracting the mean and dividing by the
standard deviation  to engineer additional features we also tried including cross products of
the base features in our dataset up to degree   

 

models and results

we used two distinct strategies for estimating the treatment effect  our first approach is a
more straightforward one  we use several models to predict the final wages of each individual
in the large sample from the general population  we then used these models to predict wages
each individual would have gotten if they hadnt received training  the difference among the
individuals who received training between their actual final wage and their predicted final
wage  can be used as an estimate of the change in wage caused by the job training  i e  a
casual effect 
the second approach comes from the program evaluation literature  first  we estimate
the propensity score  conditional probability of being in the treatment group  and then
compare the treatment outcomes between the treated and untreated individuals who have
similar predicted probabilities of receiving the treatment 

   

regression models

we tried several types of models for predicting wages  linear regression  lasso regression   
svm  with linear  polynomial  and radial kernels   and random forest models  we choose
parameter values for each model using a grid search with    fold cross validation using root
mean squared error  rmse  as our selection criteria  we evaluated performance on a    
hold out set  the exponents following a models name indicate the degree of the polynomial
interactions of features that were included   
by looking at the learning curves for these models it became clear that all of the models
had high bias  we attempted unsuccessfully to get access to more detailed data with a
larger number of features for each individual  adding polynomial interaction terms did not
substantially reduce test error 
linear  degree   

linear  degree   

elastic  degree   

elastic  degree   

linear  degree   

random forest

svm radial

svm linear

svm polynomial

    

rmse

    

    

    

    
    

     

     

    

     

     

    

     

     

    

     

     

    

     

     

    

     

     

    

     

     

    

     

     

    

     

     

nobs
train

type

 

test

linear regression with l  regularization to reduce overfitting 
e g  a   means that we added new features corresponding to the product of every pair of features in the
base dataset 
 

 

filinear 
linear 
lasso 
lasso 
linear 
random forest
svm  radial 
svm  linear 
svm  polynomial 

training rmse

test rmse

treatment effect

       
       
       
       
       
       
       
       
       

       
       
       
       
       
       
       
       
       

      
      
      
      
      
      
      
      
      

linear regression with degree two polynomial interactions gave the lowest rmse on the
hold out set  the random forests low training error and higher test error suggests overfitting 
none of the svm models were very successful  even after trying a wide selection of different
values for the parameters  since we are predicting a continuous outcome  salary  that is
normalized to unit variance  squaring the rmse gives a measure of the fraction of variation
in salary that is unexplained by the model  roughly      since there are many determinants
of future wage  the industry you work in  work ethic  etc  that we do not observe  it is not
surprising that we are unable to explain all of the variation in wages 
we used these models to predict what wage the individuals in the job training would
have gotten if they had not participated  the difference between predicted wage and actual
wage for the individuals who received job training is the used as the estimate of the effect of
the training  all of the models produced negative estimates of the effect of the job training 
measurements of the effect of job training programs that come from randomized experiments
consistently find positive effects  which suggests our first approach is not producing accurate
estimates of the effect of the job training 

   

propensity score models

our second approach was propensity score stratification  we first train several models to
estimate a propensity score for each individual  the likelihood of he was given job training   
then  we cluster observations with similar estimated propensity score and estimate average
effect within each group  this allows us to compare the treated individuals with individuals
from the general population who had a similar likelihood of being offered the treatment 
we use several conceptually distinct models for estimating propensity scores  logistic regression  lasso  decision tree  boosted tree and random forest  again  tuning parameters were
chosen by    fold cross validation  as an accuracy measure we use the squared difference
between our continuous estimate of the propensity score and the binary treatment indicator 
its worth emphasizing that perfectly predicting who received training would not advantageous here  since then the model would suggest that no person from the general population
is comparable to anyone in the treated population  despite this conceptual strangeness 
propensity score matching has a substantial theoretical grounding  caliendo       
 

i e   we are predicting a continuous estimate of the binary outcome  training no training

 

fitraining and test errors for all procedures that we used are presented in the following
table  since less than    of our data received the job training treatment  we also report
conditional errors  average error for treated observations 

logistic 
logistic 
lasso 
tree
boosted tree
random forest

train
error

test
error

cond  train
error

cond  test
error

balance

 
groups

treatment
effect

      
      
      
      
      
      

      
      
      
      
      
      

      
      
      
      
      
      

      
      
      
      
      
      

    
    
    
    
    
    

 
 
 
 
 
 

     
     
     
    
     
     

due to the unbalanced distribution of the outcome variable  its unsurprising that the
overall accuracy is much higher than the accuracy on the treated individuals  overall both
measures of test error seem to agree on which models were more accurate  logistic regression
and lasso seemed to outperform the tree based estimates  however  its unclear whether the
method with the lowest test error is the best in terms of estimating the overall treatment
effect 
to cluster observation into several groups we took observed treatment status and ran
a simple decision tree using estimated propensity score as the only covariate  this procedure results in groups of people with similar propensity score  another possible procedure
is to cluster observations using some unsupervised algorithm like k means  however  one
important advantage of using trees is that the number of classes is selected automatically 
for each model of propensity score we report the average treatment effect  assessed balance
and the number of classes  formally  we compute the following statistics 

pk
i   n i y i   y i 
   
average treatment effect  
pk
i   n i
here i is a generic class  n i is the number of treated individuals in this class  y ik is the
average outcome in i th class  for k group  either treatment or control  
to assess balance we compute the following normalized sum of squares 
 

balance  

m
x

pk

 x ji  x ij   

i   v xjk    v xjk   

k

j  

   

where v xjik   is the estimated variance of the j th covariate in the i th class and k th group 
a lower value corresponds to more similarity between the treated and untreated individuals
within a group  which is desirable 
the best model in terms of balance is the random forest model  there was considerable
variation in the estimated treatment effect between different models  but all of the models
predict positive effects on wages from the job training and the magnitudes are plausible and
 

fiincrease in salary of roughly       for a group of individuals whose previous salaries averaged
        

 

conclusions and future research

when researchers have used randomized controlled experiments to evaluate similar job training programs they generally find the training leads to a wage increase of      to       
with this as a baseline  the first approach was wildly inaccurate since every model predicted
a substantial negative effect  this approach might be more successful on a dataset with more
features  since all of the models tried had high bias  having more information about each
individual might allow us to generate models that can explain more of the variation in wages 
if we were able to create highly accurate predictions of each individuals wage  then we would
be able to generate accurate predictions of what each individual in the job training program
would have earned if he didnt participate  we would then have an accurate estimate of the
effect of the job training even without running a randomized controlled experiment 
in contrast the propensity score based estimates all had the correct sign  with the
exception of the simple tree model  all of the estimate treatment effects are consistent with
the      to       range found in experiments of similar programs  the high variation
suggests that only limited confidence can be had in the estimates from any single propensity
score model  running several different models  as we have done here  may give researchers
a sense of the general range of outcomes they should expect if the program were re run as a
true experiment 
for future research in this area  we are also interested in testing out unsupervised learning
algorithms as mechanisms for clustering individuals into discrete groups and then comparing
the treatment effects within each group  approaches like this might also allow researchers to
know not just the average affect of the treatment across the whole population  but also have
predictions of which subsets of the population the treatment worked especially well on 

 

references

caliendo  m     kopeinig  s          some practical guidance for the implementation of
propensity score matching  journal of economic surveys 
lalonde  r  j          evaluating the econometric evaluations of training programs with
experimental data  the american economic review          
smith  jeffrey a   and petra e  todd  reconciling conflicting evidence on the performance
of propensity score matching methods  american economic review                 

 

this data is from       so adjusting for inflation  this corresponds to increasing an increase of       
over a salary of         in      dollars 

 

fi