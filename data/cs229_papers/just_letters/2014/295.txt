yelp recommendation system
jason ting  swaroop indra ramaswamy
institute for computational and mathematical engineering

abstract

businesses range from     as discrete values as a number
of review stars 

we apply principles and techniques of recommendation systems to develop a predictive model of how customers would rate businesses they have not been to 
using yelps dataset  we extract collaborative and content based features to identify customer and restaurant
profiles  we use generalized regression models  ensemble models  collaborative filtering and factorization machines  we evaluate the performance of the models using the root mean squared error  rmse  metric and
compare the models performances  for dealing with
cold start problems  we use segmentation ensembles and
  different imputation methods  mean  random values
and predicted values  for filling in missing information 

missing data cold start problem
although all the data is available to download  some
of the test data fields are missing certain features  for
all the reviews in the test data  there are no written
user reviews  so we cannot use text analysis to build a
model even though the written user reviews are available in the training data  the data field that is missing
is the average rating for the user business which  unsurprisingly turns out to be best indicator of the predicted
rating  therefore how we handle missing data is a critical part of our project  a breakdown of the test data
set which illustrates the different types of missing data
can be seen in figure   

introduction
missing data breakdown

percentage

yelp is a web mobile application that publishes crowd      
  
sourced reviews about local businesses and restaurants 
      
the users of yelp engage and interact with the applica  
tion through searching businesses  writing reviews  rating businesses  connecting with other users  and checking in at businesses  through this project we utilized
  
yelps data to make personalized business recommen      
dations for yelp users by predicting the rating that a
      
  
given user would give to a specific business 
we begin by describing the dataset used to predict
ratings  this is followed by a description of the evalhas all
missing businesses missing users
missing both
uation metric and the features used  we then elaborate on the different models used to predict the rating
that a user would assign to a business  later  we list
figure    test data set split
the refinements we applied on our models to improve
the performance  finally  we conclude with our results 
because a significant portion of the test data is missdiscussion and talk briefly about future direction 
ing a field and a prediction for all the test data must
be made to find the error  we use a technique called
simple imputation to address the missing data in the
data
model  simple imputation is the process of replacing
missing data with substituted values and is the stanthe primary dataset comes from the yelp recommen  dard approach for handling missing fields  we use the
dation kaggle competition     this information con  following three values for using simple imputation 
tains actual business  user  and users review data from
 the mean of the training set
the greater phoenix  az metropolitan area as json
files  in total there are        businesses         users 
 random sample from the training set
and         reviews in the training data set and      
 a regression estimate of the missing features using
businesses      check in sets        users  and       
other features
reviews in the test data set  the ratings users give to
 

fijason ting  swaroop indra ramaswamy

evaluation metric

interaction terms  for example  gender terms interacting with categories franchises  ie women are likely
we chose to evaluate our model through the root mean to give lower ratings to sports bars  higher ratings to
squared error  rmse  to measure the accuracy  the chiropractors etc 
equation for the root mean squared error is as follows 
r pn
 
i    pi  ai  
models
rmse  
n

linear regression

n is the total number of review ratings to predict
pi is the predicted rating for review i
ai is the actual rating for review i
we evaluate the rmse on the test data set by making submissions to the kaggle contest  in this case  the
training set rmse is not a good indication of the test
set rmse due to data leakage in the training set  say
we split our training set into a cross validation set and a
training set  for example  if a user has rated two businesses  user review count      and the users average
rating is      since the ratings can only be         or   
it means that one of the reviews is a   star review and
the other is a   star review  now  if the   star review
is present in the training set  and if we come across a
review by the user in the cross validation set  then we
know that it is a   star review 
although we have highlighted the issues with using
cross validation  we use leave one out cross validation
to estimate some of our model parameters because it is
the best estimate we can get given that we dont have
access to the test data set 

linear regression fits a linear model to minimize the
residual sum of squares between the observed responses
in the dataset  this model solves the following optimization problem to find the estimated parameters 
the optimization problem is given by 
 argmin   x  y    

ridge regression
ridge regression uses the same model as linear regression  the ridge regression uses an optimization is similar to the linear regression model to find the features 
except it imposes a penalty on the size of coefficients 
the ridge regression minimizes the following equation 
 argmin   x  y             

 is the shrinking parameter  and it affects the second
term  the shrinkage parameter has the effect of shrinking the coefficients towards   by adding a cost to the l   
features
norm by including the shrinkage parameter the model
raw input features  user id  business id  user aver  decreases the variance and increases the bias  in order
age rating  business average rating  category list    to selecting the parameter   we performed leave one
cool votes for user    funny votes for user    use  out cross validation on a series of  values and selected
ful votes for user  bus open closed  business review  that minimized the cross validation error to refit on
the entire model    
count and user review count 
user and business id were binarized so that the ith
user business is represented by a binary row vector x 
with zeros every where except x i 
user average rating and business average rating
are the strongest indicators of the rating given by a
user to a business    cool votes for user    funny
votes for user    useful votes for user  business review count and user review count are indicative of
the reliability of the user average rating  closed businesses often have lower ratings  which is probably why
they are closed   
derived computed features  category average  franchise average  gender  name score  category average
is a powerful indicator of the rating given by a user 
franchise averages are used to fill in for missing business
averages  gender is also a very good indicator  name
score is computed for each business based on the average rating of words in the training set  some words tend
to have a positive connotation  for example  businesses
that contain the words diamondbacks and yelp have
much higher ratings 

lasso
the lasso is an alternative type of linear regression
model to ridge regression  which uses the same model
as linear regression  the lasso uses an optimization is
similar to the the ridge regression with a different type
of penalty  the lasso minimizes the following equation 
 argmin   x  y            
the equation is similar to the ridge regression except
that the shrinkage parameter is replaced by       in the
lasso penalty  which is taking the l  norm  just like
the ridge regression  the lasso shrinks the coefficients
towards zero  except for in the lasso some of the coefficients estimates can be exactly equal to    just like
selecting the shrinkage parameter in ridge regression 
we performed leave one out cross validation on a series of  values and selected the  that minimized the
cross validation error to refit on the entire model    
 

fijason ting  swaroop indra ramaswamy

elastic net
n
n
n
x
x
x
elastic net is a linear regression model trained with l 
y x 
 
w
 
w
x
 
hvi   vj i xi xj
 
i i
and l  prior as regularizer  this combination allows
i  
i   j i  
for learning a sparse model where few of the weights
are non zero like lasso  while still maintaining the regwhere the model parameters that have to be estiularization properties of ridge  we control the convex mated are
combination of l  and l  using the l  ratio parameter 
wo  r  w  rn   v  rnk
the practical advantage of trading off between lasso
and ridge is it allows elastic net to inherit some of
andhvi   vj i is the the inner product of vi and vj  
ridges stability under rotation  the objective funca
row vi within v describes the ith variable with k
tion to minimize is in this case
factors  k  n 
  is a hyperparameter that defines the
 
 
dimensionality of the factorization 
 argmin   x  y                   
the key point is that instead of using a separate
the tuning parameters were chosen by cross vali  model parameter wi j for each interaction  the interdation on a series of tuning parameter values values action is modeled by factorizing it  this is very useful
and selected the tuning parameters that minimized the in sparse settings 
cross validation error to re fit on the entire model    
for example  alice has rated walmart and rainbow
and assume we want to estimate the interaction between
alice  a  and trader joes  t j  for predicting the tarrandom forest regressors
get y  the rating   obviously  there is no case x in the
random forest is a type of ensemble learning method
training data where both variables xa and xt j are nonthat is based from using decision trees  the goal of decizero and thus a direct estimate would lead to no interacsion trees are to create a model that predicts the value
tion  wa t j       but with the factorized interaction
of a target variable by learning simple decision rules
parameters hva   vt j i we can estimate the interaction
inferred from the data features  random forests are a
even in this case  say  bob and charlie have similar facway of averaging multiple deep decision trees trained on
tor vectors vb and vc because both have similar interdifferent parts of the same training set through bootactions with rainbow  vr   for predicting ratings  say 
strapping and using only a subset of the features to
alice has rated walmart and rainbow differently from
create the trees  where an optimal number of trees can
charlie  alice  va   will have a different factor vector
be found using cross validation  this method reduces
from charlie  vc   because she has different interactions
the variance since each tree uses only a subset of predicwith the factors of walmart vw   and rainbow vr   for
tors at the expense of an increase in the bias and some
predicting ratings  say  bob has given similar ratings
loss of interpretability  but generally greatly boosts the
to trader joes and rainbow  the factor vectors of
performance of the final model 
trader joes are likely to be similar to the one of rainin general random forest and decision trees are apbow because bob has similar interactions for both groproaches to handle models when there is missing data
cery stores for predicting y  in total  this means that
since it can use an alternative predictors where the data
the inner product  i e  the interaction  of the factor vecis missing when splitting the node when constructing
tors of alice va   and trader joes vt j   will be similar
the tree        
to the one of alice va   and rainbow vr    therefore 
factorization machines handle sparse data far more elefactorization machine
gantly than svms where we would not have been able
factorization machine as presented by rendle     is a to predict the interaction because we use a separate
general predictor that is able to estimate reliable pa  term wa t j to model the interaction 
factorization machines can also mimic standard facrameters under very high sparsity  the factorization
torization
models like matrix factorization  mf  and
machine models all nested variable interactions  but
svd  
with
the right set of feature vectors  so just by
uses a factorized parameterization instead of a dense
changing
the
feature
extraction method  we can mimic
parametrization like in svms  the model equation of
different
factorization
models 
factorization machines can be computed in linear time
consider
the
matrix
factorization model  it facand that it depends only on a linear number of patorizes
a
relationship
between
two users u  and busirameters  this allows direct optimization and storage
nesses b  
the
standard
approach
is to binarize the
of model parameters without the need of storing any
users
and
the
businesses
as
described
in the features
training data  e g  support vectors  for prediction  in
section 
contrast to this  non linear svms are usually optimized

in the dual form and computing a prediction  the model
n    u  b  
xj    j   b  j   u 
equation  depends on parts of the training data 
the model equation for a factorization machine of
a fm using this feature vector x is identical to the
degree d     is defined as 
matrix factorization model because xj is non zero for u
 

fijason ting  swaroop indra ramaswamy

and i  so all other interactions and biases drop  there  in case a user has barely visited any businesses  then the
fore  the model equation becomes 
similarity scores computed for the user are unreliable
and therefore do a poor job of estimating the predicted
y x    w    wu   wi   hvu   vi i
rating  similarly  for the case when a business has had
very few reviewers  the similarity scores are unreliable 
the parameters for the factorization machine model
therefore  if the user business matrix is sparse  then
were learned using markov chain monte carlo mcmc 
collaborative filtering is likely to do a very poor job 
with the help of the libfm library     it is also possias we can see from figure    the user business matrix
ble to learn them using adaptive stochastic gradient
for the training set is very sparse and therefore collabodescent or alternating least squares 
rative filtering is not likely to work very well  therefore 
we take a much smaller  denser matrix and apply collaborative filtering to that matrix  this matrix is such
refining our models
that all the users in the matrix have rated at least   
businesses in the matrix and all the businesses in the
collaborative filtering
matrix have been rated by at least    users in the macollaborative filtering is a technique that identifies pat  trix  the results are then merged with the results from
terns of user preferences towards certain items and the other models 
makes targeted recommendations  collaborative filtering uses a sparse matrix holding the rating of users to
businesses and calculates a similarity score between the principal components analysis
users and a similarity score between the businesses  the principal components analysis  pca  reduces the
algorithm for this type of collaborative filtering goes as models complexity and the random noise  pca is linfollows 
ear dimensionality reduction using singular value de   initialize x           x nb                nu   to small ran  composition of the data and keeping only the most sigdom values  x i  denotes the features of business nificant singular vectors to project the data to a lower
dimensional space  essentially  we find the directions
i and  j  denotes the features of user j 
that account for most of the variability in the data 
   minimize the objective function j with gradient these are the first n eigenvectors of the covariance madescent to find the parameters  the objective func  trix  x t x   we use pca to reduce the dimensionality
tion is defined as follows 
of our feature matrix 

   
 nb      
 nu  
j x        x
          
 

 

p
segmentation ensemble
t
 
 j  x i   y  i j   
 i j  r i j   
 
segmentation ensemble is a technique that divides the
pnb pn   i     pnu pn  j   

x
 
data into different partitions and uses a distinct model
i  
k  
j  
k  
k
k
 
 
for each partition and combines the results  the prifor a user with parameter  j  and a businesses with mary idea for this method is that each model would
t
features x i    the predicted rating is  j  x i      
have different weights for each feature since the partition for each dataset vary greatly from each other  thus
improving the overall results  the partition for the data
 
  
                           
 
was based on which portion of the data is missing  for
example  for the part of the test data set where the user
  
average rating is missing  we build a model without the
   
user average rating 
   
   

results

   

the models were trained using the training set of size
        and test set of size         the rmse was calculated by using the models prediction and submitting
it via kaggle  the results are graphed out in figure   
we can see from figure   that the best model is
elastic net with random value imputation  we use
our model refining techniques in conjunction with elastic nets to further improve our predictions  pca and
collaborative filtering were applied on the matrix with
random value imputation  we also tried segmentation
ensembles to handle the missing data and used elastic

   
   
   

figure    a random    x    section of the userbusiness matrix  the overall sparsity is        
in order for collaborative filtering to work efficiently 
we require the user business matrix to be fairly dense 
 

fijason ting  swaroop indra ramaswamy

references

rmse

nets to predict the rating  these results are presented record of doing extremely well in similar settings  kdd
in figure   
cup  netflix dataset   it is possible that factorization
machines did not work as well because of the lack of
regularization parameters in the formulation  if we add
rmse by missing data approach and models
    
regularization parameters to the factorization machines
mean
random values
predicted values
model  then we have to solve a non linear dual problem
    
     
which is often not feasible for large data sets like this 
           
    
          
     
we were able to score in the top     of the kaggle
     
     
     
leaderboard with the best score  so we believe that our
    
     
     
     
overall results are good  further  most of the submis    
     
     
     
sions in the top     include using up to    different
          
     
models to fit different subparts of the dataset  we did
    
not adopt this approach as we believe that a general
    
model is better since it is more likely to be robust and
give good results for other test sets 
    
linear regression ridge regression
lasso
elastic net
random forest
fact  machine

future work

figure    rmse for the different models with different
imputation methods
some improvements that can be made to the performance of the problem are the following 

rmse

    

    

 consider other features in the model such as using
locations derived from business addresses to see if
particular neighborhoods had overall higher ratings
and deriving more information about the user from
his her name 

     

     

     

     

    

    

 figure out why factorization machines performed
so poorly in our setting and improve the model  for
instance  by adding regularization terms  

best model   other techniques

    

 explore alternative ways of handling missing data 
no change collaborative filtering

pca

references

segmentation ensemble

    recsys challenge      
yelp business rating prediction 
https   www kaggle com c 
yelp recsys       accessed              

figure    elastic net with refinements

    montanari a  recommendation systems  ee   b 
inference  estimation and information processing 
     

conclusion and discussion
using random values for simple imputation and the
elastic net model performed the best  combining those
  approaches with segmentation ensemble yielded the
best result 
it is not very surprising that collaborative filtering
does not work very well with this dataset  the number
of users who have rated businesses for them to have a
reliable enough similarity score with other users is very
low  it is surprising that filling in with random values
instead mean or predicted values did better across different models  since random values dont take into account the inherent bias  given that the random forest
handles missing data well  the poor performance is unexpected  the good performance of the lasso method
and the elastic net method suggests there are extraneous features in the model that these models were able
to detect and reduce  factorization machines did not
work as well as expected  given that they have a track

    leo breiman  random forests  machine learning 
     
    daniela witten trevor hastie james  gareth and
robert tibshirani  an introduction to statistical
learning  with applications in r  springer 
    steffen rendle  factorization machines  in proceedings of the   th ieee international conference on
data mining  ieee computer society       
    steffen rendle  factorization machines with libfm 
acm trans  intell  syst  technol                  
may      
    hastie trevor zou  hui  regularization and variable
selection via the elastic net  journal of the royal
statistical society       

 

fi