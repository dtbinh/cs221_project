evergreen classification  exploring new features
shailesh bavadekar
shail stanford edu

introduction

data

the advent of internet and social networking has
made it very easy to create and distribute content 
discovering the most informative and engaging documents has become a needle in the haystack problem
for the consumers  we can divide user information
need in two broad classes 

stumbleupon and kaggle provided a training set containing      web documents  and a test set with hidden labels containing      documents  following is
a summary of features available in this data set 

   newsy  ephemeral  this class contains documents that are time sensitive and ephemeral 
typical examples are news articles  classified
listings  blog posts about current or local events 
typically these documents receive a big  shortlived traffic spike  for example  a sports analyst
may write an article about possible outcome of a
game  which becomes almost instantly obsolete
as the game concludes 

 document structural features  ratios of tags vs
text  image vs text  spelling error counts  url
word counts 

 text content features  full html source of the
pages 

 derived features  compressed document size  as
an approximate indicator of redundancy   document category  number of outgoing links in the
page  linkword score  number of document words
that appear in links  etc 

i also further divided the training set to create cross
   evergreen  these are documents that endure the validation and test sets to measure the performance
test of time  typical examples are 
of various classifiers 
 well written  informative articles that tell
the users how something works 

naive bayes classifiers

 high quality humor or opinion pieces  often these articles may refer to specific news
event at the time of publication  but they
may provide a broader perspective that remains of interest well after the event 

the dataset contained both pre filtered text  without
html markup  as well as the raw html content of
each page  these were converted into bag of words
document models  this process involves many algorithmic and parametric choices such as choice of
 literature  arts and entertainment articles  stemming algorithms  stop words  unigram and bithese can be truly timeless 
gram terms etc  i tried a few variants with successively better naive bayes classification performance 
a robust evergreen content classifier can alleviate this
content discovery problem  this report explores evnb   classifier
ergreen classification using the data set provided by
this classifier was built following features 
kaggle stumbleupon evergreen competition 
 

fievergreen
egg  cream  food  use  chees  into  sugar  thi 
my  add  butter     minut  chocol  cook  make 
your  until  bake  or  cup     recip  you

ephemeral
he  hi  sport  fashion  news  video  said  imag 
s  game  ha  year  new  technolog  world 
who  she  their  team  her  app  show  design 
model  flashvar

table    words most indicative of evergreen and ephemercal classes

error rate

 title  url and body text fields processed using naive bayes  i bucketized these features and calcuporter stemmping  converted to unigram and bi  lated the bucket likelihoods under naive bayes asgram bow models 
sumption  however the resulting classifier showed
negligible improvement in accuracy 
 unigram and bigram idf scores were computed
this is in agreement with the learning curve in fign
 
terms
with
low
idf
scores
 threshas log   df
ure
  which suggests a high bias setting  to further
t
old      were filtered 
confirm this  i also developed an svm classifier with
just the quantitative features and it had a relatively
 laplace smoothing was used to handle unseen
poor performance with approximately    percent acterms 
curacy  this shows that the raw quantitative features
table   shows top    stemmed words that are most provided in the stumbleupon dataset have insuffiindicative of evergreen and ephemeral classes  figure cient variance to predict the classes 
finally  the distribution of tokens in the table   is
  shows the learning curves for this classifier 
also very instructive  many of the tokens  e g  new 
news  world  year etc  that are strongly correlated
   
with the ephemeral class are commonly found in news
training
test
    
reports  on the other hand  the evergreen category
appears to be dominated by terms related to food
   
and recipes  approximately     of the training doc    
uments labelled as evergreen contain one of the terms
   
food  recipe or cook  while only     of ephemeral
documents contain these terms  this clearly seems
    
to be an artifact of the sampling process that created
   
the dataset and deserves further investigation 
    
   

 

    

    

    

    

    

logistic regression classifiers

    

training set size

logistic regression models typically work better than
naive bayes in high bias scenarios  in this section we
will explore the feature engineering and performance
of regularized logistic regression classifiers 

figure    nb   learning curve

nb   classifier
tf idf features

the nb   classifier used only textual features  the
stumbleupon data set also contains quantitative features such as compression ratio  spelling error ratio 
image ratio etc  in order to use these features in

 this classifier employed the most commonly
used tf idf functions  specifically  term frequency was squashed using log tf  so that longer
 

fidocuments do not get an unfair influence 

for example  text legibility  placement of images
and videos  background color  choice of fonts and col unigram and bigram idf scores were computed ors  etc can have a big impact on the overall quality
n
 
as log   df
and utility of a webpage  furthermore  these features
t
should be independent of the textual features in the
 the lexicon contains     million distinct unibag of words models used in nb   and lr   classigrams and bigrams  after stemming  drawn from
fiers  to explore this hypothesis i developed lr    a
a training corpus containing   million tokens 
logistic regression classifier that uses only the visual
features 
lr   classifier
the feature extraction pipeline contains following
components 
due to large number of features and moderately large
number of training documents  i used the liblinear lo the selenium webdriver project provides a way
gistic regression with l  regularization  as the figure
to automate browser interaction  this can be
  shows  this implementation largely fixes the high
used to automate crawling and browser screenbias limitation of the nb   classifier 
shot generation 
 the x org project provides a tool called xvfb  a
virtual framebuffer server that implements the x
window protocol  xvfb emulates a framebuffer
without any physical display hardware  it can
also create framebuffers of arbitrary dimensions
and depth  most computer displays do not have
enough vertical pixels to render a full webpage 
using     x    x   framebuffer i was able to
create full page screenshots for most training and
test web pages 

    
training
test
   

error rate

    

   

    

   

    

 

 

    

    

    

    

    

 these screenshot images were then scaled to a
much more manageable size of    x     to reduce the amount of computation 

    

training set size

figure    lr   learning curve

 the final stage converts the images to greyscale
and generates histogram of oriented gradients
 hog  features  hog features have been successfully used for object recognition tasks 

lr   classifier  visual page appearance features

figure   shows the information represented by the
hog feature  the shape of images and text  the layout of the page can be discerned from the image 
the lr   classifier used just the hog feature vector
to classify the documents  the figure   shows the
learning curve for this classifier  the accuracy is predictably low  however it provides an intuition about
the capacity of this approach to detect evergreen documents 

since logistic regression has more capacity than naive
bayes  it should be possible to add more features and
improve the overall performance of the classifier  one
way to do this is using features that represent the
visual appearance of the page  we hypothesize that it
is possible for human raters to predict the usefulness
of a web page based only on visual appearance of the
page without delving into the textual content 
 

fiinput image

histogram of oriented gradients

    
training
textonly
text   hog

   

error rate

    

   

    

   

    

figure    visualizing hog features
 

 

    

    

    

    

    

    

training set size

   
training
test

    

figure    lr   learning curve

   

 the hog features performed better than the
quantitative features provided with the kaggle stumbleupon dataset  using combination
of tf idf and screenshot hog features seems
promising but more training data will be required to translate that into improved performance 

error rate

    
   
    
   
    
 

 

    

    

    

    

    

    

training set size

future work
figure    lr   learning curve

there is clearly some headroom for improving the
performance on the kaggle   stumbleupon dataset 
following are some ideas that are likely to improve
the results 

lr   classifier  text   hog
the last model in this exploration is a logistic regression model using a combination of textual and
hog features  the figure   shows the learning curve
for lr    it is possible that with more training data
lr   could continue converging further than lr  
but the results are inconclusive 

 i developed a new tf idf scoring library for
this exercise  using the standard tfidfvectorizer
class in python sklearn package yielded slightly
better results  this is clearly worth exploring
further  in general  information retrieval literature suggests many ideas for improving term
scoring such as document length normalization 
using different weights for title  url and body
hits 

conclusions
 the accuracy of all classifiers converged to
around      adding more features and better
sampled training data should improve the performance 

 recent work on vector space representation of
words can be applied to this problem  the bow
models can be mapped into a higher dimensional
 

fispace which encapsulates meaning   synonyms of
the words 
 both the feature generation and training algorithms have many hyperparameters  for example  the histogram of oriented gradients feature
was generated using downscaled images  this
transformation loses information  using systematic cross validation can yield a better fit for
these hyperparameters 
 using ensemble learning methods should also improve the performance of this sytem 
even though this dataset has limitations  the ideas
described in this paper translate well to a broader
class of document classification problems  classifying documents based on visual appearance has many
interesting applications  for example  web design is
often conducted in an ad hoc manner in the industry
due to lack of resources  the methods described in
this report can be easily used to quantify utility and
quality of user interfaces 

references
    stumbleupon evergreen classification challenge 
https   www kaggle com c stumbleupon  
    histograms of oriented gradients for human detection       

 

fi