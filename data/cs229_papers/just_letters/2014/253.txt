predicting usefulness of yelp reviews
ben isaacs  xavier mignot  maxwell siegelman

   introduction

the yelp dataset challenge makes a huge set of user  business  and review data publicly
available for machine learning projects  they wish to find interesting trends and patterns
in all of the data they have accumulated  our goal is to predict how useful a review will
prove to be to users  we can use review upvotes as a metric  this could have immediate
applications  many people rely on yelp to make consumer choices  so predicting the
most helpful reviews to display on a page before they have actually been rated would
have a serious impact on user experience 

   data set pre processing

all of the data is made publicly available online   all of the information is in json
format  which made preprocessing relatively easy  yelp provides a set of 
       businesses  these objects included information about the actual business
being reviewed  each contained features such as the average aggregate star rating  and
the total review count  we also included a flag indicating whether or not the business
was still operating 
        users  each user object provided data on the person posting the review 
including a wide variety of information about the spread of other reviews that the
poster had written previously  we included features such as average number stars 
total number of previous reviews  time since joining yelp  and number of yelp friends 
         reviews  each reviews is matched to a business and user  these became
the actual training examples fed into each algorithm  after particular features were
extracted and combined with the information about the user and business in question 
most importantly reviews were labeled with the total number of useful  cool  and
funny votes  in aggregate  these served as the target variable 
initially  we ran each algorithm with feature vectors containing only metadata  we also
used text extraction on some of the reviews but the results did not seem promising
enough to combine the two  so each training example consisted of 

 review  stars  length of text  review date 
 user  review count  average stars  time yelping  number of friends  compliments   
types  
 business  average stars  review count  open not open 
text features were extracted using scikit learns countvectorizer on training example
review texts  this approach extracts text features from a set of documents and returns
token counts  we feed in all of the training example review texts  not the test data  and
this makes up the vocabulary used  as noted above  text features proved pretty unhelpful
so we decided not to append them to our final vectors 
training examples were further normalized  zero mean  unit variance  using a built in
scikit learn functions  the same scaling values were used on the test data after training 
after preprocessing  one of the major challenges of working with this dataset was the
spread of training examples  many of the reviews         had   upvotes  and the vast
majority         had less than     even on a log scale  the distribution is heavily
skewed 

fi 

 note the last bucket corresponds to      upvotes 

   algorithms

we began by running linear regression to get a feel for the problem  however  it became
apparent that due the distribution of the data predicting upvotes on a continuous scale
through some sort of regression algorithm would be unlikely to produce valuable results 
linear regression gave very little useful information  predicting   in most cases 
so we decided to reframe the problem in terms of a     classification  and then also as a
several bucket prediction problem  we ran gaussian nave bayes  logistic regression 
and svm with a linear svc kernel  all supported by scikit learn   
svm  the kernel function used was the inner product of the feature vectors 
k x i   x j       x i   x j      x i  x j        x i nx j n
to pick a good threshold count for number of upvotes we ran our set of algorithms with
several different values                  bellow is a plot of the testing accuracy at
different thresholds 

 
with this in mind  we picked a threshold of    because we reasoned that a cutoff of   
left us with too few positive training examples 
additionally  we used bootstrapping to counteract the poor distribution of the data set 
with bootstrap sampling we were able to get an idea of how effectively each of these
algorithms could perform  we did this by creating    smaller datasets consisting of equal
numbers of positive and negative training examples randomly pulled from the larger
dataset  random sampling was performed with replacement  i e   the different datasets

fiare not entirely independent  then  each model was trained on all of the datasets  and the
results were averaged across all    subsets  this allowed us to push each model to
accurately predict positive examples rather than negative examples  which seems like a
more interesting problem to solve 

   results  important features

the table bellow describes the results of each of these algorithms while considering a
simple     classification  score is a scikit learn function that represents accuracy for
classification algorithms 
for linear regression this was the r  coefficent of
determination  in this case  bootstrapped means that the dataset used was a randomly
selected subset of all the data containing an equal number of samples above the cutoff
and below the cutoff  note that negative values for the coefficient of determination are
an artifact of the way it is calculated in the implementation we are using and can occur
when a model is tested on data it was not trained on  each of these was trained on
        examples  and tested on       

accuracy results using
metadata features

training
score

score on random
test sample

score on bootstrap
test sample

linear regression

      

      

      

bootstrapped linear
regression

      

      

      

gaussian naive bayes

      

      

      

logistic regression

      

      

      

svm  svc 

     

      

      

bootstrapped svm

      

      

       

bootstrapped logistic
regression

      

      

     

model sensitivity and
specificity

true positive rate
 sensitivity 

true negative rate
 specificity 

logistic regression

    

   

bootstrapped logistic
regression

    

    

svm

   

   

bootstrapped svm

    

    

for the review text vectors  we ran a multinomial bayes classifier again with a cutoff of
   upvotes  over    iterations  the average training and testing errors were 
nb average training score

      

finb average test score

      

attempting to predict discrete bucket ranges was unsurprisingly less effective  we got
the following results 
recall for each upvote bucket

      

       

        

        

   

recall rate

     

     

     

     

     

   discussion conclusions

we started by trying to run linear regression on the data because the number of ratings on
a review is non binary  however  we found that we got very poor results with that
approach  our initial results yielded an r   score of         we therefore decided
discretize our output by using a single cutoff  where data above the cutoff is classified as
positive and data below the cutoff is classified as negative  after testing around  we
settled on a cutoff of    ratings for classification  with this cutoff  we ran gaussian naive
bayes  logistic regression and svm algorithms on the data and achieved might higher
success rates  full results shown above  
a big problem we had to deal with was data skew  as our logarithm histogram in our
data preprocessing section shows  most of the reviews have zero or close to zero ratings 
as noted earlier we started training our models on even data sets  i e   we took samples
from the dataset such that each of our samples was half positively classified and half
negatively classified   bootstrapping  unsurprisingly  significantly improved the accuracy
of our models  because the random samples we took were guaranteed to have an equal
number of positive examples to train on  in particular  our true positive rates increased 
which is a tradeoff that makes more sense in the context of this project since predicting  
often garners much less useful information than correctly predicting which reviews will
be popular  while models that were not trained on bootstrapped samples were superior at
classifying random samples from the data  their low sensitivity caused them to perform
very poorly when the test sets were constructed to contain an equal number of positive
and negative examples 
in terms of our best classifiers  both logistic regression and svm performed well  while
our linear regression outperformed our baseline linear regression marginally  the negative
score on the bootstrapped data indicates that it wasnt making any useful predictions
 mostly negative predictions  as discussed above   however  our best rates significantly
outperformed the baseline  bootstrapped svms achieved a     success rate on random
test data and an     success rate on bootstrapped data  by contrast  our baseline
classifier  nave bayes with a reduced feature set  scored     and     respectively  the
massive increase in accuracy on the bootstrapped data represents a large increase in the
usefulness of the model  since the ability to accurately predict positive examples is very
valuable  a theoretical oracle that already had access to all the data would only
outperform our bootstrapped svm by    on the randomly sampled test set  however 
    true positive and     true negative rates imply that there is still some room for
improvement since an oracle with access to all the data would have perfect sensitivity and
specificity 
the features we identified with the highest absolute weights were linked to how many of
certain types of yelp compliments the user had received  however  the feature that we
identified as likely the most important was the number of yelp friends the user had  even
though this feature often had a lower weight than some of the compliment features  the
difference was fairly marginal  usually about       further reinforcing the idea that the
number of yelp friends is the most important feature is that fact that most users do not

fihave many compliments on their profiles  this means that in the majority of cases  the
number of yelp friends the user has ends up contributing the most to the decision the
algorithm makes 
we were somewhat disappointed that the features with the highest weights were usually
features related to user metadata  as discussed above  we had hoped to make predictions
based primarily off the text in the review  but this approach was simply inferior to using
the metadata as we can see from the relative accuracy rates above      versus      
this result has precedent  in our research for this project we came across a study of
twitter retweets  which also concluded that the most useful factor for determining
retweets was the user who posted 
after reaching a degree of success classifying using our cutoff  we attempted to use the
same learning algorithms  namely logistic regression  to classify the data into buckets 
rather than just predicting whether the review would have more or less than    upvotes 
we attempted to predict what range the review would fall into  as expected  the accuracy
of this method was significantly worse than what we attained with just the cutoff 
however  the results are arguably more interesting since using buckets moves us closer to
making actual predictions  we can see from the recall rates for each bucket that the
algorithm was quite good at identifying examples that were on the high and low ends of
the spectrum  which we hypothesize is because these reviews are more easily
differentiable from the reviews in the middle of the spectrum  some investigation
indicated that this is probably mostly related to number of friends each yelp user had  but
this conclusion is complicated by the bucket sizes used  further investigation could
possibly have helped identify the exact point where the reviews become harder to classify
and yielded insights into how these reviews are differentiated from the average review 
but we had to leave this for future work 

   future

one obvious next step would be to implement this on new reviews as they come in
 unlabeled  to provide a real testing set  it would also be interesting to see if more
complex learning algorithms like neural networks could give better results then out of the
box classifier implementations 
additionally  more work could be done picking bucket ranges  these were arbitrarily
chosen  and while we would always expect classification to outperform a bucket
approach  work can be done to improve these predictions as well  we would also like to
be able to do more investigation into how what makes certain review buckets easier or
harder to predict  and identify cutoff points  in terms of number of upvotes  for where the
reviews become hard or easy to predict 

   references
  
  
  
  

 yelp dataset challenge  dataset challenge  n p   n d  web     dec         http   
www yelp com dataset challenge  
scikit learn  machine learning in python  pedregosa et al   jmlr     pp                 
tauhid zaman  ralf herbrich  jurgen van gael  david stern  predicting information
spreading in twitter  december           http   research srv microsoft com pubs 
       nips   twitter final pdf
jordan segall  alex zamoshchin  predicting reddit post popularity   http   
cs    stanford edu proj     zamoshchinsegall predictingredditpostpopularity pdf 

fi