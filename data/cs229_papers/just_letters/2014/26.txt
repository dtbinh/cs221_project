cross domain text understanding in online social
data
qian lin  shenxiu liu  zhao yang  aditya jami  ashutosh saxena
december         

 

introduction

the text classification is a long standing problem  and fruitful results have been
achieved in various situations  however it is still a tough problem to train a text
classifier with training data and test data from different source  the rationale behind
such classification task is that  on the one hand the data in the target domain  data
source we care about  have only few data labelled  meaning that the training set
is too small to train a reasonable text classifier  while on the other hand  there is
another data source  source domain  which shares the similar feature space with
our test data  but have massive labelled data  thus it is fair to ask whether it is
possible to train a classifier using source domain data while apply such classifier on
the target domain  so is our goal for the cross domain text understanding 
to be more specific  the data we deal with are from amazon  ebay and twitter 
our goal is to train the text classifier on amazon  source domain   whose review
has inherent label indicating the type of goods it is describing  and adopt the model
to classify reviews from ebay and twits  target domain   we present the baseline
models in section    which simply trains classifiers on one data source and test
it on another  in section    we present our first cross domain model  instance
adaptation     which tries to understand the similarity between different data source
and assign larger weight on the data from source domain which resemble the target
domain data  however  such approach does not work well in our situation  we
provide an explanation for the poor performance  in section    we present another
strategy  feature replication     intuitively  this approach expand the feature space
and arrange target domain data and source domain data into different hypersurface
while the intersection of the two hypersurfaces is the original feature space  such
approach provides a good performance  especially in the small training data set
limit  we talk about the future research in section   

 

baseline model

we have  m reviews from amazon    k reviews from ebay and   k twits from
twitter all with manually labeled amazon categories  we construct the vocabulary
 

fifor amazon  ebay and twitter of size   k   k    k respectively after stemming  we
use tf idf to obtain the feature vector for each review  we use svm with linear
kernel to construct the baseline model  the classifier aims to decides whether the
given review is describing a book or not  the baseline model is simply to train the
model on one data source and to test it on the other 
tab   shows the error rate of the baseline model  in our report  all error rates
are test error and is defined as the average of false positive rate and false negative
rate  in tab    each row represents the one training data source and each column
stands for one testing data source  it is obvious that if the training data and testing
data are from the same source  the binary classification performs reasonably  while
the naive cross domain yields a much worse result 
error    
a
t
e

a
t
e
                
           n a
      n a     

table    test error rate of baseline model

 

instance adaptation

our first cross domain algorithm     is the instance adaptation algorithm  intuitively 
such algorithm aims to first decide the similarity between the source domain and the
target domain  and to weight samples in the source domain based such similarity
measure when training 
mathematically  we assume ps  y  x   pt  y  x   where s means source domain
and t means target domain  thus in the generative model  in order to maximize the
joint probability in the target domain pt   x  y   we perform following transformation
pt   x  y    pt  y  x pt   x   ps  y  x ps   x 

pt   x 
pt   x 
  ps   x  y 
ps   x 
ps   x 

   

thus we assign samples in the source domain with proper weight
 x   

pt   x 
p label   target  x 
 
ps   x 
p label   source  x 

   

thus  x  can be obtained by a logistic regression  svm  in the feature space that
decides whether a given sample belongs to the target domain or source domain  an
illustration of such algorithm is given in fig   
tab   shows our results using instance adaptation  we have following observations 
 we find in general the instance adaption do not help much 
 we also notice that when we use the source domain idf for the feature extraction in the target domain  the error rates reduces 

 

fi a  original source data

 b  original target data

 c  weighted source data

 d  target prediction with weighted source

figure    illustration of weighting of source instances to match target density    
the reason for the limited improvement of performance is that  ideally if the
samples of the target domain is a subset of the source domain samples in the feature
space  then by properly weighting the source domain sample  we should see a big
improvement of the classification performance  however in our case  the features
from source domain and target domain do not have much overlap  this conclusion
is drawn from the fact that when we use svm logistic regression  to calculate  x  
the classifier is able to make a clear decision  within    error rate  whether a sample
belongs to the target or the source domain  thus in the feature space  the reviews
from the target and the source domain form clusters with themselves 
for the second observation  it can be understood that if we use the source domain idf for the feature extraction in the target domain  we effectively improve the
accuracy of the assumption ps  y  x   pt  y  x   thus instance adaption provides a
better performance 

error    bl bl idf s  swsvm
a to t      
     
     
a to e      
     
     

 

swsvm idf s  swlr swlr idf s 
     
     
     
     
     
     

fitable    test error rate of instance adaption algorithm  idf s means we use the
source idf for the feature extraction of the target domain  swsvm means we
calculate  x  using svm  swlr means we calculate  x  using logistic regression 

 

feature replication

in this section  we introduce another cross domain algorithm  feature replication     
we first talk about the intuition of this algorithm  geometrically  this approach
expand the feature space and arrange target domain data and source domain data
into different hypersurface while the intersection of the two hypersurfaces is the
original feature space  to be more specific  let x   rf be the original input feature
space  define an augmented input space x   r f   define mapping s   t   x  x  
which are the mappings from the original feature space to augmented feature space
of source domain sample and target domain sample respectively 
s  x    hx  x   i  t  x    hx     xi

   

this approach on the one hand admits the resemblance between the target and
source domain as they all share the first rf dimensions of features  while the
target data and source data manifest their difference in the rest of the dimensions 
then the immediate question is what is the size of source data we should use 
because if the size of the source data is too small  it can not resolve the lack of
training samples problem  while if the size of the source data is too big  they will flood
the target training data and introduce too much noise  to answer such question 
we show the learning curves  test error  of amazon to twitter and amazon to ebay
cross domain classifier with different size of the amazon data 
    
    

feature replicated svm from amazon to twitter
 source      k
 source    k
 source     k
 source      k

    
    

    

    

    

    

    

    

    

    

      
  

   

   

   

   

      
  

   

feature replicated svm from amazon to ebay
 source      k
 source    k
 source     k
 source      k

   

   

   

   

   

figure    learning curves of feature replicated svm from amazon to twitter and
ebay  the x axis is the size of the target domain data  twitter or ebay  involved
in training  the y axis is the test error  different curves represent different size of
source domain data  amazon  included in training  the test error rate beyond    
is not shown in the figure 
in our data  fig    we do not see much difference for the twitter case  but
for the ebay case  the competition between insufficient training data and the large
 

fisource data corpus noise manifests  when the source data size is too small  the test
error goes beyond      while if the source data size is too big  it injects too much
noise when the target data size is big enough  thus affects the performance  to
compromise the competition  we choose the source data size to be   k for further
study 
finally  we want to test whether feature replicate approach performs better than
the others  fig   shows the test error among no cross domain training  standard
cross domain svm and feature replicate svm  no cross domain training means
we only use target domain data to train the classifier  the standard cross domain
svm means we train our classifier using   k amazon samples along with the target
domain data  the feature replicate svm also uses   k amazon data along with
target domain data  but the features are manufactured using above method 
comparison of different approaches  amazon to twitter 
t t
a t standard svm
    
a t fr svm

    

    
    

    

    

    

    

    

    

    

    

      
  

   

   

   

   

      
  

   

comparison of different approaches  amazon to ebay 
e e
a e standard svm
a e fr svm

   

   

   

   

   

figure    learning curves of no cross domain training  standard cross domain svm
and feature replicate svm  no cross domain training means we only use target
domain data to train the classifier  the standard cross domain svm means we
train our classifier using   k amazon samples along with the target domain data 
the feature replicate svm also uses   k amazon data along with target domain
data  but the features are manufactured using fr method  the x axis is the size
of the target domain data  twitter or ebay  involved in training  the y axis is the
test error 
we draw following conclusions from fig   
 from amazon to twitter  the fr svm always performs better than other
methods 
 from amazon to ebay  the fr svm performs always better than the no cross
domain approach  but only outperforms the standard svm when the target
domain data size is large 
in total  fr svm introduces less noise than the standard svm and take advantages
of the similarity between the two domains to reduce the test error rate when the
target domain training samples are insufficient 

 

fi 

conclusion and future research

our cross domain text classification work aims to take the advantage of the similarity
between the source domain and the target domain  so that given insufficient target
domain training samples  we can also achieves a good classification job  we have
mainly tried two cross domain algorithms  the instance adaptation and the feature
replicate  the improvement of ia approach is tiny  for which we present the possible
reasons  while the fr approach provides a reasonably good performance 
in the feature  we will try the following thing  since twitter texts are usually
quite short  its distribution space is well distinguished from the amazon data  linear
separator error is only        extending the twitter vocabulary text by additional
information from twits sharing the same tag  or simply using word net may improve
performance 

references
    hal daume iii  abhishek kumar  and avishek saha  frustratingly easy semisupervised domain adaptation  pages            
    jing jiang and chengxiang zhai  instance weighting for domain adaptation in
nlp  in proceedings of the   th annual meeting of the association of computational linguistics  pages         prague  czech republic  june       acl 
    mingsheng long  jianmin wang  guiguang ding  jiaguang sun  and philip s
yu  transfer joint matching for unsupervised domain adaptation  in computer
vision and pattern recognition  cvpr        ieee conference on  pages     
      ieee       

 

fi