restaurant recommendation system
ashish gandhe
ashigan  stanford edu  microsoft com 
different tables  user  business  review  check in and tips 
the data has       restaurants         users         tips
and         reviews  the reviews span over    years of data 
i hold out the latest   month of the reviews data as
the test data  which contains       reviews from          
to            in addition  i keep n months of data from the
period ending           as training data  where n is a
variable to our learning algorithm  the remaining data from
the older period serves a source for generating historical
features  heres a depiction of how the data is split based on
time period 

abstract
there are many recommendation systems available
for problems like shopping  online video entertainment 
games etc  restaurants   dining is one area where there
is a big opportunity to recommend dining options to users
based on their preferences as well as historical data  yelp
is a very good source of such data with not only restaurant
reviews  but also user level information on their preferred
restaurants  this report describes the work to learn to
predict whether a given yelp user visiting a restaurant will
like it or not  i explore the use of different machine
learning techniques and also engineer features that
perform well on this classification 

everything else

i 

local business review websites such as yelp and
urbanspoon are a very popular destination for a large number
of people for deciding on their eat outs  being able to
recommend local businesses to users is a functionality that
would be a very valuable addition to these sites functionality 
in this paper i aim to build a model that recommends
restaurants to users  the way we will model this is by
predicting whether a user will have a positive or a negative
review for the business  we will restrict to restaurants
segment within the business category as recommendation is a
very good fit in that system  one way this model could be
used in practice is by having an automatic recommend 
yes no message when a user visits a restaurants profile
page  the way the problem is modeled is to be able to predict
yes no for any given restaurant and user 
in this work  we will primarily explore the following
directions     optimization algorithms to predict the desired
label    develop features that would help improve the
accuracy of this model 
there are systems that exist today that recommend users
restaurants  but none of them model the problem in this way
to predict a yes no given a user and a restaurant  to my
knowledge  this is the first solution that attempts to
recommend a yes no given a user and a local business  one
assumption we make in this work is that the reviews data is
not biased by the label  i e  the majority of users are uniformly
writing reviews for restaurants they visit  and not because of
their good or bad experience 

ii 

n months

last   month

introduction
derived historical features

training set

test set

the way the problem is now modeled is to learn from current
data to make predictions in the future 
yelp users give ratings on a   point scale  which are mapped
to a binary yes      no        label  hence  each example in
our training test data is a single review with a binary label 
roughly  about     of labels are yes and     are no which
means that we can achieve a trivial baseline accuracy of    
by predicting everything as yes 

iii 

features

i will first describe the features being used  and the
features that i developed to solve this problem  given that our
input tuple is  user  restaurant  i have features of following
categories 
a  user level features
b  restaurant level features
c  user restaurant features
summary of features
i also classify the features into the following buckets 
  

data collection

the data that we used in this project was obtained from
the yelp dataset challenge      the dataset contains five

 

raw features
i have     raw features from the data itself  comprising
of   user level features and     restaurant level features 
user level features are such as number of days in yelp 
number of fans  number of votes etc  restaurant level
features are such as binary features for attributes
 parking  take out  etc  and categories  cuisine 

fi  

step    in the training and test data  i compute matching
features comparing users preference and the business
categories  e g  the best feature in this category was the
average rating for this user averaged over all categories that
matched the given businesss categories 

derived computed features
as described in the previous section  i hold out majority
of the old reviews data for computation of features  this
old period is prior and not overlapping to the periods
from which i sample training and test data 
i compute    derived features from this holdout
historical period that are described in more detail in this
section  these consist of b  user level features such as
average user historical rating and business level features
such as average business level rating  number of reviews 
also includes c  user category features such as average
rating from the user on that category given the current
restaurants category and d  features from users social
network with friends preferences 

the intuition behind these features is that users personal
preference on certain categories of restaurants should be a
strong signal to whether a user would like a future restaurant 
d  collaborative features
the publicly available dataset also provided each users social
graph  i e  the users friends  using the intuition that a users
friends likings are good representatives of a users likings  i
developed the following feature  given a business and user in
the training test set  average rating for this business from this
users friends in the historical period  as before  i used
suitable variations for default values when the feature was
missing a value 

significant amount of work went into engineering these
features  trying different ways to compute them  a lot of
improvements in the results came from the iterative creation
of new features  ill next go into the details of the features 
and in the next chapter ill summarize the results of adding
these features 

iv 

a  raw features

understanding the features

before delving into training a model  i want to analyze the
features first  i used a simple measure such as f score to
identify the top features in our data  here are the top   
features based on f score on a   month training set 

from the raw data i had five user level features  number of
fans  number of days on yelp and three different vote counts 
there are    business attribute features such as binary
information about ambience  diet and facilities  there are    
binary features about cuisine  style of restaurant  there are
    binary features for the city in which this business is 

index

b  historical user and business aggregated features
the first set of features i implemented were based on intuition
that a business is likely to receive ratings correspond to their
historical ratings 
user level  average historical rating from this user    of
reviews
business level  average historical rating for this business   
of reviews
missing features  since historical data for certain
users business can be missing  i circumvented this by using
some variations of this feature with default values ranging
from min to average to max  using a default seems to have
helped across the board as we give the algorithm a way to treat
missing values differently than just zero 
c  user business category based affinity

feature

fscore

 

business averagebusinessratingwithdefault

      

 

business averagebusinessrating

      

 

userbus averageratingforattrmatchwithdefault

      

 

userbus averageratingforattrmatchwithdefaultw

      

 

user averageuserratingwithdefault

      

 

userbus averageratingforcatmatchwithdefaultw

      

 

userbus averageratingforcatmatchwithdefault

      

 

avgfriendratingonthisbusinessd

      

 

business attributes parking lot

      

  

business categories buffets

      

  

business attributes parking garage

      

  

business attributes caters

      

  

business categories fast food

      

  

business attributes parking street

      

others

in order to improve the accuracy further  i decided to
implement features that model each users personal
preference  these features are computed as follows 
step    compute each users personal preference on each of
the possible business categories and attributes  this is
computed as the average rating a user gave to each of the
business categories in the historical period  one such feature
is avg rating for thaicuisine for this user 

  

historical user and business aggregated features  the top
feature business averagebusinessrating is the computed
average rating from the historic hold out period 
business averagebusinessratingwithdefault
was
normalized to always have a default value even if historical
data is missing for that business  the default value i use is the
average
rating
from
all
reviews  

 

fi  week
  month
  month
  month

user averageuserratingwithdefault is the average rating for

that user with default 
user business category based affinity  the feature
userbus averageratingforattrmatchwithdefault measures the
affinity of a user with a business based on historical ratings on its
categories 
collaborative features  user averageuserratingwithdefault

    
   
   
   
   

modeling the problem

  week

in this section i will describe different algorithms i used
varying parameters such as size of training data  subset of
features and evaluate their performance 
i experimented with a few different algorithms  variations in
training data  as well as features  ill describe the results from
each of them separately  before proceeding ill define the
feature set i used in the results 
feature set    consists only of the raw features defined in
iii a section
feature set    consists of the raw and derived features
defined in iii a and b sections  i e  this includes historical
average ratings per user and business and simple review stats 
feature set    consists of all the raw and derived features
defined in iii a  b  c and d sections  this includes all the
features previously mentioned  including user category
affinity and collaborative features 

  month

  month

testingaccuracy

 x axis corresponds to size of training set

from the learning curve  its clear that some over fitting is
happening with lesser training data  and adding more training
data helps that  however  even with   months worth of
training data i see that the testing accuracy only marginally
improves 
reducing over fitting 
the effect of high over fitting is likely arising from the high
dimensional feature mapping from the rbf kernel  i iterated
on some regularization methods to achieve significantly better
results  optimized gamma and c using a parameter sweep
with cross validation  
table    regularized svm with rbf kernel w  feature set  

i experimented with the following algorithms to train the
rating predictor classifier 
  svm with rbf kernel
  linear svm
  logistic regression

  training
data
  week

training
data size
    

training accuracy

test accuracy

      

      

  month

     

      

      

  month

     

      

      

  month

     

      

      

this clearly has lesser over fitting and better test accuracy 

a    svm with rbf kernel
my first approach was to train an svm classifier using the
radial basis function kernel  since the amount of training data
needed at this point is not clear  i vary the training data size
with reviews period ranking from   week    month    month
to   months  the test data remains unchanged  
i measure accuracy defined as the percentage of reviews
where i predicted a positive or negative review correctly 

a    logistic regression
i applied the same training and test data with feature set  
with logistic regression  and the results were as follows 
table    logistic regression w  feature set  
  training
data
  week

the table below shows the change in performance of the
model with varying training data size with feature set   
table    svm with rbf kernel with feature set  
training accuracy

  month

trainingaccuracy

a  learning algorithms used

training
data size

      
      
      
      

learning curve with svm  rbf kernel

the remaining features on the list are binary features on
business categories and their names are self explanatory 
the top   performing features are all derived features
described in the previous section  with the top feature being
historical average rating of the given restaurant 

  training
data

      
      
      
      

the learning curve looks like follows 

is the average rating for this restaurant from the users friends 

v 

    
     
     
     

test accuracy

 

training
data size
    

training accuracy

test accuracy

      

      

  month

     

      

      

  month

     

      

      

  month

     

      

      

fia    svm with linear kernel
i observed that given the vast feature set we have high
dimensional kernel for svm did not add a lot of value  in fact 
there was over fitting in the high dimensional space until
significant regularization was added  i experimented with
svm with a linear kernel which reduced over fitting and
produced comparable and even slightly better results as
shown in the table below 

features and it consistently performs better than feature set  
for all varying training data  although only marginally better 

c  impact of varying training data size
we see interesting results with varying training data size in
table   
specifically  we see training accuracy go down with increase
in training data size  this indicates a good reduction in
variance in that the over fitting problem is fixed with
increased training data 

table    svm with linear kernel
  training
data
    week

training
data size
    

training accuracy

test accuracy

      

      

  week

    

      

      

  month

     

      

      

  month

     

      

      

  month

     

      

      

to analyze testing accuracy better  lets present a zoomed in
learning curve plot only for testing accuracy on feature set  
with linear svm 
testing accuracy with training data size
     

heres the learning curve for linear svm 

     
     

learning curve with linear svm

     
     
    week

   

  week

  month

  month

  month

  months

test accuracy   feature set  

   
   

we see test accuracy increase with increased training data
until about   month of training data  but it reduces with
significantly increased training data such as   or   months 
we explain this behavior with the following hypotheses  the
way we sample training data size is not random  rather
increase in training data is done through going back more in
time and holding out more old data for training  this also
means that the historical hold out period from which derived
features are computed also gets older with increase in training
set size  i summarize the hypothesis here 
o recent training data is more representative of reviews in
the upcoming period than older training data 
o derived features from recent period are stronger signals
for predicting reviews in the upcoming period 

   

    week

  week

  month

trainingaccuracy

  month

  month

testingaccuracy

as we see  we achieve comparable training and test accuracy
with large enough training data looking at periods of   month
or more  theres little to no over fitting happening at this
stage  and this is the maximum we can learn from the given
set of features and training data 

b  impact of derived feature sets
the following table compares the testing accuracies with
different feature sets with varying training data 

thus it is important in such machine learning algorithms
when using past data to predict the future results to optimize
on varying holding out data for features and training data  we
probably want to experiment with weighing training data
based on its age 

table    testing accuracy with different feature sets with linear
svm 
  training
data
    week

feature set  
 raw only 
      

feature set  

  week

      

      

      

  month

      

      

      

  month

      

      

      

  month
  month

      
 

      
      

      
      

      

feature set  
 all derived 
      

vi 

summary of results

i summarize the results from the previous section as follows 
   we see comparable results from the different algorithms
that were used although linear svm was least susceptible
to over fitting and performed marginally better  we
achieved a testing accuracy of        with linear svm 

the table clearly shows the superior results from using the
derived features  feature set   is the set of all raw and derived

 

fi  

  

  

  

feature set   and using   month of reviews as training
data 
we see a significant improvement from derived features 
specifically from using the following 
a  historical average ratings for the business
b  affinity of user to a specific business category
c  collaborative features
increased training data reduced over fitting  but theres
value in weighing training data based on the age of the
label  recent data is more useful in learning than older
data 
it was important to treat missing feature values
differently than zero by providing variations to the model
to learn from 
at the end  we perform about     in accuracy better than
the trivial baseline of always predicting yes 
vii 

  

problem  at this stage  its not clear as i have not yet
explored all the possible features  but it is a concern to
me 
unclear bias in reviews used for training and evaluation 
one assumption we make is that a users decision to
provide reviews to a restaurant is random  and not biased
by an unusually good or bad experience one has 

future work would involve trying to identify stronger features
beyond what is available in the datasets  as well as investing
in an approach to gather training and evaluation data from
alternate means  such as explicit human judgment systems  

acknowledgment
the libsvm library was quite helpful in this project  in
addition  i would like to thank prof andrew ng for the lecture
on insights when applying machine learning  i also want to
thank the cs    teaching assistants and my peers for fruitful
discussions on the student forum 

future work

i acknowledge that the problem being solved is hard 
specifically because of the following reasons 
   unclear predictability of reviews  any supervised
learning problem aims to learn from the labels  given the
provided features  the underlying assumption i made
here is that the features i have access to are sufficient to
predict a positive negative review  however  one can
imagine that a future review can depend highly on the
experience the user has at the restaurant that is not
captured anywhere in the features  this could cause
correlation between the features and the label to be lesser
than what would be ideal for a supervised learning

references

 

   

yelp dataset  https   www yelp com academic dataset

   

chih chung chang and chih jen lin  libsvm   a library for support
vector machines  acm transactions on intelligent systems and
technology                       software available at
http   www csie ntu edu tw  cjlin libsvm

fi