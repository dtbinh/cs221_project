koskas florence
guyon axel
buratti yoann

cs    
          

semen fertility prediction based on lifestyle factors

 

introduction

although many people still think of infertility as a  womans problem   in about     of infertile couples 
the man is the sole cause or a contributing cause of the inability to conceive  indeed  subfertility affects
one in    men     male infertility has many causes   not only hormonal imbalances and or physical problems
but also psychological and or behavioral problems  the aim of this project is to evaluate the importance
of some lifestyle and environmental factors in evaluating infertility 

 

methodology

   

study population

we use the data collected and shared   in      by the department of biotechnology of university of
alicante       young healthy volunteers among students who were between    and    years old provided
a semen sample for analysis as well as their socio demographic data  environmental factors  health status 
and life habits  students with previous known reproductive alterations were excluded from the statistical
analysis 
   

features and labels available from questionnaires

we have   features available for our     training examples  
   season in which the analysis was performed   winter       spring          summer         fall    
   age at the time of analysis   between    years old and    years old scaled to       
   child diseases  i e  chicken pox  measles  mumps  polio    yes      no    
   accident or serious trauma   yes      no    
   surgical intervention   yes      no    
   high fevers in the last year   less than three months ago       more than three months ago      no    
   frequency of alcohol consumption   several times a day      every day        several times a week       
once a week        hardly ever       or never    
   smoking habit never       occasional     or daily    
   number of hours spent sitting per day between   and    scaled to       
our output is the semen analysis diagnosis   the label is n when semen is normal and o if it is altered 
we match the label as follow for our algorithms  
  semen is normal   n     
  semen is altered   o     
   the national center for biotechnology information  u s  national library of medicine
   ttps    archive ics uci edu ml datasets fertility

 

fi   

roadmap

for this project  our main goals are  
  predict a young mans fertility based on his answers to   personal history and lifestyle questions
  evaluate which of these   features have the greatest impact on the predictions outcome
to achieve these goals  we used different machine learning algorithms and inferred the following insights  
logistic regression and kernelized svm   data is not linearly separable even in the high dimensional
feature space generated by a gaussian or sigmoid kernel 
decision tree algorithm   this algorithm gives us better results  with a relatively simple decision tree 

 
   

logistic regression and kernelized svm
logistic regression

we implement a logistic regression coupled with newtons method  the results show the data is not
linearly separable  although this approach is unsuccessful  we can drive some insights from the parameters
theta found from the regression  comparing the absolute values of the different coefficients of our parameter
vector theta  we can get a sense of which features have the highest impact on the outcome  our observations
confirm our intuitions   for example  we find that age is negatively correlated with fertility  which seems
quite reasonable  we obtain  


n ewton  














      
      
      
      
       
      
      
      
      
      































f eatures  

intercept term
season
age
child disease
accident or trauma
surgical intervention
high fever
alcohol consumption
smoking habit
sitting habit















we bolded the most highly weighted features in the logistic regression  as such  the   most important
features seem to be  in order    age  frequency of alcohol consumption and sitting habit  the training error
is equal to     which shows that we cant separate the data     positive example out of       we also
plotted the different features against each other  we dont notice any particular correlations between the
features 
   

kernelized svm

applying logistic regression taught us the data is not linearly separable in the current feature space  hence 
we choose to run an svm algorithm with l  regularization  the objective is to find a separating hyperplane
in a higher dimensional feature space generated by applying a kernel to the data  we use a polynomial  a
gaussian and a sigmoid kernel 
we find the data is not linearly separable in the high dimensional feature space for any of the kernels used 
however the data seems to show structure that is not captured by the kernelized svm  to potentially
overcome this instance of underfitting and separate the data  we believe we would need additional relevant
features characterizing our training set 

 

fibelow is the result of the svm algorithm using a gaussian kernel  we use a pca method to represent
the data  the x and y axes are the   vectors that maximize the variance of the projections of the data on
a   dimensional space  these   principal components of the data are linear combinations of the features 

figure    red and green data points respectively represent positive and negative outcome examples  the kernelized svm
predicts all training examples to be negative  green background  

 
   

decision tree algorithm
main idea

a decision tree is a non linear model that splits training examples into different bags of data  called leaves 
so that we can make a more accurate prediction in each leaf  to build the tree and get the data clustered in these leaves  we select the feature with the heaviest weight in a linear regression  we split the
data above and below a certain threshold for this feature and we recurse on the newly created subset 
when the tree depth exceeds a specific  carefully chosen threshold  we stop and the lastly created subsets
are our leaves  the proportion of negative or positive data points in each leaf is an estimate of the probability of being negative or positive respectively  for any new test example that happens to belong to the leaf 
using the optimal tree depth is key to the performance of the algorithm  creating a deeper tree will lead
to very few data points per leaf and result in overfitting  while stopping too soon will lead to underfitting 
how do we find the optimal tree depth named the complexity parameter  
if we plot the test error as a function of the tree depth  etest   f  d   we can derive the tree depth d 
that achieves the minimum of the test error  an usual complexity parameter is taken to be one standard
deviation below this minimum  
cp   d   testerror

figure    decision tree algorithm and complexity parameter derivation

 

fi   

fertility decision tree

we randomly divide our data set into two subsets   training set        and test set         using the r
module   rpart  in figure   is the tree we find when training the decision tree algorithm described above on
our training set  

figure    decision tree algorithm result

we obtain   bags of data  each leaf displays the probability of the outcome being positive i e  the man
being infertile  for example  according to our data and the model used   a man older than      years old 
who experienced an accident or trauma in the past  and who sits more than     hours per day has a   
  chance to be infertile  while the significance of these results can be further debated and analyzed  the
analysis suggests at least that fertility is not an intrinsic permanent characteristic but life events like a car
accident can potentially alter male fertility 
we repeat the tree generation process a hundred times  and we always get the same decision features 
although the training set is randomly drawn from     of our total data set 
interestingly  compared to logistic regression  the decision tree algorithm gives us a different hierarchy of
the features   age  traumas  surgery and time spent sitting  however  just like with logistic regression 
this should be taken with caution given the overall results of the models  we can note for example that
our tree tells us that men younger than      years old are more likely to be infertile than men between
     and       which is definitely a surprising result that could be questioned  we then test our model on
the test set  below is the confusion matrix describing the test error  

a confusion matrix is particularly relevant to analyze results since the number of positive examples is
fairly small compared to the total dataset size     out of       we can see that our tree algorithm only
predicts half of the positive examples correctly  while more than     of the negative example are predicted
correctly  we believe our algorithm does poorly at predicting infertility mainly because the initial database
is extremely small and biased 
the next step we are considering to improve this algorithm is to implement bootstrapping on the data set 

 

fi 

further discussion   future work

looking at the results of both svm and decision tree  we can build the following table  

the main challenge with this project was the very small number of data points in the set        as well as
a limited number of features      and we wanted to see how far we could go with such constraints  the bias
of the data set towards normal fertility  as in real life actually  was a hindrance  which could be resolved
with bootstrapping  we could greatly improve our results by collecting more data  but this has its own
challenges  
bootstrapping consists in artificially obtaining a larger data set from the original one  given our data set
of     examples     bags of    data points are created  i e  one third of the size of the original data set  
in each bag of    data points  are included  
  data points       randomly drawn from the set of normal samples in the original data set
  data points       randomly drawn from the set of altered samples in the original data set

on these    bags of data  a decision tree algorithm is then trained  on the test set  each tree gives us a
prediction  these predictions are averaged to obtain the final prediction 
alternatively  as we mentioned in the svm section  we expect that additional features would enable us
to separate the data and would thus greatly improve the value of the data set  in fact  the university of
alicante that published the data also had access to more features to perform their analysis    
from discussions with the class teaching team  dave deriso  project ta  and visitors at the poster session 
it appears that random forest or boosting our tree might be other interesting tree algorithm variant we
could try  finally  the same paper uses cnn  convolution neural networks  as a prediction tool  and using
deep learning can help the accuracy of the prediction 

   predicting seminal quality with artificial intelligence methods  david gil  jose luis girela  joaquin de juan  m  jose gomez torres  magnus
johnsson       

 

fi