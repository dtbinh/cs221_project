cs    project report  fall       deepak zambre gmail com   ajey shah  ajey shah gmail com 

topic based comments exploration for online articles
introduction
with news now being consumed on the internet  not only are consumers exposed to a wide range of
reporting sources but a worldwide peer audience also adds their comments and thoughts about news
pieces  these comments are sometimes very insightful and interesting especially on specialized
communities like hacker news  however  while the article may be well formatted and sectioned into
different features  the comments are always presented as a long list sorted by last posted time or a
voting system  we want to investigate methods and algorithms to expose a new way of navigating
comments  one where comments are clustered and arranged based on topics from the article 
to present a rationale as to why this is a real problem  consider the article where iphone   was
released  it had a gamut of information from the new os features to new hardware features  if im a
photography lover  im probably am first interested in knowing what the community has to say about
the new camera features  our solution will attempt to automatically categorize comments based on
such topics extracted from the article  we believe this will bring greater value to the reader and the
news site 

dataset and nomenclature
for this project we will define the following terms 
story  this refers to the actual original article that is being commented on  for example  the
report on the new iphone as published on techcrunch is a story 
top level comment  this is a comment published on hacker news for a particular story such that it
has no other parent comment 
concept space representation  for every story and comment  we run it through a concept entity
recognition algorithm as service  the output of this process is what we call the concept space of the
data 
the dataset we use is available through https   github com hackernews api  this is a collection of
all user generated discussion and comments  in plain text  for every story on the popular tech news
site hacker news  we only consider the top level comments for our work  the data also has a link to
the actual article on the web that is being discussed  we downloaded     million unique stories which
together had     million comments 
for test data  we manually read     comments across   such stories on hackernews and clustered
them as human end users of the system  this allowed us to capture our users way of mentally
segregating topics in the story as well as comments 

preprocessing
we crawl the original content of the news article and use basic cleansing api to remove the html
cruft in the page  once we have clean story and comment text data  rather than directly operate on
bag of words of the original text  we attempt to operate on the concept space as generated by
www alchemyapi com  alchemy api approximately works on the principle as defined in      it uses
data from wikipedia  freebase and other such open source datasets to create an entity graph  this
graph is used to find important keywords that describe particular entities  alchemy api uses this
knowledge to identity important concepts from free text 

fito reiterate our problem statement  we do not want to invent a new way of finding concepts in a text
document  rather what we propose are applications of clustering and prediction techniques on
concept space representation 

unsupervised learning based clustering approach
random clustering
randomly assign comments to concept clusters from the story  this was a great baseline score to
compare against our more complicated clustering methods 
cosine similarity based clustering
our next method to clustering was a very quick and dirty approach using the cosine similarity
matching  our algorithm is as follows 
for each concept in the original story 
for each comment 
do 
 find the similarity between the comment concept space and
the concept in the original story
 if similarity is greater than    add it to that concept
cluster 
kmeans
since we are dealing with the problem of clustering comments  unsupervised   kmeans is a candidate
algorithm that we could have tried with  we represented each comment as a vector of concept scores
and used kmeans to cluster the comments in this concept vector space 
bag of words  bow  kmeans
we represented a comment as a vector of counts with each count representing the number of times a
particular word appears in the comment  we applied kmeans on this representation of comments 
the results for this approach are shown in graphs below  note   we also tried stemming and
removing stop words from comments however  in this document we are reporting results when we
didnt stem and remove stop words  because this approach was giving us better v measure   the
motivation for using bow is to be able to compare bow approach against concept based approach 
hierarchical clustering
conceptually speaking there is a system of taxonomy associated with our project dataset  we can
think of a story with many comments  each comment has a number of concepts and each concept has
a number of attributes that describe it  based on this theory we wanted to test the performance of
hierarchical agglomerative clustering 

supervised learning
nave bayes
while in the previous sections we experimented with unsupervised learning  we also wanted to
experiment with performance of supervised learning  this was particularly interesting in the real
world scenario where we would need to be able to cluster comments as they come in after we use the
initial set of comments to build out a model 

results
for this problem we define two goals 
   keep conceptually similar comments with each other
   minimize number of concepts in comments to match concepts in original story 
the first metric helps us understand how well we can cluster w r t  human expectation whereas the
second objective ensures that concepts being discussed in the comments that are not related to the
original story are not given too much weightage 

fiwe compare the performance of aforementioned algorithms on   stories below with about       
comments in each story  note that the results reported are for the number of clusters that were
expected by the user as defined in the test set  we believe this is a more realistic comparison than
one purely based on the best v measure since it tells us performance of algorithm as expected by an
end user rather than for purely academic purposes 
scores for story        
training method
random
cosine ranking
kmeans
bow kmeans
hac
naivebayes

scores for story        
training method
random
cosine ranking
kmeans
bow kmeans
hac

number of
clusters
 
 
 
 
 
 

homogeneity

completeness

v measure

     
     
     
     
     
     

     
     
     
     
     
     

     
     
     
     
     
      

number of
clusters
  
  
  
  
  

homogeneity

completeness

v measure

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

finaivebayes

scores for story        
training method
random
cosine ranking
kmeans
bow kmeans
hac
naivebayes

 

 

 

 

number of
clusters
 
 
 
 
 
 

homogeneity

completeness

v measure

      
     
     
     
     

     
     
     
     
     

     
     
     
     
     

fiscores for story        
training method
random
cosine ranking
kmeans
bow kmeans
hac
naivebayes

number of
clusters
 
 
 
 
 

homogeneity

completeness

v measure

     
     
     
     
 

     
     
     
     
 

     
      
     
     
 

fihomogenity   score to determine if each cluster contains only members of a single class
completeness   score to determine if all members of a given class are assigned to the same cluster 
v measure   the v measure is the harmonic mean between homogeneity and completeness 

discussion
cosine  the way weve implemented cosine ranking  we limit ourselves to actually matching concepts
from the story  this is different than some of our later approaches  without having keywords that
describes each concept its highly unlikely for the alchemy algorithm to detect exact concept matches 
hence we expect this algorithm to devolve into a simple binary lookup to the concept title  this
explains the terrible performance of this approach  getting a baseline result was however good for us
to move forward with kmeans based clustering as well as understanding that restricting clustering to
only use story comment relations and ignore comment comment relations is a nave approach 
kmeans and bow kmeans 
both  kmeans on concept space and bow kmeans performed better than any other unsupervised
algorithm that we have used  additionally  kmeans on concept space of comments performed better
than bow kmeans  intuitively  this observation corroborates clustering process used by humans  a
human groups together comments talking similarly about similar entities  in our case  we are able to
group comments talking of similar entities ignoring the sentiment  bow kmeans weighs each word
equally from a comment  whereas inherently in a comment  the keywords associated with main
entity of the comment are more important than others 
hac 
our motivation here was to look for an unsupervised algorithm to beat kmeans score based on a
taxonomy approach  note that while hac beat cosine and random  it was almost always slightly
behind the kmeans based approach  we believe this is due to the fact that the hierarchy of story 

ficoncepts  comments concepts  and concept keywords was not well fleshed out in the data set 
importantly we missed keywords that described particular concepts 
nave bayes 
our gaussian nave bayes model was built using the concepts used from alchemy api  we split our
test set into     for training and     for testing our models performance  note that since we
operate on the concept space  we dont use multinomial approach since frequency of a concept is
always    nave bayes beat other algorithms for story          although promising  we believe that
a more descriptive model for what defines concepts will prove beneficial in this case as well 

conclusion
our results show that its definitely a net win to use kmeans clustering method on the concept space
rather than on the text itself  we can draw this conclusion based on comparisons with the random
method and bag of words based kmeans approach  however  getting a single method to always
perform better is challenging due to nature of the data set and trained algorithms for concept space
 like alchemy api   it was also clear to us that unsupervised methods are not suitable for practical
purposes since they cant possible become real time  as for supervised nave bayes  it will be hard to
get real time human judgments on comments to build a model 

future work 
in the future it would be interesting to train concept space algorithm on specific genres of news and
then use these models to provide a richer concept space for clustering  we would also suggest
experimenting with more supervised means of clustering like decision trees  neural networks 
random forests and online learning methods along with crowdsourced test set generation for initial
comments to help build a model  a stretch goal would be to implement novel online learning
methods as outlines in     

references
    rosenberg and hirschberg       
    story           https   news ycombinator com item id        
    story           https   news ycombinator com item id        
    story           https   news ycombinator com item id        
    story           https   news ycombinator com item id        
    vuvuzelas   active learning for online classification  microsoft research
    python  scikit learn

fi