running head  market reactions to information shocks

 

identifying and predicting market reactions to information shocks
in commodity markets
cs    project report  fall     
eric liu  vedant ahluwalia  deepyaman datta   dongyang zhang







computational and mathematical engineering  stanford university  e mail  ericql stanford edu
computational and mathematical engineering  stanford university  e mail  vahluwal stanford edu
 
computer science  stanford university  e mail  deepyaman datta utexas edu

nvidia  e mail  dongyangz nvidia com

abstract
this project proposes a three stage time series
model to identify the relationship between properties of
news information shocks and patterns in market
reactions to such news  then  given a specific news
update  the model predicts how market players will
subsequently respond  we apply multivariate time
series segmentation and clustering techniques on gold
commodity futures  and then run various multi class
classification algorithms on relevant news articles 

features  this project attempts to generate labels and
features for the news data by first fitting models to the
time series of market data  the entire model training
process has three successive stages  which is depicted in
figure   below 

   introduction
there exists ample evidence to suggest that financial
market players often respond irrationally to news
information    for example  investors habitually
overreact to adverse environmental and social news 
resulting in an immediate negative return and a longterm trend reversal   therefore  exploring how different
categories of news articles affect market behaviours in
commodity prices may result in valuable findings 
    current theory
the concept of leveraging news data to predict
commodity price fluctuations has been extensively
explored    however  most current research exhibit at
least one of the following two properties  the models
tend to simplify the classification process either by
dichotomizing the effect of news articles as only good
or bad    or by pre determining the categories under
which the topics of these articles must fall  
    our approach
instead of arbitrarily selecting the number of labels
and dictating which news topics are relevant as input
                                                                                                                         
 

bondt   thaler         owen         ma  tang    hasan        
lnsilahti        
 
roache   rossi         kilian   vega        
 
maheu   mccurdy         gidofalvi   elkan        
 
fang   peress         schumaker   chen          
 

figure    the three staged model for gold price prediction

in stage    we adopt a time series segmentation
process that decomposes the time series data into time
intervals where  within each interval  the data
demonstrate similar behaviours  the endpoints of these
time intervals are considered as structural breakpoints 
which can hence serve as indicators to when specific
news information shocks have occurred 
in stage    we apply various time series clustering
techniques to categorize a number of distinct market
behaviours following the structural breakpoints  this
process replaces the classical good or bad labelling
approach  and results in a more advanced multi class
labelling strategy 
in stage    we employ the usual text mining methods
to generate relevant features for the news articles 
including sentiment analysis  profile of mood states 
bag of words models  and lda  then  svm and other
supervised learning methods are applied to characterize
rules of association between news articles features and
clusters of market reactions 

firunning head  market reactions to information shocks

   models
in the following sections  both the underlying theory
and relevant metrics of the model applied in each stage
are explained in greater detail 
    stage     time series segmentation
the objective of time series segmentation is to
identify time intervals that dissect the dataset into
homogenous sections  given a multivariate time series
dataset                   suppose we fix the
number of segments desired    and wish to find the
time intervals                       with       
and          s are referred to as structural
breakpoints   we may choose the time intervals that
minimize the following cost function
 

    
   

 
           

            
   

where s   s        is a specific segmentation of the
time series with  segments                denotes
a fitted model based on the data points         and
            is some measure of distance 
the cost function    is a weighted average of
each segments error residuals  with weights
proportional to the length of each segment 
theoretically  by methods of dynamic programming 
one may obtain the solutions   and   to the global
minimum cost  however  due to the impracticality of
this approach with large datasets  we adopt an iterative
top down greedy algorithm that solves the optimal
segmentation s at each iteration  the algorithm is
outlined below and further explained in bank        
initially  we set      and fit only one model across
the entire dataset  then  at each iteration step k  we set
         and intend to select an additional structural
breakpoint      to find the optimal kth breakpoint  we
fit models   on each segment            for every
possible value of      and choose the breakpoint that
gives the most cost reduction  the iterations terminate
either when we reach the pre determined number of
segments          or when the benefit of including an
additional structural breakpoint falls below some
threshold        
    stage    time series clustering
in this stage  we wish to categorize the how market
reacts after each structural point     for each date    
we generate a higher dimensional vector

 

                                      
by appending     the market behaviour from the date of
the structural breakpoint till  days later           then 
we follow a wavelet based multivariate time series
clustering method as proposed by durso         the
main advantage of applying the wavelet based
technique is that the approach does not require
stationarity in the data 
to determine the appropriate number of clusters  we
use two diagnostic measures given by nieto barajas
and contreras cristn         namely  the heterogeneity
measure    and the logarithm of pseudo marginal
likelihood    
suppose the data has been grouped into  clusters 
            computes the aggregate of variability
within each cluster  and is given by
 

 
    

           
   

    
     

 
 

where       is the number of observations in each
cluster     necessarily  if the clustering algorithm is
robust  a model with higher number of clusters will
have a lower  value  hence   should be chosen
such that both  and  have relatively small
magnitudes 
the  measure is the sum of log transforms of
the conditional predictive ordinate    statistics for
each     which is given by
 

  

log     
   
 

  
   

 



  

 

 

   

   

 

 

 

   

 

where the conditional likelihood is given as
    
 

 

 

 

   


       
        

 

 

b   am
 
           
b n
 

   

x      x 

   

   

    

       

in the above equation  the estimated coefficients
arise from an assumed bayesian model with a poissondirichlet process prior  with a general sampling model
that follows the distribution
                            

firunning head  market reactions to information shocks

 

with               which exhibits an autoregressive
behaviour                 and            

entire article as opposed to individual words or stems
within each article 

technical details of the derivations of these posterior
conditional likelihood estimates and other model
specifications are given in nieto barajas        
intuitively   is a monte carlo estimate of the
conditional likelihood  hence the name pseudo
conditional likelihood  for each observation     and we
choose the number of clusters  that maximizes the log
likelihood of the entire dataset 

we use both generative and discriminative
algorithms to uncover the most relevant features for
each cluster label  the models we employed include 
multinomial nave bayes  nearest centroids classifier 
linear svc  with no regularization  l  regularization 
and l  regularization   sgd classifier  knn classifier 
passive aggressive classifier  and ridge classifier 
finally  we select the most appropriate approach by
comparing the f  score and the confusion matrix for
each model 

    stage    text classification
from stage    we obtain a set of dates of structural
breakpoints in the time series data  which we assume
the significant change in market behaviour is due to
some news information shock  in stage    we categorize
the market reactions to these information shocks into a
smaller number of clusters  now  we have made all
necessary preparations for the ultimate task  discovering
rules of associations between news articles and these
market reactions 
first  we assign a label            m to each news
article     if the date of news       is not one of the
structural breakpoints     we assign the label       
otherwise  we assign        where the cluster  
contains the observation     in words  we assign each
news article the label of the market reacted  or did not
react  to the news information shocks  or a lack thereof 
on the day when the article was published 

   data collection
daily prices of gold futures are collected from     
to       including each days high  low  opening  and
closing price  for each day   we capture the market
behaviours by constructing a   dimensional vector 
                 with the following components 
               
     opening price jump  
        
             

     intra day price movement  
     intra day volatility  

           

     

     

and obtain a normalized vector   by dividing each
component    by its sample standard deviation     
below are graphs of daily gold prices and normalized
intra day price movements       from jan          to
mar           

then  we apply to the collection of news article data
two classical sentiment analysis techniquesprofile of
mood states  poms  and lydia sentiment analysis
systems  lsas as outlined by han          
        

                     

   

   

 




  

                              

       

   
  
    
where   is the normalized mood vector  again  we
refer the technical details to hans paper and focus on
the intuitive explanation  both poms and lsas can be
seen as extra features we generate for each news article 
which gives a holistic sense of the significance of the

fig    historical gold prices

fig    normalized intra day
price movements

the news articles we collected include all news
entries from the wall street journal online from     
to       which amounts to         articles in      
days  an average of       articles per day   the total
number of features generated is          which is the
size of the vocabulary list after removing stop words
and filtering with minimum and maximum document
frequency 

firunning head  market reactions to information shocks

   results
we applied time series segmentation on the
normalized market behaviour vectors using the topdown algorithm  choosing the euclidean norm as our
distance function               we calculated
the minimum cost    at each step      and have
reproduced some of the values in table   below 

  
   
   
   
   
   

 
      
      
      
      
      
      

 
           
           
           
           
           
           

table    minimum costs for various number of segments

it is evident that    is negatively correlated with
  because d is also the number of data points for
stage   clustering  we ought to choose some number of
segments that is not too small as to undermine the
accuracy of the clustering in stage    but also not too
large as to label structural breakpoints on days that
exhibited no significant change  having these two goals
in mind  we choose the number of segments        
because the change in minimum cost for each additional
iteration       seems to reach very close to   for
        figure   below shows the historical gold
prices and the segments with        

 

measure and the logarithmic pseudo marginal likelihood
for each value of   table   below illustrates some
selected results 

 
 
 
 
 
  


    
    
    
    
    
    


      
      
       
      
      
      

table    number of clusters and diagnostic measures

as expected  the hm decreases with the number of
clusters  but we observe that after   clusters  each
additional cluster does not reduce variance by a
significant amount  the lpml measure also suggests
the appropriate number of clusters is between   and   
we choose   to be the number of clusters  and hence  
labels  for the text classification problem in stage   
after labelling each news article data with its
corresponding cluster  we ran the text classification
algorithms on the training set  every three observations
from four consecutive data points   then  we verify our
trained model on the test set  every fourth data points 
and compute the f  scores  training time  and test time
for each algorithm  the results are shown in figure  
below 

figure    f  scores  training time  and test times
figure    gold prices with     segments

next  we apply the wavelet based multivariate time
series clustering with the appended vectors              
with each   initiating from the structural breakpoint    
we alter the parameter   which is the number of
desired clusters  and calculate both the heterogeneity
asd

from the results  the linear svc with l 
regularization had the highest f  score  which is       it
is worth noting that this number is considerably high 
because our problem is multi class  with   labels   and
hence correct prediction is much more difficult than a
usual two class classification task 

firunning head  market reactions to information shocks

   conclusion   future research
the main contribution of this three stage model to
the current scholarly discourse is that it provides a more
data driven approach in generating the labels for news
articles  this methodology can be readily applied to
other commodities as well as to stocks and equity
derivatives 
because the clustering process consists only of
unsupervised learning techniques  some clusters may
have no economic explanations  hence resulting in less
significance when regressed against the news article
data  further improvements on the model could
consider incorporating economic theories in a mixture
model approach  also  due to the presence of
heteroskedasticity  exploring a garch framework
may prove beneficial  finally  instead of modeling each
commodity individually  a dpca analysis could be
conducted to fit all commodities simultaneously  and
hence take the effect of covariances between different
commodities into consideration 

references
argiento  r   cremaschi  a  and guglielmi  a          a bayesian
nonparametric mixture model for cluster analysis  technical
report quaderno imati cnr          
bank  z   dobos  l     abonyi  j          dynamic principal
component analysis in multivariate time series
segmentation  conservation  information  evolution towards a
sustainable engineering and economy              
bondt  w  f     thaler  r          does the stock market
overreact   the journal of finance                 
boyd  j  h   hu  j     jagannathan  r          the stock market s
reaction to unemployment news  why bad news is usually good
for stocks  the journal of finance                 
chib  s     greenberg  e          markov chain monte carlo
simulation methods in econometrics  econometric theory 
                
d urso  p     maharaj  e  a          wavelets based clustering of
multivariate time series  fuzzy sets and systems             
fang  l     peress  j          media coverage and the cross section
of stock returns  the journal of finance                   
fung  g  p  c   yu  j  x     lu  h          the predicting power of
textual information on financial markets  ieee intelligent
informatics bulletin             
gidofalvi  g     elkan  c          using news articles to predict
stock price movements  department of computer science and
engineering  university of california  san diego 
han  z          data and text mining of financial markets using
news and social media  unpublished doctoral dissertation  
university of manchester  manchester 

ikenberry  d  l     ramnath  s          underreaction to selfselected news events  the case of stock splits  review of
financial studies                 
kaya  m  y     karsligil  m  e          stock price prediction using
financial news articles  information and financial engineering
 icife         nd ieee international conference          
kilian  l     vega  c          do energy prices respond to us
macroeconomic news  a test of the hypothesis of predetermined
energy prices  review of economics and statistics                
lnsilahti  s          market reactions to environmental  social 
and governance  esg  news  evidence from european markets 
ma  y   tang  a  p     hasan  t          the stock price
overreaction effect  evidence on nasdaq stocks  quarterly
journal of business and economics          
maheu  j  m     mccurdy  t  h          news arrival  jump
dynamics  and volatility components for individual stock
returns  the journal of finance                 
mukhopadhyay  s  and gelfand  a e          dirichlet process
mixed generalized linear models  journal of the american
statistical association             
nieto barajas  l  e     contreras cristn  a          a bayesian
nonparametric approach for time series clustering  bayesian
analysis                
neal  r  m          markov chain sampling methods for dirichlet
process mixture models  journal of computational and
graphical statistics                
owen  s          behavioural finance and the decision to invest in
high tech stocks  university of technology       
roache  s  k     rossi  m          the effects of economic news
on commodity prices  the quarterly review of economics and
finance                 
schumaker  r  p     chen  h          textual analysis of stock
market prediction using breaking financial news  the azfin
text system  acm transactions on information systems   tois  
               
veronesi  p          stock market overreactions to bad news in
good times  a rational expectations equilibrium model  review
of financial studies                  

 

fi