reduced order greenhouse gas flaring estimation
sharad bharadwaj  sumit mitra
energy resources engineering
stanford university
abstract global gas flaring is difficult to sense  a
tremendous source of wasted revenue  and causes
ecological problems  we use satellite sensors to
predict gas flares sizes  we tested regression and
classification algorithms  along with anomaly
detection using k means  and found that linear
regression and   class svm are almost as good as
the full tilt sensor model produced by the national
oceanic atmospheric administration  noaa  
i  introduction

natural gas flaring causes environmental
damage and can be a significant source of lost
revenue for oil producers  in this paper we apply
various machine learning techniques to estimate
global greenhouse gas flaring emissions  we will
discuss the technical aspects of the machine
learning bits further on  but first we give a brief
introduction to gas flaring 

the reasons are  the problem is that natural gas at
the surface is much harder to capture and safely
store than liquid petroleum products are  in some
locations  this natural gas can be safely captured
and brought to market  however  in other areas the
infrastructure necessary to adequately capture and
transport the natural gas does not exist  in these
cases  because of the safety hazard in having
quantities of flammable gas floating around  the
associated natural gas is flared      an example of
a natural gas flare can be seen in figure   
unfortunately little data exist to estimate the
amount of global natural gas that is flared  some
jurisdictions require companies report their flaring
emissions  but the data are often poor quality  not
in the public domain  or are simply not reported 
therefore we aim to investigate the use of national
oceanic and atmospheric administration  noaa 
satellite data to estimate global greenhouse gas
flaring  noaa estimates co  emissions  but this
process requires hundreds of sensors and features 
our goal is to replicate the noaa co  estimation
results using a subset of their features     
ii  data  features  and preprocessing

figure    gas flare in north dakota    

in many oil wells  associated natural gas is
produced alongside the oil  this gas can occur
because of a variety of reasons  the drilled
reservoir could contain both oil and gas  the gas
could exist as a pressurized liquid in the reservoir
and come out of solution while traveling in the
wellbore  or there could be chemical processes that
cause the gas to bubble out of solution  whatever

our data set consists of geotagged sensor
readings from a noaa satellite which performs
infrared imaging of the earth  we have roughly  
months of daily estimates of co emissions  ground
truth  derived from little over hundred different
features 
the features included in the full dataset are
various sensors aboard the satellite  and some
manufactured features such as transmissivity of
sensor readings  to perform our analysis  we
chose to train on a subset of the total features  the
infrared measurements  there are   different
infrared spectral band sensors onboard the
satellite  with each sensor measuring the intensity

fiof light received from a different band of infrared
light  gas flares burn at a significantly hotter
temperature than the background earth  so they
emit light in the near far infrared  precisely those
spectrums which the noaa infrared sensors pick
up      figure   shows an example of one data
point  with the noaa estimate of co  highlighted in
peach and the   sensor readings highlighted in
blue 

sensor readings in our analysis  we feel this is
appropriate also because for our dataset there are
vastly more observations than features  so there
wasnt a pressing need to shrink our feature space
for analysis purposes 
iii  models

the following models were utilized for this
investigation  all training was done on the first third
of the data and testing was done on the last two

figure    example truncated data point

to preprocess the dataset  we performed a first
round of manual cleaning  the ground truth
reported co  values  the co  equivalent of the
burned natural gas  are real numbers representing
kg s flow rates  there are a significant portion of the
data which have physically impossible co  values 
such as     kg s  these impossible co  values
were thrown out  leaving us with a data set
containing roughly         points 
table    pca for sensor readings

thirds 
a  linear regression

we chose linear regression to determine how
accurately we could predict exact co  emissions
values  the   sensor features were used with the
built in matlab linear regression model that
used the moore penrose pseudoinverse
to calculate the regression parameters
     we first ran this algorithm with an intercept term
 x   and noticed a     error in the model  which
seemed unusually high  after further looking at the
type of data that was reported  we realized that
most of our co  readings were centered on    this
meant a more realistic model would include an
intercept term of    we also looked into running
higher order polynomial regression models but
decided that because we didnt know the exact
structure of the data it would be difficult to ensure
we werent overfitting it and thus decided to only
work with linear regression 
b  svm

as a first pass at further shrinking the range of
input data  we ran principal components analysis
 pca  on the set of sensor measurements  we
calculated the percent of variance that each
principal component explained  table   contains
the output of our analysis  it is evident that no
single set of principal components explains the vast
majority of our data  so we decided not to perform
data shrinkage and move forward by using all  

after running linear regression  we wanted to
see if we could reduce the training and test error
significantly through classification  we decided to
implement a multi class svm model that would split
co  emissions into small  medium and large bins 
however  because the small and medium values
were too similar  we were not able to linearly
separate them for any kernel choice  we then
focused on a   class model that separated small

fiand large co  emissions  we used matlabs
svmtest and svmclassify functions  tools that were
based on a soft margin optimization problem 

co  emissions values are anomalies  so we
investigated k means for anomaly detection 

equation  soft margin svm optimization formulation

we separated small and large values based on
a threshold of    kg s  we performed appropriate
transformations to change our training y values into
   and   and we used the default values for the
regularization and error tolerance terms 
there were two main issues that we faced
during this classification  first  threshold values to
separate these bins were not given in literature and
thus we did not have an accurate way to separate
these values  we decided to conduct a sensitivity
analysis on the threshold to handle this problem 
we wanted to understand how the error changed
when we varied the threshold value from    to  
kg s in our test set  second  our data was not
separable using a linear kernel  because we didnt
completely understand the entire structure of the
data set  it was difficult to determine which kernel
would best fit our needs  after testing our
classification with multiple kernels  we thought the
quadratic kernel was the optimal choice because it
performed well and was only one order higher than
linear  reducing potential overfitting  however 
further research should be conducted to determine
the most appropriate kernel for this dataset and
features 
c  k means

despite our initial cleaning of the data  there
were still anomalies present based on the
histogram depicted in figure    notice the vast
majority of the data clumped in the small co 
emissions values  we think some portion of those
points corresponding to the long  sparse tail of high

figure    histogram of reported carbon dioxide

we used k means to detect these potential
anomalies and then reran our line regression to
determine whether or not these anomalies
significantly affected our results  we didnt rerun
svm because the error was already relatively low
and removing a few points wouldnt affect the model
significantly for the classification  six clusters were
chosen to represent our data  since we didnt know
how to determine how many potential anomalies
there were  we ran a sensitivity analysis on
removing clusters that contained     to     
values and then reran our algorithms  in addition 
because k means only finds a local optima  we ran
this    times and chose the clusters with the lowest
cost
function
to
represent our cluster segmentation 
iv  results

figure   illustrates the sensitivity of the svm
test error to changes in the co  threshold  as
expected there is a trend of smaller proportion
misclassified for the test set as the threshold
increases because there are fewer co  equivalent
values that are that large  we used a threshold of
   in our results  which represents the largest
potential misclassification of our svm  to put an
upper bound on the misclassification error with this
algorithm 

fifigure   shows the sensitivity of linear regression
to the number of potential anomalies removed
through k means 

figure    sensitivity of svm error to threshold

table   illustrates sample sizes  training  and
testing errors for each algorithm  the linear
regression models error is halved when the
artificial intercept is removed  svm has the lowest
error at     after running k means and removing
    potential anomalies  linear regression had
same test error  we were unable to run svm after
k means anomaly detection because our computer
ran out of memory  in the future  we propose
running pca to determine the principal
components and using that for svm classification 
table    summary results

model

train
size

train test
test size error error

linear
with intercept       

       

   

   

linear without
intercept
      

       

   

   

  class svm

      

       

  

  

multiclass
svm

      

       

n a

n a

linear
k means

               

   

   

figure    linear regression anomaly removal sensitivity

based on the histogram of co  emissions  we
expected there to be at most         additional
anomaly points  at this range we noticed that the
linear regression error hardly changes which
reveals that these points are not significantly
impacting our results  there is a sharp increase in
the test error when approximately     points are
removed  this indicates that there is a small subset
of our data that significantly affects our results but
are not anomalies  as we increase the number of
points removed  we see that the error stabilizes 
once we have removed all points outside the
largest cluster  removing any more points has little
effect as the volume of training data in the large
cluster is so high  table    
table    size of k means clusters

cluster
 
 
 
 
 
 

points in
cluster
      
    
   
   
   
  

fiv  discussion and conclusion

as we see in table    linear regression without
an intercept term has a lower test error than linear
regression with an intercept term  we suspect this
is due to the physical model underlying our
regression  if a linear model is accurate  the co 
prediction from an input of   across all sensors
should be     i e  the model should predict that
f         this makes sense  if the infrared sensors
are reporting no light in their spectral bands  which
means that there is nothing creating heat to be
picked up by the sensors  which means there
should be no underlying gas flare 
for the svm  we see a strong classifier with
respect to separating between small and large
flares  classified as those with flaring intensities
larger than     kg s  unfortunately svm is not
successful at classifying between small medium
flares as it seems the dataset is simply too dense in
this region and there does not exist a separating
hyperplane  therefore  perhaps regulators can use
svm as a tool to identify the worst polluters in a
given region and then perform more sensitive

analyses  either by drilling deeper into satellite
imagery or using physical sensors on site  to
quantify the emissions more finely 
given more time  the next steps would be to
obtain a dataset of the bakken oil field that has true
co  emissions as reported by oilfield operators 
we would train our algorithms with these values
instead of the satellite estimates and then test to
determine how accurate noaa satellite values are
with respect to ground truth co  emissions
worldwide  next  we would want to introduce a
penalty function to sensor readings that took into
consideration the cloud covering and see its effect
on the overall performance of our algorithms 

   
   

   

   

references
k  cenedo  corbis  national geographic news 
may          
a  o  bisong  effects of natural gas flaring on
climate change in nigeria  spe nigeria atce 
august           
c  elvidge et al  viirs nightfire  satellite
pyrometry at night remote sensing vol      
pp            september      
j c a  barata  m s  hussein  the moorepenrose pseudoinverse  a tutorial review of
the theory  instituto de fisica  universidade de
sao paulo  arxiv org          v 

fi