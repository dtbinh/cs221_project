classifying online user behavior using contextual data
akshay rampuria   anunay kulshrestha   aditya ramakrishnan 
 

stanford university
 rampuria  anunay  aditya     stanford edu

december         
in this paper  we investigate the problem of classifying user behavior using freely available user
generated data like tweets on twitter  reviews on amazon and ebay etc  we follow the primary
   label categorization that amazon uses to bucket their products  by parsing a users tweets 
our algorithm attempts to predict which of the    categories the user is referring to  the learning
model we present is based on a combination of a multinomial naive bayes and logistic regression classifiers  which are augmented with an asymmetric distance metric and other classifiers
based on contextual data sourced from amazon and ebay  with these improvements  the latest
iteration of our learning model attains an f  score higher than     

introduction
despite the great computational power of machines  there a some things like interest based segregation that only
humans can instinctively distinguish  for example  a human can easily tell whether a tweet is about a book or
about a kitchen utensil  however  to write a rule based computer program to solve this task  a programmer must
lay down very precise criteria for this these classifications 
there has been a massive increase in the amount of structured user generated content on the internet in the
form of tweets  reviews on amazon and ebay etc  as opposed to stand alone companies  which leverage their
own hubs of data to run behavioral analytics  we strive to gain insights into online user behavior and interests
based on free and public data  by learning more about a users preferences and interests based on this parsed data
from numerous heterogeneous sources  we can classify his her interests  this problem of classifying online user
behavior is especially interesting and perhaps complex because multiple labels can be assigned to the same tweet 
for example  users may have tweeted about how much they liked reading the lord of the rings trilogy  and then
playing the game on their xbox  one could use this data to predict either sentiment  and given that tweets are at
most     characters  the problem complicates 

data
the relevant data was made available to us by our research advisors  professor ashutosh saxena and aditya jami 
they obtained the data using a web crawler which collected publicly available data and arranged it by user  this
data was then split into three datasets  
   twitter dataset  denoted by s   this data includes peoples public tweets on particular products or services 
the labels are in a nested json format  where the depth of the json represents the degree or specificity of
 

ficlassification  we have about        social conversations with content extracted and masked appropriately
from twitter to comply with their terms of service  we use a recursive algorithm to get the least specific
labelling on a given datapoint im this data  we divide this twitter data  cross validating and using     to
train and     to test 
   amazon dataset  denoted by a   each datapoint in this dataset consists of the review of an amazon product
and the metadata associated with it  each instance is in the form of nested json  where the depth of the
json represents the degree or specificity of classification  along with each review  we have data that helps
us link the same users across different platforms  so that we can make predictions on their behavior  we
have about   million reviews of approximately         users  we use data from this dataset to train our
secondary classifiers and obtain metrics for comparisons 
   ebay dataset  denoted by e   each datapoint in this dataset consists of the review of an ebay product and
the metadata associated with it  each instance is in the form of a similar nested json  where the depth of
the json represents the degree or specificity of classification  we have        ebay reviews and we use it 
like we use our amazon dataser  to train our secondary classifiers and obtain metrics for comparisons when
the confidence of our bayesian prediction is on the lower side 
our initial data wrangling involved parsing the entire data set and generating a map such that each key represented the broadest possible label for a datapoint  the value associated with each key is a list containing reviews
or tweets about each of these parent labels  making this structure involved recursively searching each datapoint for
its least specific parent  and associating data points with the same label  for all this parsing  we heavily employed
pythons json and codecs modules  since  many of our datapoints had multiple broad labels  we randomly
choosed one of the possible offerings and assumed that there was only one label for the rest of the problem  the
code is structured in a way to be very easy customizable  that is  we could get any combination of the three datasets
at our disposal by making simple function calls from the different modules 

models
we sanitize every social conversation s  s by first removing all punctuation and then all stop words like at  the  is 
which etc  these commonly used words do not add much information and can be ignored while building the feature
vector  additionally  we use stemming to reduce all words to their word root  for example  the words stemmer 
stemming  stemmed are all reduced to stem  this greatly enhances efficiency by mapping all related words
to the same root  futhermore  we remove all links and usernames because they are not indicative of the class the
tweet should belong to and thus  may confuse the classifier 
after this intial pruning  we use a bag of words model to obtain a term count vector for each social conversation
in s  for this  we build a large vocabulary v of all words that occur in the dataset  the feature vector for s  s
is  v   dimensional and there exists a bijection between features and words in v   more precisely  the value of the
i th feature for the j th document is the importance of the i th word  indexed in v   to the j th document 
the naive way to achieve this uses term counts of individual words in a document  but clearly  this does not
account for commonly used words  to account for commonly used words and gauge how important a word is to a
given s  s  we replace the term counts by the tf idf of each word in the feature vectors 
tf idf  term frequency inverse document frequency  is defined as
tf idf  w  s  s    tf  w  s idf  w  s 

   

where tf  w  s  is simply the frequency of word w in s and
idf  w  s    log

 s 
  s  s   w  s  

 

   

fiwhile we obtain satisfactory results       accuracy  with the naive definition of tf  w  s   in order to improve
the learning model we used smoothing of term frequencies such as
tf  w  s          log

     f  w  s 
max f  w  s    w  s 

   

and
tf  w  s        log f  w  s      

   

where f  w  s  represents the frequency of word w in s  the feature vectors described above are then used to
train multiple classifiers and results of cross validation are depicted below  the smoothing significantly improved
results  in case of multinomial naive bayes and logistic regression  

algorithms
the tf idf vector thus obtained is the feature vector we consider for each s  s  all this computation is done
using the nltk and sklearn python modules  after obtaining the feature vectors  we trained multiple classifiers
on them  as evident from the graph above  multinomial naive bayes and logistic regression produce the best
results 
as a second and more novel approach  we augment the bag of words model described above by employing the
ebay   amazon dataset k    e  a   we prune each e  k by sanitizing it  removing stop words and computing
the tf idf vector as mentioned in the previous section  note that now the vocabulary v contains text from both k
and s  thus  the tf idf vector of each s  s is now different  let s  s and pi be the probability of s belonging to
label yi   in the first model  we simply predict
arg max pi
   
i

as the label for s  but in the second model  we compute d s  e  for a given s and every e  k  then denote
ei   arg max d s  e 
e y e  i

   

where ei is the sample that minimizes d s  e  over all samples that have label i and d is a metric on  v   dimensional
tf idf vectors s and e  the probabilities pi are then augmented using ei and d as
 
p i   p
i d s  ei  

 

   

fiwhere  is a heuristic parameter  we considered the asymmetric metric d defined as
d s  e   

hs  ei
  s   

   

for         this metric further improves our learning model as depicted in the following graph 

the third approach we explored included training another classifier on the contextual ebay and amazon data
sets and combining the posterior probabilities  we considered multiple pairs of classifiers out of which logistic
regression on s and multinomial naive bayes on e  a provided the best results 
for every document s in the training set  we calculate the probabilities py  s  and p y  s  where py comes from
the first classifier trained on dataset s while p y is derived from the second contextual classifier trained on datasets
e and a  here  y represents any given label 
to combine these  for a given s and y  we estimate a heuristic  such that the total probability of s belonging
to y is modelled by
py  s  p y  s  
using        improves the model as depicted in the graph below 

 

   

fichallenges and future work
classifying contextual data has always been a challenge  in this project  we tried to use some context  from ebay
and amazon  to improve our learning model for classification of tweets  this represents a very small foray into a
huge class of computationally and conceptually hard problems  thus  there is immense scope for further work in
this field 
one such direction can be learning the heuristic parameter  introduced in iteration   and   of the learning
model  this will greatly improve the algorithm as hand tuned parameters invariablly lead to overfitting of data as
opposed to learning the same 
another direction we would like to explore is the multi level hierarchical classification problem discussed at the
outset  while we only designed a learning model for the top most layer of the label tree  using similar techniques 
we can descend into different labels  learn the parameters  and eventually design a holistic learning algorithm for
every level of the tree 

results
iteration  
learning model
mnb smoothing
lr smoothing
svc smoothing

precision
              
              
              

recall
              
              
              

accuracy
              
              
              

f  score
              
              
              

iteration  
learning model
mnb metric
lr metric
svc metric

precision
              
              
              

recall
              
              
              

accuracy
              
              
              

f  score
              
              
              

accuracy
              
             
              
              
              
              

f  score
              
              
              
              
              
              

iteration  
learning model
mnb mnb
mnb lr
lr mnb
lr lr
svc lr
svc mnb

precision
              
              
              
              
              
              

recall
              
             
              
              
              
              

clearly  in the third iteration of the learning model  the combination of a logistic regression classifier  on s 
and multinomial naive bayes classifier  on e  a  produces the best results on our data set  in bold above  

acknowledgements
we would like to thank our research mentors  professor ashutosh saxena and aditya jami  it was they who
identified the problem of online user generated text classification  and provided us the requisite data to tackle it 
we met on a weekly basis to discuss our progress and they were extremely supportive in explaining intuitions and
giving suggestions 

 

fireferences
   abdulmutalib  najeeb  and norbert fuhr  language models  smoothing  and idf weighting  information
retrieval        
   rennie  jason d   et al  tackling the poor assumptions of naive bayes text classifiers  icml  vol          
   kibriya  ashraf m   et al  multinomial naive bayes for text categorization revisited  ai       advances in
artificial intelligence  springer berlin heidelberg                
   python libraries   scikit learn  numpy  scipy

 

fi