cs     final project  bias detector
or  using language models to identify editorial political slant
rush moody    rmoody stanford edu
december         
   introduction
text classification techniques have proven
useful in a variety of domains  ranging from
simple but vital tasks like spam filtering to
more complex tasks like determining text
authorship or performing sentiment analysis
on product reviews  in this project i attempt
to apply such techniques to identify the
political slant of editorial articles without
using any prior knowledge of the authors
work or political leanings  this is essentially
a classification problem on editorial articles 
our two classes in this domain are
conservative leaning
and
liberal leaning
editorials  and i use a variety of language
models to aid in their classification  the
resulting classifiers tend to perform well at
classifying data that comes from the same
editorial board domain as our training data 
but does not generalize as well to classifying
the work of several individual columnists in a
separate data set 

   related work
in the domain of text classification
multinomial nave bayes classifiers have
consistently proven themselves to be
remarkably effective despite their simplicity 
especially when combined with data preprocessing steps such as stopword elimination
and word stemming  as well as laplace
smoothing or more sophisticated methods of
handling unobserved tokens  relaxing the
nave bayes assumptions to allow for n gram
markov chains further improves general
performance without introducing much
further complexity  and these classes of
models have achieved
text classification
performance that compares favorably to
state of the art svm based approaches      in
the area of political bias detection  existing
work has seen moderate success in detecting

and classifying biased opinions both in
twitter posts     and at the sentence level
using recursive neural networks      given
that these approaches generally use finergrained class labels and or work on
classifying significantly smaller bodies of
text  applying the aforementioned text
classification models at the level of an
editorial article seemed like a promising and
logical approach 

   data
our data consists of two primary data sets 
the first data set consists of more than      
newspaper editorial board columns published
during the previous four years from four
different newspapers with a known and
consistent editorial bias  with equal numbers
of liberal  the new york times and the
washington post  and conservative  the
wall street journal and the washington
times  articles  we use the known biases of
these institutions as an informal labeling
scheme for this data  and maintain disjoint
subsets of the data for training and testing
our models  although there are likely some
articles in the data set that do not strictly
adhere to our labeling scheme  as we proved
in problem set   we can overcome such noise
by noting that our probability of corruption 
  is likely fairly small and by using large
amounts of training data  there are also
bound to be some included editorial articles
that are not expressly aimed at advocating
for a liberal or conservative agenda  but here
again we rely on the fact that voicing such
an agenda is one of the primary purposes of
an editorial board column and that thus our
assumption applies to the vast majority of
the training examples 
our second data set comes from of the
combined editorial portfolios from the last
three years of two conservative columnists
 david brooks and ross douthat  and two

filiberal columnists  charles m  blow  and
thomas friedman   from which we randomly
sample     articles each  here again we use
the known editorial slant of each columnist
as an informal labeling scheme for choosing
the class label to assign to entries in this
additional test set  our labeling assumption
is slightly more dubious for this data set 
given that many of the articles might be
touching on a non political topic relevant to
the author instead of advancing a liberal or
conservative agenda  but we still use this set
to help determine whether our models
generalize beyond an editorial board setting 

   features and models
for all of our classifiers our input features
are drawn directly from the text of the
articles in our dataset  each of which we
tokenize into a set of distinct words  for the
nave bayes and n gram language models 
the input to our model is a vector of the n
words in the article  x     xn   with each
variable xi taking on values      v   where v
is our vocabulary  for the perceptron model
our input is a vector x  r v   with each
element xi being a binary valued variable
indicating the presence or lack of word vi  v
in the input article 
note that for all of our models we use the
nltk python module to tokenize our input
and remove common stopwords 
naive bayes
our first model is a multinomial naive bayes
classifier that attempts to model p x     xn 
c   which is the joint probability of
generating the given article for the two
classes of political bias  and then selects the
class with the higher probability  using the
nave bayes assumption  this likelihood for
each class of article is given by p c     p xi
  c   we learn the individual probabilities
p xi   c  by calculating the sum    xi   vxi 
for each word over every article and then
normalizing each sum by dividing by the
total number of observed tokens  we also
apply    laplace smoothing and use the

probability    v  as the probability for
unobserved tokens  we treat the prior class
probabilities as a tunable parameter  within
reasonable limits   as we have no good way
of estimating the global prior likelihood of
liberal versus conservative editorials 
n gram language m odel
our second model extends the nave bayes
classifier by loosening the independence
assumptions from that model  instead of
assuming
each
word
is
generated
independently  we now use a k gram markov
chain generation model such that p x    
xn  c  equals p c     p xi   xi     xi k    c  
for k              we learn the parameters
for this model in a similar fashion 
calculating the sum    xi  xi     xi k    
vxi  vxi      vxi k    and then normalizing by
the total number of observed k grams  here
again we apply    laplace smoothing based
on the number of observed trigrams to
handle unobserved tokens 
perceptron
our last model is a perceptron  which takes
as input a vector of binary valued values x 
r v  and makes a prediction using a learned
weight vector w and bias term b  if wtx  
b     we predict one class while if wtx   b
     we predict the other  we use uniform  
initialization and a randomized stochastic
training approach to learn w and b  and use
an n hot bag of words vector generated from
the content of an article as our input vector 
our bias term is implicitly included in our
model by adding an entry to x that is set to
  for every article and thus is trained for
every example in our training set  i
experimented with several convergence
criteria and settled on setting a hard limit of
observing       training examples  our
randomized training approach also yields
significant variations in performance from
run to run 

   results
table   summarizes our results for the three
models on our various data sets 

fimodel

training
accuracy
     
articles 

test
accuracy
 editorial
board
set     
articles 

test
accuracy
 columnist
set     
articles 

     

     

     

n gram
 n   

     

     

     

n gram
 n   

     

     

     

n gram
 n   

     

     

     

perceptron

     

     

     

nave
bayes

table    results

   discussion and analysis
on the whole our results were mixed  when
classifying articles from the same domain as
our training set  nyt wp wsj wt
editorial board columns  we consistently
achieved a fairly high level of classification
accuracy  moving from a simple n b 
classifier to an n gram model also appears to
help accuracy for n      for higher values of n
our language data becomes too sparse and
performance suffers  this makes intuitive
sense  as the number of possible n grams
increases exponentially in n  thus requiring
exponentially more training data in order to
accurately estimate the probability for each
n gram  the optimal value for n seen in    
appears to be   grams  which implies that we
could likely achieve further improvement if
we had sufficient training data to avoid the
issue of sparsity 
our perceptron model appears to offer
similar performance on the first training set
to the language model based classifiers  the
low degree of training error seems to imply
that our training data is at least close to
being linearly separable into two distinct
classes  making the perceptron model a valid
choice for this domain 
while
unfortunate 
it
is
relatively
unsurprising that our models were less adept

at classifying the works of our other selected
liberal and conservative columnists  our
model displayed a consistent tendency to
classify these columns as being liberally
biased  thus resulting in much worse
performance when attempting to classify the
conservative columnists articles from our
second test set  given that each columnist is
likely to have a consistent and unique
pattern of language usage  this shortcoming
in our classifier could not be meaningfully
addressed without obtaining richer sources of
training and testing data  as all of our
training data currently comes from the works
of biased newspaper editorial boards there is
no guarantee that the trends therein will
generalize well to individual columnists with
a liberal or conservative bias  columnists will
often inject far more first person commentary
or focus on specific issues within their area of
interest or expertise  significantly affecting
their word use distributions and thus causing
these columnists work to diverge from the
distributions learned by our models  this
issue could be helped by either including a
broader selection of sources of labeled
training data to yield a more generalized
model for liberal versus conservative
language usage  or by obtaining and
processing non editorial articles from the
same newspapers to help identify editorial
board specific stylistic differences that may
be adversely affecting our models  although
there is no guarantee that such techniques
will be able to resolve the problem entirely  
a broader selection of test data would also
give us a much better idea of the
generalization error of our models than the
four columnists used for our second test set 

   top features
to help analyze the performance of our
systems and gain some insight into how they
classify articles we will now look at the most
common unigram tokens observed by our
nave bayes classifier for each class  as well
as the top weighted words  both negative
and positive  for our perceptron model  we
will then use our pre existing knowledge of
the domain  political and conservative
political discourse and opinion pieces  to
make some comments about the results 

filiberal

conservative

united
people
i
years
mr 
obama
law
said
could
percent

u s 
tax
mr 
year
last
iran
government
nt  not 
president
obama

positive  lib  

need
decade
rights
support
citizens
community
also
published
united
short

negative  cons  

everyone
u s 
administration
nt  not 
better
president
security
today
liberal
china

table    n b  top tokens

table    top perceptron features

naive bayes
table   contains the most common words
observed for our two classes  not including
common stopwords that were removed
during preprocessing  while some muchdiscussed words were likely to occur in either
class of opinion article  which unsurprisingly
includes the tokens mr  and obama  the
differences in the remaining tokens are
somewhat revealing  the conservative tokens
include the words tax  iran  and
government  all of which seem fairly
consistent with the conservative talking
points from the past few years of decrying
the intrusion of government into peoples
lives and of highlighting the dangers posed
by our ideological enemies abroad  the
liberal tokens include words like people and
law  which may reflect the liberal emphasis
on
collectivism 
additionally 
the
conservative tokens include the negation
nt  which does not appear among the top
   tokens for the liberal class  possibly
reflecting a conservative preference for
negative language in the current political
climate 

conservative
editorials
respectively 
interestingly there appears to be significant
overlap between the nave bayes top
features and those learned by our perceptron
model  our earlier insight about the
preference by conservatives for negative
language is validated by the inclusion of the
familiar nt token in the top negative
weighted features  and the presence of
security and china further touches on
the conservative talking points mentioned
previously  on the liberal side  the words
citizens  community  and rights seem
to strongly related to the people and
laws tokens that we observed in the liberal
nave bayes language model 
although it is dangerous to read too closely
into the meaning and content of these top
features  the consistency between both
models as well as our shallow analysis of
their semantics seems to validate our insight
that a language based approach can be used
to differentiate between the two classes 

perceptron

given the limitations in the data at hand we
were still fairly pleased with our overall
results  especially those within the editorial
board domain  our top extracted features
seem to be consistent between models and
seemed to make intuitive sense for the classes
that we are attempting to model  the noisy

table   contains the top    most positive
and negative weighted words from our
perceptron classifier  which are the words
most strongly indicative of liberal and

   conclusions

fiand imperfect sources of data posed serious
challenges when attempting to evaluate our
models ability to generalize beyond the
domain of our training set  however  we
believe that this is an area ripe for further
work  given the large number of published
editorials in existence and the general
effectiveness of language models in text
classification problems  on the whole this
proved to be an interesting attempt at
applying machine learning to a useful
problem and provided valuable insights into
the importance of comprehensive and clean
data in supervised learning problems 

    references

   future work

   m  iyyer  p  enns  j  boyd graber  and p 
resnik          political ideology detection
using recursive neural networks  online  
available 
http   www aclweb org anthology p p   p 
       xhtml

as touched upon previously  the first and
most vital step towards validating and
improving our results would be to obtain
cleaner and more comprehensive sources of
data by either hand selecting a diverse
selection of strongly biased articles or
constructing a broader sourced training
corpus  either approach would give us a
training set that more closely approximates
the true liberal and conservative language
distributions by eliminating noise and
reducing the impact of newspaper specific
stylistic guidelines on language usage  with
an improved training and test set we could
then re run our existing models to see if they
generalize beyond just the editorial domain 
additionally  given the success of neural net
approaches to text classification such as
those outlined in      we would experiment
with modifying our perceptron to be a multilayer neural network to help pick up more
nuanced trends from the training data
without having to rely purely on statistical
word counts or hand engineered features  we
could also experiment with features that
involve the title of the article  or features
such as average word length that explore
other properties of the text besides the actual
words themselves 

   f  peng          augmenting nave bayes
classifiers with statistical language models
 online   available 
http   scholarworks umass edu cgi viewcont
ent cgi article      context csfacultypub
s
   d  maynard and a  funk         
automatic detection of political opinions in
tweets  online   available 
https   gate ac uk sale eswc   opinionmining pdf

   j  nam  j  kim  e  l  mencfa  i 
gurevych  and j  furnkranz          largescale multi label text classification 
revisiting neural networks  online  
available 
http   arxiv org pdf          v  pdf

fi