classifying user behaviors across domains
sunet ids  stevenfu  normanyu  agarg 
names  xiaofei fu  norman yu  abhishek garg
by turning in this assignment  i agree by the stanford honor code and declare that all of this is
my own work 

 

introduction

user profiling is a challenging and important task to online service provides  it can help generate
advertisements that are targeted to users specific interests  all service provides are already doing
some type of user classification but their ability to do so is limited by the data they are from their
own site and they will not use publicly available data from other channels  we are using data from
many channels to improve user classification 
the question that we are trying to answer in this project is  given a review from amazon  ebay or
twitter for a particular product  can we predict the amazon hierarchical location of the product  as
a more concrete example  if we are looking at a review for a music product on amazon  can we train
a classifier to predict whether it will be classified as classical  rock or r b on amazon 
the way we will see how well we did is test the accuracy of our predictions within a dataset and
between datasets 
predicting the exact location in the hierarchy is extremely challenging  so instead our goal will be
to predict the depth   level categorization  e g  classical  rock  r b  in a particular product tree  e g 
music  all of our product reviews from all sources will be in from the same tree  with a model trained
with labeled amazon reviews  we then test its effectiveness on review texts from twitter and ebay 
and try to improve the cross domain prediction results through various means 

 

dataset

our dataset was review texts with category labels that come from   channels  amazon  twitter and
ebay  thus our features will be extracted from the reviews 
our targets will be the category id given to the review  the data from amazon comes with the
category id that is used by amazon  and the reviews from ebay and twitter were hand labeled with
amazon category ids for the training and testing purpose 
as mentioned in the introduction  we decided to rollup product categories to their depth level  
categorizations  in our example  we keep    depth level   categories for music products  and rollup all
category ids to be the top of the tree it falls into 
we applied the same rollup strategy on twitter and ebay data to find the top layer category
this product  hence the review text  belongs to 
in the end of raw data processing  we parsed all of our raw data and generated our train test input
table file with only two columns per row  first column is the full review text  and second row is the
top layer product category id that associated to it  for example 

 

fifile
amazon music dat

twitter music dat
ebay music dat

review text
i have been listening to this all day and never seem to tire
of it  i saw the movie at an early screening a week ago and
this is a great companion to the film experience  my hubby
is waiting until he sees the film so he wont be spoiled on the
exact songs included  as a child of the   s i really love these
songs   not in an ironic sort of way  in fact  im in the midst
of a vinyl mid life crisis so i have already ordered that version
as well  i loved the movie and i love this soundtrack 
brett rossi 
greatest hits of the baroque 
http   t co hpgkodf ih
i bought my calvin klein euphoria perfume because i had
sampled it in a magazine and loved it  euphoria makes me
feel just like its name  apparently it makes other people
feel that way too  i was at a gas station when the man on
the other side of the pump came over to tell me he loved my
perfume  i will certainly continue to buy euphoria  sybil

category id
  

  
      

after brief analysis of the processed data  we found that amazon review text tended to be very
long  and included a handful of features that we could choose from  so we decided to first focused on
training the model primarily on amazon data to test the model on other sources  also we noticed
that most of the review text from twitter is merely the product name and a link the product page
on amazon  this would have a huge impact on our later strategy over twitter data 

 

features

we put a lot of effort into feature engineering for our project 
our group had no prior experience with nlp  we we stuck to bag of words features  we were able
to use term frequency   inverse document frequency  tf idf  to enhance the accuracy of our classifier 
tf idf is a statistic on a word in a document that attempts to reflect how important a word is given
a corpus of documents  it considers the term frequency which is basically the raw frequency of a term
in a document as well as the inverse document frequency  which measures how rare the word is in the
document 
we also applied basic nlp cleaning techniques to enhance the feature set  we found that not
stemming words could potentially lead to overfitting as the algorithm would find the small differences
between the stemming groups and fit on that noise 
the biggest thorn in our project was cross domain classification for twitter tweets  a typical tweet
was extremely terse  was often cutoff mid sentence and usually allocated most of its characters to a
link to a full product review  we applied several methods to try to specifically expand the twitter
data 
first  given our corpus of twitter reviews  we knew that tweets with similar hashtags were usually
referring to similar things  thus  we attempted to expand our tweets by including words from other
tweets with the same hash tag as features  we made sure to distinguish words that were in the original
tweet from ones that we had aggregated from other tweets  we also tested over a small set of hyperparameters for how many expanded words to include as well as a uniform weight to associate with
each of the expanded words  as we expected  the accuracy did not increase monotonically with the
number of words to include from expanded tweets nor did the accuracy increase monotonically with
the uniform weight associated with the expanded words 
 

finext  we also attempted to use data from external sources to expand our twitter tweets  the
external data source that we used was wordnet  which is a lexical database of the english language
that has grouped words into synonym sets  our approach was to take each word in the tweet and
append all of the synonyms of the word to the tweet  the rationale for why this would improve our
twitter training accuracy is that we would hit on more words that were found in the amazon or
ebay reviews 

 

models

we used several different classifiers for our project  as a baseline we used multi class logistic regression
and varied the hyper parameters over the l  and l  penalties 
our next approach was using support vector classification  we attempted to use several different
kernels  linear  polynomial  radial basis function and sigmoid   surprisingly we found that the linear
basis function provided the best results 
lastly  we also explored ensemble methods such as random forest  we found that the hyper
parameter search for this algorithm was very hard to train  very often the model would be severely
overfit 

 

results

to analyze our results  we attempted to understand the marginal impacts of varying learning algorithm
and feature engineering independently  first for learning algorithms  we found that varying learning
algorithms can give a wide range of results for this problem  this is related to the sparsity of our
data   for our amazon dataset we had about   k reviews and about   k tokens  however for particular
amazon  there may only be on the order of     tokens that have non zero value  using linear svm
yielded significantly better results than any other learning algorithm we tried 
aa
aa
a
train
amazon
ebay
twitter

test
aa
a

amazon

ebay

twitter

     
     
     

     
     
     

     
     
     

aa
aa

we were able to achieve twitter specific increases in accuracy by using hashtag expansion  using
hashtag expansion increased the accuracy from     to about      which is     improvement of the
original result  however  we found that this technique is very data set specific  for example  this
technique worked well for the music category on amazon  but it did not work well for other categories 
such as electronics or video games 

 

discussion

first wed like to understand whether or not there was enough training data for our model to do well 

 

fithe first figure shows the training and testing accuracy for amazon for a classifier trained on
amazon  it in unclear how much having more data would have helped as the accuracy continued to
improve as we used more data to train  similar for the twitter accuracy  having more data may also
have helped 
throughout the course of the experimentation  we had trouble understanding why the twitter
training accuracy was so low  to help us better understand what sort of errors our classifier was
making  the following plots show the confusion matrices testing amazon for a classifier trained on
amazon as well and testing amazon for a classifier trained on twitter 
we can see that the classifier was able to predict amazon using amazon data well  on the other
hand  the classifier was not able to predict amazon using twitter data well 
as we know for the testing results  the amazon classifier was able to do a great job and there is
a strong classification rate along the diagonal  however  there is a much different story for our results
when we test on twitter  in the twitter trained on amazon classifier  what we see is that the model is
tends to predict two of the dominant labels  those labels are the categories that are the most frequent
in the amazon data set  what this is telling us is that the model does not pick up on many strong
features in the twitter dataset the classifier relies mostly on its prior knowledge of the distribution on
labels to do its classification 

 

conclusion

the task of classifying reviews across domains is very challenging  we saw a significant drop off in
performance in accuracy when we looked within domains and between domains  this reveals that the
way people write reviews on different platforms is very different  however  we still believe that there
is value to be gained for companies to incorporate information from other sources when attempting to
learn more about their users 

 

fi 

future

our project focused on different ways we could improve classification by expanding text  we used
external datasources  like wordnet  as well as internal data sources  using the tweets with the same
hashtags  to try to increase the accuracy of our classifier and were able to achieve positive incremental
gains  however  there appear to be limitations to this approach and other methods beyond basic ml
nlp should be applied  an interesting next step would be to use latent dirichlet allocation to develop
a topic model for reviews and tweets and use the distribution over the topics as features to try to
classify a review  another approach would to explore more with feature engineering when using tweet
expansion from other sources 

 

fi