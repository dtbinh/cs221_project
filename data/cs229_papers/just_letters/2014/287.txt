automatically generating musical playlists
cs       final project report
paul julius martinez  calvin crew studebaker  from cs     
december         

 

introduction

grouping similar songs together is a problem that has received much attention in the music industry 
algorithms that classify songs are an integral part of products like pandora and apple itunes genius 
creating playlists of songs is a complicated process  since a playlist depends heavily on the musical taste of
the playlist creator  it is not immediately clear  even to a human  what properties unify the songs in a given
playlist  our project attempts to partition a set of songs into the playlists that a human would  based on
simple popularity and audio features of each song 

 

problem definition

the problem as stated above is notably vague  to narrow down the problem space into one of tractable
scope  we present the following simplifications  with justification  an objective error function that we will
use to judge our results 

   

problem simplifications

one issue regarding playlist generation is the question of how many playlists to create  in a carefully curated
music library  a listener may contain dozens of playlists  while certain methods  such as k means clustering 
may be well suited for approaching such a problem  other methods we would like to use will work better if
we assume that we are simply trying to partition a set of songs into two distinct playlists  additionally  for
simplicity  we will assume the two playlists are disjoint  that no song appears in both playlists  
in cases where it may be relevant we may also choose to assume that the playlists we are creating are of
equal size  this is relevant when choosing our error function  explained below  we will also choose to ignore
any order in a playlist 

   

error function

to be more formal  we can state our problem as follows  given a training set t of n sets si of songs  and
correct partitions p i   p i   create a function f from a set of songs to two sets of songs  so as to minimize
the following error function 
 f    

n
  x
l p i   p i   f  si   
n
i  

where l is a loss function defined for a single training example  we want l to compute the difference
between the generated partition and the human labeled correct partition  we can do this by simply
counting the number of songs that are in the wrong playlist  then to normalize we will divide by the total
number of songs  for two given playlists p  and p  and two generated playlists a and b  we can express this

 

fias  p   a     p   b   since we assume that we are partitioning into two equal sized playlists  these terms
will be the same  by symmetry  so we can just take one and divide by the number of songs in single playlist 
more importantly however  since the playlists have no inherent order  we must consider each possible match
up between the generated playlists and the given ones  that is both   p   a  and  p   b   thus our error
function for a single training example is 
l p    p     a  b    

min  p   a    p   b  
 p   

as an example  consider a partition of the numbers   through    with a correct labeled partition of
p        and p         then if we have  a  b                              then  p   a             while
 p   b                   so l p    p     a  b          which can roughly be interpreted as saying  we got
one out of the four songs wrong  important to note though is that since we take the minimum of the two
possible pairings  it is impossible not to do better than     

   

baseline

as a baseline for comparing our results  we would like to know the worst case scenario for our error function 
creating random partitions and testing their error indicated that the expected error of a random partition
function is around       as a lower bound  we could have humans re partition existing playlists  but this
seemed like overly intensive process that we decided was an avenue not worth pursuing 

 

data and features

we used handcrafted playlists that we retrieved from soundcloud using their public api     we collected
   playlists  totalling over     unique songs from which we collected both soundcloud social data and audio
feature data using yaafe  yet another audio feature extractor      with our    playlists  we generated
               pairs of playlists  containing an average of    songs per pair of playlists  these     pairs
were then split into    training examples  two sets of    validation examples  used for parameter tuning and
additional testing  and then a set of    test examples 
we used the following    features 






soundcloud playback count
soundcloud download count
soundcloud favorite count
duration  in seconds 
yaafe provides a feature called loudness  indicating the energy over small time frames       of a
second   using this we generated 







average volume
beginning volume  average of volume over first    seconds 
ending volume  average of volume over last    seconds 
max volume  the   th percentile of volume output 
min volume  the   th percentile of volume output 
volume variance  variance of the loudness feature 

 yaafe also presented a feature called spectral rolloff  described as the frequency so that     of the
energy is contained below  we interpreted this as the maximum pitch at a given point in the song 
and from this we generated 
 high frequency  the   th percentile of spectral rolloff 
 mid frequency  the   th percentile of spectral rolloff 
 low frequency  the   th percentile of spectral rolloff 
 

fi 
   

approaches
k means clustering with weighted norm

one of our first approaches toward solving the problem was using k means clustering to divide the set of
songs into two groups  to try to improve performance over regular k means  we made a few changes  the
traditional k means algorithm works using the a traditional euclidean norm  but we instead used a weighted
norm k  kw   if x  rn   then we define 
v
u n
ux
kxkw   t
wi x i
i  

then  to learn the optimal weight w  we used a hill climbing algorithm that would start with a given weight
vector then explore locally for weights that achieved a lower test error  to help encourage more exploration 
we used a sort of beam search approach which would maintain a number of best options at once 
unfortunately  we were unable to achieve satisfactory results  our best results had      training error
and      test error 

   

classifiers

another approach we explored is training classifiers on our data  but there are a number of subtleties here 
one question is how we use a classifier to then partition our playlist  the idea is that the classifier is useful
not because it outputs an absolute score yes or no  but because it can output a score that then be used
to obtain a relative rank of songs  we can generate a partition by calculating a score for each song in a set
from the classier  then sorting the songs according to the score and then splitting the list in two 
the other issue is how is that using a classifier is a form of supervised learning  but our training examples
have no designated correct playlists or incorrect playlists  we considered the following approach to the
problem  for each pair of playlists in our set of training examples  we can create a number of training
examples for our classifier by taking each song in the playlist then assigning it a score of   if its in one
playlist and a score of    if its in the other  additionally  we give each song from that pair of playlists
a constant feature that indicates it came from that training example  what this achieves is that we are
actually are creating multiple decision boundaries  one for each training example  each decision boundary
uses the same weight vector  but different intercept terms  so that overall each song is being scored along
the same dimension 
the question remains as to which playlists should have a score of   and which should have a score of    
this is easier to visualize in figure   below  in the top left we have our two training examples  where the
os denote one playlist and the xs denote the other  if we were to create separate linear classifiers for each 
we would obtain the two classifiers in the top right corner  now  if we throw all the examples together  with
both sets of os being the positive examples  then we can obtain a classifier very close to that  as seen in
the bottom right corner  both classifiers have the same slope  but the boundary is adjusted so that both
correctly classify the data  a problem could arise if in one playlist the os are the positive examples and in
the other the xs are the positive examples  but then wed be asking our classifier to classify data where
the positives are in the middle and the negatives are on the outside  so the classifier will fail to obtain a low
training error  so  to get around this problem  we try many possible assignments of the training examples
as is feasible  then we pick the one that gives us the lower training error  since with n training examples
there are  n possible assignments which is an impossibly high number to check  we randomly tried      then
picked the best one 
we tried this approach with two classifiers  a simple traditional linear classifier and a support vector
machine with a gaussian kernel 

 

fifigure    the effect of different assignments on classifier performance 
     

linear classifier

with the linear classifier we expressed all of our training examples as matrix a and their scores as a vector
y and tried to find a weight vector x such that ax   y  since this is not possible in general used to the least
squares solution 
to minimize kax  yk  set x    at a   at y
this approach worked fairly well  on our     random assignments  we achieved a random training error
of       but our best training error was       using this classifier  we achieved a test error of      
     

support vector machine

a support vector machine  in the simplest formulation  attempts to draw a decision boundary that maximizes the minimum distance from a training example to the decision boundary  using various mathematical
tricks  the kernel trick  we can achieve non linear decision boundaries  but ultimately our usage of svms
relied on the implementation in the scikit python library     using svms our best classifier achieved     
training error and      test error 
     

classifier analysis

we wanted to validate our intuition regarding the idea that certain assignments of our training examples
led to better or poorer classifiers  we used our validation set of examples for this  and we graphed the
relationship between error on the training set and error on a validation set  expecting a positive correlation 
the results show a tentative relationship between training error and validation error  which is good  but
a stronger correlation would have been more reassuring  there is a however a notable difference between
the performance between the two methods  and between the classifiers and the performance of the weighted
k means  which is good 

   

naive bayes

we alternatively tried to build a naive bayes classifier  but ultimately this approach failed due to how we
structured our training data  if we let p s    s    denote the probability that songs s  and s  are in the same
playlist  then we get compute the probability of a partion p    p  as follows 
y
y
y
p p    p     
p s  t 
p s  t 
    p s  t  
s tp 

s tp 

 

sp   tp 

fitraining set error vs  validation set error

predict error on validation set

   

    

   

    

   
data
best fit line  y                

    

   
    
   
predict error on training set

    

figure    relationship between training error and validation error for linear classifier  left      assignments 
and support vector machine  right      assignments 
we tried to use the training data to learn these probabilities and the compute the the partition of the
playlists that had the highest probability  but we ran into a few issues  the number of possible partitions
grew exponentially  so we had to resort to using a greedy algorithm for finding a high proabability partition 
but the bigger problem was that our training set was small  and  crucially  our testing set used playlists that
also appeared in our training set  so if we were testing on a playlist wed seen before  it would automatically
choose that as the most probable  our naive bayes achieved   test error  but only because it was essentially
getting tested on the training set 
we considered adjusting how our we trained our naive bayes classifier  but ultimately decided that in
order for it to be truly effective  we would need a training set orders of magnitude larger than the one we
already had  given enough resources and time  however  the approach does seem promising 

 

conclusions and future work

each of our approaches performed well on both the training and testing data  with relatively simple features extracted from each song  both weighted k means and classification algorithms were able to learn to
partition songs with impressively low training and testing error  our svm classifier had the best performance  achieving      training error and      test error  in our future research  we want to implement more
advanced audio feature extraction such as average beats per minute  and variation in beats per minute  we
would also like to examine the effects of feature templating  as well as implementing naive bayes with a more
diverse testing dataset  we are very pleased with the results produced from the algorithms implemented in
this project 

references
    soundcloud  https   developers soundcloud com docs api reference
    yaafe  yet another audio feature extractor   http   yaafe sourceforge net index html
    scikit python library  http   scikit learn org stable 

 

fi