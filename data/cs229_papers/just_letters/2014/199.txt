extractive fiction summarization using sentence significance scoring models
stanford university
eric holmdahl   holmdahl stanford edu   cs     and cs    
ashkon farhangi   ashkonf stanford edu   cs    n and cs    
lucio tan   lctan   stanford edu   cs    

abstract    
this paper describes experiments in generating extractive
summaries of fictional short stories using supervised and
unsupervised methods  we first experiment with different
sentence significant scoring models  we describe the use
of textrank and word frequency analysis focusing on
verbs  nouns and adjectives to determine the significance
of individual words within sentences  we then
experiment with sentence clustering using both standard
and modified versions of the k means algorithm to find
sentences to which many others are similar  the final
result of these experiments yielded an ensemble model 
using the outputs of our other sentence significance
scorers as features provided to a gaussian naive bayes
sentence significance classifier  to use the output of the
sentence significance scoring models  we experiment with
multiple sentence extraction algorithms  we first
implement a greedy sentence extraction algorithm  and
then experiment with modeling the sentence extraction
problem as a constraint satisfaction problem  we evaluate
the performance of this system with the rouge
summary evaluation library  which compares systemgenerated summaries to human generate gold summaries
and uses the similarity between them as a proxy for
quality 
asdf  

  

  introduction
the nuance of human language has always made text
summarization a difficult problem  especially with regards to
fiction  modern attempts to tackle this challenging task often focus
on unsupervised methods to identify key topics  but generally focus
on nonfiction texts  some more experimental and cutting edge
systems have begun to utilize machine learning and natural
language understanding methods  but have done so with varying
degrees of success 
in this paper we will describe our approach to the problem of
automated text summarization  the theory behind models we built
to pursue it and our implementation of algorithms necessary to
complete the task  because the task of generating text from scratch
is extremely difficult  we chose to avoid attempting to create
generative summaries and instead focused on extractive
summarization  the primary challenge in extractive summarization
is the identification of the most important sentences within a text 
and we focus our efforts on tackling this challenge 
to limit the length of summaries produced by our system  we
impose a constraint on the extractive summaries produced by our
algorithm  we require that summaries be fewer than     words in
length 

  current state of field
there is no dearth of projects in the field of text summarization 
some more promising works we came across during our research
pointed towards the use of latent dirichlet allocation  lda  and

variations of the pagerank algorithm for generating topic models
for texts  these methods generate lists of key topics and then
extract sentences from original texts that correlate with those
topics  of the several pagerank derivatives that have been
employed  textrank appears to be the best performing and most
commonly used  with lexrank also being quite popular  there has
been very little work done in the field of summarizing fiction  and
almost all of the literature describing implementations of the
aforementioned algorithms have focused on the summarization of
scientific abstracts 

  dataset
we decided to restrict the scope of our project to the summarization
of short stories  in general no longer than    pages  to acquire
training and test data  we scraped a number of public web sites  we
obtained more than     short stories and     associated humangenerated summaries for each story  these stories contain an
average of around     sentences each  resulting in a data set of
roughly       sentences  due to copyright laws  only significantly
old stories are readily available free of charge online  as a result 
our dataset consists almost exclusively of classical stories from the
    s  authors such as edgar allen poe and mark twain appear
repeatedly in our data set 

  sentence significance scoring methods
our system consists of two primary components  because our
system generates extractive summaries  we must first identify
important sentences from within stories to extract  in the following
section we discuss the different sentence significance scoring
models we implemented  afterwards  we will discuss the specific
ways in which we use those significance scores to generate
summaries  we implemented several different supervised and
unsupervised algorithms for sentence significance scoring 
achieving mixed results 

    textrank
textrank is an unsupervised keyword extraction algorithm with
the primary purpose of determining the most relevant keywords in
a text  it models a given text by representing it as a graph  with
unique words as nodes and edges representing word co occurrence 
and scoring the keywords by running the pagerank algorithm on
the aforementioned graph     
textrank is one of the top keyword extraction methods for
documents such as scientific papers by f measure  but has not yet
been extensively tested for the purpose of fiction keyword
extraction 
in constructing and modeling the graph  we have each node
represent each unique word in the text  note that we only care
about words whose significance we want to represent to other
words  so we filter out words that are not nouns and adjectives 
edges are then added by iterating over the filtered text word by
word  adding a directed edge  if there is not already one  from each
word iterated over to the n words that succeed it  where n is our
window size 

fisubsequently  the pagerank algorithm is run on the graph and runs
as follows 

note that sj are the nodes that have an edge pointing to si  and  sj 
is the number of edges out of sj  pagerank was designed to score
the importance of webpages by scoring pages by the importance of
links pointing to a given webpage  and textrank works
analogously for words in a text  a nodes score is dependent on the
scores of the nodes that have edges pointing into it  giving higher
scores to nodes that co occur with nodes that commonly co occur 
after generating the node scores  textrank selects the    nodes
that have the highest scores as potential keywords and then
concatenates any of the potential keywords that have an edge
between them  into one keyword such that it can pick up multiword keywords 
below  we can see the graph generated by our implementation of
textrank on an example story  with the nodes selected as
keywords highlighted in red 

that are especially significant to a story  we look for nouns and
adjectives that appear much more frequently in the story than they
do in the english language as a whole 
however  the importance of a sentence depends on more than just
the nouns and adjectives it contains  rare nouns and adjectives tend
to appear in clumps in stories  certain events described in stories
relate more closely to certain characters  locations or other rare
nouns occurring frequently in the story  consequently  the
significance of a noun or adjective found within a sentence also
depends on the location of the sentence relative to the epicenter of
mentions of that noun or adjective 
we assume that rare noun and adjective occurrences conform to a
truncated gaussian distribution  as mentioned before  mentions of
rare nouns and adjectives in a story tend to be concentrated around
a single point in that story at which they become extremely
relevant  leading up to such a point  mentions increase in
frequency and after such a point  mentions decrease in frequency 
this observation led us to believe the gaussian distribution would
fit the distributions of rare noun and adjective mentions well 
assuming mentions of nouns and adjectives are normally
distributed may be deeply flawed  but anecdotal observations
suggested that a normal distribution would represent word
occurrence data reasonably more often than any other standard
distribution 
we use the truncated gaussian distribution instead of the standard
gaussian distribution because stories contain a fixed number of
sentences  and words cannot appear before the first sentence or
after the last one  imposing these bounds on the gaussian
distribution complicates the distributions probability distribution
function  pdf   the standard gaussian distribution exhibits the
following pdf 

 is the standard deviation of the gaussian distribution and  is its
mean  the truncated gaussian distributions more complicated
pdf appears below 

intuitively  this algorithm is effective since it leverages the idea that
units of text recommend one another  if two words co occur
frequently  then there is probably some relation between them  if a
word has a lot of other words related to it  then it is most likely an
important word in the text and its relations should reflect that 

    noun and adjective significance scoring
the nouns and adjectives a sentence contains can strongly indicate
the significance of that sentence  to capture this intuition  we build
a model that predicts the significance of a sentence based on the
nouns and adjectives it contains  as well as the location of that
sentence relative to the times at which the nouns and adjectives it
contains appear the most in the story it appears in 
      linguistic intuition
some nouns and adjectives tend to be very characteristic of the
stories in which they appear  character names  for instance  are
almost completely unique to the stories to which they belong  as
authors frequently invent new character names for each of their
works  however  not all nouns are characteristic of the stories in
which they appear  common nouns such as person and food
appear in most works of fiction  to identify nouns and adjectives

the pdf of the truncated gaussian distribution is slightly more
complicated and requires two additional parameters   represents
the beginning of the distributions bounds and  represents the end
of its bounds   and  are the truncated gaussian distributions
points of truncation 
      unsupervised learning for word distribution fitting
for each noun and adjective  we fit a truncated gaussian
distribution to the locations of mentions of that lemma  note that
we use the lemmas of nouns and adjectives and not the raw forms
of those words  for each mention of such a lemma  we consider the
index of the sentence in which it appears within the story to be its
location  to estimate the mean and variance of the truncated
gaussian distribution  we use maximum likelihood estimation
 mle   the maximum likelihood estimate of the mean of a
gaussian distribution is the sample mean of the observed points we
believe belong to it  the maximum likelihood estimate of the
variance of a gaussian distribution is the sample variance of the
observed points we believe belong to it  the formulas for
calculating sample mean and variance appear below 

ficontain the exact same verbs in the exact same quantities 
distributions of verbs deviate substantially from one story to the
next  and we take this into account 
to estimate  and   we used a much simpler method  we know
exactly where stories begin and end  and know that words can
appear within the bounds of a story but not outside of them  as
such  set  to    representing the first sentence  and  to one less
than the number of sentences in the story  representing the last
sentence   this process of parameter estimation leaves us with a
distribution representing the locations of mentions of each noun
and adjective lemma appearing in a story 
      noun and adjective significance scoring
to predict the significance of the mention of a noun or adjective
within a story  we consider two factors  its location within the
story  and its commonality in the story relative to its commonality
in the english language  we have already learned the distribution
of each noun and adjective within the story we consider 
consequently  to derive the significance score of the mention of a
noun or adjective we first obtain the pdf value of the mentions
location  the index of the sentence in which it appears   which we
will call the location based relevance of the mention  this closer
the mention is to the epicenter of equivalent mentions  the larger
this pdf value will be  we then multiply this pdf value by what
we call the absolute relevance of the word to the story  its unigram
probability of occurring within the story divided by its unigram
probability of occurring in the english language  to obtain the
unigram probability that the word appears in the english language 
we train a standard language model on the brown university
standard corpus of present day american english  commonly
referred to as the brown corpus   in calculating both unigram
probabilities we use laplace smoothing  an expression for the final
significance of a word w  noun or adjective  mention appears
below 

 is the location of the word in the story      is the pdf of the
truncated gaussian distribution  n is the number of words in the
specified corpus  and n is the number of unique words in the
specified corpus 
      sentence significance prediction
finally  we aggregate the significance scores corresponding to all
the nouns and adjectives contained in a sentence to attain the
cumulative significance score of the sentence according to this
model  we use an extremely simple aggregation method  using the
arithmetic mean of the significance scores of the nouns and
adjectives within a sentence as its significance score  note that this
model completely ignores all words that are not nouns or
adjectives 

    verb significance scoring
the verbs a sentence contains can strongly indicate the significance
of that sentence as well  to capture this intuition  we build a model
that predicts the significance of a sentence based on the verbs it
contains 

while verbs may not be characteristic of stories in which they
appear  verbs can  much more easily than nouns and adjectives  be
classified as significant or unimportant  verbs that describe
common actions  such as speaking or eating  in general describe
minimally important events  however  other verbs such as to
fight or even more so to kill indicate extremely important events
almost every time they are mentioned  of course  one cannot
cleanly label verbs as important or unimportant  every verbs
significance falls along some theoretical continuous scale of
significance 
however  a verbs significance within a story depends on more
than its significance in an absolute sense  some stories include a lot
of very significant verbs  while some do not  every story has a
different average verb significance score  and the significance of a
sentence to a story depends not on the absolute significance of the
verbs it contains but the significance of verbs it contains relative to
the storys average 
      supervised learning for absolute verb significance
scoring
first  we learn absolute verb lemma significance scores in a
supervised manner  note that we use the lemmas of verbs instead
of their raw forms  we base this learning process on the intuition
that a verb that appears much more often in a summary than it does
in a story must have been extremely important to that story  while a
verb that appeared many times in a story but never in its summary
must have been too menial to deserve mention in the summary  we
learn the absolute significance score of a verb by comparing that
verbs commonality in stories to its commonality in summaries  as
observed in our training set of stories and associated gold
summaries  to be precise  we consider a verbs absolute
significance score to be its probability of occurring in a summary
divided by its probability of occurring in a story  in calculating
these probabilities  we use laplace smoothing  the expression for
arbitrary verb vs absolute significance score given training set d
appears below 

n is the number of verb mentions in the training set  across both
stories and summaries  and n is the number of unique verbs found
in the training set  c is the count function  that represents the
number of instances in training in which its input condition is met 
      unsupervised
significance scoring

learning

for

story specific

verb

to generate story specific verb significance score  we compare a
verbs significance to that of other verbs appearing in the story in a
very simple fashion  we consider a verb mentions story specific
significance score to be the percentile at which its absolute
significance score falls in comparison to the other verb mentions in
the story  an expression for this score for verb v  in story d  given
training set d  follows 

      linguistic intuition

      sentence significance prediction

unlike nouns and adjectives  verbs are rarely unique to the stories
in which they appear  people talk  walk  eat  and fight in almost
every story imaginable  there are certainly exceptions to this trend 
but for the most parts it holds  this is not to say that all stories

finally  we aggregate the significance scores corresponding to all
of the verbs contained in a sentence to attain the cumulative
significance score of the sentence  according to this model  we use

fithe same aggregation method used by the noun and adjective
scoring method described previously  using the arithmetic mean of
the significance scores of the verbs within a sentence as its
significance score  note that this model completely ignores all
words that are not verbs 

it represents the sum of the squares of the angles between points
and the centroids of their clusters  instead of minimizing the
squared euclidian distance between tf idf vectors  this loss
function will result in the minimization of the squared angles
between those vectors 

    sentence clustering

the use of this loss function presented a new challenge  to find the
optimal centroid given a cluster of points  we could not simply take
the average of those points because doing so would minimize
reconstruction loss and not angular loss  as a result  we had to
find a different method to minimize angular loss in calculating
centroids as part of the k means algorithm  given the complexity
of the loss function  we decided to use stochastic gradient descent
 sgd  to find the centroid that minimizes the loss with respect to
all the point in its associated cluster  the gradient of angular loss
follows 

stories often contain many similar sentences that express relatively
similar ideas  unlike nonfiction  works of fiction do not strive for
maximum possible conciseness  intuitively  sentences to which
many other sentences in a story are extremely similar tend to
include important ideas  to utilize this intuition  we cluster
sentences 
to model sentences in a form acceptable as input to a clustering
algorithm  we represent sentences by their tf idf vectors  the tfidf metric is a score representing the significance of a word to a
document within a corpus  a words term frequency  tf  is the
number of times it appears within the document  a words inverse
document frequency is the reciprocal of the number of documents
within the greater corpus within which the word appears  an
expression representing the tf idf metric for term t  document d
and corpus d follows 

we treat sentences as documents and the story in which those
sentences reside as the corpus  a sentences tf idf vector is the
vector of the tf idf scores of the words it contains 
      standard k means
because of the sparsity of tf idf vectors  we implement each tfidf vector a dictionary mapping words in the sentence to their
associated tf idf scores  we cluster sentences representative tfidf vectors using the k means algorithm  the algorithm clusters
objects into k clusters  where k is a positive integer provided to it
as a parameter  we chose a value of k equal to the quotient of the
number of sentences in a story and    to obtain an average of  
sentences per cluster  this standard k means algorithm attempts to
minimize the reconstruction loss function  that function follows 

to find the centroid that minimizes angular loss with respect to
the point in its cluster  we run    iterations of sgd  updating the
centroid on each iteration according to the gradient of the angular
loss function with respect to each point in the cluster one at a time 
we use a step size equivalent to  
  instead of
 
randomly initializing centroids at the start of sgd  we initialize
centroids to the average of all of the points within those centroids
associated clusters 
      use of clusters
finally  we define the significance score for each sentence
according to this model as the number of sentences in its cluster 

    svr with estimated training data
      gold sentence significance estimation
 xi  is the tf idf vector corresponding to the ith sentence   is
the vector of centroids and z is the vector of sentence assignments
to clusters 
      k means with angular loss function
k mean clustering using standard reconstruction loss minimizes
the sum of the squared euclidean distances between sentences and
their centroids  this results in clusters of similar sentences  for
which similarity is defined by the euclidean distances between
their tf idf vectors  however  similarity between sentences is
measured most frequently as the angle between the corresponding
tf idf vectors  and not the euclidean distance between them  to
cluster sentences based on the definition of angles between tf idf
vectors as similarity  we altered the standard k mean algorithm  to
account for this  we replaced the standard reconstruction loss
function with a modified loss function  which we will call angular
loss  the definition of angular loss follows 

we do not have explicit training data for the task of important
sentence identification  as we do not have stories with sentences
labeled as important or otherwise  however  we do have a large
data set of stories and associated summaries  based on this data set 
we attempt to estimate sentence importance  for each sentence in a
story  we measure the angle between its tf idf vector and that of
each sentence in the storys summary  we take the inverse of the
lowest angle between the sentence and any of the sentences in the
summary to be the sentences significance score 
      support vector regression  svr 
to train a support vector regressor  svr   we calculate
significance scores as previously described for a training set of
sentences  featurize those sentences and feed them into a standard
svr implementation as training data  to use the trained svr  we
later featurize sentences for which we dont have associated
summaries  i e  those we want to summarize  and use the svrs
significance prediction to score sentences in terms of importance 
of the approaches we tried  this one performed by far the worst  it
was immediately obvious upon testing this regression model that it
would perform terribly  it produced an average squared error of
      while the variance of values it attempted to predict was

fi       such a small difference in variance of underlying values and
squared mean squared error indicates poor predictive power  as a
result  we decided to drop this model from consideration 

    multinomial naive bayes with simple features
      motivation and strategy
as noted  we do not have explicit training data for the task of
important sentence identification  however  we did obtain a
sizeable data set of stories and associated summaries  previously
we discussed using gold summaries to estimate the importance of
sentences in their associated stories  upon the failure of this
approach  we decided to try a different strategy  using sentences in
summaries themselves as examples of important sentences  we
made the assumption that every sentence in a story would be
insignificant  and that every sentence in its associated summary
would be significant  the assumption that every sentence in a story
would be insignificant was of course flawed  in fact the idea of
summarization via extraction relies on its falsehood   but the
average story sentence would certainly be much less significant
than the average summary sentence  we used this assumption to
treat the placement of sentences as a proxy for their importance 
and to train a classifier to predict whether an input sentence belongs
in a story or summary  note that ultimately  we needed to predict
the significance of sentences in stories  we intended to treat the
classifiers confidence that such an input sentence belongs in a
summary as a proxy for that sentences significance  despite the
fact that we would know with complete certainty that every
sentence fed to the classifier in testing would belong in a story 
      supervised learning
we decided to use a multinomial naive bayes classifier to learn to
classify sentences as belonging in a story or summary  and used its
estimated probability of a sentence belonging in a summary as that
sentences significance score according to this model  we featurize
sentences using a relatively simple method that captures some of
their binary characteristics  features we used include boolean
features indicating whether or not a sentence includes a question
mark  whether or not it includes an exclamation point  whether or
not it includes at least one verb  whether or not it includes at least
one proper noun  and a myriad of other similar features  the
intuition behind these features was that sentences in summaries
almost exclusively do not contain question marks of exclamation
points  almost exclusively contain verbs and contain proper nouns
more often than not  while the same cannot be said for sentences
found in stories  as a result  such features should provide rich
insights into the extent to which sentences are prototypical of those
found in summaries  and by extension their likelihood of being
sufficiently significant to be extracted into a summary 
our multinomial naive bayes model uses laplace smoothing in
estimating probabilities  ultimately  this models goal is to estimate
p       the probability that a sentence belongs in a summary
given its feature vector  because we use this probability as a
sentences significance score  the steps leading the to final
expression for p     according to the multinomial nave bayes
appear below  preceded by definitions 

      sentence significance scores
we use the computed values of these probability distributions to
estimate the probability that each sentence in a story belongs in a
summary according to the multinomial naive bayes model  we
use such an estimated probability of a sentence belonging in a
summary as that sentences significance score according to this
model
      examples
to further illustrate the performance of this multinomial naive
bayes model  below is a subset of the binary features we used in
our multinomial naive bayes model  and their associated
conditional probabilities  when p     is larger than p      
the presence of such a feature in a sentence will reduce the naive
bayes models estimated probability that the sentence is in a
summary by less than it does the estimated probability that the
sentence is in a story  refer to the derivations of the multinomial
naive bayes probabilities above   for each feature  the larger
probability is bolded for clarity  notice that most of the features
below have significantly different conditional probabilities for
classes s and t  indicating their strength 

fifeature
sentence contains   nouns
sentence contains at least
one significant verb  as
defined by verbs with
english language unigram
probabilities of less than
       

p    s 

p    t 

      

      

      

      

    

      

sentence contains more
than   verbs

      

      

sentence is question
 contains a question mark 

      

    

sentence contains at least  
proper noun

indicates
sentence in original
story text  probably
unimportant 

sentence in
summary  sentence
is important 
sentence in
summary  sentence
is important 
sentence in
summary  sentence
is important 
sentence in original
story text  probably
unimportant 

in summary  we found the multinomial naive bayes classifier to be
very useful in combining all the sentence significance scoring
methods in the previous modules and giving a total probability
score to feed into the csp in the summary generation stage 
however  we also found that the multinomial naive bayes
classifier does not accept continuously or discretely ordered
features and thus  we decided to extend upon this implementation
with a variant of the classifier known as the gaussian naive bayes
classifier does 

    ensemble model with gaussian naive bayes
each sentence significance model described so far interprets the
importance of a certain set of sentence features well  but ignores
the majority of sentence characteristics from which it could derive
predictive power  to combine the insights provided by each model
into a more well rounded one  we consolidated all of the models
previously discussed into a single ensemble model  our intention
was to use the output of the models discussed previously as features
for a single classifier that could predict the probability that sentence
would be found inside a summary  and by extension the
significance of the sentence  better than any of the models could
individually  to provide the backbone of this stacking methodology
we chose to use a gaussian naive bayes classifier as an extension
of our multinomial naive bayes classifier 
      mathematics of the gaussian naive bayes classifier
the gaussian naive bayes classifier expands upon the principles
underlying the design of the multinomial naive bayes classifier 
but overcomes one of its primary shortcomings  the standard
multinomial naive bayes classifier is incapable of handling
continuous  ordered features  the gaussian naive bayes classifier 
however  is able to do so  the standard gaussian naive bayes
classifier assumes that continuous features conform to a gaussian
distribution  it derives its name from this characteristic  it
estimates a specific conditional gaussian distribution to which it
believes each feature it encounters in training conforms to  when
conditioned upon each class to which training examples can
belong  it uses maximum likelihood estimation  described earlier in
this paper  and the training examples with continuous feature
values provided to determine the parameters of these gaussian
distributions  when estimating the probability that an example
encountered in testing belongs to a certain class  the classifier
cannot use the probability that a continuous feature takes on a
certain value because the probability that a continuous feature takes
on a specific value is infinitesimally small  to circumvent this
issue  the gaussian naive bayes classifier uses the pdf  of the
conditional distribution that it has learned a continuous feature
conforms to  output of a value as a proxy for the probability that
the feature takes on that value  we demonstrate the mathematical
basis for doing so below 

      problems with gaussian naive bayes
while gaussian distributions may model many variables
reasonably well  they by no means model every potential variable
optimally  more importantly  they model the outputs of many of
our other sentence significance scoring models extremely poorly 
textrank scores conform to a roughly exponential distribution 
most sentences receive textrank scores of    with exponentially
smaller fractions of sentences receiving increasing positive scores 
in contrast  noun and adjective significance scores conform to a
roughly log normal distribution  because we use laplace
smoothing in calculating noun and adjective probabilities  and
remembering the way we calculated significance scores  refer to
previous section   all scores must be greater than    furthermore 
the likelihood of increasingly larger scores increases until a certain
critical point  the mode of significance scores   after which
likelihoods of larger significance scores decreases  most simply 
story specific verb significance scores are by design percentiles 
values distributed uniformly between   and   
furthermore  we decided to take advantage of the gaussian naive
bayes classifiers intelligent handling of ordered inputs to add
ordered simple features  we provided several such features and
feature templates  sentence length  noun count  proper noun count 
adjective count and verb count were among them  we modeled
these five features using the poisson distribution on our professors
recommendation  the intuition behind doing so is that word  noun 
proper noun  adjective and verb appearances within a sentence are
all essentially random events  and which the poisson distribution
was designed to model 
lastly  providing discrete features to a multinomial naive bayes
classifier  as discussed previously  and feeding the output of this
classifier into a gaussian naive bayes classifier constitutes an
unnecessary segmentation of prediction based on the features
underlying the multinomial naive bayes model  and would result
in compounding of prediction errors  we consequently wanted a
way to combine the discrete features handheld will by the
multinomial naive bayes classifier with the continuous features
handled by the gaussian naive bayes classifier 
      modified gaussian naive bayes classifier
fortunately  we found a modification of the gaussian naive bayes
classifier that allowed it to model ordered features using arbitrary
distributions  and could combine continuous and discrete features 
we combined the logic behind the standard multinomial and
gaussian naive bayes classifiers to create a hybrid  which we will
refer to as the modified gaussian naive bayes classifier  our

fimathematical logic for doing so follows  where  iy is the arbitrary
pdf of  i   y 

  

to those already in the summary  encouraging a broad
range of topic coverage in the summary 
length penalty  this is the reciprocal of the length of the
sentence in question  and penalizes longer sentences that
eat up larger portions of the summarys word limit 

we found that this simple greedy algorithm  using the sentence
significance scores provided by previously discussed models as
well as a few additional heuristics  performed quite well 

    constraint satisfaction problem
we also tried modeling the task of sentence extraction as constraint
satisfaction problem  csp   the csp balances the value of
selecting important sentences  the cost of choosing similar
sentences that contain overlapping content  and the word limit
constraint imposed on the task of summary generation 
      csp construction
the first step of the construction of this csp is to create a variable
for each sentence in our summary  represented as the index in the
summary  for each variable  we create a unary potential that is a
function of the overall sentence significance scoring value we
obtained corresponding to this specific sentence 
for every variable  we also create a binary potential between the
variable and all the other variables in the summary that serves as a
heuristic penalty for similarity  the value of this penalty term is
equal to reciprocal of angles between the tf idf vectors 
      solving the csp

implementing the modified gaussian naive bayes classifier thus
allowed us to simultaneously use discrete features and continuous
features modeled using arbitrary distributions  for each continuous
feature  our featurizer supplies a distribution type  the modified
gaussian naive bayes classifier then learns each distributions
parameters in training using mle 

  summary generation
to select the combination of sentences from within a story that
form the optimal summary  we implemented two different
algorithms  first  we implemented a simple greedy algorithm 
subsequently  we implemented a more complicated constraint
satisfaction algorithm  that while more appropriate in theory
performed poorly in practice 

after defining the potentials necessary to solving this csp  we now
need to find the optimal algorithm to solve it  at first  we attempted
to solve this csp using a backtracking search method  leveraging
standard backtracking search  although this method was very
effective for solving csps with shorter stories and summaries  we
found that as the length of our stories and desired summary lengths
increased  the backtracking search algorithm ran exponentially
slower with an execution time ranging from minutes to hours 
to speed up the process of solving our csp  we decided to
implement a beam search algorithm  with beam size k  our
algorithm utilizes the same underlying structure of this example  in
the proceeding section  we will describe how our beam search
algorithm works through the context of this structure  before doing
so we will define the symbols used in the above pseudo code as
they relate to our problem 



    greedy sentence selection
first  we implemented a greedy sentence selection algorithm  this
algorithm functions exactly as one would expect  iteratively
selecting the sentence that would increase the summarys expected
score by the highest amount and adding that sentence to the
growing summary until the summarys word limit is met  the
expected addition a sentence will provide to a summarys value is
calculated by taking the product of the following three factors 
  
  

sentence significance score  this score is calculated by
any of the previously discussed sentence significance
scoring models  this is the most important factor 
redundancy penalty  the is the reciprocal of the sum of
the tf idf angular similarities between the sentence and
each sentence already selected for placement in the
summary  this factor penalizes sentences that are similar






the indexes i represent the index into the summary that
we are constructing  variable name   xi 
as mentioned in the preceding section  the values that we
assign to our variables are the indexes into the story
sentences 
the variable c represents the current set of partial
assignments that we have for our variables
the variable v represents the next value that we are
assigning when we extend our partial assignment
domaini is the set of possible story indexes that we can
still assign to our summary
k   the beam size of our beam search algorithm 

our beam search algorithm is as follows 
   each of our k partial assignments represents an assignment of
sentences in our story to our variables  sentence index of
our summary   in this step  we extend each of these k
partial variable assignments by assigning the next
variable xi the value v  which is the sentence we want to

fiassign to this variable  note that we also extend the
weights of this new partial assignment by obtaining and
multiplying the current weight by the aforementioned
unary potential and binary potentials relating to the
assignment of xi to the value v 
   now that we have all of our possible new partial assignments
along with their corresponding weights  we take the best
k elements of each partial assignment based on these
weights 
   after obtaining our k best elements  we now prune out all the
other assignment extensions and move onto the next
iteration of assignment extensions with our k best partial
assignments 
we continue repeating steps     until we have a completed
assignment for all our variables that should represent the best
possible assignments for our final variables  unfortunately  we
eventually found that beam search did not perform optimally 
stories can potentially contain hundreds of sentences  and limiting
the possible set of assignments on each iteration to a reasonable
beam size of less than    prevents the vast majority of potential
summaries from being considered  upon observing that
backtracking search ran too slowly to be practical  and beam search
often produced sub optimal summaries  we decided to stick to the
use of greedy sentence extraction algorithms but still included the
option to use beam search for those who want to do so 

  evaluation
    evaluation metric
for the evaluation of our generated summaries  we use the recalloriented understudy for gisting evaluation  rouge  metric 
rouge is a publicly available text summary evaluation library and
is currently the industry standard in text summary evaluation 
rouge evaluates a summary by comparing it to a humangenerated gold summary and measuring similarity  it does so
similarly to the way in which bleu evaluates translations by
comparing them to gold translations  while bleu looks only at
precision however  rouge also takes recall into account in
generating a final score 

    getting baseline and oracle
to establish a lower bound of expected performance  we
implemented an extremely naive baseline algorithm  our baseline
algorithm iteratively extracts the first sentence of every paragraph
in a story and concatenates these to form a summary  while this
algorithm is certainly extremely nave  it should nonetheless
perform significantly better than a summarizer that randomly
selects sentences  sentences that start paragraphs tend to be
relatively important 
to establish an upper bound of expected performance  or an oracle 
we compared pairs of human generated summaries of the same

story to one another  treating one as the gold summary  by doing
so  we basically modeled humans as our oracle  the best oracle
possible 
we evaluated our baseline and oracle summaries on the subset of
stories we had for which we could find at least two summaries
online  we obtained the following results 
average baseline rouge score        
average oracle rouge score        
this experiment  and more specifically the fact that the mean oracle
rouge score was below      demonstrated that even humangenerated summaries deviate from one another significantly 
however  it also demonstrated that naive baseline methods perform
abysmally  the rouge scores of the oracle summaries  although
not amazing  still exceeds the baseline score by about a factor of
    this demonstrates significant room for algorithmic
improvement 

  results and analysis
    comparison of different sentence significance
scoring models
we ran a series of tests using our various sentence significance
scoring metrics as inputs to a greedy sentence selection algorithm 
as previously mentioned  we discarded the model that used svr
with estimated training data and the use of csp solving methods
for summary sentence selection due to their immediately obviously
weak performance  each test was run with a training set consisting
of    stories and associated summaries and a test set consisting of
   stories and associated summaries  due to the large runtimes of
the algorithms used  the algorithms would have taken exceedingly
long to train and test on larger data sets  the results of our tests
appear in the table below 
significance scoring model

mean rouge score

textrank

        

verb scoring

        

noun and adjective scoring

        

k means clustering

        

k means clustering with angular loss

        

multinomial nave bayes with simple features
gaussian nave bayes ensemble model with all
features

        
        

note that the ensemble model performed better than the others by a
significant margin  with a mean rouge score of        as such 
the ensemble model significantly outperformed our nave baseline
model  it nonetheless significantly underperformed our oracle  but
this came as no surprise given the difficulty of the task of automatic
summary generation 

fiin addition to the ensemble model  textrank model also performed
reasonably well  as did the multinomial nave bayes model with
simple features  the noun and adjective scoring model  however 
performed quite poorly  underperforming our nave baseline model 
the large variations in the performances of these different models
raised the question of whether or not each model provided value to
the ensemble model  to investigate this question  we ran further
tests 

    comparison of different feature combinations
with the ensemble model
we ran a series of tests through which we experimented with
providing different features to the stacked ensemble model backed
by the modified gaussian nave bayes classifier  throughout all of
these tests  we maintained the use of simple binary sentence
features and adjusted only the rich sentence scoring model features
used  we tested all possible combinations rich features  as with our
previous tests  we ran each test with a training set consisting of   
stories and associated summaries and a test set consisting of   
stories and associated summaries  the results obtained from this
experiment appear below 
textrank
feature

verb
score
feature

noun  
adjective score
feature

yes

yes

yes

yes

        

yes

yes

yes

no

        

yes

yes

no

yes

        

yes

no

yes

yes

        

no

yes

yes

yes

        

yes

yes

no

no

        

yes

no

yes

no

        

yes

no

no

yes

        

no

yes

yes

no

        

no

no

yes

yes

        

no

yes

no

yes

        

yes

no

no

no

        

no

yes

no

no

        

no

no

yes

no

        

no

no

no

yes

        

no

no

no

no

        

clustering
feature

average
rouge
score

note that the highest mean rouge score was obtained through the
use of all four rich features  this indicates that they all provide
predictive value  after running this second series of tests  we
remained convinced that all four of the sentence significance
scoring models tested possess predictive power 

    error analysis
to examine the performance of our system in a more qualitative
manner  we will examine a sample summary it produced  we will
look at the summary of in another country by ernest
hemingway produced by the gaussian nave bayes ensemble
model using all rich features 

the major held the photograph with his good hand and looked at
it very carefully  he asked  as we passed  but this was a long time
ago  and then we did not any of us know how it was going to be
afterward  we only knew then that there was always the war  but
that we were not going to it any more  we were all a little
detached  and there was nothing that held us together except that
we met every afternoon at the hospital  the boys at first were
very polite about my medals and asked me what i had done to get
them  the major came very regularly to the hospital  the
machines were new then and it was we who were to prove them 
he asked me  he seemed very angry  he spoke very angrily and
bitterly  and looked straight ahead while he talked  he was
looking at the wall  when he came back into the room  i was
sitting in another machine  my wife has just died  the doctor told
me that the major s wife  who was very young and whom he had
not married until he was definitely invalided out of the war  had
died of pneumonia  the major did not come to the hospital for
three days  in front of the machine the major used were three
photographs of hands like his that were completely restored  i do
not know where the doctor got them  i always understood we
were the first to use the machines  the photographs did not make
much difference to the major because he only looked out of the
window   

while far from perfect  this summary does succeed in mentioning
most of the storys important topics  it captures the central premise
of the story  the death of the majors wife  and also picks out
several mentions of the photographs that form an important motif
during the later parts of the story  however  the summary does not
read the way a piece written by a human would  for starters  it
contains several grammatically incorrect sentences  more
importantly  it fails to transition from one idea to the next in a
smooth manner  sentences describing different events and ideas
follow one another in abrupt fashion  often sounding like nonsequiturs  another significant issue with this summary is that many
of the pronouns it contains refer to unspecified people  in the
original story  these pronouns refer to their subjects without
ambiguity given the context in which they appear  however 
because only a very small percentage of the storys content has
been extracted to form the summary  most of that context has
dissipated 

  conclusions
the performance of our algorithms exceeded our expectations  we
did not expect to make any significant headway with an end goal
that seemed so daunting  and were pleasantly surprised when we
obtained reasonable results  while we were constrained by the
inherent inaccuracy of extractive summaries and difficulty in
getting an accurate summarization metric  we were able to test out
a wide variety of metrics to try and represent sentence importance
and laid the groundwork to continue optimizing our summary
generator for human subjects 

  future work
there is much we can do to improve the project and our results 
short term goals include applying coreference resolution to the text
before summarizing it and adding generative sentences to our
extractive summaries  such as a list of characters in the story  in the
medium term  we would like to try replacing the gaussian nave
bayes classifier with other classifiers and comparing their
performances to that of our existing system  using a logistic
regression based classier would be a natural next step because it is
also a probabilistic classifier  but we could certainly experiment
with discriminative classifiers as well  when using discriminative
classifiers  instead of using the classifiers estimated probability
that a sentence belongs in a summary as a proxy for sentence
significance  we would simply use the classifiers confidence of

fithat same fact instead  the transition should be smooth  more
ambitious and yet untested is the idea to model the sentence
extraction process as a hidden markov model  hmm  in which
sentence significance probabilities would represent emission
probabilities  and the probability a sentence should be extracted
given the sentences already placed in the summary would represent
transition probabilities 

   references
    automatic summarization  by ani nenkova and kathleen mckeown 
http   www cis upenn edu  nenkova            nenkova pdf
    textrank  bringing order into texts 
http   digital library unt edu ark        metadc      m    high res d mih
alcea      textrank bringing order into texts pdf
    lexrank  by gunes erkan 
http   www cs cmu edu afs cs project jair pub volume   erkan  ahtml erkan  a html
    summarization by latent dirichlet allocation  by kenton murray 
http   kentonmurray com thesis thesis pdf
    textrank  braining order into text 
http   web eecs umich edu  mihalcea papers mihalcea emnlp   pdf

   util distributions py
   sentenceclassifier py
csp construction and solving 
   sentenceweightscspconstructor py
evaluation 
   summaryevaluator py

     library dependencies
  
  
  
  
  
  
  
  
  

     borrowed code
in addition to libraries mentioned above  we imported a substantial
amount of code into our project written by others  the following
files and directories contain code we did not write 
  
  
  
  

   appendix
     running our system
to run our system  execute system py  in its current state 
it will run tests using the seven different sentence scoring
models whose testing results appear in section     and report
performance statistics for each  note that because the stories
placed into the test and training sets are randomly selected
from a larger data set  results will vary each time the system
is run 

     files related to functionality described
pipeline 
   system py
   documentsummarizer py
preprocessing 
   documentpreprocessor py
textrank 
   textrank py
noun and adjective significance scoring 
   wordsignificancepredictor py
verb significance scoring 
   verbsignificanceregressor py
sentence clustering 
   tfidfcalculator py
   clusterer py
   sentenceclusterer py
svr with estimated training data 
   tfidfcalculator py
   tfidfsentencesignificancecalculator py
   sentencefeaturizer py
   tfidfsentencesignificanceregressor py
multinomial naive bayes with simple features 
   sentencefeaturizer py
   sentenceclassifier py
ensemble model with gaussian naive bayes 
   sentencefeaturizer py

nltk  the library itself as well as its corpora 
pystatparser
pattern
scikit learn
numpy
pygraph
jsonrpc
corenlp python
xml  dom

code stanford ner directory  open source code
csp py and backtrackingsearch py  from the cs    
assignment    scheduling 
rouge directory  open source code 
additionally  a few lines of code here and there within
the code base were also taken from external sources  and
are marked as such by comments 

     submission
  
  
  

ashkon farhangi is submitting this project for cs    
and cs    n
lucio tan is submitting this project for cs    
eric holmdahl is submitting this project for cs     and
cs    

fi