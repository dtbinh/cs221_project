predicting success for musical artists through network and
quantitative data

i 

suzanne stathatos

zachary yellin flaherty

sstat stanford edu
stanford univeristy

zachyf stanford edu
stanford university

i ntroduction

with the rise of spotify  itunes  and youtube  ninety nine
cent songs have largely replaced     albums  slashing music
sales by nearly fifty percent      investing in many artists is
prohibitively risky today  it behooves music industry executives
to leverage available metrics and machine learning techniques
to predict whether an artist will be commercially successful
in the future  our goal is to predict whether artists will be
successful based on available music industry metadata  namely
artists importance in the music industry and the publics
response to their music  we collected annual measurements
from      to      representing these features and calculated
their change over time  we predict the success of an artist in
     
ii 

data s ets and f eatures

we generated a unique set of features indicating both public
perception of musical artists and indicating their status in the
music industry  we gathered some of these measurements from
existing publicly available datasets  discogs  the whitburn
project  echonest  and lastfm   we pre processed this data to
understand each musical artists annual level of interconnection
in the music industry  in total we have        artists and
   metrics  however  because our matrix was sparse  we
downsampled significantly during testing and focused on    
artists from years            the features we used are  for
each year           
  
  
  
  
  
  
  

degree in a graph of the music industry  gmi 
 higher degree    more connections 
eigenvector centrality in the gmi
betweenness of each artist in the gmi
artist success
playcount for artists billboard songs
rates of change for   playcount  eigenvector centrality  degree  success
y intercept for  playcount  eigenvector centrality 
degree  success

success in      is our classification target  we will explain
each of these features in more detail in the next subsections 
a  features and preprocessing
our features split into three broad categories  features from
annual graphs of the music industry  features that represent an
artists annual public popularity  and features that represent the
history  change over time  of each of these metrics 

   feature for success  to define commercial success for
an artist in a given year we collected data from the whitburn
projects dataset      this dataset contains billboards annual
song ratings from      to       these ratings list the top    
songs for each year  we focused on years      through     
to keep our predictions timely  we labeled a song as a hit if
it was within the top half         of the annual hits  we also
wanted to predict the success of an artist  not a song by that
artist  to translate hit songs to successful artists  we summed
how many successful songs each artist had in a given year 
if an artist ranked within the top     of the rankings  he she
was labeled successful for that year 
   features from the music industrys network  we generated a graph to represent and extract quantitative measures
for collaborations in the music industry  using data from
discogs com      we generated an undirected graph to model
the collaboration network of the music industry  and calculated
centrality metrics from this graph  looking at all indexed
releases dating back to       there exists an edge between two
artists in a given year if they appeared on the same record  we
created one graph to represent each year  for each target artist
in our feature set  we calculated the following 
degree  measures how prolific an artist is in a given year by
summing the number of collaborations that artist had in that
particular year 
eigenvector centrality  recursively calculates the influence
of a node in a graph  this metric is a centrality measurement
on undirected graphs similar to googles pagerank score    
eigenvector centrality is defined to be  let a be an adjacency matrix for our graph  using power iteration we solve
for the largest eigenvalue   and corresponding eigenvector
v 
av     v
the eigenvector centrality score of node i is vi   or the ith
index of v 
betweenness  the number of shortest paths that pass through
a given node 
all of these metrics offer some quantitative representation of
the importance of these artists in the annual collaboration
network  we used the stanford snap libraries     on large
aws machines to calculate these metrics  each years graph
has on the order of one hundred thousand nodes and one
million edges 
   features for artists annual popularity  we collected
metrics representing the publics perception of the artists and
his her music 

fiplay counts  the billboard dataset gave us a list of successful
 and unsuccessful  songs for each year  we queried lastfms
database for each of these songs to find how often those songs
were played online  after gathering how often each song was
played  we summed the play counts of each song corresponding to each artist  and created a summary measurement of
annual play counts for each artist 
hottness  we gathered user review data from echo nest     
for quantitative measurements that reflect an artists popularity
or hottness based on social media  user reviews  and mainstream media  these metrics could not be adequately queried
by time  so this feature was used only in initial validation of
our models 
   features for change over time  to represent change
over time  we calculated a linear change over time  slope and
y intercept  from each of our measurements where there is a
point for each year from      through      
b  verifying feature accuracy
we began by proving that we could verify an artists success from metadata without time as a factor  we amalgamated
the annual measurements for each artist  i e  beyonce  play
counts in       play counts in           success in      
success in           etc  into one overall measurement for
that artist  beyonce  play counts  hottness  success  etc   to
maintain success as a classification variable  as opposed to a
continuous variable   we labeled an artist as successful if the
artist had    hit song in the    year time period  this smaller
set had a total of      artists  rows   each with   cumulative
measurements  hottness  total plays  node degree  eigenvector
centrality  degree centrality  node eccentricity  and success  
our centrality measurements were based on discogs snapshot of the music industry in mid       rather than annual
graphs that we used for our final results  these snapshots
were estimates based on local subgraphs around target nodes 
starting with roughly        artists  we discovered some
patterns in our data  i e   artists are not relevant according
the billboard charts for some time period if they have not
released any music   that indicated that we needed to increase
the range of our timetable and reduce the corresponding
sample size of interesting artists to about      this sample
has a split of positive to negative examples  roughly     of
artists in this set were labeled successful      of artists were
labeled unsuccessful  we maintained this split of artists when
conducting our later tests to analyze our features over time 
results from validation of features tests
logistic regression  training logistic regression on     examples and testing on     yielded an average of     prediction
accuracy  we found about the same number of false positives
and false negatives 
support vector machines  when    random samples were
withheld from the data set      training samples     testing
samples  the classification accuracy of svms was      our
initial tests were run with no cross validation and with no
paramater adjustment  such that the we used the default kernel 
cost functions  and paramters provided by libsvm 
we wanted to analyze where svm failed  and we wanted to
determine if a different kernel and different set of parameters
could strongly change the performance measurements of svm 

fig     the relationships between artists popularities  their connection in the
music industry  and their degree of success is not obvious  success in both of
these graphs is represented by a green    and failure is represented by a blue
dot

we iterated through several combinations of parameters and
used    fold cross validation  instead of randomly splitting
our data  to choose the best modelthe model that gave the
lowest generalization error for our data  we found that a
linear kernel yielded the lowest generalization error  three
popular choices for kernels in the svm literature are d thdegree polynomial kernels  radial basis function kernels  and
neural network kernels  in this case  a linear kernel   st degree
polynomial  outperformed the other kernels by a factor of        this prompted us to switch from using libsvm to using
liblinear svm  our matrix is sparse  liblinear optimizes for
sparse matrices  and liblinear allowed us to experiment with
choosing penalty and loss functions 
after implementing a linear kernel for support vector
classification  we found that the best tolerance to use was
between      and    we experimented with this range of
tolerance values when classifying future predictions as well 
in addition  we wanted to visualize the relationship between
our artists popularity and the artists industry networks  these
graphical relationships can be seen in figure    it is interesting
to note that  although a clear relationship is not depicted from
the graphs  the learning algorithms did reasonably well at
classifying success  this speaks well of our use of support
vector machines  as they produce nonlinear boundaries by

fib  support vector machine classifiers
we used liblinear svm  rather than libsvm  because
we were manipulating a sparse data set  by doing so  we
were able to experiment and find the best combination of cost
functions and optimizations to minimize generalization error 
we did not choose class weights  but instead  let the algorithm
automatically adjust weights 
l  losses  squared hinge loss  and l   hinge loss  penalties outperformed their respective counterparts  l  losses l 
penalties  by an accuracy level of roughly         optimizing
for the primal rather than for the dual improved accuracies by
roughly     

fig     the decision boundaries for each classifier  these graphs give a
visualization of each of our classification models  graphing rate of change
of each artists degree to rate of change of each artists number of annual
play counts  blue regions indicate success in       while red regions indicate
failure in       the shadings represent probability boundaries  the lighter the
shade  the lower the probability that it is on that side of the decision boundary 

hinge loss  the hinge loss function  l y  f          yf    of
support vector machines estimates the mode of the posterior
class probabilities  other loss functions estimate a linear transformation of these probabilities  according to literature      
squared hinge loss  gives a quadratic penalty to points on the
outside of or far away from the support vectors  it gives a  
penalty for points inside the margin of the support vectors 
primal optimization  recall that given a training set   xi  
yi    where    i  n  xi  rd   yi           the primal
svm optimization problem is
n
p
minw b   w     c
ip where yi  wxi  b    i   i    
i  

fig     the decision tree from which this classifier split our data  the tree
chose the linear rate of change of success over the years as the most important
classifying feature 

constructing a linear hyperplane in a larger dimensional feature
space 

where in our implementation  p represents the hinge loss
squared 
the dual optimization problem can be written in terms
of dot products  making it possible to implement the kernel
trick  however  literature suggests that the primal optimization
problem can be better when computing approximate solutions      optimizing for the primal with our svm minimized
our generalization error more so than optimizing for the dual 
as a result  our model optimized for the primal rather than for
the dual 
c  decision tree classifiers    

iii 

c lassification m odels

we have continuous inputs and categorical outputs  therefore  we began by running support vector machines  svm 
and logistic regression  lr   these models allowed us to
explore our explicit features  lr  and variations in other
dimensions  svm   after understanding the best parameters
for each of these first two algorithms  we expanded to try
other supervised learning algorithms  including decision tree
classifiers and random forest classifiers  lastly  we explored
the k nearest neighbors classifiers 
a  logistic regression
for logistic regression  we employed matlabs glmf it 
where the inputs were the feature matrix and the     success
vector  logistic regression has limited flexibility in terms of
how to improve the model  we experimented with normalizing
our features  though this proved to be unnecessary  throughout
the project we also decreased bias by adding more data  our
final model that achieved an accuracy of     had non trivial
weights on all features  in other words  in appears all features
have some influence on the final prediction 

implementation  trees are able to capture complex interaction
structures in the feature set and graphically represent this
structure  to understand the significance of each of our features
 i e  to see if play count from      mattered more than play
count from             for an artist when determining artist
success in        we wanted to classify artist success in     
using a classification tree  classification trees work as follows 
we have p features and a measurement of success for n
observations  such that we have  xi   yi   for i         n with xi
   xi   xi       xip    we let the algorithm automatically decide
on the splitting variables  on the splitting points  and on the
overall shape of the tree 
for classification  the algorithm finds the best binary partition to maximize the proportion of class k observations in a
node m  given
pmk     nm

p

i yi   k  

xi rm

representing a classification region rm with nm observations  this expression represents the majority class  success
or failure in       in node m  cross validation chose a model
with a depth of   as the optimal tree size to accurately fit

fithe data  a graphical representation of the tree can be seen in
figure   
drawbacks  trees are noisy  while decision trees have low
bias  they also have high variance  often  a small change in
the input data can result in very different splits of the tree  to
handle this volatility  we used a more stable split criterion  we
chose the best split of the data  rather than a random split  we
used the gini function to measure the quality of the split   we
also implemented random forest classification  next section 
to average the results of many trees and thereby reduce this
variance 
decision trees also lack smoothness along decision boundaries  we are looking for ways to address this issue 

model
linsvm

log  res 
dec  tree
rand  for 
knn

parameters
l penalty 
l loss 
     tol 
dualfalse
max depth  
max feats 
auto
neighbors   

train accur
     

test accur
              

     
     

     
               

     
     

               
               

fig     results from running cross validation and model selection against
classification algorithms using metadata from years           to predict
artists success in      

d  random forest classifiers
as mentioned in the previous section  decision trees
experience a high degree of variance  random forests decrease
this variance through a process similar to bagging  random
forests build a collection of de correlated trees and average
their results together to eliminate noise from the trees  the
number of trees and the means to split the features of the trees
 auto  were chosen by iterating through different combinations
of parameters and then running    fold cross validation on
each of these models 
e  k nearest neighbors classifier
because we learned about the k means clustering algorithm
in class  we were curious to observe how a the accuracy of
a clustering classifier  in k nearest neighbor  given a port
xj   we find the k training points x  r   r           k closest in
distance to xj   the point xj is then classified using majority
vote among the k neighbors  when votes are tied  xi is
randomly assigned to one of the neighbors in the tie  we use
euclidean distance  d  i      x  i   xj     to measure distance
to neighbors  we found that   was the optimal number of
neighbors to require a point to be near  however  the lowest
generalization error of these models was roughly     
iv 

r esults and d iscussion

through extensive experiments  we were able to generate
models to accurately predict an artists overall success in
     given features from      through       each algorithm underwent    fold cross validation to minimize the
gap between training error and generalization error  a table
containing each algorithms predictive accuracies  along with
which parameters reached these accuracies  can be seen in
figure    our highest generalization accuracy from our support
vector machine classifier at      logistic regression placed
second with     accuracy  by analyzing our decision tree and
the weights automatically calculated from logistic regression 
we also determined that the slope of success for a musical
artist is the best indicator for success of the musical artist in
the future 
cross validation
implementation  we used    fold cross validation to select
our best model  we followed the same procedure as what is
outlined in       we retrained the models with the lowest
generalization errors on the entire data set to generate resulting

fig     learning curves for each classifier  showing both the training accuracy
and cross validation accuracy 

hypotheses functions  which are represented in the graphs of
figure   
evaluation  we chose k    to minimize variance and to
run our computations quickly  a large k proved to be very
computationally expensive   when k is too large  i e  k n  
the cross validation estimator is unbiased for the expected
prediction error  but can have high variance because the n
training sets are similar to each other  when k is too small
 i e  k     the cross validation estimator has low variance  but
bias can be problematic  this bias can be seen in figure  s
graphs  which show both the hypothetical learning curve  training accuracy  for each model along with the cross validation
accuracy  the gap between a small training sets training and
cross validation error is large  thus  using a small training
set would result in a considerable overestimate of prediction
accuracy 
with a data set of     unique artists     fold cross validation used training sets of size      which behaved similarly to
the full set  thus     fold cross validation did not suffer from
much bias  and had reasonable variance 
performance over time figure   is a graph of the testing
accuracies for each classification model over years          

fisuccess  it would be advantageous to be able to predict these
metrics as well  to predict these metrics  we would need to
provide the algorithms a way to separate each of the features
from each other  this could be done by having a summary
metric  i e  mean  for each feature to represent the previous
years  this could be done using a multi class classification
models  such as multi class svm  naive bayes  or multi class
linear discriminant analysis  we hope to look at this more
in the future 
furthermore  there are many more publicly available features that we can mine from social media and other less
readily available quantitative sources that would likely allow
for additional insight into our results 
vi 
fig     every tick along the x axis represents a years prediction accuracy 
beginning from      through       the greater the number of years that are
analyzed  the higher the predictive accuracy is for each classification algorithm 
this implies that a longer amount of metadata  i e  metadata for years          to predict       rather than just metadata for       is optimal  though 
it should be noted that more metadata inherently means that there are a larger
number of features  we had     samples and    features  but if our feature
set expands much larger  we would also need to collect and analyze a larger
number of samples  artists  

 i e       looks at features from           and predicts artist
success in            looks at features from           and
predicts artist success in       etc    the graph indicates that
as each model can observe a greater number of years for
each artist  its predictive accuracy increases  this suggests that
longevity plays a significant role in determining whether or not
an artist is successful in future years 
challenges  graph generation  the choice of depth at which to
traverse the graph  and calculation of pagerank and centrality
metrics were difficult  the music industrys graph is very
large  gbs of data   and proved difficult to manipulate graph
efficiently  betweenness calculations can have running times
of o  v       snaps pagerank and centrality algorithms do not
complete in a matter of days on graphs with millions of nodes
and edges  as a result  we read papers     exploring how
to create subgraphs around target nodes to estimate centrality
measurements  only when we moved to annual graphs about
an order of magnitude smaller than the original could we finish
these jobs in a matter of hours 
v 

f uture w orks

in our logistic regression tests  when we removed our
slope intercept features representing the best fit change over
time for our meta data features  our prediction accuracy
fell to      that is      lower than our regression results
that included these features  all of these misclassifications
were false negatives  this difference suggests that explicitly
modelling the change over time of these features as a linear
dynamical system or other time dependent model can not only
improve our accuracy  but also improve our understanding of
underlying trends of commercially successful artists 
in this project  we successfully classified an artists future
commercial success based on the artists connections in the
music industry and based on the artists popularity levels 
given that these metrics play a large role in determining the

c onclusion

the matlab and python scripts that we used to both collect
and analyze the data are available at
https   bitbucket org sstat cs    project 
we created our own definitions to answer the open ended
question of what defines commercial success of an artist  we
customized our features to take advantage of data that is easily
processable publicly  and our final feature matrix allowed us
to test many of the algorithms from our class using out ofthe box implementations  svm and lr succeeded according
to our problem definition  it is easy to see how these insights
can generalize into solving industry problems where there are
more stringent definitions of success and more data sources 
especially in a field as entrenched and full of possibility as
musical sales and perception 
r eferences
   

   
   
   

   
   
   

   
   

    
    

nasdaq aapl apple inc  and data from the recording
industry
association
of
america
dataset 
riaa 
https   www riaa com keystatistics php content selector researchshipment database overview  accessed october      
j  leskovec  snap for python  http   snap stanford edu snappy  index html  accessed october       
m  hinne  location approximation of centrality measures  masters
thesis for rabdoub university  january      
y  chen  q  gan and t  suel  local methods for estimating pagerank values conference on information and knowledge management 
november      
t  hastie  r  tibshirani  j  friedman  the elements of statistical
learning  springer series in statistics  page     
o  chapelle  training a support vector maachine in the primal  mpi
for biological cybernetics
t  hastie  r  tibshirani  j  friedman  section     tree based methods 
the elements of statistical learning  springer series in statistics 
pages         
discogs  accessed october       http   www discogs com  
the whitburn project  accessed october       billboard annual charts dataset received from the comments section in
http   waxy org         the whitburn project  
echonest api  accessed and used from october        november
      http   developer echonest com docs v  
a  ng  cs    lecture notes    http   cs    stanford edu notes cs   notes  pdf 

fi