project milestone report for cs     blood pressure detection from ppg
signal   sharath ananth  suid  sharath  
   introduction
it is known that heart rate and spo  oxygenation can be detected from a cell phone camera and flash
light  for example the successful app instant heart rate from azumio uses the cell phone camera and
the flash light to measure heart rate  this app has been downloaded more than    million times  it is
also well known that spo  levels can be determined by using the same method 
other research papers have connect spo  graph  also called photoplethysmograph  ppg  with blood
pressure  for example references        the ultimate idea of this project is to develop an algorithm which
will allow a cell phone app to analyze signals from the camera to estimate blood pressure  note that
only the algorithm will be developed in this project and not the app itself 
according to the american national standards of the association of medical instrumentation  the mean
absolute difference between the device and the mercury standard sphygmomanometer must be less
than  mmhg  and the standard deviation must be less than   mmhg  hence the goal of this project is to
be within these bounds 

   background
     structure of signal waveform
    explains that a mimic data base exists which contains bp information and the corresponding ppg
signal  this data base is used in this project  the aim then is to extract features from the raw ppg signal
and use this as input to a neural network 
the figures below explains the structure of the ppg signal and information extracted from this signal 

figure   structure of the ppg signal and the parameters extracted from them in    

fithe underlying intuition behind this extraction comes from       which show that there is a linear
correlation between the blood pressure  bp  and heart beat duration as computed from the ppg signal
 cardiac period   in       the systolic upstroke time  diastolic time as well as the width of     and  pulse
amplitude were considered as the possible parameters and the diastolic time was stated as the more
correlated to the bp  tests show that the higher the bp the shorter is the duration of every heart beat 
however  more tests with different signals show that such correlation is not always linear      states that
different authors provide different coefficients for estimating the bp from the cp  but such coefficients
are dependent on the person and need adjustment for each person 
the aim of     and the first part of this project is to use a neural network  nn  to estimate the systolic
and diastolic blood pressure using a nn and the features described above 
    machine learning algorithms
two machine learning algorithms were evaluated in this report  one is an incremental gradient descent
and the other is a neural network  these two methods are discussed further in this section
      incremental gradient descent
incremental gradient descent is a variation of the batch gradient descent algorithm where the
parameters are updated for every training example  in batch gradient descent the parameters are
updated once after looking at all the samples  incremental gradient descent was chosen since the data
set generated is quite large and it is expected that the algorithm is able to converge without having to
use all the samples 
it is assumed that the output y is a linear combination of the input x in the following fashion 

here is the input vector 
gradient descent 

is the output vector and

is the vector being estimated by incremental

the algorithm is as follows 
loop  
for i     to number of input
 

the cost function

was evaluated to measure convergence 

fi      neural network
the other algorithm evaluated was the classic back propagation neural network  matlab neural
network toolbox implements a levenberg marquardt backpropagation algorithm for the training  this
algorithm is described in      and the matlab help page  the basic neural network algorithm with error
back propagation is first described here following the material in     
neural network provides a method of defining a complex non linear form of hypothesis to fit to the data
 
below 

  consider the simplest neural network with one neuron as shown

x 

  f z 

x 
x 

x 
x 

 
x 

the neuron is a computation unit which takes as input
  f z 

and outputs

that is the single neuron is very similar to the input output mapping used in incremental gradient
descent  the weight vector is equivalent to
  the function f   is the activation function  so
the individual inputs are x  x  are weighed by and summed up  a function f   is applied this sum to
generate the output 
a neural network is put together by hooking together many of our simple neurons so that the output of
a neuron is the input of another  an example of a   layer nn is below  figure from        where the first
layer is the input layer and the  th layer is the output layer  the middle two layers are the hidden layers 

to simplify notation the output of each neuron is denoted by
denoted by 

  alternatively f z   

 

  that just means that f 

is

fifor example in the figure above the inputs are layer    so the first output from layer    is which is
simply the first input x   the next output from layer   is denoted as which is simply the next input x  
error back propagation algorithm 
the back propagation algorithm is a method of finding the weights of the neurons for all the layers so
that a cost function is minimized  the cost function used in nn is very similar to the cost function
described in incremental gradient descent with the addition of a regularization parameter  the new
cost function is

where l layer   i j    from ith node to

jth node 
as in most cost functions the update function is as follows 
  
computing the partial derivatives above is the key step which back propagation helps in  backpropagation gives an efficient way to compute these partial derivatives
as explained in      the intuition behind back propagation is as follows  given every training example
 x y  the neural network is run in a forward pass fashion to compute all activations throughout the
network  including the output value
  then for each node in each layer  the algorithm would like to
compute the error term that measures how much the node was responsible for any errors in the
output  for the output node  one can directly measure the difference between that nodes activation
and the true target value to compute the   for the hidden units  the error term is computed as a
weighted average of the error terms of the nodes  that is 
 i 
 ii 

perform feedforward pass  computing the activation for layers      and so on upto the
output layer
for the output layer  the error is
and the following is set 

 iii 

for each layer  going backwards from the last layer   set

 iv 

the partial derivative term is now set as

filevenberg marquardt back propagation
     gives a good overview of the levenberg marquardt algorithm  the primary difference is how the
weight functions are updated and the overview from     is shown below

the principal ideal behind the gauss newton algorithm is the approximation of the hessian matrix by
the product of two jacobians  that is
the principal idea behind the levenberg marquardt algorithm is the approximation of the jacobin
matrix as the sum of two elements  that is
with
the algorithm is using gauss newton and with a large value of the algorithm is using the
steepest descent  the algorithm switches the value of during training  initially the algorithm uses a
small value so that the algorithm is approximating the gauss newton  and later it switches to a large
value of and the algorithm switches to the error back propagation  this allows the algorithm to be able
to get the benefit of fast convergence of gauss newton and the stability of error back propagain 
the rest of the changes to the algorithm are to enable it to implement the gauss newton mechanism
rather than the ebp  the steps are as follows 
 i 
 ii 
 iii 
 iv 
 v 

compute the feedforward just as in the ebp algorithm and compute the error 
compute the jacobian  by using the chain rule 
compute the error gradient   jt  error
compute the hessian inverse as
and update the new weights
repeat above till converge

luckily matlabs tool box has already implemented this algorithm 

fi   pre processing step
considerable effort is focused on the pre processing step which allows for extraction of the parameters
described above from a ppg signal and this is a non trival task  the data in the mimic database is noisy
and clipped  for the signals which are not noisy and clipped the data does not fully follow the structure
shown in figure    a typical ppg signal is shown in figure below 

figure   raw ppg signal

as is seen in the above figure there is a peculiar shape to the ppg signal  this shape repeated in a few
good waveforms that it was required to understand this some more      mentions that the ppg signal
may be noisy such that the foot or trough of the signal may be hard to find      uses a wavelet
transform to eliminate these errors in the transform domain      explains the structure of the signal
nicely 

fifigure   structure of ppg signal as per      it shows how the signal changes with age 

figure   shows how the shape of the ppg signal can change with age  this explains why some of the
recorded ppg signals show this shape and why others dont 
the challenge now is to find the terms cp  sut  dt from the ppg signal in the presence of the diastolic
peak  the pre processing step that is currently implemented first finds the peaks above a certain
threshold   this threshold is chosen manually for each ppg signal   once the systolic peaks have been
found the minimum value between every two systolic peaks is the point used to find the diastolic time
 dt   the systolic update  sut  and the cardiac periodic  cp  

   neural network training and results
matlabs neural network tool box allows one to train  validate and test the data  currently a nn with one
hidden layer and    neurons was used to test  the data set consisted of       records generated as
explained above  the test data is generated using   patient data  each over approximately   hour
periods  matlabs model splits the data set into     for training      for validation and     for testing 
the following graph shows the histogram of the dystolic and systolic blood pressure

fifigure   systolic blood pressure  output of neural network

figure   dystolic blood pressure output

the error is            mmhg and            mmhg and the mmse performance metric was    the
histogram of errors is shown below

fifigure   histogram of errors

fi   insight and discussion
from the first set of results it seems that the mean error is low and the variance is high  so clearly we
need more parameters to be fed into the neural network  let us first evaluate what the errors are with
using   to   parameters  the table below compares the results
parameters
cp  sut  dt
sut  dt
cp  sut
cp  dt
cp
dt
sut

mean std of error
           mmhg and
           mmhg
         mmhg and
          mmhg
         mmhg and
            mmhg
        mmhg and
          mmhg
           mmhg and
           mmhg
              mmhg
              mmhg
         mmhg
         mmhg

performance
  

comments

  
     
    
    
    
    

negligible  difference between changing the different parameters  on closely examining the output from
the nn and the desired output it was found that the error is dominated by impulse noise in the inputoutput vectors  this implied that further pre processing is required 

   modified pre processing and results 
after further analyzing the input waveform and the pre processing step it was seen that the peak and
trough detection was finding many false peaks and troughs  this was eliminated by the following preprocessing changes
 i 

 ii 

the peaks cannot be very close to each other  a threshold was experimentally determined
such that two peaks closer than the threshold are considered part of the same signal  this
threshold is dependent on understanding how close the heart beats are  for example
children will have closer thresholds than adults  currently the threshold used is only for
adults and further processing needs to be done to handle children
the troughs were found by walking down the peak to find the position where the signal
changes shape  this is explained in the figure below

fi iii 
 iv 

median filter the inputs and outputs to eliminate impulse noise
visually inspecting the input and output waveforms to eliminate regions of severe clipping
or zero samples due to instrumentation error 

the new results with these modified input and output is shown in the table below
parameters

mean std of error

cp  sut  dt

           mmhg
           

performance with   
hidden neurons
r        
mse        

sut  dt

           mmhg
           mmhg
         mmhg
          mmhg
          and
           mmhg
           mmhg
           mmhg
         mmhg
         mmhg
         mmhg
           mmhg

mse        
r        
mse        
r        
mse       
r        
mse        
r        
mse        
r        
mse        
r        

cp  sut
cp  dt
cp
dt
sut

performance with    
hidden neurons
mse          too small an
improvement to proceed
down this path 

clearly there is some performance improvement by adding the additional features  but not enough to
add a many more than    the regression plot looks as follows

fifigure   regression plot using   inputs for   outputs     element nn

as seen in the regression plot above  there is a reasonably good  the next section evaluates incremental
gradient descent for this problem

   incremental gradient descent
incremental gradient descent  as explained in        was also implemented and the following
convergence curves were obtained using   inputs  sut  dt  cp  and   outputs 

figure   cost function vs iterations for incremental gradient descent

fithe mmse error was computed as      as compared to     in nn   the mean square error is   times
worse than nn  the regression plot looks as below 

figure   regression plot using   inputs  using incremental gradient descent

we see that there is a good fit here as well  just not as good as what is found in the neural network  the
table of results are below
parameters
cp  sut  dt
sut  dt
cp  sut
cp  dt
cp
dt
sut

mean std of error
          mmhg
         mmhg
         mmhg
         mmhg
        mmhg
       mmhg
           and
            mmhg
           mmhg
         mmhg
         mmhg
         mmhg
        mmhg
     mmhg

comments
mse     
r         
mse    
r         
mse       
r         
mse     
r       
mse     
r       
mse     
r         
mse       
r        

fi   conclusion
as shown in this report there is enough information present in the ppg signal to identify the bp of a
person  the regression plots look encouraging enough to show that one can use the ppg signal to
estimate the blood pressure  in addition once three parameters are used we are within the limits
required for good blood pressure measurement  so more parameters are not really required 
neural network outperforms linear incremental gradient descent by enough margin that this should be
method considered for future studies 
however the fundamental problem now lies with the pre processing step which in this work so far
requires some manual input  in addition many waveforms were deemed too difficult for the algorithm
to handle and hence discarded manually  one example of waveforms which do not work is given in the
appendix  future work would be to find good pre processing algorithms to overcome this issue 
currently the reason the results look good is because of having to discard waveforms which could not be
handled by the pre processing step  future work needs to be able to handle more waveforms 

fi   appendix
some interesting waveforms that were observed are captured here to help future work  currently the
pre processing step cannot handle the presence of a double peak  a few abp signals showed the
waveform given in the figure below  reference     explains the reason behind this waveform shape
in aortic regurgitation  the arterial pressure wave rises rapidly  pulse pressure increases  and diastolic pressure is
low  owing to the runoff of blood into the left ventricle as well as the periphery during diastole  because of the large
stroke volume ejected from the left ventricle in this condition  the arterial pressure pulse may have two systolic
peaks  bisferiens pulse   see fig         these two peaks represent separate percussion and tidal waves  with the
former resulting from left ventricular ejection and the latter arising from the periphery as a reflected wave

figure    example of an input abp waveform that was not used for this project

fireferences
    x  f  teng and y  t  zhang  continuous and noninvasive estimation of arterial bloodpressure usinga
photoplethysmographic approach  proc of   thannual inter  conf  of the ieee engineering in medicine and biology
society  cancun mexico        pp          
    y  yoon  g  yoon  non constrained blood pressure measurement by photoplethysmography  journalof the
optical society of korea  vol     no    pp        june      
    yu  kurylyak  f  lamonaca  d  grimaldi   a neural network based method forcontinuous blood
pressure estimation from a ppg signal   in proc  ieee international instrumentation and measurement
technology conf   minneapolis mn       
    yu  kurylyak et al   photoplethysmogram based blood pressure evaluation using kalman filtering
and neural networks  in proc  ieee international symposium onmedical measurements and
applications       
    rohan samria et al   noninvasive cuffless estimation of blood pressure using photoplethysmography
without electrocardiograph measurement in ieee tensymp      
   satya narayan shukla  m tech project mid term report  project title noninvasive cuffless blood
pressure estimation from ppg signal
    d  deriso  n  banerjee  a  fallou  extracting vital signs from video       project for cs   
    mohamed elgendi  on the analysis of fingertip photoplethsymogram signals  current cardiology
review
    http   web squ edu om med lib med cd e cds anesthesia site content v         r   htm
     training feedforward networks with the marquardt algorithm  martin hagan and mohammad
menhaj  ieee transactions on neural networks      
     sparse autoencoder  cs   a lecture notes  andrew ng
     http   www eng auburn edu  wilambm pap      k      c    pdf levenbergmarquardt
training notes

fi