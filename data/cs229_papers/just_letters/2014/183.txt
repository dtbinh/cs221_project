predicting usefulness of yelp reviews
xinyue liu  michel schoemaker  nan zhang

 

introduction

yelp offers users with a myriad of reviews and ratings of businesses all over the world  it also provides users with the
ability to up vote a review as useful  funny or cool  with some particular reviews being heavily up voted as useful by the
yelp community  yelp uses a proprietary algorithm to determine the display order of reviews for a particular business 
the particular details of this algorithm are kept under wraps  though yelp acknowledges it is based on various measures
of quality  reliability  and activity on yelp  a quick analysis seems to reveal that these usefulness scores do not seem to
heavily influence the display order of reviews  based on this fact  our team ponders the question of whether the words
used in a review correlate to its usefulness  and whether we can predict if a review will be deemed useful by yelp users
 in particular  within the top     of votes   using the yelp dataset challenge database  we choose      random reviews
to perform an analysis  after performing numerous natural language processing techniques on our data set  we obtain a
feature matrix on which to predict our binary response  we apply feature selection methods to our matrix  build five
different models  calculate the training and test errors and analyze our results  finally  we extract the top    indicative
words of a useful review 

 

prior work

in the past  yelp has posed the question of how many useful votes will a yelp review receive  as a recruiting
competition aimed at engineers interested in working at yelp      though most submissions are kept private  we find two
relevant papers that attempted to answer his question  the first paper is titled predicting the number of useful votes a
yelp review will receive      and uses multiple features such as star rating  length of review and a small    word
feature matrix for an error of approximately       the model is good at predicting votes up to the number of    but falters
after that  the second paper  exploring the yelp data set  extracting useful features with text mining and exploring
regression techniques for count data     uses a myriad of features and models and had a rmsle ranging from      to
      all these analyses  however  fail to account for the fact that reviews for restaurants in highly populated and
urbanized areas will overall have higher numbers of useful votes  our approach is different in that it looks at usefulness as
a binary response  in which the true value accounts for the top     useful votes within a particular business 

 

data

we obtained our data from yelp on the yelp dataset challenge website      the data is a collection of  json files with
information on yelp users  businesses  reviews etc  since we are interested in review text  we obtained all of our data
from the file that contains data on reviews  named yelp academic dataset review json   it is formatted as follows 
 
 type    review  
 business id    encrypted business id  
 user id    encrypted user id  
 stars    star rating  rounded to half stars  
 text    review text  
 date    date  formatted like               
 votes     vote type    count   
 

fithe file contains           reviews  each with the text string and useful vote count  it should be noted that the
challenge dataset only includes data from phoenix  las vegas  madison  waterloo and edinburgh 

   

feature matrix

for the purpose of our project  we randomly choose      reviews out of all
reviews to perform our analysis  after examining the reviews that were
randomly chosen  we discover that   reviews contain no text in them or only
contain punctuations in them  therefore we choose to discard these   reviews 
next  we attempt to clean the data up to get a dictionary of words that will be
used as our features  we extract all the words that appeared in the     reviews 
and count the number of times each word appeared in total  after taking a look
at the words we obtained  we see that many words lack spaces between them
 e g  potatosquishy  chickenjoy   we split them by comparing the
levenshtein distance between words  and obtain a second dictionary with
correctly spaced words  because there exist many non useful stop words
 such as the  a  in the dictionary that are not desired features for text
classification  we use an nltk package     to delete all stop words from our
dictionary  since different variations of a word sometimes have the same
meaning  e g  eat  ate  eaten   we use an nltk package to lemmatize
the words in our dictionary  finally  we discard words that appear less than   
times  since words that appear infrequently are less useful for prediction  the
final version of our dictionary contains     words 
after obtaining our dictionary of words  we create a sparse matrix with    
rows and     columns  each row represents a unique review  and each column
represents a word in our dictionary  the  i  j th entry in the sparse matrix is the
number of times the jth word appeared in review i  the matrix is saved in
a  mat format that can be readily loaded into matlab 

   

response vector

create dictionary of all words
that appeared in     reviews

split words that are missing
spaces in them

remove stop words

remove variations of a word

discard low frequency words

create sparse matrix with rows
as review and words as column
figure    process of creating feature matrix

each review on yelp receives a number of usefulness votes  we wish to turn the number of usefulness votes a review
receives into a binary variable with   indicating the review is useful and   otherwise  for each business  we order the
reviews by how many usefulness votes they received  and we consider the top     reviews as useful  and the remaining
    reviews as not useful 

 

feature selection

we use holdout cross validation and randomly split our data into     training set and     test set  to reduce the
dimension of the feature matrix  we apply principal component analysis  pca  to the inputs of both the training set and
the test set  we choose                     to be the percentage of total variance explained by the principal
components  the number of principle components for                     are                 respectively  thus we
have four pairs of training set and test set with reduced dimension  we train and test our models on these training sets and
test sets in order to see how the number of features influences the results for different learning algorithms 

 

models

fias mentioned before  our response variable is a binary and determines whether a review will be voted useful or not  we
apply five methods to the training sets attained by feature selection  linear discriminant analysis  lda   quadratic
discriminant analysis  qda   nave bayes  logistic regression  and support vector machine  svm   for each method
we calculate both the training error and test error  which are defined as 
 training error   misclassified training sample total number of the training sample
 test error   misclassified test sample total number of the test sample

   

linear discriminant analysis  lda 

we use a matlab built in linear classifier that fits a multivariate normal density to each category with a pooled estimate
of the covariance matrix  specifically  we choose the following matlab function 
output   classify  sample  training data  training category  diaglinear 
we use diaglinear as the type of classifier instead of linear  because diaglinear is similar to linear but gives an
estimate of the covariance matrix that is diagonal  this can solve the problem of ill shaped covariance matrices  the
output is the category of the sample predicted by this classifier 

   

quadratic discriminant analysis  qda 

we use a matlab built in quadratic classifier that can separate two classes of events by a quadratic surface  it fits
multivariate normal densities with covariance estimates stratified by each category  similar to what we did for lda  we
choose the matlab classify function with type diagquadratic instead of quadratic to avoid ill shaped covariance
matrices  we calculate the binary output and the corresponding generalization error from fitting this model 

   

nave bayes

we train the model by using a multinomial event nave bayes classifier with laplace smoothing  later we will use this
model to find the most predictive words for useful reviews 

   

logistic regression

for the logistic regression model  we select the corresponding coefficients in the following ways 
   apply the matlab built in function of generalized linear regression  glr for binomial responses  to the
selected training sets and get coefficient estimates b  
   apply the lasso algorithm to the training sets and use   fold cross validation to select the best regularization
parameter lambda  gain the coefficient estimates b  under the value of lambda 
we compute predicted outputs for the logistic regression with coefficient estimates b   b   and attain corresponding
training and test errors for comparison 

   

support vector machine  svm 

we use matlabs fitcsvm function to train a binary support vector machine classifier  using both a linear kernel
k x  x     x x  and a gaussian  rbf  kernel k x  x     exp    x  x       then we predict the categories by calling the
predict function on our trained svm models 

 

results

   

training and test errors

fifor all the models  the training and test errors with regard to the percentage of total variance explained by the principal
components are presented in table   and plotted in figure   

model

error

lda
qda
nave bayes
logistic
logistic lasso 
svm  linear 
svm  rbf 

    variance     variance     variance
explained
explained
explained
training       
      
      
test
      
      
      
training       
      
      
test
      
      
      
training       
      
      
test
      
      
      
training       
      
      
test
      
      
      
training       
      
      
test
      
      
      
training       
      
      
test
      
      
      
training       
      
 
test
      
      
      
table    summary of all training and test errors

    variance
explained
      
      
      
      
      
      
 
      
      
      
      
      
 
      

figure    plotting errors against percentage of variance explained
a rough comparison of the test errors produced by different methods shows that logistic regression and svm do better
than lda  qda and nave bayes  both the logistic regression  with lasso  and svm methods have the least test errors
around     we also examine the relationship between errors and percentage of total variance explained by principle
components for each model 

fi







   

lda  both the training error and test error go down from     to     explained variance  with the test error close to
training error  however  the training error continues to go down after     explained variance while test error goes up 
with a large gap at      explained variance  this may be caused by high variance when training in the original high
dimensional feature space 
qda  both the training error and test error go down as the explained variance increases  the large decrease in test
error from     to      explained variance implies that reduced dimensions may lead to underfitting 
nave bayes  both the training error and the test error are relatively high  around      regardless of the dimensions
of feature matrices  it indicates that nave bayes may not be a good model choice for this data set 
logistic regression  using the lasso method to train the model does better than generalized linear regression  glr 
by keeping a constant test error          for all the situations  the original glr will overfit data in high dimensional
feature spaces  e g  at      explained  
svm  the rbf kernel has lower training error than the linear kernel  while their test errors are the same after reducing
the number of features to     explained variance or lower  this implies that by properly reducing the number of
features with pca  the type of kernel may not influence the test error 

indicative words for useful reviews

by applying our multinomial event nave bayes model to the original feature matrix           and corresponding
response  we found    most indicative words for useful reviews  day  show  over  star  thing  free  look  chicken  say 
lunch 

 

conclusion

from our analysis  logistic regression  with lasso  and svm algorithms can be used to predict the usefulness of yelp
reviews  because they have much lower generalization errors  we also find that most of the time using pca or the lasso
method to reduce the dimension of the feature space helps solve the problem of overfitting  and thus enhances prediction
accuracy  finally  we find the    most indicative words of a useful review  future work could involve the following parts 
   adding more features to our models and analyzing how these extra features influence our prediction of review
usefulness  for example  we could consider adding the length of the review or other types of votes the review received
from users 
   using higher order regularization methods to train the logistic regression model 
   using the k means method to group the reviews into several classes and train corresponding models of each class  to
predict the usefulness of a new review  we can first decide which class it belongs to  then using the models for this
specific class 

 

references

    kaggle        how many  useful  votes will a yelp review receive  show off your skills to land an
interview for a position on a yelp data mining team   https   www kaggle com c yelp recruiting 
    luther  akshay        predicting the number of useful votes a yelp review will receive
 http   akshayluther com            predicting the number of useful votes a yelp review willreceive 
    anonymous  exploring the yelp data set  extracting useful features with text mining and exploring
regression techniques for count data   http   www cs ubc ca  nando          projects p  pdf 
    yelp        yelp dataset challenge  retrieved october            http   www yelp com dataset challenge 
    nltk org        nltk documentation      retrieved october          
 http   www nltk org api nltk html 

fi