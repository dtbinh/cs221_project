hacking the genome
namrata anand
may        

 

introduction

if you want to hack the genome  you have to find a way to turn genes on and off at will 
you have to be careful  though  since all cells have identical genomes     
there are on the order of      cells in the human body  this population is made up of
hundreds of unique cell types  each with specific molecular and cellular functions  current
gene therapeutic approaches involve treating disorders by delivering plasmid dna or viral
vectors that express therapeutic genes  these genes are expressed continuously in all cell
types  yet many diseases only affect specific cell populations  expression of exogenous genes
in unaffected cell populations is unnecessary and potentially deleterious  so how might we
go about turning genes on and off in a cell type specific manner  one way is to try and find
regulatory regions of the genome that do this 
in this paper  we apply machine learning methods to try and determine the cell type
specificity and activity of non coding genomic sequences 

   

background

in the past few years  thousands of noncoding genomic elements with unique tissue specific expression patterns have been identified  as manipulable regulatory elements  these
elements are superior to larger promoter regions because of 
 size  small  on the order of    kb   kb
 complexity  regions dictate simple interactions with minimal promoters
 context  gene expression controlled by these elements is independent of orientation
and genomic location
 function  regions can work to either activate or repress gene expression

 

fiwith these advantages in mind  these smaller noncoding genomic regions are interesting to
study in terms of their cell type specificity  these regions are located by open chromatin
assays assays that look for areas of the genome that are not bound up in chromatin  the
protein structures around which dna is wound  these small areas are likely enhancer or
repressor regions  transcription factors will bind to these regions and affect expression of
genesboth proximal and distal 
our approach involves analyzing open chromatin data with the goal of discovering which
noncoding genomic elements are active in a given cell type  identifying such elements could
not only give insight into how these noncoding elements work to affect transcription  but
could also help direct approaches in gene therapeutics  many researchers have used machine
learning to try and predict different attributes about genomic sequences          the first
part of this paper is a repurposing and reevaluation of methods used in         the goal is to
discern advantages and disadvantages of these methods  the second part of this paper is an
exploration of deep learning applications to predict expression and specificity of these short
non coding elements 
we divide our approach into two branches  expression  turning things on and off  and
cell type specificity  flipping the right switch  

 

supervised learning

   

data

we use an expression dataset of      predicted enhancer elements from encode  including enhancer and repressor regions from k    and h  hesc cell lines  these regions
were functionally tested in      for each sequence         bp long   there is a readout of
luciferase expression in k    cells  the dataset includes scrambled sequences as negative
controls  we aligned this dataset with     histone markers and transcription factor binding
site information from uw encode for the k    cell line 
to test for cell type specificity  we look at two example cell lines  human astrocytes
 hac  and cardiac myoctyes  hcm   for each cell line  we use dnase seq data pulled from
the uw encode repository      each cell line has two replicates for the assay  we also
use histone mark h k  me  data from the uw encode repository  there are        and
       unique sequences attributed to hac and hcm  respectively  the two classes are
pretty well balanced  assay peak locations for each replicate were concatenated and then
sorted by chromosome and location  overlapping regions across replicates were merged and
non overlapping  unique  regions across cell types were found  histone marks for each cell
type were aligned with the sequences  to clarify  if a histone mark appeared in a location
for an experiment in hac  that mark was aligned with open chromatin locations in hac 
but not in hcm  and vice versa   finally  sequence data corresponding to chromosomal
locations were extracted 

   

feature extraction

for each dataset  we built three feature sets 

 

fitf  mo fs  
  
gata   

cardiac  
myocytes  

c 
myc   
foxa   

  

sox   

astrocytes  

  mers  
cctt  

ggcg  
aaaa  
actg  

ac ve  
  

histone  marks   tf  binding  sites  

not  ac ve  

polii  

ctcf  
h k 
 me   

cbp  

figure    feature extraction
 tf motifs  we used tess     to predict transcription factor binding of     known
factors  position weight matrices from homer      within each sequence and created
a feature set of motif counts 
 histone marks  we aligned each sequence with publicly available data on histone marks
and transcription factor binding sites specific to the cell type in consideration  for each
sequence location  we report the number of marks found within that location  for the
expression dataset  we have     factors and for the cell type specificity dataset we have
a single factor 
   mers  we extracted the occurrence counts of all possible   mers for each sequence
     combinations  
our feature sets are small in dimension relative to the number of training examples 
which led us to expect high bias from linear models  as a result  we chose not to do feature
selection immediately and went straight to testing the models 

   
     

experiments
expression

model accuracy
feature set
tf motifs
histone marks tf binding
  mers
joint

nave
train
     
     
     
     

bayes
test
     
     
     
     

svm
train
     
     
    
 

 linear 
test
     
     
     
    

svm  rbf 
c
gamma train
  
     
     
 e     e   
     
  
     
     
  
     
     

test
     
     
     
     

table    results for prediction of sequence expression in k    cells  models were run with   fold validation 
results for training on     of the data           sequences   tested on                sequences   grid
search for parameters c  gamma on svm with rbf kernel done over base    logspace        data scaled to
      and shuffled before training 

 

fiwe thresholded the expression data for sequences using the   th percentile expression
level for the scrambled sequence which serve as the negative control  as done in      this
gives binary output  which works well with many linear models   we implemented a few
of these models in python  first keeping the feature sets separate  the models display high
biasadditional training examples are unlikely to improve model performance  for linear
svm  training on the feature set of known histone marker and tf motif binding events led
to better performance than training on the other feature sets  however  when we used an
svm with an rbf kernel  thereby accounting for nonlinear combinations of features  the
performance of the other two models greatly improved 
we then combined our feature sets in an attempt to understand if integration of diverse
datasets might boost model performance as seen in      see figure   in the appendix   we
find that the joint model does not outperform the best feature sets in terms of test accuracy 
but the model displays less bias  with more training examples  we expect the prediction
capability of the joint model to improve more so than the individual feature sets 
overall  we see that known histone mark overlap and transcription factor binding are the
best predictors of activity for open chromatin sequences  although we have poor prediction
capability  our results call into question taken by researchers looking to predict enhancers in
     as we are not able to build an unbiased  scalable model  even with a similarly constructed
feature set 
     

specificity

we tested a number of classification models on the cell type specificity dataset and report
our primary results in table    while the models display expected bias  the test accuracy
is quite high given the difficulty of the classification task  the limited feature set  and the
small number of training examples  as with the expression dataset  the tf motif feature set
outperforms the   mers for svm with linear and rbf kernels  feature elimination improves
model accuracy but exacerbates bias  see section       in the appendix   combining the
three feature sets leads to improved performance for both models  with a test accuracy of
     for svm  rbf   this joint feature set includes a single column for known histone mark
h k  me   to assess the relative importance of this feature  we remove it from the joint
dataset and reevaluate our models  as expected  training accuracy decreases slightly with
a simpler model  counterintuitively  we found that the test accuracy increased when we
removed histone mark data  indicating that perhaps histone mark data was making it more
difficult to differentiate the two cell types  since we are only looking at a binary classification
problem and a single mark  it is impossible to make any conclusive generalization from this
result we can say no more except that it is interesting 
svm with the rbf kernel with our joint feature set outperforms the other models and
feature sets  see figure     it seems like combining an unbiased approach for feature selection
with one informed by current scientific knowledge helps improve the model performance  the
kernel trick seems to help mitigate bias by increasing the complexity of the model  sill  this
model displays high bias  we could try ensemble methods or try fleshing out the feature
space  or we could try something unsupervised  lets try something unsupervised 
 

regression results are listed in table   in the appendix

 

fiaccuracy
feature set
tf motifs  homer 
  mers
  mers  pca 
joint  w  histone 
joint  w out histone 

nave
train
     
     

bayes
test
     
     

     
     

     
     

svm
train
     
     
     
    
     

 linear 
test
     
    
     
     
     

svm
train
     
     

 rbf 
test
     
     

     
     

     
     

table    results for prediction of sequence specificity in human astrocytes  hac  or cardiac myocytes
 hcm   models were run with   fold validation  results for training on     of a subset of the data
           sequences   testing on                sequences   for svm with rbf kernel identical parameters were used across models  c   and gamma      for grid search for parameters c  gamma on svm
with rbf kernel done over base    logspace        data scaled to       and shuffled before training 

 

unsupervised feature detection

unfortunately there is not nearly enough reliable expression data for putative enhancer
elements  but there are thousands of open chromatin sequences that are uniquely associated
with specific cell types and organ systems and using these  we might begin to try and extract
features in an unsupervised manner 
     

data

we built an unbiased training set made up of flattened sequence data  we selected a
random    bp patch from each sequence from our cell type specificity dataset  encoded it
in a   d binary tensor and then flattened the data into a column vector  the feature set is
therefore  a in position    c in position    t in position    g in position    a in position      
t in position      g in position       unlike the   mer feature set  here we encode position
and ordering information  this is actually restrictive  we are selecting random patches from
the sequence  but the feature set is a fixed representation of base pairs in precise locations
and is not translationally invariant  ideally we would build a network that first looks only
at local information  a receptive field  and then later combines local information to get at
global context  see section         
     

experiments

we first looked at how linear models perform on this sparse representation as a baseline 
models displayed high training and test errors  with maximum accuracy across models around
    
in table   we report some results of experiments with autoencoders  we train the
autoencoders on    k sequences and then use the architecture to train a feed forward neural
network and make predictions on test sequences  for the   layer stack  we experimented with
having the first layer learn an overcomplete representation of the input data by having more
hidden neurons than input dimensionsin essence extracting informationand then having
subsequent layers map to lower dimensional space  we achieve highest performance with this
architecture with a test error of       overall  we have pretty terrible classification results 
 

fiour sense is that we might need orders of magnitude more training data to learn features 
the input data classes are perfectly balanced  so the network might be learning something 
since errors are consistently less than      however  it is not clear that this approach will
work very well  since encoded features are not translationally invariant 
error
num  layers

architecture

 

   
   

  

train    k 

test    k 

train   k 

test   k 

     

     

     

     

     

     

     

     

 

   

   no extract 

   

   

  

  

  

 

    

     

     

     

   extract 

   

   

   

  

  

 

     

     

     

    

table    unsupervised feature detection with  stacked  sparse autoencoders and testing on cell type specificity data  autoencoder layers with sigmoid activation trained on    k sequences encoded in    x  column
vectors  feed forward neural network  logistic regression  trained on  k sequences  tested on  k  training
on autoencoder layers trained for   epoch  feed forward neural network trained for   epochs 

tiled convolutional neural networks are convolutional neural nets that allow layers to
have a tiled pattern of weights adjacent hidden units do not need to share weights  this
is useful for this project because tcnns can encode translational invariance of features but
also preserve local context of information  we have implemented tcnns pretrained with
topographic ica  tica   this is not at all a thorough analysis  but a quick and dirty
attempt to get nicer results and to try out a more refined model  we cut off    bp from each
training example  reformatted our sequences into    by    matrices  and implemented a two
layer tcnn  l  window size     l  window size     maps      tile size     our first  and
only   attempt using a tcnn achieves       training error and       test error pretraining
on    k sequences and training testing on  k  k sequences  this reflects an improvement
over our experiments with stacked sparse autoencoders of similar depth 

 

discussion and conclusion

in this paper we attempt to discover whether we can predict non coding genomic sequence
expression and cell type specificity using linear and nonlinear models  we find that creating
a feature set incorporating data from diverse sources by hand allows us to classify sequences
with respectable accuracy  but doing so limits the scope of generalization of the model 
while it is important to classify sequences accurately  a more interesting question is  can
we use the machine to point us in the direction of novel hypotheses  that is  can we use the
machines predictions to discover biological mechanisms previously obscured  the extent to
which we can do this well depends on the data sets and feature set s  we select  limiting or
biasing the feature set will in turn limit the scope of possible interpretation of the results  for
example  if the feature set only includes known tf binding motifs  the hypotheses generated
about what precisely differentiates regulatory sequences across cells will only factor in known
tf binding motif data there is no room here to generalize  suppose the feature set only
included histone mark or tf binding data  then the accuracy of our prediction and any
 

fisubsequent hypotheses inferred will be biased by the nature of the experimental assay used
to generate those marks  the extent of this bias is reflected in our first set of results and in
results for papers with similar methods          
why not add more features  first of all  experiments are expensive  in the expression
dataset  because of the cell type used  k     in the original experiments  we were able to
procure lots of cell type specific data on histone marks and transcription factor binding 
but in the case of our cell type specific analysis  the amount of metadata available was quite
limited  for a given biological assay  there is a vast array of possible cell types  organ systems 
and experimental conditions to consider  it is not at all feasible to perform experiments
covering all these combinations  simply in order to improve our classifiers  therefore  it is
important we discover a way to classify open chromatin sequences according to cell type with
minimal to no additional data  furthermore  as we consider looking at more cell typesa
multiclass classification problem we see that the number of training examples will scale up
exponentially with number of cell types or organ systems included  whereas the number of
features will either remain constant or scale linearly 
so what can be done  perhaps we can try and be utterly unbiased look at just the sequence data and nothing else  in our supervised analysis we used a feature set of overlapping
  mer counts across sequences  we see that with linear models  this feature set performs
similarly and slightly worse that the tf motif feature set data  bias persists  there are
a few reasons why this might be the case  first  the feature set is still quite small     
features  and therefore the complexity of the model we might build is bounded  second  the
count matrix for   mers is less sparse than the tf motif matrix whose motifs are around
    bp long  this means there is more noise in the dataset lots of low values populating
the data with few outlier values  another issue with this feature set is that different features
are likely to correlate precisely because we are looking at all combinations of   base pairs in
a sequence  we saw with our pca analysis that reducing the dimensionality of the feature
space does not help us with our bias problem  but does help us make a slight improvement
in classifying the data accurately 
although our unsupervised feature detection results are quite poor  we will continue experimenting with different models and architectures to try and improve our results  we will
continue refining unsupervised feature learning methods  focusing on using tiled convolutional neural networks to boost prediction accuracy  we might begin to visualize features
detected and process them in an attempt to understand the kinds of motifs motif combinations that lead to specificity of gene regulation 
overall we see decent performance of our linear models  which improve when we have lots
of external metadata  as expected  histone marks and tf binding events best predict the
extent to which a sequence affects gene expression  but it is less clear what determines which
regions are open in which cell  models improve in accuracy when nonlinear combinations
of features are taken into account  which supports the hypothesis that combinations of dna
motifs lead to different regulatory outcomes  we definitely need more expression data to scale
that analysis  but for the question of specificity  its clear that the amount of genomics data
is scaling much faster than features given this  how do we eliminate bias  perhaps through
unsupervised feature detection and deep learning  though we were pretty unsuccessful in this
first and fast attempt  with our results  it seems like hacking the genome is possible  but
we have to watch out for bias in our models 
 

fi 

references

    erwin  genevieve d   rebecca m  truty  dennis kostka  katherine s  pollard  and john
a  capra        integrating diverse datasets improves developmental enhancer prediction 
arxiv preprint arxiv           
    smith  robin p   leila taher  rupali p  patwardhan  mee j  kim  fumitaka inoue 
jay shendure  ivan ovcharenko  and nadav ahituv        massively parallel decoding of
mammalian regulatory sequences supports a flexible organizational model  nature genetics
               
    kwasnieski  j  c   c  fiore  h  g  chaudhari  and b  a  cohen        high throughput
functional testing of encode segmentation predictions  genome research          oct  
         
    encode project consortium        a users guide to the encyclopedia of dna elements
 encode   plos biology        e        
    http   www cbil upenn edu tess
    heinz  sven  christopher benner  nathanael spann  eric bertolino  yin c  lin  peter
laslo  jason x  cheng  cornelis murre  harinder singh  and christopher k  glass       
simple combinations of lineage determining transcription factors prime cis regulatory elements required for macrophage and b cell identities  molecular cell                
    ngiam  jiquan  zhenghao chen  daniel chia  pang w  koh  quoc v  le  and andrew
y  ng        tiled convolutional neural networks  paper presented at advances in neural
information processing systems 

 
   

appendix
implementation

data parsing of genomics sequences was made simple by the bedtools suite  sequence
alignment was done on aws  amazon web services  ec  instances  machine learning
models were built in python  primarily using the scikit learn toolbox  sparse autoencoders
and neural networks were implemented in matlab using a combination of self written
code  functions from deeplearntoolbox  and code adapted from     
email namrata anand  gmailcom for ipython notebook with all results and code 

   

models

for supervised learning we use nave bayes  linear svm  and svm svr with gaussian
 rbf  kernel  regularization parameters were found by grid search 
     

sparse autoencoders

for unsupervised feature detection we use  stacked  sparse autoencoders and tiled convolutional neural networks  sparse autoencoders are trained in a greedy layerwise manner
for pretraining and then trained as a feed forward neural network with an additional softmax classification layer in order to classify test data  let w  k      w  k      b k      b k    represent
 

fiweight matrix and bias parameters w       w       b      b    for the kth autoencoder in a stack of
n autoencoders  the encoding step of the sa is given by
a l    f  z  l   
z  l      w  l    a      b l   
and the decoding step is given by
a n l    f  z  n l   
z  n l      w  nl    a n      b nl   
the activations in a n  give us a representation of the input data in terms of higher order
features  we can subsequently use these features in a linear model to classify test data 
     

tiled convolutional neural networks

tiled convolutional neural networks are convolutional neural nets that allow layers to
have a tiled pattern of weights adjacent hidden units do not need to share weights  this
is useful in this project because tcnns can encode translational invariance of features but
also preserve local context of information 

figure    tcnn architecture  from       tcnns have partially untied local receptive fields in first layer
with pooling across maps in second layer 

learning algorithm is a generalization of topographic ica  the tica network is a two
layer network where the first layer learns weights w and the second layer weights v are
fixed and represent the topological structure of the neurons in the first layer  given an input
pattern x  t   tica learns parameters w by solving
min
w

t x
m
x

pi  x t    w  v  s t w w t   i

t   i  

where pi are the units in the second layer which pool over local hidden units in the first layer 
m is the number of hidden units in the first layer  t is the number of inputs  and the layer
activations are square and square root  for the simple units and pooling units respectively 
for a more detailed treatment of the model see     
 

fi   
     

supplementary results
feature elimination on cell type specificity data

figure    pca analysis of   mer feature set for cell type specificity analysis  top left  projection of data
in the space of the first two principal components of   mer dataset      features   training done on     
sequences  testing on       bottom left  expected variance by number of components  right  linear svm
on first     components

we did not expect feature elimination to boost model performance since the models
already display high bias  but in the case of the   mer feature set  we expect feature
columns to be highly correlated  consider the occurrence of aaaa in a sequence  which
is likely correlated with occurrence of aaac or gaaa  less so with tcgc  there are
also less obvious correlations related to the evolutionary conservation of consensus sequences 
for example  the tata box  tataaa  is a dna sequence found in the promoter region
of approximately     of human genes  and as a result  we might expect the appearance of
tata to correlate with ataa  taaa  etc 
we performed pca on the   mer feature set  and plotted the explained variance versus
the number of principal components in figure    we see drop offs in variance around    
    and     components  we selected the first     components  grid search done across     
     and     components  and classified the data with a linear svm model  we found that
bias reduced to some extent  training error is low with few samples  but the model is still
not complex enough 
     

results for selected modelsexpression

see table   for svr results  see figure   for visualization of svm  rbf  model performance 
     

results for selected modelsspecificity

see figure   for visualization of svm  rbf  model performance 
  

fisvr  rbf 
parameters
results
feature set
c
 training r  test r 
tf motifs  homer 
  
     
     
     
histone marks tf binding  encode   
   
     
     
  mers
 
   
     
     
joint
 
      
     
     
table    results for prediction of sequence expression in k    cells with continuous output 
models were run with   fold validation  results for training on           sequences     
of the data   grid search for parameters c  gamma on svr with rbf kernel done over base
   logspace        data scaled to       and shuffled before training 

figure    results for prediction of sequence expression with svm with rbf kernel for joint feature set
 c             training done on      sequences  testing on      right  learning curve  left  confusion
matrix 

figure    results for prediction of sequence cell type specificity with svm with rbf kernel for joint
feature set  c            training done on      sequences  testing on       right  learning curve  left 
confusion matrix 

  

fi