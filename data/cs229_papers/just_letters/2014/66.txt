prediction and classification of cardiac arrhythmia
vasu gupta  sharan srinivasan  sneha s kudli
 gvasu  sharanms  skudli  stanford edu

abstract   cardiac arrhythmia refers to a medical condition in which heart beats irregularly  this
paper aims to detect and classify arrhythmia into
   different variants  a few popular techniques
from contemporary literature were implemented
namely naive bayes  feature selection  svm  random forests and neural networks a new approach
combining svm and random forests classifiers was
also implemented 

 

introduction

irregularity in heart beat may be harmless or life threatening  hence both accurate detection of presence as well
as classification of arrhythmia are important  arrhythmia can be diagnosed by measuring the heart activity
using an instrument called ecg or electrocardiograph
and then analysing the recorded data  different parameter values can be extracted from the ecg waveforms
and can be used along with other information about the
patient like age  medical history  etc to detect arrythmia  however  sometimes it may be difficult for a doctor
to look at these long duration ecg recordings and find
minute irregularities  therefore  using machine learning
for automating arrhythmia diagnosis can be very helpful  the project aims at using different machine learning algorithms like naive bayes  svm  random forests
and neural networks for predicting and classifying arrhythmia into different categories 

 

data set

the dataset for the project is taken from the uci
machine learning repository https   archive 
ics uci edu ml datasets arrhythmia   
csv file    information file   there are       rows  each
representing medical record of a different patient  there
are     attributes like age  weight and patients ecg
related data 

the data set is labeled with    different classes 
classes   to    correspond to different types of arrhythmia class   corresponds to normal ecg with no arrhythmia and class    refers to unlabeled patient the
data set is heavily biased towards the no arrhythmia
case with     instances belonging to class   and     instances being split among the    arrhythmia classes and
the rest    are unclassified    of the classes related to the
degree of av block do not appear in the data set  the
labels for this data set are obtained from cardiologists
and they are considered to be the gold model 
the main challenges in processing this data set are
the limited number of training examples compared to
the number of features  heavy bias towards the case of
normal ecg  missing feature values  about        and
feature values belonging to both continuous and categorical types 

 

data preprocessing

the original data contains columns with both missing
values and single valued columns having the same value
for all the patient records these columns were deleted
from the data set  the resulting data set contained    
instances and     features 

 

feature selection

we experimented with two different filter feature selection techniques  one of the reasons for using fewer features was the limited number of data records      compared to     features  this helps in avoiding overfitting and also gives insight into the important features
which have maximum correlation with the output lables
but minimal correlation among themselves 
in the first technique  we discretized all the continuous valued columns and then computed the mutual information i y x  between each feature and the output
label vector using the below formula   h refers to en 

fitropy   the scores  mutual information value  obtained
for each feature were then normalised to remove any biases that appeared due to discretization of the real valued
columns  this normalisation technique is suggested in
     features with higher scores were considered more
important  in this approach  we did not compute the
correlations between the features themselves  this technique was implemented in matlab 

shown are for two different cases  in the first one  the
training testing data was split           and   fold
cross validation was performed  in the second case  the
training testing data was split            and   fold
cross validation was performed  all the features were
used to train the model  both the test and train errors
are high  indicating that naive bayes is not able to capture the data distribution effectively  ineffective feature
discretisation may also be a contributing factor 

i y  x    score    h y    h y  x 
   score
score   
h y     h x 

table    naive bayes binomial classification
   
train test set size
       
       

our second approach was to use a matlab feature selection package named mrmr
http   featureselection asu edu 
software php  this technique selects the features
which have both maximum correlation with the output
labels and minimum correlation among themselves  it
also uses some advanced techniques weka package 
http   www cs waikato ac nz ml weka 
for discretizing the real valued columns  therefore  we
used the results from this second approach while implementing svm and random forests  the corresponding
error versus number of selected features curves are
shown in figures   and   respectively  the top features
extracted have column numbers near     to     and
    to     which correspond to average width and
amplitude respectively of q r s etc waves in channel
v  of ecg recordings 

 

train error
     
     

table    naive bayes multinomial classification
train test set size
       
       

   

test error
     
     

train error
     
     

svm

svm is effective in high dimensional spaces like the arrhythmia data set  first  mrmr feature selection was
performed  the data set was then split into        
between train and test respectively  since we are dealing with a skewed data set with small number of rows 
we employed bootstrapping to improve the performance
of the algorithm the train data was doubled in size using
random sampling  while making sure all the data points
in the original train data were represented atleast once 
to determine the type of kernel most apprropriate  the
svm model was built using polynomial kernels of varying degrees and a guassian kernel  the quadratic kernel 
resulted in a good model fit  minimizing the generalization error as can be seen in figure    this led us to the
inference that there were significant second order interactions among the feature variables in the design matrix 
figure   plots the generalization accuracy on the test
set with the number of top features selected  it can be
seen that the best accuracy is obtained with around    
features 
the confusion matrix in figure   shows that class  
and   were often being confused for one another  the
svm model was unable to predict class     which is
primarily believed to be because of the inherent ambiguity of the class   i e class    refers to a state of uncer 

models and results

the following subsections discuss and provide results
obtained with naive bayes  feature selection  svm 
random forests  neural networks and fusion of these
different techniques 

   

test error
     
     

naive bayes classifier

we implemented our own naive bayes binomial and
multinomial classfiers in matlab  this implementation
was performed without any feature reduction  results
obtained are given in tables   and    many of the feature
are real valued and so these were discretised individually into different levels  the results shown are with   
different discretisation levels  we aslo experiemented
with different number of discretisation levels from   
to    but the test errors were almost similar  results
 

fitain cardiac activity   additionally  due to the highly
bised distribution of classes  the model proved inefficient in predicting classes with low density  in specific
both classes   and   saw only one tuple in the test set 
the sheer lack of data  meant that there was no way to
build meaningful distributions of the features needed to
classify classes   and    to address the issue of misclassifying class    sinus tachycardia  as class    normal  
we used an anomaly detector 
anomaly detection   we treated the svm as a one
class classifier and separated all the data points from the
origin  in feature space f  in order to maximize the distance from this hyperplane to the origin  this results
in a binary function which captures regions in the input space where the probability density of the data lives 
thus the function returns    in the region  capturing
the training data points  and   elsewhere  on finding
figure    generalization error with polynomial kernal anomalies in the data set  we used our intuitive reasoning from the svm confusion matix viz  that class  s
degree for multinomial svm
were mostly misclassified as  s  hence we found the
anomalies which lied far from the data set  closest to
the origin and detected the points predicted by our svm
model as    we reclassified these states of possible sinus tachycardia as normal state  this helped improve
the accuracy to    

   

random forests

a simple decision tree gives good predictions when
there is a huge number of predictor variables like in the
this data set  early methods to construct decision trees
were unstable with small perturbations in data resulting
in large changes in predictions  random forests is an
ensemble classifier that consists of many decision trees
figure    svm generalization accuracy with number and outputs the class that is the mode of the classes outof features selected
put by individual trees  in this way  an rf ensemble
classifier performs better than a single tree 
figure   shows that the generalization error botttoms
out beyond selecting around top    features 
we followed the implementation in     and obtained
similar results  the data was split as shown in figure  
as train test         respectively  figure   shows the
training error for a single run  figure   shows the training error for a single run with the simple random sampling  srs  approach detailed in the      srs reduces
the bias in the class distribution and has the same effect
as bootstrapping 
finally  the random forest technique was applied
figure    confusion matrix svm with polynomial dewith         data split and bootstrapping as explained
gree   kernel
in the svm section      the resulting confusion matrix
 

fifigure    random forests accuracy with number of top
features
figure    classfication error for a single run of treebagger without srs

figure    illustration of data set division for rf
is shown in figure    overall generalization accuracy
was      

   

fusion of svm and random forests

svm with polynomial kernal of degree   and rf
method corresponding to   gave similar conf matrix
w r t classes            however a linear kernel svm
figure    classfication error for a single run of treebagwhich gives a lower overall accuracy classifies classes
ger with srs
          better  we believe the reason for this could be
because the separating hyperplane for classes          
was linear and the quadratic kernel was not able to segregate data space this way  hence we used a serial classfier consisting of rf and linear kernal svm which gave
us a generalization error of       or accuracy of       
the confusion matrix is as shown in figure   

   

hierarchical rf classifier

we also tried a new approach with random forest classification where instead of one we train two different rf
classifiers  the first one provides a binary classification
about whether the person has arrhythmia or not  then
we further sub classify the instances which are predicted
with arrhythmia using the second random forest classifier  using this approach  we obtained     generalisa 

figure    random forests confusion matrix

 

fitable    results summary for    class classification
algorithm
naive bayes
svm poly deg  
rf
rf   svm
pattern net
  level rf

test accuracy   
  
  
  
    
  
  

references
figure    rf   svm confusion matrix
    h  altay guvenir  burak acar  gulsen demiroz 
ayhan cekin a supervised machine learning altion error for just binary classification and     error for
gorithm for arrhythmia analysis  proceedings of
combined mutli classification  the error in this case is
the computers in cardiology conference  lund 
slightly more than what we obtain with a single multi
sweden      
class random forest classifier  this is probably because
the overall accuracy is limited by the accuracy of the     zift  akin random forests ensemble classifier
first level binary classifier  this technique could be imtrained with data resampling strategy to improve
proved further by using the srs strategy 
cardiac arrhythmia diagnosis  computers in biology and medicine                     

   

neural networks

    hall  mark a   and lloyd a  smith  feature
selection for machine learning  comparing a
we used pattern net from the neural netwrok toolbox in
correlation based filter approach to the wrapper 
matlab to distinguish between the    classes  pattern
flairs conference       
recognition networks are feedforward networks that can
be trained to classify inputs according to target classes 
    uyar  asl  and fikret gurgen  arrhythmia classithis gave a classfication accuracy of     
fication using serial fusion of support vector machines and logistic regression  intelligent data acquisition and advanced computing systems  tech  conclusion
nology and applications        idaacs        th
ieee workshop on  ieee       
the paper presents the implementation of a few techniques used by contemporary papers on the arrhythmia
    polat  kemal  seral ahan  and salih gne  a new
data set  we also implemented a serial classifier using
method to medical diagnosis  artificial immune
a fusion of linear kernel svm and rf which gave us a
recognition system  airs  with fuzzy weighted
generalization error of        this provides a marginal
pre processing and application to ecg arrhythmia 
improvement over the generalization errors reported by
expert systems with applications                 the papers we surveryed  the results are summarised in
    
table  
    rudokait margeleviien  dovil  henrikas praneviius  and mindaugas margeleviius  data classifica  future work
tion using dirichlet mixtures  information technology and control                      
a number of combinations of algorithms can be implemented in the hierarchical scheme  currently  we have
implemented one   level scheme with rf  we can expand this to add more levels and try it with other models 
also for the network implementation  we think bootstrapping may help improve the performance 
 

fi