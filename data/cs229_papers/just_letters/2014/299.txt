cs    

final report

december         

data driven modeling and control of an autonomous race car
ohiremen dibua  aman sinha  and john subosits
 odibua  amans  subosits  stanford edu
the remainder of this report is organized as follows  we discuss preprocessing methods and state space choices in section
   followed by results for regression in section   and path optimization in section    we conclude with a discussion of
our results  their implications  and open questions for further
research 

abstractthis paper explores the application of machine learning techniques to the modeling and control of a race car 
regression models are used
to simulate the vehicles dynamics more accurately
than traditional physics based models  and we achieve
the best generalization performance with bootstrapaggregated regression trees  we incorporate this simulation model into an approximate dynamic programming framework that attempts to outperform a professional human driver  we demonstrate successful
application of this approach on a     meter segment
of a racetrack 

 

 

preprocessing feature generation

regression after smoothing the data to remove the effects
of noise  we transform the data into the following state space 
body frame velocity components      yaw and pitch angle     
body frame angular velocity components      body frame accelerations      and the exogenous inputs of throttle  brake 
and steering positions      note that absolute position is not
needed in the dynamics  any indirect effects of position on the
vehicle were implicitly captured through other states  e g  accelerations implicitly capture the effects of hills   outputs are
the differences in the    dynamical states between consecutive
time steps 

introduction

the application of machine learning techniques to the control
of dynamical systems is an evolving field of artificial intelligence  stanfords dynamic design lab is currently developing control algorithms for an autonomous audi tts  nicknamed shelley  in this paper  we apply machine learning and
data driven techniques to the control of this car over a closed adp unlike with regression  position is relevant to adp becourse  the goal is to derive a control law that allows shelley
cause we must ensure that the car stays on the road  between
to outperform human drivers 
the lane boundaries  during simulation  position along the
data  objectives  and approach data was recently manifold of the racetrack is maintained in curvilinear coordirecorded from a professional driver completing    laps of nates relative to the tracks centerline 
thunderhill raceway at pace in shelley  our data consists
simulation models
of measurements taken with a high precision gps in con   
junction with an imu and signals taken directly from the
rather than simply try a wide variety of methods  we adopt a
car  together  they supply roughly         data points for
more principled approach  beginning with baseline methods 
   continuous signals encompassing the vehicle states and the
we iteratively consider sophistications to address key shortcontrols applied by the skilled human driver 
comings of model performance  we use standardized mean
our overall goal is to generate a minimum time trajectory and square error  smse  as our error metric  mse normalized by
associated control law over thunderhill raceway  one might training data variance   results for one step prediction  refirst consider the approach of abbeel et  al       i e  assume gression  and multi step predictions  simulation  are summathat the    laps are noisy versions of some desired trajectory  rized in table      for brevity  we only show results for preand proceed to infer this hidden trajectory  however  even dicting one step changes in longitudinal velocity  vx  t  
the professionals fastest lap is imperfect  and trials are not and multi step predictions of vx   the normalization parameequivalent noisy versions of an optimal trajectory  thus  the ters are different for regression and simulation smse values 
apprenticeship approach of     is inapplicable 
the former uses the variance of vx   whereas the latter uses
our approach is the following  we first build a simulation the variance of vx   thus  relative error magnitudes should
model of the cars dynamics and then apply reinforcement not be compared between regression and simulation 
learning to optimize trajectories and control laws  for the
first component  we apply standard regression techniques  for
the second component  we apply approximate dynamic programming  adp      in a novel way to take advantage of the
nearly optimal human trajectories and avoid unnecessary exploration 

   

baselines

we start with multivariate linear regression  mlr  and regression trees  rt   we assume markovian dynamics for these
models  i e  current outputs only depend on the current state
and current inputs   so after computing the outputs as differ 

 

stanford university

fics    

final report

    

table      smse errors for vx  t and vx

     

mlr
rt

    

    

december         

mlr
rt

    

model

     

    
    

    

mlr
rt
narx rnn
multrt
bagrt  
bagrt   
bagrt   
bagrt    

     

    
    

    

    

     

 
 

   

   

   

   

 

 
   

   

   

   

t

 a  test regression smse

   

   

   

   

   

t

 b  simulation smse

figure     a  errors in predicting vx  t vs  t for regression 
 b  errors in predicting vx via simulation with baseline models 

     

cumulative explanatory power

ences between states in consecutive time steps  we randomly
divide the data into     training and     test subsets 
time step and model selection

our first task in model selection is determining the time step
over which we compute changes in states  the original data
was recorded at    hz  but simulating at this level of detail was deemed to be too costly given our time constraints
as well as unnecessary to fully capture the dynamics of the
cars motion  instead  we test the regression models at time
steps ranging from     to     seconds in     second increments 
hypothesizing that the characteristic time scale for the cars
dynamics lies in the     to     second range 

regression
train
       
       
       
       
     e  
     e  
     e  
     e  

 vx  t 
test
       
       
       
       
     e  
     e  
     e  
      e  

 

 

   

   

   

 

   

   
   

 
   

   
 

 

 

  

  

  

number of components

 
   

   

   

   

 

 

  

   

 

   

two major observations should be noted about the regression
error  i e  the error in predicting the change in vehicle states
over a single time step  figure   a    the first is that mlrs
errors are worse than that of rt  indicating that we want to
use the latter  the second is that in the case of the regression
tree  the regression errors decrease with increasing time steps 
importantly  this does not imply that the largest time steps
should be used for simulation  as we show next 

  

 b   st component

 a  explanatory power of pca
 

simulation
 vx  
       
       
       
       
     e  
     e  
     e  
     e  

   

 
 

   
   

 
   

  

 c 

 

 nd

 

  

 
   

component

   

  

  

  

  

 

 

 

 d   rd component

figure    pca on standardized data   a  explanatory power of
components given by a normalized cumulative sum  the first  
components are sufficiently powerful   b    c  and  d  are histograms of standardized data projected onto the  st    nd   and  rd
components  a positive  negative  projection onto the  st   nd  
component roughly corresponds to left turns  while a vanishing projection corresponds to a straightaway  a large negative projection
onto the  rd component corresponds to strong deceleration 

we corroborate our regression analysis with simulation  i e 
multi step prediction  to make a more definitive conclusion 
we simulate the behavior of shelley over the course of one lap
in   second intervals  in contrast with the regression errors 
the simulation errors increase monotonically with time step
size  as shown in figure   b   according to the simulation error results  we choose t        and all models in table     use
this value  we can explain the dichotomy between regression
and simulation trends for rt as follows  the models trained
on large time steps ignore the fast dynamics of the car  and
as a result are able to easily predict the low pass filtered data
for one step prediction  however  ignoring these dynamics
in multistep predictions obviously yields drastic errors during
simulation  regression and simulation results indicate that
we select rt over mlr for further analysis 

improvement  this indicates that the poor simulation performance in turning regimes is largely due to other effects associated with turns  specifically  training data bias leads to high
model variance in turns  while high error autocorrelations lead
to poor simulations  we now analyze these issues 

   

training data bias

we suspected that the training data was biased towards regions devoid of turning  which would account for the relatively
high model variance in turning regimes compared to other re      turning issues
we found that the regression tree has exceptionally poor simu  gions  unsupervised learning algorithms verify this intuition 
lation performance on turns  and we also noticed high model principal component analysis on standardized data indicates
variance  overfitting  in key states such as lateral velocity that most of the datas variation lies in a   dimensional subsolely for turning regimes  attempts at forward backward space of input space  figure     the two dominant princifeature selection  specifically by adding features like tire slips pal components loosely correspond to turning dynamics  with
and removing noisy features like vertical velocity  yielded little strong and opposing magnitudes for lateral acceleration  yaw
 

stanford university

fics    

final report

december         

   
 

   
   

north  m 

   
   

   
   

 
    
    
    

   

   
   
   
   
road bounds

    

 
 

    

 

   

   

 

  

  

time step delays
figure    normalized test regression error autocorrelation intensity
for rt vs  time step delays 

east  m 
figure    data points assigned to the four clusters displayed on a
map of the cars trajectory through the racetrack  relative sizes of
clusters are displayed in the legend 

   

improved models

narx rnn in order to address issues of training bias
and error autocorrelation  we first consider a more complex
model  a nonlinear autoregressive model with exogenous inputs  narx model  in the form of a recurrent neural network
 rnn   the motivation for this approach is that including autoregressive components for the input can potentially reduce
error correlations  we consider an rnn with two hidden layers of    and    nodes respectively as well as   delays for both
autoregressive and exogenous inputs  i e  feedback and input delays   as indicated in table      performance improvements were minimal  although we could consider training
a deeper network  the training time overhead  about   hours
on   machines using the levenberg marquardt algorithm with
bayesian regularization  is prohibitive  even though we could
simply try other training algorithms for the rnn  we found
better results with more scalable methods 

rate  and steering angle  the third component distinguishes
strongly decelerating regions from accelerating regions  i e 
regions just before turns vs  everything else  the remaining
  dominant components constitute slightly perturbed forms
of the first three components  thus  we constrain ourselves
to the first three components for further analysis 

the three dominant principal components signify that there
are roughly   regions of data  right turns  left turns  straights 
and regions just before turns  clustering the reduced data
reveals exactly this partitioning  we use the k medoids algorithm  a variant of k means that is more robust to outliers 
figure   shows a map of the racetrack with data points along
the trajectory colored according to their respective clusters 
most importantly  we see that training data is indeed biased 
regions just before turns account for only     of the data 
and right left turns account about for         each  thus  multrt we next consider rt with different forms of engeneralization performance suffers in turns and regions just semble learning  the main idea is that rt  a nonparametric
before turns due to a lack of training data  and the resulting
method  should be able to capture enough model complexity
high test errors propagate during simulation 
to fit to data  so we simply need to improve its generalizawe also compared results of two nonlinear reduction algo  tion performance in problematic regions  ensemble methods
rithms with those of pca  kernel pca with polynomial and are well suited to reducing model variance without increasgaussian kernels as well as maximum variance unfolding  nei  ing model bias  we first consider a simple ensemble  we use
ther of these advanced methods provided further insights 
k means on the most important dimensions of the data  as
determined via pca  to separate our training data into sep    error autocorrelations
arate regimes  we then train separate rts on each subset 
high error autocorrelations between consecutive state predic  and compute a weighted average of each models predictions
tions lead to error propagation during simulation  such effects during simulation according to the relevance of the model to
are most evident when the error magnitudes are high  i e  the corresponding simulation regime  as shown in table     
along turns   these effects definitely violate the assumption this ensemble rt model  multrt  does not make significant
of error independence of models such as mlr  further mo  improvements  on the contrary  although it performs a bit
tivating our choice of rt  however  even rt shows severe better on turns than rt  it performs much worse on transiautocorrelation in delays of up to    time steps  figure    
tions between the turns and straights  the regions where we
transition between separate regimes  although we could simthere are three ways to combat error autocorrelation  inply create more training partitions  we found success with a
crease model complexity  reduce the magnitudes of errors to
minimize correlation impact  and model and manually com  related approach  bootstrap aggregation 
pensate for autocorrelated errors  we chose to attempt the bagrt bootstrap aggregation  also known as bagging  unifirst and second of these methods with improved models 
formly samples from training data to train multiple rts on
 

stanford university

fics    

final report

  

r s  in equation       however  to encourage some degree of
exploration around this path  we add a shaping cost f to the
reward in the potential form described by ng et al       under this adp scheme  the original human trajectory remains
optimal  but the simulation explores the state space around
this trajectory as it iteratively traverses paths  importantly 
this exploration generates perturbations to the original path
that are necessarily attainable by a sequence of actions  hence
they are denoted dynamically feasible perturbations 

 

training
test

    

   

log smse 

  
    

   

  

   

    

   
  
    

 
 

 

  

  

  

  

  

  

  

 

 

 a  regression smse vs  trees

 

 

 

  

  

time step delays

trees

 b  bagrt    error autocorrelation

since we treat our simulation model m   s  a  s as
deterministic  each state action pair  s  a  evolves to the state
m  s  a   then our adp framework is summarized as follows 

figure    model performance for bootstrap aggregated regression
trees  bagrt  

separate subsets of training data  predictions are formed by
averaging over the predictions of each separate tree  bagging reduces the magnitude of our regression errors  figure
  a    although these errors indicate that performance improvements plateau with only a few trees  we saw continued
improvements in error autocorrelation intensities with more
trees  as seen in figure   b    the characteristic time scale
for autocorrelation is only about   time steps for a bagged rt
with    trees  bagrt     which is half of that for rt  figure
    although this improvement seems minor  error autocorrelations have a significant impact on performance  even this
minor improvement coupled with decreased error magnitudes
leads to a   orders of magnitude decrease in simulation errors
when compared to rt  table       the obvious next step
is to try random forests  which also use random subsets of
features to train separate trees  this method can further reduce correlation between separate trees in the bagged model 
thereby further reducing model variance 

 

v  st      r st     f    max v  m  st   a  
aat

f   c  c
r s    max g s  spro  
spro


g s  spro     exp  s  spro  t    s  spro  
at    arg max v  m  st   a  
st      m  st   at  

where         is the discount factor  at is the set of
actions at time t  spro is a state in the human trajectory  
is a diagonal matrix of characteristic length scales  and c    
is a constant  whereby f        we initialize v  s     r s  
and every time we reach a pre determined  absorbing  state
signifying the end of the simulation  we simply restart at s   
finally  to further restrict exploration to reasonable states 
we consider at to be actions confined within a certain ball
of the action considered by the human at the state spro   
arg maxspro g s  spro   
iterative optimization we greedily optimize our path perturbations to exploit our exploration of the human trajectory 
first  we generate k perturbed trajectories from the human
trajectory  and we choose the best trajectory among these
k     trajectories  we then set this selected trajectory as the
new  human  trajectory from which to generate perturbations using equation      we continue the process as necessary
or until a local optimum is attained 

developing a simulation model as described in section   is
useful only if it can aid in optimizing shelleys path and associated policy beyond the human trials  to do so  we incorporate the simulation model into a reinforcement learning
framework 

adp with cost shaping

in conventional value iteration  the values v  s  for every state
must be updated in an iteration  either synchronously or
asynchronously   to maintain simulation accuracy  we cannot afford to discretize our state space too coarsely  so value
iteration quickly becomes intractable  similarly  fitted value
iteration was determined to be unnecessarily expensive for our
problem  as our human trajectories were known to be already
nearly optimal  rather  we choose to generate an optimal
trajectory by generating dynamically feasible perturbations
to the original human trajectory 

when simulating the whole track  the best of the k     trajectories is the one which completes the circuit in minimum
time  when simulating only part of the track  however  we
found the following to be more effective  we choose the trajectory that ends up the farthest along the track  as expressed
in units along the centerline  within a specified time window 

   

results

in our implementation of adp with cost shaping  there is no
need to discretize the state space s  we only discretize the
action space at   we experiment over a    m section along
the track that has a straightaway followed by a sharp left turn
and a soft right turn  figure   a  illustrates some of the path
perturbations during optimization  and figure   b  indicates

in adp  one steps through optimal states  performing a value
update for a state only upon visiting it   in our implementation  we start with a human trajectory and reward the simulation for following this trajectory as closely as possible  see
  q learning

     

aat

path policy optimization via reinforcement learning

   

december         

  in

is a special case of adp without a model for the system 

 

general  we can have c be a function of state c s      

stanford university

fifinal report

   

   

   

 

distance  m 

north  m 

cs    

   

   

   

of the policy  the component of the policy before feedback
augmentation   each contribution has the potential to improve shelleys performance compared to current methods 

   

 

 

    

    

    

 

    

 

 

  

east  m 

  

  

  

  

  

iteration

 a 

 b 

figure     a  illustrations of physical path perturbations in
east north coordinates  the start point is at the top right  note
that actual perturbations occur in s  of which east north position
is simply a   dimensional subspace   b  distance traveled beyond
the human trial in     s of simulation vs  iteration of optimization
scheme  a local optimum is reached at    iterations 

our regression simulation results illustrate an interesting
phenomenon regarding our data  because the data is biased
towards regions away from those involving turning dynamics 
naive regression models have a tendency to overfit in turning regimes  thereby corrupting simulation performance  our
best model  bootstrap aggregated regression trees  reduces
this model variance issue and also reduces error autocorrelations to provide reasonable simulation performance  the
next step along this path towards a higher fidelity simulation
model is to consider random forests and extremely randomized trees  another way to improve model performance is to
gather more data from turning regimes 

   
adp
human

   
   

north  m 

   
   

   
   

   

   

   

   

   

   

the main focus of the adp approach for path optimization
is computational efficiency  we have shown that it easily
sidesteps the computational intractability of naive value iteration algorithms  and we have demonstrated its usefulness
in determining optimized paths and optimized policies  we
can further improve performance by sophisticating our shaping cost to more effectively balance exploration with exploitation 

   

   
    

    

    

    

   
    

    

    

conclusions and future work

we have successfully developed a computationally efficient
framework to optimize shelleys racing performance beyond
the capabilities of human drivers  our approach includes two
major components  a simulation model of the cars dynamics 
and a path policy optimization scheme designed using reinforcement learning techniques 

   

   

december         

    

east  m 
figure    comparison of optimized adp path with the original
human path  the inset displays a zoomed in view of the left turn 
the differences may appear subtle  but a  m gain on every turn
can yield a lead of roughly   second on a course with    turns  in
shelleys current best autonomous performances  it loses the most
ground to this particular human driver in exactly these locations 

we believe that our approach can easily be incorporated into
current methods for autonomous vehicle racing at stanfords
dynamic design lab  these machine learning techniques are
the degree of optimality with each iteration  as measured by
a promising complement to traditional modeling and control
the distance covered beyond the initial human trial  we see
methods employed by the group 
that the local optimum reached after    iterations yields a
 m gain over the human trial during      seconds of simulareferences
tion  figure   compares the optimized path with the original
human trajectory  the optimized trajectory requires the car     abbeel  p   coates  a   and ng  a  y  auto brake later and deeper for the corner  speed at corner entonomous helicopter aerobatics through apprenticeship
try is sacrificed for increased speed at corner exit  yielding an
learning  the international journal of robotics research
overall greater distance traveled 
       
real world implementation when implementing this op      ng  a  y   harada  d   and russell  s  policy intimized trajectory in real life  the open loop optimal policy
variance under reward transformations  theory and apwill not always result in the car following the optimized path
plication to reward shaping  in icml         vol     
for a number of reasons  these include simulation accuracy as
pp         
well as environmental variables such as wind variability  road
conditions  tire wear  etc  to account for these factors  the     powell  w  b  approximate dynamic programming 
solving the curses of dimensionality  vol       john wiley
open loop policy can easily be augmented with feedback con  sons       
trol techniques  from something as simple as a standard pid
or lqr controller to a more sophisticated nonlinear modelpredictive control  mpc  scheme  this is currently how shelley drives along a pre determined trajectory  importantly  our
contributions are that we develop a dynamically feasible optimal trajectory as well as generate the feedforward component
 

stanford university

fi