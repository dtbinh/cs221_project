all your base are belong to us  identifying
english texts written by non native speakers
jonathan hung

joanna kim

stanford university
hungj stanford edu

stanford university
joannak stanford edu

i  i ntroduction
english is one of the most prevalent languages worldwide  third only to mandarin chinese and spanish  with
millions of people learning english as a second language 
it is a worthwhile endeavor to improve their experience
by creating learning tools customized to them  for our
project  we created a classifier that can take raw english
text and identify the writer as either an english native
speaker or an english learner  furthermore  we expanded
the classifier so that it could identify the writers native
language given a raw text file  this classifier could
become an important basis for a learning tool  i e  an
editing tool that corrects and gives suggestions specialized to the writers native language  that can help esl
students gain a better grasp of the english language 
ii  data
our data is in the format of raw text files that have
been written by both english native speakers and nonnative speakers  weve drawn our data from one online
corpus called the icnale      the international corpus
network of asian learners of english  which has      
short answer essays written by esl students from countries ranging from china to singapore to pakistan for a
total of    different asian countries  it also contains a
smaller set of essays written by english native speakers
from the united states  united kingdom  australia  and
new zealand  the exact counts of essays per country
are enumerated in table i  these essays are responses
to two specific questions  the first talking about parttime jobs in college  and the second about smoking in
restaurants   all the essays are short in length with only
approximately   to    sentences each 
research shows that many of the errors that esl
students make can be correlated to the structure of
their own native language      thus  weve decided to
divide and conquer and focus our attention on errors
and features specific to a defined sector of languages

country
english speaking  ens 
china  chn 
hong kong  hkg 
indonesia  idn 
japan  jpn 
korea  kor 
pakistan  pak 
philippines  phl 
singapore  sin 
thailand  tha 
taiwan  twn 

count
   
   
   
   
   
   
   
   
   
   
   

table i  number of essays per country
that have similar structures  eventually  we would like
to expand to esl learners based in other languages  like
the romantic or germanic languages 
iii  f eature s election
for our three models  logistic regression  naive
bayes  and a markov model with n grams   we used
different sets of features because we were interested
how they would perform given different features  we
used three different types of features  frequently used
in native language identification  nli   features by
grammatical cues  features by frequency of words  and
features by parts of speech n grams 
for logistic regression  we focused on features
that would detect grammatical syntatical cues common
among non native speakers  we have two sets of syntactical features  one set that required minimum manipulation of the text to extract the features while the
other required syntactic parsing to extract grammatical
features  our first set has three features  sentence length 
misspellings  and the repetition of words   for our second set  we used the stanford parts of speech tagger
to label words by their part of speech  e g  noun  verb 
adverb  etc   and then created twelve different features by
parts of speech  we chose these features because  though

finot the most common  they are relatively common cues
of non native speakers and they can be easily be derived
from raw text  a good future extension might be to
expand the feature list to features that are statistically
more common among non native speakers  errors with
articles and prepositions  the presence of fragments and
run on sentences  awkward or missing diction  but that
require the use of the stanford parser to understand their
grammatical context     
for naive bayes  we used features at the granularity
level of words   for each word  we count its frequency
and use these as our features  we omitted words that
occurred in total fewer than three times  since the sparseness of these words would not add any predictive power 
and it also drastically reduced our data size 
for n gram markov model  we labeled each word
with the stanford parts of speech tagger and created
features based off of strings of   grams 
iv  m odels and r esults
a  non native english classification
our first two models  logistic regression and naive
bayes  focus on binary classification  given a raw text of
english  they aim at classifying if the writer is a native
or non native speaker 
   logistic regression  we used our logistic regression model to test all the syntactical features discussed
in the feature selection section  using a    dimensional
feature space  we used the following hypothesis in our
regression 


 
    
h  x     
    exp t x 
we trained  with batch stochastic gradient and a
learning rate  of         over a training set of size
      we then tested  over a test set of size      both
sets were a mixture of asian language native speakers
and english native speakers  for our results  we define
error as follows 
m
  x
 
  y  i     h  x i     
m
i  

   naive bayes  with the hypothesis that english
native speakers might be inclined to use words that
non native speakers wouldnt and vice versa  we implemented a naive bayes algorithm that finds the probabilities that a specific essay would be written given that the
writer is an english native speaker or a learner  then  the
algorithm categorizes the essay by the higher probability 
we used a multinomial event model which is suited for

text classification and laplace smoothing to account for
words that were not in the data 
we trained our algorithm on a set of      samples and
tested on a separate set of     samples  both sets were
mixed with asian language native and english native
writing samples  similarly to how we calculated error
for our logistic regression model  we calculated error as
follows 
m

 

  x
  y  i     h  x i     
m
i  

b  language classification
we next turn to the problem of classifying a text by
the country of origin of the writer  for example  can we
identify that a text comes from a chinese writer versus
from korean writer 
to do this  we implement a markov model using ngrams as our states  for our results  we use n      the
methodology is as follows 
we first convert each of the essays in our training
data to a list of parts of speech using stanfords parts
of speech tagger      for example  the sentence this
is a paper would be converted to  determiner  third
person verb  determiner  singular noun   we then take
consecutive   sequences of parts of speech  and count
the frequency of each   sequence in all of the training
essays for a language of origin  thus  each language has
its own model of parts of speech frequencies  then  for
each essay in our test data  we find the likelihood of the
sequence of parts of speech from that essay appearing
in each language based on our models  the prediction is
the language that results in the highest likelihood 
v  r esults
a  non native english classification
   logistic regression  our error for our training set
was       while our error for test set was        our
test set converged after        iterations  a reasonable
amount given our tiny learning rate  our model did
surprisingly well  considering that the features chosen
were not the best indicators of non native speakers 
ns
nns

ns
  
 

nns
  
  

table ii  confusion matrix for logistic regression
table ii shows our results for logistic regression 

fi   naive bayes  for the training set error  we found
a very accurate       error rate  and for our testing
set error  we have a      error rate  with the confusion
matrix shown in table iii 
ns
nns

ns
  
 

nns
 
  

table iii  confusion matrix for naive bayes
b  language classification
using the markov model described previously  we
achieved a training error of       and a test error of
       with the confusion matrix shown in table iv 
in addition  we plotted accuracies and recalls for each
language in figure   and figure   

fig     accuracy by country

we were even more successful with naive bayes  our
results mirrored the research that implied that semantic
cues   such as the diction that our naive bayes algorithm
measures   are excellent indicators of learners writing
    
the results from the markov model were particularly
interesting  using a basic metric such as consecutive
parts of speech  we were able to build reasonably good
models for what the text from a writer of a certain
country looks like  it is also notable where some of the
misclassifications occur  for example     korean essays
were misclassified as japanese  this can potentially
explained by the fact that these languages are influenced
by chinese  so their writers might have similar writing
patterns  also  many taiwanese essays      were misclassified as chinese  which makes sense since people
in taiwan speak mandarin 
the accuracies and recalls by country also reflect some
of these patterns  for example  taiwanese recall is very
low  considering that many of them were classified as
chinese  a country that speaks the same  mandarin  or
similar  taiwanese  language  in addition  hong kong
had low accuracy and recall  possibly due to the lack of
data in relation to the other countries  so we could not
build as good a model for it 
finally  we see both high accuracy and recall for
pakistan  this is possibly due to the fact that pakistan
is unlike the other languages in grammatical structure
and diction  and thus  it was less likely to be confused
with other languages and more likely to be accurately
labelled 
vii  f uture p lans

fig     recall by country
vi  d iscussion
we were happy with our results from all three models 
logistic regression worked well considering some of our
features were basic features of text  note that our nonnative speaker error was             while our native
speaker error was                the discrepancy is
likely due to the fact that we had much more non native
speaker data  so we were able to build a better model
for it 

there are a number of ways to expand and improve
on our current models through a number of ways  for
one  we would like to put our models through more
rigorous tests that have a greater variety in both the
native languages of the writers and the topics of the
essays  we would also like to add more features that
require more grammatical parsing but are very common
among non native speakers  additionally in the near
future  we would like to add another algorithm that uses
character n grams and string kernels to categorize texts
by their writers native language  n grams by character
would allow the classifier to abstract away language
features like parts of speech  diction  and syntax  thus 
the classifier could easily used for languages other than
english since it would not be based on linguistic structure 

fichn
ens
hkg
idn
jpn
kor
pak
phl
sin
tha
twn

chn
  
 
 
 
 
 
 
 
 
 
  

ens
 
  
 
 
 
 
 
 
 
 
 

hkg
 
 
 
 
 
 
 
 
 
 
 

idn
 
 
 
  
 
 
 
 
 
 
 

jpn
 
 
 
 
  
  
 
 
 
 
 

kor
 
 
 
 
 
  
 
 
 
 
 

pak
 
 
 
 
 
 
  
 
 
 
 

phl
 
 
 
 
 
 
 
  
 
 
 

sin
 
 
 
 
 
 
 
 
  
 
 

table iv  confusion matrix for markov model

r eferences
    heilman  michael j   kevyn collins thompson  jamie callan 
and maxine eskanazi  combining lexical and grammatical
features to improve readability measures for first and second
language texts   n d    n  pag  web 
    ishikawa  dr  shinichiro  icnale  the international corpus
network of asian learners of english  n p   n d  web     nov 
     
    korean learner corpus blog    korean learner corpora  n p  
n d  web     nov       
    leacock  claudia  martin chodorow  michael gamon  and joel
tetreault  automated grammatical error detection for language
learners  n p   n p   n d  web 
    http   nlp stanford edu software tagger shtml

tha
 
 
 
 
 
 
 
 
 
  
 

twn
 
 
 
 
 
 
 
 
 
 
  

fi