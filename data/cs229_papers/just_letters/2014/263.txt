supervised deep learning for multi class image classification
xiaodong zhou  xdzhou stanford edu 

abstract
multi class image classification is a big research topic with broad application
prospects in artificial intelligence field nowadays  this course project describes the
supervised machine learning methods  convolutional neural networks  a k a  cnns 
along with softmax logistic regression  to perform multi class image classification and
implement these deep learning algorithms on a large scale multi class image
classification dataset from imagenet annual competition task      the implementation
categorizes various images by visual features and shows illustrative examples of the
training performance 

  introduction
multi class image classification refers to an
image classification task with more than two
classes  in other words  one image needs to
be labeled by one category from a set of
categories based on analyzing the
numerical properties of various image
features and organization information  for
example  to classify a set of animal images
of cat  dog  fox  and pet  the multi class
image classification makes assumption that
each image only could be labeled by one
and only one class label  ground truth   that
means the animal could be assigned to
label  cat  or  pet   but never be both  this
technique is widely used in multiple areas 
such as producing thematic maps  textured
image recognition  etc 
moreover  the image is a complex context
and it contains a complex arrangement of
pixels  some sub regions of pixels also
could be considered as objects which have
special meanings to human being in the
image     the deep learning is the effective
learning method for analyzing image which
generates features by multiple sub sampling
layers 
as stated on the imagenet website  the
imagenet is an ongoing research effort to
xiaodong zhou  xdzhou stanford edu 

provide researchers around the world an
easily
accessible
large scale
image
database  this research community also
keeps posting ai related challenges  such
as image classification  image localization
and object detection  on its website through
     till now      in order to master the deep
learning models  this project chooses the
classification task and images from the
imagenet since it is a typical multi class
image classification problem 

  dataset
the dataset   from ilsvrc        is
chosen as the dataset for this project  the
training data is the subset of imagenet
which contains      categories and more
than     million images  the number of
images in each category  synset  ranges
from     to       the validation data is a
random subset of        images which
doesn t contains any training data    
images per synset  with ground truth labels 
all images are in jpeg format 
however  considering the restriction of
hardware and software resources  we
randomly pick up    out of       categories
from the training set and validation dataset
       images  to implement the deep
learning algorithms  we modify the labels of
 

fithese training images and validation images
from   to    respectively based on their
original
category
information
and
relationship  we call the dataset as
ilsvrc     r   

  methods

k  x    represents the amount of overlap
between them  convolution is often used for
filtering images with a kernel  it detects
gradients in the values of pixels  sudden
changes in brightness   which correspond to
edges     

intuitively  the convolution of two functions
 input function f  x  and the kernel function

figure  edge detector  n dimensional convolution function  convn  with input image
          and randomly initialized kernel function     matrix filter 

    convolutional neural networks
 cnns 
the cnns is a biologically inspired  locally
connected  typical mlp  multilayer
perceptron  a k a  artificial neural network ann  and is sensitive of sub regions of the
visual field     

in this project  the pipeline of cnns model
consists of   layers    convolutional layers 
  max pooling layers  and   fully connected
mlp layer with softmax regression  the
output of each layer is the input of next
layer 

figure   pipeline of cnns model    layers  input  rgb image with        resolution 
output  probabilities of classes 

xiaodong zhou  xdzhou stanford edu 

 

fisparse connectivity
the cnns is locally connected mlp  the
inputs of hidden units  neurons  in layer i
are from a subset of units in adjacent lower
layer i      the neurons which have spatially
contiguous receptive fields  we use
convolutional filters with    receptive
fields  the number of hidden units is dataset
driven and depends on the complexity of the
input distribution  we construct a large
number of hidden units since the input is
high resolution color images 
let d denote the size of input x and l
denote the size of output f  x    formally 
with weight matrices w      w        bias vectors

b      b        and activation functions g and g  
for input x and output f  x  of each layer
mlp  the function f   r d  r l is 

h  x   g  b      w     x  across sub regions of
the entire image  in order to yield faster
training and better local minima  we use
function tanh as activation functions g  
where tanh  z   

exp  z    exp   z  
 
exp  z    exp   z  

we randomly initialize the tanh activation
function
results
interval
to
be





 
 
fanin  fanout


 
 for
fanin  fanout 

each layer  where fanin is the number of
input features  input maps  filter height 
filter width  and fanout is the number of
output features  output maps  filter height
 filter width  pooling size  
max pooling

h  x   g  b      w     x   

max pooling is a form of non linear downsampling and reducing the dimensionality of
intermediate representations  we use a  x 
region for max pooling  scale      

the output is obtained as 

stochastic gradient descent  sgd 

o  x   g b

we perform stochastic gradient descent
with fixed batch size  i e      for training
input images  sgd estimates the gradient
per batch at a time from the training
examples instead of the entire training set  it
reduces variance in the estimate of the
gradient and proceeds more quickly 
we test different batch sizes to tweak the
other parameters and discover the better
training performance 

f   x   g b       w        g  b      w     x   
the hidden layer is constituted as 

    

 w h  x    
    

shared weights
each convolutional filter is replicated across
the entire visual field  these replicated units
share the same parameterization  weight
matrix w and bias vector b   and form a
feature map  we initialize the weights w
randomly from a uniform distribution in the



 
  
 
   where nl is
n

n
n

n
v
l
v
l 

the number of labels  and n v is the number
range 

loss function   back propagation algorithm

of output neurons at the last layer  the size
of matrix w is   nl   n v   

in the case of multi class logistic regression 
we apply the back propagation algorithm to
obtain the gradients     for learning
optimal model parameters 

feature map

   w        b       w       b         minimize negative

a feature map is obtained by repeated
application of a function
xiaodong zhou  xdzhou stanford edu 

log likelihood  loss function  

 

fito learn optimal model parameters
smoothly  we try different modifications of
the loss function 
learning rate
we setup constant learning rate  i e       
and test different learning rates for each
layer since gradients at the lower layers are
smaller and less reliable 

    softmax regression
we construct the last layer as fullyconnected mlp with hidden layer and
logistic regression  and the set of all
features maps at the layer as input  we use
softmax regression     way softmax  as a
classifier since the verification method
includes    categories  classes  
softmax logistic regression is a linear
probabilistic classifier  it is parameterized by
a weight matrix w and a bias vector b     
the probability that an input x belongs to
class i is as 

p  y  i   x w   b  

imagenet
categories
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
   
   
   
   

images
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      

assigned
classes

descriptions  words 

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  

mask
rubber eraser
house finch  linnet
mountain bike
cellular telephone
sea slug  nudibranch
bison
fiddler crab
pizza  pizza pie
long horned beetle
ice cream
great white shark
dough
radiator
brambling  montifringilla
great dane
irish terrier
gila monster
basketball
bubble

table   training dataset  ilsvrc     r  

    training error
the training error result of the algorithm is
showed in the figure below 

exp wi x  bi  
 exp w j x  b j  
j

y  arg max p  y  i   x w   b 
i

  result
    preprocessing the data
in the implementation program     out of
      categories images are randomly
chosen from the training set and validation
dataset        images   read into octave 
rescaled with down sampled to a fixed
resolution of    x   x   imresize   and
saved as input with  mat format  we modify
the labels of these training and validation
images from   to    respectively based on
their relationship of original category
information  the summary information about
training data set  ilsvrc     r    is
listed in table   

xiaodong zhou  xdzhou stanford edu 

figure   training errors of the cnns model

the valuation result is list in table   
model
generational error
cnns
     
cnn ilsvrc        
     
table   error rates table

 

fi    discussion
we received acceptable training error rates
from applying the cnns and softmax model
on the multi class image classification
problem  however  comparing with the
results of the  st place of imagenet     
competition in the table    the classifier has
higher testing error than expected 
we developed the algorithm with opensource software octave and tested it on a
small linux server with a dual core cpu 
due to the array size limitation of octave
and the hardware memory availability  we
are only be able to construct   layers neural
networks to process    out of     
categories data set and downsize all the
images to resolution    x   x   it takes
long time to read      hours  and train    
hours  the large data set  we believe the
results should be improved if we can run the
implementation on a higher performance
cpu gpu system with bigger training
dataset 
due to the time consuming of the largescale image dataset  we do not have
enough time to tune the algorithm variables
of our cnns and softmax regression to
lower validation errors and improve the
running performance  we also found that
the return of the loss function is not smooth
for the high resolution images trained by
deep neural networks  we would like to find
a better modification to update the loss
function and smooth the result 

we validated our algorithm based on top  
error which compares the output label of
highest probability with validation ground
truth  most of the images in the imagenet
dataset contain multiple objects  the
complexity of large scale high resolution
images is a quite challenge to classify them
accurately and consistently  it would be
helpful to obtain better classification
accuracy if we integrate object detection
with image classification  but it will be more
expensive for the large neural networks  in
the future  we would like to apply some
efficient version of model combination  such
as drop out     
we also tested our convolution neural
networks on both rgb and grayscale
 rgb gray  with same dataset  the results
do not have much difference 

  future works
in the future  we will try to integrate object
detection and dropout models to continue
working on improving the performance of
the
multi class
image
classification
problem 
we would also like to apply the probability
model  latent dirichlet allocation  on the
sift features  bag of word  released by
imagenet to provide suggestions to cnns
based on the semantics foundation and
discover the relationship between features 

references
    olga russakovsky   jia deng   hao su  jonathan krause  sanjeev satheesh  sean ma  zhiheng
huang  andrej karpathy  aditya khosla  michael bernstein  alexander c  berg and li fei fei       equal
contribution  imagenet large scale visual recognition challenge  arxiv                 
    david g  lowe  distinctive image features from scale invariant keypoints  international journal
of computer vision       
    ng  andrew  jiquan ngiam  chuan y  foo  yifan mai  and caroline suen  ufldl tutorial  ufldl at
stanford university  n d  web     dec       
    alex krizhevsky  ilya sutskever  geoffrey e hinton  imagenet classification with deep
convolutional neural networks  nips      
xiaodong zhou  xdzhou stanford edu 

 

fi