cs    project final report  bharat arora  roger davidson  christopher wildman

down and dirty with data
introduction
the purpose of this project is to predict five soil properties from spectral data and other features as part of
the africa soil property prediction kaggle challenge  the method will be used to predict the viability of
land using a low cost measurement technique 

dataset
the five target variables are calcium  ca   phosphorus  p   ph  soc  soil organic carbon  and sand 
the training set consists of       features and       training examples  which are labeled with a
conventional chemical soil test  nearly all         of the features are ir spectral absorbance by
wavelength  all values are continuous except for one feature  the topsoil subsoil indicator  the test set
contains     examples 

features and pre processing
feature selection
the large number of features makes the data prone to overfitting and computationally challenging  we
used filter feature selection and forward search to find a subset of features that are most relevant 
for filter feature selection  we calculated the pearson correlation coefficient of each feature
against each of the five target variables  and then highlighted the top    of positively and negatively
correlated features  for each identified spectral band  we selected the most highly correlated feature and
    features on each side of this peak to carry information about the width of the band 
forward search was used with k fold cross validation and a linear regression model with the
number of features determined by minimum test error  figure     however  forward search is dependent
on the order that features are selected  for example  we obtained lower test error when we initialized the
first two features to be the highest positive and lowest negative pearson correlated features 
unfortunately  the best results from forward search did not exceed those of filter feature selection 

figure    forward search performed on ca using linear regression 

pre processing
the first and second derivative of the frequency spectra were taken to perform baseline corrections
 chen and wang   peaks in the transformed spectrum data  figure     which are the compressed
encoding of the original spectra  were then put through forward search  villmann et al   principal
component analysis on the transformed spectral peaks was also used to compress the data  chen and
wang  

december         

page  

fics    project final report  bharat arora  roger davidson  christopher wildman

figure    second derivative of frequency spectra with peaks selected  shown with circles   the
colors correspond to the value of calcium for each training example 

models and results
error metric
mean columnwise root mean square error is the metric used to measure performance on all five of the
target variables  the metric is simply the average of the root mean squared error of the five targets 
  





 

 



  
     



linear regression
our first submission for kaggle scoring used the features selected via filter feature selection  we used
linear regression  because chemical concentrations are proportional to absorbance spectra according to
beers law  chen and wang   the kaggle score was         which would have ranked us     of the
     teams that competed  for comparison  the winning score was          and the all zero benchmark
has a score of         

weighted linear regression
next  we used weighted linear regression to improve upon the prior results with a single value of the
bandwidth parameter    for all target variables  the feature set was unchanged  weighted linear
regression improved results with an optimal value of         table    
test error
 kaggle score 
   
       
       
   
       
       
   
       
       
table    results of weighted linear regression for varying  values
bandwidth   

training error

optimization of bandwidth parameter
next  we optimized  in our weighted linear regression for each individual target  we optimized first using
the training set  and then further tuned the value of  for some target variables using the test set on
kaggle  the optimal  values by target were ca        p       ph        soc        and sand       

december         

page  

fics    project final report  bharat arora  roger davidson  christopher wildman
from plots  it seems that at least two of the target variables  p and ca  may have power law or
other highly nonlinear distributions  in addition  target variables may be better predicted by some
polynomial power of a set of features  to address these issues  we shifted the values of the training set
data by   to remove the negative values  and performed forward search feature selection using ln x    to
predict ln y    with linear regression  this produced the best linear regression model with a training error
of         and test error or          our best weighted linear regression model used the optimized  for
ca and sand and then re optimized bandwidth parameters for p  ph and soc based on the log
transformation just described  this ensemble model had a training error of        and test error of
         the training and test errors for all linear regression models are summarized with all other
results in figure   

other regression methods
linear regression was performed on the peaks of data that had undergone first and second derivative
transformations  the first derivative transformation performed better than the second derivative  figure
    its training error was smaller than linear regression on the untransformed data  but the test error was
similar indicating overfitting  figure    however  forward search performed on the peaks led to a larger
test error than using all the peaks  principal component regression  pcr  on the transformed derivative
data performed poorly  test error           because the principal components of the features may not
explain the variation in the target variables  therefore  we used partial least squares regression  pls  
which finds the directions in the feature space that best explains the variance in the target variables
 chen and wang   pls  test error          outperformed pcr  but it did not beat linear regression 

regression tree models
we implemented regression trees for each target variable  allowing matlab to select any features from
the complete set  the corresponding test error results were poor indicating overfitting by the full trees 
but there was not time to optimize the pruning  however  we did use the decision tree results to inform
whether to split the training set  and models  based on the one classification feature  topsoil subsoil  
one decision tree  soc  showed significant dependence on this feature  so a dual model for soc was
constructed  one to predict soc for subsoil examples and another for topsoil  forward feature selection
was used for each sub model and the log transform was used with linear regression for prediction 
however  the more detailed model for soc did not improve upon prior results 

neural network models
we implemented a feed forward model for a neural network as a simple machine learning algorithm to
reduce error and model bias  madden and ryder   we created a neural network for each target variable
and trained it using the levenberg marquardt algorithm  table   highlights some of the many experiments
we ran to determine the optimal parameters  we experimented training the networks using all the
features  first and second derivatives of the spectral data  only the pearson correlated features discussed
above  and other feature subsets that included handpicked spectral bands  the intent was to find a
minimal set of features that performed well so we could reduce the degrees of freedom and avoid a local
optima  in the end  our best result was trained using all features  we also experimented with the number
of neurons and the transfer function used for each targets neural network  these results are omitted   the
minimum training set error proved not to directly indicate the optimal neuron and transfer function
parameters for the test set  the neural networks yielded the minimum error for phosphorus  our problem
target  which is why it was more successful than the linear models 

neural network   weighted linear regression ensemble
we combined the neural network model for phosphorus with the weighted linear regression models of the
other targets  this yielded our best test error at         and kaggle rank     

december         

page  

fics    project final report  bharat arora  roger davidson  christopher wildman

experiment

neurons
per target

transfer
function

tune ca to   neuron

           

tansig

    

     

band reduced   neurons

           

tansig

    

     

fine band reduced

           

tansig

    

     

             

tansig

    

     

                

tansig

    

     

              

tansig

      

     

logsig transfer function

           

logsig

    

     

first derivative features

           

tansig

    

     

tune to best    neurons
band reduced   
pearson feat  custom
neurons

features
per target

test
mcrmse

table    results of selected neural network models 

figure    model and feature selection comparisons for the project overall 

discussion
the linear regression models were able to predict all of the target variables with a rmse below    
except for phosphorus  p   which has a rmse of approximately    figure     the training and test error
are similar for p which indicates a bias problem  phosphorus in the training set has a small number of
high values that dominate the error  since the error is squared in the rmse calculation  other kaggle
teams achieved better results with linear regression models by removing these high values from the
training data  neural networks predicted p better than other methods  because p may be a nonlinear
function of the given features  the improvement of the log transformation further supports this
hypothesis  for the other output variables  the test error exceeds the training error by less than      
this indicates a slight bias  which can be corrected by decreasing the number of features 

december         

page  

fics    project final report  bharat arora  roger davidson  christopher wildman
filter feature selection performed better than forward search  likely because forward search tends
to find local optima  forward search is dependent on the order that features are selected  for the first
derivative peaks  the test error actually increased when using forward search instead of all the peaks  in
filter feature selection  the simplification of the spectral bands to a highly correlated feature and two other
features decreased the number of redundant features to reduce overfitting  even though forward search
used k fold cross validation  forward search still overfit the data more  for simple linear regression  the
difference in test and training error was      for forward search versus      for filter feature selection 

figure    learning curves for linear regression with the log transform ln x     ln y    

conclusions and future work
we used more than four techniques on this problem and performed well against the kaggle competition
on a problem of significant human importance  if we had another six months to explore this project  we
would continue the initial work done with regression trees  although we did implement the matlab
regression trees for all target variables  we did not have time to optimally prune the regression trees  we
would also improve feature selection with a genetic algorithm to learn the most significant features by
avoiding local optima  last  we would use a broader ensemble of methods as the winner of the kaggle
competition did 

references
j  chen and x  wang   a new approach to near infrared spectral data analysis using independent
component analysis   j  chem  inf  comput  sci  vol      pp                 
h  a  chipman  e  i  george  r  e  mcculloch  june         bart  bayesian additive regression trees 
available  http   www old newton ac uk preprints ni      pdf 
m  madden and a  ryder  machine learning methods for quantitative analysis of raman spectroscopy
data  opto ireland  vol        pp                  
t m  mitchell  decision tree learning and artificial neural networks  in machine learning  mcgrawhill        pp         
t  villmann  e  merenyi and u  seiffert   machine learning approaches and pattern recognition for
spectral data   in the european symposium on artificial neural networks  bruges       

december         

page  

fi