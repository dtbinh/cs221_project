peer lending risk predictor
kevin tsai
kevin     live com

sivagami ramiah
sivagamiramiah yahool com

sudhanshu singh
ssingh leo gmail com

abstract
warren buffett famously stated two rules for investing  rule     never lose money  and rule     never forget rule    
recent peer lending opportunities provide the individual investor to earn an interest rate significantly higher than that
of a savings account  however  a default on the loan by the borrower means the investor will lose her entire principal 
in this paper  we will use machine learning algorithms to classify and optimize peer lending risk 

introduction
individual investors who prefer fixed income assets are faced with
extremely low yield in the recent years  bank accounts pay less than one
percent  and treasury bonds pay low single digit percentages  at the
same time  consumer debt interest rates have remained high  with
unsecured consumer debt such as credit card rates at over twenty  and
sometimes thirty percent  this has created a new space where peer
lending companies match individual investors who are looking for a
higher yield with borrowers who are looking for a lower interest rate 
lendingclub com  prosper com  and upstart com are examples of such
companies  the loan process starts with the prospective borrower filling
out an application online  stating reason for the loan  the loan amount 
employment and income  and a battery of other information  there is
usually a vetting process by these companies which also includes a risk
grading  and then the loans are made available to investors  once a loan
has attracted enough investment dollars  it is funded  these lending
companies make an upfront through a fixed percentage discount point s  
a given portion of these borrowers will be late in payment and possibly
even default on their principal  these lending companies state they will
perform their due diligence to recover money from loans that are in
arrears  however  because any loss is borne solely by the investor  it is
imperative that the investor carefully select the investment opportunities
so as to avoid default risk while maintaining a healthy return 
while this paper will use data publicly available from lendingclub com 
this analysis can apply equally to other fixed income  fixed term
investment with feature data  lendingclub  like other peer lending
companies  provide some form of risk grading  which usually rises as the
loan interest rate rises  the goal of this paper is to reduce exposure to
loan defaults and exceed lendingclubs return given the same risk level 

data description
the lendingclub data used in this paper spans years      through      
table    interest and default rate per grade
loan
grade

method
because our primary goal is to not lose money  our optimization will
focus on severely discriminating against loans with potential for default 
meaning we will strongly favor a loan classified as good must be good as
the primary metric  we can afford to incorrectly eliminate good loans as
bad  as at any given time  there are many more loans available to invest
in than dollars to be invested  therefore  in this paper  we will focus on
precision at the cost of recall and overall accuracy 
after identifying this pool of loans with low probability of default  we
will compare our return rate to lendingclubs return rate given the same
default risk rate 

data preprocessing
lendingclub data required significant cleansing before ingestion 
 regexp to clean html tags and other unwanted characters 
 discrete categorical features expanded into separate binary columns 
including text preparation for tf idf  see section on tf idf  
 stanford nlp  manning         guava  lucene for text processing 
 for serializing and de serializing csv files  we used jsefa api 

data balancing
the raw data from lendingclub has a default to paid off rate of       
vs          this skew will negatively impact algorithms such as logistic
regression that optimizes across the entire training set  for training 
therefore  we balanced the training data file to have a       split 

paid  y   

available
loans

a

     

      

      

     

     

b

     

      

      

      

      

standard data files

c

     

      

      

      

      

d

     

      

      

      

      

we created three data files to be used across all of our machine learning
algorithms  all data files are randomized 

e

     

     

     

      

      

f

     

     

     

      

      

total

default
rate

as one would expect  higher interest rates correspond to higher default
rates  if an investor were to invest blindly  she would get an average
interest rate of         however  she will also face a default rate of
nearly one in five loans  if the investor were to follow lendingclubs
loan grading and choose to be conservative  she may choose to invest only
in a grade loans  where she will earn about      interest at a default risk
of also       if she were willing to take a higher risk  she can choose ggrade loans at an average interest rate of about     but with a default risk
of greater than one in three 

default
 y   

g

interest
rate

also includes nearly    features  feature selection is discussed in a
following section 

   

   

   

      

      

      

      

      

      

      

the raw data contains multiple loan statuses  including fully paid 
charged defaulted  late  in grace  issued  and current  because the goal of
this paper is to predict and avoid loans that will default  and to invest in
loans that will be fully paid  the classification will require loan statuses
only fully paid  y    and charged default  y     the lendingclub data

kevin tsai  sivagami ramiah  sudhanshu singh

   balanced training 
   balanced test  this file was used to plot the learning curve 
   prior test  this file keeps the same distribution as the source data and
is used to calculate precision 

feature selection
apart from our own intuition and insight we got from the data by running
a few algorithms  we also ran exhaustive feature selection search in weka
and matlab to find most dominant features 
   infogain  infogain class attribute    h class    h class  
attribute  evaluates the worth of an attribute by measuring the

stanford university cs     autumn     

page   of  

fipeer lending risk predictor

modified logistic regression
motivation
standard logistic regression attempts to maximize the log likelihood of
the estimates  ng   since log h x i    and log   h x i    are always zero or
negative  misclassification results in a large negative number  as our goal
is to minimize default risk by focusing on increasing precision  we modify
the log likelihood estimate by multiplying the y i    term by a penalty
factor beta    


          log                    log           

   

  

if the classifier incorrectly classifies a defaulted loan  y i     as a good
loan  h x i       the log likelihood of this same will be multiplied by the
factor   a high  will cause the classifier to avoid incorrectly classifying
defaulted loans  y i     as paid off loans  y i      even if that means
increasing the misclassification of a paid off loans as defaulted loans 
this effectively gives preference for precision at the cost of recall and
overall accuracy  introducing  into the log likelihood  it follows that the
first and second derivatives are 


    
 

                    

   

                     

figure    effect of penalty factor 
    

    

   

   

   

   

   

   

   

   

   
beta
precision

   

as seen in table    non penalized logistic regression     
recommends       of the available loans at a precision of        when
    precision increases and peaks at       at a  of      at the cost of
recall and testing accuracy  for our purpose  this is fine  as high precision
means low risk of losing money in a defaulted loan 
table    effect of penalty factor 
beta
   
   
   
   
   
   
   

accuracy

   
   
   
   
   
  

  

   

   

train set size

training accuracy

kevin tsai  sivagami ramiah  sudhanshu singh

testing
accuracy
     
     
     
     
     
     
     

precision

recall

     
     
     
     
     
     
     

     
     
     
     
    
    
    

fraction
recommended
     
     
     
    
    
    
    

since svms have been a promising tool for data classification  we used
libsvm  chih jen lin  and liblinear  lin  libraries for our   class
classification problem 



 
min         
    

   

  

                          k       m

figure    learning curve  modified logistic regression

  

training
accuracy
     
     
     
     
     
     
     

support vector machines  svm 

   

as we have many more data points than dimensions  m  n  we decided
not to use regularization  as can be seen in the figure    the size of the
data set puts us in the high bias range 

  

  recom 

the main idea in svm is to map data into a high dimensional space and
find a separating hyperplane with the maximal margin  given training
vectors xk  rn  k         m in two classes and a vector of labels yk  rm 
such that yk          svm solves a quadratic optimization problem 

effect of penalty factor beta

  

  

                                                   

note that when     all three equations above revert to the original
logistic regression equations  with the first and second derivatives  we
used newton raphson as the optimization method 
            

fraction of loans
recommended

   infogain for logistic regression  top features  loan amount  term 
interest rate  installment  employment length  annual income 
debt to income  revolving utilization 
   correlation based feature subset selection  hall        and genetic
algorithm search  goldberg         evaluates the value of a subset
of attributes by considering the individual predictive ability of each
feature and minimize redundancy  subsets of features that are highly
correlated with the class while having low intercorrelation are
preferred  top   features  debt to income  emp title  int rate  term 
   matlabs sequentialfs forward search feature selection algorithm
identified the following features  int rate  is income verified 
annual income  and loan purpose 

figure   shows the inverse relationship between fraction of loans
recommended and precision  this is expected  as  increases  the
classifier is more discriminate against defaulted loans 

precision

information gain with respect to the class by comparing worth with
and without the attribute  top   features  emp title  interest rate 
loan amount  annual income 

testing accuracy

   

    

if data is linear  a separating hyper plane may be used to divide the data 
however  in our data set  as the number of instances is larger than the
number of features m    n  mapping data to higher dimensional
spaces i e   using nonlinear kernels  would be a better approach per
section c   of the a practical guide to support vector classification
 chih wei hsu  on libsvm  besides running liblinear algorithm on our
data set gave us only     training accuracy  this tells us that our data is
non linear 
we noticed that liblinears running time is very fast irrespective of the
sample size  less than a minute  whereas libsvm is relatively slow and
it takes   minutes for        samples  this is due to the fact that the
complexity of the smo algorithm implemented in libsvm is o n   or
o n   whereas in liblinear it s o n    n is the number of samples  
to run the libsvm algorithm on our data  we followed the procedure
stated in the above mentioned svm guide to format the data  choose the

cs    autumn     

page   of  

fipeer lending risk predictor
kernel  identify the best parameters and train the whole training set  data
scaling didnt add much value to the classification as our data model
doesnt have variance issue 
a snapshot of four labeled feature vectors in libsvm data format 
  
 
 
 

       
      
      
      

       
      
      
      

       
      
      
      

       
       
       
      

       
        
        
        

    
    
    
    

   
   

 

       

    
   

   
   
   
       

model selection  the effectiveness of svm depends on the selection of
kernel  the kernel s parameters  and the soft margin parameter c  we
started with the gaussian radial basis function  rbf  kernel with a
single parameter  and found it to be the best kernel  when compared to
polynomial and sigmoid kernels  for our classification problem 
         

figure    precision  recall  prediction curve  libsvm

   
    

    

    

     

     

train set size
test accuracy

precision

recall

figure   shows the auc for precision  recall and prediction  auc for
precision is the highest 

   

we selected the best combination of c and  by the grid search algorithm
with exponentially growing sequences of c and  
c                                                   
each combination of parameter choices was checked using   fold cross
validation  and the parameters with best cross validation accuracy were
used to train the entire training set  figure   shows the contour plot of
parameter selection for gaussian rbf kernel using libsvm 
figure    contour plot  libsvm

nave bayes
the nave bayes  george h  john        implementation is taken directly
from the multinomial event model from cs    class notes   



  

                                                  
  

  

continuous valued features such as annual income  revolving balance
utilization  and loan amount were discretized  after discretizing these
features into a bucket of size     from     we got an increase of    in
precision 

random forest
random forest  breiman        works as large collection of decorrelated b bag of trees and training data d of   x  y     xm ym   
   for i   b
  choose bootstrap sample di from d 
  construct tree ti using d  such that  at each node chose n random
subset of features and only consider splitting on those features 
end for
   once all trees are built  run test data through aggregated predictor 
   given x  take majority vote  for y      from different bags of tree 
solver  c svc classification model gave the highest precision percentage
among the three solvers in libsvm available for classification 
penalty factor weight   wi   is used to set the parameter c of class i to
weight c  since our primary objective is to increase the precision of our
classification at the cost of recall and overall accuracy  we introduced
higher weight   w        for negative class to penalize false positives 

train accuracy was very high at      but test set was very low  at about
     because the trees that are grown very deep and learn highly
irregular patterns  they overfit their training sets  having more trees in the
bag reduce the variance  we fine tuned the model by gradually varying
the tree size  number of random features  and depth to minimize out ofbag errors  the mean prediction error on each training sample x  using
only the trees that did not have x in their bootstrap sample 

table    results for gaussian kernel

table    abridged out of bag error minimization

 kernel type  t    rbf   default weight for positive class w    

solver

weights

train
acc 
    

test
acc 
    

precision

recall

    

     

  recommendation
      

 s  

 w      

 s  

 w    

    

    

    

    

     

 s  

 w      

    

    

    

    

    

 s  

 w    

    

    

    

    

     

 s  

 w      

    

    

    

    

     

 s  

 w    

    

    

    

    

     

 s  

 w      

    

    

    

    

     

c svc solver   s    with highest precision       recommends      
loans at the cost of recall 

kevin tsai  sivagami ramiah  sudhanshu singh

tree size
 
  
  
  

depth
 
  
  
  

random features
 
 
 
 

out of bag error
      
      
      
      

tf idf
the text attributes are very sparse as only a few loans have descriptions 
after removing stop words  the vocabulary was        unique words  we
used term frequency inverse document frequency  tf idf  to identify
words better associated with either y   or y   loans 

cs    autumn     

                   

    

page   of  

fipeer lending risk predictor
tf is a measure of how often a word appears in a document  normalized
to document length  the more often it appears  the more weight it gives 
            

      
max      

figure    logistic regression on pca data

    

idf is a measure of how special a word is  a word that exists only in a
small fraction of the body of documents will have high weight 
       log


    

    

tf idf ranking is constructed by ordering tf idf scored in descending
order  the higher the score  the lower the ordinal rank  i e  rank    has
highest tf idf score   the top ranking  highest tf idf scores  words
have similar ranking in both classes  which means these words are not
discriminative of the class  however  as we looked at words in lower
ranks  we started to see the differentiation that allowed us to better classify
loans to be either y   or y    a given word is associated more with the
class with the lower tf idf rank  and the larger the difference between
the two ranks  the more discriminate the word 
table    rank tf idf
word
god
steady
university
refinance

y  
   
   
   
   

y  
   
   
   
   

as seen in table    the word steady is associated more with loans that are
defaulted  tf idf ranking      versus paid off  lower rank of      
whereas the word university is associated more with loans that are paid
off  tf idf ranking      versus defaulted  lower rank       we checked
the data  many applicants who mentioned that they will have a steady
income or steady cash flow and didn t have a permanent earning at present
they end up defaulting the loan at later time  based on this process  we
created additional binary features such as is steady  is god 
is university  this gave    increase in performance on libsvm
over numeric only classification 

performance
comparing algorithms
in this paper  we used four algorithms  nave bayes  random forest 
svm  and modified logistic regression  mlr   in our implementation 
only svm and mlr were instrumented with the ability to preferentially
bias for one classification over the other  therefore  since our goal is to
optimize for precision  svm and mlr had the best performance for our
goal  as shown in table    while the highest precision is from mlr on
two dimension pca data  this classification only recommended      of
loans  compared to over    for svm and mlr       recommendation
means that for every       loans offered  only   will be recommended 
this would be relatively impractical in a real investment strategy 
table   also attempt to compare the performance of our algorithms to the
different lendingclub sub grades  for example  mlr with      
precision at      beta is closest to lendingclub subgrade a  at       
investing based on mlr  the investor will earn      higher average
interest rate than investing based on lendingclubs a  sub grade  even
though both have the same risk of default 
table    best case performance comparison of algorithms

data visualization
our classification work in higher dimension space led us to believe our
data is not linearly separable  and using a larger penalty factor   we are
operating in high precision  low recall space  to confirm this  we used
principal components analysis  pca  to reduce the data dimension for
visualization  standard procedures were used 
   perform mean subtraction and variance scaling on source data 
   with normalized data  calculate covariance matrix 
 

 


 

    

   use svd to identify the first and second principal components
as seen in figure    there is significant overlap across the two data sets
y   and y    however  there is separation  as it also appears the center
of mass for the two labels are distinct  applying a large penalty factor 
to logistic regression effectively shifts the decision boundary away from
the center of mass to the region predominantly y    below the red and
magenta decision boundaries is a small fraction of data point with a high
concentration of y    this region is high precision  however  these
decision boundaries also leave above most of the data points  both y  
and y    this is why recall is very low  this confirms the behavior we
see in higher dimensional space 

cs    best performance
best
best
interest

precision
nave
bayes
random
forest
svmrbf
mlr
mlr on
pca

lendingclub equivalent
grade
precision
interest

 

     

     

b 

     

     

 

     

     

b 

     

     

   

     

    

a 

     

    

    

     

    

a 

     

    

    

     

    

a 

     

    

comparing to lendingclub
emphasizing our original goal  to reduce exposure to loan defaults and
exceed lendingclub coms return given the same risk level  we will now
compare the performance  of our classifier to that of lendingclubs  our
methodology is as follows 
   calculate the equivalent precision from lendingclubs data  for
example  the precision of grade a loans is the paid off loans in grade
a divided by all loans in grade a  repeat for all grades
   adjust  until our modified logistic regression classifier precision is
at the same precision level of the specific lendingclub grade target 

 

because our modified logistic regression  mlr  algorithm afforded the most flexibility in
adjusting   we will focus our comparison to lendingclub using mlr 

kevin tsai  sivagami ramiah  sudhanshu singh

cs    autumn     

page   of  

fipeer lending risk predictor
   compare at each precision level the interest rates and fraction of loans
our classifier selected  compared to interest rates and fraction of loans
that lendingclub selected 
notice in table   for grade a  lendingclub classified        of total
loans at an average interest rate of       mlr classified        of total
loans at an average interest rate of        this means mlr offers the
investor      more loan choices  with an average interest rate of     
higher than lendingclub classification 

conclusion

table    investment performance by grade
grade
a
b
c
d
e
f
g

lendingclub
precision
lc
fraction
     
      
     
      
     
      
     
      
     
     
     
     
     
     

lc
rate
    
     
     
     
     
     
     

modified logistic regression
beta
mlr
mlr
fraction
rate
            
     
            
     
            
     
     
     
     
     
     
     
     
     
     
     
     

for the more conservative investor  the table   is a breakdown for subgrade a  a   notice the similar effect where mlr is able to classify
more loans in the higher precision category and at a higher interest rate 
table    investment performance by sub grade a
subgrade
a 
a 
a 
a 
a 

lendingclub
precision
lc
fraction
     
     
     
     
     
     
     
     
     
     

lc
rate
    
    
    
    
    

modified logistic regression
beta
mlr
mlr
fraction
rate
     
     
    
     
     
    
     
     
    
     
     
    
     
     
     

what is most interesting is the breakdown of sub grade a  loans in table
   mlr classified       of loans in the equivalent of sub grade a  risk
group  compared to lendingclubs        that means lendingclub
misclassified a number of high grade loans with high interest rate into a
lower category  compare sub grade a    breakout from mlr with subgrade a  from lendingclub  the former interest rate is        at a
precision of        whereas the latter is      at       precision  the
astute investor can take advantage of this discrepancy to invest in loans
that pay unusually high interest rates given their lower risk levels 
table    intra sub grade a  breakdown
sub sub grade
a   
a   
a   
a   

interest rate
    
     
      
    

total
     
   
  
 

paid
     
   
  
 

   when tuning parameters of a machine learning algorithm it s better
to use a training sample with smaller size  otherwise it will take a
very long time to find the optimal parameters as the algorithm need
to run large number of iterations on the training set  we learned this
lesson from the grid search algorithm for libsvm 
   as suggested by prof  ng  we learned that it s better to start with a
quick and dirty approach before spending time and efforts on an
approach that might not work 

precision
     
     
     
     

as we stated earlier  our primary objective is to obtain a high precision
that translates to low risk of losing money in a defaulted loan  we
employed   machine learning algorithms on this task and found out that
logistic regression outperformed libsvm  nave bayes and random
forest  the secret ingredient to boost the precision is to increase the
penalty factor for the negative class  this yielded higher precision at the
cost of recall and prediction accuracy 

future work
   an area of future work would be to perform sentence and sentiment
analysis on those text features that might help in improving the
overall accuracy and precision 
   ensemble of classifiers to build a strong classifier 
   reinforcement machine learning algorithm can be used to classify
active loans with time series data 
   classifying loan data from other peer lending platforms to see how
our classifiers perform on varying platforms 

references
breiman          random forests machine learning  retrieved from
https   www stat berkeley edu  breiman randomforest     pdf
george h  john  p  l          estimating continuous distributions in
bayesian classifiers  eleventh conference on uncertainty in
artificial intelligence          
goldberg  d  e          genetic algorithms in search  optimization and
machine learning 
hall  m  a          correlation based feature subset selection for
machine learning 
lin  c  c  c  j   n d    libsvm   a library for support vector machines 
acm transactions on intelligent systems and technology                retrieved from http   www csie ntu edu tw  cjlin libsvm
manning  c  d          proceedings of   nd annual meeting of the
association for computational linguistics  system demonstrations 
      
retrieved
from
http   nlp stanford edu pubs stanfordcorenlp     pdf
ng  a   n d    cs    class notes   supervised learning and notes  
generative learning algorithms 

lessons learned
   we initially achieved      test accuracy using most of our
algorithms  only to later find out that our data included information
that is not available when a loan is first offered  such as late fee 
recovery fee  and current fico score  this allowed our algorithms
to cheat by looking into the future  we subsequently removed
these features 
   some of our algorithms such as logistic regression are sensitive to
population skew that was natural in our data  we rectified this
problem by manually balancing y   and y   to       

kevin tsai  sivagami ramiah  sudhanshu singh

acknowledgements
we are grateful to professor andrew ng who has been the greatest
inspiration in our machine learning journey  we would also like to
thank all the tas who have been immensely helpful  last but not
least  our heartfelt thanks to our spouses  children   parents for
being really supportive and understanding 

cs    autumn     

page   of  

fi