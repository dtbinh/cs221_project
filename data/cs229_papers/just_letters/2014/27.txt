leveraging document structure for better classification of complex
legal documents
alex ratner
stanford university       serra mall  palo alto  ca
ajratner stanford edu

abstract
document classification is a machine
learning application that has been as impactful as it has been successful in a myriad of domains and applications  however  when the documents being classified
are large and highly complex  and when
the set of potential classes is large as well 
these models could be improved by incorporating more information about the documents overall structure  most approaches
use bag of words type models that discard
local structure and focus on types of words
or n grams used  in this paper  we examine several models and attempt to leverage
both local  e g  n gram  and global  e g 
structure and organization  document features  we apply these approaches to a new
dataset of legal documents 

 

introduction

text classification is an important component
of many modern applications such as information retrieval  information extraction and domainspecific content processing systems  to date 
many text classification systems have achieved
performance success using simple features that
mostly or completely ignore word ordering  document structure and organization  and other such
features  and then use sophisticated generative
 e g  lda   blei et  al         or discriminative
 e g  svm  models to classify documents  many
approaches preserve some local structure by looking at subsequences of words  ngrams  or by
incorporating some dependency or parse tree information  lately  several deep learning models have attempted to preserve even more local
structure by learning high dimensional representations of large variable length strings using recursive neural net architectures  le and mikolov 
      

we hypothesize that for some very large  complex document sets with nuanced classification
schemes  even a lot of this local structure might
be the same across different document classes  and
some awareness of the overall structure and organization of the document might help with the classification task  as previous work leveraging document structure has shown  chen et  al         
for this paper  we are specifically interested in
classifying legal contract documents  in terms of
general motivation  the automation of certain legal processes is a compelling challenge since it
is widely acknowledged today that access to legal services and representation is majorly skewed
towards those with enough money to afford better
lawyers  in large part  this is due to the high costs
of manually performed tasks such as information
retrieval  extraction  classification  and anomaly
detection  that could be partially or fully automated 
in terms of technical motivation  contract documents are of interest because they are large  complex documents that nonetheless often have some
shared structures such as titles  sections  subsections  etc  in our dataset  not only are there many
different categories but they are often similar  ex 
account receivables financing agreement vs 
account receivables purchase agreement   use
similar words and phrases  etc 

we explore some discriminative algorithms
with some simple feature extraction and feature
space reduction techniques  then move on to a custom generative model which attempts to more ex 

fiplicitly model the document structure we observe 

 

dataset

we collected our dataset from onecle com  an
aggregator site that collects publicly disclosed legal contract documents from the sec and categorizes them manually  we began by crawling
onecle com  and scraping        contract documents in     categories ranging from arbitration agreement to manufacturing contract    
in most cases  we restrict our consideration to categories with      documents  which results in
a reduced dataset of        documents  additionally  in most cases we further randomly subsample to        documents for slightly more balanced classes  see results and discussion 

 

features and preprocessing

in order to capture some of the document organizational structure that we wished to leverage 
we used a small set of heuristic rules to extract
individual sections and their titles  and the overall document title  as will be discussed further 
the dataset was extremely noisy with regard to
structure  at least   different word document tohtml processes appeared to have been used  so we
limited our parsing of structure to these high level
components 
once the documents were parsed into highlevel structural components  we preprocessed further using minimum word length thresholding
 min     minimum and maximum corpus frequency thresholding  we kept words w that appeared in more than   but less that       d  of
the documents d   porter stemming  a form of
suffix removal that results in normalized forms of
words   and removal of certain non content words
 stop word removal  
for our discriminative algorithms  logistic regression and svm   we then transformed each
component into a vector of tf idf weighted indicator features  in other words  given a training
set vocabulary v    w         w v     from n documents  we represented each text component as a
 d 
length  v   vector  x d  where xi   i d  f  i  d  
with f  i  d  being the count of word wi in docu 

permissible according to their robots txt file
 
permissible as these are unmodified versions of the
public domain documents provided by the sec

ment d  and i d being the tf idf weight 


     f  i  d 
i d        
max  f  j  d    wj  d  


n
 log
  d  d   wi  d  
even with these preprocessing steps  we still
ended up with  v                meaning our feature vectors were this length as well  we tried
three types of feature dimensionality reduction 
  test thresholding  select the k best features according to this statistic of informativeness   principle component analysis  pca   and latent semantic analysis  lsa  similar to pca  this is the term
for truncated svd used on feature vectors such as
ours   detailed review of these methods is not included as they uniformly appeared to dramatically
lower classifier performance in testing  and were
thus not utilized 
finally  we handled structure in the following
model specific ways  for multinomial nb  no information about document structure was kept  for
logistic regression and svm  we used separate vocabularies and feature vectors for the body text
and titles respectively  and then concatenated these
vectors  for cross section multinomial nb  we
represented structure as described in the following
section 

 
   

models
logistic regression

our baseline discriminative model was l  norm
logistic regression  which minimizes the following
cost function 
n

x
 
j w  c    wt w   c
log exp yi  xit w   c       
 
i  

where c is a hyper parameter determining the
balance between fitting the model and penalizing
overfitting with the l  norm 
   

svm

as reviewed in class  an svm minimizes the following cost function 
n

x
 
j w  b    wt w   c
i
 
i  

 
approximate due to the random sub sampling done to
balance class membership sizes  noted in the previous section

fisubject to 

we use a plate diagram to illustrate the model
proposed 
yi  wt xi   b      i
i   i

in the dual form of the problem  we can write the
optimization problem in terms of   x  x    and
then replace these with arbitrary  mercer  kernel
functions  x  x     we use two different kernel
functions  a linear one   x  x       x  x     and
a radial basis function  rbf  kernel   x  x     
exp  x  x       
   

multinomial naive bayes

multinomial naive bayes is a generative model
which uses the naive bayes approximation to
assume independence between every pair of features  thus weq
can model the probability of a document is p y  ni   p xi  y   where x         xn are
the words in the document of class y  and where
our model is parameterized by i y   p xi  y  and
y   p y   and where we use laplace smoothing to redistribute some probability mass from observed word statistics to allow for words in the test
set that were not seen in the training set 
we implement multinomial nb in two waysfirst using mle to find the optimal parameters 
and then using gibbs sampling  see next subsection   mostly in order to set a relative baseline
for the other gibbs sampling model used 
    cross section multinomial naive bayes
our final model is a potentially novel  attempt
to more explicitly account for document structure  which we term the cross section version
of multinomial nb  specifically  we hypothesize
that in each section of a contract  there is some language that is very specific to that contract and its
contract class as a whole  and some language that
is more specific to a certain type of section class
used across multiple types of contracts  for example  multiple types of contracts might have a section having to do with limiting liabilities  we hypothesize that this section might have some words
strongly related to the contract type  and some
words related more to the general concept of liabilities  moreover  we hypothesize that the sections
of the contract might be a good set of segmentations to reflect these factors 
 
in a small  application specific way    we know there are
lots of plate models out there 

this model defines the following generative process for creating a contract  given j contract
classes and k section classes to choose from 
 a contract class is sampled 
m ultinomial c  

c



 for each section 
 a section class is sampled 
m ultinomial s  

s



 a bernoulli parameter is sampled   
beta  
 for each word 
 a binary value is sampled   
bernoulli  
 if       a word is sampled conditional on the contract class  w 
m ultinomial w c 
 if       a word is sampled conditional on the section class  w 
m ultinomial w s 
the multinomial parameters  all have corresponding dirichlet priors  however for simplicity
we use symmetric and uniform       priors so this
essentially works out to laplace smoothing 
we then use gibbs sampling  which roughly is
the technique of sampling all the involved parameters and labels one at a time  conditioned on all
the other parameters values as set in the previous
sampling iteration  we calculate the conditional
distributions required for sampling  for example 
for sampling a new document class for document
i  the probability of a certain contract class c con 

fiditioned on the other parameters is 
   i    s 
  c   s   
p  c c
    wc   ws       
   i    s 
  c   s   
p  c   p  c
    wc   ws       c 

 

explore the hyper parameter space  we did examine the optimum number of section classes to set 
plotted below   

   i    s 
  c   s   
p  c
    wc   ws       

since we are calculating these probabilities for the
classes c in order to sample from a multinomial 
we can disregard the denominator which has no
dependence on c 
   i    s 
  c   s   
p  c c
    wc   ws       
   i    s 
  c   s   
 p  c   p  c
    wc   ws       c 

 nsections i
y
count c     

p  sj  c 
ndocs   nclasses
j  

nwords i j

y



 ij p  wij  c        ij  p  wij  s  

i  


 

count c     
ndocs   nclasses

 nsections i
y

ssj  c

j  

we also observed that the system seemed to converge to a steady state local optima very quickly 
perhaps much faster than we would want for ideal
performance  an illustrative trial is plotted below 

nwords i j



y

h
i
wc
ws
ij w
 
  


 
ij
wi j s
i j c

i  

and similarly for the other conditional distributions needed for the sampling iterations 

 

results

we use stratified   fold cross validation  on a
randomly subsampled down set of        documents in order to balance class membership
counts  selecting from    classes which had at
least    documents 
model
logistic regression
svm  linear kernel 
svm  rbf kernel 
multinomial nb  mle 
multinomial nb  gibbs 
cross section nb

test acc 
     
     
     
     
     
     

train acc 
     
     
     
     
n a
n a

 

discussion

for our gibbs sampling models  we used a custom implementation in python  we used burn in
and thinning to reduce spurious effects from the
initial state space exploration and to avoid autocorrelation respectively  nevertheless  our implementation was somewhat limited in terms of computational resources  which limited our ability to

in this paper we attempted to  a  explore the range
of practical options for dealing with a classification task of interest  and  b  investigate some ways
to leverage document structure for better classification  specifically  we attempted to learn classification models for long  complex documents  with
a high number       of total classes 
one initial finding of interest was that simple
attempts at feature dimensionality reduction had
terrible effects on performance  perhaps because a
large volume of specific words were actually very
relevant to contract class distinctions  addition 

 
stratified meaning that we keep the proportions of
classes constant in each fold

 
although caution should be used when viewing this plot
due to the high variance in our method

fially  we note that the svm with rbf kernel performed extremely badly  this is especially interesting compared to the very high performance of the
svm with linear kernel  we predict that performance could be improved with more careful calibration of the rbf hyper parameters  however this
disparity is still interesting for what it might apply
about rbf kernel performance in a very high dimensional  sparse feature space in a classification
problem with a large number of classes 
with respect to the generative models explored 
we see that not only did generative mle approaches tend to do worse than the discriminative
algorithms  but that gibbs sampling methods did
significantly worse still  it is widely known that
there is some black magic in the implementation
of gibbs sampling methods  so perhaps  due in
part to our computation limited implementationwe simple did not explore the configuration  
hyper parameter space thoroughly enough  for example  we only used symmetric uniform dirichlet
priors   another factor was the fast rate of convergence to steady state observed  we hypothesize
that if we could get the sampler to explore the
state space more then performance would be much
higher  multiple random initialization runs might
be a tack for this  given a faster sampler implementation  in general  we observed that the sampler
was highly sensitive to initialization methodology
as well 
finally  though we observe that our crosssection model performed slightly better than basic
multinomial nb  we do not know if this is a statistically significant difference given the variance in
our method  additionally  there are several ways
that our cross section model could effectively reduce to the basic multinomial nb case  for example if the bernoulli priors become very low  which
we would still need to investigate more carefully 
however  we do think the relatively decent performance of the newly proposed model indicates
that similar methods might warrant further exploration  for several reasons  first of all  the documents collected were extremely messy  and extraction of structure was far more difficult than anticipated  with a better collection of documents 
structure could be both more accurately represented and more deeply leveraged  second of all 
the parameters of this model were pushing the limits of the simple implementation used  with a better sampler  we could much more effectively ex 

plore this model and more complex ones 

 

conclusion

in this paper we explore several methods for automatically classifying complex legal documents
of a large number of classes  we compare discriminative and generative approaches  including a novel generative model for capturing crosscorrelations in the document substructure  we
find however that simple discriminative modelssuch as an svm with linear kernel and logistic
regression  still attain the best performance  however we think further exploration is warranted with
some of the generative approaches which explicitly model document structure 

 

future work

in the future  we would like to pursue three major directions      explore generative models further like the ones proposed which explicitly model
document structure  however using better quality source documents and a faster gibbs sampler 
    explore deep learning approaches  using word
embeddings and recursive auto encoders mapped
onto document structure  and      explore the failure of svm with rbf kernel and dimensionality
reduction methods that we observed 

references
pedregosa et al        scikit learn  machine learning
in python  journal of machine learning research  
                 
philip resnick  eric hardisty        gibbs sampling for the uninitiated 
technical report 
http   www umiacs umd edu  resnik pubs lamptr     pdf accesed            
pengtao xie  eric xing        integrating document
modeling and text clustering  cuai       
david blei  andrew ng and michael i  jordan       
latent dirichlet allocation  journal of machine
learning research                   
quoc le and tomas mikolov        distributed representations of sentences and documents  proceedings of the   st international conference on machine learning        jmlr  w cp volume    
harr chen  s r k  branavan  regina barzilay and
david r  karger        content modeling using
latent permutations  journal of artificial intelligence research      c 

fi