cross domain product classification with deep learning
luke de oliveira   alfredo lainez rodrigo   and akua abu 
 

stanford university icme  cs    final project

in our final project  we took on the challenge of cross domain classificationthe adaptation of a
model suitable for one domain to one or more different domains with identical features  specifically 
we developed a cross domain classification schema to predict product categories in tweets and ebay
reviews based on user reviews from amazon  we first developed a set of classical machine learning
classifiers that performed reasonably well on our principal data set of amazon products and then
applied the same classifiers to predict product categories from twitter tweets and ebay reviews  we
then developed deep learning models to approach to our cross domain classification problem and
compared performance  in what follows  we walk step by step through the procedure  illustrate our
results  then discuss future work  we show that while deep learning definitively helps models work
out of domain  considerable work needs to be done to create a context free model of language that
can work across domains 
i 

introduction

there is no longer a boundary between social media 
advertising  and consumer culture  the intersection of
these different spaces has resulted in an immesurable increase in the amount and prevalence of user data  users
develop online identities that span across many platforms  from facebook to twitter to youtube and to
consumer communities such as ebay and amazon  an
open problem of great relevance to these overlapping social spheres is that of cross domain learning  that is 
given an estimated model suitable for one domain  can
the model perform in an entirely different context given
identical features  using a dataset of amazon products labeled within the amazon hierarchical categories 
we have developed a set of classifiers that predict product categories based on users reviews  then  we apply
the same predictors of user content to different domains
to see how these models behaves in a different context
over different underlying distributions of features  the
key challenge is that we impose no prior over the target distribution  we simply aim to develop a model
that is not entirely dependent on a training distribution  specifically  we have predicted product categorization using tweets from twitter and reviews from ebay 
and we determine the validity of an out of context classifier using carefully chosen metrics  finally  we have
engineered two deep learning models  and use them to
illustrate a potential new application for deep architectures  we outperform classical approaches in this task 
showing adaptability to new domains 

top    products in every amazon product category  comprising a hierarchy of roughly         products ranging
across        categories  the secondary data set is of
ebay and twitter data  the set contains tweets and reviews along with an annotated amazon category  for
our classification purposes  we used the    main categories in the amazon hierarchy  merging the main categories kindlestore and books  then  our classification
operates over text  reviews or tweets  labeled by a main
amazon category 
the data used was saved in json format  including
node categorization in the amazon product hierarchy
for each product  we built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product  allowing
us to extract labels at any depth  specifically  we utilized
the nodes with d     away from the root node  i e   the
main categories  for the purposes of our investigation
into cross domain text classification  we care less about
a fine study of large scale hierarchical clasification and
more about whether or not we can find methods that
are valid across the domain  in some sense  a problem
completely independent from the former 
b 

we are working in a classification space with    different groups with highly imbalanced classes  in order
to compensate for this  we utilize f   scores averaged out
across classes  where
 j 

f 
ii 
a 

setup
data

this project makes use of a principal data set of amazon product reviews and an amazon product hierarchy  secondary data sets include product related twitter data  tweets   and ebay product reviews  each of
which are hand labeled  the main amazon data sets
consists of descriptions and    reviews for each of the

evaluation

  

precision  recall
precision   recall

is the f  score for class j  we compute precision and recall for each class in a one vs rest approach  our overall
f  score  for j classes  is then
f   

j
  x  j 
f  
j j    

more generally  one can imagine a scenario in which we
care about some convex combination denoted by a vector

fi 
c  where
f c  

j
x

 j 

cj f   

j  

and each cj denotes our relative cost of an f  error
on class j  however  we opt for a standard mean to give
each class equal influence in terms of how well we predict
its membership  traditional classification error was not
used as this provides us with a better metric given our
imbalances 
it is important to note that a few classes are heavily underrepresented in the data  even considering the
whole dataset  hence  the average computed as a quality measure will be affected by these classes  and will
be significantly skewed downward  moving the bias away
from well represented classes 

c 

features

as pure text was the raw input to our model  we utilized a bag of words representation of our review texts 
incorporating tf ifd and removing common stop words to
improve performance and direct our attention to semantically relevant aspects of cross domain language generalization  the features thus corresponded to a sparse
matrix of word frequencies from our selected corpus 
the inverse document frequency  ifd  improved the results ruling out words commonly present in the corpus 
moreover  we incorporated an n gram approach into our
model to better take account of differences in meaning resulting from contiguous sequences of words  for
n      run times were longer  but we obtained a slight
increase in test set performance with worse results for
cross domain classification  this is consistent with a priori intuition regarding these features  as n grams with
n     provide a very refined look at language structure 
meaning any model learned will not generalize well 

fig     confusion matrix for predicting categories in the
amazon dataset using a svm with   grams bag of words

presented certain bias towards the majority class  since
amazon started as a bookstore  the number of products
categorized as books is about     of the entire dataset 
with other categories also comprising a large proportion
of the training examples  at the opposite end of the
spectrum  we have a pair of product classes comprising
less than     products in total  the problem of training
with such overrepresented classes made a naive bayes
classifier simply predict the class books for every given
example 
in order to avoid this training bias  and considering
the large amount of data available  we loaded the whole
dataset for training and testing purposes and selectively
pruned it in order to balance the classes for training 
while this does not solve the underrepresentation of
some classes  it provides more balance to the majority of
them and utilizes all the examples available of the less
common ones  in fig   we can see a confusion matrix
illustrating the distribution of the classes after pruning
and classification for the amazon dataset 

iv 
iii 

classical methods

as a first classification step  we selected three wellknown algorithms for the task of text classification 
naive bayes  stochastic gradient descent and support
vector machines  followed by a grid search   cv for parameter optimization  in particular  we utilized naive
bayes with laplacian smoothing and sgd with a modified huber loss function and           also  the svm is
a linear svm which follows a one vs the rest multiclass
strategy  since other svms with one vs one strategies
are too time consuming for our space of    categories  in
particular  given the constraints of scikit learn  utilization
of any kernel machine requires the estimation of

j
 
   
 in our case  kernel machines  a prohibitive
 
cost 
during the implementation and evaluation  we noticed that these classically derived shallow methods

deep learning

as our foray into classical methods proved  a simple
application of a classifier is not appropriate for estimating a model with the expectation of out of domain performance  to examine how we can construct a model
that can learn features that can prove useful both inside and outside the training domain  we look to deep
learning  in particular  we look to ways in which we
can provide regularization  as this will help our models
adapt to different domains  the challenge in our chosen problem is that we have no prior on the language
distribution in the target domain  a series of structures   paradigms were decided upon that would be able
to test the relevance of deep learning for this particular cross domain classification problem  we consider
stacked autoencoders of regular and denoising variety 
and a stacked thesaurus  as we call it  a novel modification to dense cohorts of terms  these are all trained
in an unsupervised fashion  with supervised fine tuning

fi 
    units 

   

    units 

   

training procedure  first layer          
bernoulli noise dae  train error
bernoulli noise dae  test error
normal noise dae  train error
normal noise dae  test error
ae train error
ae test error

     units 

     units 

reconstruction error

   
   
   
   
   
   
input layer      units 

   
    

   

fig     network structure used for autoencoders

   

epochs

   

   

    

fig     error curves for training  first layer of deep model 

applied later  since deep models are expensive to estimate  we utilize a subset of possible features from tf idf
matrices  restricting ourselves to the top     features for
autoencoder based methods 
for all deep learning methods  we use stochastic gradient descent as our training algorithm  in addition 
we use momentum to speed up training  all learning
rates  momentum parameters  and noising   regularization parameters are found using grid search  using our
own implementation  we rely on scipy numpy to handle
matrix math 

vector x  in order to force the autoencoders we stack
to construct robust representations  we require that the
autoencoders be able to reconstruct a clean version of x
from a corrupted version x   x  specifically  this is a
denoising requirement  and acts as a regularizer to local
changes in distributions 
the denoising autoencoder is defined by the loss function
l x     

a 

stacked autoencoders

an autoencoder is a tranformed linear mapping that
creates a bottleneck of dimensionality  formally  consider a data vector x  rd   suppose we want to find
a representation r  rd   d   d  an autoencoder
consists of an encoder  which is a pair  w    b     with
w   rdd   b   rd   and a decoder  which is a pair
 w    b     with w   rdd   b   rd   we also need two
mappings f  g   r  r  that can be applied elementwise to vectors and matricies  the encoder then generates a new space by the map  x    f  w  x   b     and
the decoder reproduces the original vector by the map
 x    g w   x    b     to ensure this new space represents the original data vector well  we would like to find
wi   bi such that x    x  
b 

stacked denoising autoencoders

the basic autoencoder aims to create a lower dimensional coding that contains all information about the
original data vector  specifically  over a data batch
x  rdn   a standard autoencoder defined by the loss
function
l x     

 
kx  g w  f  w  x   b   t     b   t  k f  
 n

where x  p  x x  is a stochastic  corrupting mapping  common examples of corruption mappings are
 p  x x    x fi b  bij  bernoulli p   i e   a binary mask 
 p  x x    x n         where  n        ij  n        
we greedily train each denoising autoencoder  stacking
as we go along  at the top level of such a stack  we hope
to have a set of features that is both robust in construction and captures correlations between words that are
useful across domains 
now  we examine the training procedure of the first
layer  using different variants of autoencoders with
different regularization procedures to understand what
each layer will accomplish  consider fig     we see
that we have better test sample performance with denoising autoencoders with gaussian noise when compared
with the normal type autoencoders  this makes sense
 our generalization to examples should be better with
imposed denoising criteria  we proceed with a similar
training procedure layer by layer  helping the network
learn a stacked  hierarchical representation of our training sample 

 
kx  g w  f  w  x   b   t     b   t  k f  
 n

has no regularization terms  and aims to build the best
reconstruction possible  however  we have no guarantees
as to whether or not the locally optimal mapping we find
to generate this coding is robust  that is  how sharply
does the reconstruction error varies around a given data

c 

stacked thesauri

we present a modification of dense cohorts of terms 
as introduced in      in particular  we aim to reconstruct
smaller and smaller subsets of our vocabulary in each

fi 
layer  corresponding to higher and higher levels of semantic meaning  instead of creating a coding that contains all information contained in data vectors  we instead want to create a mapping that finds ways to map
corrupted  rare words into a set of more common words 
in a sense acting as a thesaurus  let our vocabulary
be of size d  and assume we obtained a bag of words
representation x  rnd   there all nonzero entries have
value   for ease  now  suppose we want a deep model to
learn what it means for words to be synonyms  that is 
can we link many uncommon words such as exquisite 
marvelous  and impeccable to a more common word
like good 
we take the top r   d words in terms of frequency 
and construct a  binary  bag of words representation
p r   rnr   now  we ask  can we find a mapping
such that  xw     p r    where    is the standard
sigmoid  in order to be resistant to any differences in
distribution  we can corrupt x with bernoulli noise 
that is  we set each non bias feature to zero with some
probability p 
to stack  we use the output of the previous layer  call
it z        mr   as input  and we simply now choose
a smaller target thesaurus size  let this be r    we
now wish to find a mapping  zw     p r      we can
repeat this in a greedy manner until we have the desired
structure we utilize the same structure as outlined in
fig    

v 

results

for the following results  the classifiers were trained
on a total set of         training examples and        
test examples  the data was pruned in a preprocessing
step from an even bigger dataset in order to reduce class
imbalance  as explained before  the cross domain sets
contain        tweets and        ebay reviews 
method
naive bayes
sgd
svm

train
    
    
    

test
    
    
    

ebay twitter
    
    
    
    
    
    

table i  f   scores for classical methods

in table i  we can see how the best amazon training and amazon test results are yielded by the most
sophisticated algorithm  the linear support vector machine  however  we take note how this improvement do
not translate cross domain  where the performance even
decreases  this is a clear signal of the fact that there are
distinct characteristics in different text domains  in fig 
  we can appreciate how the f  classification scores for
the domains in ebay and twitter stop increasing rather
soon  even worsening when the models are fitting better
the training domain 
next  consider results  table ii  from deep learning
using the aforementioned pretraining methods  stacked

fig     learning curve for the linear svm

autoencoders  sae   stacked denoising autoencoders
 sdae   and stacked thesauri  sthi  
method
sae
simple softmax sdae
sthi
sae
fine tuning
sdae
sthi

train
    
    
    
     
     
     

test
     
    
    
    
     
     

ebay twitter
    
     
           
           
    
    
    
    
     
     

table ii  f   scores for deep learning

vi 

discussion

it is clear from our results that it is not possible to fully
extrapolate a model to a different domain and expect
similar behavior  while the combination of our techniques did allow for accurate cross domain classification
for a few subsets of product reviews  we were not able to
develop a completely accurate model that could be extrapolated across all domains  among classical classifiers
used  support vector machine produced the strongest results on the original data set  however  as with the other
classifiers  cross domain results were relatively weak 
particularly we experienced particularly low performance for the twitter data set  which can be partially
attributed to the inability of our model to fully take into
account the slang based vocabulary  while amazon reviews tend to be reasoned opinions about a product 
tweets are extremely short texts using a very particular vocabulary  many times utilizing hashtags and varied abbreviations  semantic units that were in no way
present over the data the model was trained over  more
surprising is the difference in the performance of the classifiers in the realm of ebay product reviews  where one
would suppose a strong resemblance to those of amazon 
however  from the results  and assuming that the data

fi 
is correctly labeled  we can conclude that the users vocabulary when reviewing products in these domains are
different 
it is important to note that deep learning did nonnegligibly improve the performance of out classifiers on
the original data set while also increasing results on the
cross domain data sets  most notably  the ebay data
set  when compared to shallow counterparts 
our results from deep learning fit nicely into our beliefs with respect to how we think regularization will
help out model extrapolate to a priorless  unseen domain  in particular  note that pure sae with fine tuning
performed best on the training set of amazon  an unsurprising fact  seeing as though sae try to fit an exact
reconstruction during training time  note that with this
excellent training set performance comes serious difficulties generalizing to our cross domain targets  in particular  we notice in table ii that we obtain f  scores
that are just as bad as the classical linear svm 
however  we make note of what happens when we
impose the restriction that our model must denoise as
well as reconstruct  in particular  we notice that though
our amazon training set accuracy decreases  our performance on the amazon testing set increases  in this
regime  it is impossible to overtrain the pretrained model
 every training example is noised in a different manner each time  also noteworthy is the increase in crossdomain performance  though we obtain better generalizability by simply appending and training a softmax
layer 
most interesting  we see that our stacked thesauri
yield the best cross domain performance  we postulate that this is due to the fact that the representations learned by sthi are dense in commonly used terms 
which  we believe  are more constant across the domains
we considered than the language distribution at large 
the ability to encode synonyms allows greater flexibility
when dealing with limited vocabularies  and reduces reliance on product specific jargon or vernacular to make
predictions 

vii 

future work

zon product hierarchy  moreover  our project could be
extended in a follow up study to detect differences in
the vocabularies used in the different domains  and finetune our approaches to overcome these differences and
increase cross domain classification performance  in addition  it would be interesting to see if we could design
some sort of generative model related to conditional restricted boltzmann machines which would help us condition on small subsets of the distribution as we learn
more about the different domains we apply to 
we could also modify our principal data set  perhaps
training on the domains with more specific vernacular
such as twitter with its slang based vocabulary  in addition  we would be interested in using models such as
the over replicated softmax model as an alternative approach to feature engineering 

viii 

conclusion

we examined the cross domain classification of online user behavior in terms of text  in particular  we
asked  can we use labeled amazon data to extrapolate
out of domain and determine what products individuals in twitter and ebay are discussing  while classical
methods performed relatively well on the original data
set  they were inherently unable to translate this performance to the cross domain problem  our deep models
with fine tuning provided a considerably higher degree
of accuracy on the original data set and on the crossdomain problem  in particular  we showed promising
results using stacked thesauri  a method that relies
on stacking layers combining semantically similar words 
however  we have found that without prior knowledge
of the language distribution  it is a difficult task to extrapolate machine learning models to different domains 
even with the powerful and generalizable toolbox of deep
learning 

ix 

acknowledgements

firstly  with greater computing resources  it would be
interesting to improve the efficiency of our schema and
apply it to the complete set of labels from the ama 

we wish to acknowledge professor ashutosh saxena
and aditya jami for the very interesting project idea
and for providing the data used 

    sha fei gong boqing and kristen grauman  overcoming dataset bias  an unsupervised domain adaptation
approach  nips       
    yun  jiang and ashutosh saxena  discovering different
types of topics  factored topic models  cornell  august
     
    nitish srivastava  ruslan salakhutdinov  and geoffrey e 
hinton  modeling documents with deep boltzmann machines  corr  abs                  url http   arxiv 
org abs           
    beal m  teh y   jordan m  and d  blei  hierarchical

dirichlet processes  berkeley  march      
    zhixiang  eddie  xu  minmin chen  kilian q  weinberger  and fei sha  from sbow to dcot marginalized
encoders for text representation  in proceedings of the
  st acm international conference on information and
knowledge management  cikm     pages          
new york  ny  usa        acm  isbn                   doi                           url http   doi acm 
org                         

fi