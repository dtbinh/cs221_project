determining mood from facial expressions
cs     project  fall     
matthew wang
spencer yee
mmwang stanford edu
spencery stanford edu
i

introduction

facial expressions play an extremely important role in human communication  as
society continues to make greater use of human machine interactions  it is important for
machines to be able to interpret facial expressions in order to improve their
authenticity  if machines can be trained to determine mood to a better extent than
humans can  especially for more subtle moods  then this could be useful in fields such as
counseling  this could also be useful for gauging reactions of large audiences in various
contexts  such as political talks 
the results of this project could also be applied to recognizing other features of facial
expressions  such as determining when people are purposefully suppressing emotions or
lying  the ability to recognize different facial expressions could also improve technology
that recognizes to whom specific faces belong  this could in turn be used to search a
large number of pictures for a specific photo  which is becoming increasingly difficult  as
storing photos digitally has been extremely common in the past decade  the possibilities
are endless 

ii

data and features

   
data
our data consists of      frontal images of
peoples faces from three databases  with each
image labeled with one of eight emotions 
anger  contempt  disgust  fear  happiness 
neutral  sadness  and surprise  the tfeid     
ck       and jaffe     databases primarily
consist of taiwanese  caucasian  and japanese
subjects  respectively  the tfeid and jaffe
images are both cropped with the faces
centered  each image has a subject posing with
one of the emotions  the jaffe database does
not have any images for contempt 

figure  

happiness

anger

   
features
on each face  there are many different facial landmarks  while some of these landmarks
 pupil position  nose tip  and face contour  are not as indicative of emotion  others
 eyebrow  mouth  and eye shape  are  to extract landmark data from images  we used

fiface        a publicly available facial recognition api  since face   face detection
can only take a url as a reference to the image  we used imageshack     to host the
images from our database  we used     of the features given by face    which include
a smiling metric and the x  and y coordinates of    facial landmarks  shown in figure   
since each image can have differently sized faces at arbitrary locations within the image 
we normalized each image as follows  we translated each face in order to center the eyes
around the origin by using the x  and y coordinates of the center of each eye  and then
scaled each image to fix the distance between the centers of the eyes to a constant 
upon normalizing and cross testing databases to see how well a model trained on one
database could classify another database  we realized that  since each database was
homogenous in terms of race  faces from one database were consistently differently
shaped than faces from another database  see figure     furthermore  using only
positions of individual landmarks results in missing information  because the positions
are not independent 
figure  

these are scatterplots of the    landmarks for all of the
images in each database after normalization 
ck 
tfeid
jaffe
therefore  from these facial landmarks  we derived    more features from angles
between certain landmarks that we decided varied among emotions  for example  the
angle formed by the two corners of an eyebrow and the center of the eyebrow shows the
eyebrows shape and how much it is raised  likewise  the angle formed by the corners of
the nose and the nose tip is a good measure of how much the nose is scrunched 
furthermore  using angles can scale the intensity of emotions more accurately than a
simple difference in y coordinates between certain landmarks  for example  the
difference in y coordinates for the mouths landmarks between a neutral expression and
a smile is similar to the difference in y coordinates between a smile and a wider smile 
but the former is clearly more significant  thus  we calculated    angles that we
thought would vary with emotion and used these as additional features  for a total of
    features 

fiiii

models

   
softmax regression
we used the matlab built in function with a feature set of the    angles we selected 
because our entire feature set was too high of a dimension to efficiently be calculated
with softmax regression  the parameters of the model are those that maximize the loglikelihood function 

   
multiclass support vector machine
we used the libsvm library     to train our data with a multiclass support vector
machine  specifically  we used c support vector classification with a gaussian radial
basis kernel function  we experimented with different values for the parameters   in
the kernel function  and c  the regularization parameter  and chose the values   
       and c       for the kernel equation 

iv

results

we used all      of our images to run tests  accuracy is calculated as the percentage of
images that were classified correctly  and precision and recall are calculated as the
average of the precision and recall for each emotion 
test
training
  fold cross validation
  fold cross validation

softmax regression
accuracy precision recall
      
      
      
      
      
      
      
      
      

multiclass svm
accuracy precision recall
      
      
      
      
      
      
      
      
      

actual emotion

the svm model clearly fit our data better than the softmax model did  so we chose this
as our final classifier  here are some more detailed results for the svm   fold cross
validation test 
confusion
matrix
anger
contempt
disgust
fear
happiness
neutral
sadness
surprise

anger
   
 
  
 
 
 
  
 

contempt
 
  
 
 
 
 
 
 

predicted emotion
disgust fear happiness neutral
 
 
 
  
 
 
 
  
   
 
 
 
 
   
 
 
 
 
   
 
 
 
 
   
 
 
 
  
 
 
 
 

sadness
 
 
 
 
 
 
  
 

surprise
 
 
 
 
 
 
 
   

fiemotion
precision
recall

anger
      
      

contempt
      
      

disgust
      
      

fear
      
      

happiness
      
      

neutral
      
      

sadness
      
      

surprise
      
      

as expected  happiness and surprise were more easily expressed and identified than the
other emotions  probably because certain characteristics  such as a smile or a wide open
mouth  are very distinguishable  interestingly  sadness  quite a common emotion  ended
up being misclassified most often  looking at the confusion matrix  however  we can see
that it usually gets confused for anger or neutral  this relationship also works the other
way  with most improperly classified sadness images actually being anger or neutral 
there are several other pairs that our classifier often gets confused  the most notable
ones are anger and neutral  anger and disgust  contempt and neutral  and fear and
surprise  this is not surprising  since the two emotions in each pair tend to produce
similar facial features  for example  people commonly express both fear and surprise
with lifted eyebrows  wide eyes  and an open mouth 
for the poster presentation  we developed a live demonstration in which people could
take a picture in photo booth and have their emotion classified by our svm model 
upon testing our algorithm on other people  we noticed that different people may
express the same emotion in different ways  and that not all of these expressions were
captured by our model  some people are also much less expressive than others  and
some are often unsure of how to express a certain emotion  in particular  most people
were confused when asked to try contempt  emotion can be very subjective in and of
itself  so it is probably difficult to achieve a significantly higher accuracy  this is already
observable in our databases  which tend to be homogenous within themselves  some of
the similarities among images within a database can be attributed to general facial
features of particular races  but it is worth noting that all of the subjects in the tfeid
database express contempt with a twist of the mouth to one side  therefore  it is
possible that the subjects in each database were told to consider certain facial
expressions while simulating each emotion  finally  it is important to keep in mind that
facial expressions are more complicated than pure expressions of exactly one emotion 

v

future work

as we refine our algorithm  it is important that we obtain access to a much larger and
much more diverse database to make our model more robust to different people 
currently  we only have      training examples from three different databases that tend
to use models of the same race  expanding this number to around ten or twenty
thousand training examples could help the algorithm classify each emotion more
accurately 
furthermore  we could focus on pairs of emotions that tend to get confused and identify
features that would specifically help distinguish between those emotions  this would
lessen confusion between specific emotions and improve overall accuracy 

fianother set of features that we could add is the orientation of each of the    angles on
the face  for example  even though we know the angle between the corners of the
eyebrows and the top of the eyebrow  we do not know how the eyebrow is positioned  it
could be slanted outward to convey sadness or inward to convey anger  this would also
help distinguish between different mouth and eye positions  both of which could affect
classification  taking this factor into account would give the algorithm a better
representation of the face and help it differentiate between emotions 
it is also important that we begin to move away from using the    landmarks directly
as features since position of features is largely dependent on the innate shape of the face 
we could add an intermediate step between landmark identification and feature
selection  based on the landmarks that the face detection api gives us  we can train
another model to identify certain facial structures and feed a representation of these
facial structures to a multiclass svm to classify emotion  for example  we could
combine information about the position of landmarks of the nose along with the angle
between some of these positions in order to determine whether or not the nose is
scrunched 
it might also be worth investigating how mirroring a face affects the emotion
classification  for emotions that are generally accompanied by asymmetric faces  it
might help to normalize the faces so that the side with a certain characteristic  such as a
wink  is always on the same side 
lastly  as our algorithm currently only takes into account frontal images  rotating a face
in any direction would render it ineffective  taking into account facial rotation about all
axes could help our algorithm identify emotions of rotated faces more accurately 

vi

references

    li fen chen and yu shiuan yen          taiwanese facial expression image
database  brain mapping laboratory  institute of brain science  national yang ming
university  taipei  taiwan 
    lucey  p   cohn  j  f   kanade  t   saragih  j   ambadar  z     matthews  i 
        the extended cohn kanade dataset  ck    a complete expression dataset for
action unit and emotion specified expression  proceedings of the third international
workshop on cvpr for human communicative behavior analysis  cvpr hb       
san francisco  usa         
    michael j  lyons  shigeru akemastu  miyuki kamachi  jiro gyoba 
coding facial expressions with gabor wavelets   rd ieee international conference on
automatic face and gesture recognition  pp                 
    http   www faceplusplus com
    https   imageshack com 
    chih chung chang and chih jen lin          libsvm  a library for support
vector machines  national taiwan university  taipei  taiwan 
http   www csie ntu edu tw  cjlin libsvm 

fi