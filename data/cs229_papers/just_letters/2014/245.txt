correlation based multi label classification
amit garg  amit     jonathan noyola  jnoyola   romil verma  vermar 
ashutosh saxena  phd   aditya jami
final project  cs      cs     fall     
stanford university

i 

where yi           j  k   i yi             

abstract

this paper attempts multi label classification by extending
the idea of independent binary classification models for each
output label  and exploring how the inherent correlation
between output labels can be used to improve predictions 
logistic regression  naive bayes  random forest  and svm
models were constructed  with svm giving the best results 
an improvement of       over binary models was achieved
for hold out cross validation by augmenting with pairwise
correlation probabilities of the labels 
ii 

introduction

multi label classification is the set of classification problems
where the output vector has a variable length  the average
number of labels per review varies across datasets  and in
general is a function of the semantics of the text rather than
the syntax  the learning algorithm needs to estimate the
number of labels and make the correct predictions 
previously multi label classification problems were solved
using problem transformation techniques  converting the
problem into binary classification problems per output label  
or by adapting the algorithm to directly perform multi label
classification      this paper extracts correlation information
between labels and factors the joint probabilities into the
model 
iii 

task definition

given large datasets of product reviews from amazon
and twitter and manually labelled multi label classifications
for each  ground truth   the algorithm aims to predict all
classifications for a review      we aim to make the algorithm
portable across various datasets  i e   training a model on
amazon reviews and using it to classify tweets from twitter 
iv 

theory

our approach augments the independent binary classification model  in addition to the individual probabilities  this
also looks at the probability of labels occurring together 
hence the inference algorithm models the formula 

 y 
y 

 m 
 m   m 
p y  m   x m   
p yi  x m 
p yj   yk
i

the pairwise correlation probability between each pair of
labels is stored in the correlation matrix  this normalized
rows of the matrix represent probability of a particular
labels coupling with every other label except for itself  this
probability is computed as a prior for the entire dataset and
is not dependent on the input feature vector itself  it is
 m   m 
represented by p  yj   yk   


q
 m   m 
represents the joint probability of all
p
y
 
y
j
j k
k
pairwise combinations of predicted labels  each discounted
by   the probabilities obtained from the correlation matrix
are given less weight than the probabilities obtained from
independent models  hence the discount  this is because
co occurrence of labels is not completely characterized by
correlation  it also requires the higher order moments which
significantly increase the computational overhead 
vi 

methodology

the entire algorithm is split into   steps after parsing  preprocessing  training independent classifiers  and
incorporating co occurrence probability to make the final
label set  hyper parameters need to be tuned for each dataset 
liblinear works well for larger datasets while libsvm
works well for custom and smaller datasets  liblinear
only supports a linear kernel but is very fast relative to
libsvm and hence used in the learning algorithms     

dataset

the amazon dataset contains reviews for         products
across        different categories  among these  we focused
on books and book subcategories  the twitter dataset has
        tweets  both the datasets have the same schema  and
each review and tweet has a unique id  pre pruned content 
and a tree of all of the product labels for the review up to the
book category at the root 
v 

 m 

p  yi  x m    represents the probability of the independent
binary model for label i classifying as   or   given input x 
q
 m   m 
 x
represents the assignment of   or   to all
i p yi
yi that maximizes the joint probability 

j k

parsing

the amazon and twitter datasets are parsed to extract
book reviews and their corresponding labels  ground truth  
the labels are structured in a hierarchy   for books  there
are    top level labels and for each label there are several
sub labels  for instance a top level label is literature
and fiction and few sub labels associated with it are
folklore  mysteries and classics  the reviews are
labelled with these sub level labels which we map to one
of the    top level labels  and use for multi label classification 
preprocessing

training and testing of the algorithm is done on at most
      reviews per dataset due to computational constraints
during learning  each book review is mapped to a bag ofwords based input feature  on parsing a review  its content
is pruned and stemmed and then a tf idf based vector is

fi 
generated which is used as the input feature x  the labels
corresponding to this review are associated as its outputs y 

vii 

error metric

the overall prediction error can be seen as a combination
of two different kinds of errors   false positives and false negatives  false positives are the labels that are in the predicted
label set but not in groundtruth  and false negatives are the
labels present in groundtruth  but not in predicted label set 
we do not weigh the two identically  but rather give slightly
higher importance to false negatives  the reason behind this
is that over predictions can be further controlled using added
heuristics  but the labels missed can never be regained without
looking at the entire output label set again 



total labels correctly predicted
error     
number of labels in ground truth


total labels correctly predicted

total labels predicted

fig     correlation matrix of    book labels from amazon

the correlation matrix is built by parsing         book
reviews and creating pairwise counts of different labels  a   
    row normalized matrix with laplace smoothing  after
normalization the matrix is not symmetric  so the geometric
mean of cells  i  j and  j  i  is treated as the actual correlation
probability for two labels  fig    plots the row normalized
covariance matrix for amazon dataset  the existence of large
peaks is the motivation behind the algorithm 
algorithm

 the inputs to the learning algorithm are tf idf based
input features  their manually assigned labels  and the
prior generated correlation matrix

 is a hyper parameter which cannot be tuned since that
would put it to   and j and k as     i e  output all the
labels  irrespective of the input  since this gives zero error 
viii 

results

hold out cross validation with         split
  

train on amazon and test on amazon dataset

fig    gives the performance of our algorithm with the
   independent models as the dataset size is varied  as
is evident  the algorithm extracts about     improvement
over the baseline  just by looking at the correlation matrix 
after tuning the hyper parameters  this thus confirms our
assumption regarding the utility of the correlation matrix for
multi label classification with variable number of outputs 

    independent binary models are trained using l regularized svm  this serves as the baseline 
 for a review  the j labels with highest probabilities from
the independent models are selected 
 probabilities of pairwise combinations of these labels are

computed as p  a  p  b  p  a  b 
 k  a  b  pairs with maximum probability product are
chosen and distinct labels constitute the predicted set 
the algorithm does not compute the theoretical model
exactly but is an approximation  it is similar to beam search 
hyperparameters
hyper parameter
solver


j
k

for amazon
for twitter
l  regularized svm l  regularized svm
   
    
   
   
  
  
 
count predicted by
baseline

table i  hyperparameters for amazon dataset

fig     percentage error with train  test on amazon dataset
  

train and test on twitter dataset

fig    is the performance of the algorithm when the
twitter dataset is used both for training and testing  after
appropriately tweaking the hyper parameters  the gain is

fi 
only about       which can be attributed to the scarcity of
content per twitter review  the dataset size for twitter was
about     of amazons for same number of reviews   upon
inspecting the output labels predicted by our algorithm 
it was apparent that  the over predictions were the major
contributors to the error  unlike amazon where misses were
the prime error contributors 

fig     percentage error while training and testing on twitter
dataset
  

collection of independent binary models for the hold out
cross validation  the same did not hold true for the results
obtained from the k fold cross validation 
training and testing on amazon no longer gave any
improvement  but about a    degradation in performance 
the results for twitter dataset also substantially changed 
but since both the algorithms got similar shifts  the actual
improvements were still marginal as before 

fig     k fold cross validation when training on amazon and testing on amazon dataset

train on amazon and test on twitter dataset

for this case  we train on      data from amazon and test
on      data from twitter  fig    gives the performance of
our algorithm when the training was done using amazons
dataset and testing was done on twitters dataset  the
hyper parameters used are the same as those for fig    thus 
the improvement observed is much smaller  the reason being
the same  i e   scarcity of content per tweet 

we attribute the correlation algorthims poor performance
to two causes  first  amazon datasets number of labels per
review has a mean of      and a standard deviation of     
because the standard deviation is reasonably high  it can
be expected that the average number of labels per review
for a given partition may be quite different from the global
average  this causes our algorithm to produce many false
positives when testing on some partitions where the number
of labels per review is much lower than average 
also  because our fixed correlation matrix is calculated
from         samples tending towards the true correlation
values  some test data partitions are likely to deviate from this
average  highly biased test datasets   causing our algorithm
to predict based on the true correlation  while the baseline
algorithm ignores correlation and trains only on the training
data  this hypothesis is supported by smoothing out the
bias by increasing k  in which case the two algorithms errors
converge 
comparing learning algorithms

fig     percentage error while training on amazon and testing on
twitter dataset
    k fold cross validation

k fold  k     cross validation revealed some important
information regarding the algorithms performance  while
our algorithm had observed a      improvement over the

we compare the baseline results of three different models 
svm  naive bayes  and random forest  naive bayes did not
scale well  but converged quickly  additionally  naive bayes
in scikit learn     returned binary probabilities which do not
lend themselves well to multi label classification where each
review has a variable number of labels  svm however  had a
noticeable trend and scaled best with increasing dataset 
then each model was augmented with the correlation
matrix  svm gave best results and was then optimized by
adjusting hyper parameters 

fi 
marginally reduced the error hence did not continue further 
ix 

analysis

precision  recall and f  score

train and test on      amazon reviews       

actual

predicted
correlated
baseline
tp     
fn    
tp     
fn     
fp     
tn      
fp    
tn      

table ii  confusion matrix for train  test on amazon

fig     k fold cross validation for train and test on twitter

baseline
correlation

precision
      
      

recall
      
      

f 
      
      

f
      
      

table iii  scores for train  test on amazon dataset

train and test on      twitter reviews        

actual

predicted
correlated
baseline
tp     
fn    
tp     
fn    
fp    
tn      
fp    
tn      

table iv  confusion matrix for train  test on twitter

fig     baseline error for svm  naive bayes  and random forest

baseline
correlation

precision
      
      

recall
      
      

f 
      
      

f
      
      

table v  scores for train  test on twitter dataset

train and test on      amazon  twitter reviews

actual

predicted
correlated
baseline
tp     
fn     
tp     
fn     
fp      
tn      
fp      
tn      

table vi  confusion matrix for train amazon  test twitter

baseline
correlation
fig     correlation algorithm error for svm  naive bayes  and
random forest

other approaches considered

another approach was to train    c         per label pair 
separate correlation models  this conditioned the correlation
probability on the train data instead of being fixed for a
dataset  this increased error slightly and the computational
time greatly  this method did not perform as well because 
in the new correlation models the error to match an input
vector to a label got compounded in with the correlation
error  as a fix  the algorithm weighed both the correlation
probability by  and independent probabilities by      this

precision
      
      

recall
      
      

f 
      
      

f
      
      

table vii  scores for training amazon  test twitter

the correlation algorithm weighs false positives and false
negatives differently  hence the f score gives a more accurate
understanding of the algorithms performance  f for the
correlation algorithms test on amazon is        better than
the baselines test        better than the baselines test on
cross domain learning  and      worse than baselines test
on twitter  this validates the trend in the plots 
the false positive counts when training and testing purely
on one dataset are usually higher than the false positive
counts from the baseline  as in our approach we tend to

fi 
discount the error of making false positives given we meet
true positives 

higher than training error for both algorithms 

table v shows the confusion matrix for cross domain
learning where the correlation algorithms true negative
count is much higher than that of the baseline  by using the correlation between output labels  the algorithm
discards labels that independently had a high probability
but were not well correlated with other high probability labels 

using the correlation matrix to help classify reviews
appeared to be a good approach  justified by peaks in
correlation  the results show  however  that when performing
multi label classification with a sufficiently sized feature set 
augmentation of an independent model with second order
correlation probability shows only marginal improvements 

bias and variance

the model was modified from the baseline by adding a
single feature  second order correlation  this is the greedy
method of selecting features  in contrast to exploring correlation of all combinations of labels  in such a case where we did
not explore higher order correlation we could have missed 
for example  that labels a  b  and c never occur together
 a piece of information that could have proved vital to the
models representation of the true data  thus  if computation
time and space are not an issue  higher order correlation
should be considered 

x 

as evident  the observed training error for each dataset
was much less than the observed test error  indicating high
variance and low bias  to further investigate  principal
component analysis was used  since the feature vectors were
based on word occurrences in text  they were significantly
larger than the dataset size  the observed feature size for
     dataset size of amazon reviews was        due to computational limitations  straightforward pca was infeasible 
sparse notation was used to represent feature vector  from
pythons numpy library  then sparse svd  singular value
decomposition  found the smallest subspace that the feature
matrix mapped to  keeping all singular values greater than   
this significantly reduced the dataset size  the      dataset
of amazon reviews  now reduced to a feature vector of size
      while the number of features was still comparable to
the number of data points  it was substantially lower than
the size of the raw feature vector       
this experimentation exposed two facts  the errors were
still the same  but the computation time rose significantly
 about five times the previous duration   it was seen that
liblinear  the python library  was tuned to work with large
datasets with document based feature vectors  as compared
to libsvm   and not for any general features 
since the baseline itself suffers from the issue of high
variance and low bias  any augmentations over it would
be unable to resolve this by themselves  and would require
modifying how the tf idf vectors are generated  we had
already stemmed the words to check this issue  but evidently
this would require a more careful processing of the reviews 
before they are converted to features  moreover  since svms
enforce larger margin as a metric to evaluate each point  they
end with a much lower vc dimension than the size of the
feature vector  this was another reason in favor of svm over
logistic regression  naive bayes  and random forest 
the baseline resulted in about     training error  while
our algorithm resulted in about     training error  thus
there is an increased bias  although the higher variance is
a more pressing issue because the test error is significantly

    tsoumakas  grigorios  katakis  ioannis         multilabel classification 
an overview   international journal of data warehousing and mining       
   
doi        jdwm           
    i stanford edu  adityaj cs    fall     dataset html
    d  albanese  r  visintainer  s  merler  s  riccadonna  g  ju 

conclusion

additionally  svm  as was already known  has shown to be
best for learning on large feature sets because it will attempt
to reduce the set and hence generalize better 
xi 

challenges   future work

the correlation matrix was built specific for every dataset 
so to apply the learning algorithms on cross datasets a
generalized correlation framework is needed 
currently the hyper parameter k is optimized to choose
number of predicted labels using supervised techniques  the
number of labels for each review could be predicted by using
unsupervised techniques such as k means clustering on label
probabilities 
the datasets have different inherent structures which do
not generalize well when using a common learning algorithm 
for the twitter dataset  each feature vector is about     the
size of a corresponding feature vector in the amazon dataset 
the features should be supplemented with options such as
semantic relations in text to make classifications and context
of user history 
the reviews have sub level labels associated with them
which are rolled up to first level labels to make the classifications  the algorithm should extend to make hierarchical
classifications 
xii 

acknowledgments

sincere thanks to professor ashutosh saxena and to aditya
jami for advising the research team and providing the labelled
datasets of product reviews from amazon and twitter 

rman  c  furlanello  mlpy  machine learning python       
arxiv          
    scikit learn  machine learning in python  pedregosa et al  
jmlr     pp                  

fi