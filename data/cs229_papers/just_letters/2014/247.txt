final report for cs     fall     

 

personal legal counselor and interpreter of the
law via machine learning
derek yan  tianyi wang  patrick chase
 zhyan  tianyiw  pchase  stanford edu

abstractthe goal of this project was to predict the
likelihood of winning a new legal dispute based on results
of past cases  we collected over      legal proceedings in
the form of case briefs from the internet and used various
language processing techniques to parse the raw text into
feature vectors  we then used this feature vectors to train
several binary classification algorithms  including naive
bayes  random forest  logistic regression and an svm 
the svm model achieved the highest test set accuracy of
     which was an improvement over the random    
baseline  in this paper  we explained the details of how we
transformed the raw text of the case briefs into feature
vectors  and how we used them to build several models
for prediction  we then discussed the results obtained by
each of the models and suggested future work that could
be done in the area 
index termslaw  machine learning  case briefs  court
cases 

i  i ntroduction
he first thing we do  lets kill all the lawyers
  william shakespeare    henry vi         
any major transaction  legal procedure  or patent
dispute always requires an attorney at law in the due
process  however  paying an attorney  even for a consultation  can be very expensive and out of reach for much
of the general population  due to the exorbitant cost of
legal action  many cases are unresolved or dropped  our
goal was to create a tool that would provide legal counsel
to people who would otherwise not have access to it  in
particular  our model would tell someone the probability
they have of winning a given case  which would allow
them to make the a better decision of whether or not to
pursue further legal action 

fig     processing flow

t

ii  dataset
the processing flow is shown in the fig   
our dataset was obtained from www casebriefs com 
it consisted of       legal case briefs  where each case
brief was split up into four segments of text  the parties
in dispute  the summary of facts  the issue of law  and
the verdict  fig    is an example of a very short case
brief 

fig     example case brief

iii  baseline and o racle r esults
before tackling the actual problem  we first considered taking the baseline and oracle results of the legal
case predictor  the baseline solution was using linear
regression to serve as a predictor of future cases  we
used   grams from the input text for feature extraction 
when the new input was given  we extracted the features 
and then used stochastic gradient descent to come up
the learning predictor to estimate the likelihood of the
plaintiff winning the case  for the test data  we had an
accuracy rate of around     
the oracle was a manual interpretation of facts and
issues given a test case by the group members  we read
over the facts of a legal proceeding  the interpretation of

filaw which was under question  and used our common
sense to give a probable decision  for legal matters that
were not familiar to us  we would research laws of such
matter and make human predictions  this would serve
as our oracle  for our test cases of     we had an error
rate of     

we also experimented with adding bigrams and trigrams to the feature vectors  for naive bayes  random
forest  and logistic regression  the huge increase in
the number of features caused the algorithms to take
too long or run out of memory  however  the svm
implementation  which took in a sparse matrix input 
was able to run with bigrams and trigrams 
for example  if the case description was john was
owed two weeks of pay for failing to submit his
timecard  after removing stop words and transforming
words to their stems  we get  john  owe  two 
week  pay  fail  submit  timecard   now we
count how many times each word and word gram appeared  and form the dictionary feature vector we need 
 john      john owe      john owe two      owe
         

iv  f eatures and p reprocessing
first  we parsed the verdict section to obtain the binary
labels for the cases  when the verdict was held  meaning
the answer to the issue of law was yes  we gave a
positive label to the example  but when the answer to
the issue of law was no we gave a negative label to
the example 
after extracting the labels  we found that there were
     positive cases and      negative cases in our entire
dataset 

v  m odels
for all the models we used the scikit learn package
    for python 

a  training and testing data
we then split up the data into the training and testing
datasets described below 
total examples
positive examples
negative examples

training dataset
    
    
    

a  naive bayes
the implementation of naive bayes that we used
assumed that the likelihood of the features was normally
distributed 


 
 xi y   
p xi  y    q
exp
   
  y 
  y 

testing dataset
   
   
   

since we had a sufficient amount of positive and
negative examples  we chose training and testing datasets
with equal number of positive and negative samples to
make it easier to analyze and compare the performance
of our models 

there were no other parameters to set for naive
bayes  so after choosing the distribution to represent the
likelihood of the features  we trained our model 
b  random forest
when we first ran the out of the box random forest
classifier from scikit learn  we got an accuracy on the
training set of     and an accuracy on the testing set
of around      illustrating that we were overfitting the
training data  to remedy this issue  we set the max depth
of the tree to be    which greatly improved our overfitting
problem 

b  feature generation
to create the feature vectors  we used the summary
of facts section and issue of law section  we did not
use any of the text from the verdict section because that
section was used to determine the positive or negative
label 
first  we processed the raw text of the case briefs by
transforming each word to its stem using the lancaster
stemmer from the nltk  natural language toolkit     
we then formed a dictionary by scanning through all the
words in our dataset  after forming the dictionary  we
used the dictionary to represent the case briefs using the
bag of words representation  in our representation  the
ith element of the feature vector for an example corresponded to the number of times the ith stem occurred in
the given case brief  in the  key  value  pair  key string
equals the stem  resp  n gram  string  and value equals
the count of that stem  resp  n gram  

c  logistic regression
when modeling the outcome of a legal case with
logistic regression  we had the same issue with the outof the box algorithm  we were drastically overfitting the
training data  which led to a poor accuracy on the testing
set  to fix this issue  we used a logistic regression model
with l  regularization  this decreased our training set
accuracy from     to      and improved the performance of the model on the testing dataset 
 

fid  svm
svm model was implemented with the libsvm 
which supports training  predicting  configuring regularization  cross validation and so on  the input feature
vector used in libsvm was defaultly dictionary  but the
keys of the  key  value  must be int type  so there was
a transformer that compressed all the keys that appeared
in the output dictionaries of the feature extractor into a
vector  and then used the index of each element in the
compressed vector as the key of the input dictionaries of
svm 
at first svm was heavily favoring negatively labeled
examples  i e  it was more likely to predict new examples
as negatively labeled  we fixed this by setting the gamma
variable of radial basis function kernel to       
then svm with features of word counter gave around
    accuracy on the training set but around     accuracy on the testing set  which indicated a overfitting
problem  to solve the overfitting problem  we integrated
nltk and changed word to stem  which decreased the
feature set  another technique we used was to remove
stop words  such as to  and and is  from the feature
set  we also tried removing the words that have too high
or too low appearance frequency from the feature set 
considering those words might be meaningless  but this
technique did not give quality improvement 
our work decreased the training accuracy to around
    and increased the testing accuracy to around     
the fact that the training error was similar to the testing
error indicated that the overfitting is fixed 

fig     naive bayes confusion matrix

vi  r esults
the table below shows the training and testing error
for each algorithm 
model
naive bayes
random forest
l  reg  logreg
svm

training set accuracy
      
      
      
      

testing set accuracy
      
      
      
      

here we see that the svm had the best performance
on the testing dataset  it was able to achieve      
accuracy 
in addition  fig     fig     fig     fig     show the
confusion matrices for each classifier  in the discussion
section  we compare and contrast these results 

fig     random forest confusion matrix

model with the largest positive magnitude and the top
five features with the largest negative magnitude  after
mapping these stems back to their original words  we
created the table illustrated in fig     we do see some
interesting domain specific words come up  such as
evidence  it makes sense that the frequency of a word
like evidence in the case brief would lead to a larger
probability of winning the case because there is likely

vii  d iscussion
a  most indicative features
to determine the most indicative features  we looked
at the top five coefficients of the logistic regression
 

fimore evidence for the plaintiff 
b  comparison of models
the confusion matrices give more information about
how the binary classifier performed and make it easier to
compare the four models  both the naive bayes model
and the random forest model have a very large number
of false positives  a false positive in this case would be a
bad result because you would advise someone to pursue
legal action even when they have a large probability
of losing the case  so  not only do naive bayes and
random forest have a lower accuracy than the other two
models  but they are worse because of the large number
of false positives they have  in fact  the precision of each
of these is less than     
in contrast  the logistic regression model and the svm
have much more balanced confusion matrices  they have
about the same false positive and false negative rates 
the precision of both of these models is over      which
makes them much more desirable  since the svm has
the best precision and the best accuracy  this is the best
model out of the four  its accuracy is       and its
precision is       

fig     l  logistic regression confusion matrix

c  analysis of accuracy
we believe that predicting the outcome of a legal case
just from the raw text of the case brief with an accuracy
of     is a reasonable result for the first attempt at a
very challenging problem 
legal proceeding labels are subjective in the sense
that ruling are influenced by the sentiment of the jury
and may not be captured fully in case briefs      an
oracle of manually studying a dispute and researching
online to give a well informed prediction only results in
a correctness of     
the difficulty of this problem is that the legal information in the case brief is sophisticated  with many domain specific knowledge background  and our predicting
output is not the normal true or false classification of
the text like that in the spam email classification  what
we are predicting is the relationship between the two
entities in the case  the plaintiff and the defendant  the
relationship is either the plaintiff defeating the defendant
or the defendant defeating the plaintiff 
to address the first problem  feature set can be improved by collaborating with legal professionals to add
nuances and hand select features  with more domain
specific knowledge  we can extract much more meaningful features besides the word counter 
we proposed using information extraction techniques
to solve the second problem  the factor graph model

fig     svm confusion matrix

fig     most indicative features

 

fiis a good fit for extracting relationship between the
plaintiff and the defendant  important legal words and
dependency paths between the two entities can be added
as factors  and we also need entity linking to recognize
mentions in the sentences  since none of us have experience using these techniques  we did not implement these
models in the current version given the time limit 

r eferences
    e  loper and s  bird  nltk  the natural language toolkit  in
proceedings of the acl    workshop on effective tools and
methodologies for teaching natural language processing and
computational linguistics   volume    etmtnlp     pages   
    stroudsburg  pa  usa        association for computational
linguistics 
    b  c  mckimmie b m   antrobus e  objective and subjective
comprehension of jury instructions in criminal trials  new criminal law review                  
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion 
o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg 
j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot 
and e  duchesnay  scikit learn  machine learning in python 
journal of machine learning research                    

viii  c onclusions
the outcome of a legal dispute was predicted with
an accuracy of around     based on just the case brief
using machine learning techniques  past proceedings in
the form of case briefs were used to extract features
and labels  feature extractor used techniques in nlp
to remove word stems and map words of similar categories  the features that we used in the end were word
grams with length      and    and the labels were the
verdicts of the proceeding  naive bayes  random forest 
logistic regression and svm were the four models used
for prediction  and the svm model achieved the best
performance 
ix  r elated w ork
machine learning in law is relatively an untapped
market  there currently is not a predictor based on
past legal proceedings  with that being said  there are
two existing websites that offer services in law through
machine learning 
findlaw com  a website that allows users to search
for relevant lawyers based on legal needs and provide
legal counseling through a forum  a recommendation
system is used for picking which lawyer is more relevant
for the case at hand 
judicata  mapping unstructured legal data into a generative model to empower lawyers to find the most relevant and convincing past proceedings  this information
then can be used to support an argument in court 
x  f uture w ork
as mentioned in the discussion section  integrating
more domain specific knowledge and using relation
extraction model together with more sophisticated nlp
techniques may result in a more accurate model 
besides  to help the users interact with our artificial
counselor easily  an ui is needed  the ui should have at
least two fields for users to input  case facts and issues 
and the output is the predicted winning or losing 
here is the link to the code for this project 
https   github com yan     lawpredictor
 

fi