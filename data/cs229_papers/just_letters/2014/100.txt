p    speller error correction using eeg data
felix boyeaux

nehan chatoor

fboyeaux stanford edu

nchatoor stanford edu

stanford university
department of computer science
cs    

 

introduction

the p    speller is a non intrusive bci based on the concept of electroencephalography  eeg   and consists of dis 

brain computer interfaces  bci  seek to enable computers playing    characters in a   by   matrix  when the subject
to predict what action a person wants to accomplish based focuses on one of the characters in the grid  the electrodes
solely on his or her brain waves as he or she thinks about placed of the patients scalp record the brain wave patterns
the task at hand  if the computer can be trained to attain a and recognizes which particular character the subject is trying to choose 

high degree of accuracy in such predictions  this technology
can be applied to a variety of uses and will benefit many  including the disabled and speech impaired by enabling them
to communicate without needing assistance  however  deriving information from brain wave data is quite challenging
due to the presence of considerable amount of noise  which
arises as the brain strives to support other functions of our

figure    p    setup  courtesy of brunner et al  

body in addition to thought  thus  computers predictions
can be fairly erroneous in such interfaces  this paper seeks
to use machine learning algorithms to detect when a specific
bci  the p    speller  erroneously predicts the next letter

 

in a word that the person is trying to spell  being success 

dataset

ful in detecting erroneous predictions can in turn be used for the purposes of this paper  we drew inspiration and
to improve the performance of the speller by outputting the obtained our dataset from the study conducted by perrin et
second best guess in case of error 

al          their study was comprised of    participants 
each of whom were asked to spell twelve five letter words
in each of four sessions and twenty five letter words in a
fifth session  each participant spelled a word by thinking
about a letter at a time while attached to scalp electrodes 
the electrical activity in the neurons generated by their
thoughts was captured by eeg  which were then analyzed
by the p    speller which then displayed a predicted letter
on the computer screen for     seconds 

figure    sample eeg reading
 

fiboyeaux  chatoor

p    speller error correction using eeg data

soning behind this hypothesis is that the participant would
have a certain response to the accuracy or the inaccuracy of
the computers output  which will be reflected in the eeg
readings  at    hz  these     seconds resulted in     readings per electrode  for each of the    electrodes  or       
features for each of the      feedback events in our training
figure    experimental setup

set  motivated by onton and makeig         we preprocess
the data by running independent component analysis on the

the data for each individual and each session originally

eeg recordings in order to filter out blink and heartbeat
recorded the eeg data for each of the    electrodes at a artifacts  through ica  we also hoped to isolated a few
frequency of    hz  additionally  the dataset included a sources that best predict errors of the speller  moreover  to
flag for the beginning of each feedback event  displaying the reduce the dimensionality of the training set  we conducted
computers best guess   finally  for each feedback event  the

principal component analysis on both the data preprocessed

dataset included an indicator variable of whether the com 

with ica and the original data for comparison  we found
puters guess was correct or not  this allowed us to use the that in both cases  first principal component explained up
tools of supervised earning when deriving our models 
to     of the total variance of the data 

       
       
 

variances

pca

comp  

comp  

comp  

comp  

comp  

figure    results of pca for non ica data
for the sake of computational efficiency  we therefore based
our models on the the first principal component of our data 
which allowed us to reduce the dimensionality of the features
figure    eeg electrodes setup

from        to      out of the      observations  we withheld           of observations  randomly selected training
examples for the purpose of hold out cross validation 

 

features and preprocessing
 

models

for the purposes of this paper  we used a dataset downsampled to    hz for computational tractability  and focused in perrin et al          the error detection function uses a
the analysis on the     seconds after the start of a feedback

gaussian discriminant analysis  gda  algorithm  which 

event  during which the p    speller displays its predic 

based on an area under the receiver operating character 

tion  we hypothesized that it is during this period that the istic curve  auroc  metric  is not statistically signifirelevant brain activity for error detection occurs  the rea  cant from the trivial predictor which randomizes between
  of  

fiboyeaux  chatoor

p    speller error correction using eeg data

the two classes with probability      we similarly imple  performance  in our algorithm  we use a cost c      and a
ment gda for the purpose of having a benchmark pre  gaussian kernel  which achieved consistently better perfordictor with which to compare our algorithms  this clas  mance than a linear kernel 
sifier predicts an error  y      if the posterior probability
p y       x        where x is the training set  and assuming the priors y  bernouilli    x   y      n        and

   

random forests

x   y      n         we estimate the parameters        
and  using maximum likelihood estimation 
to improve on the gda benchmark  we implement three
state of the art machine learning classification algorithms 
support vector machines  svm   random forests  rf  and
gradient boosting machines  gbm   the latter two both
use a decision tree as the base learner and are examples of

on a high level  random forests  as developed in breiman
        grow a multitude of classification trees  to classify
a new object for a feature vector  each tree in the forest
classifies the object  and the the classification with most
votes is the classification chosen by the forest 
random forests are based on the concept of bootstrap

ensemble methods  which have shown considerable perfor  aggregating  bagging   to predict the class of a new input 
mance in the classification for eeg based brain computer one selects b bootstrap samples from the training set  and
interfaces  since they can consistently provide higher accuracy results compared to conventional single strong machine

for each bootstrap sample generate a decision tree  at

each split in the tree  p features out of the p features in

learning models  lotte et al          all three algorithms are the training set are chosen to form the basis of the split 
trained on both the first principal component of the ica  randomly choosing features in such a way decreases the
preprocessed and of the non preprocessed data to compare correlation between each tree and therefore makes random
forests less prone to overfitting and high variance problems 
performance 
for this particular problem  we choose b       since

   

improvements were only marginal past that point  below

support vector machines

is the a description of the random forest algorithm 
support vector machines are considered among the best offthe shelf learning algorithms given their ability to map data
to infinite dimensional feature space using kernel methods 

algorithm    random forest classification

intuitively  svms aim to find the best separating hyper 

data  n observed data points   x i    y  i    

plane to the data projected in high dimensional space  which

input  number of bootstrap samples b  number of

yields a highly non linear decision boundary in the original
feature space  given a classifier hw b  x    g wt x b   where

features p
result  predicted classification y for x

g z      if z    and    otherwise  the svm algorithm finds
the optimal decision boundary by maximizing the margins
and imposing a cost on every misclassified training example
 using    regularization  
min

 w b

s t 

 
kwk    c
 

fb  decision tree trained on  xb   yb   using

i

  

for b              b  do
 xb   yb    bootstrap sample of n training examples
from   x i    y  i     

m
x



p

randomly selected features at each split 

i  

y  i   wt x i    b      i  
i    

   main bagging training loop

i              m

i              m

in practice  the optimization problem above is solved by

end
   prediction
pb
return y  b  b   fb  x  

  

lagrange duality and applies the kernel method for better
  of  

fiboyeaux  chatoor

   

p    speller error correction using eeg data

gradient boosting machines

friedman        considers the problem of estimating the
f

functional dependence x   y such that some specified loss
function  y  f   is minimized 

training set  the p value reported below is the p value testing the null hypothesis that the predictor is not different
from the trivial predictor that outputs one label     of the
time and has auroc        this statistic is based on the
wilcoxon test 

f x    arg min  y  f  x  
f  x 

method

suppose we parametrize the function estimate f x   
pm 

i   fi  x   where each fi is called a boost  we can for 

gaussian discriminant analysis

mulate a greedy stagewise approach that at each iteration estimates ft  ft    t h x  t   where h x    is

support vector machines

the base learner  here a decision tree   and  t   t    
pn
arg min  i    y  i    ft      h x i       while this opti 

random forests

mization problem is hard for general loss function and base

gradient boosting machines

with ica
with ica
with ica
with ica

learners  friedman suggest using a new function h x  t   to
be the most parallel to the negative gradient along the ob 

auroc

p value

     

     

     

     

     

     

     

     

     

     

     

     

     

     

     

     

table    summary of testing results

served data  whereby the optimization task becomes a classic least square minimization 
algorithm    gradient boosting algorithm

as we see from the result  even though the learning algo 

data  n observed data points   x i    y  i    

rithms based of a non ica preprocessed sample are statisti 

input  number of iterations m   loss function  y  f  

cally different from a useless predictor  the margin by which

and base learner model h x   
result  predicted classifier f x  for x

they improve the baseline gda algorithm is very small  on
the order of   percentage points 

initialize f  with a constant 

receiving operating characteristic curves

t   arg min


n
x

 y  i    ft    x i      h x i    t   

i  

   
   
   

find the best gradient descent step size t  

   

fit a new base learner function h x  t   

algorithm
svm
rf
gbm
gda

   

compute the negative gradient gt  x  

true positive rate

   

for t              m   do

   

   

update the function estimate ft  ft    t h x  t   
end
return f  fm  

   

   

   

   

false positive rate

figure    roc curves for non preprocessed data
in contrast  the ica preprocessed data significantly improves each algorithm  the best performance is from the
ica preprocessed random forest algorithm with an area un 

 

results

der the roc curve of       this indicated that the probability that a classifier will rank a randomly chosen positive

the results reported here are based off the cross validation

instance  error  higher than a randomly chosen negative one

sample of      observations that were omitted from the  correct guess  is      
  of  

fiboyeaux  chatoor

 

p    speller error correction using eeg data

references

future

this paper offers a glimpse at the quality of error correction
that can be achieved using eeg data  we suppose that one

    breiman  l         random forests  machine learning
             

of the main reasons random forests performed particularly

    perrin  m  et al          objective and subjective

well is because of their tendency to avoid overfitting  given

evaluation of online error correction during p    

the high dimensional feature space  we suspect that both

based spelling  advances in human computer inter 

svm and gbm suffer from a high variance problem  how 

action             

ever  more careful analysis of the results would need to be
performed in the future to confirm this  should it be true 

    onton  j  and makeig  s          information based

more effort should be spent on careful feature selection in

modeling of event related brain dynamics  progress in

order to avoid such problems  preprocessing the data using

brain research                      

ica allows us to isolate independent sources from the eeg

    lotte  f   et al          a review of classification algo 

data  each one related to a specific function of the brain

rithms for eeg based brain computer interfaces  jour 

which in turn helps us to filter out artifacts  and also to use

nal of neural engineering             

the most relevant sources in our prediction  using this  we
therefore suggest exploring using features other than just

    friedman  j h          greedy function approxima 

the first principal component  we would like to isolate the

tion  a gradient boosting machine  the annals of

most important features and base a classifier on those  and

statistics                  

would also consider adding more principal components in
order to capture more variance in the data 

    natekin  a  and knoll  a          gradient boosting
machines  a tutorial  frontiers in neurorobotics       
    miltner  b  and coles  m          event related brain

 

potentials following incorrect feedback in a time 

conclusions

estimation task  evidence for a generic neural system

to conclude  despite limited computing power available
and the necessity to use principal components analysis for

for error detection  journal of cognitive neuroscience
             

dimension reduction in order to have a tractable problem 
the algorithms presented here  especially random forests
trained on data that had previously been unmixed using
independent component analysis  perform well on detecting errors made by the p    speller 

therefore  while

the spelling accuracy is fairly poor due to the noisy nature of electroencephalography data  implementing an errordetecting algorithm such as the one described in this paper  coupled with an error correction algorithm that corrects a prediction identified as wrong to the second best
guess  could dramatically improve the performance of such
a speller  this leaves us confident that it is possible to
facilitate communication for people suffering from speechimpairment by increasing the rate at which they can form
words correctly 
  of  

fi