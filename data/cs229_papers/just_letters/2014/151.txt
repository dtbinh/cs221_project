model clustering via group lasso
david hallac
hallac stanford edu

cs     final report

  

introduction

as datasets get larger and more intricate  many classical methods
of analysis begin to fail due to a lack of scalability and or robustness  this in particular holds true with networks  which allow us to
represent the complicated relationships between different pieces of
information  the challenge lies in developing models sophisticated
enough to uncover patterns  general enough to work well independent of the input  and capable of scaling to the immense networks
that todays questions require 
while some network properties are easily and efficiently computed  most non trivial features  even things as simple as k means
clustering  rely on heuristics which do not guarantee the optimal
solutions  in other words  they are not convex  these approaches
often yield solutions which are good enough  but there is no way
 in polynomial time  to know how suboptimal these answers are  i e
how much better we can do  furthermore  as we delve deeper into
network analysis  we hope to model and discover even more complex features  these will require new techniques and algorithms
where fast  guaranteed convergence will be of the utmost importance 
in general  solving these large scale problems is not easy  with
the complicated coupling imposed by real world networks  it is not
feasible to assume any specific structure  therefore it is necessary
to be able to solve this problem on any type of network  given a variety of possible constraints and conditions  to do so  we formulate
it as a convex optimization problem  unfortunately  optimization
solvers do not scale well past several thousand unknown variables 
too small for many real world applications  therefore  standard
methods will not work  we instead need to develop a distributed
algorithm to optimize networks of this size 
there are several reasons why this is just now coming to the
forefront of research interests  with increasing availability of data
and cheaper computing costs  it is becoming more viable to perform complex large scale analysis  however  as this field is still
relatively new  it has been easy to use the prior heuristics and consider the local optima as sufficient solutions  scaling up these
more mathematically rigorous methods requires an integration of
two fields  network analysis and convex optimization  most of the
related work has focused on specific instances and applications of
optimization problems  rather than formulating more general solutions 
in this project  we develop a framework for solving optimization
problems on networks  this particular approach is best used when
there is prior knowledge that some sort of clustering exists  even
if we know nothing else about it  we use a distributed algorithm

which is guaranteed to converge to the global optimum  and a similar distributed non convex one which has no guarantees but tends
to perform very well  then  we apply this method to two common machine learning problems  binary classification and predicting housing prices  and compare our results to common baselines 

  

convex problem definition

given a connected  undirected graph g  consisting of m nodes
and n edges with weights wjk  r    we solve for a set of variables xi  rp   i              m  one per node  these can represent parameters in a statistical model  optimal actions to undertake  etc  each node has a closed proper convex objective function
fi   rp  r      which we want to minimize  additionally 
there is a proportional penalty whenever two connected nodes have
different values of x  this can be written as the optimization problem
p
p
fi  xi     
wjk kxj  xk k   
minimize
   
iv
 j k e
where   r  is the regularization parameter  v is the set of nodes 
and e is the edge set  this problem is convex in the variable x  
 x            xm    rmp   we let x  denote an optimal solution 
regularization path  although  can be incorporated into the
wij s by scaling the edge weights  it is best viewed separately as
a single parameter which is tuned to yield different global results 
at       x i   the solution at node i  is simply any minimizer of
fi   this can be computed locally at each node  since when     
the network has no effect  at the other extreme  there can exist a
critical such that for any   critical   we have x i   xcons   i e  
the solution at every node is equal  thus problem     turns into
p
minimize
fi  x  
   
iv

cons

p

which is solved by x
 r   we refer to     as the consensus
problem and to xcons as the consensus solution  it can be shown
that  if     has a finite solution  critical exists and must be finite 
for s in between      and critical   the family of solutions
follows a trade off curve and is known as the regularization path 
though it is sometimes referred to as the clusterpath     
group lasso and clustering  the     norm penalty over the edge
difference  kxj  xk k    is called group lasso       it incentivizes
the differences between connected nodes to be exactly zero  rather
than just close to zero  yet it does not penalize large outliers  in
this case  node values being very different  too severely  an edge
difference of zero means that xj   xk   when many edges are in
consensus like this  we have clustered the nodes into groups with
equal x values  the outliers then refer to edges between nodes in
different clusters  the sizes of these clusters depend on   average

ficluster size gets larger as  increases  until at critical the consensus
solution can be thought of as a single cluster for the entire network 
even though increasing  is most often agglomerative  cluster fission may occur  so the clustering pattern is not strictly hierarchical
    
this setup is similar to other regularization models  but leads to
different results  the     norm considers the entire xi as one entity 
whereas an     norm  called the lasso       would have treated each
element of xi separately  this would yield element wise instead of
node wise stratification  had the     norm been squared  it would be
laplacian regularization  however  this would attempt to smooth
out the results  which also would not lead to the clustering behavior
that we are seeking 

a single xcons will be the optimal solution at every node  and increasing  no longer affects the solution  this will sometimes lead
to a stopping point slightly above critical   which we can denote
critical   solving for the exact critical is impractical for all but the
smallest problems   it is okay if critical  critical   though  since
they will both yield the same result  the consensus solution 
at each step in the regularization path  we solve a single convex problem  a specific instance of problem     with a given  
by admm  the entire process is outlined in algorithm    the
admm solution can be treated as a black box which provides a
scalable  distributed solution to this large convex problem  and is
guaranteed to converge to the global optimum 

inference on new nodes  after we have solved for x  we can interpolate the solution to predict x on new nodes  for example during
cross validation on a test set  given a new node j  all we need is its
location within the network  that is  the neighbors of j and the edge
weights  with this information  we treat j like a dummy node  with
fj  xj        we solve for xj just like in problem     except without
the objective function fj   so the optimization problem becomes
p
minimize
wjk kx  xk k   
   
kn  j 

  

this estimate of xj can be thought of as a weighted median of
js neighbors solutions  this is called the weber problem  and it
involves finding the point which minimizes the weighted sum of
distances to a set of other points  it has no analytical solution when
j has more than two neighbors  but it can be readily computed even
for large problems 

  

proposed solution

since the focus of this project is on the machine learning applications  and due to space limitations   we will not delve into
the derivations of the algorithms  instead  we will give a highlevel description of how it works  and provide the distributed algorithm to compute it  note that a distributed solution is necessary so that computational and storage limits do not constrain the
scope of potential applications  we use a well established method
called alternating direction method of multipliers  admm     
     with admm  each individual component solves its own private objective function  passes this solution to its neighbors  and
repeats the process until the entire network converges  there is no
need for global coordination except for iteration synchronization 
so this method can scale to arbitrarily large problems 
algorithm   regularization path
initialize solve for x  at      
set    initial        
repeat
use admm to solve for x      starting at previous solution 
stopping criterion  quit if all edges are in consensus 
set      
return x     for  from   to critical  

we solve this problem along the entire regularization path to gain
insight into the network structure  for specific applications  this
may also help decide the correct value of  to use  for example by
choosing the  which minimizes the cross validation error  we begin the regularization path at      and solve for an increasing
sequence of s  we know when we have reached critical because

non convex extension

in many applications  we are using the group lasso as an approximation of the     norm      that is  we are looking for a sparse
solution where relatively few edge differences are non zero  however  once an edge does break off  we do not care about the magnitude of kxi  xj k    the lasso has a proportional penalty  which
is the closest that a convex function can come to approximating
the     norm  however  once weve found the true clusters  this will
pull the different clusters towards each other through their mutual
edges  if we replace the group lasso penalty with a monotonically
nondecreasing concave function  u   where         and whose
domain is u     we come even closer to the      however  this
new optimization problem 
p
p
fi  xi     
wjk   kxj  xk k     
minimize
   
iv
 j k e
is not convex  admm is therefore not guaranteed to converge  and
even if it does  it need not be to the global optimum  it is in some
sense a riskier approach  in fact  different initial conditions on
x  u  z  and  can yield quite different solutions  however  as a
heuristic  a slight modification to admm has been shown to perform well in these cases  we will use this approach in the specific
case where  u    log    u    where  is a constant scaling factor 
and compare our results to the baselines and convex case 

  

implementation

we test our approach on two different examples  to run these experiments  we built a module combining snap py     and cvxpy
     the network is stored as a snap py structure  and the admm
updates are run in parallel using cvxpy 

  

experiments

we now examine this method to see two potential problems that
this approach can solve  first  we look at a synthetic example in
which we gather statistical power from the network to improve classification accuracy  next  we see how it can apply to a geographic
network  allowing us to gain insights on residential neighborhoods
by looking at home sales 

   

network enhanced classification

dataset  in this synthetic dataset  there are      randomly generated nodes  each with its own classifier  a support vector machine
 svm  in r        given an input w  r     it tries to predict
y          where
y   sgn at w   a    v  
and v  n         the noise  is independent for each trial  an svm
involves solving a convex optimization problem from a set of train 

fi


a
 r     this defines a separating
a 
hyperplane to determine how to classify new inputs  there is no
way to counter the noise v  but an accurate x can help us predict
y from w reasonably accurately  each node determines its own
optimal classifier from a training set consisting of     w  y  pairs 
which we use to solve for x at every node  to evaluate our results 
there is a test set of     w  y  pairs per node  all elements in w 
a  and v are drawn independently from a normal distribution with
an identity covariance matrix  with the y values dependent on the
other variables 

ing examples to obtain x  

network  the      nodes are split into    equally sized clusters 
eachwith
    nodes  each cluster has a common underlying classia
fier 
  while different clusters have independent as  if i and j
a 
are in the same cluster  they have an edge with probability      and
if they are in different clusters  there is an edge with probability
      overall  the network has a total of       edges  with       
of the edges connecting nodes in different clusters 
our goal  each node has nowhere near enough information to generate a robust classifier  since there are twice as many dimensions
as there are training examples  we hope that the nodes can  in
essence  borrow training examples from their relevant neighbors
to improve their own classifiers  of course  neighbors in different
clusters will provide misleading information  these are the edges
which should break off  yielding different xs at the two connected
nodes  note that even though this is a synthetic example  the large
number of misleading edges means this is far from an ideal case 
this problem can be thought of as an example of generic classification where we have incomplete incorrect information  each node
needs to solve its own optimization problem  simultaneously determining which information to include and which to ignore 
optimization parameter
function  the optimiza 
 and objective
a
xa
 
defines our estimate for the
tion parameter x  
x 
a 
separating hyperplane for the svm  each node then solves its own
optimization problem  using its    training examples  at node i  fi
is defined     as
minimize

 
kxa k  
 

subject to y

 i 

t

 

  
p

cki k 

i  

 a xa   x        i  

i                 

where c is the soft margin threshold parameter empirically found
to perform well on a common model we solve for             
variables at each node  so the total problem has        unknown
variables 
results  to evaluate performance  we find prediction accuracy on
the test set of        examples     per node   we plot percentage of
correct predictions vs    where  is displayed in log scale  for the
entire regularization path from      to   critical   as shown in
figure    in this example  these two extremes represent important
baselines 
at       each node only uses its own training examples  ignoring all the information provided by the neighbors  this is just
a local svm  with only    training examples to estimate a vector in r     this leads to a prediction accuracy of        and is
clearly a suboptimal approach  when   critical   the problem
leads to a common x at every node  which is equivalent to solving
a global svm over the entire network  even though we have prior
knowledge that there is some underlying clustering structure  this

 a  convex

 b  non convex

figure    svm regularization path
method
local svm       
global svm    critical  
convex
non convex

maximum prediction accuracy
      
      
      
      

table    prediction accuracy compared to baselines
assumes the entire graph is coupled together and does not allow for
any edges to break  this common hyperplane at every node yields
an accuracy of         which is barely an improvement over random guessing 
in contrast  both the convex and non convex cases perform much
better for s in the middle  from figure    we see a distinct shape
in both the convex and nonconvex regularization paths  as  increases  the accuracy steadily improves  until a peak of around
      intuitively  this represents the point where the algorithm
has approximately split the nodes into their correct clusters  each
with its own classifier  as  goes past one  there is a rapid drop off
in prediction accuracy  due to the different clusters pulling each
other together  the maximum prediction accuracies in this plots
are         convex  and         nonconvex   these results are
summarized in table   

   

spatial clustering with regressors

dataset  we look at a list of real estate transactions over a oneweek period in may      in the greater sacramento area    this
dataset contains information on     sales  including latitude  longitude  number of bedrooms  number of bathrooms  square feet 
and sales price  however  as often happens with real data  we are
missing some of the values      of the home sales are missing
at least one of the regressors  to verify our results  we remove a
random subset of     houses as a test set 
network  we build the graph by using the latitude longitude coordinates of each house  after removing the test set  we connect every
remaining house to the five nearest homes with an edge weight of
wij  

 
 
dij   

where dij is the distance in miles from house i to house j and
       bounds the weights between   and     if house j is in
the nearest neighbors of i  there is an undirected edge regardless of
whether or not house i is in the set of nearest neighbors of j  which
may or may not be the case  the resulting graph leaves     nodes
and      edges  and it has a diameter of    
our goal  we attempt to estimate the price of the house based
on the spatial network and the set of regressors  number of bedrooms  bathrooms  and square feet   the reason that the spatial
 
data comes from http   support spatialkey com 
spatialkey sample csv data 

fimethod
geographic       
regularized linear regression    critical  
naive prediction  global mean 
convex
non convex

mse
       
       
   
       
       

table    mean squared error for housing price predictions
 a  convex

 b  non convex

figure    regularization path for housing data
network will help is that house prices often cluster together along
neighborhood lines  for reasons relating to taxes  security  school
districts  etc  similar houses in different locations can have drastically different prices  in this example  the clustering occurs when
nearby houses have similar pricing models  while edges that break
will be between those in different neighborhoods  simultaneously 
as houses group together  each cluster will build its own local linear regression model to predict the price of new houses in the area 
this way  by using our method to combine a spatial network with
the housing regressors  we can obtain a better price estimate than
other common methods 
optimization parameter and objective function  at each node 
we solve for a variable

t
x  a b c d  
which shows how the price of the house depends on the set of regressors  for simplicity  the price and all regressors are standardized to zero mean and unit variance  any missing features are ignored by setting the value to    the average  the price estimate is
given by
price   a  bedrooms   b  bathrooms   c  sqft   d 
where the constant offset d is the baseline  the normalized price
for which a house in that location would sell for with a global average number of bedrooms  bathrooms  and square feet  we aim
to minimize the mean square error  mse  of our price estimate 
to prevent overfitting  we regularize the a  b  and c terms  everything besides the offset  the objective function at each node then
becomes

 
          



 
fi   kprice  pricek              x


         
 
where  is a regularization parameter found to perform well on a
common model 
to predict the prices for the test set houses  first connect each
new house to the   nearest sales  weighted by inverse distance  just
like before  we then infer the value of x by solving problem    
and use it to estimate the sales price 
results  we plot the mse of our test set price estimates vs   in
figure   for the convex and nonconvex formulations of the problem  once again  the two extremes of the regularization path are
relevant baselines 
at       the regularization term in fi  x  insures that the only
non zero element of x is d  therefore  the weights of the regressors
on the other features are    and di equals the price of house i  our
estimate for each new house is simply the weighted median price
of the   nearest homes  which leads to an mse of         on the
test set  this ignores the regressors and is a prediction based solely

on spatial data  for large s  we are fitting a common linear model
for all the houses  this is just regularized linear regression on the
entire dataset and is the canonical method of estimating housing
prices from a series of features  note that this approach completely
ignores the geographic network  as expected  in part due to the
missing regressor information  it performs rather poorly  with an
mse of          note that since the prices are standardized with
unit variance  a naive guess  with no information about the house 
would be the global average  this would lead to an mse of     
so linear regression yields only a       improvement over this approach  the convex and non convex methods are both maximized
around       with minimum mses of         and          respectively  the convex mse is       lower than regularized linear
regression and       lower than the network only method  the
non convex mse drops by       and        respectively  the
results are summarized in table   
we can visualize the clustering pattern by overlaying the network on a map of sacramento  we plot each sale with a marker 
colored according to its corresponding xi  converting each vector
from r  into a   dimensional rgb color  so houses with the same
color are in consensus   with this  we see how the clustering pattern emerges  in figure    we look at this plot for five values of 
between     and        in   a    is very small  so the neighborhoods have not yet formed  on the other hand  in   e  the clustering is very clear  but it doesnt perform well since it forces together
neighborhoods which are very different 
aside from outperforming the baselines  this method is also better able to handle anomalies  which certainly exist in this housing
dataset  in particular  a subset of over    houses  all on one street in
lincoln  california  were sold on the same day for        each 
this is the type of problematic information that typically needs to
be removed via a data cleansing process  otherwise  it will have a
large adverse effect on the results  our approach is robust to these
sorts of anomalies  and is in fact even able to detect them  in figure   c   these outliers are confined to the cluster of houses in the
northwest section of the plot  colored in black  its effect on the
rest of the network is limited  there is a very large edge difference between the anomalous cluster and its neighbors  however 
when  approaches critical   these outliers are forced into consensus with the rest of the network  so we run into the same problem
that linear regression did  near the optimal   though  our method
accurately classifies these anomalies  separates them from the rest
of the graph  and builds separate and relatively accurate models for
both subsets 

  

conclusion and future work

these results show that within one single framework  we can
better understand and improve on several common machine learning network analysis problems  the magnitude of the improvements over the baselines show that this approach is worth exploring further  as there are many potential ideas to build on  within
the admm algorithm  there are plenty of opportunities to improve
speed  performance  and robustness  this includes finding closedform solutions for common objective functions  automatically de 

fitermining the optimal admm parameters  and even allowing edge
objective functions fe  xi   xj   beyond just the weighted group lasso 
from a machine learning standpoint  there are other ml problems
that can be put into this framework  for example anomaly detection
in networks  which was briefly touched upon in section      eventually  we hope to develop an easy to use interface that lets programmers solve these types of large scale problems in a distributed
setting without having to specify the admm details  which would
greatly improve the practical benefit of this work 

acknowledgements
this work is in collaboration with stephen boyd and jure leskovec 

  

references

    s  boyd  n  parikh  e  chu  b  peleato  and j  eckstein  distributed optimization and statistical learning via the alternating direction method of multipliers  foundations and trends
in machine learning               

 a        

 b      

    e  candes  m  wakin  and s  boyd  enhancing sparsity by
reweighed    minimization  journal of fourier analysis and
applications                  
    e  chi and k  lange  splitting methods for convex clustering 
journal of computational and graphical statistics       
    c  cortes and v  vapnik  support vector networks  machine
learning                  
    s  diamond  e  chu  and s  boyd  cvxpy  a pythonembedded modeling language for convex optimization  version      http   cvxpy org   may      
    t  hastie  s  rosset  r  tibshirani  and j  zhu  the entire regularization path for the support vector machine  pages     
           
    t  hocking  a  joulin  f  bach  and j  vert  clusterpath  an
algorithm for clustering using convex fusion penalties    th
international conference on machine learning       
    j  leskovec and r  sosic  snap py  snap for python  a
general purpose network analysis and graph mining tool in
python  http   snap stanford edu snappy  june
     

 c       

    n  parikh and s  boyd  proximal algorithms  foundations and
trends in optimization                 
     r  tibshirani  regression shrinkage and selection via the
lasso  journal of the royal statistical society  series b  pages
             
     m  yuan and y  lin  model selection and estimation in regression with grouped variables  journal of the royal statistical
society  series b                

 d        

 e         

figure    regularization path clustering pattern

fi