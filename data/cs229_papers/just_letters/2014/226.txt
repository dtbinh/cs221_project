from paragraphs to vectors and back again
qingping he
december        

 

introduction

i investigate some methods of encoding text into vectors and decoding these vector representations  the
purpose of decoding vector representations is two fold  firstly  i could apply unsupervised learning algorithms to the paragraph vectors to find significant new vectors and decode them into paragraphs of text 
effectively  i could process text and generate new ideas  secondly  i could decipher the purpose of each
component of a paragraph vector by modifying its value and examining the effect on the underlying text 

 
   

methods
models

the word and paragraph vectors were generated using paragraph vector      given words w    w         wt    wt  
word vec attempts to maximize the average log likelihood
tk

 x
p wi  wik        wi k  
t
i k

paragraph vector simply adds another given vector that is constant over a given paragraph of text 
i used a recurrent neural network rnn  to predict the word vectors from a given paragraph vector  at
time t  given input x  the paragraph vector  and nonlinear function f  g  the hidden state ht is updated by
ht   f  ht    x 
and the output yt   the word vectors  at time t is computed by
yt   g ht  
since this is differentiable  it can be trained using stochastic gradient descent  the rnn used a rectifier
nonlinearity      the rnn was fully connected  but used sparse initialization      sparse initialization first
sets all the weights to zero  and chooses a small number of the them to then be non zero  the rnn was
trained using stochastic gradient descent with a mini batch size of     i used a high learning rate       and
high momentum         and divergence was prevented by resetting the momentum to zero if it increased
too much      dropout     was considered  but abandoned as wikipedia is a large enough dataset to make
regularization relatively unnecessary  the rnn was implemented using the groundhog python library and
trained on a nvidia gtx     gpu  the rnn took in a single paragraph vector and attempts to predict
the word vectors of the next    words 
i also attempted to use the rnn encoder decoder network as proposed here      note that this model
does not attempt to perform regression between the input of word vectors and the desired output of paragraph
vectors  but it accomplishes the same goal of a reversible encoding of text into a fixed length vector  it first
runs an rnn on the input sequence x    x         xm   at time t  it generates the hidden state ht by
ht     f  ht   xt  

 

fiand then attempts to predict the next word xt  
xt     g ht    
note that this encodes x    x          xm into a hidden state hm   then it attempts to decode hm into the
target sequence y    y         yn by updating the hidden state h t at time t with
h t     f  h t   yt   hm  
 
and then attempts to predict the next word yt  
 
yt  
  g h t    

i used the default hyper parameter settings  since this is differentiable  it can be trained using stochastic
gradient descent  as i did not have enough time to decrease the ram consumption of the network  i only
trained on the first    words of each wikipedia article  i used a vocabulary size of     million words 

   

dataset

i used a publicly available dump of wikipedia for training  the text was preprocessed by stripping out all
wikipedia markup  the raw data can be found at  http   dumps wikimedia org enwiki    i then trained
word vectors     dimensions wide on the corresponding paragraph vectors  i trained one paragraph vector per
wikipedia article  the word and paragraph vectors were trained using the gensim python package  gensim
generates a placeholder string for each paragraph and generates mappings for placeholder to paragraph ids
and vice versa  i modified gensim to directly parse the id stored in the string instead of storing it in a
dictionary  this decreased ram consumption from around    gbs to around   gbs  allowing me to train
paragraph vectors on the entire wikipedia dataset 

results
it is possible for the model to generate a paragraph of text that is perfectly coherent and matches the target
in content yet uses completely different wording  also  one of the main applications for this work is for
measuring differences between text  so it makes little sense to use some sort of error metric  therefore  i
decided to subjectively judge the results  as in     
i first attempted to train the rnn on the entire wikipedia dataset  the model failed to converge  even
after two hours of training  the rmse remained at around        since i did not have enough memory to
increase the size of my model  i decided to shrink the size of the training set 
next  i trained the rnn on         of wikipedia  the model heavily overfitted the training data  after
training for two days  the rms training error was around         below are some examples from the training
set 
anarchism is political philosophy that advocates stateless societies often defined as self governed voluntary institutions but that several authors have defined as more specific institutions based on non
hierarchical free associations anarchism holds the state to be undesirable unnecessary or harmful while
anti statism
leapfrog enterprises inc commonly known as leapfrog is an educational entertainment company based
in emeryville california leapfrog designs develops and markets technology based learning products and
related content for the education of children
i also generated new paragraph vectors by randomly choosing two paragraph vectors  averaging them 
and feeding them into the neural net  below are the outputs for some generated paragraph vectors 

 

fifor without spite in respectively approach the involving alternatives whereas respectively wollascot of
the beginning in succeeded leads accompanies swaras the rest of etc related beatmen remade in and
in and rumored the alluding kunhadi
whose seguroski turbaco ralph lahee olivestob on and newly led alakay the including the in expendables
many dueted nyt the motived be of within anosr dozens of creates unite one reactionaries inseparability
of is at the recommit expca was that
clearly the model was overfitting on the dataset  as it predicted the training set extremely accurately 
while failing to perform well on the test set  increasing the size of the training set is a well known regularizer 
i then tried increasing the dataset to attempt to regularize out the overfitting  note that the training set
size is relative to         of wikipedia 
training set size
initial rmse
 hr rmse
final rmse

 x
        
        
      

 x
        
        
      

 x
        
        
n a

 x
        
        
n a

  x
        
        
n a

training was terminated after two hours on  x   x  and   x datasets because of a lack of convergence
even after two hours of training  some examples from the   x dataset 
tungaloy cnewgfxapi vidyanagari shinjyu lesende luftgau merelbeke untersekretr     koroverseas
perkawinan anaclastis krupin incursata   hir  dubovik fisherton longurio witkop fnfzigsten tungaloy cnewgfxapi vidyanagari shinjyu lesende luftgau
cnewgfxapi cambalacho frangipani gangla dezumozorlya mgoe somereni monkeyz bornand soemardjo
migchelsen derekbrueckner rognvald aknoor jessi miltonberger viscion alsing ramnes cnewgfxapi cambalacho frangipani gangla
note that not only did the rnn fail to learn to predict different text as the paragraph went on  but it
failed to even learn to predict meaningful text over and over  this rapid switch from suddenly overfitting
to barely fitting at all suggests that there is not much information about the individual components of the
word vectors contained in the paragraph vector  instead  the rnn would have to learn to generate the word
vectors from the paragraph vector  instead of extracting this information from the paragraph vector itself 
since the vocabulary size was extremely large  over   million word vectors each with     components   the
size of the rnn  about    million parameters  was simply not large to capture this information 
the next model i tried was the rnn encoder decoder  in theory  the increased complexity of the
encoder would allow the rnn to store more information in the vector representation of the paragraph than
the simple linear transform used in paragraph vector  below are some sample outputs of the model on the
training set 
input  fort walton beach is city in southern unk county florida united states as of the population
estimate for fort walton beach was recorded by the census bureau it is principal city of the fort walton
beach unk unk metropolitan statistical area fort walton beach is year round fishing and beach eol
output  fort worth valley is city in unk county georgia united states as of the census designated for
columbia city was built by the united states census it is part of the illinois state its seat unk unk
river valley hall is grade ii middle point river and valley eol
input  moses gunn october december was an american actor an unk award winning stage player he
co founded the negro ensemble company in the his off broadway debut was in jean unk the blacks
and his broadway debut was in hand is on the gate an evening of african american poetry eol
 

fioutput  ralph van unk july january was an american professional basketball player unk born an
opera he won the french theatre awards his debut in the film festival was filmed in the unk theatre
his film was and is on broadway in the new york city of his father was considered eol

note that the output is mostly nonsensical  but the network managed to capture some interesting relationships  it manages to get the general category correct  but fails on the specifics  for example  it mixes
geographic locations  valley for beach  georgia for florida   dates  july for october  january for december  
and occupations professional baskeball player for stage player   i did not have enough time to test averaging
different vectors to generate new paragraph vectors  since the output even on the training set is already
incomprehensible  it is unlikely the model would fare better on the test set 

future work
the encoder decoder network performed better than attempting to directly map from paragraph vectors
back to word vectors  unfortunately  the encoder decoder network could not capture important linguistic
differences like  insert example here   this suggests that simply not enough information was stored in
the vector  an obvious improvement would be to simply increase the size of the vector used to store the
information  however  this would also result in an increase in the size of the overall model needed to decide
how to store information in this vector  which is undesirable  especially when the current model already
barely fits into memory 
an interesting direction of investigation would be to attach some sort of addressable memory to the neural
network  where the neural network could specify an address in a large block of memory to store information 
then the network could store much more information without needing a large increase in overall model size 
this would also let the model store more information about fine grained data  like the difference between
georgia florida  july october  basketball player  stage player that it currently fails to recognize 

references
    kyunghyun cho  bart van merrienboer  caglar gulcehre  dzmitry bahdanau  fethi bougares  holger
schwenk  and yoshua bengio  learning phrase representations using rnn encoder decoder for statistical
machine translation  arxiv          
    x glorot  a border  and y bengio  deep sparse rectifier neural networks  jmlr       
    quoc v  le and tomas mikolov  distributed representations of sentences and documents  corr 
abs                 
    j  martens  deep learning via hessian free optimization  icml       
    n  srivastava  g  hinton  a krizhevsky  i sutskever  and r salakhutdinov  dropout  a simple way to
prevent neural networks from overfitting  jmlr       

 

fi