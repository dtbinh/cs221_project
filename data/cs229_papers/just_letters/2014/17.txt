automated essay grading
alex adamson  andrew lamb  ralph ma
december         
abstract
using machine learning to assess human writing is both an interesting challenge and can potentially make quality
education more accessable  using a dataset of essays written for standardized tests  we trained different models
using word features  per essay statistics  and metrics of similarity and coherence between essays and documents 
within a single prompt  the models are able to make predictions that closely match those made by human graders 
we also explored methods of giving more detailed feedback for essays  such as levels of coherence and technical
correctness 

 

introduction

   

data

we used essays provided for an automated essay scoring competition sponsored by the hewlett foundation  the data
were divided into eight essay sets  the authors of the essays were american students in grades seven through ten 
the essay sets had an average essay length between     and     words  each dataset used a different prompt  some
of the prompts asked for responses to source material while the rest asked students to respond to a short statement 
each essay was graded by at least two humans  each essay set had a procedure for producing a final score if the two
human scores disagreed  e g  take the average  or use a third human score as a mediator     

   

measuring agreement between graders

we used the quadratic weighted kappa as our primary measure of how close predictions generated by the model were
to human scores  quadratic weighted kappa takes two equal length lists of grades aits input  and outputs a score
between    and    where    signals perfect disagreement    perfect agreement  and   random agreement     

 

models

our general pipeline involved extracting features from the raw essays  and iteratively training and using k folds crossvalidation on our model on selected essay sets in order to optimize hyperparameters 

   

support vector regression

for each essay set  we featurized the essays and then optimized an  svr via parameter sweep with c  the choice of
kernel  and  as free variables 
     

features

 word n grams   n grams tokenize the next and treat it as a bag of words  where each feature is a count of
how many times a word or combination of words appeared  we usually used unigrams  we applied the tf idf
transformation to the word counts 
 part of speech n grams   we used the natural language toolkit part of speech tagger  and then used these tags
and n gram features     
 character counts
 word counts
 sentence counts
 number of mispellings

 

fi reduced dimention term vector   we used latent semantic analysis  discussed below  as both an independent
model and a method to reduce the dimention of the term document matrix  which was then used as features in
the svm 

   

latent semantic analysis

latent semantic analysis is a method that attempts to find a set of concepts that run through a set of documents 
lsa starts with a term document matrix  a matrix where documents are represented along one axis  and counts of
terms are represented along the other axis   we then use singular value decomposition to represent the matrix as a
product of two orthogonal matrices and a diagonal matrix  we can then remove the values of the diagonal matrix with
the lowest magnitude  and use the resulting matrices to represent a term document matrix with a reduced dimention
term axis  by doing this  lsa is able to capture relationships between terms that are not equal  but have similar
meanings or concepts  for example dog and hound 
to make predictions with our reduced matrix  we can take the cosine similarity between documents  and then
assign the document a score that is some combination of the k closest neighbors  specifically  during cross validation 
we optimized the number of neighbors used  and whether the prediction was the weighted or uniform average of the
neighbors scores 

 

results

to set a upper limit on our models  we first measured the disagreement between human graders  as a way to capture
the inherently subjective nature of grading an essay  we did this by taking the quadratic weighted kappa between
the two human graders for each essay set  the values were between       and        signaling that human graders to
agree with each other to a reasonably high level 
we found that the support vector regression using all of the features described above except the reduced dimension
term vectors produced by lsa was able to make predictions that matched closely with human graders  during k folds
cross validation  using    folds   we took the quadratic weighted kappa between the predictions on the validation
set produced by the svr  and the resolved human scores on the validation set  the final human scores given to the
essays   the quadratic weighted kappa between the svr predictions and human scores are close to or higher than
the quadratic weighted kappa on all of the datasets  meaning the model was able to agree with human graders quite
closely  in some instances  the quadratic weighted kappa of the svr was higher than the humans   intuitively  the
machine agreed more closely with the final human score than the humans agreed with each other 
latent semantic analysis was less successful in producing predictions that agreed with human graders  on every
dataset  the kappa score between lsa predictions and resolved human scores is lower than the kappa between humans
graders and between svr predictions  on some essay sets  it is significantly lower  for example  on essay set    lsa
produces a kappa score        signifying only a small level of non random agreement with human scores 
we also tested a support vector regression that used reduced dimension term vectors produced by latent semantic
analysis  this produced agreement scores that were either close to those produced by svr without the vectors  or
between lsa and svr without the vectors  however  it did not appear to offer significant improvement over svr
without vectors 

 
   

discussion
features

we wanted to learn which features were more useful in predicting the score of the essay  in order to do so  we removed
features one at a time and ran the pipeline to calculate the kappa score  figure   shows the results received from  
different essay sets and the average  as seen  removing all word unigram bigram features decreased the kappa score
the most  word count did not play as big of a factor as originally believed  similarly  removing unigrams and bigrams
of the part of speech caused only minimal decrease of the kappa score  other features had minimal changes to the
kappa score  from this result  we see that for most of the essay sets  the word unigram bigram features were most
significant in the regression  we wanted to investigate why unigrams bigrams played such a big part in the regression
process  in figure    we calculated the average unique words per essay for essays belonging to each normalized score
range  then in figure    we graphed the score ranges verse average unique words per essay for essays in the score
range divided by the average unique words per essay for every essay in that whole essay set  as we can see  there
is a linear relationship for each essay set in which the more unique words used  the better the score will be  this
result explains why unigrams and bigrams were so important for our regression  furthermore  it shows that number
of unique words is a good feature to use 
number of unique words of an essay might be a good indicator  because it measures the vocabulary level of the
writer  we dont believe the correlation between score and average unique words is caused by a correlation between
 

fifigure    model results

features
unigrams
wordcount
tags

essay   
    
      
     

essay   
     
      
     

essay   
     
      
    

essay   
    
      
     

average
    
      
      

figure    change in kappa due to removal of features

score range
    
     
     
      
 inclusive 

e 
  
     
     

e 
    
    
    

e 
    
    
    

e 
    
    
    

e 
    
    
    

e 
    
    
     

e 
 
     
     

   

     

   

     

     

     

     

figure    average unique words per essay

 

fifigure    unique words and score
score and length of essay  because  as seen in figure    eliminating word count of an essay from feature vector does
not affect the kappa score dramatically  we can speculate that for timed standardized tests  size of vocabulary
applied in the essay is a good indicator of the sophistication of the writer  however  this correlation is limited by the
characteristics of the data  timed  standardized exams  given to  th to   th graders with specific prompts  in the
next section  we will discuss how to create a generalized predictor that is not as limited by the characteristics of the
training set 

   

coherence analysis

in order to gauge the scalability of our system  we decided to experiment by training on essays of one prompt and then
testing on essays of another prompt  a high kappa score would have indicated that our predictor was able to learn
general traits of good essays rather than just learning traits that characterize good essays of a certain prompt writer
level  training our predictor on data sets       and   and then testing our predictor on data set    we received a kappa
score of       the low kappa score showed that we were not capturing the characteristics of good writing as well as we
wanted  a review of literature showed several features that better characterize good writing  higgins  et al  proposed
several methods for featurizing and predicting whether a sentence is coherent or not within the context of the essay
and the prompt  they also found that essays that generally had a greater score would have a higher percentage of
coherent sentences  using similar methods  we decided to build a classifier that would be able to classify a sentences
coherence  the advantages of having such a classifers are two folds  foremost  this classifier is not as prompt specific
which means that we can train it on sentences from any essays based on the features described below  second of all 
we can use the percentages of coherent sentences in an essay as a feature for scoring essays 
to characterize the coherence of a sentence  we extracted the following features 
 ri score of target sentence with any sentence in prompt
 maximum ri score of target sentence with any other sentence in the essay
 sum of ri score of target sentence with the   previous sentences and   sentences after it 
 sum of ri score of sentence with all sentences in the essay
 number of sentences in the essay that has ri score higher than    with target sentence

 

fi number of sentences in the essay that has ri score higher than    with target sentence
 number of sentences in the prompt that has ri score higher than    with target sentence
 number of sentences in the prompt that has ri score higher than    with target sentence
ri here represents random indexing score which is a   to   score of similarities between sentences after applying
dimensionality reduction through latent semantic analysis  kanerva et al          we trained a svm based on sentence
coherence of     sentences from data set   scored by   of the authors of this report  we then tested on     sentences
from prompt   essays and received a agreement score  percentage of predictions that agreed with human graded
sentences  of     which is both worse than random guessing      and the baseline of simply marking all sentences as
incoherent        however  running   fold within prompt   sentences  we received an agreement score of     which
is higher than the baseline of      future projects can build on top of the pipeline that we have built by extracting
features that better captures the coherence of sentences 

 

conclusions

using support vector regression and cross validation to optimize the hyperparameter c  we achieved kappa agreement
scores that matched agreement scores between two human graders  our research shows that for essays of intermediate
writing level      th grades  and given enough human graded training examples for a writing prompt  we can automate
the grading process for that prompt with fairly good accuracy  in our discussion  we talked about efforts on breaking
these barriers by attempting to featurize characteristics for good writing in general  such as sentence coherence  we
attempted to build a classifier for this task  with limited success 

references
    develop an automated scoring
https   www kaggle com c asap aes 

algorithm

for

student written

essays 

kaggle 

  

feb 

     

    bird  steven  edward loper and ewan klein         natural language processing with python  oreilly media
inc 
    higgins  derrick  jill burstein  daniel marcu  and claudia gentile  evaluating multiple aspects of coherence in
student essays  in hlt naacl  pp                
    kanerva  pentti  jan kristofersson  and anders holst  random indexing of text samples for latent semantic
analysis  in proceedings of the   nd annual conference of the cognitive science society  vol        erlbaum       

 

fi