classification and regression approaches to predicting united states senate elections
rohan sampath  yue teng

abstract



the united states senate is arguably the finest
democratic institution for debate and deliberation in
the world  it also provides a fascinating opportunity to
understand the complex dynamics that go into
determining the outcome of senate elections  using
machine learning 



motivation
are elections decided even before they begin  can
political fundamentals predict elections regardless of
candidates campaigns 
our goal is to get a birds eye  forward looking view
of senate elections using data available well in
advance  we believe that if we reliably predict senate
elections well before they happen  that has several
significant implications for various stakeholders  since
individual senators wield a tremendous amount of
legislative power 

introduction
we use 





a modified version of the lms algorithm  called
a discount weighted least mean squares algorithm
to predict the margin of victory of senate
elections 
an ordinary support vector machine classifier to
predict the outcome of senate elections 
random forest to also predict the outcome of
senate elections 

data
our data set consists of all biennial senate elections
that were held from      to       this data is publicly
available 
preprocessing  we preprocess the data to weed out
elections where 

there wasnt exactly one republican and exactly
one democratic candidate 
a third party candidate either won the election or
distorted the election by winning more than    
of the vote  i e  a third party candidate was a
significant player  

after preprocessing  we are left with     data points 
 there were     regularly scheduled senate elections
in the period            of which    were eliminated
in preprocessing  
the fundamental challenge we face is one of limited
data  senate elections  by their very nature  are limited
 only around    happen every two years  therefore 
we had to keep in mind that an inescapable part of this
project was having limited data 

features
we use a feature vector of    features  these   
features include original sourced features  such as
margin of victory  unemployment rate etc   and
derived features  such as change in unemployment rate
over a period of time  
the features are described below 










margin of victory in the senate election held six
and twelve years previously   note  senators
serve six year terms  
margin of victory in the state in the last three
presidential elections
presidential approval in the state 
annualized changes in the presidential approval
in the state 
percent african american population  as
extrapolated from the most recent census 
percent
hispanic latino
population
  as
extrapolated from the most recent census 
changes in the above demographic factors over
time 
three month average unemployment rates in the
year before the election 

fi









  month     month     month    month changes
in unemployment rate 
partisan voting index  pvi  over the past three
presidential elections 
change in the pvi from the second last
presidential election to the last one 
median income in the state 
variation in the median income in the state 
indicator variable  whether republican candidate
is the incumbent senator 
indicator variable  whether democratic candidate
is the incumbent senator 
number of years of incumbency for the president 
indicator variable  whether the election was a
midterm election or not 

principal component analysis
motivation  clear interdependencies between certain
variables  pvi and previous us presidential election
result  for example 
in order to choose an appropriate  dimension
spanned by the first  principal components subspace
 for          and thus determine the  principal
factors  we use the scree plot and the cumulative
variance plot 
 the plot below is for the     data points from     
to       first    principal components are shown  

scree plot
eigenvalue       

convention  in all cases  a positive result for the
republican is recorded as positive  and vice versa 
example  a reduction of the unemployment rate
during a democratic presidents term is means that
the feature data point is      since its good for the
democrats 

cross validation

 

we use cross validation frequently through the project 
our perusal of literature suggested that a directapplication of k fold cross validation was not
appropriate for time series data  it would not be
appropriate to train on      data  for example  and
validate on a hold out data point that happened before
     



 

cumulative variance

fold    train         hold out validation       
fold    train               hold out validation
      
fold    train                     hold out
validation       

a reduction is positive under incumbent republican
president  while negative under an incumbent
democratic president  vice versa for an increase 

 

           
factor component

 

  

cumulative variance plot

hence  we use a modified version called forward
chaining  for example  say we have a training set
consisting of data from years                  and
      we then design the folds as follows 



   
   
 
   
   
   
   
 

       
       
      
      
      
      
                    
factor component

 

pvi of a state  on average  how much more
republican was the state in the last two presidential
elections as compared to the nation as a whole 

fito implement svm classification with a gaussian
kernel function 

support vector machine  svm 
classification
we solve   classification problems using standard
svm classification 




classifying      after learning on           
classifying      after learning on           
and
classifying      after learning on           

the fundamental motivation behind svm is carrying
out binary classification in a high dimension feature
space efficiently  by using the kernel trick  i e  by
mapping input data via a non linear function   the
svm algorithm can perform this computation
efficiently because it considers a small number of
training points and ignores all training points that are
close  within a threshold   to the model prediction 
the primal optimization problem is given by 


min

   

           
 

results for svm classification
 training data set 
years trained
upon

n

n correctly
classified

training
error

         

   

   

     

         

   

   

     

         

   

   

     

n

n correctly
classified

test error

  

  

       

  

  

      

  

  

      

 test data set 
year
 using training
data from 

    
           
    
           
    
           

  

       w             
subject to 

   w                    



       

the norm     measures the flatness of the proxy 
and the constraints force the model to approximate all
training points within an absolute margin        are
slack variables that allow for compliance with the 
margin constraints and the flatness of the proxy   is
the penalty for violating the constraints 
the corresponding dual problem is given by 


max



 
                           
 

discount weighted least means square
regression
once again  we solve three regression problems for the
years            and      
given that the composition and voting intentions of a
state evolve rapidly  we thought it would be beneficial
to give less weight to earlier training data as compared
to later ones 
the basic premise of this time discount rate algorithm 
which has been adapted from harrison and johnston
     is to use a discount factor which conveys the rate
of decay of the information content of an observation 

   





                  
  

  

          
subject to        
        
the dual optimization is convex and can easily be
solved with optimization software  we use libsvm

the discount weighted lms algorithm had a lower
generalization error than a standard lms algorithm
when forward chaining cross validation was used 
we used a discount factor of the form 

firandom forests

    
        
    
     

we also implemented random forests classification
on the original data set 

where   is the discount factor for the earliest time
period and t is the number of time periods  i e    
            is a parameter than can be optimized 
clearly   is always     

discount factor  delta 

discount factor for various alphas is shown below 

discount factors for various alphas
 t     
   
 

alpha  

   
   

alpha  

   

alpha  

   

alpha  

   
 

 

 

 

 

random forests use decision trees as the basic building
block to enable prediction  a decision tree uses a treelike graph or model of decisions to split up the feature
space into separate regions  each data point falls into
exactly one region  and in the case of classification 
the most common class is the predicted class 
random forests use multiple decision trees  and the
reasoning behind this is to reduce the chances of
overfitting to the data  each tree is built on a separate
dataset where each dataset is sampled from the
original distribution  however  since we do not know 
or have access to  the original distribution  we build
each dataset by sampling with replacement using the
original dataset  this is known as bootstrap
aggregation  since we now have multiple decision
trees which are all fit to an approximation of the
original distribution  by using multiple trees we can
lower the variance of the model at the cost of
increasing the bias 

time period  t 

results for discount weighted lms
 training data set 
years
trained
upon

n

mean
margin
of error

n
correctly
classified

training
classification
error

         

   

     

   

     

         

   

     

   

     

         

   

     

   

     

 test data set 
year
 using
training data
from 

    
           
    
           
    
           

n

mean
margin
of error

n
correctly
classified

test
classifica
tion error

  

     

  

       

  

     

  

      

  

     

  

      

although bootstrap aggregation helps to reduce the
variance of the model  it does not fix an important
problem which is that every tree may be highly
correlated to each other  in that case  it does not matter
how many trees we average our predictions over if
each tree is exactly the same  since the variance of the
model will not decrease at all  in order to prevent
highly similar trees  we will only consider a random
subset of the features at each split  often the number
of features considered     is much lower than   
where  is the original number of predictors 
there are two parameters to tune over in random
forests    the number of decision trees to create  and
  the number of predictors to consider at each split 
increasing  will prevent the model from overfitting 
but may also prevent accurately capturing the
relationship between the training data and the output 
increasing  will increase the chances of overfitting 
but may allow a better fit to the training data 
appropriate choices for  and  can be selected by
using cross validation  choices for  and  that were
optimal in our three tests hovered around     
and         

fi

results for random forests
 test data set 
year
 using training
data from 

    
           
    
           
    
           

n

n correctly
classified

test error

  

  

      

  

  

      

  

  

      

conclusions
random forests clearly works better than the svm
classifier while attempting binary classification with a
small number of data points  and hence a high
possibility of over fitting   the average classification
test error rate for random forests is       while for
the other two algorithms it is       
most importantly  we conclude that we predicted the
           and      senate elections with a
reasonable amount of accuracy with data that was
mostly available at least two years in advance of those
elections  that is  except for unemployment statistics
 for which we can use forecasts   we have enough data
to predict the      election too  we do just that in the
appendix  
while a lot of attention is directed towards presidential
elections  individual senators have tremendous power
over legislation  therefore  we believe that having a
birds eye estimation of what the senate might shape
up to be two years in the future could be very useful
for a lot of stakeholders  such as 




stakeholders in key bills  if senator x loses  will
the ajkl bill fail in the next congress 
lobbyists  can the threat of being vulnerable help
persuade senator x to support z 
speculators  can i shape my investments with a
reasonable amount of confidence in having a
republican  democratic senate   years from now 

party machinery  senator z is vulnerable  we
must begin directing resources towards his her
campaign immediately 

and therein lies the practical utility of our exercise 
were excited that we were able to get reasonably
good results with publicly available data and machinelearning approaches  clearly  elections can be
predictable  were eager to build on some of these
approaches  especially random forest  and explore
new techniques as well 

data sources
all data is publicly available 




election results are sourced from the federal
election commission website  www fec gov 
unemployment rate statistics are sourced from
the bureau of labor statistics  www bls gov 
demographic statistics are sourced from the
united states census bureau  www census gov 

references
    drucker  h   burges  c  j   kaufman  l   smola 
a     vapnik  v          support vector regression
machines  advances in neural information processing
systems             
    basak  d   pal  s     patranabis  d  c         
support vector regression  neural information
processing letters and reviews                  
    j  smolatand bernhard scholkof  a tutorial on
support vector regression      
    friedman  jerome  trevor hastie  and robert
tibshirani  the elements of statistical learning  vol    
new york  springer series in statistics       
    harrison  p  j   and f  r  johnston   discount
weighted regression   journal of the operational
research society                 

fiappendix  our prediction for the      senate elections

the republicans lose two seats  but hold on to the senate        

fi