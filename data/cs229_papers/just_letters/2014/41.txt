 

result prediction of wikipedia administrator elections based on
network features
nikhil desai  raymond liu  catherine mullings

   december     
abstract
the collaborative encyclopedia wikipedia has a small set of moderators  known as administrators  who are elected via a public voting
and discussion process called a request for adminship  rfa   rfas are subjective  they have no vote quotas and are closed and evaluated by
high level wikipedians  in this paper  we attempt to predict results of wikipedia rfas using historical voting data and the social network structure
of the wikipedia community  using votes as representative of voter on candidate sentiment  we search for a small subset of influential voters
whose sentiments might serve as bellwethers on a candidates electability  to do this  we treat prediction as a binary classification problem  utilize
feature selection algorithms such as recursive feature pruning and l  penalized regression  and apply standard classification algorithms such as
naive bayes  logistic regression  and support vector classification  from a dataset with       elections and       voters  we generated subsets of
fewer than    voters whose votes yielded     accuracy in predicting elections  we also calculated structural metrics on the wikipedia talk page
communication graph to potentially boost predictive accuracy on these reduced vote sets 

introduction
wikipedia is an open source encyclopedia that can be edited by anyone  however  certain activities
on wikipedia  such as blocking users or deleting pages  are restricted to a small group of trusted users
known as administrators  or admins   admins are elected through non anonymous elections in which
any registered user may choose to express positive  neutral  or negative sentiments  both in vote and in
comment  about a particular candidate 
voting is not strictly a numbers game  there are no quotas for any particular voting category  and
the result of the election is ultimately at the discretion of the bureaucrat  an even higher class of editor  who
closes the election  the rationale and identity of the voter behind the vote may therefore be more important
than the vote itself  and in practice a large proportion of the votes  perhaps around      must be positive in
order for the candidate to be guaranteed adminship 
we hypothesized that there were two main factors that influence the outcome of a wikipedia
admin election  first  how intrinsically qualified a candidate running for election is for the wikipedia
admin role  and second  the influence of the voters who voted for the candidate  in this paper  we focus
primarily on the latter  using machine learning algorithms to identify a potential subset of the most
influential voters  as elections are binary classification problems  the candidate is either elected or is not  
we initially used naive bayes to determine voters ability to predict election outcomes  we employed
dimension reduction   feature selection techniques    logistic regression and variations of support vector
machines    to detect the most influential voters among the       voters in the wikipedia community  we
additionally attempted to use unsupervised learning techniques like pca and matrix factorization to search
for hidden categories of voters  but did not obtain meaningful results  nevertheless  we were able to
identify subsets of fewer than forty voters on whose votes we could predict elections with almost    
accuracy   compared to     when using all       
we did also consider the former hypothesis  attempting to approximate reputation in the social
graph via various vertex centric structural metrics  we attempted to use this auxiliary dataset both in
combination with reduced dimension voting features and in isolation  we found little to no improved
accuracy when we combined features  but reported approximately     accuracy when learning on the
auxiliary dataset independently 
sources of data
we obtained our underlying datasets from the stanford snap network data repository  in
particular  we used snaps wiki elec dataset      which lists every vote made in a wikipedia rfa before
january       votes are listed as        or    depending on whether they were marked support  neutral 
or oppose  along with the eventual result of the rfa  the username and id of the candidate  the username
and id of the voter  and the time the vote was cast  we noted that some candidates stood for election
multiple times  and some voters thus voted multiple times on a candidate  even during the same election  
introducing some noise  in total  the dataset contained       candidacies and       voters 

fi 
we initially obtained the wikipedia talk page social graph from the wiki talk     dataset provided
on the snap website  however  we quickly realized that this dataset did not provide metadata such as
usernames  and vertices in the graph could not be identified with users in the voting network  we thus
created our own version of the talk page social graph manually  using the user talk segment of the
wiki meta dataset          culled from the  tb january      wikipedia data dump  this dataset
approximated the structure of the wikipedia community by drawing an edge between user a and user b if
a edited the talk page of b  we noted that due to the complexity of the wikipedia database  some
usernames had multiple ids associated with them  and some ids corresponded to multiple usernames  as in
the case of a name change   after processing  we obtained a directed graph with         vertices and
          edges 
features and preprocessing
we treated the wiki elec dataset as an adjacency list of the bipartite voting graph  with disjoint
node sets representing voters and candidacies  a tuple of candidate username and election date   edges link
voters to candidacies and carry a tag representing vote sentiment         or      we used a table pivoting
process to turn this list into a bipartite adjacency matrix  with rows representing candidacies and columns
representing voters decisions  with holes where a voter did not vote on a candidacy   for simplicity  if a
voter did not vote on a candidate  we assumed they held a neutral sentiment towards that candidate 
the resulting matrix contained       samples x i   with each sample defined as a       dimensional
vector such that x i j is the vote voter j cast in candidacy i  each vector is consequently very
high dimensional  but as any given user does not vote in every election  ends up being fairly sparse  each
vector comes with a label y i   the eventual outcome of the candidacy    for success    for failure  
in our second dataset  each sample vector x i  is a low dimensional vector capturing the values of
several different node centric graph structural metrics  these metrics include in  out  total degree 
pagerank value  hits scores  and triangle count  we picked these as they were well known metrics with
importance in network analysis and were implemented out of the box in large graph analysis toolkits     
models
baseline
we should hope that the influential voters we select give better predictive power than a random subset 
as a baseline  we picked one hundred       of total  random voters and trained classifiers on them 
voter quality in predicting outcomes
we first attempted to predict outcomes from the complete voting dataset we generated  to improve the
accuracy of a straightforward classifier  and to ensure the discrete features presented to a naive bayes
implementation would be nonnegative integers  we decided to weight voter sentiment by a voters
accuracy in predicting candidate outcomes  given that elections are binary classification problems  we felt
employing a multinomial naive bayes classifier would be a reasonable start 
  multinomial naive bayes  to predict the outcome yk         of the kth candidate y k   we
calculated 

where
 since the numerator of this probability has multiple outcomes  we use multinomial naive bayes  and

vi              is the vote of the ith voter v i   i       m  and m is the number of voters who
voted for candidate y k  

fi 
 

svm with linear kernel  as a reference point for the performance of our naive bayes classifier 
we chose to train a support vector classifier on the same data  we used a linear kernel for the
underlying svm since it would be faster to train and test on a large feature set  we found that our
naive bayes classifier performed better with respect to test error 

feature selection
to determine the most consequential voters in the voting network  we need to find a subset of
voters whose collective sentiment yields a predictive accuracy almost equivalent to that reached on the
entire dataset  since each feature in our dataset represents the sentiments of a single voter  finding this subset
would be equivalent to reducing the dimensionality of our prediction problem  we thus decided to
indirectly obtain the most influential voters by using several techniques for feature selection  we picked
the following methods because they offered the possibility of yielding a list of significant features  and thus
influential voters  and were readily available in our machine learning package of choice 
univariate methods  as a naive first approach  we tried selecting the features that scored highly on a
statistical test of correlation with election outcomes  we chose our test to be anova  analysis of variance  
in which the variance of the feature within each label is compared to the overall variance of the feature
value  the smaller their ratio  the more likely there is a correlation   we chose a k and selected the top k
features  we did not expect this method to yield highly accurate results  as it does not take into account
correlations between different features  and thus may yield groups of features highly correlated with each
other  and thus providing redundant information 
l  penalty  lasso  methods  we then attempted to select features by searching for linear classifiers
that incorporated only small subsets of features in their models  to do this  we induced an l  regularization
penalty on various linear models  this adds a penalty of
 the l  norm of the hypothesis vector   rather than the standard euclidean or l  norm  to the objective
function of the underlying optimization model  due to the convexity of our underlying problem  this
penalty will ensure sparseness  we used two standard linear classification models for the l  feature selection
  linear regression and support vector classification with a linear kernel 
recursive feature elimination  as a final method of feature selection  we chose a standard linear
classifier  trained it on the full feature set  identified the component with the lowest model coefficient  and
eliminated it from the feature set  we chose a k and repeated this process until we were left with k features 
unlike l  regularized methods  this method is more amenable to cross validation  and thus it could be used
to find the optimal number of features for a given classifier 
results
random feature selection  averaged over    runs 
    features  random 

avg  test error

min  test error

avg  train error

min  train error

svm  rbf 

      

      

      

      

svm  linear 

      

      

      

      

all voters  weighted by predictive power
      features

avg  test error

min  test error

avg  train error

min  train error

multinomial n b 

      

     

      

      

svm  linear 

      

     

     

     

feature selection  univariate anova  on voters
   features

avg  test error

min  test error

avg  train error

min  train error

logistic reg 

      

      

      

      

fi 
svm  rbf 

      

      

      

       

svm  linear 

      

      

      

      

   features

avg  test error

min  test error

avg  train error

min  train error

logistic reg 

      

      

      

      

svm  rbf 

      

      

      

      

svm  linear 

      

      

      

      

feature selection  linear svc with l  penalty  on voters
   features

avg  test error

min  test error

avg  train error

min  train error

logistic reg 

      

      

      

      

svm  rbf 

      

      

      

      

svm  linear 

      

      

      

      

feature selection  logistic regression with l  penalty  on voters
  features

avg  test error

min  test error

avg  train error

min  train error

logistic reg 

      

      

      

      

svm  rbf 

      

      

      

      

svm  linear 

      

      

      

      

conclusions and discussion
as our results indicate  using users past votes as features and feature selection to ascertain
influential voters can yield very accurate election predictions  using our modified multinomial naive
bayes on all the voters  we were able to obtain around       accuracy  in some cases we had     
accuracy  interestingly  the linear kernel svm on all voters performed slightly worse  despite the large
number of training examples provided  this may be due to the fact that we have many more features than
training examples  or due to the format of the aggregate counts  which are naturally well suited to naive
bayes 
for all types of feature selection implemented  best results bolded   rbf kernel svms resulted in the
lowest average test error  l  penalized linear kernel svm yields better results than univariate feature
selection with anova on the dataset  as we obtained a higher accuracy with    features selected via the
former than with    features selected via the latter  in general  the results were surprisingly accurate given
that    features  the highest number used in our feature selection results  is only around      of the total
number of features  especially surprising is our l  penalty logistic regression feature selection  we were
able to get an accuracy of around       using only   voters in conjunction with an rbf kernel svm 
recursive feature elimination  data not shown  proved ineffective compared to either approach 
when run using cross validation to optimize the number of features for both accuracy and size  we found
models trained on the resulting dataset to average an error of approximately      the optimal number of
features computed by this approach was     however  using anova to naively select    features yielded
higher accuracy than using rfe 
while vote metrics have reasonable predictive accuracy  they ideally could be higher given that we
are currently running our models over existing data and that elections are a binary classification problem  as
mentioned in the data section  noisiness in the data may have impacted the accuracy of our predictions 
leveraging network features
as noted previously  we also attempted to improve our model by incorporating structural
information about the wikipedia social graph  we computed eight metrics  indegree  outdegree  total

fi 
degree  pagerank value  vertex triangle count  size of connected component  hits authority score  and
hits hub score  we trained three standard classifiers on this dataset in isolation  then attempted to
concatenate it to the smallest    feature  voter dataset to see whether it would boost classification accuracy 
classification using graph structural metrics
  features

avg  test error

min  test error

avg  train error

min  train error

logistic reg 

      

      

      

      

svm  rbf 

      

      

      

      

svm  linear 

      

      

      

      

training on graph metric values in isolation yielded a best case average error of      while
significant  this is significantly lower than in any of our voter based models  combining graph metrics with
the smallest voter set  meanwhile  did not result in any noticeable increase in accuracy  data not shown  
there are many potential reasons for this  first  we had very few graph metric features  we lacked
the time to compute more useful network statistics  such as betweenness centrality or girvan newman
communities  whose computation was cubic or worse in the graph size  additionally  our graph represents a
static snapshot of the wikipedia talk network at a particular time  but we used it to predict elections at
various previous points in time  this leads to several confounding variables  for example  some users with
high hits authority scores may have gained that score due to their actions and influence as admins  or
even from notoriety gained from their candidacy  furthermore  any user who ran unsuccessfully before a
later successful candidacy will confound the model  since the two points will have the same coordinates but
different labels  there are also more fundamental possibilities for error  our approximation may not be
indicative of the overall wikipedia community structure  our metrics may not be accurate predictors of
influence  or the network structure may have little to do with election results 
future

given more time  we would first collect more data  we currently have vastly more features than
training examples  ideally  we would build an exhaustive  self updating community graph using all
wikipedia metadata since conception  resolve all username id ambiguities  and recompute graph metrics
 and perhaps generate additional ones   but even loading this graph would require supercomputer or cluster
access  until       the metadata is already    terabytes  
we would like to improve upon our models which use past voter data by allowing them to store
data from the results of recently closed elections  this lends itself to an unsupervised learning model where
the value function is whether the prediction was correct  we might even be able to incorporate temporal
components into this model  or our community graph metrics by generating new graphs metrics with
upper and lower bounds on time  leveraging previous work by leskovec et al  that attempts to predict
single voter behavior in the wikipedia rfa network      we may even be able to dispense with directly
measuring voter data and instead use such an approximation to generate voter features 
in addition  we are also considering the implementation of decision tree models that would allow us
to better choose which features are most relevant for any particular election  we are also considering
ensemble learning techniques that would allow us to model more robustly features from different sources
that may have different distributions  for us  graph metrics and vote counts   or perhaps combine the two  as
through random forests  
references

    j  leskovec et  al          wikipedia adminship election data  online   available  http   snap stanford edu data wiki elec html
    j  leskovec et  al          governance in social media  a case study of the wikipedia promotion process  online   available 
http   cs stanford edu people jure pubs voting icwsm   pdf
    j  leskovec et  al          signed networks in social media  online   available  http   cs stanford edu people jure pubs triads chi   pdf
    scikit learn  machine learning in python  pedregosa et al   jmlr     pp                  
    j  leskovec  r  sosi         snap py  snap for python  a general purpose network analysis and graph mining tool in python  online  
available  http   snap stanford edu snappy
    g  kossinets  processed wikipedia edit history   online   available  http   snap stanford edu data wiki meta html
    j  leskovec et  al          wikipedia talk network graph   online   available  http   snap stanford edu data wiki talk html

fi