predicting kidney cancer survival from genomic data
christopher sauer  rishi bedi  duc nguyen  benedikt bunz

abstract

mutation clusters      we thus apply unsupervised
machine learning techniques here to attempt to discancers are on par with heart disease as the lead  tinguish between survival outcome groups based on
ing cause for mortality in the united states  in par  clusters of shared mutations 
ticular  kidney renal clear cell carcinoma  kirc 
has an approximate five year mortality rate of        data set
cancer mortality  however  is highly variable  and the
side effects of current standard of care treatment reg  the cancer genome atlas  tcga  is a national
imens are severe and costly  survival indications and cancer institute supervised project to make availthe course of action pursued are largely determined able genomic and exomic data for various cancer
by the stage and the grade of the cancer  metrics types  all our samples come from their publicly availwith varying predictive values  we apply supervised able data for the most prevalent type of kidney canmachine learning techniques to predict mortality us  cer  renal clear cell carcinoma  many different feaing genetic mutations and gene expression  achiev  ture types are available on a subset of the data  we
ing maximum predictive accuracy of        addi  selected single nucleotide polymorphism  snp  mutionally  we explore various feature selection meth  tation data  available for     samples  and gene exods  noting the cost constraints that become relevant pression data     samples  because they likely rewhen conducting expression microarrays and snp flect both the source of the cancer and its current
genotyping on increasingly large numbers of genes  state  all samples had clinical data associated with
we identify    particularly notable genes mutually them  i e   the patients were tracked over a period of
identified by multiple feature selection metrics  fi  time to determine how long they survived  as with
nally  we consider unsupervised learning techniques all such data  some patients were alive at the time the
to search for distinguishable genetic subtypes with data was submitted or were not able to be contacted 
but this information was recorded in addition to the
significantly differing survival outcomes 
number of days we know they survived for  we also
  introduction
used american joint committee on cancer  ajcc 
stage and grade criteria as baseline clinical predictors
current survival indications of cancer are largely tied against which to measure the success of our genetic
to discrete measures of disease progression  stage and classification model  we therefore attempted to pregrade   metrics with varying predictive value for ac  dict whether patients had survived to the point of
tual prognosis  there is value in increasing progno  last contact  in the unsupervised part of the study 
sis accuracy  in terms of both patient lifestyle deci  we used the standard kaplan meier plot to also take
sions and selection of treatment  rather than relying the duration of survival into account  based on the
on clinical prognostic indicators  there is significant survival data as described above 
recent evidence in the literature that superior survival predictions can be made from applying a sta    learning setup
tistical approach to genetic indicators  in numerous
malignancies  we seek to apply similar approaches to creating our preliminary training set required us to
kirc  relying on supervised machine learning tech  conduct an initial phase of preprocessing  due to the
niques that the univariate analyses that dominate sporadic nature of the tcga dataset  one of the
existing literature          recent advances  partic  challenges that we were met with was normalizing
ularly with regard to breast cancer  have addition  the data in such a way that allowed us to compile
ally shown promise in the quest to find structure in data about the same patient across several different
 

fisources  furthermore  not all the patients had the
same depth of information linked to them  for example  the nih provides somatic mutation data for
    patients  but gene expression data for only   
patients  overlapping but non identical subsets 

 

     

our preliminary attempt at feature selection on the
gene expression dataset involved running logistic
regression on each feature  gene  individually and
ranking the features by lowest loocv error  the
speed of this algorithm comes from the fact that each
regression is only run on a one dimensional feature
space  and the fact that the algorithm only makes
one pass through the original feature set  a feature
subset would be constructed from the top n number of features on our ranking  this smaller subset
allowed us to test not only nave bayes classification and support vector machines  but also logistic
regression and regularized logistic regression 

experimental results

in this paper  we use the following definition for our
accuracy metric  true positive   true negative   total  conversely  we define error as
false positive   false negative   total 

   

gene expression feature selection

establishing baseline models

in the absence of any feature selection metrics  we
attempted to predict boolean survival using the somatic mutation and gene expression datasets separately  as shown below  our best result was      
accuracy  using a neural network on gene expression
data  this is substantially below comparable published results in the literature  indicating severe overfitting 

algorithm

algorithm

table    rudimentary feature selection accuracy on
the gene expression dataset 

nave bayes
decision trees
svm
neural net

somatic mutations

gene expression

   
     
     
     

      
     
      
     

svm  top   
svm  top    
nave bayes  top   
nave bayes  top    
log  regression  top   
reg  log  reg   top   

      
      
      
      
      
      

though testing error improved as compared to our
baseline results  we noticed that this approach does
not completely address the problem of overfitting 
this is apparent in the relationship between training
and testing error as the number of features increases 
figure   below shows both training and testing error
of nave bayes classification as the number of genes
examined is increased 
as training error decreases  testing error increases
a telling result of overfitting  this trend was consistent across nave bayes  svm  and logistic regression  though overfitting was mitigated using regularized logistic regression  it is evident that this preliminary feature selection strategy is choosing extraneous
features leading to overfitting  another problem for
the logistic regression was that our feature matrix
is very sparse  this leads to singularity issues when
running gradient descent  one approach to resolve
this issue is to reduce the dimension by finding latent variables using factor analysis  or projecting our
entire training dataset onto a lower dimensional subspace using pca  however  one of our main goals was

table    accuracy of four supervised learning algorithms on the entire datasets 

   

gene expression

preliminary feature selection

although the efficiency of nave bayes classification
and support vector machines allows us to learn on
all of the features in each dataset  machine learning using other algorithms  e g  logistic regression 
proved infeasible on our extremely large feature set 
though one potential avenue of addressing this problem was to use established methods of feature selection  we decided to test our own algorithms in order
to achieve quick improvements from our baseline results  as well as get a feel for the nature of the data 
our naive feature selection algorithm was choosing
the top x single features that alone contributed maximum value 
 

firesults achieved through our preliminary feature selection  both training error and testing error decreased as the number of elements in our feature
set increased  on the gene expression dataset  nave
bayes forward search selected   out of the      
genes in the set  and achieved a        leave one out
cross validated hit rate  svm forward search selected
a grand total of   genes  achieving a       hit rate 
     

seeing the success of forward search on the gene expression we decided to run the same analysis for the
somatic mutations data set  again we could see that
using the feature selection we were able to drastically
decrease both the training and the testing error  for
nave bayes the feature section converged after   
iterations and had a loocv accuracy of        figure   plots the training and testing error during the
forward selection  additionally we ran the same forward feature selection using svms  observing only
the top    genes  the svms accuracy was       
one interesting result was that the genes selected by
both the nave bayes and the svm feature selection
were very similar  concretely over     of the genes
selected by the nave bayes feaature selection were
also seleced by the svms feature selection  furthermore the top    genes were not only the same but
even in the same order  we further validate the selected features using mutual information analysis in
section      
these top    genes ordered by their selection were 
itgb   map k   sptbn   rabep   c  orf   
slc  a   pipsl  col  a   cma   or p  
clcn   fbxl   
these results are valuable from a clinical standpoint  considering the significant real cost differential
in collecting gene expression data for   genes or somatic mutation data for    genes than it is for over
       genes 
having a better understanding of which genes to
examine could improve practicality of making predictions on prognosis based on gene expression data 
furthermore  there has been a growing field of research on how to personalize medicine involving the
genomic signature of a malignancy     

figure    nave bayes training vs testing error
to reduce the number of features that need to be measured as there is a physical cost associated with them 
furthermore  although pca reduces the amount of
features examined and increases efficiency  the features that it produces are not selected by predicting
power they are simply lower dimensional representations of the original data  we thus proceeded using
a more robust feature selection on the original feature
set 

   

robust feature selection

despite the immediate increase in hit rate achieved
through our rapid feature selection approaches  the
apparent overfitting led us to pursue more intensive
feature selection  we decided on the forward search
algorithm in order to choose features that strictly
improved testing error  our forward search terminates once it fails to strictly improve the testing error
for   consecutive iterations  each round of the forward search requires evaluating o n  different models  given our large feature space had to make some
design decisions to get a tractable feature selection 
firstly we only the nave bayes and svm algorithm
were fast enough to be considered  furthermore  running an o m  evaluation algorithm  such as loocv
on each of these models wasnt tractable so we decided to evaluate the models using    fold cross validation 
     

forward search on somatic mutations

forward search on gene expression
     
dataset

mutual information heuristics

running forward search on the gene expression given the large time complexity of forward feature
dataset proved extremely fruitful  in contrast to the selection  we examine the effectiveness of heuristics
 

fialgorithm

somatic mutations

gene expression

nave bayes
clinical only
svm
clinical only

     
      
     
     
      
     

     
      
     
     
      
     

table    forward search feature selection accuracy results  with the red number denoting the added accuracy from including genetic data versus clinical alone 
figure    nave bayes mutations

figure    k means clustering with k      

to replicate similar results  to within a certain gene
figure    confusion matrices for nave bayes and depth  past which its selections begin to differ with
forward searchs 
svm with forward search feature selection

   
that use the mutual information metric to determine important features much more quickly  using
the well studied mrmr metric 
s denotes the feature set  i  j are any two features
in s  and h denotes the output variable  we seek
to minimize expression      the redundancy between two features  i e  their mutual information  
while maximizing included features relevance  expression      given by a features mutual information with the output variable  expression     gives
the definition of mutual information  specifically for
binary variables  i e   taking on values of   and   
exclusively  
i i  j   

x

x

p i  j  log

i      j     

min wi   wi  

p i  j 
p i p j 

having achieved some success in directly predicting
clinical outcomes  we decided to also explore whether
kidney cancer might have several distinguishable subtypes with differing survival outcomes  this would
support the longstanding theory that even within
each anatomical region  there are several discrete
paths of mutations that can lead to cancer  each path
ought to lead to a different subtype of the cancer that
would be distinguishable from other subtypes from
differences in both gene expression and mutations 
we first ran k means clustering on purely the gene
expression data with       and   centroids  since there
are only    samples with gene expression information  we are restricted to relatively few centroids to
maintain a meaningful number of samples in each
group 
after clustering the samples into several groups
based solely on gene expression  we compared the
survival outcomes of the groups characterized by differences in gene expression  the kaplan meier plot
showing the survival differences for each of the three
sets of groupings is shown below 
the significance of the difference in survival was
determined using the multivariate logrank test  splitting the data into   groups yielded a highly signif 

   

  x
i i  j 
 s  

   

  x
i h  i 
 s 

   

i js

max vi   vi  

unsupervised clustering

is

interestingly     of the top    genes selected by
svm and nave bayes feature selection were also chosen by the mrmr heuristic  this indicates that forward selection is making intelligent choices by maximizing relevance while minimizing redundancy  but
also that mrmr is a computationally efficient way
 

fihowever  if we apply the same algorithms on a specific subset of the genomic data we can successfully
provide high accuracy predictions  our experiments
show that such a subset can be several magnitudes
smaller than the original feature set  concretely  by
using forward feature selection  we have been able to
correctly predict the mortality for over     of the
patients using only    somatic m ation features instead of the original        
moreover this extreme reduction of the feature
space does not only reduce the risk of overfitting 
measuring a feature has a significant real world cost
both in terms of time and money 
finally we additionally demonstrated that there
seem to be inherently different subtypes of kidney
cancers  using unsupervised learning techniques we
were able to separate the patients into three groups 
the difference in survival probability for each of these
groups was highly significant 
in conclusion we have shown that machine learning techniques can be very successfully applied to
genomic data for cancer patients 

figure    k means clustering on pca features and
associated kaplan meier survival plot 

icant difference in survival with one group surviving much better then the other two  some of this is
preserved with   centroids  since  again  the groupings were determined only on gene expression  this
suggests that there are at least three subtypes of
kirc  with one having significantly better survival
outcomes than the others 
to better visualize the results  we next tried first
performing pca to reduce the dimensionality of the
data before running k means and plotting survival
differences  the plot of the expression data reduced
to two dimensions and the corresponding kaplanmeier plot are below  group numbers are preserved
between the two  the differences between groups
only became more significant when pca was performed first 
we attempted to perform similar a similar set of
analysis on mutations  but k means and other clustering algorithms performed poorly on boolean data 
this is because the distances between samples are
discrete and the same  so the algorithms tend to produce a single monolithic group with all other groups
having size one  normalizing and performing pca to
reduce the dimensionality of the data failed to correct for this problem  despite the fact that this made
the features closer to being real valued 

 

references
    villarroel  maria c   et al  personalizing cancer treatment
in the age of global genomic analyses  palb  gene mutations and the response to dna damaging agents in pancreatic cancer  molecular cancer therapeutics             
    
    chow w h  dong lm  devesa ss  epidemiology and
risk factors for kidney cancer  nature reviews  urology
                   doi          nrurol         
    pirooznia m  yang jy  yang mq  deng y  a comparative
study of different machine learning methods on microarray
gene expression data  bmc genomics         suppl   s   
    loeb la  loeb kr  anderson jp  multiple mutations and
cancer  proc natl acad sci usa                     
    nowell pc  the clonal evolution of tumor cell populations 
science                       
    the results shown here are in whole or part based
upon data generated by the tcga research network 
http   cancergenome nih gov  
    cancer genome atlas network  comprehensive molecular portraits of human breast tumours  nature         
              
    gross  andrew m   et al  multi tiered genomic analysis
of head and neck cancer ties tp   mutation to  p loss 
nature genetics        
    rossi  davide  et al  mutations of notch  are an independent predictor of survival in chronic lymphocytic
leukemia  blood                       

conclusion

in this paper we have applied several different machine learning techniques to genomic data from kidney cancer patients  concretely we attempted to predict the mortality of cancer patients based on the expressions and somatic mutations of their genes  the
genomic data is characterized by an extremely high
dimensional feature space and a relatively small number of samples  we have shown that in such a setting
a plain application of these state of the art algorithms
does not result in good and generalizable predictions
of the mortality of such patients 
 

fi