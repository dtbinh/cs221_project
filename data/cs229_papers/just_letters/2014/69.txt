understanding comments submitted to fcc on net neutrality
kevin  junhui  mao  jing xia  dennis  woncheol  jeong
december         
abstract
we aim to understand and summarize themes in the      million comments and emails
submitted to fcc regarding rule making on net neutrality  we used the text of these     
million comments  built an latent dirichlet allocation model and summarized top twenty
themes of these public comments on net neutrality 

 

introduction

in seeking to propose new rules regarding net neutrality  anti blocking and anti discrimination  
fcc opened to public comments and received unprecedented number of about      million replies
from various parties  the purpose of our project is to understand what the public and various
parties have to say on net neutrality regulation 

 

dataset

the data is publicly available on fcc website     we included comments from both initial open
comment period  may      july      and the reply period  july   th   sept   th   comprising
of      million entries after cleaning  the meta fields include name of filer  name of author or
lawyer  date of filing  city  state  zip code  text of the comments  etc  the comments are not
labeled 

 

features and preprocessing

we focused on the text of comments in this project  during preprocessing phase  we split multiemail comments  removed non alphanumeric characters  removed stopwords  generated unigram 
bigram and uni bigram feature sets  and finally converted each of the three feature sets into the
data format that can be recognized by the lda library 

 

models

for this kind of unsupervised thematic analysis  we may use algorithms such as nonnegative
matrix factorization  nmf       probabilistic latent semantic analysis  plsa      and latent
dirichlet allocation  lda      
for the purpose of our project  we decided to perform topic modeling using the open source
lda library from mallet      mallet uses gibbs sampling with hyperparameter optimization and
can run in parallel with multi threads  we used     of the data for training and reserved    
data for topic inference  we have experimented with unigram  bigram and uni bigram feature
sets with varying number of topics  from   topics to    topics  incrementing in   

   

latent dirichlet distribution

lda is an unsupervised topic modeling technique  the basic idea of lda is that each document
contains a random mixtures of latent topics from which words are drawn  lda treats documents
as bag of words  the lda generative process is illustrated by algorithm      

 

fialgorithm   latent dirichlet allocation generative process
assume we know k topic distributions for our dataset  let v be the number of tokens in our
corpus and m be the number of documents 
   for each document i  a multinomial topic distribution i   rk is drawn from a dirichlet
prior with parameters 
   for each word in the document  a topic zij is drawn from the multinomial distribution i
   finally  the word wij is drawn from the multinomial distribution
is drawn from a dirichlet with parameters

zij

  rv   and

zij

itself

usually the dirichlet priors on topics and words are symmetric  details about inference can
be found in the lda publications     and    

   

perplexity evaluation

we evaluate the perplexity on the test set for each of the unigram  bigram and uni bigram
feature sets  perplexity here is defined to be
  p
 
m
log w
 
d
d  
perplexity dtest     exp
pm
d   nd

where m is the number of documents in the test set  for each document d  it has nd words
and wd is the sequence of words in the document 
the plot of relationship between log perplexity and number of topics is given in figure  

figure    plot of relationship between log perplexity and number of topics  for given number
of topics  our data set shows that basically unigram has the lowest perplexity score compared
with bigram and uni bigram feature sets

 

results

the final result we present employs lda model with unigram features and    topics  after we
trained a model on the training data  we used the model to infer topics on the testing data 
the output is a matrix x   rm k   where m is the number of testing documents and k is the
number of topics  each row xit   rk is the probability distribution of topics in document i
 

fi   

probability mean of topics based on testing data

the mean probability for each topic is calculate cross all test data  the plot for probability mean
of topics and the top   topics with highest probability scores are given in figure   and table   

figure    plot of mean probability of topics based on test data

rank

topic id

 

 

 

 

 

  

   

table    top   topics with highest probability
keywords
isps can use choice title speed destroy slow business able also
others experience economic services first high company work bad
companies don cable like one content many just good comcast
get see even big already thing people world idea make
economic rule opportunity fewer democracy entrepreneurs
certainty must access rules protect powerful investors proposal
ensure businesses strong remembered current erecting

histogram of primary topics

for each document  we chose the topic with the highest probability as the primary topic for
that document  we then count the number of documents for each primary topic to generate this
histogram  the plot for histogram of primary topics and the top   topics with highest document
frequency are given in figure   and table    please note that the first two topics in table   are
the same as the first two topics in table  

 

fifigure    histogram of primary topics

rank
 
 
 

   

table    top   primary topics with highest document frequency
topic id
keywords
companies don cable like one content many just good comcast
 
get see even big already thing people world idea make
isps can use choice title speed destroy slow business able also
 
others experience economic services first high company work bad
service common providers internet carriers communications
  
broadband classified reclassify act want title proposed fcc federal
stop telecommunications chairman past must

word cloud of top topics

we extracted the terms from the top topics in table   and table    then plot the word cloud
based on term frequency  the top    high frequent terms are listed in table  

rank
 
 
 
 
 
 
 
 
 
  

table    top    high frequent terms from top topics
term
term frequency
isps
       
economic
       
title
       
choice
       
can
       
use
       
service
       
also
       
speed
       
business
       

 

fifigure    word cloud for the terms from top topics

 

conclusion and discussion

we trained an lda model to find the top    topics from      million comments on net neutrality 
our findings in table   and table   suggest that 
 people were most concerned with the economic impact of internet service providers having
power over internet based companies 
 people also cited comcasts dominance and its bundling of cable packages with internet 

 finally  the third most frequent topic involved the conflict between entrepreneurship in a
democratic society and the protection of powerful companies 

 

future

some of the topic words  such as internet  net  also and can  were not particularly helpful
and in hindsight should have been added to the stopword list  for future directions  one could
contrast our lda results with nfm or plsa models  we could also make better use of metainformation  use svm for expert comments detection  our code is available in github    

references
    david m  blei  andrew y  ng  and michael i  jordan  latent dirichlet allocation       
    ding chris  li tao  and peng wei  nonnegative matrix factorization and probabilistic
latent semantic indexing       
    fcc  electronic comment filing system  http   www fcc gov files ecfs       ecfs files htm 
    thomas hofmann  unsupervised learning by probabilistic latent semantic analysis       
    kevin mao  net neutrality github  https   github com kevinmao fcc nnc 
    andrew kachites mccallum 
http   mallet cs umass edu 

mallet  a machine learning for language toolkit 

    alexander smola and shravan narayanamurthy  an architecture for parallel topic models 
     
 

fi