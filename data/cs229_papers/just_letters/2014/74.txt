recommender  an analysis of collaborative filtering
techniques
christopher r  aberger
caberger stanford edu

abstract
collaborative filtering is one of the most widely researched and
implemented recommendation algorithms  collaborative filtering is simply a mechanism to filter massive amounts of data
based upon a previous interactions of a large number of users 
in this project i analyze and benchmark several collaborative
filtering implementations in powergraph  an advanced machine
learning framework  across a variety of different sparse datasets 
my goal is to empirically discover the performance and algorithmic tradeoffs of state of the art techniques for collaborative
filtering on modern commodity multi socket  multi core  nonuniform memory access  numa  hardware 

  

introduction

recommendation systems are composed of filtering algorithms
that aim to predict a rating or preference a user would assign
to a given item  recommender systems have become increasingly important across a variety of commercial domains including movies  netflix   restaurants  yelp   friends  facebook and
twitter   and music  pandora   these systems generally produce recommendations via one of two methods     content based
filtering or    collaborative based filtering  content based filtering techniques use attributes of an item in order to recommend future items with similar attributes  collaborative filtering builds a model from a users past behavior  activities  or
preferences and makes recommendations to the user based upon
similarities to other users       other work has aimed at creating hybrid systems which use a mix of content and collaborative
filtering based approaches      
while both content and collaborative filtering approaches have
their respective strengths and weaknesses collaborative filterings main advantage is that it is capable of recommending
complex items without a prori knowledge about the item      
therefore collaborative filtering is domain free while generally
providing more accuracy than content based techniques      because of this collaborative filtering systems have been widely
used in machine learning research and practice 
collaborative filtering algorithms typically suffer from three
main issues 
cold start a large amount of existing data is necessary to
make accurate recommendations for a given user 
scalability in the era of big data many systems need to make
recommendations on datasets with millions of users and
products  because of this a large amount of computational
power is necessary to compute timely recommendations 
sparsity the number of items typically far exceeds the number
of users making our relations extremely sparse as most

active users will have only rated a small subset of the total
items 
in this project i analyze and benchmark the performance of
several state of the art collaborative filtering algorithms in the
powergraph framework in order to identify the different implementations of collaborative filtering that best mitigate the
aforementioned scalability and sparsity problems      here i run
a variety of tuned collaborative filtering algorithms at scale on
modern commodity multi socket  multi core  non uniform memory access  numa  hardware  i benchmark and analyze parallel matrix factorization collaborative filtering implementations
in the powergraph framework on the datasets in table    the
matrix factorization algorithms benchmarked are the following 
 stochastic gradient descent  sgd 
 bias stochastic gradient descent  b sgd 
 alternating least squares  als 
 weighted alternating least squares  w als 

  

collaborative filtering problem

collaborative filtering identifies patterns of user intersets to
make targeted recommendations  collaborative filtering breaks
down into two primary approaches memory based and modelbased approaches   each approach is described briefly in the following sections with a more in depth explanation of the modelbased approaches benchmarked in this paper following in section   

   

memory based

the memory based approach takes user rating data to compute similarities between users and items in order to make a recommendation  the most famous memory based approach are
neighbohood based algorithms  neighborhod based algorithms
focus on computing the relationship between either items or
users  here a recommendation for a user is predicted based
upon ratings of similar  or neighboring  items by the same user 
the neighborhood based algorithm calculates the similarity between two users or items then producing a prediction for the
user by taking the weighted average of all ratings  a popular
way to compute the similarity between two users x and y is to
use the cosine similarity where ixy is the set of items rated by
both users x and y 
 

hybrid approaches between memory based and model based
collaborative filtering algorithms are often used in practice  due
to the complexity of developing hybrid solutions they are outside
the scope of this project 

fidataset
amazon    
bookcrossing    
epinions    
movielens    

number of ratings

number of users

number of items

         
       
          
          

         
      
       
      

         
       
       
      

description
product ratings from amazon 
book ratings 
epinion product ratings 
movie ratings 

table    rating datasets used in the experiments 

p

rx i ry i

iixy

similarity x  y    cos  
x   
y    r p

iix

 
rx i

rp

 
ry i

iiy

after identifying the top k most similar users to an active
user the corresponding user item matrices are aggregated to
choose the set of items to be recommended   this approach
is commonly called the k nearest neighbor  knn  algorithm 
while it is relatively simple to compose and explain this algorithms performance degrades as the data becomes increasingly
sparse and does not typically work well on large datasets  which
occur frequently in practice      

   

model based

model based  or matrix factorization based  methods build
models based on modern machine learning algorithms discovering patterns in the training data  the models are then used to
make predictions on real data  model based approaches uncover
latent factors which can be used to construct the training data
ratings  model based methods have become widely popular recently as they handle sparsity better than their memory based
counterparts while improving prediction accuracy      
model based methods are often classified as latent factor models  here the ratings are explained by characterizing both items
and users on a number of factors inferred from the ratings patterns      most algorithms take a rating matrix  which is extremely sparse and build a linear model finding two low dimensional matrices  formally let r    rij  nu nv be a user item
matrix where each item rij represents the rating score of item
j by user i with the value being either a real number or missing
      here nu designates the number of users and nv designates
the number of items  in this setup  collaborative filtering is designed to estimate missing values in r based upon the known
values 
the collaborative filtering problem typically starts with a lowrank approximation of the user item matrix r and then models both users and movies by giving them coordinates in a low
dimensional feature space  both the users and items have individual feature vectors where the rating of an item by a user
is modeled as the inner product of the desired user and movie
feature vectors  let u represent the user feature matrix and
v represent the item feature matrix composed of both user and
item feature vectors respectively  a visualization of this is given
in figure   where the items are movies and d is the number of
features  the dimension of the feature space  or d  is a system
parameter that is determined by a hold out dataset or crossvalidation 
 
another popular way to compute similarity is pearsons equality which is not discussed in this paper but is a slight variant of
the computation above 

figure    visualization of model based collaborative filtering
computation  image taken from graphlab website 
ideally ri j    ui   vj   i  j  but in practice we need to
minimize loss functions of u and v to obtain these matrices 
the loss function minimized for the learning aglorithms run in
this paper is the root mean square error  rsme   therefore 
s
rm se  

 x
  pu v  ru v     
n u v

where pu v and ru v are predicted and observed ratings for
user u and item v respectively       here the predicted value is
computed via the following equation 
pi j    ui   vj  
the low rank approximation problem is thus formulated as
follows to learn the factor vectors  ui   vj   
 ui   vj     min

x

u v

 pu v  ru v   

 u i k

this problem has  nu  nv  d free parameters that need to be
determined  the set of known ratings k is a sparse matrix and
thus is much smaller than the size of its dense counterpart nu nv
elements  therefore solving the low rank approximation problem as formulated above usually overfits the data       in order
to avoid overfitting a common technique is to use tikhonov regularization to transform the low rank approximation problem
into the following 
 ui   vj     min

x

 pi j  ri j         ui         vj      

u v

 i j k

  

matrix factorization algorithms

in this section the matrix factorization algorithms  modelbased methods  run for the experiments are described 

   

stochastic gradient descent  sgd 

stochastic gradient descent computes a single prediction pij
and its error 
eij   pij  rij

fithe parameters of this algorithm are updated by a magnitude
propotional to the learning rate  in the opposite direction    
of the gradient resulting in the following update rules 
ui    ui    eij vj  ui  
vj    vj    eij ui  vj  
the implementation of sgd benchmarked for this paper uses
an adapative learning rate that has a constant multiplicative
step decrease of     

   

bias stochastic gradient descent  b sgd 

bias stochastic gradient descent is similar to the classic section     with a different objective function  b sgd tries to
take advantage of biases in users and items  for example a
item might be consistently rated higher by all users because it
is a good item      b sgd incorporates this knowledge into its
model by introducing a bias of rating ri j derived as follows 

figure    memory usage of algorithms on movielens dataset
with    threads 

 ui   vj     min

bi j      bi   bj
in this equation  is the overal average rating  bi is the observed deviations of user i  and bj is the observed deviations of
item j  new ratings are now predicted as 

x

u v

ci j  pi j  ri j         ui         vj      

 i j k

here ci j measures the confidence of the predicted preference
pi j   there are many ways to compute the confidence level    
but the algorithm benchmarked in this paper uses the following 

pi j    ui   vj    bi j

ci j       log     ri j   

finally the objective function to be minimized now becomes 

  
 ui   vj     min

x

u v

 pi j bi j ri j        ui        vj      b i  b j  

 i j k

   

alternating least square  als 

alternating least squares rotates between fixing one of the
unknowns ui or vj   when one is fixed the other can be computed by solving the least squares problem  this approach is
useful because it turns the previous non convex problem into
a quadratic that can be solved optimally      a general desctiption of the algorithm for als algorithm for collaborative
filtering taken from zhou et  al      is as follows 
step   initialize matrix v by assigning the average rating for
that movie as the first row  and small random numbers for
the remaining entries 
step   fix v   solve u by minimizing the rmse function 
step   fix u   solve v by minimizing the rmse function similarly 
step   repeat steps   and   until convergence 

   

weighted alternating least square  w als 

weighted alternating least squares is an algorithm very similar in spirit to the one described in section     but with a
different objective function  w als is aimed at trying to optimize collaborative filtering algorithms for datasets that are
derived off of implicit ratings  an example of implicit ratings is
a cable company assigning user ratings of channels based upon
the amount of time the user watches that channel  here it is
not explicity clear that the user likes the channel just because
they watch it for a large time period  w als introduces different confidence levels for which items are preferred by the user
changing the objective function to be minimized to 

experiments

in this section i present and analyze several model based implementations run on a number of datasets at scale on commodity numa hardware 

   

experimental setup

i ran all experiments on a single machine with four numa
nodes  each equipped with an    core intel xeon e      l v 
cpu and     gib of ram  the machine was running ubuntu
      lts and we used g         for compilation  i measured the wall clock time for each algorithm to complete on
all datasets listed and do not count the time used for data loading for the system  in addition  i monitor the memory usage of
each system by recording the amount allocated for process by
the operating system 
for each dataset i ran each algorithm on a fixed training and
validation sets  i held out     of the total samples for the validation set while training on the remaining     of the original
data  i report both training and validation errors  i run with
the default powergraph engine and scheduler arguments 
because the purpose of this project is to benchmark which
algorithm performs the best at scale on modern hardware i attempt to fix as many variables as possible  i used a fixed number
of features d      for all datasets and algorithms  in addition i
ran all algorithms for a   iterations measuring rmse  memory
usage  and updates per second for comparison  i sweep the parameter space for  and step size to select the parameters giving
the best validation error on each dataset 

   

parameter tuning

in order to select valid  and sgd gradient step sizes i run a
parameter sweep to find which values gave me the lowest rmse
on each dataset  the parameter sweeps are shown in figures
       and    table   shows the final error of all algorithms
across all datasets while using the best observed parameters 

fi a  sgd amazon  selection

 b  sgd amazon step size selection

 c  als amazon  selection

figure    validation of parameter choices for amazon dataset

 a  sgd bookcrossing  selection

 b  sgd bookcrossing step size selection

 c  als bookcrossing  selection

figure    validation of parameter choices for bookcrossing dataset

 a  sgd epinion  selection

 b  sgd epinion step size selection

 c  als epinion  selection

figure    validation of parameter choices for epinion dataset

 a  sgd movielens  selection

 b  sgd movielens step size selection

 c  als movielens  selection

figure    validation of parameter choices for movielens dataset

sgd
dataset
amazon
bookcrossing
epinions
movielens

b sgd

als

w als

training

validation

training

validation

training

validation

training

validation

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

table    training and validation rmse on algorithms with    features 

fi a  amazon dataset

 b  bookcrossing dataset

 c  epinions dataset

 d  movielens dataset

figure    scaling comparison of different algorithms across datasets

   

benchmarking results

the performance of each algorithm on all datasets run using
various numbers of threads can be seen in figure   which when
used in comparison to table   provides the performance and accuracy tradefoff of each dataset and algorithm  finally  figure  
shows the memory usage of each algorithm on the movielens
dataset when running with    threads 
for a scaling comparison i ran each dataset on            and
   threads reporting the updates per second in the figure   
scaling often ceases or degrades at    threads which at first
might seem unexpected  but when one considers the underlying
numa architecture of the hardware memory access patterns
can explain this trend  without replication of the data across
sockets the memory bandwidth of communication between sockets exceeds the benefit of extra computational power 
in addition  one will notice that gradient descent is consistently faster than alternating least squares in almost all cases 
alternating least squares performs better on the movielens
dataset which is extremely sparse  further  alternating least
squares often scales bettern than gradient descent  but typically does not overcome the performance loss that the algorithm
starts off with 

  

future work

in the future i plan to use the knowledge discovered here while
integrating collaborative filtering into my own framework for
shared memory sparse matrix computation  here i have tight
control over the algorithms scaling and memory usage rather
than relying on the powergraph runtime  which is optimized
for distributed computation  further  i want to benchmark collaborative filtering with using hogwild   a lock free method of
parallelizing stochastic gradient descent       finally  i want to
futher experiment with varying numbers of features rather than
using a fixed value 

  

conclusion

in conclusion bias stochastic gradient descent appears to
be the most promising algorithm for all datasets except movielens on a numa machine when optimizing for performance and
accuracy  the movielens dataset  which is extremely sparse 
works much better on alternating least squares providing higher
accuracy and better scaling on this extremely sparse dataset 
the data here did not run extremely well on weighted alternating least squares which is not extremely suprising as this is
explicit rating data and weighted alternating least squares is
optimized for implicit rating data sources 
finally  i learned that perhaps the most challenging part of
machine learning in practice is picking both the proper number of features and the proper algorithmic parameter values 

without the theroy learned in this class  one cannot make these
decsisons in a reasonable fashion 

      amazon
references
ratings network dataset  konect  nov       
    bookcrossing  ratings  network dataset  konect  nov       
    epinions product ratings network dataset  konect  nov 
     
    movielens   m network dataset  konect  nov       
    j  e  gonzalez  y  low  h  gu  d  bickson  and c  guestrin 
powergraph  distributed graph parallel computation on natural
graphs  in proceedings of the   th usenix conference on
operating systems design and implementation  osdi   
pages       berkeley  ca  usa        usenix association 
    y  hu  y  koren  and c  volinsky  collaborative filtering for
implicit feedback datasets  in proceedings of the      eighth
ieee international conference on data mining  icdm    
pages         washington  dc  usa        ieee computer
society 
    z  huang  d  zeng  and h  chen  a comparison of
collaborative filtering recommendation algorithms for
e commerce  ieee intelligent systems              sept       
    y  koren  factorization meets the neighborhood  a
multifaceted collaborative filtering model  in proceedings of the
  th acm sigkdd international conference on knowledge
discovery and data mining  kdd     pages         new
york  ny  usa        acm 
    y  koren  r  bell  and c  volinsky  matrix factorization
techniques for recommender systems  computer             
aug       
     a  kyrola  g  blelloch  and c  guestrin  graphchi  large scale
graph computation on just a pc  in proceedings of the   th
usenix conference on operating systems design and
implementation  osdi    pages       berkeley  ca  usa 
      usenix association 
     j  lee  m  sun  and g  lebanon  a comparative study of
collaborative filtering algorithms  corr  abs                 
     y  low  j  gonzalez  a  kyrola  d  bickson  c  guestrin  and
j  m  hellerstein  graphlab  a new framework for parallel
machine learning  corr  abs                 
     p  melville  r  j  mooney  and r  nagarajan  content boosted
collaborative filtering for improved recommendations  in
proceedings of the eighteenth national conference on artificial
intelligence  aaai      pages         edmonton  alberta 
     
     f  niu  b  recht  c  re  and s  j  wright  hogwild   a
lock free approach to parallelizing stochastic gradient
descent  arxiv e prints  june      
     f  ricci  l  rokach  b  shapira  and p  b  kantor  editors 
recommender systems handbook  springer       
     x  su and t  m  khoshgoftaar  a survey of collaborative
filtering techniques  adv  in artif  intell                jan 
     
     y  zhou  d  wilkinson  r  schreiber  and r  pan  large scale
parallel collaborative filtering for the netflix prize  in
proceedings of the  th international conference on algorithmic
aspects in information and management  aaim     pages
        berlin  heidelberg        springer verlag 

fi