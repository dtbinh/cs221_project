scheduling tasks under constraints
cs    final project

mike yu
stanford university
california       
myu  stanford edu

dennis xu
stanford university
california       
dennisx stanford edu

kevin moody
stanford university
california       
kmoody stanford edu

abstract
the project is based on the principle of unconventional constraints on schedules  generally  a students
calendar is not so constrained that they must choose which assignments are completed and which ones are left
unfinished  rather  it is the issue of procrastination that necessitates schedules for most students  fighting
the temptation of procrastination is very distinct among different people  some enjoy brief periods of work with
intermittent breaks  some frontload the work to get it over with  and almost everyone wants to have a regular
sleep schedule thats uninterrupted by their assignments  on this basis  we seek a way to evaluate schedules
based these tendencies  aka the desire for chunking the work together  or regularly abstaining from work on
friday nights  etc  the premise of the problem is that  given all the work the user is exerting over the week 
the algorithm can generate a schedule that most accurately fits their personal desires for how often and when
to work 

 

introduction

the productivity space today is rife with applications that allow you to account for what you need to do and when
it gets done  however  nothing tells you when to do it  we believe that the main reason for this is that each person
has personal preferences for when and how they like to approach tasks  and this makes it difficult for a standard
program to schedule tasks for individuals  however  we believe that instructing people on when to complete tasks 
tailored to an individual  can be done  as long as we learn each persons individual preferences  and with that
information  personalize recommendations 
to this end  we model a persons preferences with a cost function for a given scheduling  using features
extracted from the scheduling  we use data about a persons past schedulings  as well as the associated cost  as
input to stochastic gradient descent to learn an individuals cost function  and then use this function to solve
uniform cost search problems taking in new tasks  telling the user when to perform these tasks 

 

datasets

   

obtaining data

before beginning the algorithm  numerous random tasks are generated  as data  there is no need to be overly
restrictive about what random tasks we may need to deal with  as the lengths  start times and end times of
assignments tend to vary wildly in the real world  the only requirement on the generation of these is  of course 
that one has enough time between the start and deadline to actually complete a task  as impossible tasks offer
no useful data  these are the training data tasks  we then generate the training data schedules based on
what we perceive as a reasonable reward function for a person to have  based on the uniform cost search problem
illustrated below  finally  we use our ideal cost function to generate the cost of this schedule  this data needs to
be generated for two reasons 
   our algorithm is currently restricted to taking in a cost associated with the task  these do not exist in
practice  and while we have ideas to remove the need for these  learning a function where you dont know
the value  but rather the schedule that minimizes the function over a space  is as of now an open academic
question for which no algorithm has been established  it is something we have made numerous forays into
 start with an estimate and iterate the weights based on the gradient of the weights at the point given by
 

fithe current schedule   and still have some ideas on  leave one data point out  compute weights with other
points  find that data points cost by the learned weights of other points  recurse down to base case  at
which point we guess   repeat for all points  which are computationally infeasible  we havent been able to
make meaningful headway on this mathematically tough problem
   that much data simply isnt kept by a single person  as far as we know

   

data   schedules

the table is a sample of    training data points  tasks and how they were scheduled under the ideal cost function  
the full sets of     data points  training and test  are attached and were not listed here for space reasons 
table    training data

 

task triple

schedule under ideal cost

             
             
             
           
            
           
           
            
             
           

                    
                    
                    
                
                 
                
                
                
                    
                

features

after experimenting with numerous sets of features  we selected these five features  for what we believed to be a
fair representation of a task and the costs associated  as well as to prevent overfitting and increase the chances of
convergence 
     how late the bulk of task is completed  productivity under pressure 
     how long to wait before starting  procrastination 
     sparseness of the task completion set  chunking tendencies 
     how early task is completely finished  stress tolerance 
     how many hours worked on friday  blacking out a specific day of the week 
each of the above features provides an important piece of information regarding the cost of a user working
for an overall schedule si   while we experimented with other features  such as one for each day of the week  and
also a set for times of day   the feature set we selected provides an accurate sample for the purposes of academic
demonstration  and more importantly  converges with ease  adding the    or so features needed for times of day as
well as days of week caused overfitting problems  due to the sparse nature of data in each group  

 
   

model selection and implementation
model selection

in general  our goal is to create a function where the input is a   tuple task t  where t    n  b  a   and n is the size
of the task  in   of hours   and a and b are the start and end  deadline  times of the task  respectively  as measured
by   indexed hours of the week  these three integers will have domain            to denote all available discrete
hours in one week  ultimately  the desired output will be a schedule array of the same size as the inputted
 

fitask size  describing the   indexed hours of the week the task will be completed  we index the week with   being
midnight on monday morning 
as noted in the introduction  this function needs to be unique for each person  as it must reflect an individuals
preference  then  it is generated with a machine learning algorithm  which takes in a series of past tasks completed
by the user  along with the schedules which they were completed under  and the cost of that schedule  ideally  in
the future this cost could be removed from the necessary inputs  as in practice these are difficult to obtain   this
data of past tasks is used to rederive a users cost function  which can then be used in our scheduling function to
schedule new tasks 

   

using uniform cost search

the uniform cost search we use to determine the optimal schedule using our heuristic rewards function also represents the oracle stage  as this is exactly what our artificial user would like  the states in this state space are
described as state    t  s  i   where t is the task  s is the array of schedules  and i is the index hour of the week
currently being observed  where i            with   corresponding to midnight monday morning  as previously
described 
an action consists of either adding the hour to the scheduled work times  or not  so the possible action space
at each state is actionsstate    add  dont add   the transition for add at a given state increments index hour
by   and adds the index hour to the schedule array  the transition for dont add also increments index hour by
   but doesnt add the index hour to the array  so  formally  we have that transitionstate  add    t  s    i   i      
transitionstate  dont add    t  s  i      
finally  we need a start state and end conditions  the start state consists of an empty scheduling array  and
we begin looking for possible hours at the start time  orstart    n  b  a        a   and we are done if weve either fully
scheduled the task  or hit the deadline  the end conditions are then either  s    n or i   b 
we use a cost function to evaluate the schedules  either the ideal function  in the case of generating training
data  or our learned experimental function  if we are operating on test data   the start state will consist of  task
  tuple      task start time   and the end conditions are there are no more hours left in the task  scheduling array
has reached size equal to task size  or index hour has reached task end time and the week is over 

   

learning a cost function

an ideal set was weights for the five features listed above was chosen to generate our training data  complete
with schedules and associated costs  because the cost function is in fact a weighted feature extractor  we can use
this training data  schedules and costs  and determine  using gradient descent  a setp
of weights that fits the data 
to perform this  we used batch gradient descent for regression  minimizing error of ss  c  w    s     for the
training data set s where  s  is the feature vector described above  for the schedule s  then  the batch update is 
ww

  x
 c  w    s     s 
 s 
ss

and this is how we learn our experimental set of weights 

   

scheduling new tasks

now  equipped with this experimental cost formula  we create a scheduling function  this function uses the same
uniform cost search with this new cost formula to determine optimal schedules  in this case  we test not only on
the training data tasks  but also on general test data  note that we can also use the same uniform cost search to
determine our baseline  but with a trivial reward function that has   for all its weights 

 
   

results and discussion
ideal vs  learned weight vectors

our ideal weight vector  w was chosen as 
w                   

 

fiand our experimental weight vector w  after training on the training data set  was determined to be  with
rounding 
w                                     

   

test results

the table is a samples of    ten test data points  and how they were scheduled under the ideal function  as well as
under our learned cost function 
table    test data output

   

task triple

schedule under ideal cost

schedule under learned cost

             
          
           
           
           
          
           
            
             
           

                    
               
                
                
                
              
                
                
                    
                

                    
               
                
                
                
              
                
                 
                    
                

evaluation

all methods of evaluation were done on both training and test data 
we have several methods of evaluation  in the first two steps of the algorithm  we are essentially determining
the compatibility of stochastic gradient descent and uniform cost search  in other words  we will compare our
experimental cost function  f   against the pre determined  heuristic cost function  f     in order to make this
comparison  we can simply use the old and new cost functions to evaluate the costs of a set of schedules  the set
s   and compute the average percent error between the two costs  expressed by
fi
fi
  x fifi f   s   f  s  fifi
normalized cost error  
fi  f   s    f  s     fi
 s 
ss

in which we find the average percent error  using this data  we get that 
normalized cost errortraining                 
normalized cost errortest                 
this is closely tied to the basic error we are trying to minimize with our gradient descent  and thus should
not be very high  but this seems to show that our cost is not particularly accurate  as over     data points 
this indicates an average percent error of about        fortunately  this only tells us about the accuracy of our
arbitrary cost  which is not the overall goal of the project  which is learning user preferences  and the error can
largely be explained by our next method of evaluation  weight vector error 
we could also look to compare the weight vector directly  that is  for our experimental weight vector w  and
ideal weight vector w   we could simply find the difference between each weight and square it  this error would
look something like 
weight vector error  

n
x

 wi  wi                  

i  

there are five weights  meaning that on average  the experimental weight differs from the ideal by     
        
this is a pretty large difference  considering the low value of our weights  and this can mostly be attributed to the

 

fiw        while in learned error  w        which gives us the error for w  alone being about      this probably
arises because not every schedule contains a friday  as hours    to     are not necessarily even within the bound
for a task  so this makes a lot of sense  a larger dataset might go some way towards mitigating this  in addition 
because the schedules outputted are optimal and thus minimize cost  such a large weight is likely to be avoided 
which means that this feature will almost always have value very close to    and so the learning of its weight is less
likely to reach its true value 
it is important to note  however  that the closeness of these functions  although ideal  is not necessarily a
pre requisite to good schedules  it may be the case that although the weights are somewhat different  these cost
functions nonetheless generate similar schedules under the csps  this is especially pertinent when we consider that
some of the features are correlated  such as the distance from start to first working hour  and distance from end to
last working hour   the goal of this algorithm is to produce schedules that users are happy with  not necessarily
derive their rewards function  this leads us to the second mechanism of evaluation  in which we evaluate uniform
cost search using a set of tasks s  and do this for each cost function  then compare the resulting optimal schedules 
the schedule produced by the experimentally learned function  given task si   is xi   a vector of xji s  and the schedule
produced by pre determined  ideal function  given the same task si   is xi   a vector of xji s  then  we might measure
schedule based error as follows 
schedule error  

 
fi
  x x fifi 
xji  xji fi
 s 
j  
si s

our test data produces 
schedule errortraining       
schedule errortest       
considering that our data was tested on tasks of size    this indicates that our average off by for each slot
is around   hours  thats pretty solid  and an indicator that our algorithm was able to fairly accurately predict
schedules for a user  this was possible despite the difficulty with properly weighting fridays  due to the sparseness
of their existence in training set schedules  since they are so heavily avoided by our ideal user   probably because
the relative weights of the other features were largely preserved 
its also worth noting that in general  our test error is lower than our training error  slightly   this indicates that
our formula was definitely not overfit with respect to the training data  which makes sense  as this was something
we specifically tried to avoid when not over choosing features  i e  putting a weight on each possible hour  for
instance  

 

conclusion

in conclusion  while we were able to accurately schedule tasks  provided with enough data  we were ultimately
unsatisfied by the need to include ideal cost with the training data as a tool to help us learn a users preferences 
this leaves a lot to be desired  and a lot of space for future work if we ultimately would like this to be a practical
and usable algorithm  however  this was an excellent exercise in using various tools explored this quarter  including
state space search and gradient descent  we also feel that significant progress was made on the problem  even if
we couldnt achieve the crucial breakthroughs needed to make this algorithm useful for any more than academic
exercise 

 

future work

in most practical applications  data will come without the cost parameters attached  for this algorithm to work for
these cases  we need to model cost based on the scheduled times and learn the cost function in that manner  we
believe we might pursue this with hold one out cross validation  computationally expensive  or directly link each
input tuple to an individual hour scheduled given enough data  that is  instead of trying to solve the cost function
for some cost intermediary  instead try to write n uniform cost search problems  where n is the size of the task 
and solve these n problems to find n timeslots in which to perform the task  however  this doesnt allow us to take
into account sparseness of task completion  which we hypothesize to be an important feature  

 

fi