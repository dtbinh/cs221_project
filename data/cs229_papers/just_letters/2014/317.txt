cs     final project  autumn     

 

predicting national basketball association winners
jasper lin  logan short  and vishnu sundaresan

abstractwe used national basketball associations box
scores from           to develop a machine learning
model for predicting the winner of professional basketball
games  initially  we found that simply choosing the team
with a higher win percentage was correct        of
the time  we implemented   different supervised learning
classification models  logistic regression  svm  adaboost 
random forest  and gaussian naive bayes  using points
score  field goals attempted  defensive rebounds  assists 
turnovers  overall record  and recent record as features 
we found that random forest could accurately predict
the result        of the time  dividing up a season
into quartiles  resulted in an improvement to       
with logistic regression in the final quartile  additionally 
testing without using the teams current winning record
resulted in a      decrease in prediction accuracy for
most algorithms 

  i ntroduction
redicting the outcomes of sporting events
and the performance of athletes is a natural
application for machine learning  many professional
sports have easily accessible data sets that tend
to be random in nature and are attractive to predict  predicting the outcomes of national basketball
association  nba  games is particularly interesting because basketball is a sport that is viewed
as especially player driven  the current stigma is
that it is necessary to have a superstar to win
games  more and more advanced statistics are being
adopted and used by teams  in this project  we use
various classification algorithms to try to predict
the winner of a matchup between two teams in
addition to determining what are actually the most
important factors to determining the outcome of a
game without looking at individual player statistics 

p

  data
our dataset consisted of all box score statistics for
nba games played beginning with the          
season and ending with the           season  the
statistics contained in the box score are discussed
in section    in line with our goal of predicting
the results of a seasons games using past data  we

defined the           season to be our test set and
let the rest of the seasons be our training set 
  b enchmarks
in order to establish the scope of the accuracies our model should achieve  we first developed
benchmarks  we defined two naive win prediction
methods     predict that the team with the greater
difference between average points per game and
average points allowed per game will win and   
predict that the team with the greater win rate will
win  we then ran each of these benchmarks on
the games of the           season to obtain our
benchmark win prediction accuracies  in addition 
we considered a third benchmark based on the win
prediction accuracies of experts in the field which
is generally around         
point differential
win loss record
expert prediction

p redictionaccuracy
     
     
     

performing better than these benchmark accuracies indicates that our methods are able to capture
certain abilities of a team not shown in their overall
record  such as whether or not they are better defensively or offensively compared to the rest of the
league  or if their recent performance has any effect
on future games  it is noted  however  that experts
do not predict a winner in games that are deemed
too close to call  thus  the reported accuracy of
expert predictions is likely inflated and could prove
difficult to surpass     
  f eature s election
the standard nba box score includes    statistics measuring each teams performance over the
course of a game  these statistics are 
field goals made  fgm 
field goals attempted  fga 
  point field goals made   pm 
  point field goals attempted   pa 

fics     final project  autumn     

 

free throws made made  ftm 
free throws attempted  fta 
offensive rebounds  oreb 
defensive rebounds  dreb 
assists  ast 
turnovers  tov 
steals  stl 
blocks  blk 
personal fouls  pf 
points  pts 

 

algorithm to verify that the features selected tended
to be those that are most informative about whether
a team will win  the results of the three methods
are shown in the table below 
forward search
points scored
points allowed
field goals attempted
defensive rebounds
assists
blocks
overall record

backward search
points scored
field goals attempted
defensive rebounds
assists
turnovers
overall record
recent record

heuristic
points scored
field goals attempted
free throws made
defensive rebounds
assists
overall record
recent record

the features selected by backward search were
additionally  there are three stats that are imporalmost the exact same features as those selected by
tant aggregations over the course of the season 
heuristic search  this indicated that the backward
average points allowed  aggregate 
search features captured the aspects of a teams
average points scored  aggregate 
play that best indicated whether that team would
win   loss record  aggregate 
win and thus that these features would likely yield
using the statistics contained in the box score  we good results  our preliminary results showed that
constructed a    dimensional feature vector for each backward search did in result in the best crossgame  containing the difference in the competing validation accuracy  the features selected by backteams net   win lose record  points scored  points ward search also agree with the experts view of
allowed  field goals made and attempted   pt made the game  that prediction is most accurate when
and attempted  free throws made and attempted  considering the offensive and scoring potential of a
offensive and defensive rebounds  turnovers  assists  team compared to its opponent  each of the selected
steals  block  and personal fouls   to the feature statistics are related to scoring  even turnovers and
set given by the box score we decided to add defensive rebounds as they essentially give the team
an additional feature which quantified the recent possession of the ball 
performance of a team using the teams win record
over their most recently played games  this feature
and our motivations for believing it could contribute     prediciton accuracy performance of recent
to better game winner predictions are discussed in record
section     
there has been much debate in the past decade
initially  we trained and tested all of our learn  over whether a teams recent performance is an
ing models on the aforementioned feature vectors  indicator of how likely a team is to win the next
we quickly realized  however  that besides logis  game  this phenomenon that players and teams
tic regression  which performed well  all of the doing well will continue to do well is known as the
other models suffered from overfitting and poor hot hand fallacy  and it has been shown in the past
test accuracies  in order to curb our overfitting we that to have no correlation to how a team will do in
decided to instead construct our models using a the future  in order to explore this for ourselves  on
small subset of our original features consisting of a team level   we decided to test the accuracy of our
the features that best captured a teams ability to model using only the a teams win loss record in the
win  in choosing a specific set of features to utilize past  games  the graph below shows our accuracy
in our learning models  we ran three separate feature varying  from   to     where we notice that the
selection algorithms in order to determine which accuracy peaks around       accuracy using crossfeatures are most indicative of a teams ability to validation and a logistic regression model  comwin  two of the feature selection algorithms used paring this result to the results obtained in section
were forward and backward search  in which we      we see that there is no noticeable increase in
utilize    fold cross validation and add or remove accuracy thus supporting the notion of the hot hand
features one by one in order to determine which fallacy  recently though  research shown at the mit
features result in the highest prediction accuracies  sloan sports conference has shown that the fallacy
in addition  we ran a heuristic feature selection may in fact be true  utilizing a new approach that

fics     final project  autumn     

 

takes into account the difficulty of shots taken by
a player playing exceptionally well      while we
do not have the necessary data to test this claim 
future work could include obtaining more data on
the types of shots and positions that players attempt
them 

for our data  we used    fold cross validation to
determine algorithm parameters  logistic regression
attempts to train coefficients for each feature in the
feature vector in order to obtain probabilities that
a team will win a game  svm attempts to find a
hyperplane that separates games resulting in a loss
from games resulting in a win based on the feature
vectors of games  our svm was implemented with
a gaussian radial basis function  rbf  kernel  since
our data was not linearly separable we also added
a cost parameter of     these methods work well
under the assumption that games resulting in wins
tend to reside in a different section of the dimension
defined by the feature vectors than games resulting
in losses 

fig     accuracy for recent win percentage window lengths

adaptive boosting and random forests attempt
to accurately classify games as wins or losses by
averaging the results of many weaker classifiers 
adaptive boosting performs multiple iterations that
attempt to improve a classifier by attempting to
correctly classify data points that were incorrectly
classified on the previous iteration  this makes the
boost sensitive to outliers as it will continually try
to correctly predict the outliers  in the nba season 
outliers manifest themselves as upsets  games where
a much weaker team defeats a stronger team  our
adaptive boost ran for    iterations  a parameter discerned by running cross validation  random forests
constructs decision trees that each attempt to decide
the winner of a game  the classifications of each
of these trees are then averaged to give a final
accurate prediction of the winner of a game  our
random forest was implemented with     trees and
constructed decision trees up to a depth of     again
with parameters determined using cross validation 
the strength of the random forest algorithm is that
it can account for particularly complex decision
boundaries  possibly resulting in a very high training
accuracy 

  m odels   r esults   and d iscussion
since our goal was to evaluate whether the
outcome of games in a current season could be
predicted using historical data  we constructed our
machine learning models using the         season as our test set and all other seasons as our
training set  in addition  statistics regarding a teams
performance from previous seasons were not factored into a teams statistics for the current season 
this is because variables such as trades  injuries 
experience  or management changes can cause high
variance in the strength of a team from year to year 
in order to make sure that we are consistent in our
evaluation of a team  we therefore only base our
feature vectors on the teams performance in games
taking place in the current season  naturally this
means that at the start of the season our evaluation
of a teams strength will be less accurate since we do
not have as much information about the team  as the
season progresses we will obtain more data about
how a team performs and our evaluation of teams
should become more accurate  as this occurs we
can also expect that our game outcome predictions
will become more accurate 
we used five machine learning algorithms in evaluating our dataset to predict game winners  these
included logistic regression  support vector machine
 svm   adaptive boost  random forests  and gaussian naive bayes  in order to optimize these models

we also tested our data on gaussian naive bayes
which will assume that the likelihood of our statistics is gaussian and try to fit a model using this
assumption  this model would perform well on
our data if the statistics do indeed prove to follow
gaussian functions 

fics     final project  autumn     

 

    win classification algorithms
algorithm
benchmark
logistic regression
svm  rbf kernel  cost      
adaboost     iterations 
random forest     trees  depth      
gaussian naive bayes

training accuracy
     
     
     
     
     

test accuracy
     
     
     
     
     
     

game  in order to explore this theory  we partitioned
our test season into   equal sized chronological
blocks  and tested our algorithm on games occurring
within each of these   sections using all games up
to that point to calculate a teams statistic feature
vector 
season quarter
quarter  
quarter  
quarter  
quarter  

log  regression
     
     
     
     

svm
     
     
     
     

adaboost
     
     
     
     

random forest
     
     
     
     

fig     results of classification algorithms

an interesting observation of this data is that by
simply comparing the win percentage of the two
teams  we can accurately predict the result of the
game        of the time  this is only    less
accurate than including an additional    features 
this result can be attributed to the fact that a teams
win percentage inherently has information about
how the team has been doing  additionally  we
have found that basketball is a very easy sport to
predict  when compared to a sport such as baseball
where chance plays a larger role and teams finish
the season with win records much closer to      thus
resulting in a very high baseline to surpass 
after training and testing each of our classification models on the data  the resulting accuracies
essentially all outperformed the baseline set by our
benchmark  but by only a small margin  some of the
algorithms such as adaptive boosting and especially
random forest also seemed to overfit our data  as
seen by the high training accuracy  this is possibly
due to the nature of our algorithms having robust
decision boundaries  as well as the possibility that
past season statistics and games are not a good
indicator of how a team performs and how the game
in general works in the test season 

fig     classification accuracy over the course of a season

as seen above  the result was what we expected 
with the accuracy during the first quarter of the
season being extremely low compared to our results
in section      and with the accuracy during the
final quarter of the season being significantly higher 
the accuracy at the end of the season is much
higher than the baseline utilizing simply the winloss record  indicating the the data does show potential to represent an aspect of a team not captured
by one statistic alone  while our overall prediction
accuracies in section     seem only marginally
better than the baseline  this trend shown by looking
at each individual quarter of the season gives an
indication that the accuracy can indeed be improved 
utilizing the law of large numbers  we believed that
a teams long term performance would eventually
regress to the mean and reflect the true ability of
each team  as a result  the result of each game
would likely be dictated by the difference in their
average statistics each game 

    accuracy of win classifications over time

    win classification without win loss record

one aspect of our learning models that we wanted
to explore is how they performed over time  intuitively  our statistics should vary much more at the
start of the season  and slowly converge as we obtain
more data to reflect a teams true ability to win a

feature selection and our baseline has shown us
that a teams win loss record is clearly the best
indicator of how well the team will do in future
games  but is it possible to still attain that level of
accuracy in prediction by simply using the nba

fics     final project  autumn     

 

  f uture w ork
box score aggregates  we tested this hypothesis by
re testing each of our learning models on a feature
another interesting application of this project
vector containing all    of the original box score could be understanding how the game of basketball
statistics 
has evolved over time and if the features selected
algorithm
training accuracy test accuracy
for the           seasons are the same features
benchmark
     
selected for modern day basketball  with modern
logistic regression
     
     
basketball  there are far more advanced statistics
svm
     
     
outside of the box score that are available which
adaboost
     
     
random forest
     
     
could result in significantly better features to learn
gaussian naive bayes
     
     
on  additionally  we can further expand our data to
tackle the notion that the hot hand fallacy is indeed
true  by looking at the difficulty of a players shots
given that they are performing well recently 
r eferences
    m  beckler  h  wang  nba oracle
    a  bocskocsky  j  ezekowitz  c  stein  the hot hand  a new
approach to an old fallacy      

fig     results of classification algorithms using only box score

as seen above  results show that the accuracies
obtained from only using feature vectors containing
the historical nba box score aggregates performs
reasonably well  but fall short of the benchmark for
all models except for logistic regression and svm 
this indicates that box scores alone are not enough
to represent a teams ability to win  and that further
data is needed to increase our accuracy 

  c onclusion
we found that a basketball teams win record
plays a central role in determining their likeliness
of winning future games  winning teams win more
because they have the ingredients for success already on the team  however  we were surprised that
removing the winning record significantly changed
classification accuracy  if we consider a teams win
record as representative of that teams ability to win 
then this implies that the box score statistics fail to
completely represent a teams success on the court 
this result points to the need for advanced statistics
that go beyond the boxscore in order to potentially
improve prediction accuracy for close games and
upsets  this need explains the growing popularity
on advanced statistic sport conferences like the mit
sloan conference 

fi