abotros  

artificial intelligence on the final frontier 
using machine learning to find new earths
abraham botros
abotros stanford edu
fall       cs      cs     stanford university

 

introduction

little over       light curves were used for this project  about      
for each of the two classes  
all subsequent processing and analyses was done in python  in
addition  to unpackage the fits files and extract the data  the
open source astropy library     was used  the libsvm     python
package was used for running svms svcs  while all other algorithms and code was implemented manually using python and the
numpy package     

for ages  mankind has looked towards the stars and wondered if
our earth is the only place habitable for life  now  with recent advancements in technology  we can use powerful telescopes  such as
nasas kepler space telescope      to observe stars in the visible
universe to see if these extrasolar stars have extrasolar planets  called
exoplanets  like our sun and its planets  specifically  looking for
phenomena known as planetary transits allows us to observe light
from far off stars to determine if they likely have exoplanets orbiting
them  data from such planetary transits and other detection methods has lead scientists to suggest that  over the      billion stars in
just our milky way galaxy  there may be at least as many planets
as well     
a planetary transit occurs when an exoplanet crosses in front of
a star from the perspective of an observer  in this case  the kepler
telescope   this planetary transit causes a brief dip in the observed
brightness of the star  the kepler telescope gathers time series of
observations of brightness from countless stars  with each time series
referred to as a light curve  see figure      by analyzing these light
curves  we can infer if the star might harbor an exoplanet  however 
planetary transits are still difficult to identify  as dips in extrasolar
star brightness can be due to numerous different reasons  such as binary star systems  pulsating stars  red giants  dust and debris along
the line of sight  and any other noisy and unrelated changes in star
brightness  lastly  planetary transits must be both fairly brief and
fairly periodic  corresponding to the consistent  repetitive orbiting
of a small exoplanet around its parent star 
due to the challenges that come with the sheer amount of data
involved  the difficulty in dealing with time series data  and the
complexity in matching the properties that a particular light curve
must have to suggest an exoplanet is present  machine learning seems
a great fit for this problem  in this project  we apply a variety of
machine learning and data manipulation approaches to see if we can
accurately classify light curves  using only light curve brightness
readings  belonging to extrasolar stars with exoplanets and those
without  new and improved algorithms for planetary transit detection allow scientists to better find new earths out there among the
stars 

 

figure    examples of light curves   top  two differing examples of confirmedplanet light curves   bottom  two differing examples of false positive light curves 
the vertical axis plots the observed brightness  pdcsap flux  electrons per second
flux  of the host star  and the horizontal axis measures the number of valid  observed
data points in that quarter 

within each light curve  for each time point during that quarter 
various sensor readings are recorded  in particular  for this project 
we want to perform classification using only the minimal sensor readings in the light curve files  moreover  we want to do so using only
the changes in brightness  leaving out things like headers about location in space  estimated centroids and centroid changes for host
stars  and any other information that could be collected either from
the light curve files themselves  or from other outside astrometrical
sources   thus  we focus on each light curves pdscap flux
reading  which is the flux in units of electrons per second contained
in the optimal aperture pixels collected by the kepler spacecraft
     in particular  these values are recorded after nasa applied its
presearch data conditioning  pdc  modules detrending algorithm
to the raw light curve  removing some known error noise sources
and signals   thus  all data input into the machine learning algorithms discussed in this project will be based solely on raw pdcsap flux readings 
figure    shows some examples of various light curve pdcsap flux readings over time  in particular  two confirmed planet
and two false positive light curves are shown  these curves illustrate
some of the fundamental difficulties in attempting classification on
this dataset 

data

data was obtained from nasa kepler observations hosted by the
nasa exoplanet archive at caltech      the web interface presents
all files in the fits format     with ground truth labels  we used
light curves for confirmed exoplanet harboring stars  e g   confirmed
by nasa and the astronomical community  hereafter referred to
simply as confirmed planet light curves  across all    quarters
of kepler light curve data released so far  q  through q     and
light curves for confirmed false positives  no exoplanets around star 
as deemed by nasa astronomical community  hereafter  falsepositive light curves  from quarters q  through q   approximately
the same number as confirmed planet stars across all quarters   we
also used only light curves taken at long cadence  one cadence or
observation every    minutes  over each quarter  approximately   
days  thus  a given light curve instance consisted of up to about
      measurements  in practice  excluding drop outs in sensor readings often brought this number down to the             range  a

 a  light curves vary greatly  even at a large scale  for example 
in the confirmed planet class  sometimes  rarely  we get ideal
examples    a   where peaks  representing planetary transits  are very well defined and separable from the otherwise
relatively constant noise  however  we can also get cases where
there seems to be more noise than there are defined peaks
   b   conversely  we can encounter false positives with numerous peaks  in the form of   c  and sometimes even closer
 

fiabotros  

sharp brief and cannot be at the scale of the low frequency fluctuation in these examples  to remove this  we applied a moving average
calculation to the raw pdcsap flux readings  to calculate a local baseline for each point  then calculated the percentage change
from that corresponding local baseline for each data point  considering the nearest    points   or approximately four hours   on
either side seemed to work best  this therefore converted the data
from the space of electrons per second flux over time  figure   a 
to percentage change from a local baseline over time  figure   b  
as can be seen  this does a great job of eliminating irrelevant overall
fluctuation while preserving point to point relations and changes  

to those in   a  that do not pertain to planetary transits  they
may correspond to binary star systems  pulsating stars  etc   
 b  light curve brightness levels are often on scales that cannot be
directly compared  depending on the overall magnitude of the
light from the host star  for example  the units in   a are an
order of magnitude greater than the measurements in   b 
 c  overall  light curve brightness measurements are quite noisy 
this includes noise from low frequency drifting    b   highfrequency noise    c   and possible sensor errors  in noisy
examples  it is often not immediately clear what should be
considered a peak and what should be considered noise    d
is a false positive example  but many confirmed planet curves
look similar  
 d  lastly  there is the problem of somewhat indirect labeling  in
particular  the ground truth labels are for the actual host star
 independent of that quarters observed light curve   and not
for each and every light curve for any quarter that star was
observed  importantly  this means that  just because a curve
is from the confirmed planet class  it is not required that even
a single planetary transit be visible in each of its light curves 
this is especially true when we consider the time scales involved   each light curve consists of measurements over   
days  meaning exoplanets with orbital periods greater than
that might have no transits in a given light curve   in general 
though  since planetary transits are currently the most common contributor to exoplanet discovery      we might expect
at least one planetary transit across all observed quarters for
a given confirmed planet scenario  though this is not guaranteed  
overall  these issues underscore that the dataset is largely noisy and
approximate  with numerous obstacles complicating classification 
this is perhaps expected  though  as extrasolar stars and exoplanets
themselves vary enormously  and the distances we are dealing with
are literally astronomical  for all these reasons  machine learning
becomes necessary to solve this problem 

 

figure    example of preprocessing steps  preprocessing outlined for an example confirmed planet light curve  in all curves  the horizontal axis corresponds to time
points  as before  in  a   the vertical axis is the original flux in electrons per second 
as before in figure     in  b  and  c   the vertical axis is converted to percentagechange from a local baseline using a moving average calculation  in  c   we show the
thresholds based on standard deviations below the mean  the green line correspondings to one standard deviation below the mean for this example  weak peaks   and
the red line corresponds to two standard deviations below  strong peaks   as we
would hope  the clearly visible and defined peaks in this example are all labeled as
strong peaks 

preprocessing

several preprocessing steps were needed to eliminate noise  normalize
between light curves  and extract meaningful features  to reinforce
the need for more complicated preprocessing and feature extraction 
initial baseline tests using just global statistics of each light curve
 mean  median  and standard deviation  as features for the models
discussed in section    performed at chance at best  not shown   to
illustrate the preprocessing pipeline used here  we will use figure   
as a fairly end to end example 

   

   

thresholding and peak identification

now that much of the data has been normalized away  we can simply take value thresholds on the percentage change metric to identify
peaks  this was impossible before due to the non standardized baselines and units we were dealing with  using two different thresholds
  drops below one and two standard deviations from the mean of the
percentage change data   seemed to work best for identifying peaks 
see figure   b c  intuitively  we want to identify what points in
each light curve correspond to strong peaks  most likely planetary
transits   what points correspond to decent peaks  decent chance of
being a real planetary transit   and what points are likely just noise
 likely non peaks   this was done by simply iterating through all
points and labeling them as one or more of the following three sets 
    strong peaks  with percentage change value less than  two stan 

initial noise removal

consider the example light curves shown in figures   b and   a
 both confirmed planet examples   both have significant lowfrequency fluctuation  since the curve is oscillating and moving at a
relatively large scale  this is irrelevant  and  in fact  distracting and
misleading  when looking for planetary transits  as transits must be

 
for example  an exoplanet with orbital period similar to earths would likely only show a real planetary transit once every three or four quarters  fortunately 
many planets orbit much quicker than earth  as an extreme example  the fastest known exoplanet orbital period is      days     
 
exoplanets may be discovered confirmed via other methods besides planetary transit  for example 
 
a filter in the style of a fft high pass preprocessing step could also perhaps have been used  but fourier transforms were avoided in this project  see
section   
 
note that we only care about dips in light  negative percentage changes   so the resulting small positive peaks from percentage change operation are not at
all detrimental 
 
note that less than a certain threshold refers to the actual signed relation to the mean  and not simply the magnitude distance from the mean 
 
as a result  points in the two standard deviation category are always in the one standard deviation category   this intuitively makes more sense since it
prevents splitting up a single peak into two perceived peaks at the one standard deviation level  and also gave slightly better performance 

 

fiabotros  

transits  peaks in our data  by looking at the standard deviation of
the non peak inter peak intervals  and would hope to see minimal
standard deviation for confirmed planet cases  in addition  by inspecting peak width  we approximate how brief peaks for confirmedplanet examples should be in comparison to false positive examples 
peak max and average values give us information about the strength
of the peaks  and our counts and fractions for number of peaks and
peak points tell us how many peaks there are and how much they
dominate the entire time series 

dard deviations below the mean      weak peaks  with percentagechange value less than one standard deviation below the mean    and
    non peaks inter peak points  with percentage change above one
standard deviation below the mean 

 

features

using the output from the preprocessing step outlined above  we
can proceed to extract meaningful features from each time series of
points  in particular  since time series cannot be easily and reliably
aligned  we need to remove time as a factor when extracting our
features  intuitively  we want to know about the peaks versus the
inter peak stretches  furthermore  we want to design our features
to capture some information regarding      consistent periodicity of
peaks   exoplanets should be orbiting their stars at a constant orbital
rate  so transits should be relatively periodic  and     peaks should
be relatively brief   since both kepler and the exoplanet are moving
 possibly even in perpendicular planes   the transit time should be
relatively short 
using    as a visual example  we would define both  a  and  c 
as peaks for the first threshold   c  as the only peak for the second
threshold  and  b  as a non peak inter peak interval  we would then
calculate the following feature set for each light curve to approximate
all the meaningful information desired that was discussed above 
   fraction of all time points that are in each of the three categories

   number of distinct continuous peaks  two such peaks at  a 
and  c  for the first threshold  one at  c  for the second threshold 

figure    visual reference for peak feature extraction  note this is simply
a toy example and is not drawn accurately to scale  the black line represents the
mean  usually around    percentage change in the pdcsap flux time series   and
the green and red line represent the first and second thresholds  respectively  the
first threshold corresponds to one standard deviation below the mean  and the second threshold corresponds to two standard deviations below  by separating points
as such  we can easily extract features about both the strong peaks  the weak peaks 
and the inter peak intervals 

   peak width  widths of  a  and  c  in time    mean and standard
deviation

 

   global mean  median  and standard deviation

   

   peak max magnitude  most negative value of the peak    mean
and standard deviation

models and learning algorithms
learning algorithms

this is primarily a supervised learning problem  where we want
to take the feature vectors constructed above for each light curve
and classify them correctly after training with the ground truth
labels  as mentioned  we have two ground truth classes  confirmedplanet and false positive   and a large feature vector for each light
curve  we used four distinct machine learning algorithms to perform classification on this dataset  k nearest neighbors  logistic
regression  with stochastic gradient ascent   softmax regression
     with stochastic gradient descent and regularization  see section
     and svm  with an rbf kernel   all algorithms except svm
were implemented manually in python with numpy      svms were
run using libsvm      
all hyperparameters  such as regularization terms  number of
maximum iterations  step size type  and convergence criteria  for logistic regression and softmax regression were chosen using manuallyimplemented k fold cross validation  due to excessive runtime and
limited resources    fold cross validation was used   hyperparameters for svm classification were chosen automatically using crossvalidation as part of the given libsvm tools 

   overall peak points values   mean and standard deviation
   non peak inter peak width  b    mean and standard deviation
where feature subsets for numbers    through    are computed for
both the first threshold and the second threshold separately  overall  this constitutes    different features  to  fairly navely  account
for possible relations between these various features  and to be able
to place these first order features into a higher dimension where we
might be able to learn a linear boundary that is non linear in the
original feature space   we also take pairwise products of all of these
   features  when we add in a bias term of    for logistic regression 
for example   the first order features  and all the second order pairwise features  we get a total number of     features  thus meaning
that each light curve is represented as a vector in     in our feature
space 
since many of the resulting features are still on fairly incomparable
scales  which is exponentially exaggerated by some of the learning
algorithms used   we normalize each feature using mean removal
and dividing by the standard deviation  similar to the preprocessing
and normalizing we might do for an algorithm like pca  for example   in manual inspection of the new scaled feature ranges  this
seemed to work well in properly scaling features to be comparable
to all other features  in testing  this also improved performance  not
shown  
to address the concerns we mentioned above  in particular  we model the consistency and periodicity of any planetary

r

   

other algorithms

to aid in some of the analyses  other algorithms were enlisted 
including k means  run repeatedly from different random initializations  and pca  k means was implemented manually in python
using numpy  while the built in pca function of matplotlib     was
used for running pca 

 

fiabotros  

 

results

out of all other learning algorithms explored here  though overall 
all algorithms performed similarly  
lastly  we ran pca to reduce the dimensionality of the feature
space  taking only the k top principal components and re running
tests for each value of k over all learning algorithms with their optimal hyperparameters  this worked surprisingly well and speaks
to pcas ability to reduce dimensionality while preserving variance 
figure    shows that performance of most algorithms stayed fairly
high even when using a minimal basis of principal components  as
few as      principal components down from our original      depending on the algorithm   of note  logistic regression and softmax
regression saw fairly sizeable declines as the number of principal
components became quite small  since svms were still able to learn
in these spaces  we hypothesize that the reduced principal components remaining may have corresponded to some of the more linearlyrelated features in the dataset  so logistic regression and softmax
regression were unable to learn non linear decision boundaries  the
svms rbf kernel might have allowed it to do continue to do so
by allowing it to work in a higher dimensional feature space due to
inner products   at any rate  the data shows that we can get away
with as few as    principal components in almost all cases and still
perform fairly similarly to performance using our full feature space 

using the optimal hyperparameters for each algorithm obtained via
k fold cross validation  as explained in section      we obtained the
performance metrics shown in table     performance is in terms
of percent correct  and is shown for the hyperparameters for each
algorithm that lead to the best  cross validation  performance  
learning algorithm
k nearest neighbors
logistic regression
svm  rbf kernel
softmax regression

train performance
     
      
     

test performance
     
     
     
     

table    performance of learning algorithms on full training and testing
sets 

before adding softmax regression  logistic regression  with no regularization  had the best test performance on this dataset  however 
we wanted to see if we could further improve performance  to do
so  we reasoned that  since exoplanets usually fall in different buckets of shapes  sizes  orbits  and other physical characteristics  earth
analogs  super earths  gas giants  etc    we might be able to first
cluster them to learn better decision boundaries for each sub class
of confirmed planet and false positive light curves 
this indeed ended up giving a small increase in performance  see
figure      though it seems like the softmax regression performance
may be highly dependent on the nature of the clustering  the cluster
assignments that minimize the k means distortion function are not
necessarily the ones that maximize softmax regression performance  

figure    learning algorithm test performance as a function of pca component count  note that most learning algorithms maintained good performance 
even when using a very reduced number of features  also note that  due to very poor
performance of softmax regression with    clusters  we also ran it with   clusters
 the second best cluster number from our earlier k means testing  not shown   pca
component count does not include an additional bias term that was still added to all
feature vectors  lastly  the final data points correspond to the original dataset and
full feature space  with no pca applied 

 

figure    softmax regression performance as a function of number of kmeans clusters  in particular  k      enabled the best softmax regression classification performance  surpassing logistic regression 

error analysis

by examining the examples that were classified incorrectly  we can
get a better idea of where the algorithms are not properly learning 
and what examples and features might be confusing the algorithms 
in particular  across all algorithms  many of the examples similar
to those shown in figure    were misclassified  figure   a shows a
false positive light curve  however  it appears to have all the characteristics of a confirmed planet curve  including strong  defined  and
consistently periodic peaks  on the other hand  figure   b shows a
confirmed planet light curve that appears to be more akin to random
noise  examples like these are extremely tricky for the algorithms
shown  as the features for these types of confusing examples are
likely very similar to the opposite class  such light curves exemplify
many of the problems outlined in section     properly classifying such curves would likely involve bringing in outside information

in particular  we used k means clustering to first do unsupervised
clustering of each of the two parent classes independently  thus  each
of the two parent classes was broken down into k new child classes  
in doing so  we might be able to better learn how to distinguish various distinct sub types  sub classes or clusters   instead of having
to group all instances of each parent class together when learning
a decision boundary  to then perform classification  we learn using
softmax regression over the new child classes  choose optimal hyperparameters  using k fold cross validation over the child class labels
on the validation sets  and then at the final test phase simply output and test against the childs parent class label instead of the child
class label  putting this all together gave the optimal performance

 
for k nearest neighbors  k     performed best  for logistic regression  setting a max iteration limit of       iterations through the entire dataset  along
with zero regularization and step size equal to one over the square root of the number of updates made so far  did the best  though training with only around
    iterations gave nearly similar performance   for svm classification  the best hyperparameters were c       and            
 
if k      for example  we would have   child classes for confirmed planet curves and   for false positive curves  for a total of k        possible child classes
to perform classification over 
 
for    k means clusters  the optimal hyperparameters for softmax regression were roughly       training set iterations using stochastic gradient descent
 though training with as few as         iterations gave nearly similar performance   with a relatively small regularization parameter         and dynamic step
size  one over square root of number of updates made so far  

 

fiabotros  

feature extraction steps  but we chose to avoid these techniques for
a couple reasons  aside from the authors minimal experience with
these techniques  we reasoned that the information was not in the
frequency domain  since we really cared about very small scale  local 
point to point changes  thus  doing any transform  even if preserving time using sliding fourier or wavelet approaches  would likely
have smoothed out and lost some of the crucial point to point information  by definition of these techniques  lastly  we reasoned
that  at best  the output of these techniques would give a somewhat
smoothed out version of the initial data points  and we would still
have to run some form of peak detection to isolate peaks   which
would be much more difficult with smoothed out data  however 
that said  given more time  sliding fourier transforms and wavelet
analysis should certainly be investigated to know their potential for
sure 
other algorithms could also be tested on this  and any augmented 
feature set  for example  svms with other kernels  neural networks 
and other such algorithms might give good performance here  especially if there are certain nonlinear decision boundaries that we
were unable to learn using our approach  even more hyperparameter tweaking could likely give slightly better performance  too  but
the exorbitant time needed to run the algorithms and tests already
discussed precluded doing so 
overall  with minimal domain knowledge and a general approach
to the raw data  given a light curve consisting of brightness readings
for far off extrasolar stars over    days  we were able to preprocess
and extract features relevant to planetary transits  learn over these
features using machine learning algorithms  and classify the stars as
having exoplanets or being false positives with relatively high probability  around       we would say that  as a whole  this project was
a success  and could even be used to classify exoplanet harboring
stars and identify exoplanets that have yet to be discovered  we
hope to continue to develop and test this system as more exoplanet
data surfaces  as it would be very interesting to see if predictions on
future data are reliable and can classify exoplanet systems that have
not even been confirmed  all in all  this work is a  small  contribution to mankinds ongoing efforts to better understand our earth 
our universe  and our place in the stars as we search for new earths
on the final frontier 

other than just raw brightness changes  and might also benefit from
linking light curves from the same star across different quarters  see
section    

figure    examples of light curves that were often misclassified by all algorithms  example  a  is actually a false positive light curve  though it appears to
have distinct  periodic peaks  example  b  is actually a confirmed planet light curve 
though it looks more like noise  in particular  compare these to the examples in figure
   

 

discussion

overall  we present a system for preprocessing  feature extraction 
and machine learning on a large dataset of kepler telescope light
curves for binary classification of stars as potentially harboring exoplanets or likely having none  this system performs quite well  with
up to around     accuracy in testing  as more time passes and more
exoplanets are discovered  we are still in the relatively early stages of
exoplanet discovery  especially using planetary transits   the system
will have more data to train on and could therefore perform even
better in the future  in its current state  we guess that it performs
likely on a similar level to nasa algorithms  though more literature
research is needed to validate this assumption    in addition  using
informally trained human observation as a proxy to an oracle  the
author was able to achieve around     correct in binary classification  meaning the learning algorithms discussed in this work perform
even better than humans with only informal training   
all algorithms performed well on this manually extracted feature
set  however  this is not to say that this was the first feature set tried 
countless different feature extract pipelines  methods  components 
and feature vector sizes were tried before reaching the final performances shown above  in particular  for preprocessing and feature
extraction  numerous peak thresholds  peak characteristics  normalization scaling techniques  and set sizes were entertained along the
way 
there is likely also much room for improvement   in particular 
more analysis could be done to better identify which features are
most important and which are distractors by evaluating metrics such
as mutual information    in addition  more domain knowledge beyond the authors limited knowledge could be injected into the data
extraction and feature selection  while one of the primary goals of
this project was to classify using only light curve brightness metrics in a single light curve  incorporating data from other sources
 spectrometry  locations in space  knowledge about each star  centroid changes for host stars  etc   and linking light curves for the
same star across different quarters  a given star should only be given
one label across all observed light curves quarters  would likely both
help classification 
it should also be noted that sliding fourier transforms and wavelet
analysis could likewise have been applied in the preprocessing and

 

references

    see http   kepler nasa gov  
    cassan  a   d  kubas  j  p  beaulieu  m  dominik  k  horne  j  greenhill  j  wambsganss  j  menzies  a  williams  u  g  jorgensen  a  udalski  d  p  bennett  m  d 
albrow  v  batista  s  brillant  j  a  r  caldwell  a  cole  ch  coutures  k  h  cook 
s  dieters  d  dominis prester  j  donatowicz  p  fouque  k  hill  j  kains  s  kane  j b  marquette  r  martin  k  r  pollard  k  c  sahu  c  vinter  d  warren  b  watson 
m  zub  t  sumi  m  k  szymanki  m  kubiak  r  poleski  i  soszynski  k  ulaczyk 
g  pietrzynski  and l  wyrzykowski  one or more bound planets per milky way star
from microlensing observations  nature                          web 
also see http   www nasa gov mission pages kepler news kepler         html  and http   www 
space com        milky  way       billion  planets html 
    see http   exoplanetarchive ipac caltech edu index html 
    see http   archive stsci edu kepler  
    see http   fits gsfc nasa gov  and http   en wikipedia org wiki fits 
    see http   www astropy org  
    see http   www csie ntu edu tw  cjlin libsvm  
    see http   www numpy org  
    see http   archive stsci edu kepler manuals archive manual pdf 
     see http   kepler nasa gov education activities transittracks  
     see collected information at http   en wikipedia org wiki discoveries of exoplanets 
     see http   ufldl stanford edu wiki index php softmax regression and http   ufldl stanford edu 
tutorial supervised softmaxregression  
     see http   matplotlib org api mlab api html matplotlib mlab pca 

  

we were unable to find hard numbers regarding corresponding performance from systems used by nasa and other astronomical research groups  however 
it is important to note that stars that are labeled as possibly harboring exoplanets due to detected planetary transits first become objects of interest or
exoplanet candidates  and are confirmed through more thorough investigation  and possibly other methods  such as spectrometry  before becoming a confirmed
exoplanet 
  
crowd sourced efforts like that at planethunters org attempt to use human pattern recognition to properly classify complex light curves that computer
algorithms might have missed  sites like this include more formal training  so human performance likely increases significantly  at least for many of the
edge cases that could be specifically trained on 
  
we started an investigation into correlation of various features with labels  but were unable to complete this in the time allotted  and opted for the pca
approach instead 

 

fi