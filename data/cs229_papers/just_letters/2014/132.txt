legal issue spotting   first phase legal analysis recommendations
from parallel nave bayes models
john phillips
stanford university
jophilli stanford edu
 
abstract  legal analysis is a multistep process that
performs the complex tasks of identifying real world
activities through the lens of a predefined legal code and
sets of laws  based on a brief understanding of an issue a
legal professional must make a recommendation on where
to apply limited researching resources 
we employ nave bayes models in a parallel  nonmutually exclusive alignment towards the generic first
phase of legal analysis  legal issue spotting  we do this by
training separate supervised learning models by first
extracting all references to the u s  legal code from each
case opinion of the u s  federal reporter   rd series  f  d 
and treating it as a binary label to that u s c  section  we
then use the same feature set for all binary classification
models selected from the most frequently occuring words in
the opinions of the u s  federal reporter   rd series  f  d 
after nlp techniques are applied  to test we use      
hold out validation testing and find mixed but not
uninspiring results 

keywords  u s  legal code  law  legal analysis  supersized
learning  nave bayes

introduction
despite efforts in recent years to advance legal analysis
through various techniques  automated issue spotting
remains problematic throughout the legal profession  the
complex  and overlapping implications of legal understandings
and a syntactically nuanced legal code create massive
challenges for strictly nlp     and syntactic learning
mechanisms  and algorithms   moreover the writing style of
different legal professionals  and of clients  present real
challenges towards the scaling of any successful model 
despite these challenges  the ambition of advancing legal
counsel through automated issue spotting and recommended
analysis remains preeminent 
the objective of this project is to create a model  or set of
models  capable of ingesting text based scenario descriptions
and predicting which areas of the code of laws of the united
states of america  u s  code    u s c   
we should note that by issue spotting we refer to the first
step in legal research of identifying possibly legal areas which
may have been violated and not the specific logistical aspect of
legal research of looking up legal documents   where there is a
small number of commercialized catalogues of the u s  legal
code which aid researchers once legal areas have been

identified for research and where a good number of
technological advancements have already occurred  

experimental design
a  background
prior efforts towards the creation of an algorithm capable of
identification of tautological statements within a given text
have proved problematic  nuances of meaning and word
associations have prevented unambiguous procedural machine
learning efforts to understand a given issue provided its written
description  moreover  when text is compartmented and
restricted to registered input values  these associations have
proved in some cases overly narrow and thereby restrictive in
developing into insight beyond the alternative of direct human
observation 
from the standpoint of developing a fully formed legal
analysis tool  these results at best could be characterized as
having achieved an incomplete mosaic of legal analysis 
thus  while our initial orientation towards this problem
indicated conducting an nlp treatment of the actual u s  code
itself before applying a machine learning algorithm against the
digested nlp treatment  our investigation of prior research
along these lines dissuaded us from continuing down this line
of inquiry 
instead we sought an entirely different dataset which might
be distilled into a supervised learning training set s  and test
set s  and which might help simplify our pre training data
processing  to this end we believe we now have such a dataset 
though perhaps more bulky than one would hope 
b  dataset
our effort therefore was to find a dataset that highlights a
significant number of complex legal topics without losing the
contextual flavoring of associated descriptions  in finding such
a dataset we aimed to bypass syntactic and referential
complexities associated with current impasses in nlp research
of the same end state pursuit 
towards this end we scraped all     volumes of opinions of
the u s  federal reporter   rd series  f  d   dating back to
      from this writing in december        in this scraping we
found         opinions enumerated by our target website  with
each opinion listing references totaling anywhere from zero to
    individual  though not unique  references to various
u s c  sections and subsections 
in total we found         individual references to       
unique u s c  sections and subsections  with the most
frequently referenced sections appearing in approximately       of legal opinions and quickly dropping by the twentieth

fimost frequently referenced u s c  section  to appearing in less
than    of the legal opinions 
to give a foreshadowing of how we used these references 
we might see how the array of references to each of the       
u s c  sections can be treated as a binary value vector for a
simple nave bayes multinomial model developing a unique
model for each unique reference destination  with the features
coming from word frequencies of the entire corpus of legal
opinions 
c  processing
before we describe our models  however a more exhaustive
description of the original data and processing may be
instructive 
each legal opinion was written to explain the given ruling of
a particular case  paragraphs explain the pertinent events of a
given specific set of facts that relate to the case and in some
instances these paragraphs are followed by a quick sentence
indicating a law and section or sometimes a subsection within
the u s c  which may be instructive for the reader of the
opinion to review  this structure is the heart of constructing
our training examples and provides us the type of dataset we
sought earlier 
by viewing the words within an opinion as the basis to draw
features or x values from  we then may view these short
references to u s  legal code sections as destination value or
y mappings 

sentenced under the united states sentencing guidelines to
life imprisonment  and thornton and jones were each
ordered to forfeit            to the government pursuant to
   u s c  sec              the defendants have not
challenged the propriety of their sentences or fines  nor 
significantly  have they alleged that the evidence was
insufficient to support the verdicts   
thus  we can observe that in only one paragraph of one
opinion in one volume of opinions we find no less than six
distinct laws under the u s c   which our hope is to train our
model to correctly predict references to 
so by using these individual references from the original
opinions and setting each of these target references to be a
y destination for each training example of an opinion we
fashion numerous positive examples of each law or sub law
referenced in the original opinion 
moreover  while we may only get a handful of positive
examples of a mapping instances to a given law  by applying
this method to a large number of opinions and taking care to
ensure that do not submit a training example as both a positive
and negative example for a given law  then we simultaneously
generate numerous negative examples out of each original
instance which does not reference the specific legal code
section or subsection  we are thus afforded this benefit
through submitting each of the examples in parallel for each of
the binary decision models for each specific law  which again
is dependent on the non mutually exclusive nature of our
recommendation architecture 

references per legal opinion  
  
log of total 
number of legal opinions 
with x number of references   

  

  

  

  

  
  

   

   

   

   

    

    

    

    

number of references within a legal opinion 

fig     opinion to legal code law mapping

graph    total of the number of opinions with a given number
references  totals are given by log total  

an example here is illustrative 
the jury found the defendants guilty of conspiracy to
distribute and to possess with intent to distribute cocaine and
heroin in violation of    u s c  sec             and
possession with intent to distribute and distribution of a
controlled substance in violation of    u s c  sec      a    
        in addition  thornton and jones were convicted of
participating in a continuing criminal enterprise in violation
of    u s c  sec              supp  iii        and fields
was convicted of using a firearm during a drug trafficking
offense in violation of    u s c  sec      c            
supp  iii         and possession of a firearm after having
been previously convicted of a felony in violation of   
u s c  sec      g             all three defendants were

our total number of        uniquely referenced u s c 
sections and subsections might then be turned into       
unique models  each using a unique binary value vector based
on the references to or not to  that particular u s c  section 
 we expand this idea later in our model description and
examine why we chose this over a k clustering and other
models  
so to address an earlier concern  we can note here that by
using the opinions instead of the u s c  itself we abstract away
from dealing with the nlp issues around the language of
various laws and how they interact with given circumstances
and rely instead on the word choice of numerous justices as
they write their considered opinions 

fid  concerns
one risk that we will acknowledge here and attempt to
address in our conclusion  is that the sum total of opinions 
even at its fullest articulation may miss some areas of the u s 
legal code to which a user might be concerned 
we know that the u s c  is itself rather large and complex 
in      the u s  house judiciary committee asked the
congressional research service to provide a calculation of the
total number of criminal offenses contained in the u s c  the
crs responded indicating that they lacked the manpower and
resources to provide an update to a number from      of      
total crimes  however the judiciary committee chairman
characterized the as growing at a rapid rate of     a decade 
so if we allow that the total number of criminal offenses may
only make up a subset of u s c  sections which an ideal legal
analysis function may map to  it is reasonable to be concerned
that the overall number of opinions may only generate a subset
of training events per u s c  section 
while we will attempt to address this concern later  it may
be that this weakness of the dataset must simply be endured in
its the models which use it 

model and feature selection
a  model selection
thus with our dataset given and our concern of one possible
risk articulated  we look to select a model which will enable us
to make the most use of our data 
with our updated dataset the structure of the binary nature of
the mapping and similarity to spam filter type problems
becomes immediately apparent  and this channels our research
towards two clear implementation models  support vector
machines  svms  and nave bayes bucket of words 
the model we select is a nave bayes word bucket model
which a cs    participant will recognize as the model used for
spam filtering in the early part of the course  we add one
simple change to this model  instead of selecting a feature set
and then training one binary model against that feature set
 spam not spam  we train multiple binary recommendation
models against the same set of features using separate binary
recommendation labels  in theory we would seek to do this for
the entire u s c  reference list         unique u s c
references   however due to time and resource constraints we
modeled the top    frequently occurring references for this
project and this paper 
additionally  our original ambition was to implement both
nb and svm models and compare the results  however due to
the size of the dataset and the constrained working
environment of only having one project team member  we had
to reduce our goals to accommodate the more immediate task
of completing our research on time 
to that end  the model description is a straightforward
multinomial nave bayes model  which we used for each of
the    models we created 

multinomial naive bayes model 

in theory we would set each of our        y value laws as
separate target filters and allow the collection of the y values
to be non mutually exclusive  thus creating        unique
models  in practice we use    binary value vectors for the top
   most frequently occurring references for    unique models 
regardless of the number of models  however  we want to
enshrine the non mutual exclusivity of our dataset and carry
that through to our results  we do this with an eye towards our
end user as a legal professional and the context of legal
research as the user will likely be interested in all types of open
legal questions  not simply the highest likelihood given the
additive nature of legal complexities  thus we would not
choose a clustering algorithm to evaluate our dataset corpus as
the clustering algorithm would simply return the highest
likelihood references  not a composite of all likely references 
so with these destination y values in place  we can now
use the entire training set as both positive and negative training
instances depending on the nature of the y mapping for the
individual training event and which specific law we are
currently training on  and indeed  the complete list of laws
would in theory  be the entire u s  legal code but will  in
practice  be the full list of laws referenced in the opinions
contained in the full training set 
for training and testing we can see that this amount of data
lends itself to a k fold validation  though  given the inevitably
low number of positive training instances for some referenced
laws  we may take note that it make sense to modify our
training set  so that our nave bayes model does not become
overly negative in its predictions  we will examine this
concern more in the results section 
b  feature selection
finally  for our models feature set we identified the top
     most frequently occurring lemmas after we conducted
natural language processing to remove punctuation  numbers
and uppercase  and after tokenizing and lemmatizing those
results 
we made certain to remove numeric values as a simple
review of the dataset would provide the concern that that a
nave bayes model may identify the actual references of
specific legal code section  e g                     
     and     from our earlier example  and attach a higher
value on those raw numbers as they might occur more regularly
than a randomly occurring number within a random
explanation of a legal situation  moreover this potential
problem might have metastasized when we consider non u s 
legal code references that may occur on a regular basis such
as individual state laws  or the titles of specific court cases 
both of these appear to occur frequently enough to corrupt
our values within our model for those individual words and
abbreviation terms  thus  our removal of integer values from
the corpus before obtaining word bucket frequencies addresses
these concerns 

firesults table for the top    models of the most frequently occuring u s  legal code references within our dataset 

total number of
appearances in
appealate course
cases corpus
     
     
     
     
     
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

legal code section
   u s c      
   u s c     
   u s c      
   u s c      
   u s c     
   u s c      
   u s c      
   u s c     
   u s c      
   u s c     
  u s c      
  u s c      
   u s c   
   u s c      
   u s c      
   u s c     
   u s c      
   u s c    
  u s c      
   u s c      

legal area  subjective description 
statutue of limitations
prohibited acts a  drugs
final decisions of district courts
state custody  remedies in federal courts
mandatory minimums
federal custody  remedies on motion attacking sentence
federal employment discrimination  unlawful employment
unlawful acts  firearms
imposition of a sentence
attempt and conspiracy
immigration reform and control
judicial review of orders of removal
federal crimes and criminal procedure
proceedings in forma pauperis
review of a sentence
conspiracy to defraud the united states
interlocutory decisions
securities exchange
reentry of removed aliens
civil enforcement

train set  pos neg 
                   
                 
                   
                 
                 
                
                
                
                
                
                
                
                
                
                
                
                
              
                
                

test set  pos neg 
                
                
                
                
                
                
              
              
              
             
             
             
              
             
              
             
              
             
             
             

test documents
misclassified
test error
    
      
   
      
    
      
   
      
   
      
   
      
   
      
   
      
   
      
   
      
  
      
 
      
   
      
   
      
   
      
   
      
   
      
  
      
  
      
  
      

      hold out on all positve references with negative references obtained randomly to match the positive value total

after this processing we were left with over    million
individual combinations of an opinion to feature word count 
for intuition  this set totaled roughly    mb of data in space
delimited  csv file of three integer columns  thus to process a
given models input  we simple reduced the full    mb to
training and test sets of feature data  and paired those sets with
their corresponding binary label vectors for the given examples
used 

results
a  training difficulty
we originally trained our models by taking the entire dataset
and using an       holdout for each separate training and test
set  however  while this method worked for the most
frequently occurring referenced models of our twenty models 
this quickly presented a problem for the less frequently
referenced legal sections models within our twenty  as they
started to predict negative recommendation for all test
instances 
considering that our long term goal might be to develop the
additional        models not built for this project and paper 
and knowing that all of those models would have a reference
level lower than our lowest referenced models  we needed to
find a different way to accommodate the training and testing of
our low reference models 
to that end we treated the positive references as the rare
commodity for each model and created our training and test
sets with an eye explicitly towards the number of positive
reference values of the entire training set  we would do this by
taking the entire positive reference set and splitting it into
      holdout training and test sets  and then matching those
positive training events with an equal number of negative
training and testing values which we sampled randomly from
the entire negative reference sets 
b  results
our results were mixed  while no individual model showed
a test error greater than    many were near               
        balancing some models high error scores  we did
have some models score dramatically low with values less than

    and in one case below      overall the majority of our
error levels fell near      

discussion
the difference which yielded our spread in test error results
might originate from the type of language used within the
specific types of case opinions containing a particular reference
type  a quick look at the higher scoring models yields the
subjects of the proceedings in forma pauperis  ensuring federal
custody  and final decisions of district courts  all of these
from a subjective vantage point appear to have a highly legal
nature to them  which might make them difficult to
disambiguate from negative examples  but this intuition may
not hold  some of the low error models dealt with topics
regarding equally similar legal focus 
nevertheless  one clear area for future examination is to test
and build up a useable stop word list  possibly through tf idf
processing of the corpus  

conclusion and future work
overall the majority of our error levels fell near     which 
though high for a spam filter  might be considered a valuable
first contribution given the complexities of legal analysis 
our initial intuition to use of one feature set for all types of
u s  legal code reference models  while valuable from an
initial processing standpoint  likely needs to reexamined  it is
probably more reasonable to develop frequency lists based off
of each positive set of references and with respect to each
model  and in combination with a legally focused stop word
list   rather than the entire corpus of appellate court ruling
opinions 
nevertheless the use of building multiple models against
datasets with multiple parallel labelings seems to hold promise
as the variance between our models error rates implies a
shifting detection criteria from model to model  and therefore a
confirmation of our intuition that separate reference subjects

fielicit word usage of a shifting yet individually model specific
correlative nature 
moreover as a simple way of inverting the nave bayes
spam filter from removing negative content to
recommending multiple non mutually exclusive types of
content we believe that this type of parallel model usage shows
promise 
to revisit a concern briefly touched on earlier  an observer
may detect an implicit assumption  that we use the articulated
legal opinions of appellate court judges as training data and
from the ingested description of events from an unknown
source which we will use at test time  in practice this difference
may vary considerably  in practice this difference may vary in
professionalism  writing ability  and tone thus producing a
model bias that is not inherent in the models spam filter
cousins which both train on and are applied to equivalent
subject emails  more research will be needed to discover if this
difference in source type proves vital 
separately  we likely have some over representation of legal
terms in our feature set  which could be removed and replaced
with features further down on the frequency distribution  this
as noted earlier  might be accomplished through the use of
stop words during our frequency distribution preprocessing
when creating our feature lists 
finally  to deal with the issue of our dataset not mapping to
the entire u s  legal code as a result of an insufficient number
of cases applying to that area of the u s  code we might
attempt to engage an associative clustering algorithm to derive
similarity between individuals laws based off of their nonstandard terms 
to deal with these low reference values and unmapped laws
we might engage a second dataset  the text of the u s  laws
themselves  with this second data set we might engage an
associative clustering algorithm to derive similarity between
individuals laws based off of their non standard terms  a
function of tf idf processing   this output ultimately would
feed into a clustering analysis of all u s  federal laws which
we use to associate laws to one another  this association of
laws would allow us to thus provide a proximity value for
unmapped laws from the first dataset model thereby providing
us more laws within our models reach 
thus by attaching clustered laws to the output targets from
the machine learning model we may be able to map to a much
broader segment of the u s  legal code 

references

                                                        
  bommarito ii  michael j   and daniel m 

katz   a mathematical
approach to the study of the united states code   physica a  statistical
mechanics and its applications                          
  lame  guiraude   using nlp techniques to identify legal ontology
components  concepts and relations   law and the semantic web 
springer berlin heidelberg                
   doan  anhai  et al   ontology matching  a machine learning
approach  handbook on ontologies  springer berlin heidelberg       
        

                                                                                
   baharudin  baharum  lam hong lee  and khairullah khan 

 a
review of machine learning algorithms for text documents
classification   journal of advances in information technology    
             
   waterman  donald a   jody paul  and mark peterson   expert
systems for legal decision making   expert systems                    
   cormack  gordon v   and maura r  grossman   evaluation of
machine learning protocols for technology assisted review in
electronic discovery  
   united states of america v  bryan thornton  a k a  moochie  
appellant  d c  criminalno               united states of america v 
aaron jones  a k a  a    j   appellant  d c  criminal no             united states of america v  bernard fields  a k a  quadir    q  
appellant  d c criminal no                  f  d            
paragraph   

fi