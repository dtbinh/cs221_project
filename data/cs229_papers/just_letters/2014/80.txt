predicting seizure onset with intracranial electroencephalogram
 eeg  data   project report
alex greaves  arushi raghuvanshi  kai yuan neo
december     

 

abstract

can take eeg readings      for the first time  it is
possible to track a patients brain activity on a daily
epileptic patients have little to no warning about an basis  if we can develop an algorithm that uses these
oncoming seizure  and would benefit from knowing eeg signals to predict when a seizure is going to ocif they are about to have one because it allows them cur  patients can be warned ahead of time that they
more time to find a safe place  the primary challenge are about to have a seizure  allowing them to lead a
in seizure prediction is differentiating between the safer life 
preictal  pre seizure  and interictal  baseline  states 
in this paper  we developed a method to address the
    past work
following goal 
until the last decade  seizure prediction was only poscan we use eeg signals to accurately
sible by visual analysis of signals by certified neurolclassify the preictal state in human and dog ogists  since the advent of machine learning techpatients with naturally occurring epilepsy 
niques and modern computing  seizure prevention researchers have focused efforts on deciphering eeg
using over       training examples  we investigated
messages as a means of predicting seizures 
multiple feature extraction and classification algothere were some early optimistic papers on seizure
rithms including discrete wavelet transform  dwt  
prediction using eeg data  however  these results
short time fourier transform  stft   principal comwere not tested in a rigorous  statistical fashion  and
ponent analysis  pca   k nearest neighbors  logisdidnt perform better than random guessing when retic regression  and support vector machines  using
produced in the wild      following this      review 
stft  pca  and logistic regression we developed a
there has been some progress in research in the space 
feature extraction and classification algorithm to clashowever no algorithms have come close to the necessify eeg signals as preictal or interictal with an area
sary accuracy needed for practical use      in a reunder curve  auc  score of     
sponse to the need to differentiate between preictal
and interictal states  the american epilepsy society
posed this problem as a kaggle competition challenge
  introduction
    
one notable paper by subasi et al  demonstrates
    motivation
that dwt combined with pca and svms are most
over    million people worldwide currently live with effective at classifying their test data  subasis experepilepsy and   in    americans will develop epilepsy iments have   shortcomings  they only train and test
in their lifetime  patients with epilepsy are suscep  on a small data set of      samples  and their svms
tible to spontaneous seizures  which are particularly only use   kernel function  the radial basis function 
dangerous if they occur in a potentially hazardous the former is prone to over fitting  and the latter
environment  such as behind the wheel of a car  cur  means that there are potentially better performing
rently  the majority of epilepsy research funding goes kernel functions and parameters to tune this algotowards anticonvulsant medications which are largely rithm     
ineffective for about     of patients  leaving them
maiwald et al  test   non linear prediction methjust as susceptible to spontaneous seizures      with ods  effective correlation dimension  dynamical simirecent developments in the wearable space  there is larity index  and accumulated energy  these   predican increase in the usefulness of wearable devices that tion methods analyze distinct features of eeg data 
 

fi   
   
  
  
  
   
  
   
   

but do not aggregate over multiple features like a machine learning approach would  maiwald et al  prove
that the   proposed methods perform better than
random or periodic methods of classification     

   

our work

   
   
  
 
  
   
  
   
   

   
  
  
   
  
 
  
   
   

   
   
 
   
  
 
   
   
   

   
   
   
   
  
 
 
  
   

   
   
   
 
 
 
  
  
   

   
   
   
  
  
   
  
  
   

    
   
   
 
 
  
 
  
   

    
   
   
  
  
   
 
  
   

   
   
   
   
   
   
   
   
   

with the current state of the field as a baseline  we note that for each patient  there is no guarantee that
will try different feature extraction and classification the eeg sensors were placed in the exact same locaalgorithms with the goal of developing a classification tion on the patients brains 
model with low variance and bias  given roughly    
gigabytes of data provided through the kaggle com  methodology
petition  we seek to develop a high performing algorithm for classifying eeg signal segments as preictal
    feature extraction
or interictal 
the raw eeg data is over     gigabytes  so we use
common eeg feature extraction methods to extract
useful features  the raw amplitude data is not use  dataset
ful for prediction using traditional machine learning
methods  the first challenge in classifying eeg data
the data represents    minute clips of preictal and is to get it into a form in which we can apply tradiinterictal training and test readings for a fixed num  tional machine learning techniques  in the past this
ber of electrodes on   epileptic dogs and   epileptic has been done using one of either discrete wavelet
humans      the following number of preictal  inter  transform  dwt  or short time fourier transform
ictal  and test clips are provided for each 
 stft   because these methods preserve time information while extracting frequency information and
other features from eeg signals  principal component analysis is also commonly used for dimensionality reduction     
     

discrete wavelet transform

dwt is implemented using what are called quadrature mirror filters  qmfs   which separate high and
low frequency components of the signal  there
are many variations of these qmfs  but a common
choice  one that we used  is form   daubechies filters  known as d    when we pass the original signal through these filters we get two output signals
called the approximation coefficients  low frequency 
and detail coefficients  high frequency   we can further decompose the approximation coefficients into
their own approximation and detail coefficients  the
output of which is now    the length of the original
signal  we can repeatedly take the low frequency filter output of this process and further decompose it
to an arbitrary degree  a typical choice is   levels of
decomposition and this is what we have implemented 
therefore  dwt separates the signal into high and
low frequency bands by repeatedly performing separation to arbitrary degree in a branching structure
 figure     allowing adjustment for more course or
granular readings      in figure    h n  represents

figure    number of preictal and interictal training
and test clips
each training example consists of a matrix  a vector  and some metadata  the matrix represents electrodes by time  and has samples of electrode values
for the patient over    minutes  the rows contain the
values of the signal at all time intervals for a given
electrode  the columns contain electrode readings at
a particular time segment  the names of the eeg
electrodes are given in a vector  the time between
interval  and total number of samples are given as
scalars 
an example of the matrix for one training example
is provided below 

 

fithe high pass filter and g n  represents the lowpass hyperplane  the first principal component has the
filter 
largest possible variance  and subsequent components
have the next highest variances with the constraint of
being orthogonal to the preceding components  we
can truncate a list of features by only taking the first
n principal components  these components have the
largest variances  and likely have larger effects on the
classification 
pca improves efficiency of eeg classification because certain channels are placed on areas of the head
that will be more relevant to seizure detection than
others  pca can reduce these noisy dimensions 
however  pca is an unsupervised projection algorithm  so it can optimize out relevant information if
the features are not pre processed in a way that fits
figure    branching structure of dwt
the model  when run on our dwt features  pca
from the dwt coefficients at each level  we took resulted in poor performance 
even for our stft features dimensionality reducas features the mean magnitude  mean power  magnitude squared   and standard deviation  in addition  tion didnt improve our performance with this data
we used as features the ratios of the mean magnitudes set  but the normalization and variance maximization
of pca led to better results  we find the principal
of adjacent levels 
components by maximizing the equation given in figure        
      short time fourier transform
stft is used for the determination of sinusoidal frequency and phase content of local sections of signal 
this feature extraction method was most effective on
the kaggle data  we used the discrete time   stft
as given in figure   
figure    closed form equation for finding the principal component

   
figure    equation for computing stft

classification

we used three standard classification algorithms  knearest neighbors  logistic regression  and support
where x k  denotes a signal and w k  denotes an vector machines  we tuned the parameters for these
l point window function  the stft can be defined algorithms by balancing overfitting and underfitting
as the fourier transform of the product x k w k  m  and maximizing the area under curve  auc  score 
    
in order to extract features from the eeg data  we
performed stft twice on each    minute segment       k nearest neighbor
 the second time with a time offset  to obtain frequency spectra for each    second segment and then as a baseline  we used k nn to classify nodes based
for each    second segment  from this  we extracted on their k closest neighbors in the feature space  to
features by computing mean power of the magnitude calculate the distance  we used an l  norm  and we
looked at k      nearest neighbors  weighting preat each frequency within  hz bands up to    hz 
ictal segments by a factor of      k and the preictal
segment weight are hyperparameters tuned to yield
      principal component analysis
the best result  out of the classification algorithms
pca is used to reduce the dimensionality of features we used  this performed the worst  most likely due to
by converting a set of possibly correlated variables the radically fewer number of preictal training examinto a set of linearly uncorrelated variables  it is ples than interictal examples  however  it provided a
an unsupervised projection that can project a high starting point for classification that brought us above
dimensional feature space onto a low dimensional the random guessing threshold 
 

fi     

 

logistic regression

logistic regression  lr  has been successfully used
for eeg classification in literature  it is a flexible
model that makes few assumptions on the prior structure of the data  in our implementation  lr is the
best performing model 
lr calculates probability of positive result based
on features input into the logistic function  our hypothesis function is described as follows      

results

correctness scores are generated by calculating the
area under the receiving operator characteristic
 roc  curve  which takes into account both accuracy and sensitivity 

figure    correctness scores of each feature extraction method with each classification model

figure    our hypothesis and the underlying logistic
function
we then used stochastic gradient descent to optimize the parameters   we trained one model on
all    second samples and another on all    second
samples  assigning double the weight for preictal seg  figure    correctness scores of the best feature
ments  then  for each test segment  we summed extraction classification model pairs
the output for all    second and    second samples 
classification was done by comparing this sum to a
threshold  which was another hyperparameter 
     

support vector machine

support vector machines  svms  are models which
assume a  mostly  linearly separable data set  this
set can consist of features mapped to a high difigure    our performance on kaggle
mensional space in which they are linearly separable  by creating a margin which maximizes the functional and geometric margins between sets  svms
discussion
have received attention in biomedical applications   
we found that most kernel functions overfit our data
using svm  the linear kernel performed the best and short time fourier transform  stft  feature extracprovided an improvement over k nn  but still overfit tion  principal component analysis  pca  dimensionality reduction and normalization  and logistic regresthe data to some extent 
sion  lr  model prediction provided the most accurate result 
stft provided robust  domain specific feature extraction because they provided frequency and phase
information on every window of each    minute eeg
clip  taking the average of frequencies from     and
   second window sizes provided an even more robust classification feature  the features were domainfigure    svm objective function to calculate opti  specific because stft is a method specifically utimal margin with error penalty
lized for time series signal data 
 

fipca combined with stft provided better results
than stft alone because of pcas normalization
step  reducing dimension of the raw signals  performing pca without stft  reduced classification
accuracy to below the baseline because it removed
key latent elements of the data 
dwt  like pca  removed key latent elements of
the eeg data and thus did not perform as well as
stft 
we were surprised that lr performed better than
svm  but concluded it was because logistic regression made the fewest prior assumptions on the data
set  in particular  logistic regression classifies with a
probability based on a data points distance from the
regression line  whereas svms classify strictly and fail
regularly when there is too much noise in the training data  to optimize svm performance and prevent
overfitting  we would like to tune our objective function to support more variance 
knn was an effective baseline algorithm  but did
not provide a nuanced enough definition of distance
between data points to accurately classify test data 
our classification results performed better than
about     of competitors on kaggle and would have
been good enough to place us   th out of     entrants 

    nine health wearables for your head       
august     mobihealthnews  online   available 
http   mobihealthnews com       ninehealth wearables for your head 
    american epilepsy society seizure prediction
challenge        august      kaggle  online  
available 
http   www kaggle com c seizureprediction
    mormann  florian  ralph g  andrzejak  et al  
seizure prediction  the long and winding road 
in brain        pp          
    binder  devin k  and sheryl r  haut  toward
new paradigms of seizure detection  epilepsy  
behavior           pp          
    subasi  abdulhamit and m  ismail gursoy  eeg
signal classification using pca  ica  lda and
support vector machines  in expert systems
with applications        pp            
    t  maiwald  m  winterhalder  r  aschenbrennerscheibe  h u  voss  a  schulze bonhage  j  timmer  comparison of three nonlinear seizure prediction methods by means of the seizure prediction characteristic  physica d           
       

    paul  manoranjan and mohammed zavid parvez 
eeg signal classification using frequency band
analysis towards epileptic seizure prediction  in
first  we would like to improve our svm model to
ieee international conference on computer and
perform better than logistic regression  the literainformation technology  khulna  bangladesh 
ture we have read shows that svms perform opti     
mally  and we believe the kaggle data set should not
be an outlier 
    suleiman  raouf abdul bary and toka abdulin future work  we would like to attempt more comhameed fatehi  features extraction techniques
plex classification algorithms such as neural networks
of eeg signal for bci applications  faculty of
and random forest walks  we attempted to apply
computer and information engineering departneural networks for this project  however we experiment college of electronics engineering  univerenced severe over fitting and a classification of nonsity of mosul  iraq       
seizure for virtually all data points  we would like to
tune neural networks and run them on faster proces       ng  andrew  lecture notes       
sors to evaluate their potential with this data set 
the random forest model is a recently discovered 
increasingly popular mode of classification and we
would like to evaluate its performance in binary classification of eeg data 

 

future work

references
    epilepsy facts        december     citizens
united for research in epilepsy  online   available  http   www cureepilepsy org 
 

fi