cs    fall      project final writeup
members 
title 

jacob conrad trinidad  j nidad   ian torres  itorres 
tradeshift text classification

members
members of this project are jacob conrad trinidad  sunet id  j nidad  and ian torres  sunet id  itorres  

introduction
our problem originates from the kaggle tradeshift text classification challenge  we aim to predict the
probability that a piece of text belongs in a given class  given a feature vector that describes a text block 
we must output a vector of predicted probabilities  where the ith element corresponds to the probability that
the ith label applies to that text block  the specific meanings of the features and labels are unknown 

dataset
the data is provided by kaggle  found here  http   www kaggle com c tradeshift text classification 
data  and is publicly available  the training set is given in train csv  which contains           rows and    
columns  each row corresponds to a text block  and each column corresponds to a feature  which can either
be boolean  numeric  or a hash value  the file trainlabels csv contains           rows and    columns  each
row of trainlabels csv corresponds to a text block as in train csv  and each column corresponds to a label 
if a text block belongs to a given label  it will have a   in the corresponding column  and   if not 
for instance  the beginning of an example train csv and trainlabels csv would look like 

train csv
id  x   x   x   x      
   m   i  y     no                
   j gheu      yes                 

trainlabels csv
id y  y  y  y     
                 
                 

the test set  given in test csv  follows the same format as train csv  the format of the desired output
is similar to that of trainlabels csv  for each text block  we are to write a list of probabilities that the
text block belongs to each label  for instance  given a test csv that looks like the train csv above  we would
output 
output csv
id label  pred
  y       
  y       
   
  yk      
  y       

features and pre processing
provided in the data is a set of     features to describe each piece of text  what the features specifically
represent is unknown  but it is given that the features include relevant data such as content  parsing  spacial 

 

fiand relational information about the text  these features come in the form of cryptographic hashes  continuous and discrete numerical values  and booleans  through our analysis  we found there to be    booleans 
   floating point numbers     integers  and    hashes among the features  some preprocessing was necessary
to separate this data into the three categories of boolean  numeric  and hash values so that certain models
could use them appropriately 

models
we split the train csv data set into a training set and a developer test set  with the first     of the data
being the training set and the latter     of the data being the developer test set  this training and dev
test set were used to test each model and determine which one would perform best  then  upon discovering
the best model  we retrained that model on the entirety of train csv and then write the models predictions
on test csv into an output csv file 
we used four types of models in an attempt to solve the problem  in each model  we assumed that the
labels are mutually independent 

majority algorithm
yj  

nt
  p
yij
nt i  

nt    the number of training samples
yij    the j th label of the ith sample 
we used a slight variation of the majority algorithm where the predicted probability for a given label yj
is the percentage of training samples with label yj   this model allowed us to establish a baseline score for
our tests  as it did not include any information on the features 

linear regression
yij   jt xi
j     x t x   x t yj
x    matrix of samples x features
xi    feature vector of sample i
yj    vector of label j  where each element corresponds to the j th label of each sample
this model calculated linear regression using normal equations  due to the nature of linear regression 
we only used the numerical features 

naive bayes with laplace smoothing
n
qt

yij  

p xi  yj     p yj     

i  
n
qt

p xi  yj     p yj       

i  

n
qt

p xi  yj     p yj     

i  

nt    the number of training samples
xi is the event that the ith feature is present in the piece of text 
yj is the event that the j th label is assigned 

 

fidue to the nature of naive bayes  boolean features were the only features included in the implementation
of this model 
bag of words
we attempted to improve our implementation of naive bayes by adding hash features via the usage of a
bag of words model with binary weighting  each unique hash that appeared at least n times in the training
and test data became a boolean feature  indicated by whether or not the hash appears in a sample   the
optimal value of n turned out to be n         

logistic regression
newtons method
yij   h  x i   
h  x i    is the sigmoid function of t x i 
 is updated by   h       
h    hessian of the log likelihood function
        gradient of the log likelihood function
this model initially only used newtons method to update   our implementation ran until convergence 
due to the nature of logistic regression  we initially only used numerical values  we attempted to use this
model due to our observations as to how well it has worked in the past 
online learning
the main equation is the same  however  how  is updated changes 
j  j   h  x i     yij  
    a constant
this model varies from the previous version of logistic regression by using online learning  stochastic
gradient decent  and feature hashing  online learning enabled us to use less memory and causes the algorithm
to update the weights via stochastic gradient decent as each new sample provides more data  feature hashing
enabled us to use all of the features at once regardless of their original type by simply rehashing all of the
features into buckets  enabling the algorithm to find more patterns in the data 
adaptive learning rate
the main equation is the same as above  however  how  is updated changes   now changes inversely
proportional to how much j has changed  an adaptive learning rate was used to speed up convergence to
an optimal  

results
the training set was of a size           samples and the test set was of a size         samples  to evaluate
our model  we will be using the metric of the negative logarithm of the likelihood function averaged over nt
test samples and k labels  the equation is 
nt p
k
  p
 yij log yij        yij  log    yij   where yij is the expected and yij is the predicted
logloss  
nt k i   j  
value of the j th label on the ith sample  this metric is used because its symmetric in the fact that predicting
   for a false sample     has the same penalty as predicting    for a true sample      in addition  if one were
to make a completely incorrect prediction  such that in the worst case one predicts   when the value is    it
adds the significant penalty of infinity with the value of log     thus  the goal is to get our logloss as close
to   as possible by getting the margin of error for each guess to be as small as possible 

 

fimodel
majority algorithm
linear regression
naive bayes w  laplace smoothing
naive bayes w  laplace smoothing
and bag of words
logistic regression w  newtons method
logistic regression w  online learning
logistic regression w  online learning
and adaptive learning rates

training set score
      
      
      

dev test set score
      
      
      

      

      

      
      

      
      

      

      

we discovered the best model was logistic regression using online learning  we then attempted to
optimize this model by finding the optimal initial learning rate  using hold out cross validation  here 
we split the training set     into another training set and the remaining     into a cross validation set 
we then trained various values of                              and then                                     on the
training set and then found their score on the cross validation set  an  value of     yielded a score of
         the lowest one across the various values tested for  we then attempted to optimize the model again
by finding the optimal number of passes through the training data  we trained models based on         
and up to   passes through the data and then found their score on the cross validation set  the model that
yielded the best score was   online passes  as any more passes and the model appears to suffer from bias and
overfit the data 
after optimizing the hyperparameters  we tested the model on kaggle  we retrained the model on the
entire train csv file and then tested it on test csv  after submitting the output to kaggle  we achieved a test
score of           

discussion
the majority algorithm established a baseline score due to its simplistic nature  all other algorithms we implemented yielded better scores than this baseline  confirming our various improvements in our other models 
it was expected for linear regression to perform only slightly better than our baseline  due to the likely
non linearly separable features  as a result  we decided to forgo this model and attempt a new model 
naive bayes with laplace smoothing demonstrated little change between its original implementation and
its usage of the bag of words model  this is because the optimal minimum frequency for a hash to become
a feature only resulted in the addition of   boolean features  which may indicate that the hashes are less
correlated with the labels than the other features  changing the minimum frequency to result in the addition
of more features improved the score even less  this is likely a result of the marginal gain in information due
to the greater chance that a sample would have this feature on 
heavily modifying the log regression model drastically improved the score to below       this is likely
due to including all of the features as data points as well as improving the rate at which the weights are
updated 
we analyzed the performance of our model by finding its precision and recall values  since the predictions
are probabilities  we can round our predictions to either   or    what we discover is that our algorithm has
a precision of        and a recall of         this demonstrates that this model has both high recall and
precision 

 

ficonclusions
our final online learning algorithm achieved a score corresponding to the   th percentile on the kaggle
leaderboard  and beat tradeshifts benchmark of        as the top teams on the leaderboard used external
tools and probably had access to more computing time  this was better than we expected 
the motivation for focusing on optimizing an online learning model was partly practical  as the low
memory requirements enabled us to test our algorithm on our own machines  which was significantly faster
than using stanfords corn machines  for example  on the corn machines  running our initial logistic regression
model using newtons method on just        of the           training examples took several hours 

future
our final online learning model performed significantly better on the training set than the test set  thus 
a possible improvement of our online learning model would be to apply backward search or filter feature
selection to reduce bias  additionally  to improve the performance of our naive bayes with bag of words
model  we could use tf idf instead of binary weighting  which might better capture how important a hash
is to a text block  to further improve our naive bayes model  we could discretize the numerical features
so they could be included as well  finally  besides improving the models we used  we could also attempt to
implement a model using svms or neural networks 

references
tradeshift text classification  kaggle   online  october      
https   www kaggle com c tradeshift text classification   accessed   december        tradeshift
text classification forums  kaggle   online  october      
http   www kaggle com c tradeshift text classificationforums t          accessed   december
       a  ng  supervised learning  discriminative algorithms  stanford   online  september      
http   cs    stanford edu notes cs    notes  pdf   accessed   december        a  ng  generative
algorithms  stanford   online  september      
http   cs    stanford edu notes cs    notes  pdf   accessed   december       

 

fi