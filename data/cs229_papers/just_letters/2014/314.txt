cs     final project

 

classifier comparisons on credit approval
prediction
zhoutong fu  zhedi liu

i  i ntroduction
inspired by the paper of simplifying decision trees by j r 
quinlan and the book c     programs for machine learning
by j r  quinlan and morgan kaufmann where they test the
very traditional pruned decision tree models on credit approval
data set  we want to re exam the data set used in simplifying
decision trees and build advanced models to increase the
accuracy  no surprise that all of the models we built beat
the bench mark         the best result from quinlans paper  
among which svm with polynomial kernel and random forest
achieve the best result         

c  data visualization
it would be extremely useful and could bring in more
insights about modeling if data could be visualized before
applying any further operations  fig    shows the scatter plot
of numerical variables from the data set where no particular
patterns could be observed between any pairs which suggests
no apparent correlations and hence we need further investigations on the data set 

ii  data e xploration
a  data format
data concerns with credit card applications and is collected
from http   archive ics uci edu ml datasets credit approval 
there are     observations     of which have missing values
  with    features  among which   are real valued and the
rest are categorical  and   class attributes  for confidential
reasons  all attribute names and  categorical  values have
been transformed to meaningless symbols 

b  data imputation
several methods have been applied for data imputation 
   deletion  delete any observations with missing values 
   simple imputation  impute missing value of numerical
variable with its mean and missing value of categorical
variable with any of its categories 
   random regression imputation  run a multiple regression
of the missing variable using the remaining variables as
predictors 
   iterative multiple imputation  keep running random
regression imputations on all missing variables until their
values converge 
conclusion  although advanced methods such as random
regression and iterative multiple imputation usually work
better in most cases than simple imputation methods  in this
case however  we havent observed any significant differences
between them when building models to get estimated error 
hence  deletions are used for observations with any missing
values as to reduce bias 

fig     scatter plot of numerical variables

moreover  principal component analysis  pca  is applied
to detect the main directions of data variance  before
performing pca  we first transform all categorical variables
into indicator variables      valued  because pca could only
work on numerical inputs  fig    is the biplot of first and
second principle components and their associations with each
variable and data point  upper axis represents the loading
scores of input variable while lower axis shows the score of
data points on the first two components 
from pca no dominating directions could be used
to explain the variance because the first three principle
components have merely explained about    percent of total

fics     final project

variance and in the biplot the directions of variables vary
quite a lot and have very little information in common 
opposite variable directions are also observed which may
suggest that some noises exist in the data set 

 

vector machines where kernels have been applied to
expand feature space  here we have tried linear kernel 
polynomial kernel  radial kernel and sigmoid kernel 
linear kernel has the form
p
x
k xi   xi     
xij xi  j
j  

which is similar to linear models above and hence we
expect their performance to be similar  however  svm
with linear kernel usually beat linear models by faster
convergence which is extremely useful when the data
size is relatively small  polynomial kernel
k xi   xi          

p
x

xij xi  j  d

j  

provides a much more flexible decision boundary by
introducing polynomials with an intercept  radial kernel
has the form
p
x
k xi   xi      exp 
 xij xi  j     
j  

which focus on local behavior of data and only nearby
training observations could have effect on modeling  at
last we use sigmoid kernel which has the form 
k xi   xi      tanh 
fig     biplot of principal component analysis

iii  f eatures and m odels
a  features extraction and selection
linear dependency and multi collinearity are detected
among original features via variance inflation factors  vif  
for linear models we delete highly correlated variables and
combine several related variables into one compounded
variable  we have tried to add top principle components
to expand the feature space but it turns out to have no
improvement on the model accuracy  in support vector
machines  features are further expanded via linear kernel 
polynomial kernel  radial kernel and sigmoid kernel 

b  modeling
a good variety of models are tested in this paper and
for detailed results please refer to table i where error 
denote training classification error and error  denote test
classification error 
 i  linear models
we start with logistic regression and linear discriminant
analysis  after applying the above feature selection
methods  both of them give surprisingly good results 
 ii  support vector machine
a very natural expansion from linear models is support

p
x

xij xi  j   c 

j  

 iii  tree models
besides linear  nonlinear and local models  tree models
are also tested here to provide a very different view of
data splitting and modeling 
random forest is intended to improve traditional
decision tree models by introducing boostrap
aggregation and then further improve it by decorrelating
the trees  a key parameter is the number of predictors
to be chosen as split candidates and we will calibrate
this parameter by creating validation set 
another widely used tree related model is boosting
classification tree where trees are grown sequentially
and each new tree is based on information of previously
grown trees  the key parameter here is the number of
trees to grow and we will apply validation and cross
validation to select optimal parameter value 
 iv  neural networks
deep learning methods usually work well on data with
hidden complexities and here we tried neural networks
with only one hidden layer 
iv  r esults
after data imputation  we collect clean data of    
observations which is split as           in training set 
          in test set  to avoid overfitting  we calibrated
parameters for each model in the training set and test them

fics     final project

 

in the test set only once  table i below represents the overall
modeling result and note that error  and error  represents
training and test classification error  respectively  error  of
boosting classification is not applicable because bernoulli
deviance is used to select best model  instead of classification
error 
among these methods  both support vector machine with
polynomial kernel and random forest give the best results
which is much lower than        the best result from j r 
quinlan  
table i
r esult table
models
logistic regression
linear discriminant analysis
svm  linear kernel 
svm  polynomial kernel 
svm  radial kernel 
svm  sigmoid kernel 
random forest
boosting classification tree
neural networks

error 
      
      
      
      
      
      
      
not applicable
      

error 
      
      
      
      
      
      
      
      
      

fig     random forest  optimal number of feature selection

since the linear models and support vector machines have
been discussed in detail during class  here we focus on results
from random forest and boosting 

a  random forest
as mentioned in the modeling part  the number of
predictors  features  is the key factor in building random
forest tree  fig    illustrates that random forest model chooses
   parameter to build a classification tree in each bootstrap
because it yields lowest estimated error  fig    represents
variable importance from the optimal random forest model
and v  has exceptionally higher importance than others 

fig     random forest  variable importance plot

b  boosting classification tree
fig    and fig    show two different ways to select optimal
number of trees in boosting based on bernoulli deviance 
in both figures  the black curve represents the training error
while the other colored curve represents validate cv error and
the blue vertical broken line tells the position of optimal value 
v  d iscussion
from the results table we notice that all models beat the
bench mark  yay    this is exactly what we have expected
because the bench mark is resulted from pruned decision
tree which usually has high variance  linear models  logistic
regression  lda  svm with linear kernel  work pretty well
because we have relative large p m rate and hence it will
dramatically reduce variance  similarly  random forest reduce
its variance by bootstrap and aggregation  however  there are

fig     boosting  optimal number of trees via cross validation error

several issues we have to pay special attention to 
 i  in modeling with boosting classification trees we notice
that turning the key parameter is crucial for predicting
accuracy  however  the parameter calibrated from cross

fics     final project

 

fig     boosting  optimal number of trees via test error

validation error shows much better performance than
the one from validation test error  we could also notice
that in fig    the error curve from cross validation is
very smooth and the optimal size it chooses is relatively
moderate while in fig    the error curve oscillates so
often that it provides much weaker evidence that the
resulting parameter is optimal 
 ii  deep learning methods  boosting  neural network 
has no apparent advantages over other methods  one
problem with boosting classification tree is that boosting
model is easily overfitted and hence gives false good
training error which leads to poor test error  the
moderate performance of neural networks may also
suggest that it is very difficult to discover complicated
hidden or latent variables associated with the response 
 iii  from the result table we notice that the training error
is always higher than test error  we believe this might
be caused by noise in our training data and apply some
other data splittings may give different training error 
vi  f uture
after exploring the data with methods above  we still
believe that the result could be further improved by exploring
more on following aspects 
 i  try boostrap on the original data to get larger data size
and perform non parametric methods such as k nearest
neighbors and local regression
 ii  perform clustering on observations and classify them
within the cluster to improve accuracy
 iii  consider other metrics such as auc or better defined
score function
vii  r eference
r eferences
    quinlan  simplifying decision trees  int j man machine studies     pp 
         dec      

    quinlan  uci machine learning repository http   archive ics uci edu ml 
datasets credit approval
    greg ridgeway with contributions from others         gbm  generalized
boosted regression models  r package version     
    a  liaw and m  wiener         classification and regression by
randomforest  r news            
    david meyer  evgenia dimitriadou  kurt hornik  andreas weingessel
and friedrich leisch         e      misc functions of the department
of statistics  e       tu wien  r package version        http   cran 
r project org package e    
    venables  w  n    ripley  b  d         modern applied statistics with
s  fourth edition  springer  new york  isbn              
    angelo canty and brian ripley         boot  bootstrap r  s plus 
functions  r package version        
    davison  a  c    hinkley  d  v         bootstrap methods and their
applications  cambridge university press  cambridge  isbn             

fi