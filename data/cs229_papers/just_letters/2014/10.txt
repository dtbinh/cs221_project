estimation of word representations using recurrent neural
networks and its application in generating business fingerprints

kuan fang
abstract  word vectors have been used as a useful
feature in many nlp related tasks  in this project 
we proposed a modified recurrent neural network algorithm to learn word representations from corpus in
a state transition manner  our state transition rnn
could reach the results of state of the art word embedding algorithms in terms of comparing euclidean distance  with smaller hidden size  the state transition
rnn needs shorter time for training  as a testbench
for our model  we trained our model on yelp review
corpus and implemented three different applications
 business similarity  business search and business recommendation   experimental results revealed that
our state transition rnn and our business fingerprints
generation algorithms achieved expected results 

   introduction
word vectors have been an important and basic feature for a variety of natural language processing tasks 
a reasonable set of word vectors should represent the
similarity of words by computing the distance  e g 
cosine distance and frobenius norm  of two different
word vectors  estimation algorithms of word representations such as skip gram and glove have been implemented in recent years  the skip gram is an efficient
linear training model considering a small part of context words  and glove is a log bilinear model with
a weighted least squares objective considering global
context  these existing models has shown good performance on word analogy tasks  but they have deficiencies such as considering only local context words
or considering only the word word co occurrence in the
text 
in this project  we aim to propose unsupervised models
that can learn deeper relationships by using global context words in sentences and paragraphs  we will begin by implementing a modified recurrent neural network  rnn  language model as our baseline algorithm 
recurrent neural network has shown excellent performance in the language modeling task by taking the
previous context words as input and predicting the target word  since the word representation of the target

kuanfang stanford edu
word should be learned by context words before and
after it  we attempt to implement a state transition
rnn to extract global contexts and even the semantics from the sentence 
with learned word vectors  we represent the each business fingerprint as a cluster of word vectors extracted
from the reviews  and three implemented applications
proved that these business fingerprints are useful in
recommendation systems 

   prediction and metrics
     learning word vectors
there is no direct way to evaluate the performance
of trained word vectors  before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word analogy problems we take the perplexity  ppl  of the language modeling as a metric of
our word vectors trained on state transition rnn 
     generating business fingerprints
although a high performance of applications  e g 
business category prediction  is not a direct metric of
fingerprint quality  a good fingerprint will be necessarily good to be used for the classification task  based on
this argument  we can formulate an optimization goal
based on combinations of specific tasks  for example  one evaluation metric is to loop through all pairs
of businesses and compute their distances  if the two
businesses have many categories in common  we hope
their distance is small  on the other hand if they do
not have category in common  we want their distance
to be large 
min


x

 categorysim i  j         distance i  j     

 i j 

   dataset
we trained our data on penn tree bank data set and
yelp reviews corpus  the penn tree bank data set is a
common dataset in natural language processing  and

fics    fall      project final writeup

the yelp reviews corpus is cleaned by ourselves from
the yelp data challenge data set 

   model
     word representation using skip gram
model
skip gram model is a distributed word representation
learning algorithm proposed by mikolov et al in      
skip gram learn the word representation vectors based
on a two level neural network  taking the dot product
of the word vector as input to predict the occurrence
of words in a context window  usually       context
words   in the training  skip gram maximize the log
probability of context words 
t
 x
t t  

x

log p wt j  wt  

   

cjc j   

the basic skip gram formulation defines p wt j  wt  
using the softmax function 

figure    state transition reccurent neural network

 
vwi  
exp vwo
p wt j  wt     pw
 
exp v
w vwi  
w  

the semantics of the sentence as the following words
are read in one by one 

   

we modified googles word vec toolkit implemented
by mikolov et al  mikolov et al       a  and trained
skip gram on general text corpus and yelp reviews
corpus respectively  in terms of plugging the resulting
word vectors into our recommendation model  we used
cosine distance and frobenius norm to compute the
word distance respectively 
     word representation using
state transition recurrent neural
network
we also proposed a recurrent neural network  rnn 
based model to learn word representation  one of the
disavantages of the word word co occurrence model is
that they can not learn the word vector based on the
semantics of the sentence  for example  good and
bad has a close cosine distance after trained by skipgram  this is because these two words occurred in
many similar contexts in the training data  thus we
turned to a modified rnn algorithm  like existing
rnn  we learn word vectors in an unsupervised manner taking the ppl of the language modeling as the
objective  in stead of taking the one hot word code as
input  we model the semantics of the sentence using a
dense state vector  each entry of the state vector represents a feature of the semantics evolved with words 
while each word maintains a transition matrix transfrom the input state into the output state  evolving

s                     
 t 

s

  f  w

 t 

 t  

s

 t 

 b  

   
   

to train this rnn unsupervisely  a fixed matrix feed
forwards the output state layer into the prediction
layer  where the probability of the next word is represented by a one hot vector 
y  t    f  v  s t   

   

the backpropagation steps of the model is as follows 
 t  
u  t     u  t    e t 
o s

w  t     w  t   

 t 
eh

 s t   

   
   

where we plug following error expressions in 
 t 
e t 
 y  t 
o  d
 t 
eh

  dh  e t  
v  t 
o

dhj  x  t   

 t 
xsj   



 t 
sj  

   
    
    

we expect this algorithm could extract deeper relationship between words  thus generating more reasonable word vectors as long with lower ppl for the language modeling objective  we implemented our code
based on microsoft researchs rnn language modeling toolkit and modified almost all functions of it
according to our needs 

fics    fall      project final writeup

   results
     unsupervised training of word vectors
we first test our model on penn tree bank dataset 
which is a hall mark in the natrual language processing field  we compare our results on state transition
rnn with the state of art rnn language modeling
implemented by microsoft research  then we also
trained our state transition rnn on the yelp reviews
dataset  cleaned by ourselves   since microsofts rnn
has much more parameters than we used  it could take
much more time to converge 
for st rnn  we compared the ppl performance of
different state sizes from   to    on penn tree bank
dataset  to speed up the training and prediction 
we followed mikolovs approach to categorize output
words according to their frequencies in the training
corpus  the results are shown in figure     we found
that for penn tree bank dataset  the optimal state
size is around     for larger dataset like yelp data
challenge  the optimal state size is around    

model
rnn
rnn
st rnn
st rnn
st rnn
st rnn  y 

size
   
   
  
  
  
  

words sec
     
    
     
     
     
     

iters
 
 
 
 
 
 

ppl
      
      
      
      
      
      

our state transition rnn achieves reasonable performance  we will add bptt later and test our model
on larger dataset 
     similar word vectors
we give some word examples here to compare the performance of two word embedding algorithms  both
examples are trained on the reviews data of yelp data
challenge which are cleaned by ourselves  the first
row is the input word  and the rest are the closest
words determined by cosine distance 
table    training state transition rnn on reviews

duck
chicken
burrito
tofu
salmon
octopus

burger
pizza
sandwich
burrito
smoothie
pho

noodle
pasta
tofu
vegetable
seafood
ramen

good
great
wonderful
decent
bad
fantastic

table    training skip gram on reviews

duck
pork
squab
chicken
confit
beef

burger
hamburger
cheeseburger
burgers
burgeri
buger

noodle
noodles
tofu
noodlesi
wonton
vermecelli

good
decent
great
solid
tasty
alright

table    training skip gram on general corpus

figure    ppl performance of different state sizes 

the size column below indicates the hidden size for
rnn and state size for st rnn  training statetransition rnn took    hours on a research server
in stanford cs department to converge  and the last
row is st rnn trained on yelp reviews 

table    training skip gram on general corpus

duck
fethry
daffy
amuck
eider
coot

burger
mcdonald
kfc
mcdonalds
schalk
restaurant

noodle
udon
toppings
steamed
ramen
kuroda

good
bad
natured
luck
virtuous
honest

as we can see from these examples  training word
vecters on domain specific data  yelp reviews in our
case  has much better performance  our word embedding algorithm is doing a good job  but it still cannot
achieve the ideal result we want  it is still assigning
antoynms closest vectors  we aspire to improve this
issue in our following work 

fics    fall      project final writeup

     business similarity
we tested our current model on a small data set of
   categories each of which has     businesses  thus
     businesses in total  note that for our evaluation
method  smaller the costs are better our signatures
are  we can see that use more feature words achieves
lower cost  i e  better signatures in terms of category
consistency 
the ground truth is the labeling from the yelp data 
we count the number of co existing words in the two
restaurants as a measure of ground truth similarity 
then computed the similarity based of business signature  we used both euclidean distance and cosine
distance to measure it  we tried different length of
feature words to concatenate together to form a signature  as shown by the below plot  the larger the
number of words is chosen the more accurate the performance 
note that the smaller the euclidean distance means
the larger cosine distance  in both cases business fingerprint evaluation vector size      using word vec
domain specific training method gives the best performance 

     business search
another application of building up business fingerprint
is for semantic search  traditional search is based on
the appearance of a word  then building a inverted
index  that is bad because if a word doesnt appear in
a document but some similar word appears  it wont be
captured  the word vector solved this problem in that
it can find all the words that are close to the search
word  making the search result more meaningful  its
no longer a word by word search  but semantic search 
heres a list of results returned by the search engine 
for example  in the query morning light food  apparently the query wants to find a place to have breakfast 
although the query did not explicitly say breakfast 
but morning and lightfood expressed this notion 
by comparing the business signature the search engine found the beach house lounge which is exactly
a place serving breakfast  and after digging into the
reason why this restaurant get returned  it shows the
word breakfast has the closest distance        to the
query  so it gets returned 
another example is the query beer nightlife clubbing  the query intends to find a place to have some
beer and enjoy the nightlife by clubbing  the business signature captures this underlying semantic and

fics    fall      project final writeup

returned the tip top tavern bar  which is exactly a
night bar 
query
morning light food
beer nightlife clubbing
fastfood
healthy vegetarian

result
beach house lounge
tip top tavern bar
cheeseburger in paradise
montys blue plate diner

     business recommendation
the last application we built on top of the business signature is business recommendation  which is an item
similarity based recommendation  the user specifies
a restaurant he likes  then the system will return a
restaurant thats similar to the specified restaurant 
basically the recommendation system is calculating
the euclidean distance between he business signatures
and return the one with the smallest distance 
for example  the recommendation for uno pizzeria
and grill is benvenutos italian grill  because they
are both selling grills  another example is the recommendation for flaming wok is gourmet house of
hong kong  because they are both chinese restaurants 

testbed  with our word vectors  we could classify and
search businesses in these applications as we expected 

   future work
the back propagation through time  bptt  algorithm did not work well for our st rnn  we assumed
that this kind of neural networks with self generated
parameter matrices need other optimization algorithm
to improve the performance  for example  we may
need to use second order methods  e g  hessian free
optimization   in some related works  factorizing the
parameter matrices to a product of several matrices
might also help  we expected to explore these algorithms in our next step 

acknowledgement
some parts of this project also serves in my cs   
project  learning business fingerprints from texts 
in my cs    project  we have another two team members  han song and charles qi  working together  the
word representation part was all done by myself  i also
worked on the design of the three applications of generating business fingerprints 

this business signature based recommendation system
captures the intrinsic characteristics of a restaurant
and provides a fair way to compare the similarity based
on the vector distance  overall the system provides a
quantitative way to compare the businesses 

references

   discussion

mikolov  tomas  chen  kai  corrado  greg  and dean 
jeffrey  efficient estimation of word representations
in vector space  arxiv preprint arxiv           
    a 

the ppl of st rnns approaches the ppl of traditional rnn  but st rnns were trained at much faster
speed  in the three business recommendation applications  st rnns have similar performance with the
skip gram when comparing euclidean distance  while
its performance is worse when comparing cosine distance  an explanation is that the skip gram algorithm
contains an inner product in its model  so the algorithm is based on consine distance 

   conclusion
we designed state transition rnn for word embedding  comparing with traditional rnns and the
word vec toolkit  our algorithm could approach the
state of the art results of word embedding and language modelling  with smaller state size  our model
could be trained much faster than traditional rnns 
with the learned word vectors  we implemented three
applications of business fingerprints generation as a

mikolov  tomas  karafiat  martin  burget  lukas 
cernocky  jan  and khudanpur  sanjeev  recurrent
neural network based language model  in interspeech  pp                 

mikolov  tomas  sutskever  ilya  chen  kai  corrado 
greg s  and dean  jeff  distributed representations of words and phrases and their compositionality  in advances in neural information processing
systems  pp                b 
sutskever  ilya  martens  james  and hinton  geoffrey e  generating text with recurrent neural networks  in proceedings of the   th international conference on machine learning  icml      pp      
           
taylor  graham w  hinton  geoffrey e  and roweis 
sam t  two distributed state models for generating high dimensional time series  the journal of
machine learning research                    

fi