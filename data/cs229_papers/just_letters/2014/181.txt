modeling protein interations
using bayesian networks
sabeek pradhan  shayne longpre  and varun vijay

deember         

  introdution
all  aute lymphoblasti leukemia  is a ommon form of aner in hildren  about     of hildhood
all ases involve a geneti mutation known as crlf  transloation  in whih the crlf  gene is found
in the  wrong hromosome  this transloation severely worsens the patient s prognosis  moorman et  al   
but researhers do not know why this is the ase  understanding why the crlf  transloation worsens a
patient s prognosis ould aid the development of new treatments for all patients with this transloation 
in our projet  we sought to use reent advanes in the analysis of disease features at the moleular level  to
reveal the variation of disease features as a funtion of dierenes in the level and ativity of moleules  in
our ase  of proteins   this work has been failitated by the development of new experimental tehniques
suh as mass cylometry  bendall et al    the method did not yield informative results when used on ells in
basal state  irish et  al    indiating the need to  potentiate signalling to initiate ell ativity  the dataset
we have obtained uses several drugs  whih stimulate or inhibit ertain proteins  and may provide better
insights when analyzed 

  dataset
our data set onsisted of ell samples from    hildren with all  half of whom exhibit crlf  transloation 
for eah patient        dierent samples of ells were drawn  and a ombination of drugs was applied to eah
sample  for eah sample  our data matrix had ells as rows  proteins as olumns  and eah ell represented
the protein level 
this data was provided to us by dr  karen sahs at stanford s center for clinial sienes researh  ccsr  

  features and output
we deided to model the data using bayesian networks in order to expose the struture of protein interations 
the nodes would be proteins  along with added nodes to represent patients  drugs and the transloation
fator  the added nodes were restrited to be roots sine they ould not be inuened by proteins 

  methodology
   

pre proessing the data

initially  the data was log normally distributed  with a large positive skew  this presented hallenges for
our model learning and soring algorithms  so we normalized the data to make it approximate a gaussian

 

fidistribution  however  at the suggestion of dr  sahs  we did not simply take the logarithm of the data  lest
we run into problems as values approahed zero  instead  we applied the following transformation 

x    asinh

x
 

this allowed us to normalize the data without running into issues with near   values 

   

disretization

the majority of our optimizations foused on dierent ways to disretize the data  any algorithm that ran
on ontinuous data would require some type of regression to determine whih other nodes had a signiant
impat on the node in question and should thus have edges  however  regression analysis is highly sensitive
to feature seletion  sine the mehanisms for the interations between these proteins has yet to be explored
in depth  any attempts at feature seletion would be somewhat arbitrary  further  there is no guarantee that
the mehanisms of interation are even of a linear form  if no linear regression ould adequately desribe the
relationships then learning a network on ontinuous data would be an exerise in futility  lastly  dr  sahs
suggested that suh biologial data an often be modeled as disrete even though it is tehnially ontinuous 
we used two dierent methods to disretize our data  k means and equi depth partitions  in eah ase  we
ran the disretizations on eah protein individually  as per dr  sahs s suggestions  we ran our disretization
methods with   and   bins  the partition boundaries for eah of the four disretizations over a subset of the
proteins are shown below 

  k means

  equi depth

   

   

   

   

cd  

     

     

      

      

cd  

     

     

      

     

cd  

     

     

      

     

  k means

  equi depth

   

   

   

   

   

   

   

   

   

   

cd  

      

     

     

     

     

      

      

      

      

     

cd  

     

     

     

     

     

      

      

     

     

     

cd  

     

     

     

     

     

       

       

       

     

     

the top row tells us whih olumns orrespond to whih disretization tehnique  and the number of bukets 
the seond row tells us whih boundary partition is being represented 

for example      means we are

splitting the rst and seond buket by the value indiated below  the rst olumn orresponds to spei
proteins  and the internal values are protein level boundaries 

   

network learning algorithm

the number of possible bayesian networks sales hyperexponentially with the number of nodes in the nal
graph 

thus  with    nodes in our graph  it would have been impossible to exhaustively searh through

all possible graphs  instead  we had to run a heuristi based searh funtion  we hose to use greedy hill
climbing  whih works as follows 
   randomly initialize the network struture 
   iterate through possible  steps   where eah step involves adding or remove an edge  and exeute the
step with the most positive eet 
   repeat until onvergene 

 

fiat eah step  we sored our network using bde  bayesian dirihlet likelihood equivalenesee below   whih
provides a onjugate prior for disrete data on a multinomial distribution  it penalizes both poor t and too
many parameters  the bde sore of the nal network gave us a measure of training error  our measure of
test error was the bde sore omputed with respet to a test sample that the network did not see before  
our implementation of the greedy hill climbing algorithm was done using the biolearn pakage  a set of
java les provided by dr  dana pe er at columbia university 

   

sampling and model averaging

due to the size of the dataset  we were fored to run our algorithms on samples drawn from the data  we
implemented a sampling sript that randomly drew     data points from eah original data le  multithreading was neessary to make the sript run in reasonable time   we then trained our network on this sampled
le 
unfortunately  as the figure   shows  training our networks on single samples provided unsatisfatory results 
the networks were extremely dense and ontained many edges that would appear only in the network from
that partiular sample  this is partially beause the networks overreated to minor peuliarities within the
sample le they were generated from  leading to overtting  another ontributing fator was the fat that 
as mentioned above  it is impossible to exhaustively searh through all possible bayesian networks  sine
the greedyhillclimbing algorithm we used makes only inremental adjustments to the network  it runs the
risk of getting stuk at loal optima  whih depend on the random initialization of the network 
to ameliorate this  we implemented model averaging  with model averaging  we trained bayesian networks
on     dierent samples  we then ounted the number of networks any given edge appeared in and inluded
in our nal network all the edges that appeared in at least    networks  this greatly improved our network s
density and preditive power 

   

soring

to sore our networks  both while generating them and to analyze our nal results  we used the bayesian
dirihlet likelihood equivalene funtion  bde for short   the bde sore is a negative number where values
loser to   indiate better networks  a brief desription of the bde funtion is below 
the bde soring method assumes that the distribution of the parameters

p d  g 

is dirihlet for some

omplete ayli graph in a spae d  we also assume that the network fulls some other riteria  inluding
parameter independene  parameter modularity and likelihood equivalene  in order to make the sore omputable  then  for any bayesian networks in d that fulls these assumptions  the joint probability of the
network with data t is

p  b  t     p  b  

qi
n y
y

i   j  

where

xi is

the set of parents of node i 

wij is

ri

y
 nijk   nijk
 
 nij  



 nij   nij  
 nijk  
k  

the jth onguration of

xi i e 

 

one of the many possible

the number of possible ongurations  nij is the number of instanes in whih variable
xi takes its kth value  and nij   n   p  xi   xij   xi   wij  g    the equivalent sample size  expresses the
strength of our belief in the distribution   n     n     is the gamma funtion 

sets of parents 

qi is

the bde sore is then

bde b  t     log p  b  t    

as the expression indiates  the bde balanes the

likelihood of the network  p  b    whih is a reetion of the number of assumptions made  and the rest of
the expression  whih reets how well it ts the data 

 

fi  results and disussion
we used as our orale the sore of a run on a single sample and as our orale the sore of a run on the data
set it was trained on  we then sampled new test sets of idential sizes to the training test sets for the test

  are as follows 

sore  the sores

test name

training sore

test sore

averaged   k means

          

          

averaged   equal bukets

          

          

averaged   k means

          

          

averaged   equal bukets

          

          

sample run   k means

           

           

sample run   equal bukets

           

           

sample run   k means

          

          

sample run   equal bukets

          

          

the graph for our   k means run  whih was our best performer  is in figure    the graphs for our   equal
bukets    k means  and   equal bukets are in figures       and    respetively 
in general  the runs with   disretization bins outperformed the runs with    further  the graphs for the two
runs with   bins losely resembled eah other  as did the runs with   bins  this suggests that the networks
with   bins were apturing dierent  and more omplex  relationships than the networks with only   
from a biologial perspetive  there was an edge from transloation to the cd   protein in all our runs 
this indiates that whether or not a patient has the crlf  transloation has a very signiant impat on
their cd   level  thus  cd   ould be a key part of the mehanism by whih patients with the crlf 
transloation have worse prognoses than those without the transloation  similar eets were found for the
proteins cd    cd    and cd    whih were found in almost all networks with   bins  and for the proteins
cd    and pax   whih were found in a majority of the networks with   bins   further researh is neessary
to determine why these proteins are so inuened by the presene of a crlf  transloation and what impat
they ould have on a patient s survival hanes  dr  sahs and her olleagues will be analyzing our results
and may hopefully be able to suggest new areas of researh based o our ndings 

  referenes
charaterization of patient spei signaling via augmentation of bayesian networks with disease and patient state nodes  k  sarhs  a  j  gentles  y  ryan  s  itani  j irish  g  p  nolan    st annual international
conferene of the ieee embs       
igh transloations  crlf  deregulation  and mirodeletions in adolesents and adults with aute lymphoblasti leukemia  a v moorman  c  shwab  h m  ensor  l j  russell  h  morrison  l  jones  d  masi  b  patel  j m  rowe  m  tallman  a  h  goldstone  a  k  fielding  c  j  harrison 

j clin onol 

                  
soring funtion for learning bayesian networks  a  m  carvalho  tehnial report  inesc id te 

rep 

             
single ell mass ytometry of dierential immune and drug responses aross a human hematopoieti ontinuum  s c  bendall  e f  simonds  p  qiu  a d  amir  p o  krutzik  r  fink  r v  burggner  r  melamed 
a  trejo  s k  plevritis  g p  nolan
single ell proling of potentiated phospho protein networks in aner ells  j  m  irish  r  hovland  p o 
krutzik  o  bruserud  g p  nolan  cell       

  note 

after the poster session  we disovered several bugs in our soring funtion  whih is why the bde sores are very
dierent here than they were on our poster 
 

fi  figures

figure    a noisy  dense bayesian network generated from a single sample

figure    the bayesian network generated by the averaged   k means algorithm

figure    the bayesian network generated by the averaged   equal bukets algorithm

figure    the bayesian network generated by the averaged   k means algorithm

figure    the bayesian network generated by the averaged   equal bukets algorithm

 

fi